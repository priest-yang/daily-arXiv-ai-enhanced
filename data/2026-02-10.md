<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.RO](#cs.RO) [Total: 82]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文提出了一种层级贝叶斯模型，通过建模鞋底偶发性磨损特征的稀有度，提高了法医鞋印证据分析的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 在法医学中仅通过鞋印匹配鞋子的品牌和型号通常无法锁定嫌疑人，因此需要量化鞋底由磨损等偶发因素形成的独特印记的稀有性，以增强证据说服力。

Method: 提出了一种层级贝叶斯建模方法：（1）采用潜在高斯模型及嵌套拉普拉斯逼近，从而对大规模有标注鞋印数据进行高效推断；（2）引入空间变系数以刻画鞋底花纹与偶发印记之间的空间关系。

Result: 在保留测试数据上的实验结果显示，本文方法在稀有度量化和鞋印归属判定准确性方面优于现有方法。

Conclusion: 引入层级贝叶斯模型和空间变系数能有效提升偶发性磨损印记的量化能力，从而加强法医学中鞋印证据的准确性与可信度。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于归因的人类先验对齐方法，以提升模型不仅在预测准确性上表现好，同时在决策证据上更合理。通过在训练时引入以人为先验的输入区域指导归因，并在模型依据非先验区域时施加惩罚，从而提升模型的合理性和准确性。实验证明该方法在图像分类和GUI智能体的操作决策任务中均显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习依赖类别标签，模型容易通过依赖捷径相关性获得高准确性，但未必基于合理依据做判断。此外，虽然引入人类先验可以引导模型，但由于表示差异，模型难以天然对齐人类感知。因此，需要新的方法让模型更好地遵循人类决策证据。

Method: 方法将人类先验用输入区域（如bounding box）形式编码，并采用高度忠实的基于子集选择的归因方法，在训练中展现模型决策依据。当模型归因区域与先验区域偏离时，通过惩罚措施减少对非先验证据的依赖，促使模型归因向人类期望区域对齐。具体做法是在损失函数中引入归因约束。

Result: 在图像分类和基于GUI的多模态大模型点击决策任务上进行验证，无论是常规分类还是自回归生成模式下，引入人类先验对齐方法后，模型准确率提升，且归因区域与人类先验更一致，决策合理性也增强。

Conclusion: 基于归因的人类先验对齐方法能有效提升模型既能做对也能做“对得合理”的能力，缓解模型依赖捷径相关性的倾向，为引入人类知识指导可信决策提供了新的思路。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 该论文提出了MAU-Set数据集和MAU-GPT多模态大模型，用于提升工业产品异常检测的准确性和泛化能力，在多个工业领域超过现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前工业异常分析存在数据集覆盖有限、模型对复杂异常模式泛化能力弱等瓶颈，亟需更全面的数据资源和强泛化能力的模型以提升自动质检水平。

Method: 1. 构建MAU-Set工业异常理解大规模数据集，涵盖多个工业领域，任务层次从二分类到复杂推理。2. 设计标准化评测协议保证模型评估的公平和全面。3. 提出MAU-GPT大模型，采用AMoE-LoRA机制，融合异常感知与通用专家知识，提升各类缺陷的识别与推理能力。

Result: MAU-GPT在所有测试领域和任务中均优于现有主流方法，表现出卓越的泛化与自动化工业检测能力。

Conclusion: MAU-Set与MAU-GPT构建了新基线和通用方法，为工业质检的智能化、自动化提供了更可靠的数据和模型支撑，具备广阔实际应用前景。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种通用的视网膜分割与定量分析框架RetSAM，可高效从眼底照片中分割多种结构/病灶并提取标准化生物标志物，推动大规模眼科及全身性疾病的相关性研究。


<details>
  <summary>Details</summary>
Motivation: 虽然眼底影像广泛可及、能反映眼部和全身健康状态，但因缺乏公开多标签数据和统一的分割-量化流程，使大规模相关性研究受限。

Method: 作者开发了RetSAM框架，在20万多张眼底图片上通过多阶段训练，支持三类任务，可分割五种解剖结构、四种表型特征和二十余种病灶，并进一步将分割结果转化为30余种标准化生物标志物，用于眼科和全身疾病评估。

Result: RetSAM在17个公开数据集上分割性能优于以往方法，平均提升3.9个百分点，某些多任务难点上最高提升15个百分点，并展现出优良的泛化能力。生成的生物标志物可关联多种主要眼科疾病。

Conclusion: RetSAM极大提升了眼底照片生物标志物的可获取性和标准化表现，为大规模眼科与全身疾病研究及临床转化铺平了道路。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: 论文提出了CR-VLM（可配置拒绝的视觉语言模型），解决了现有VLM拒绝机制不够灵活、适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）的发展，如何灵活、安全地进行内容拒绝成为一项重要挑战。但现有方法缺乏适应性，无法满足不同用户与场景需求，容易导致拒绝过多或过少。

Method: 作者设计了CR-VLM，包含三个核心模块：(1)利用teacher-forced机制提取可配置的拒绝向量，增强拒绝信号；(2)引入门控机制，避免对合法请求的过度拒绝；(3)提出反事实视觉增强模块，使视觉表征与拒绝要求更好对齐。

Result: 大量实验覆盖多个数据集和不同VLM，表明CR-VLM在实现灵活、准确、稳健的拒绝行为方面表现优异，同时具有高效性和可扩展性。

Conclusion: CR-VLM为提升VLM安全性和用户自适应性提供了一条可行且强大的路径，有助于未来VLM的负责任部署。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 该论文提出了Vectra，一种面向跨境电商商品图片的无需参考、多维解释型视觉质量评估框架，显著提升了对图像翻译后呈现质量的评估效果。


<details>
  <summary>Details</summary>
Motivation: 当前针对跨境电商图片翻译（IIMT）大多关注翻译文本的机器翻译评估，忽略了图片视觉呈现质量，尤其在多模态缺陷和复杂场景下，现有基于参考的评估方法缺乏可解释性，模型判官法也缺乏细粒度的、具体于电商场景的评价信号，因此需要更合适的评估方法。

Method: 作者提出Vectra体系，包括：1）Vectra Score，将视觉质量细分为14个可解释维度，并引入空间感知的Defect Area Ratio（DAR）减少标注歧义；2）Vectra Dataset，含110万商品图片，包含2K基准集、3万推理标注以及3,500专家偏好对；3）Vectra Model，一个40亿参数的多模态大模型（MLLM），能输出定量得分与诊断推理。

Result: 实验结果显示，Vectra的评估得分与人工排名的相关性达到业界领先，且其模型在打分表现上优于包括GPT-5和Gemini-3等头部大模型。

Conclusion: Vectra实现了可解释、无需参考的电商图片翻译视觉质量评估，为该领域提供了新的高效工具，未来将开放数据集和模型，促进行业发展。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 本文提出了一种结合MobileNetV3-Large和EfficientNetB0的混合CNN架构，专门用于孟加拉纸币识别，并通过融合多数据集与解释性AI方法，取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的纸币识别技术难以满足视障人士在真实环境下的需求，且存在对他人的依赖容易受到欺诈的问题。因此，作者希望开发一种高效且适合边缘设备的自动纸币识别系统。

Method: 1. 构建了包含受控与真实场景的新孟加拉纸币数据集；2. 融合了四个额外（含公开基准）数据集，提升模型泛化能力；3. 提出混合CNN（MobileNetV3-Large+EfficientNetB0）做特征提取+多层感知机（MLP）分类器，兼顾效率与低算力需求；4. 多项评估指标验证性能，并用LIME、SHAP加深模型解释性。

Result: 在受控数据集上模型准确率达97.95%，复杂背景为92.84%，综合数据集为94.98%；并通过五折交叉验证及七种指标证明了模型效果优越。

Conclusion: 该方法不仅在精度与效率间取得平衡，而且具备在资源受限设备上的部署潜力，并通过可解释性方法提升了用户信心，非常适合视障者辅助场景。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 该论文提出利用高斯约束的表示方法改进无监督3D场景重建，尤其适用于来自多场景、包含混杂内容和噪声的实际图片。通过实验证明，这种方法在IMC2025任务中比传统启发式方法具有更好的场景分离与相机位姿估计能力。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中获取的图片往往来源复杂、内容多样且包含干扰，对无监督3D场景重建提出了极大挑战。尤其是在IMC2025这样需要独立场景识别和相机位姿估计的任务中，现有方法面临视觉歧义和混合内容的问题。因此，需要一种能够提升场景区分和位姿估计鲁棒性的创新表示方法。

Method: 受LeJEPA架构启发，作者设计了三种逐步改进的管线，最终采用一种对图片嵌入施加各向同性高斯约束的方法。这些管线无需引入新理论，而是专注于实证分析高斯约束对聚类一致性和位姿估计鲁棒性的实际影响。

Result: 在IMC2025数据集上的实验表明，高斯约束的图片嵌入比传统启发式方法在场景分离和位姿合理性方面表现更佳，特别是在视觉歧义较高的环境下更为显著。

Conclusion: 论文说明了理论驱动的表示约束方法能够有效提升结构光流（SfM）流程的实际效果，为自监督学习原则与实际视觉重建任务之间的结合提供了有力证据和未来方向。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出一种名为XAI-CLIP的新方法，通过结合多模态视觉-语言模型，改进医学图像分割的可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的医学图像分割模型性能优越，但其可解释性有限，难以获得临床信任。现有可解释方法计算成本高、噪声大且相关性不强，需要更高效且有临床意义的解释方案。

Method: 提出了XAI-CLIP方法，利用多模态视觉-语言模型（如CLIP）的嵌入，通过语言信息引导的区域（ROI）定位，实现区域感知的扰动与解释。同时与医学图像分割流程融合，生成边界更清晰且结构相关性更强的显著性图，并极大降低计算量。

Result: 在FLARE22和CHAOS数据集实验显示，XAI-CLIP运行时降低60%、Dice得分提升44.6%、IoU提升96.7%，相较传统扰动方法解释效果更好且更快。定性分析表明生成的归因图更清晰，解剖结构一致性更高，伪影更少。

Conclusion: 将多模态视觉-语言表征引入扰动型XAI框架，显著提升了医学图像分割的可解释性和效率，有望推动其在临床中的透明可信应用。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [10] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的图像识别框架，能够通过摄像头自动识别鸟类种类、群体类型及规模，以提升机场鸟击预防的智能化与效率。


<details>
  <summary>Details</summary>
Motivation: 目前机场主要依赖鸟类雷达系统进行鸟击预防，但雷达无法分辨鸟的种类，而不同鸟类的飞行行为和高度偏好存在显著差异。这种识别能力的缺失限制了风险评估与干预的准确性。

Method: 基于现有的摄像头图像数据，设计CNN网络自动识别鸟类种别、群体类型和群体规模。此外，结合鸟类种类信息作为输入，建立物种特定的运动轨迹预测模型，从而实现更精准的飞行路径预警。

Result: 该方法能够区分不同鸟类物种，并准确估计鸟群类型和数量，为机场提供更细致和风险导向的管理决策支持。相关检测结果为种群识别和运动预测提供有力数据支撑。

Conclusion: 新方法弥补了雷达技术不足，实现了对鸟类多维度特征的自动识别，为机场鸟击风险预判与响应提供技术基础，助力提升航空安全水平。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [11] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 本文分析了视觉-语言模型（VLMs）在多目标视觉任务中的失败机制，提出通过表征几何结构解释其内部原理，通过“概念向量”操控实验证明，向量间几何重合度可量化预测模型的特定错误。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多目标场景下常出现与人类认知局限类似的错误（如绑定问题等），但导致这些错误的人工系统内部机理尚不明确，需要建立定量和可解释的分析框架。

Method: 研究团队分析开源VLM（如Qwen、InternVL、Gemma）内部的表征几何，提出并比较提取视觉“概念向量”的多种方法，并通过干预实验（如引导模型将红花识别为蓝花）来验证向量的有效性。

Result: 通过对模型内部概念向量的操控，能可靠操纵VLM在视觉任务中的表现。进一步发现，不同概念向量之间的几何重叠度与视觉任务中出现的特定错误类型高度相关。

Conclusion: 该研究为人工视觉-语言模型中的错误行为提供了量化、可解释的内部表征机制，为未来改进VLM在多目标视觉任务中的表现提供了新方法论基础。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [12] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出了新的模态对齐理论与方法，通过无监督方式精确消除视觉与文本表征间的Modality Gap，实现多模态大模型的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对比学习面临 Modality Gap 问题，即视觉与文本即便表达相同语义，其嵌入仍在特征空间中存在系统性偏移。之前的弥补方法受制于过于简化的各向同性假设，限制其在大规模应用中的效果。

Method: 作者首先提出 Fixed-frame Modality Gap Theory，将模态间偏差分解为稳定偏置和各向异性残差。随后提出无训练的对齐策略 ReAlign，利用大规模无配对数据，分三步（锚定、迹对齐、质心对齐）将文本嵌入对齐到视觉分布。基于 ReAlign 设计了可扩展的预训练范式 ReVision，使模型在无须高质量图文对时即可学习视觉特征分布。

Result: 该框架证明，仅靠统计对齐的无配对数据即可近似甚至替代昂贵的图文对，为多模态大模型节省了数据和训练成本。

Conclusion: 通过精确建模与无配对训练，作者提出的新方法能显著减少 Modality Gap，为多模态大模型的高效、低成本扩展提供了可行路径。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [13] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时自适应（TTA）框架Fair Context Learning（FCL），提升VLM模型如CLIP在分布变化下的鲁棒性，通过避免基于熵最小化的方法，有效缓解对共享视觉特征的偏置，实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前大多数TTA方法通过最小化预测熵提高模型在未标记测试集上的泛化能力，但这类方法容易陷入对部分视觉特征的过度依赖（spurious correlation），导致对视觉特征相似的类别产生过强自信的错误。本文旨在解决在分布偏移下VLM模型（如CLIP）性能下降的问题，避免上述熵最小化的缺陷。

Method: 提出公平上下文学习（FCL）框架，基于证据加和分解假设，将TTA过程分为两步：一是利用增强方法探索可能的类别候选，二是通过公平性驱动的校准机制调整文本上下文，使针对共享视觉证据的敏感度更加均衡。该公平约束能够有效缓解对部分特征的过度依赖，无需依赖熵最小化。

Result: 大量实验结果表明，FCL方法在多个分布偏移和细粒度识别基准下，适应性能与当前最新的TTA方法相比表现竞争力，并从实证上验证了理论动机。

Conclusion: FCL框架有效提升了VLM模型在分布变化环境下的鲁棒性，克服了传统基于熵最小化TTA方法的问题，是TTA领域有潜力的创新方向。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [14] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 论文比较了标准CNN与结合自适应神经模糊推理系统（ANFIS）的神经网络在干净样本和对抗攻击下的表现，发现ANFIS集成提升对抗鲁棒性依赖于模型架构，并非一律有效。


<details>
  <summary>Details</summary>
Motivation: CNN虽然图像分类性能强，但可解释性差且易受对抗攻击。为提升可解释性，部分研究用神经模糊方法（如ANFIS）增强CNN，但其鲁棒性的影响还没被系统性探究。

Method: 作者用标准CNN（ConvNet、VGG、ResNet18）及其融合ANFIS后的版本，在多个数据集（MNIST、Fashion-MNIST、CIFAR-10、CIFAR-100）上，分别测试其在PGD（梯度攻击）与Square（非梯度攻击）下的表现，然后进行比较分析。

Result: 融合ANFIS后，并未普遍提升干净样本准确率，对抗鲁棒性的提升与模型结构相关：ResNet18-ANFIS对抗鲁棒性提升，VGG-ANFIS表现则不如原VGG。

Conclusion: 神经模糊增强方法能提升部分CNN架构的对抗鲁棒性，但并不是对所有架构都有效，因此其增益具有选择性。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [15] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: 本文提出了UNIKIE-BENCH，一个用于评估大型多模态模型（LMM）在文档关键信息抽取（KIE）任务上能力的统一基准。通过对15种先进LMM进行实验，发现现有模型在不同场景和布局下表现不稳定，揭示了该领域的诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 当前文档KIE任务因文档布局多样、视觉质量参差以及各种复杂信息需求，仍面临很大挑战。新兴的LMM展示出从图片直接完成端到端KIE的潜力，但缺乏系统性、全面性的评测手段。为此，论文旨在填补LMM在现实复杂场景KIE能力评估上的空白。

Method: 作者提出UNIKIE-BENCH，一个包含两部分的统一LMM KIE评测基准：（1）受限类别KIE赛道，利用预定义结构化模板，反映实际应用需求；（2）开放类别KIE赛道，允许从文档中抽取任意显式关键信息。利用该基准，对15个SOTA LMM模型进行了系统分析和评测。

Result: 实验显示，随着模板多样性、稀有字段（长尾字段）以及复杂布局的增加，LMM的KIE性能显著下降。不同文档类型和应用场景间，性能分化明显，暴露出LMM在实际任务中的不足。

Conclusion: 尽管LMM在文档KIE中表现出了潜力，但在精确定位关键信息和复杂布局理解方面仍存在显著挑战。UNIKIE-BENCH为今后文档KIE研究和模型改进提供了可靠的评测工具和方向。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [16] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: 本论文提出OMNI-Dent，一个结合临床推理与视觉-语言模型（VLM）的口腔诊断框架，实现高效、可解释的牙齿健康评估，且降低对专家标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 目前AI口腔诊断主要依赖视觉模式识别，缺乏模拟专业牙医的临床推理，且数据需求大、泛化能力差，难以适应现实多样化图像。解决这些问题，可帮助提升大众获取口腔健康服务的可及性。

Method: OMNI-Dent 基于多角度手机照片输入，结合牙科专家的诊断启发式，驱动通用VLM进行逐颗牙齿评估，无需针对牙科任务对VLM进行专门微调，强调数据高效与可解释性。

Result: OMNI-Dent 能利用VLM现有的视觉-语言能力，在非标准临床图像（如手机拍照）条件下，为用户提供牙齿异常预警和就医建议，尤其适合口腔专业资源有限的场合。

Conclusion: OMNI-Dent作为一款早期辅助诊断工具，可以帮助公众初步自查口腔健康、判断是否需要专业就诊，为缺乏亲身就医机会的人群提供实用的解决方案。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [17] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: 本文提出了COMBOOD无监督半参数框架，通过结合最近邻与Mahalanobis距离，实现更精准的图像识别领域OOD检测，无论远O或近O场景均表现出色，并超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 在机器学习实际应用中推断阶段的O域样本识别至关重要，特别是在自动化或开放环境下。现有方法在近O和远O场景下均有不足，因此作者希望提升两种场景下的检测性能。

Method: COMBOOD结合了两种信号：最近邻距离（非参数方法，适合多种OOD场景）和Mahalanobis距离（参数方法，特别适用于远O情形但近O效果有限），在半参数框架内生成推断样本的置信分数。其算法可结合不同的特征提取方式，对样本进行OOD检测。

Result: 在OpenOOD各版本及documents数据集上，COMBOOD对近O和远O样本均能实现准确检测，比主流方法更优，提升在多个基准数据集上均为统计显著，且在嵌入空间规模增大时依旧线性扩展。

Conclusion: COMBOOD是一种高效、准确且可扩展的OOD检测框架，能在各种实际、复杂场景中有效区分分布外样本，有助于提升机器学习应用的鲁棒性。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [18] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: 本文提出了PipeMFL-240K，这是首个大规模、精细标注的管道磁通泄漏（MFL）检测数据集和基准，用于复杂目标检测任务。该数据集包含240,320幅图像和191,530个高质量标注，支持MFL自动化分析的发展。


<details>
  <summary>Details</summary>
Motivation: 管道完整性关乎工业安全与环境保护，而MFL检测是关键的无损检测方法。深度学习在自动化MFL图像分析中的潜力受到缺乏大规模公开数据集和基准的限制。这阻碍了模型的公正比较和复现实验，因而亟需此类数据集推动领域进展。

Method: 作者收集并精细标注了来自11条总长约1,480公里管道的MFL伪彩图像，涵盖12个类别，总计240,320张图像与191,530个高质量目标边框。设计重点强调类别极度不均衡、大量微小目标及明显类内变化，并用主流目标检测算法进行实验，为基准测试提供基线。

Result: 实验证明，尽管采用了最新目标检测器，但在该数据集上表现有限，难以有效应对MFL图像的内在复杂性。这表明当前方法尚有较大提升空间。PipeMFL-240K也显示出显著的挑战性和实用性。

Conclusion: PipeMFL-240K作为首个公开的、面向MFL检测的大规模数据集和基准测试，奠定了管道完整性诊断与维护规划的数据基础，将加速算法创新和可复现研究，为MFL自动化解读及其在工业安全领域的推广提供强有力支撑。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [19] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了专用于遥感领域复杂推理的视觉语言推理基准（VLRS-Bench），用于推动多模态大语言模型在遥感智能推理任务上的发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）已能进行复杂推理，但遥感领域的公开基准仍以感知类任务为主，未能涵盖认知和决策等更复杂推理任务，制约了相关应用发展。

Method: 构建了VLRS-Bench基准，涵盖认知、决策和预测三大维度，共包含2,000组长文本问答，覆盖14项任务与多个时间阶段，并结合领域专家知识和遥感特有先验，保证了地理空间的真实性与推理复杂度。

Result: 实验显示，现有主流MLLMs在该基准上表现存在显著瓶颈，反映了多模态推理在遥感领域的不足。

Conclusion: VLRS-Bench为遥感多模态推理能力的研究提供了关键工具，有助于推动遥感AI领域在高阶智能任务上的进展。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [20] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: 提出了一种新颖的数据感知型XCV方法ShapBPT，通过分层Shapley公式，在图像的多尺度结构下提高特征归因的效率和语义一致性，实验和用户调查均验证了其优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有分层Shapley方法未利用图像的多尺度结构，导致收敛慢且与真实形态特征对齐差，同时缺乏面向视觉任务的数据感知层次结构，影响模型的可解释性。

Method: 作者提出ShapBPT方法，基于分层Shapley公式，将Shapley系数分配到基于图像本身的多尺度二元分割树（Binary Partition Tree, BPT）结构上，实现特征归因与图像形态的对齐，并降低了计算开销。

Result: ShapBPT方法优于现有XCV方法，无论是与图像结构的对齐程度，还是计算效率；此外，在用户研究中，ShapBPT的解释更被人类用户所偏好。

Conclusion: ShapBPT实现了分层Shapley值方法和图像数据结构的有效结合，显著提升了视觉任务模型的可解释性与效率。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [21] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 本文提出了一种名为ECHWR的新手写识别训练框架，改善了特征表示和识别准确性，并且不增加推理时资源消耗，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于惯性测量单元（IMU）的手写识别在边缘硬件上面临内存受限的问题，如何在保证模型高效性和隐私的情况下提升识别性能成为亟需解决的难题。

Method: 提出了Error-enhanced Contrastive Handwriting Recognition（ECHWR）训练框架，在训练阶段引入一个临时辅助分支，将传感器信号和语义文本嵌入对齐。通过双重对比损失（批内对比损失+新颖的基于错误的对比损失）提升表征能力。训练结束后丢弃辅助分支，部署时不增加额外资源消耗。

Result: 在OnHW-Words500数据集上，ECHWR在独立书写者和依赖书写者分组下分别将字符错误率降低了7.4%和10.4%，显著超过现有基准方法。消融实验还表明基于错误的对比损失对处理未见书写风格尤其有效。

Conclusion: ECHWR训练框架在不增加推理成本的前提下显著提升了基于IMU的手写识别准确率，尤其适合边缘设备部署，对提升实际应用可行性具有重要意义。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [22] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 本论文探究了大型视频编码模型内部物理表征的机制，发现物理变量在模型中以分布方式出现，并揭示了物理信息在中间层急剧涌现的区域。


<details>
  <summary>Details</summary>
Motivation: 长期以来，人们一直在讨论视频物理推理模型是否必须明确分解出物理变量（如速度、方向等）来实现准确预测，还是可以通过任务特定和分布式的方式隐式表达这些物理属性。现有视频物理模型虽然性能优秀，但其内部表征机制尚未清晰。

Method: 作者通过层级探测（layerwise probing）、子空间几何分析、局部解码（patch-level decoding）、注意力消融等方法，系统性分析了不同架构的视频变换器模型内部物理信息的分布与组织方式。

Result: 跨多种模型架构，作者发现物理变量信息在中间层存在明显转折（Physics Emergence Zone），在此区域内物理变量表征峰值突出，并在后续层逐渐减弱。标量物理量（如速度、加速度）能在较早层提取，而方向信息则在该“涌现带”出现，并以高维结构分布编码，需要多特征协同干预才能控制。

Conclusion: 现代视频模型并未像经典物理引擎那样因子化物理变量，而采用足够分布式的方式编码物理信息，这种结构足以支持精确的物理推理。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [23] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的统一自动车牌识别（ALPR）方法，能够在单个前向推理中同时完成车牌识别、州分类和车辆属性提取，显著提升准确率并简化系统架构。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统采用多阶段处理流程（目标检测+OCR），容易导致误差积累、延迟增加以及系统复杂。作者旨在通过VLM消除这些问题，实现高效且多任务兼容的识别系统。

Method: 作者提出了名为Neural Sentinel的新方法，使用经过LoRA微调的PaliGemma 3B视觉语言模型，统一处理车牌识别与车辆多属性问答。系统还引入了人机协作的持续学习架构，通过经验回放结合用户修正提升模型能力，并利用70:30的数据比例防止遗忘现象。

Result: 在真实收费站影像实验中，该系统车牌识别准确率达92.3%，分别比EasyOCR和PaddleOCR提高14.1%和9.9%；推理延迟仅152ms，校准误差为0.048。模型还实现了对车辆颜色、是否系安全带、乘员计数等任务的零样本泛化，准确率分别达89%、82%、78%。

Conclusion: 基于视觉语言模型的统一识别框架，不仅提升了准确率、系统简单性，还具备多任务随到随用的能力，预示着车牌识别领域范式的重大转变，超越传统多阶段流水线方法的局限性。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [24] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 本论文提出了一种基于低成本可见光和红外摄像头的无标记神经导航方法，通过算法建模面部几何实现精准仪器定位，并在50名受试者上验证了方法的高精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经导航系统通常需在头部安装物理标记，需手动配准且容易移位，使用上复杂、成本高，并影响病人舒适度，因此急需更简便、低成本且舒适的替代方案。

Method: 采用多种基于可见光和红外深度相机（具备立体或深度感知能力）的无标记方法，通过算法对人脸几何进行建模，实现对头部的追踪与导航。并与传统有标记系统进行了对比，在50名受试者上评估精度。

Result: 最佳无标记方法的追踪误差中位数仅为2.32 mm和2.01°，与传统有标记系统相比已达到经颅磁刺激的精度要求，且相比以往无标记方法大幅提升。此外，多传感器数据融合可进一步提升精度。

Conclusion: 所提出的无标记神经导航系统可大幅度降低成本和操作复杂度，提高患者舒适度，有望在临床与科研中推广应用。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [25] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的城市设计工具RECITYGEN，结合潜变量扩散模型与交互式语义分割，实现了基于文本的城市街景生成，促进了公众参与。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法常忽视公众意见，导致设计初衷与实际需求脱节。新兴数字工具推动了城市设计的参与性，但生成设计的门槛依然不低，亟需更高效、易用的参与式工具。

Method: 将最前沿的潜变量扩散模型与交互式语义分割技术结合，开发了RECITYGEN工具。用户可以通过文本描述，交互式生成多样化的城市街景图像。并在北京城市更新中的试点项目上进行了实际应用。

Result: RECITYGEN在试点中允许公众提出设计建议，能够较好反映公共偏好。用户能够以较低门槛参与城市环境的再设计，提升了交互性和包容性。也发现了一些待改进的局限。

Conclusion: RECITYGEN为城市设计提供了更开放与互动的方式，显著提高了公众参与度，有助于未来更加包容和动态的城市规划实践。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [26] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: 本文提出了一种高效的数据遗忘方法FADE，专为文本到图像扩散模型实现有选择地移除特定数据影响，同时保持其它能力。


<details>
  <summary>Details</summary>
Motivation: 近年来数据隐私法规和AI责任要求推动了模型“遗忘”能力的发展，但在扩散模型中，遗忘过程计算量大且难以在有效遗忘和知识保留之间平衡，急需可行优化方法。

Method: FADE方法分为两阶段：首先，利用梯度显著性评估关键参数，通过稀疏LoRA适配器进行有针对性的局部参数调整；其次，用自蒸馏机制将遗忘概念替换为用户自定义代理概念，同时保持其他数据表现，从而达到高效且可逆的遗忘。

Result: 在UnlearnCanvas基准以及多个公开数据集上的消融实验表明，FADE在概念遗忘和知识保留方面达到了最优性能，遗忘粒度可控，适用于多领域扩散模型。

Conclusion: FADE方法为选择性遗忘提供了一种高效、内存友好、可逆的解决方案，非常适合实际生产环境中对扩散模型的敏感数据安全与隐私需求。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [27] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 本论文提出了一种可协助评估废钢污染程度和分类的新型计算机视觉流程，用于钢铁制造中的废钢检验，显著提升了准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 钢铁制造过程中，废钢中的非金属夹杂污染直接影响能耗、排放和安全。目前主要依靠人工目测判断污染程度，这种方法主观性强且作业环境危险。因此，提升废钢质量评估的客观性和自动化水平意义重大。

Method: 作者设计了一套基于计算机视觉的流程，通过对卸载废钢过程中拍摄的图像进行分析，实现污染百分比的回归评估与废钢类型分类。方法采用多实例学习（MIL）和多任务学习（MTL），基于时间序列数据估计污染程度，并融合分类任务。同时，该系统集成至接收流程，包含自动检测、分层推理、置信度输出与人工校正，配合主动学习机制持续优化性能。

Result: 多实例学习（MIL）方法在污染回归任务上达到了0.27的平均绝对误差（MAE）和0.83的R2值，表明精度较高。多任务学习（MTL）配置下，回归任务MAE为0.36，分类F1值为0.79。整体流程可实现近实时评估并支持人工纠偏与系统自学习。

Conclusion: 新系统能够减少主观判断带来的误差，提高作业安全性，支持接收与熔炼计划流程的自动化集成，对钢铁废料管理具有实际应用价值。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [28] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: OmniFysics是一个整合图片、音频、视频和文本的紧凑型多模态模型，通过创新的数据生成和物理知识注入机制，提升了物理理解能力，并在物理相关任务上取得了更好表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在物理理解方面表现脆弱，因为关键的物理属性在大规模网络数据中往往表现含糊且稀缺。该工作旨在通过模型设计和数据生成机制提升模型在复杂物理场景下的理解能力。

Method: 作者提出了OmniFysics模型，通过构建包含FysicsAny和FysicsOmniCap的物理数据引擎，分别负责基于数据库的物理属性指导和高保真物理属性视频标注生成。训练中采用分阶段多模态对齐、指令微调、潜空间流匹配和意图路由等技术提升模型表现。

Result: OmniFysics在标准多模态基准任务上表现优秀，尤其在物理为导向的评测中取得了更好的成绩。

Conclusion: 通过显式注入物理知识和创新的数据生成方式，可以有效提升多模态模型对物理场景的理解能力。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [29] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的端到端方法，能快速准确地从光学图像序列中估算物理介质的连续位移和材料可压缩性，显著提升了传统方法的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统的图像位移评估和非接触材料探测方法依赖于迭代型算法，如有限元法和有限差分法，处理速度慢，不适合高通量数据。这限制了其在一些直接物理测量不可行的工程和生物医学场景下的应用。

Method: 提出一个包含两个人工神经网络的深度学习框架，分别用于图像配准与材料可压缩性估算，实现了光学图像到材料机械属性的端到端预测。通过在参考数据集上训练模型，并对比传统方法进行效率和精度评测。

Result: 实验结果表明，该深度学习模型无论在效率还是准确度上均优于传统方法。即使在图像配准的映射与参考位移场存在较大局部偏差时，模型依然能够准确预测材料的可压缩性。

Conclusion: 端到端深度学习模型能识别矢量场的高阶认知特征（如涡度），而不仅仅依赖于传统图像局部特征。这赋予模型更高的准确性，推进了非接触式光学测量物理介质材料属性的研究。

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [30] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR提出了一种双向奖励引导的扩散超分辨率方法，在合成与真实低分辨率图像上均表现优异，兼顾结构还原与感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散超分辨率方法受限于合成数据训练，与真实低分辨率图像存在分布差异，导致实际应用时重建效果不佳。因此，亟需一种能有效泛化到真实图像的超分方法。

Method: 提出Bird-SR，将超分辨率建模为基于奖励反馈学习的轨迹级偏好优化。方法在扩散早期针对合成对优化结构，后期引入质量引导奖励提升感知质量。为防止奖励劫持，对合成结果采用相对优势空间奖励，对真实图像引入语义对齐约束。同时采用动态结构-感知加权策略，确保结构感知的逐步平衡。

Result: 在多个真实场景超分数据集上，Bird-SR在感知质量和结构保持性方面均超过现有主流方法。

Conclusion: Bird-SR通过结构-感知动态平衡与奖励反馈机制，实现了在真实超分场景下兼顾结构和感知的高效提升，验证了其实用价值和有效性。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [31] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 提出MosaicThinker技术，通过整合多帧碎片化空间信息，提升设备端小型视觉语言模型的跨帧空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在跨视频帧的空间推理能力弱，尤其缺乏处理复杂3D空间关系的能力，限制了自主智能体在处理高级操控与规划任务中的效果。

Method: 提出了一种推理时计算技术MosaicThinker，将多帧视频中的空间信息整合为全局语义地图，并通过视觉提示引导小型视觉语言模型（VLM）在该地图上进行空间推理。

Result: 在资源受限的设备上，实验结果表明MosaicThinker可显著提升小型VLM在多样化、复杂空间推理任务中的准确率。

Conclusion: MosaicThinker为嵌入式AI设备在处理复杂空间推理任务时提供了一种高效方法，有效增强了设备端小型VLM的空间推理能力。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [32] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: 本文提出了WorldEdit数据集和测试集，专注于通过因果推理实现复杂、隐含指令的图像编辑，并验证了相关方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型擅长基于明确指令（如风格转换、特征操作），但对隐含、需依赖因果知识的编辑指令表现不佳，主要是因其采用的统一编辑策略难以处理涉及世界知识和推理的复杂任务。

Method: 提出了WorldEdit数据集，包含高质量、结合世界因果逻辑的隐含编辑指令范例，并提供WorldEdit-Test用于评估模型的因果编辑能力。实验中采用两阶段训练框架，并将因果验证奖励机制整合至模型（如Bagel）微调过程。

Result: 基于WorldEdit和新训练框架的方法，在遵循指令和知识合理性方面取得优异表现，大幅缩小了与强大闭源大模型如GPT-4o和Nano-Banana的差距。

Conclusion: WorldEdit及相关方法有效提升了开源图像编辑模型应对因果推理与隐含指令的能力，为实现更智能与具知识推理力的开放型图像编辑系统奠定了基础。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [33] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种直接生成可用矢量格式平面图的新方法TLC-Plan，显著提升了自动化建筑设计的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化平面图生成方法多在栅格空间操作，需要后续的矢量化步骤，这会带来结构不一致和端到端学习难题。作者希望解决这些问题，并使生成过程更符合建筑师的工作和思维方式。

Method: 作者提出了一种分层生成模型TLC-Plan，结合两层VQ-VAE，将空间全局布局压缩为房间包围盒，再用多边形级编码细化局部几何。所有编码通过树形结构统一，并用自回归变换器生成代码，实现从边界条件下生成多样、合理拓扑结构的平面图，无需显式房间拓扑或尺寸先验。

Result: TLC-Plan在RPLAN和LIFULL数据集上表现优异，达到先进的性能指标（如FID=1.84, MSE=2.06），优于现有方法。

Conclusion: TLC-Plan推动了矢量化、具约束性且可扩展的自动平面图生成，为实际建筑设计提供了更优解决方案。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [34] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 本文提出基于可重光照3D高斯散射（Relightable 3D Gaussian Splatting）和端到端强化学习的新型无人机导航系统，实现了无需微调的复杂户外环境零样本迁移，提升了对光照变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有利用单目视觉的无人机户外导航受限于仿真与现实间的显著视觉域差异，且3D重建方法常将照明与几何绑定，难以适应真实世界动态光照，影响策略泛化。

Method: 提出端到端强化学习框架，直接从单目RGB输入到连续控制指令。引入可重光照的3D高斯散射技术，将场景分解为可物理编辑的光照和几何部分，通过合成多样化光照场景增强训练，使策略习得光照不变的视觉特征。

Result: 在复杂森林实地中，轻型四旋翼无人机无须微调即可实现最高10米/秒下的稳健避障导航，并对剧烈光照变化展现出较强的适应能力。

Conclusion: 该方法显著弥合了仿真与现实间视觉域的差距，提高了无人机在变化多端的真实户外环境中的自主导航能力。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [35] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种针对多模态大语言模型（MLLMs）的三维物理环境提示注入攻击（PI3D），通过在现实空间中放置带有特定文本的物体，诱导模型发生错误行为，并证明现有防御手段对抗此类攻击效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs不断发展，它们在机器人、对话体等3D视觉应用中变得普及。然而，当模型依赖相机视角感知物理世界时，现实中可以通过物理对象进行攻击，安全风险增大。已有攻击主要针对文本输入或2D图片编辑，尚未系统探索真实3D环境中的类似攻击，因此亟需评估物理层面的安全性。

Method: 作者提出PI3D攻击：攻击者有意识地将嵌入有提示文本的物体以合适的位置和姿态放入真实三维场景中，目的是让MLLM完成攻击者注入的任务。论文针对获得有效攻击的物体位置与朝向提出求解方法，并确保放置方案在物理空间可行。然后，实验在多种模型和摄像机路径下检验攻击效果，并测试常见防御方法。

Result: 实验结果表明，PI3D攻击能够有效影响多种主流MLLM的行为，攻击在物理环境和不同拍摄路径下均成功。同时，主流防御措施难以阻止此类物理注入攻击。

Conclusion: MLLMs在物理世界中面临新的安全威胁，常规防御难以抵御基于物理对象的提示注入攻击。该研究为未来对抗物理世界攻击以及模型改进提供了重要参考。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [36] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: 该论文提出了Ex-Omni框架，实现了语音驱动的3D人脸动画与大语言模型的结合，并开源了相关数据集和方法。实验结果显示，其在多模态任务中表现良好，支持稳定同步的语音与人脸动画生成。


<details>
  <summary>Details</summary>
Motivation: 目前的全模态大语言模型（OLLMs）虽能处理多模态任务，但对自然交互非常关键的“语音与3D人脸动画结合”仍鲜有探索。主要难点在于：语言模型习惯于离散、基于token的语义推理，而3D面部动画生成需要密集且细致的时序动态，中间存在表示差异和优化难度，尤其是在数据有限的情况下。

Method: 作者提出Ex-Omni开源框架，通过解耦语义推理与时序生成来降低学习难度。具体做法为：用语音单元作为时序支架，采用统一的token-as-query gated fusion（TQGF）机制进行语义信息的受控注入。此外，还建设了InstructEx语音-3D人脸动画增强数据集，进一步促进该方向研究。

Result: 实验表明，Ex-Omni在多模态理解与生成任务中与现有开源OLLMs具有竞争力，并能实现稳定对齐的语音和人脸动画联动生成。

Conclusion: Ex-Omni框架有效解决了全模态语音-3D人脸动画结合的表征不匹配难题，为自然人机交互提供了关键技术，推动了全模态大模型发展。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [37] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 本论文系统性分析了大规模网络数据集（如LAION-400M）中怀孕超声图像的隐私泄露风险，发现包含大量敏感个人信息（如姓名、位置），并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 生成模型广泛使用未经充分筛选的大规模互联网数据集，这带来了敏感或个人隐私信息被采集和滥用的风险。怀孕超声图像作为经常被公开分享的高敏感性医疗图像，是评估此类风险的典型案例。

Method: 作者利用CLIP嵌入相似度，对LAION-400M数据集进行了系统搜索，检索并分析了包含怀孕超声图像的条目，并识别出其中的个人敏感信息实体（如姓名、地理位置等）。

Result: 在数据集中检测到数以千计包含敏感个人信息的怀孕超声图像样本，其中部分图像包含极高风险的信息，有可能导致个体被重新识别或身份冒充。

Conclusion: 作者建议在数据集构建和应用过程中，必须加强筛选与隐私保护流程，并对公开图像数据集的伦理和合规使用提出更严格的最佳实践和标准。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [38] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: 提出了一种无需成对纵向数据即可适应不同年龄脑MRI分割的元学习方法DuMeta++，通过多项创新实现更强的跨年龄泛化能力，并在多个数据集上验证了优越性。


<details>
  <summary>Details</summary>
Motivation: 脑MRI组织分割对科研和临床关键，但不同年龄阶段脑部形态变化大，导致分割算法在全生命周期上难以保持一致表现。以往用自监督和成对纵向数据来缓解，但现实中常常拿不到这些成对数据。亟需一种能在缺乏成对数据的情形下提升跨年龄泛化能力的新策略。

Method: 提出一个双重元学习框架DuMeta++，包括：1）用元特征学习提取不和年龄相关的表征，捕捉脑结构的时空语义信息；2）用元初始化学习提升模型的小样本适应性；3）用基于记忆库的类别感知正则化增强分割模型的纵向一致性，无需显式的纵向监督。同时给出理论证明其收敛性。

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多种数据集和小样本设置下实验证明，DuMeta++的跨年龄通用性和分割精度优于现有方法。

Conclusion: DuMeta++无需成对纵向数据，能高效地跨年龄分割脑组织，在实际应用中较强泛化能力，为脑MRI智能分割方案提供了新方向。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [39] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出在3D头部生成对抗网络（GAN）训练中，使用视角无关的语义特征作为条件信号，解决了现有方法因使用视角作为条件而导致的模式坍塌和生成质量/多样性不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 以往全头部3D GAN通常以视角作为条件输入，带来了生成空间在条件视角方向的严重偏置，导致其他视角下生成质量与多样性大幅下降，影响头部各区域的一致性和全局生成效果。作者希望消除该偏置，提升各角度生成一致性和多样性。

Method: 作者提出用视角无关的语义特征（源自正面图像的clip特征）作为条件输入，并构建了包含多视角合成头部图片的新数据集。采用FLUX.1 Kontext技术将高质量正面人脸数据集扩展到多视角图像，并以正面clip特征统一作为所有视图的条件，消除视角偏置，实现不同视角的图像通过同一语义条件监督训练。

Result: 实验结果表明，在全头部3D GAN合成及单视角GAN反演任务上，该方法在保真度、多样性和泛化性方面均显著优于现有方案。

Conclusion: 通过视角无关语义条件，增强了模型训练稳健性、提升了3D头部生成的一致性和多样性，并推动了更高质量的生成效果。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [40] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: 论文提出了RoadSafe365，这是一个大规模视觉-语言基准，用于提升交通安全事件的精细分析，填补了与官方安全标准接轨的评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有多模态交通基准缺乏与官方安全标准对齐的系统性评估，难以支持细粒度和全面的交通安全分析。

Method: 作者独立收集和精细标注了36,196段交通视频片段，覆盖多种事件类型和环境，并配套多项选择题和场景描述。基准采用分层分类结构，以弥补数据驱动系统和官方标准之间的差距，并提供标准化评测流程。

Result: 在RoadSafe365上微调模型可获得持续性能提升，并通过跨真实和合成数据域实验验证了其有效性。作者还建立了多项基准实验供后续比较。

Conclusion: RoadSafe365为交通安全分析提供了数据丰富、评测标准化的多模态基准，将推进基于实际场景的可复现研究。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [41] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: 本文提出了一种新的攻击方式，可以在超分辨率（SR）模型训练时将对下游任务的对抗性行为直接植入模型权重中，而无需在推理阶段对输入进行修改。这样可实现高攻击成功率且对图像质量影响较小。


<details>
  <summary>Details</summary>
Motivation: 作者发现许多数据驱动的超分辨率方法被用作图像预处理以提升分类、检测等下游任务的性能，但此前很少有人关注这些SR模型本身可能成为攻击载体。动机是揭示SR模型权重层面可构建的新型攻击面，提醒业界注意模型采购和验证的安全风险。

Method: 提出AdvSR框架，训练过程中联合优化重建质量和下游攻击目标，将对抗性行为直接编码进SR模型权重。与传统输入扰动或后门触发器不同，AdvSR完全在模型层面实现攻击。实验在SRCNN、EDSR、SwinIR三种SR架构上配合YOLOv11分类器验证效果。

Result: 实验证明，AdvSR能够在保持较高图像质量（标准评价指标下表现正常）的前提下，在下游分类任务中实现高攻击成功率。

Conclusion: 本研究揭示了超分辨率模型训练中可被引入的、针对下游任务的新型模型层面威胁，对安全关键的成像流水线提出了模型采购和验证的更高要求。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [42] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 提出了一种基于输运理论的新型3D医学图像分析工具3D-TBM，实现形态学分析、可解释性与可视化，并开源发布。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析任务（如分类、回归等）中，传统方法难以直观解析模型输出与原始影像空间的对应关系，因此急需一种既能提取有判别力特征，又便于结果可解释的分析框架。

Method: 基于可逆变换，将3D医学影像嵌入到输运（transport）域，通过计算最优输运嵌入，并结合主方向可视化、判别方向分析等方法；并开发出3D-TBM工具，实现数据预处理、特征计算和结果回投影等功能。

Result: 3D-TBM实现了3D医学影像形态学特征的高效提取、分析与可视化，支持主方向和判别方向等多种分析任务，并配套了详实文档和实用教程，为广大研究者提供技术支持。

Conclusion: 3D-TBM为医学影像研究引入了更具可解释性和实用性的形态学分析工具，促进了输运理论方法在医学影像领域的广泛应用，且工具已开源，方便学界使用和借鉴。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [43] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: 提出TwistNet-2D，通过局部空间上的通道对特征交互，更好地结合空间和通道统计，实现精细纹理识别，参数和计算开销小，性能优于多种主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有二阶统计方法（如Gram矩阵、双线性池化）虽能建模全局通道相关性，却损失空间结构；自注意力虽建模空间上下文，但未显式捕捉双通道特征间相互作用，难以兼顾两者优势。

Method: 设计TwistNet-2D模块，提出螺旋扭转通道交互（STCI），将特征图按方向偏移进行元素级通道乘法，捕捉局部、定向的通道特征共现模式。融合四个方向的结果，经通道重加权和sigmoid门控的残差路径嵌入主网络，参数和算力消耗极低。

Result: TwistNet-2D仅比ResNet-18新增3.5%参数和2%计算量，在四个主流纹理及细粒度识别基准上，均优于同参数甚至大幅超参数的模型（如ConvNeXt、Swin Transformer和混合架构）。

Conclusion: TwistNet-2D有效结合空间与通道二阶特征，代价低廉却性能突出，是纹理/细粒度识别任务的高性价比结构。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [44] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: 提出了一种从视频生成模型中提取可复用神经材质资产的新方法VideoNeuMat，大幅提升3D渲染中材质的逼真性与多样性。


<details>
  <summary>Details</summary>
Motivation: 3D渲染中制作高质量写实材质非常依赖艺术家手工能力，而现有生成式方法受限于高质量训练数据稀缺。虽然视频生成模型能表现写实外观，但材质信息与几何和光照混杂，难以独立复用。

Method: 提出VideoNeuMat两阶段管线：第一步是对大型视频生成模型进行微调，让其在受控的相机与光照条件下生成材质视频，模拟类似虚拟反射计的测量流程；第二步借助小型视频骨干微调的大型重建模型（LRM），从17帧生成视频中单次推理出泛化性强的神经材质参数，实现对新视角和光照的拟真再现。

Result: 生成的神经材质在真实感与多样性上均远超稀缺的合成训练数据，材料知识得以从大规模视频模型迁移，成为独立可重用的3D资产。

Conclusion: 验证了互联网规模视频生成模型所蕴含的材质知识可被提取、结构化并应用于3D资产生成，对3D内容生产技术带来显著提升。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [45] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: 该论文提出一种跨视角世界模型（XVWM），通过训练模型在不同视角间进行未来状态预测，增强了模型对环境3D结构的理解，并提升了智能体的规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要基于单一（通常是自我中心）视角进行推理和规划，限制了模型的空间想象和泛化能力。例如，在导航等任务中，从鸟瞰视角进行规划可能更有效。因此，提升模型的跨视角理解能力，能更好地支持多样化任务和多智能体协作。

Method: 提出了Cross-View World Models（XVWM），通过跨视角预测目标：输入为某一视角下的序列帧，要求模型预测相同或不同视角下的未来状态，从而促进模型学习视角无关的空间表征。训练过程中使用来自Aimlabs平台的多摄像头、时间同步的高频动作数据，实现精准的跨视角监督和训练。

Result: 实验结果表明，跨视角一致性监督能够显著增强模型空间表征的能力。XVWM可以为智能体提供多个视角下的“想象”流，支持基于实际任务需求选用最合适的视角进行规划，并在执行时仍以自我中心视角完成任务。

Conclusion: 通过跨视角预测任务，可以作为强有力的几何正则，提高模型的空间理解以及泛化能力。此外，该方法为多智能体环境下的“换位思考”奠定了基础，具有广阔的应用前景。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [46] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 本论文提出了一种结合注意力机制和DeepLab-V3+的深度学习方法，对糖尿病性视网膜病变（DR）相关病灶进行了像素级分割，并在公开数据集上取得了优于基线的方法表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习自动化算法虽多，但能实际应用于病灶分割的很少，临床针对具体病变（如微动脉瘤等）的精确检测需求未被很好满足。该研究旨在提升DR病灶（尤其是早期症状微动脉瘤）的分割准确性，助力临床筛查。

Method: 作者针对DR的四类典型病灶（微动脉瘤、软性渗出物、硬性渗出物和出血），基于DDR数据集，融合注意力机制到DeepLab-V3+模型进行像素级分割，并分别与无注意力机制的基线模型进行了对比实验。

Result: 相比于基线，Attention-DeepLab模型mAP由0.3010提升到0.3326，IoU由0.1791提升到0.1928。尤其是对微动脉瘤的检测效果有显著提升，mAP从0.0205提升到0.0763。

Conclusion: 引入注意力机制的DeepLab-V3+能有效提升DR病灶的分割性能，为临床提供更精确的病灶检测工具，特别是在早期微动脉瘤的分割上具有临床意义。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [47] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 该论文提出了一种基于线性遗传编程（LGP）优化的图像过滤与分割算法，用于自动检测微观图像中析出相，实现了与人工标注接近的分割精度，极大提高了合金开发迭代速度。


<details>
  <summary>Details</summary>
Motivation: 手工标注增材制造铌基铜合金的微观组织图像因对比度、噪声和图像伪影差异大，效率低下，限制了材料研发的进展。

Method: 采用基于遗传算法的优化环境，将图像处理过程描述为一系列参数化的过滤模块，由领域专用语言编码，并通过LGP优化生成最优的图像过滤流水线，实现了自动分割和算法优化。

Result: 在理想条件下（种群规模60，程序最大长度5），所提算法分割性能接近人工标注，像素级平均误差仅为1.8%，3.6兆像素图像处理时间约2秒。

Conclusion: 该方法显著提高了增材制造铜合金材料研究的迭代效率，有助于加快强韧、低活化析出强化铜合金在聚变反应堆零件应用中的开发进度。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [48] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一视觉-语言稀疏自编码器（LUCID），用于在图像和文本领域间学习可解释的共享特征，提升特征对齐、解释性及多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自编码器（SAEs）仅能在单一模态下训练，其生成的特征难以理解且不能跨模态迁移，限制了多模态解释性研究的发展。

Method: 作者提出LUCID模型，能为图像块和文本词元学习一个共享的稀疏字典，同时保留部分模态特有的特征容量。通过借助最优传输匹配目标实现无监督的特征对齐，并引入自动化的字典解释流程。

Result: LUCID学到的共享特征可用于图像块级别定位、建立视觉-语言神经元的对应关系，并提升对概念聚类问题的鲁棒性。此外，自动化的特征解释方法无需人工参与。

Conclusion: LUCID能学习到涵盖物体、动作、属性、抽象概念等多类别的可解释共享特征，推动了多模态、可解释表征的研究，具有广泛的实际应用前景。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [49] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: 本文提出了CLARITY方法，通过动态调整RGB与热成像数据的融合策略，有效提升了在复杂光照条件下道路场景的语义分割表现，并在MFNet数据集上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-热成像融合方法在各种复杂照明环境中采用静态融合策略，导致特定模态噪音影响语义分割性能，因此需要一种能自适应场景条件的融合方法。

Method: 提出CLARITY系统，基于视觉-语言模型(VLM)先验，动态调整RGB与热成像数据的融合比例，并利用物体嵌入进行分割。设计了针对暗部物体语义保留机制和多层次结构一致性的解码器，以兼顾信息保留和分割边界锐度。

Result: 在MFNet数据集上，CLARITY方法取得62.3%的mIoU和77.5%的mAcc，超越了现有方法。

Conclusion: CLARITY通过场景感知和多模态动态融合，有效解决了复杂光照下的道路场景语义分割难题，提升了分割精度并设立了新的业界标杆。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [50] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新的分布匹配蒸馏（DMD）优化框架与改进方法，显著提高了生成模型的样本质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然DMD是一种高效的加速方法，但在某些区域（Forbidden Zone），因教师信号不可靠和排斥力不足导致性能下降。因此，提升DMD在这类区域的稳定性是该研究的动机。

Method: 作者统一分析了现有DMD方法，提出了‘自适应匹配蒸馏’（AMD）新机制，利用奖励代理检测并逃离不良区域，并通过结构信号分解和能量势场锐化实现自我修正。

Result: 在图像与视频生成任务（如SDXL、Wan2.1）及多个基准测试（如VBench、GenEval）中，AMD相比于主流方法均有明显提升，如SDXL上的HPSv2分数从30.64提升到31.25。

Conclusion: 主动校正优化轨迹、避免进入‘Forbidden Zone’对于进一步提升快速生成模型的性能至关重要。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [51] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种结合改进U-Net和Row-Column Separated Attention（RCSA）模块的新方法，用于低照度图像及视频增强，并通过两个时序损失函数增强视频的一致性，取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net在低照度增强时缺乏对全局信息的指导，导致局部噪声较大及细节缺失；而直接引入注意力机制虽然可弥补此短板，却带来了参数和计算量大幅增加。

Method: 作者提出将行列分离注意力（RCSA）模块应用于改进版U-Net结构中。RCSA模块利用特征图每行和每列的均值及最大值，低参数地整合全局信息以指导局部特征。另外，提出了两个时序损失函数以扩展方法到视频增强，并保持帧间时序一致性。

Result: 在LOL、MIT Adobe FiveK和SDSD等公开图像及视频数据集上的实验结果表明，该新方法在增强质量、细节保持与时序一致性方面均明显优于现有方法。

Conclusion: 该方法在不显著增加计算量和参数的情况下，有效提升了低照度图像和视频的增强效果，显示了其在相关领域的应用前景。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [52] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 本论文提出了一种透视感知的log-depth融合方法，结合深度图和法向量图实现高精度的3D表面重建，并能利用法线信息填补深度缺失区域。


<details>
  <summary>Details</summary>
Motivation: 现有利用深度图和法向量图重建3D表面的方法多假设正交投影，未能充分考虑实际中常见的透视投影，从而影响了三维重建的精度。

Method: 提出一种透视感知的log-depth融合方法，扩展了基于梯度的正交融合算法，在计算过程中显式考虑了透视投影关系。同时，当存在深度缺失时，利用表面法线信息对缺失区进行填充。

Result: 在DiLiGenT-MV数据集上的实验表明，该方法能实现高精度的3D重建，特别是在存在缺失深度数据时也能有效恢复物体表面。

Conclusion: 透视投影下深度与法线的联合融合对于高质量三维重建十分重要，提出的方法在提升重建精度和数据修复能力方面具有明显优势。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [53] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: 本文提出了PTB-XL-Image-17K合成心电图（ECG）图像数据集，包含17271个高质量12导联ECG图像及完整标注，专为心电图数字化和深度学习研究设计。数据集及生成工具已开源。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏同时含有心电图图像和真实时序信号（带完整标注）的大规模数据集，限制了ECG数字化和深度学习发展的进展。作者希望通过构建此数据集，解决心电图数字化模型训练和评估所需数据的关键短板。

Method: 作者基于PTB-XL信号数据库，自动生成真实感强的ECG合成图像。每条样本数据包含ECG图像、像素级分割掩膜、真实时序信号、目标检测标注、完整元数据。开发开源Python框架，可灵活控制图纸速度、电压标度、采样率、网格色等参数。所有样本均成功生成，平均处理时间1.35秒。

Result: 共生成17271条高质量ECG样本，配有多种类型的全面标注。数据集覆盖完整评测流程，并实现100%自动生成成功率，效率高，适用于模型训练和全流程自动评测。

Conclusion: PTB-XL-Image-17K有效填补了心电图数字化领域在大规模带标注数据集上的空白，可用于导联检测、波形分割及信号准确提取等任务。该数据集及生成框架推动了心电图AI与数字化相关研究的发展，是研究和应用的重要资源。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [54] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了SoulX-FlashHead框架，用于实时、高保真、无限时长的音频驱动人像生成，兼顾效果与速度，并引入了新颖的特征提取和模型蒸馏机制，在多个基准数据集上取得先进表现。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人像生成模型难以兼顾高质量视觉效果和低延迟流式生成。大模型计算量高，小模型则难以保持整体人脸表达和时序稳定性，因此需要方法在效果和效率之间取得更好平衡。

Method: 提出了1.3B参数的SoulX-FlashHead框架，针对流式音频特征不稳定引入Streaming-Aware Spatiotemporal Pre-training和Temporal Audio Context Cache机制，实现了短时音频片段的鲁棒特征提取。为缓解长序列生成累积误差和身份漂移问题，提出Oracle-Guided Bidirectional Distillation方法，借助真实运动先验进行物理引导。同时发布了严格对齐的782小时高质量数据集VividHead用于训练。

Result: SoulX-FlashHead在HDTF和VFHQ等基准数据集上实现了最先进的性能。其轻量化版本在单张NVIDIA RTX 4090显卡上可达96 FPS的推理速度，实现了极快的互动体验且保持视觉连续性。

Conclusion: SoulX-FlashHead在流式音频驱动人像生成领域实现了高效、高质量、低延迟的技术突破，同时通过新机制解决了特征不稳定和长序列累积误差问题，为相关任务树立了新标杆。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [55] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SpatialReward的空间推理奖励模型，用于提升在线强化学习在复杂图片编辑任务中的表现，通过精确的空间对齐和像素级验证，显著提升了奖励信号的真实性和细粒度感知能力。


<details>
  <summary>Details</summary>
Motivation: 现代在线强化学习在图片编辑领域受限于高质量、细粒度奖励信号的缺乏。现有的奖励评估器存在“注意力崩塌”问题，忽视了不同图片间的对比以及细节感知，导致评价失真，影响模型表现，需要新的方法提升评估精度。

Method: 作者提出SpatialReward奖励模型，通过将推理锚定在预测的编辑区域，实现显式的空间推理，并以像素级证据为基础，提升语义判断的准确性。模型训练依托一个26万数据量、具有空间标注的数据集。

Result: SpatialReward在MMRB2、EditReward-Bench等公开基准上取得了最优结果，并在新提出的MultiEditReward-Bench中超越了业界最强的专有评估器。此外，将SpatialReward引入在线RL后，OmniGen2在GEdit-Bench的表现提升了0.90分，超过了当前领先的判别模型与GPT-4.1。

Conclusion: 空间推理在图像编辑任务中对于实现有效对齐和奖励信号生成至关重要。SpatialReward方法极大增强了细粒度编辑检测能力，推动了在线强化学习在复杂图片编辑中的进展。

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [56] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 本文提出了一个整合并扩展现有垃圾分类数据集的大规模新数据集GlobalWasteData (GWD)，以促进更通用、高效的垃圾自动分类系统的开发。


<details>
  <summary>Details</summary>
Motivation: 现有公共垃圾分类数据集存在碎片化、不一致和偏向特定环境等问题，导致AI模型难以泛化到真实世界。迫切需要统一、丰富、可靠的数据基础。

Method: 作者将多个公开数据集进行合并，并对89,807张图片进行了标准化标注，涵盖14大类、68个子类。还进行了质量过滤、去重和元数据生成等预处理，提高了数据集的可靠性。

Result: 构建了覆盖范围广、类别更均衡、标注一致的GWD数据集。该数据集包含丰富、多样的垃圾图片，能够更好支持泛化能力强的AI模型训练和评估。

Conclusion: GWD数据集为垃圾自动分类、智能回收等环境AI应用提供了坚实的数据支撑，并向学术界开放，助力后续研究和成果复现。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [57] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: 本文提出TOM-GS，一种结合学习型里程计与Gaussian Splatting方法、面向热红外传感器的稠密建图和位姿估计系统，能在多种环境下实现更鲁棒和高质量的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有热红外里程计和建图技术主要依赖几何方法，难以适应多样化的数据集且无法生成稠密地图。新近的Gaussian Splatting方法展现了高效、优质的三维重建能力，因此作者尝试将其引入热红外领域。

Method: 提出了一种新系统TOM-GS，将基于学习的里程计与基于GS的稠密建图结合，专门针对热红外相机设计，整合了热图像增强和单目深度估计。

Result: 通过大量实验，TOM-GS在运动估计和新视角渲染方面优于现有学习型方法，显示出更鲁棒和更精细的重建能力。

Conclusion: GS结合学习型方法有助提升热红外传感器在恶劣环境中的视觉SLAM性能，TOM-GS为热红外成像的智能感知应用提供了有效解决方案。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [58] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 该论文通过引入多预训练视觉编码器和融合先验，实现了对大脑信号中分层次、多尺度视觉信息的高效解码，兼顾了图像检索准确率和重建细节。


<details>
  <summary>Details</summary>
Motivation: 当前利用大脑信号解码视觉信息的方法多注重高层语义，忽视像素级细节，导致对人类视觉系统的理解受限。作者希望突破这一限制，更全面地解码大脑中的视觉表征。

Method: 提出多视觉编码器的脑-图像对齐策略，融合不同归纳偏置以捕捉分层多尺度特征，并采用对比学习实现脑信号与图像嵌入的高效对齐。同时，引入融合先验（Fusion Prior），在大规模视觉数据上学习稳定映射，提升多模态特征分布的一致性。

Result: 方法在定量（检索准确率）和定性（图像重建细节）实验中表现优异，在两者之间取得了良好平衡，超越了以往主要关注高层语义的解码方式。

Conclusion: 该方法有效提升了从脑信号解码视觉表征的性能，完善了对人类视觉系统分层、多尺度表征的理解，对神经科学和人工智能均有重要意义。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [59] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 该论文提出一种基于隐式运动表示的视频扩散模型用于角色动画生成，通过将每帧运动压缩成1D运动token，有效提升了生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有动画生成方法显式运动表示易出现空间不匹配和人体尺度变化问题，隐式方法则常有身份信息泄漏和运动与外观纠缠等问题。为解决这些挑战，需开发新的运动表示方式。

Method: 提出全新隐式运动表示，将每帧运动压缩为1D运动token，避免2D空间约束和身份信息泄漏，并设计了mask token驱动的时间一致性迁移模块，通过三阶段训练提升效率和保真度。

Result: 大量实验表明，该方法在运动重定向一致性和生成质量上优于或媲美现有先进方法。

Conclusion: 提出的隐式运动表示方法为视频扩散模型动画生成带来新的突破，能够更好平衡空间一致性与身份信息保护，具有很强应用前景。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [60] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 本文提出了一种适用于无人机影像的小目标自适应缩放检测方法ZoomDet，能有效提升小目标检测精度，对现有主流检测架构兼容良好。


<details>
  <summary>Details</summary>
Motivation: 无人机拍摄的图像中前景目标小且稀疏，使得常规检测模型难以精准检测小目标，因此需有专门针对小目标检测的有效方案。

Method: 设计了一种高效的非均匀缩放框架，包括轻量级偏移预测方案和新的基于框的缩放目标，实现对图像区域自适应缩放，同时提出角点对齐的目标框变换方法，确保训练和推理阶段框的一致性。

Result: 在VisDrone、UAVDT、SeaDronesSee等主流无人机检测数据集上验证，该方法在Faster R-CNN架构下，在SeaDronesSee数据集上带来超过8.4点mAP提升，仅增加约3ms延迟，对基本检测架构无依赖性。

Conclusion: ZoomDet显著提升了无人机图像中小目标检测能力，方法简单高效，具有良好通用性，实用价值高。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [61] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出一种基于CA-YOLO的仿生稳定定位系统，显著提升了小目标的检测精度和定位的稳定性，实验取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有目标定位系统在精度和识别小目标能力方面受到限制，难以满足复杂环境中对高效准确目标定位的需求。

Method: 方法上，改进YOLO主干网络，在系统中引入仿生模块，包括小目标检测头和特征融合注意力机制（CFAM）；并借鉴人类前庭眼动反射（VOR），提出一套包括中心定位、稳定性优化、自适应控制系数调整和智能再捕获功能的仿生云台跟踪控制策略。

Result: 在COCO和VisDrone数据集上，CA-YOLO平均准确率分别提升了3.94%和4.90%；时敏目标定位实验也证明方法具有良好的有效性和实用性。

Conclusion: 所提仿生稳定定位系统能有效提升目标检测精准度和小目标识别能力，在实际应用中展现出良好前景。

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [62] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 该论文旨在改善物体中心学习（OCL）模型的评估方式，提出了更全面的基准评价方法，并引入统一的评测任务和指标。


<details>
  <summary>Details</summary>
Motivation: 尽管OCL模型被认为在组合泛化和应对分布外数据中具有优势，目前的评估方式主要集中在物体发现及简单推理任务，未能充分衡量模型在复杂推理和实际应用中的表现。现有基准在衡量表示有效性及定位能力时采用割裂的评估体系，不能反映OCL模型的实际用途。

Method: 作者提出利用指令微调的视觉语言模型（VLM）作为评价器，从而扩展OCL模型在不同视觉问答数据集上的复杂推理评测。同时，提出统一的评测任务与指标，能同步考查定位准确性与表示有效性。此外，引入了多特征重建的简单基线模型作对比。

Result: 通过上述方法，作者实现了更具代表性、反映OCL实际应用能力的评测，同时方便不同模型的公平对比。统一任务指标减少了不同测评标准带来的不一致。

Conclusion: 该论文为OCL模型评测提供了更具实际意义和全面性的评价方法，为后续模型开发和研究提供了新方向，并推动了该领域实际应用效果的提升。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [63] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的猫品种识别方法，利用GCViT-Tiny模型在Oxford-IIIT Pet数据集上实现了较高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 猫品种之间在外观上的差异非常细微，现有图像识别方法识别难度大，因此需要开发更加有效的自动识别模型。近年来深度学习和transformer架构在图像分类领域表现突出，值得在细粒度识别任务中进一步探索其性能。

Method: 作者使用了Oxford-IIIT Pet数据集中猫的子集进行训练，采用了GCViT-Tiny（全局上下文视觉transformer）架构，并通过旋转、水平翻转、亮度调整等多种数据增强手段提升模型泛化能力。

Result: GCViT-Tiny模型在猫品种识别任务中取得了92%的测试准确率和94.54%的验证准确率。

Conclusion: 基于变换器的模型在细粒度图像分类任务中表现出色，未来可应用于兽医诊断、动物收容所管理和移动端识别等场景。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [64] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 本研究提出了一个双时点分析框架，结合统计描述、影像组学与深度学习特征，更全面地表征和追踪急性脑卒中患者缺血组织的变化。通过对18例患者的分析，证实该方法能有效区分可救治与不可救治脑区。


<details>
  <summary>Details</summary>
Motivation: 传统做法仅依赖单一时点影像评估卒中缺血区，但无法揭示组织异质性及卒中进展的时序演变。因此，亟需多时点、多特征的综合影像分析框架，以更精细地刻画和预测脑组织卒中后的动态变化。

Method: 研究提出双时点（入院CTP与术后DWI）影像分析，将同一区域在初始与最终状态下交集，形成6个感兴趣区域（ROI）。分别提取统计特征、影像组学纹理特征、两种深度神经网络（mJ-Net、nnU-Net）的深度特征嵌入，将所有特征聚合分析各区域差异。通过无监督聚类及分离度评价（包括Wilcoxon秩和检验），评估方法的有效性。

Result: 在18位成功再通的卒中患者中，分区特征展现出显著聚类。初始为半暗带但最终组织恢复的区域，其特征类似于健康脑组织；而最终梗死区域聚为一类。无论是纹理特征还是深度特征，半暗带的初始状态与其最终结局有关，但核心区的这些差异并不显著。mJ-Net深度特征对可救治与不可救治组织区分效果尤为突出。

Conclusion: 所提双时点结合多特征的分析框架可反映卒中组织的生物学特性及动态演变，有望促进卒中成像量化、为卒中演变研究和治疗策略提供更深入的定量依据。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [65] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型（LLM）引导的诊断证据对齐方法（LGDEA），改进医疗视觉-语言预训练，使其更符合真实医学诊断流程，并减少对大规模配对数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格的医学视觉-语言预训练过多依赖配对数据，但全局或局部对齐方法不能有效地捕捉关键诊断证据，难以获可靠的诊断表示。这限制了其在医学配对数据有限场景中的应用。

Method: 提出LGDEA方法，借助LLM从影像报告中提取诊断证据，构建共享诊断证据空间，从而实现证据级别的跨模态对齐，支持利用大量未配对的医学影像和报告进行预训练。

Result: 在多个任务如短语定位、图像-文本检索、零样本分类上的实验表明，该方法在缺乏配对数据情况下，比现有方法取得更好且更稳定的性能，甚至可以达到依赖大规模配对数据方法的效果。

Conclusion: LGDEA大幅减少对配对数据的依赖，提高了诊断相关表示的学习效果，更适合真实医学场景，是一种更可靠的医疗视觉-语言预训练新方法。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [66] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出MUFFASA框架，用于提升基于slot attention的无监督对象分割性能，通过融合ViT各层特征实现更优结果。


<details>
  <summary>Details</summary>
Motivation: 现有大多数无监督对象学习方法只利用ViT最后一层的信息，忽略了其它层潜含的丰富语义信息，限制了对象分割性能的提升。

Method: 提出MUFFASA框架，在ViT编码器的多个特征层上分别应用slot attention，并设计融合策略将多层获得的slot聚合为统一的对象中心表征。该方法易于集成到现有OCL方法中。

Result: 将MUFFASA集成到现有对象中心学习方法后，在多个数据集上实现了分割精度的新SOTA，并提升了训练收敛速度，推理开销较小。

Conclusion: 综合多层语义、融合slot表征能显著提升无监督对象分割性能，MUFFASA为现有OCL模型提供了强有力的增益和高效的改进方案。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [67] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 本文提出了一个无需训练的基线方法FSSDINO，仅利用DINOv3模型的冻结特征，通过类别原型与Gram矩阵优化，实现了强竞争力的少样本语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督ViT如DINOv3在视觉任务上具有丰富特征，但这些特征在无需训练或适应的情况下，尚未被充分探索于少样本分割。作者希望了解冻结特征如何在FSS下表现及其极限。

Method: 提出FSSDINO方法，仅基于冻结DINOv3的最后一层特征，利用类别特定原型和Gram矩阵进行特征优化，实现无训练少样本分割。同时用Oracle引导分析各层表现。

Result: FSSDINO在二分类、多分类及跨域FSS基准上表现与复杂解码器或测试时自适应方法相当。Oracle分析发现最佳中间层特征性能更优，但现有无监督或支持引导的选择指标无法有效找到最优层。

Conclusion: 冻结DINOv3最后一层是极强的基线，传统层选择启发式存在“语义选择鸿沟”。当前无监督指标难以挖掘潜在最优特征，提升层选择方法是后续研究的方向。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [68] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新方法FlexID，通过动态调节语义与视觉特征，在保持人物身份一致性的同时更好地适应文本编辑需求，取得了优秀实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化文本生成图像的方法依赖于僵硬的视觉特征注入，导致身份保持和文本适应性之间冲突，难以胜任需要灵活编辑意图的场景。

Method: 提出FlexID，将身份信息解耦为语义投影（SIP）和视觉锚点（VFA）两部分，并通过上下文自适应门控（CAG）机制，根据编辑意图和扩散步长自适应权重，动态调整语义和视觉两条信息流。

Result: 在IBench基准上，FlexID在身份一致性和文本遵循性之间达到了当前最优平衡，且能高效实现复杂叙事的图像生成。

Conclusion: FlexID是一种有效且无需训练的新方案，能在灵活编辑意图下高质量地实现身份保持和语义创新，适合复杂文本到图像的个性化生成。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [69] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑的3B参数视觉-语言-行动（VLA）智能体，用于理解语言指令并进行目标导航。该方法改进了解释性和泛化能力并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动的目标导航方法要么依赖端到端的视-语嵌入模型，泛化能力差且缺乏动作级解释性；要么依赖多模块大模型管线，存在误差累积、计算成本高、推理难以反馈至导航策略的问题。

Method: 作者提出了一种小型的3B参数VLA智能体，采用显式基于图像的推理流程，以三阶段“思考-思考总结-行动”方式解释和决定行为，直接回答目标识别和动作原因的问题。

Result: 方法实现了更强的可解释性、更好的泛化性，以及更高效的导航表现。

Conclusion: 紧凑的VLA智能体可以摆脱复杂混合管线，显著提升语言驱动目标导航的解释性、泛化能力与效率。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [70] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: SIGMA是一种扩散模型后训练框架，通过引入多属性token，首次实现了多条件交错生成，大幅提升了多元属性编辑和组合任务的可控性及生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的统一扩散模型（如Bagel）仅支持单一条件输入，难以应对多源异构信息的组合与编辑需求。因此，亟需一种能支持多条件、组合式视觉内容生成的方法。

Method: SIGMA框架在Bagel模型基础上，通过设计风格、内容、主体、身份等可选择的多属性token，实现多条件交错输入，并通过700K的交错样本进行后训练，使模型能理解并组合多种视觉条件。

Result: 实验显示，SIGMA在组合式编辑、属性转移、多模态对齐等任务上，较Bagel有明显提升，在可控性、一致性和视觉质量方面均表现出色。

Conclusion: SIGMA显著扩展了视觉扩散模型的灵活性和应用范围，为多条件视觉任务的生成与编辑提供了强大支持，优于现有统一模型。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [71] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 本文围绕远距离人体识别（HID）展开，介绍HID竞赛在复杂步态识别数据集上的进展，并报告了最新的精度突破。


<details>
  <summary>Details</summary>
Motivation: 远距离识别身份在实际应用中常因传统生物特征（如人脸、指纹）难以获取而受限。步态识别作为无需接触且支持远距离识别的手段，是极具潜力的替代方案。推动步态识别进步并提供公正评测平台是举办HID竞赛的动机。

Method: 自2023年起，HID竞赛采用了SUSTech-Competition步态识别数据集，该数据集包含服装、持物和视角等多重变化，无提供专用训练集，选手需利用外部数据集训练。每届采用不同随机种子生成评估集，以防过拟合，提升交叉领域泛化评测的公平性。

Result: 尽管任务难度逐年提升，2025年赛事的最佳方法在该数据集上实现了94.2%的新精度纪录。

Conclusion: 步态识别技术在极具挑战性的场景下继续取得进展。论文还分析了关键技术趋势，并展望了未来步态识别的可能研究方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [72] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于子空间可辨识理论的跨摄像头奶牛个体识别方法，通过特征解耦，有效提升了模型在不同摄像头下的泛化能力，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有奶牛个体识别方法在单一、受控摄像头下效果良好，但在跨摄像头场景下，由于照明、背景、视角和成像设备不同，识别性能严重下降，限制了技术在实际牧场的广泛应用。

Method: 采用基于子空间可辨识理论的特征解耦网络，将图片表示解耦成多个正交潜在子空间，从中分离出稳定的个体身份特征，有效隔离与相机相关的变化因素。构建了包含五个摄像头节点、复杂光照和角度变化的数据集，展开了跨摄像头实验。

Result: 在七个跨摄像头任务中，该方法平均准确率达到86.0%，大幅优于Source-only Baseline（51.9%）与现有最优跨摄像头方法（79.8%）。

Conclusion: 本文建立了一个基于子空间理论的特征解耦框架，有效支持复杂智能牧场环境下的多摄像头协同奶牛识别，为非接触动物精准监控提供了新思路。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [73] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MammoColor的端到端框架，可以通过色彩编码强化乳腺X线片（尤其是致密乳腺）在识别异常中的表现。实验证明MammoColor有效提升AUC和特异性，降低误报率。


<details>
  <summary>Details</summary>
Motivation: 致密乳腺中的乳腺癌筛查因组织重叠和影像细微差异而检测灵敏度较低，易漏诊和误判。为提升医生对影像异常的感知能力和诊断准确率，亟需通过视觉增强技术辅助筛查判读。

Method: 设计了MammoColor框架，核心是Task-Driven Chromatic Encoding（TDCE）模块，将单通道乳腺X线片转换为色彩编码视图，从而加强视觉关注点。系统与BI-RADS判读分类器结合，在多个公开与临床数据集进行训练和评估，并通过多读者多病例（MRMC）实验对比TDCE色彩增强与传统灰阶影像的诊断性能。

Result: MammoColor在VinDr-Mammo数据集上AUC从0.7669提升至0.8461，在致密乳腺人群提升更明显（AUC从0.749增至0.835）。MRMC实验中，TDCE影像在保持灵敏度的同时，提高了特异性（0.90→0.96），并有降低误报率的趋势。

Conclusion: TDCE模块对乳腺X线片提供了任务优化的色彩增强表现，能够提升视觉敏感性，有助于减少乳腺癌筛查中的假阳性召回，有潜力改善临床判读效果。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [74] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本研究提出了一种极简的多模态大语言模型（MLLM）架构ViCA，将视觉信息的计算负担大幅降低至原来的4%，同时保持98%的原始任务精度，实现了显著的性能-效率权衡提升。


<details>
  <summary>Details</summary>
Motivation: 当前主流MLLM采用统一自注意力结构，对视觉和文本信息在每一层都进行共同处理，带来大量计算开销。作者质疑这种对视觉特征的密集处理是否必要，并希望通过精简结构，提高模型推理效率。

Method: 作者分析发现，大多数层中视觉特征与文本的有效交互极少，因此设计了ViCA架构：视觉token跳过全部自注意力和前馈层，只在精选层以稀疏的交叉注意力方式与文本交互。该方法利用了视觉嵌入本身与语言空间的高度对齐性。

Result: ViCA在三种主流MLLM骨干、九个多模态任务、26种剪枝基线上的评测显示，准确率仅下降不到2%，但视觉侧计算下降至4%。在单batch推理中加速3.5倍，多batch下超10倍，视觉推理开销基本可忽略。同时与token剪枝等效率方法兼容，效果可叠加。

Conclusion: ViCA保留了MLLM性能的同时极大降低了视觉部分的运算开销，实现了高效轻量化多模态推理，为硬件部署和进一步效率提升提供了新思路和架构基础。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [75] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 该论文提出了一种结合地质建模与机器学习的自动岩石节理映射方法，可提升图像中节理识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的岩石节理追踪受限于真实数据稀缺和类别不平衡，影响机器学习方法的训练效果。

Method: 方法包括：1）利用离散裂隙网络模型参数化生成具有地质特征的合成岩石图像，覆盖场景相关的节理几何、连通性和节点类型；2）采用合成数据和真实图像混合训练分割模型，然后对真实数据进行微调。

Result: 实验在盒式和边坡场景下，结合多个真实数据集测试。结果显示：a）合成数据可有效补充监督训练；b）标签一致时混合训练优异，标签噪声时微调更稳定；c）完全零样本（仅合成数据）预测仍有限，但用少量真实数据微调可实现良好泛化；d）定性结果优于部分定量指标，显示出更清晰地质结构。

Conclusion: 该方法支持可靠的自动节理追踪，为领域自适应和更准确评估提供基础，对于真实数据有限的应用场景具实际价值。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [76] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一套系统性的后训练框架，将监督型策略塑造、奖励驱动的强化学习和基于偏好的优化结合到一个带有稳定性约束的优化流程中，旨在提升视频生成模型的实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成预训练模型在实际生产环境中往往难以满足稳健性、长时序一致性及可控性等需求。如何通过高效、可靠的后训练过程，使模型适应真实环境下多变且信息不足的反馈，是亟待解决的挑战。

Method: 作者设计了一个多阶段的后训练优化栈，包含：1）监督型策略塑造以打下基本能力基础；2）借助强化学习，通过奖励信号进一步提升决策质量；3）针对用户偏好进行细化优化，并在整个流程中施加稳定性约束。该流程特别针对视频生成的高计算成本、长时序失效可能，以及多样、弱判别性反馈等实际难题。

Result: 所提框架形成了一套系统、连贯的后训练方案，不仅能提升生成视频的感知质量、时序一致性和指令遵从度，还确保了模型在初始化培育的可控性。

Conclusion: 该工作为构建可扩展、易部署且稳定的视频生成后训练流程提供了明确蓝图，有助于推动预训练生成模型在现实生产环境中的可靠应用。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [77] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出Fine-R1模型，通过新颖的R1训练框架显著提升多模态大模型（MLLMs）在细粒度视觉识别（FGVR）任务中的表现，尤其在少量标注数据下也能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型擅长粗粒度视觉任务，但细粒度视觉识别表现不佳，数据标注成本高，同时泛化能力弱，特别是对未见子类别。需要一种更高效、泛化性更强的方法解决FGVR任务。

Method: (1) 基于链式思维（CoT）的监督微调，构建包含视觉分析、候选子类别、比较和预测的高质量FGVR-CoT数据集，提升模型开放世界分类能力；(2) 三元组扩增策略优化，包括同类增强（提升对类内变化的鲁棒性）和异类增强（增强对类间区分性）。

Result: 仅用4-shot训练，Fine-R1在识别已见和未见子类别时，显著优于现有通用大模型、推理型大模型及对比式CLIP模型。

Conclusion: Fine-R1在低标注数据环境下实现了出色的细粒度视觉识别能力，特别适用于子类别专家标注难以获取的知识密集领域。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [78] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HistoMet的新型多实例学习框架，可直接基于原发肿瘤的病理切片图像（WSIs）预测肿瘤是否发生转移及其转移部位。HistoMet通过顺序性地模拟临床转移风险评估流程并利用视觉-语言模型提升解释性，在多癌症类型大规模数据集上显示出优越性能。


<details>
  <summary>Details</summary>
Motivation: 肿瘤转移是癌症致死的主要原因，但仅从病理切片预测转移发生及其转移部位极具挑战性。现有方法往往孤立地处理转移风险和转移部位预测，且未模拟临床真实的评估流程，缺乏良好解释性。

Method: 提出了一个决策感知、概念对齐的多实例学习（MIL）框架HistoMet。其包括两步流程：首先估计原发肿瘤的转移风险，高危再预测具体转移部位。模型结合融合了预训练视觉-语言模型以引入临床相关的语义概念并提升可解释性。

Result: 在6504名带有转移随访及部位标注的多中心多癌种数据集上评估，HistoMet在95%灵敏度下可大幅减少后续工作量且保持高风险召回率。在已发生转移的患者中，预测转移部位宏平均F1达到74.6，宏平均AUC为92.1，标准差低，效果稳定。

Conclusion: 该方法显示，将临床决策流程建模进病理图像分析流程，可显著提升转移风险和转移部位的预测能力，具备扎实的临床可部署潜力。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [79] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态广告视频理解框架AD-MIR，旨在更好地将视觉内容与高阶营销逻辑关联起来，显著提升对广告视频意图的解析能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体虽然在一般检索任务中表现优异，但难以将像素级内容和抽象营销目的有效联系起来，导致对广告视频深层意图理解不足，因此需要专门的解决方案。

Method: AD-MIR框架采用两阶段架构：（1）结构化记忆构建：将视频转换为结构化数据库，通过语义检索结合关键词匹配提取品牌细节并过滤干扰；（2）结构化推理代理：模拟营销专家，通过反复提问和证据自我修正机制，将广告叙事拆解并验证推理结论，缺乏视觉证据时自动回溯。

Result: 在AdsQA基准上，AD-MIR取得了最先进的效果，严格准确率比当前最强通用体DVD高出1.8%，宽松准确率提升了9.5%。

Conclusion: 有效的广告视频理解必须将抽象营销策略明确地以像素级证据为基础。AD-MIR框架显著提升了广告意图解码能力，对广告及多模态推理任务具有重要参考价值。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [80] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 本文针对现有3D医学基础模型仅在结构影像方面验证的局限，构建了包含PET/CT和PET/MRI的全身医学影像数据集，深入评估并发现模型在多模态应用中的显著性能差距，强调未来需走向多模态训练与评价。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学基础模型在文献中多停留在区域和结构成像的评测，尚未深入探讨不同影像模态间的泛化及稳健性。这导致模型真实应用能力未知，可能限制其临床推广。

Method: 作者构建了包含490例PET/CT与464例PET/MRI的UMD全身数据集（约67.5万张2D图像、1.2万个3D器官标注），对代表性3D分割基础模型进行系统评测。通过对同一受试者的配对扫描进行对比，排除其它变量，单独考察模型在不同影像模态下的表现。

Result: 评估结果表明，在从结构成像（如CT/MRI）转向功能成像（如PET）时，现有模型的性能与文献基准表现存在明显差异，甚至出现系统性失败。

Conclusion: 当前3D医学基础模型尚难称得上是通用工具，必须通过多模态训练和综合评价来实现真正的通用性。论文所建立的数据集和分析为未来相关研究提供了基石。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [81] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: 本文提出了Images2Slides系统，可将静态PNG/JPG信息图转化为可编辑的Google幻灯片，通过视觉语言模型识别和坐标映射，重建出幻灯片原生元素。系统在控制测试中展现了高元素恢复率和较好的识别精度。


<details>
  <summary>Details</summary>
Motivation: 信息图广泛用于信息传播，但导出为图片后内容无法编辑、难以更新和复用，增加了后续处理成本。缺乏高效的自动化工具将像素化图像无损还原为可编辑格式。

Method: Images2Slides通过API流程工作：首先用视觉语言模型（VLM）提取图像区域规格，再将像素坐标映射到幻灯片坐标，最后利用Google Slides API批量更新接口，重建出文本、图像等元素。其模型无关性保证了可兼容多个VLM后端，基于JSON标准区域格式和确定性后处理。

Result: 在包含29个已知区域的受控信息图测试集上，系统整体元素恢复率达0.989±0.057；文本恢复0.985±0.083、图片恢复1.000±0.000。文本区域平均转写错误率CER=0.033±0.149，布局一致性（IoU）文本为0.364±0.161、图片为0.644±0.131。

Conclusion: Images2Slides可高效准确定向还原信息图为可编辑幻灯片，但在字体大小校准和非均匀背景等环节存在挑战。文中还总结了失败模式，为未来改进指明了方向。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [82] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 本文系统评估了不同医学影像重建流程中的误差来源，并比较了不同分割算法和几何类型对模型精度的影响。


<details>
  <summary>Details</summary>
Motivation: 目前医学三维重建的准确性受限于成像设备、分割方法及网格处理技术等多方面因素，因此有必要系统分析包括几何类型、类别不平衡、体素和点云配准等因素对重建精度的影响。

Method: 打印球体、人脸罩和腹主动脉瘤（AAA）并利用微型CT扫描。采用GMM、Otsu及区域增长（RG）方法进行分割，并用KU算法进行模型配准，再通过Dice、Jaccard、精度等指标及Chamfer距离、Hausdorff均值距离等评价分割与表面匹配精度。

Result: Otsu方法普遍适用于各种几何体，但AAA因壁厚小和配准误差表现较差。类别不平衡对特异性指标影响最大出现在AAA上。RG对球体效果最好，GMM、Otsu对AAA更佳。高体素指标在类别极不平衡和配准误差时可能误导。Jaccard指数比Dice更严格，适用于薄壁结构评估。

Conclusion: 分割精度由重建流程多环节误差累积，高体素指标未必可靠，薄壁结构建议采用更严格指标。务必保证体素和点云配准以获得真实、可比的评估结果。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [83] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本论文提出将音频作为新的数据源扩展智能车载LILO(Looking-In-Looking-Out)框架，建立L-LIO(Looking-and-Listening Inside-and-Outside)框架，实现音视频多模态融合以提升车辆安全。研究证明音频在驾驶员状态评估、指令理解与外部情境感知方面有独特价值。


<details>
  <summary>Details</summary>
Motivation: 传统智能车载系统主要依赖视觉或有限的驾驶员状态感知（如LILO框架）。但在许多实际情境下，单纯视觉信号不能全面解释驾驶员、乘客或外部环境的信息，特别是涉及音频信息的细微情况，如驾驶员醉酒状态、乘客自然语言指令，以及路人语音与手势并存的场合。因此，探索音频与视觉融合对于车辆智能安全具有重要意义。

Method: 提出L-LIO框架，将音频信号与已有的视觉传感器信息融合，实现对驾驶员、乘客和外部环境的多模态感知。方法包括：利用监督学习对驾驶员语音音频进行分类判断其可能的障碍状态（如是否醉酒）；收集和分析乘客的自然语言指令以实现音频与规划系统的数据对齐；在仅靠视觉难以判别外部人员意图的情况下引入音频进行辅助。实验使用真实环境中车辆内外自采集音频数据集。

Result: 初步实验显示，音频在安全驾驶相关的微妙或复杂场景下能提供有价值信息，帮助系统在纯视觉无法辨析的情况下进行决策。但也发现噪声干扰、隐私和跨人群泛化等挑战。

Conclusion: L-LIO框架通过多模态音视频传感器融合有效增强驾驶员和环境理解，为提升车辆智能安全提供新途径，但需要进一步解决实际场景中可靠性和隐私保护等问题。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [84] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）在自动驾驶安全性评估和决策中的应用，提出三种系统级用例并进行了实验分析。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs的发展，它们能够将视觉信息与自然语言概念对齐，在自动驾驶安全评估和决策中有潜力，但现有方法未充分挖掘其系统集成价值。

Method: 1）基于CLIP的图文相似性，用于快速的语义风险筛查，无需传统目标检测；2）将VLM嵌入用于Waymo数据集下基于变压器的轨迹规划，分析其对准确性的影响；3）在doScenes数据集上用自然语言指令约束规划行为，评估对安全和模糊场景的作用。

Result: 1）VLM可实现类别无关、低延迟的风险检测，对异常风险有良好检测能力；2）直接将VLM全局特征用于规划无显著提升，表明表征与任务需更紧密对齐；3）以自然语言为约束可以抑制严重失误、提升模糊情景下的安全行为。

Conclusion: VLM在提升自动驾驶安全性方面极具潜力，适用于表达语义风险、意图和行为约束。但要实现这一潜力，关键在于精细的系统设计与结构化对接，而非简单特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [85] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Process-of-Thought (PoT)的视频推理框架，将视频推断过程分解为一系列可验证的轻量级步骤，提高了多步骤、时序推理的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在处理时间长、内容噪声高的视频时，往往难以进行多步推理和因果推断，且推理过程难以追踪，影响其可解释性和可靠性。

Method: 提出PoT推理框架：通过依次进行(1)时序证据选择、(2)分步状态更新以及(3)受约束的答案合成，使模型推理过程结构化且可追溯。该方法为模型无关型，可集成在各种视觉-语言模型中，并支持外部工具增强的证据检索。其核心创新还包括用统一表示对齐中间推理步骤与具体时间片段。

Result: 在多个标准视频推理任务上实验表明，PoT框架能显著提升事实正确率与时序定位的准确性，并生成便于理解和诊断的推理链条。

Conclusion: PoT视频推理框架提高了多步推理的可解释性、准确性和健壮性，为后续的推理分析和视觉理解任务提供了可追溯证据与辅助分析能力。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [86] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种用于煤炭传送带场景中异物检测和像素级定位的新方法，并构建了新的基准数据集CoalAD，实验表明该方法优于主流基线方法。


<details>
  <summary>Details</summary>
Motivation: 在煤矿传送带场景下，煤与矸石堆积无序、背景复杂多变，异物常表现出低对比度、变形和遮挡，导致现有结构化工业场景中的异常检测方法在此环境下表现大幅下降。因此，亟需更适应此类非结构化场景的自动化异物检测与定位方案。

Method: 作者提出了一种互补线索协同感知框架，从三方面提取并融合异常线索：（1）对象级语义组成建模，（2）基于语义归因的全局偏差分析，（3）细粒度纹理匹配。通过融合上述信息，实现更鲁棒的图像级异常评分和精确的像素级定位。同时，作者还建立了无监督异物检测与定位的CoalAD基准数据集。

Result: 在CoalAD数据集上的实验显示，所提方法在图像级和像素级指标上均超越了多种主流基线算法。消融实验进一步验证了各组件对整体性能的贡献。

Conclusion: 新框架通过多视角异常线索融合，显著提升了煤矿传送带场景的异物检测与定位能力，对提升智慧矿山安全保障具有积极意义。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [87] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出了名为U-KABS的新型医学图像分割模型，结合了Kolmogorov-Arnold Networks和U-Net架构，在多个数据库上表现优异，尤其在复杂结构分割方面超越了主流模型。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床诊断和治疗中具有重要意义，但由于医学图像结构复杂且变化大，尤其是内部非线性关系难以处理，现有方法存在挑战。研究动机在于提升分割精准度，特别是复杂结构的识别能力。

Method: 提出U-KABS混合框架，将Kolmogorov-Arnold Networks（KANs）的表达力与U型编码-解码结构结合。模型分为两个关键阶段：第一是卷积+Squeeze-and-Excitation增强通道特征表示，第二是KAN Bernstein Spline (KABS) 阶段，利用基于Bernstein多项式和B样条的可学习激活函数，从而同时捕捉全局平滑趋势与局部细节模式。通过跳跃连接实现多尺度特征融合和空间细节保持。

Result: 在多个医学图像分割数据集上，U-KABS模型的分割性能优于多个主流强基线方法，尤其是在复杂解剖结构的分割任务上展现了更强的表现力。

Conclusion: U-KABS模型实现了对医学图像中复杂结构的有效分割，其混合创新设计对医学图像分割领域提升分割准确度具有重要意义。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [88] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种全光学计算框架，用于自动驾驶中的RGB图像分割和车道检测，展示了在CityScapes、定制车道和CARLA模拟环境下的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶系统中的语义分割和车道检测大多依赖于深度神经网络，但这类方法需进行大量的模拟-数字转换和大规模计算，导致能耗高，不利于实时、低延迟响应。paper旨在寻找更高效、低能耗的替代方案。

Method: 作者设计了一种新型的全光学神经网络（DONN）框架，通过光衍射以光速实现图像的全光处理。该方法不仅减少了运算能耗，也降低了AD转换等开销。论文对CityScapes公开数据集、定制车道检测数据集及CARLA仿真场景进行了实验和评估。

Result: 结果表明，这种DONN系统在CityScapes上能够有效完成语义分割任务；在定制室内赛道和CARLA仿真环境下也表现出良好的车道检测能力，并具备一定的环境泛化性。

Conclusion: 全光学神经网络框架在自动驾驶图像分割与车道检测任务中表现出了高效、低能耗的优势，并具有较强的泛化和应用前景，可望为相关系统提供更优的解决方案。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [89] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: 本论文提出了一种名为PAND（Prompt-Aware Neighborhood Distillation）的新方法，实现了将大型视觉-语言模型（VLM）的知识高效蒸馏到轻量级网络上，显著提升了细粒度视觉分类（FGVC）任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在将大型VLM的知识蒸馏到小模型时，过于依赖固定提示和全局特征对齐，难以适应细粒度分类任务的需求，导致性能不足。因此，如何设计更高效、适应性强的知识蒸馏方法成为亟需解决的问题。

Method: PAND方法分为两个阶段：首先进行提示感知的语义校准，通过自适应生成语义锚点；其次实施邻域感知的结构蒸馏，约束学生网络的局部判决结构，从而实现更细致的知识迁移。

Result: PAND方法在四个主流FGVC基准上均取得了比当前最优方法更好的表现。以ResNet-18为学生模型，在CUB-200数据集上达到了76.09%的准确率，比强力基线VL2Lite高3.4%。

Conclusion: PAND框架有效弥补了现有知识蒸馏方法在FGVC中的不足，在保证学生模型轻量高效的同时，显著提升了细粒度分类能力。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [90] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 本文提出了一种训练外（training-free）的解决策略Rolling Sink，有效缓解了自回归视频扩散模型在超长时间生成时出现的视觉质量下降问题，显著提升了生成视频的时序一致性和画面稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前，自回归视频扩散模型虽然性能优异，但由于实际训练时只能用有限长度的视频片段，导致在测试阶段遇到更长生成时（如数十分钟）会出现严重的视觉退化问题。长视频的训练消耗极大，亟需一种无需额外训练即可提升长时生成表现的方法。

Method: 作者系统分析了AR模型的缓存（cache）机制，提出Rolling Sink方案。该方法在模型只用5秒短片段训练的前提下，通过优化AR缓存维护策略，在推理时支持5-30分钟超长视频连续生成，并维持角色一致性、颜色稳定性、结构连贯和动作流畅。

Result: Rolling Sink显著提升了超长时序生成的视频质量和时空一致性，实验显示其在16 FPS下生成时长可达30分钟，远超现有技术，并在多个基准任务中表现优异。

Conclusion: Rolling Sink为自回归视频扩散模型的超长期推理提供了轻量、高效的训练外解决方案，大幅缓解了训练测试时长不一致带来的质量退化问题，对实用化长视频生成具有重要意义。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [91] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

TL;DR: 本论文提出了一种融合动态占用网格图的新框架，实现对驾驶场景中车辆与其他动态实体的高精度未来预测，兼具泛化能力与个体行为洞察，并在真实数据集上取得了优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景预测方法主要分为两类：基于占用网格图的整体（agent-agnostic）场景预测，和基于语义信息的个体（agent-specific）预测，各有局限，前者难以刻画动态体的复杂行为，后者难以泛化到感知不清或未识别的体。结合两者优势能大幅提升驾驶安全与预测效果。

Method: 作者提出了基于动态占用网格图（Dynamic Occupancy Grid Maps, DOGMs）的统一框架，采用轻量级时空骨干网络，并设计了能同时预测未来占用状态网格、车辆网格和场景流网格的时序解码流程。核心为相互依赖的损失函数，能捕捉网格间的依赖关系，以占用状态信息约束流引导的变化，兼顾障碍物与遮挡。

Result: 在nuScenes和Woven Planet两个真实驾驶数据集上的实验表明，该方法对动态车辆与一般动态场景元素的未来状态预测效果均优于现有主流基线方法。

Conclusion: 融合全局与个体信息的动态占用网格预测框架能够显著提升自动驾驶场景中各类动态元素的未来状态预测准确性，为更安全和泛化能力更强的运动预测提供了有效途径。

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [92] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: 提出UCATSC系统，结合模型约束及不确定性，提升交通信号控制的安全与可解释性，并兼顾效果与实用性。


<details>
  <summary>Details</summary>
Motivation: 实际部署自适应交通信号控制面临视觉感知不确定性、安全性隐患和控制策略不可解释等难题，导致其在现实交通中的应用有限。

Method: 提出UCATSC，将交叉口信号控制问题建模为部分可观测下的带约束随机决策过程，显式预测并执行安全和无饥饿等硬性约束，结合反事实推演于信念空间，兼顾视觉感知不确定性。

Result: 系统在保证安全及饥饿预防的同时，能改善交通延误和减排，并输出基于模型的可解释控制结果。

Conclusion: UCATSC在真实感知下提升了交通控制系统的安全性、可解释性和运行效率，为实际部署自适应交通信号控制提供可能。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [93] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出ForecastOcc，这是首个能从相机图像直接进行语义占据预测的视觉方法，解决了现有方法对外部占据预测依赖和误差积累的问题，并能够对未来环境的几何和语义进行多时刻预测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉占据预测方法多关注运动类别（如静态/动态物体），缺乏对语义信息的预测。最新语义占据预测方法虽然尝试弥补该不足，但过于依赖外部网络得来的先验信息，容易误差积累，也无法从图像中直接学习空间-时间特征。因而需要一种能直接从图像端到端预测未来场景语义占据状态的新方法。

Method: 提出ForecastOcc框架，可以联合预测未来的占据状态和语义类别，不依赖外部地图。方法基于时序跨注意力模块、2D转3D视图转换器、3D编码器和语义占据头，能实现多时刻、多视角的语义占据预测。在Occ3D-nuScenes和SemanticKITTI两个数据集上进行了多视角和单目预测实验，并首次为该任务建立了基准测试方法。

Result: 在Occ3D-nuScenes和SemanticKITTI数据集上，ForecastOcc均优于自适配的2D预测模块等基线方法，能生成兼具几何和语义丰富性的多时刻预测结果，有效反映出场景的动态变化与关键语义。

Conclusion: ForecastOcc能够端到端地实现基于视觉的多时刻语义占据预测，无需外部先验，解决了现有方法误差积累等瓶颈，为自动驾驶等领域提供了精确且丰富的场景未来预测能力。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [94] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: 本文介绍了VideoTemp-o3，一个联合视频定位和问答的长视频理解框架，在提取关键信息和准确定位方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀采样常错过视频中的关键证据，导致理解和问答效果较差。已有的'agentic thinking-with-videos'范式虽然尝试改进，但存在定位弱、效率低和流程僵化的问题。

Method: 提出VideoTemp-o3框架，将视频定位与问答任务联合建模，实现更强的视频片段定位和即时采样以及对错误定位的修正。在有监督微调阶段，设计了统一的掩码机制鼓励探索性采样并减少噪音；在强化学习阶段，引入专用奖励以避免奖励作弊。此外，作者还设计了新的高质量长视频问答数据构建流程及评测标准。

Result: 在长视频理解和定位任务上，VideoTemp-o3取得了显著优于现有方法的实验结果。

Conclusion: VideoTemp-o3能够有效提升长视频的问题回答和定位表现，同时构建的数据集和评测方法也为相关研究提供了新支撑。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [95] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Picasso的物理约束重建流程，用于增强多目标场景重建的物理合理性，并伴随新数据集及评估基准，显著提升了现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 由于遮挡和测量噪声，即便几何上精确的三维重建也可能物理上不合理，如物体穿透或不稳定等问题，这会影响基于仿真的动态预测，阻碍数字孪生在接触密集场景中的规划和控制。

Method: 文章提出了Picasso——一个结合几何、非穿透和物理约束的重建管线。该方法采用快速拒绝采样，并通过推断得到的物体接触图引导多物体交互采样。另外作者还提出了Picasso数据集，包含10个真实世界接触丰富的场景及物理可行性度量方法，均开放共享。

Result: Picasso在自建数据集和YCB-V数据集上进行评测，在物理合理性和与人类直观一致性两方面都远超现有方法，取得了更高质量的多物体场景重建结果。

Conclusion: 只考虑单个物体的重建难以保证整体物理可行性。Picasso流程通过全面推理物体间约束，能生成更符合物理规律和人类直觉的场景重建，极大推动了数字孪生的可行性和下游仿真应用。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [96] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 本文对现有的AI图像检测方法进行了首次大规模零样本评估，发现不同检测器在不同数据集表现差异极大，没有通用的最佳方案，且最新商业生成模型能够轻易绕过大多数检测器，提醒业界勿盲信基准性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在网络平台泛滥，鉴别伪造图像以防止虚假信息极为迫切。然而，大部分现有评价只关注经过微调的检测器，缺乏对实际直接部署（零样本）检测器性能的考察，导致在真实应用中有效性存疑。

Method: 作者收集了16种最先进检测方法，共23个预训练模型，对应291种生成器，跨越12个多元数据集（共约260万样本），系统性地对比这些检测器在零样本条件下的检测性能，并通过统计测试和错误模式分析揭示关键影响因素。

Result: 1) 没有检测器在所有场景下都表现最佳，排序极不稳定；2) 性能最佳与最差检测器差距高达37个百分点；3) 训练数据的匹配度极大影响泛化能力，相同架构的检测器可有20-60%的性能波动；4) 现代主流商业生成模型可成功规避绝大多数检测器，只能检测出18-30%；5) 发现三类典型跨数据集失效模式，统计学检验也证实检测器性能差异显著。

Conclusion: 作者质疑“万能检测器”思路，并提出实际部署时必须结合面临的具体威胁场景精细选择检测器，而不应简单依赖论文基准上的高分数。为业界提供了针对性和实用性更强的部署建议。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [97] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 本文提出了一种结合车辆信息的3D行人姿态预测框架，提升自动驾驶中对复杂多智能体交互场景下行人运动的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境下，实现安全、可靠的自动驾驶需要更精确地预测行人运动，尤其是在存在多辆车辆和多名行人时，现有方法很难充分建模行人与车辆间的动态交互关系。

Method: 作者增强了Waymo-3DSkelMo数据集，新增了配准的3D车辆包围盒，实现多智能体间更真实的交互建模。提出了一种场景采样方案，按行人与车辆数量对场景分类，用于适应不同交互复杂度的训练。方法方面，在TBIFormer架构基础上，引入车辆编码器与交互cross-attention模块，将车辆与行人的特征融合，利用历史行人运动和周围车辆条件联合预测未来行人姿态。

Result: 实验结果显示，该方法在预测精度上有显著提升，且不同建模行人-车辆交互的方式对提升预测效果有进一步贡献，强调了车辆感知在3D姿态预测中的重要作用。

Conclusion: 提出的方法有效提升了自动驾驶场景下的3D行人姿态预测能力，并证明了显式建模行人-车辆交互的重要性。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [98] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 该论文首次在大规模基准下系统比较了34个模型（包括22个专用架构和12个通用视觉-语言模型VLMs）在8个标准人脸年龄估计数据集上的表现，发现零样本VLM显著优于大部分专用模型。


<details>
  <summary>Details</summary>
Motivation: 虽然人脸年龄估计在内容审核、年龄验证和深度伪造检测等领域至关重要，但此前没有系统对比现代VLMs与专用模型在此任务上的性能。本工作旨在填补这一研究空白。

Method: 作者收集了22个具有公开预训练权重的专用年龄估计模型和12个主流VLMs，对它们在8个人脸年龄估计公开数据集上的表现进行了评测，并分析了零样本推理、年龄分组和极端年龄段等多种情况。

Result: 零样本VLM平均绝对误差（MAE）5.65，优于专用模型9.88。其中Gemini 3 Flash Preview（MAE 4.32）性能最优，明显超过专用模型中的SOTA（MiVOLO，MAE 5.10）。MiVOLO是唯一能和VLM媲美的非LLM模型。此外，在18岁分界的年龄验证任务上，VLM误判率大幅低于专用模型；粗分年龄区间会让MAE大幅恶化。

Conclusion: 传统观点认为专用结构在特定任务中性能更佳但本研究表明VLM在无专门训练下亦可大幅超越多数专用模型，提示未来应关注VLM能力蒸馏至高效专用模型。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [99] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了WorldArena基准，用于系统评估具身世界模型在感知和功能两个方面的表现，并引入了综合评分EWMScore，揭示了视觉质量与任务能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有对具身世界模型的评估主要集中在感知保真度（如视频生成质量），但忽视了模型在下游决策任务中的实际效用，因此亟需一个综合评估的基准框架。

Method: 该工作设计了WorldArena基准，从视频感知质量（16个指标、6个子维度）、任务功能性（作为数据引擎、策略评估器和行动规划器的表现，并结合主观人类评估）三方面对模型展开系统评估。此外，提出了EWMScore，将多维度性能整合为单一可解释指数。

Result: 对14种代表性模型进行了广泛实验，结果显示感知质量高的模型未必具备强大的任务执行能力，揭示了感知与功能之间存在显著差距。

Conclusion: WorldArena为具身AI领域提供了统一的评测框架和公开排行榜，有助于推动朝向具备真正功能性的世界模型发展。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [100] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合采集算子的深度学习重建框架（OCDI-Net），有效提升了SMS MRI图像复原的准确性，特别是在防止切片串扰和数据缺失方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前SMS MRI图像重建问题在高加速成像情况下，存在严重的切片干扰和k-space数据缺失，传统方法大多基于高斯噪声建模，难以精准对应采集物理带来的非高斯退化，导致图像质量下降。

Method: 作者提出以采集物理算子为指导的深度学习框架，即“operator-guided”建模，对成像退化路径显式建模，并通过确定性操作逆转。具体方法是设计了OCDI-Net模型，该网络结构为双流交互网络，分别分离目标切片与干扰分量，同时进行有结构的退化预测，分两步完成：先切片分离，再图像复原补全。

Result: 在fastMRI脑部数据以及实际采集的弥散加权MRI上实验表明，OCDI-Net较传统和现有深度学习SMS重建方法表现更优，能显著提升图像保真度，减少切片串扰。

Conclusion: 以采集物理为指导的重建策略（OCDI-Net）可以更有效地解决SMS MRI中的强耦合退化问题，提升高加速MRI的重建质量，对未来临床和科研成像具有重要意义。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [101] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 本文提出了OTA-Det，一个统一的遥感图像开放词汇检测与视觉指向框架，能同时实现多目标检测与细粒度语义理解，并在多个基准上取得了实时与最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感场景理解方法各自有明显缺陷，开放词汇检测（OVAD）只支持粗粒度类别语义，视觉指向（RSVG）又只能定位单个目标，难以兼顾丰富的语义理解与多目标检测。因此，亟需一个能够同时处理两类任务的方法。

Method: 提出了OTA-Det，一种统一这两类任务的架构。具体做法包括：任务表述重整，将检测与指向的目标、监督方式统一，推动两类数据集的联合训练，并引入密集语义对齐策略，实现从整体到细节的多层面语义匹配。此外，基于RT-DETR结构扩展新颖高效的模块，使系统可对开放文本类别实时检测。

Result: OTA-Det在六个开放词汇检测和视觉指向的基准上取得了最新的最优性能，数据推理速度达34帧每秒，实现了实时检测。

Conclusion: OTA-Det有效弥补了OVAD与RSVG方法的各自短板，首次将两者结合，兼顾了多目标、高语义理解和实时性，推动遥感图像理解领域新的技术进展。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [102] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 本文提出了SPD-Faith Bench基准，用于检测多模态大语言模型（MLLMs）在链式思维推理中的忠实性，并提出了新的校准方法SAGE以提升模型推理的信度。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维推理在多模态大语言模型中可以提升可解释性，但当前关于其推理过程是否与真实视觉内容一致（即faithfulness）的问题尚未解决。以往研究主要关注感知幻觉，而忽视了推理层面上的不忠实现象，因而需要更细致的诊断方法。

Method: 作者设计了SPD-Faith Bench基准，通过精细的图像差异推理任务，强制模型进行显式视觉比较，从而分离出忠实性问题，并对当前主流MLLMs进行系统评估。分析其失败模式后，提出了无需训练的SAGE校准框架，通过视觉证据加强视觉感知与推理对齐。

Result: 主流MLLMs在该新基准下表现出两类系统性失败：感知盲点与感知-推理分离。这些问题与模型视觉注意力下降和残差流内表征偏移有关。SAGE框架在多个模型上显著提升了视觉推理与感知一致性。

Conclusion: 研究强调了链式推理中对推理忠实性评价的重要性。SPD-Faith Bench为评测MLLMs的推理可信性提供了新工具，SAGE方法有效提升了模型的推理忠实性。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [103] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出了一种无需训练且可直接使用的视频换脸方法VFace，能够与基于扩散模型的图像换脸方法结合，实现高质量且时序一致性强的视频换脸。


<details>
  <summary>Details</summary>
Motivation: 视频换脸技术通常存在时间一致性差和细节保真度不足的问题，同时大多数方法需要针对特定视频进行训练或微调，使用成本高。

Method: VFace方法主要有三大创新：（1）频谱注意力插值，增强特征生成并保留身份特征；（2）结构引导的注意力融合，将目标结构高效引入生成结果；（3）基于光流的注意力时序平滑机制，提高跨帧的时序一致性且无需修改扩散模型本身。

Result: VFace无需额外训练或视频专用微调即可显著提升换脸视频的时间一致性与视觉保真度。实验表明，该方法兼容其他扩散模型换脸方法，效果优秀。

Conclusion: VFace是一种实用、模块化且高效的视频换脸新方案，对现有扩散模型换脸方法有极强的兼容扩展性，在提升时序一致性和画面质量方面具有突出优势。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [104] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频变换器编码方法ViewRope，用于增强预测世界模型在摄像头控制下的空间持久性和3D一致性，减少几何漂移和幻觉细节的发生，并辅以诊断和性能验证。


<details>
  <summary>Details</summary>
Motivation: 现有的互动式AI预测世界模型在摄像头运动轨迹较长时缺乏空间持久性，无法保持稳定的场景结构，常在摄像头重新访问旧位置时产生细节幻觉。作者认为这是由于现有方法依赖于屏幕空间的位置编码，与3D投影几何不一致。

Method: 提出ViewRope，一种将相机射线方向直接注入视频变换器自注意力层的几何感知编码方法，通过光线几何而不是像素距离来参数化注意力，使模型具备原生的3D一致性归纳偏置。进一步提出Geometry-Aware Frame-Sparse Attention，利用几何线索有选择地关注相关历史帧，提高效率和一致性。还设计了ViewBench用于诊断闭环保真度与几何漂移。

Result: 实验证明，ViewRope在长期一致性上显著优于现有方法，能有效减少几何漂移和细节幻觉，并能降低计算成本。

Conclusion: ViewRope提供了一种几何感知的方法，使预测世界模型在摄像头控制场景下获得更好的空间持久性和长期3D一致性，可为交互式AI和相关任务带来更可靠的场景理解。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [105] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种可从超高速运动模糊图像中恢复3D形状的新型逆向渲染方法，解决了传统方法在运动模糊情况下失效的问题，并在高效性与可微分性上实现突破。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法主要针对静态图像，对于由于超高速运动而产生极强运动模糊的图像往往失效。很多实际场景（如体育、工业高速部件）下，对应的3D形态重建至关重要，因此需要新的方法克服运动模糊带来的挑战。

Method: 作者提出了新的逆向渲染方法，核心是引入了一个高效的重心坐标求解器，极大减少了计算负担。该方法可对高速运动物体进行高效仿真，并保持完全可微分，实现通过2D模糊图像反向优化恢复3D形状。

Result: 在高速平移和旋转两种典型极端运动下的实验结果表明，该方法不仅能高效仿真真实的运动模糊图像，还能从极端运动模糊的2D图像中成功恢复出3D形状，相比传统多视图立体视觉等方法更为有效。

Conclusion: 新方法突破了极端运动模糊条件下3D重建的限制，既能高效模拟运动模糊，又可从中成功反演恢复3D结构。研究推进了视觉3D重建领域的前沿应用。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [106] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: 本文提出了SSI-Bench，这是一个专注于空间推理的视觉问答（VQA）基准，来自真实3D结构、受限于几何和物理约束，显著加大了对现有视觉-语言模型空间智能的考验。现有模型与人类在该任务上存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 目前多数VLM评测主要集中于2D场景，模型往往能通过简单的视觉线索取得较好成绩，无法准确反映其在真实受限空间中的推理能力。因此，需要在更受约束的空间结构里评估VLM的空间智能。

Method: 作者设计了SSI-Bench基准，涵盖1000道空间推理相关的排序问题，涉及复杂的几何与拓扑问题，并需要诸如心理旋转、截面推断、遮挡推理及力路径推理等多样化的空间组合操作。数据完全由人工精心策划和标注，最大程度减少像素层面线索的干扰。

Result: 评测31个主流VLM发现，无论开源（最高22.2%准确率）还是闭源（最高33.6%）模型，在该基准上与人类（91.6%）表现差距极大。引导模型"思考"仅带来有限提升，许多错误归因为结构归因和一致性3D推理能力弱。

Conclusion: 现有VLM在真正受约束环境下的空间推理（空间智能）能力严重不足。SSI-Bench为推动VLM空间智能的发展提供了重要测试平台，同时强调模型需提升结构归因与物理约束一致的三维推理能力。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [107] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR提出了一种利用结构化报告和骨骼区域定位，无需人工标注的儿童腕部X光图像检索方法，有效提升了断裂相关病例检索和诊断支持能力。


<details>
  <summary>Details</summary>
Motivation: 腕部骨折影像的特征细致且易被噪音干扰，且缺乏大规模标注数据，限制了基于病例的影像检索发展。

Method: 提出WristMIR框架，利用自动生成的结构化报告描述，并对腕部特定骨骼区域（桡骨远端、尺骨远端、尺骨茎突）进行局部裁剪，结合全局与局部对比编码，采用两阶段（全局匹配与区域条件化重排序）检索策略。

Result: WristMIR显著提升了检索性能和骨折分类准确性（如Recall@5从0.82%升至9.35%，AUROC达到0.949），区域感知设计大幅提升断裂相关检索诊断能力（F1均值升至0.753），且临床相关性评估分数提高。

Conclusion: 解剖引导的智能检索系统有望极大提高儿科骨骼影像的辅助诊断能力，相关代码已开源。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [108] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 该论文提出了SAGE框架，可以高效地利用互联网视频提升3D重建模型的能力，在多个基准测试中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建领域面临3D标注数据稀缺，大规模多样化3D数据难以获取的问题。虽然互联网视频拥有海量无标注数据，但其缺乏真实3D几何信息且存在观测噪声，难以直接用于几何模型训练。因此，亟需探索能够有效利用这些原始视频数据的方法。

Method: 作者提出SAGE框架，通过分层挖掘流程，将原始视频转化为可用于模型训练的轨迹和混合监督，包括：1）选择信息量大的训练轨迹；2）通过SfM生成稀疏点云，作为全局结构引导；3）利用3D高斯渲染实现多视角稠密一致性。为防止灾难性遗忘，还引入基于锚点数据的正则化策略。

Result: 实验表明，SAGE极大提升了模型的零样本泛化能力，相较于先进方法，在7Scenes、TUM-RGBD、Matterport3D等数据集的Chamfer距离降低20-42%。

Conclusion: SAGE首次实现了通过互联网视频自适应几何基础模型，为通用3D学习和大规模3D模型训练提供了可扩展的新范式。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [109] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言模型（VLM）量化校准方法，名为Token-level Importance-aware Layer-wise Quantization（TLQ），显著提升了量化后的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统的PTQ主要应用于大语言模型，校准方式通常忽视了视觉-语言模型中视觉与文本token在激活和量化敏感性上的显著差异，导致校准难度加大和性能损失。

Method: TLQ利用梯度信息实现token级别的重要性集成，通过该机制构建token级的校准集，提升校准的细粒度。同时，提出了一种多GPU分布式、层级一致的量化校准方案，使复杂的校准过程能够分散到多块RTX3090显卡，摆脱对A100大显存GPU的依赖，并保持与真实推理路径一致。

Result: 在两个模型、三个规模和两种量化设置上，TLQ均表现出一致的性能提升和较强的量化稳定性。

Conclusion: TLQ能够更细致和有效地校准VLM的量化过程，显著提高了量化部署时的模型性能和稳定性。其方法通用性强，适用于不同模型规模和硬件条件。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [110] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 本文对开源视觉语言模型（VLMs）在隐私属性识别上的零样本能力进行了评估，并将其表现与人工标注进行了比较。结果显示，VLMs在某些隐私属性识别方面与人工标注有高度一致性，部分情况下VLMs还能发现人类标注者忽略的隐私属性。


<details>
  <summary>Details</summary>
Motivation: 当前在大规模图像数据集中进行隐私敏感属性识别面临人工标注成本高和一致性差的问题。视觉语言模型(VLMs)因能够实现零样本学习，具备自动化检测潜力，因此评估其在隐私识别领域的能力具有重要意义。

Method: 作者对一系列开源VLMs进行隐私属性的零样本检测任务评测，通过与人工标注结果比较，量化模型在人类标注一致性高和低时的表现，并分析模型与人类标注不一致的案例。

Result: VLMs在高一致性隐私属性识别任务中表现较好，甚至能标注出一些人类遗漏的隐私属性。同时，模型相较于人类更倾向于检测到更多的隐私属性。

Conclusion: VLMs有望辅助或提升隐私相关属性的大规模自动标注能力，但也存在过度检测的问题。模型与人类标注的互补性为实际应用提供了新思路。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [111] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出一种无训练加速VLLM（视频大语言模型）推理的框架FlashVID，有效减少视频处理的计算量，同时基本保持原有性能。


<details>
  <summary>Details</summary>
Motivation: VLLM模型处理视频时需大量视觉tokens，导致计算效率低；现有方法空间和时间方向独立压缩，未充分利用时空关系，效果有限。

Method: 提出FlashVID推理加速框架，包括两个核心模块：1）基于注意力和多样性的Token选择（ADTS）：挑选最具代表性的tokens用于基本视频表征；2）基于树结构的时空token融合（TSTM）：进一步细致消除时空冗余；且方法无需训练即可直接使用。

Result: 在3个典型VLLM、5个视频理解基准任务上实验，FlashVID只保留10%视觉token即可保留99.1%的LLaVA-OneVision性能；在同一算力下，Qwen2.5-VL能输入10倍帧数，精度相对提升8.6%。

Conclusion: FlashVID无需训练、可直接插拔，可在保证性能的同时显著提升VLLM对长视频的处理能力，大幅扩展其适用范围。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [112] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 论文提出了一种分析和控制视觉想象的自适应框架AVIC，在多模态大模型的空间推理任务中，有选择地调用视觉想象从而提高准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型进步迅速，但在涉及非当前视角的空间推理上仍表现不可靠。以往增强想象力可以提升表现，但盲目想象却可能增加负担甚至误导模型，因此需要明确何时以及如何使用视觉想象。

Method: 提出AVIC框架，通过世界模型并行推理何时当前证据已足够、何时需要加强想象，并自适应调整视觉想象的调用与规模。通过在空间推理和导航任务（如SAT、MMSI、R2R）中测试，分析其有效性。

Result: 实验证明：部分情况下视觉想象是必要的，在另一些情况下则会增加无谓信息导致表现下降。AVIC能够根据实际需求动态控制想象调用，取得与固定策略相当甚至更优的性能，并减少了世界模型调用次数及语言Token消耗。

Conclusion: 控制和分析测试时视觉想象的度量是空间推理高效且可靠的关键，AVIC框架有效提升了多模态大模型的空间推理能力并优化效率。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [113] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 本论文提出了一种结合局部和全局密度特征的少样本人群计数方法，以提高模型在新场景下的泛化能力，并在三个监控数据集上的实验结果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数模型在面对未见过的监控场景时泛化能力较弱，因此需要一种能更好适应不同场景变化的方法。

Method: 将不同监控场景视为不同类别，引入少样本学习。提出多局部密度学习器，学习和编码支持场景中的不同密度分布原型，用于局部特征引导，并提取全局密度特征用于全局引导，实现模型对目标场景局部和全局密度变化的适应。

Result: 在三个监控数据集上进行了实验，提出的方法在未见新场景下表现优于最新的少样本人群计数方法。

Conclusion: 所提出的结合局部和全局密度特征的少样本学习方法有效提升了人群计数模型在新监控场景下的适应性和泛化能力。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [114] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Octopus的全新强化学习（RL）辐射增强框架，通过合成稠密的自我修正案例提升视觉语言模型（VLM）复杂推理任务下的自我校正能力，有效提高了学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂推理任务中需要有效的自我修正能力，但现有的RL方法学习此行为时因正确信号稀疏导致效果不佳。为提升自我校正学习能力，亟需新的数据增强和学习方法。

Method: 提出了Correction-specific rollouts（Octopus）框架，通过重组现有辐射序列合成稠密的自我修正样本，以提升采样效率和学习稳定性。同时，结合响应遮蔽策略，将自我修正与直接推理解耦，避免信号冲突，助力两者协同学习。

Result: 构建了具有可控自我校正能力的推理型VLM——Octopus-8B，并在7个基准任务上测试，性能超越所有开源VLM，其中相较最佳RLVR基线得分提升1分，且每步训练时间仅为其0.72倍。

Conclusion: Octopus框架能有效提升VLM的自我修正能力，兼具采样效率和学习稳定性，为视觉语言推理任务提供先进的解决方案。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [115] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: 提出了一种专注对话、融合多模态的大模型D-ORCA，用于鲁棒的音视频字幕生成，并构建了一个大规模双语对话数据集，模型在说话人识别、语音识别和时间定位上显著优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 视频中语音对话为关键信息来源，实现复杂的视频理解需要准确识别谁在何时说了什么。目前缺乏高质量、大规模的多方对话音视频标注数据和针对双语（英语、普通话）的视频理解模型。

Method: 1）提出 D-ORCA，大型多模态语言模型，专注于音视频中的多方对话字幕生成；2）新建了DVD数据集，含近4万条带有多方对话的视频标注，支持中英双语；3）采用创新的基于组的相对策略优化，设计三种奖励函数，分别针对说话人归属、语音内容和句子时序边界，以强化学习方式优化字幕生成。

Result: D-ORCA在说话人识别、语音转写和时序对齐等方面，显著优于现有开源模型。在多个音视频理解基准测试上，8B参数规模的D-ORCA表现接近Qwen3-Omni等更大模型。

Conclusion: D-ORCA推进了多方对话音视频理解领域，所提出的方法和数据集为开源视频理解生态填补空白，并在模块性能与效率上展现出巨大潜力。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [116] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: 本文提出了EasyTune方法，对扩散模型在每一步去噪过程进行细致调优，提升了运动生成模型的目标对齐效率和内存利用率，并创新性引入自适应偏好学习机制。实验效果优于现有方法，训练更快且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 现有运动扩散生成模型尽管进步显著，但在高效地与下游任务对齐时仍面临优化粗糙、内存消耗高等问题。论文旨在突破这些瓶颈，提升模型的对齐效率和实用性。

Method: 首先理论和实验证明传统扩散奖励优化受制于步骤间递归依赖。EasyTune提出在每个去噪步分别微调（而非全流程），实现去耦合与精细、节省内存的优化。此外，创新性地引入自精炼偏好学习（SPL），从数据中动态发现和学习偏好配对，增强奖励模型。

Result: EasyTune在MM-Dist指标上比DRaFT-50提升8.2%的对齐效果，仅需31.16%的额外内存，并实现7.3倍的训练加速。

Conclusion: EasyTune显著提升了运动扩散生成模型的对齐效率、训练速度与资源利用率，且通过动态偏好学习扩展了训练集适用范围，在多个实验中表现全面优越，有潜力广泛应用于更高效的运动生成场景。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [117] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种用于超低剂量光子计数光谱CT重建的新方法——FSP-Diff，通过深度学习提升噪声环境下的重建质量，实现更优结构与细节恢复。


<details>
  <summary>Details</summary>
Motivation: 超低剂量CT在降低辐射风险的同时，信噪比极差，导致图像出现严重伪影和细节丢失，目前尚缺有效的重建提升算法来解决该问题。

Method: FSP-Diff方法由三部分组成：(1) 融合投影域降噪与图像直接重建，两者互补提升细节与结构。(2) 利用多能投影数据得到高信噪比全谱引导图，作为结构参考。(3) 构建潜在扩散模型，将多通路特征转化为低维潜在空间，提升重建速度与质量。

Result: 在仿真和真实数据集上，FSP-Diff在图像质量和计算效率上均大幅优于现有主流方法。

Conclusion: FSP-Diff展现了超低剂量光谱CT重建的显著优势，具有临床转化潜力，有助于实现低剂量高质量成像。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [118] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CSDN（Continuity-driven Synergistic Diffusion with Neural priors）的新方法，用于提升超稀疏角度采样条件下的锥形束CT（CBCT）三维重建质量。


<details>
  <summary>Details</summary>
Motivation: CBCT在临床应用中需要兼顾辐射剂量和成像质量，超稀疏采样虽然降低了辐射剂量，但会导致图像伪影和层间不一致，使诊断变得不可靠。现有重建方法难以同时兼顾角度连续性与空间细节保真度。

Method: 提出引入神经先验作为重建基础，能够将超稀疏数据合成物理一致的稠密投影。基于该初始化，设计了协同扩散机制，包括Sinogram（投影数据）扩散优化和数字射线照片（DR）扩散优化两个路径，最后通过双投影重建融合模块自适应融合两路输出，实现协同优化。

Result: 大量实验结果表明，CSDN方法在超稀疏采样下有效抑制伪影并恢复细节纹理，超越了当前多种主流方法。

Conclusion: CSDN方法能在极低采样条件下显著提升CBCT重建质量，对提升临床低剂量成像具有重要意义。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [119] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 本论文系统评估了当前最先进的深度伪造检测方法，发现它们在面对最新合成技术生成的深度伪造时表现不佳，表明检测技术急需升级以应对不断进化的生成手段。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速进步，合成视频越来越逼真，传统与新兴的检测方法都面临被突破的风险，亟需重新评估其有效性。

Method: 作者对多种最前沿的深度伪造检测方法进行了全面的实证分析，包括基于Transformer、对比学习等技术，并引入了人类参与的评测实验，与最新合成方法生成的视频进行比对测试。

Result: 实验结果显示，无论是自动检测模型还是人类参与者，在对抗高质量新型深度伪造内容时，识别准确率都大幅下降，表现远低于预期。

Conclusion: 当前的检测技术尚难以有效应对深度伪造生成技术的发展，需要持续加大对检测模型的研究力度，缩小检测方法与合成技术之间的差距，以保障媒体真实性。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [120] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种新的复杂指令图像编辑方法（MCIE-E1），通过改进架构、数据和评估流程，有效提升了模型对复杂指令的遵循能力和背景一致性，在新提出的评测基准上取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法只能处理较简单编辑，难以适应现实世界中复杂、组合性的指令需求；此外模型在指令遵循和背景一致性方面仍有明显不足。

Method: 提出MCIE-E1方法，引入空间感知交叉注意力模块（提升指令遵循能力）和背景一致性交叉注意力模块（保持未编辑区域一致性）；同时搭建了复杂指令数据生成管线，结合MLLM自动筛选与人工验证获取高质量训练数据；提出新基准CIE-Bench及两项评测指标。

Result: 在CIE-Bench基准和相关定量、定性评测中，MCIE-E1在指令遵循性上提升了23.96%，整体效果优于现有最优方法。

Conclusion: MCIE-E1能够显著提升复杂指令驱动的图像编辑质量，尤其在准确理解并执行复杂指令、保持背景一致性方面效果突出，对推动相关实际应用有积极意义。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [121] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

TL;DR: 本文提出了PhysDrape，一种结合神经网络与物理显式求解的服装拟合方法，有效提高了物理真实性及实时性，显著减少了服装与人体的穿透现象。


<details>
  <summary>Details</summary>
Motivation: 以往基于深度学习的服装模拟虽然速度快，但在保持几何正确性和物理合理性之间存在权衡，常因碰撞惩罚导致服装网格畸变或身体穿透，缺乏鲁棒、物理一贯性强的碰撞处理方法。

Method: PhysDrape通过一个物理引导的图神经网络（Physics-Informed GNN）预测残差位移，并引入可微分的两阶段求解器：首先用可学习的力求解器（基于StVK模型）实现准静态平衡，继而用可微分投影器严格消除穿透，整个流程可端到端训练。

Result: 大量实验表明，PhysDrape在确保物理一致性的同时，其穿透几乎为零、应变能显著低于现有方法，整体表现优于最新基线方法，并能实时运行。

Conclusion: PhysDrape突破了软约束方式的瓶颈，通过融合显式物理约束与神经网络推断，极大提升了服装拟合的物理真实性和效率，为实际应用中的虚拟试衣等任务提供了更优解决方案。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [122] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了MIND，这是一个用于评估世界模型在动态视觉环境中记忆一致性和动作控制能力的全新基准数据集。MIND包含高质量的多视角视频和多样化动作空间，通过系统性框架全面测量模型的主要能力，并揭示了现有世界模型面临的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 以往世界模型缺乏针对记忆一致性和动作控制两大核心能力的统一、开放且全面的评测基准，限制了其能力客观评估和进一步发展。因此，作者希望通过构建一个涵盖多视角和多动作空间、有系统评价标准的基准，推动该领域的发展。

Method: 作者构建了MIND基准，包括250段高质量1080p/24FPS视频，涵盖第一人称与第三人称视角、八类场景与多种动作空间。并提出评价框架，系统考察世界模型的记忆一致性（跨时间、场景视角）和动作控制能力（动作泛化等）。此外，设计了MIND-World作为baseline进行性能对比实验。

Result: 通过大量实验证明，MIND基准具备全面性和挑战性，并用MIND-World baseline对现有世界模型进行了评测，发现长时记忆一致性和跨动作空间泛化依然是当前模型的突出短板。

Conclusion: MIND为世界模型的关键能力提供了首个公开、综合、系统的评测平台，填补了该领域评测基准的空白，为后续研究和算法改进提供了重要基础和工具。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [123] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

TL;DR: 本文提出了一种结合Mixture of Experts（MoE）与深度3D卷积对抗生成网络（CGANs）的新架构，用于3D对象的生成与补全，并有效应对数据不完整或缺失的场景。模型在多个数据集上表现优异，优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GANs方法在3D模型生成和补全任务中，难以高效建模复杂和多样的数据分布，尤其对于存在大量缺失区域的数据。此外，3D体素数据处理本身计算量大，进一步增加了模型设计难度。需要更加高效、鲁棒的方法。

Method: 论文提出将深度3D CGANs与MoE框架结合，采用多个具备不同特长的生成器，并通过无损辅助动态容量约束（DCC）机制动态选择合适的生成器参与任务，实现对数据各异模态的捕捉和高效建模。

Result: 在实现3D对象生成和损坏物体补全的任务上，提出的方法在多数据集上取得了优于现有主流方法的定量和定性结果，证明了新架构在效果和效率上的优势。

Conclusion: MoE-DCGAN架构显著提升了3D数据生成和补全的质量与效率，尤其在处理复杂结构和大规模缺失区域时表现出色，为实际3D视觉任务提供了更好的解决方案。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [124] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种通用框架，使ViT各模块实现完备的群等变性，从而提升性能与数据效率，并可直接应用于多种ViT架构。


<details>
  <summary>Details</summary>
Motivation: 现有群等变ViT提升了模型泛化性，但难以在性能和等变性间取得平衡，特别是ViT内不同模块（如Self-Attention和Patch Embedding）之间的等变融合存在挑战。

Method: 提出一个统一的方法，使ViT的Patch Embedding、Self-Attention、位置编码、下采样和上采样等主要部件均具备等变性，从而保证整体架构的等变性，该方法理论严谨、易于并入现有ViT包括Swin Transformer。

Result: 在多种视觉任务和架构上，实验表明该方法的等变ViT能够显著提升性能和数据效率。

Conclusion: 通过系统性地实现ViT关键模块的等变性，可以有效提升ViT家族模型的泛化能力与实用性，框架本身具有高度可扩展性和通用性。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [125] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出了一种多模态弱监督框架，实现了对视频中“隐蔽情绪”自动识别的最佳效果，准确率由<0.6提升至0.69+。通过YOLO、人像视觉特征、CoT+Reflection提示生成伪标签、OpenPose骨骼点与MLP建模、超长序列Transformer编码及多模态融合，打破了数据不平衡、伪标签泛化等难题。


<details>
  <summary>Details</summary>
Motivation: 视频中的“隐蔽情绪”难以通过传统特征直接判别，现有数据集标签稀缺且分布极不平衡。提升少量弱标注下的泛化能力、突破识别瓶颈、寻找更优的多模态融合方式，是推动情绪识别研究及实际应用的核心诉求。

Method: 1. 用YOLO 11x逐帧检测并裁剪出人物肖像，借助DINOv2-Base提取外观特征。
2. 利用Gemini 2.5 Pro结合Chain-of-Thought和Reflection两种prompt，自动生成伪标签及推理文本，作为弱监督信号。
3. OpenPose抽取137维关键点并加入帧间偏移特征，用MLP简化替代传统GNN建模骨骼信息。
4. 超长序列Transformer分别编码图片和骨骼特征，与BERT对访谈文本嵌入拼接融合。
5. 先对各模态分别预训练，再联合微调，并将伪标注数据纳入训练提效。

Result: 在iMiGUE数据集上，提出方法在高度类别不平衡下，整体准确率由原有SOTA的不足0.6提升到0.69+，树立新的公开基准线。MLP建模在关键点序列上与传统GCN表现相当或更优，方法有效提升了隐蔽情绪识别性能。

Conclusion: 多模态弱监督策略结合伪标签生成、合理的特征提取和模型结构优化，在极不平衡数据条件下实现了显著性能提升。MLP可高效替代GCN用于动作/骨骼信息建模，弱监督与多模态预训练-微调机制对情绪识别任务具有普适提升价值。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [126] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: 该论文提出了DICE方法，可以在部署端无需训练地快速去除扩散模型生成图像中的特定艺术家风格，解决了自动化风格模仿带来的知识产权风险。


<details>
  <summary>Details</summary>
Motivation: 扩散模型发展导致用户可以轻松模仿艺术家独特风格，存在版权和知识产权风险，目前的防护方法要么需要高昂的模型编辑成本，要么必须明确指定替换风格，实用性差。

Method: 提出了DICE（Disentanglement of artist Style from Content via Contrastive Subspace Decomposition）框架，通过构建对比三元组，并利用广义特征值分解在潜在空间精确分离艺术家风格和内容，无需额外训练。引入自适应注意力解耦策略，对每个token动态评估风格浓度，分别进行风格抑制与内容增强。

Result: 大量实验表明，DICE能高效去除艺术家风格，同时很好地保留内容完整性，解耦过程仅增加约3秒开销。

Conclusion: DICE是一种无需训练、实用高效的风格去除技术，为防止自动化风格仿冒和保护艺术家权益提供了新解决方案。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [127] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ReRoPE的新方法，能够在无需重新大幅训练或修改网络结构的情况下，将相机视角控制能力集成到现有的预训练视频扩散生成模型中，实现高保真的可控视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前可控相机视角的视频生成方法普遍依赖于关于固定参考视角（如首帧）的相对相机姿态信息，但这种编码方式缺乏平移不变性，容易导致投机性泛化差和累积漂移。更通用的、基于任意视角对之间的相对相机姿态编码虽然鲁棒性更强，但若直接集成到预训练视频扩散模型中却成本高昂、甚至需要结构性更改。

Method: 作者提出ReRoPE（Relative Rotary Positional Embedding）框架：基于现有视频生成模型采用的RoPE（旋转位置编码），发现RoPE中的部分低频谱带尚未被充分利用。ReRoPE方法通过在这些“闲置”的低频带中注入任意视角对之间的相对相机姿态信息，在无需大规模再训练和结构修改的前提下，实现可控相机视角的视频生成。

Result: 实验结果表明，ReRoPE在图像到视频（I2V）及视频到视频（V2V）生成任务中，无论是相机控制精度还是视觉质量方面，均优于现有方法，并能显著减少训练成本。

Conclusion: ReRoPE实现了无需牺牲生成质量和训练代价的大幅提升视频生成的可控性，为具备视角可控需求的视频创作、游戏、仿真等应用提供了更高效和易用的解决方案。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [128] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViT-5的新一代Vision Transformer骨干网络，通过组合近年来架构进展，对原有ViT组件进行改进，在图像分类与生成等多项任务中显著超过现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 近年来，Vision Transformer（ViT）已成为视觉任务中的主流骨干网络之一，但其基本结构自提出后变化不大。为进一步提升其性能，并顺应过去五年的深度学习架构革新，亟需对ViT做系统性现代化升级。

Method: 在保持Attention-FFN主结构的同时，对ViT的归一化、激活函数、位置编码、门控机制和可学习token等组件进行逐项改进，并将这些更新结合为ViT-5骨干网络。

Result: ViT-5在众多视觉理解与生成任务（如ImageNet-1k分类、扩散模型生成等）上优于当前最先进的ViT模型。例如，ViT-5-Base在ImageNet-1k上达到84.2%的top-1准确率，生成任务中的FID也优于经典ViT。

Conclusion: ViT-5以模块化方式吸收了近年来Vision领域的创新，兼容主流训练和应用流程，无缝替换传统ViT，提升了表达能力和泛化性能，是面向2020年代视觉基础模型的理想骨干选择。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [129] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文探讨如何利用生成式多模态大模型（MLLMs）提升视频文本嵌入和检索任务表现，提出了无需视觉监督的新对齐策略，在主流视频检索基准上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs在视觉任务中被作为特征提取器使用，但在视频任务上与专门的视频基础模型（VFMs）相比表现不佳。作者希望探索改进MLLMs在视频文本检索应用中的效果，减少对视觉监督和微调的需求。

Method: 作者首先分析了MLLMs在不同层级的表征能力，发现其中间层已编码了大量与任务相关的信息。基于此，他们将中间层嵌入与校准后的MLLM首部结合，实现了无需训练即可有较好零样本检索性能。进一步提出了一种轻量级的文本对齐策略，将密集视频描述映射为简短摘要，通过纯文本方式实现视频文本嵌入学习，无需视觉监督。

Result: 方法在未进行视觉微调的情况下，性能超过现有方法，在多个常用视频检索基准测试中取得了领先甚至是大幅度的提升。

Conclusion: 即使不对MLLM进行针对性微调或利用视觉监督，通过挖掘其预训练中间层和文本对齐策略，也可实现高效且优秀的视频文本嵌入与检索。该方法降低了视频任务的训练门槛，推动MLLMs在多模态检索等应用中的实用化。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [130] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: 论文发布了MMLSv2——一套针对火星地表滑坡分割任务的多模态数据集，包括七个不同的影像通道，并提供更具挑战性的异地测试集。


<details>
  <summary>Details</summary>
Motivation: 现有滑坡分割数据集多针对地球，数据模态有限，不足以支撑在火星等特定环境中开展鲁棒的分割模型研发与泛化评估。为推动火星滑坡自动识别研究，该数据集应运而生。

Method: 作者构建了包含RGB、数字高程模型、坡度、热惯量和灰度在内的七通道多模态火星地表影像数据集MMLSv2，共计664张影像，并设计了地理隔离的独立测试集（另有276张），采用多种分割模型在不同数据划分下进行实验评估。

Result: 不同分割模型在MMLSv2数据集上传统训练测试分割下取得较稳定且有竞争力的结果，但在碎片化、细长及小规模滑坡区域仍具挑战；在独立测试集上，性能显著下降，验证了数据集在表征泛化能力上的价值。

Conclusion: MMLSv2为火星滑坡分割任务提供了丰富的多模态数据和挑战性测试环境，对评估和提升模型的泛化能力具有重要意义。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [131] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 本文提出了一种针对卫星图像的基于Vision Transformer（ViT）的小规模架构与新型过程方法，显著改善了在灾害后建筑物损伤分级场景下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 灾后快速评估建筑损伤对应急响应至关重要。虽然基于卫星图像的自动化模型具有可扩展性，但存在标注噪声高、类别极度不均衡等问题，亟需提升模型在多样化、高噪声数据集（如xBD）上的表现。

Method: 评估了DINOv2-small与DeiT两种小型ViT模型在xBD数据集上的多分类性能，提出了基于patch的预处理流程以提升结构特征并减少背景噪声，训练中采用frozen-head微调策略以降低算力需求，最终通过准确率、查准率、查全率和宏平均F1分数系统评估表现。

Result: 采用上述方法后，小型ViT模型在灾害损伤分类上的宏平均F1分数达到了与先前CNN基线模型可比的水平，尤其在高噪声、类别不平衡环境下表现突出。

Conclusion: 针对灾后建筑损伤评估，采用创新patch预处理和ViT微调的新流程能提升小型ViT的综合性能，有望成为高效且实用的卫星图像分析方案。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [132] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

TL;DR: 提出了MambaFusion，一种高效自适应的摄像头-LiDAR多模态BEV 3D检测方案，实现了对上下文建模和不确定性推理的强化，并在nuScenes数据集创下新性能纪录。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要高可靠性的3D目标检测。现有摄像头与LiDAR的融合方法面临上下文建模低效、空间融合缺乏自适应以及对不确定性处理不足等挑战。

Method: 1. 融合selective state-space models和windowed transformers，用线性复杂度实现全局上下文传播与局部几何保真。
2. 利用MTA模块及自适应融合门，对多模态特征进行空间置信和标定一致性动态加权融合。
3. 设计结构约束扩散头，将基于图的推理与不确定性消噪结合，提升物理合理性和置信度校准。

Result: MambaFusion在nuScenes基准测试上取得了新的SOTA表现，并且处理复杂度为线性量级。

Conclusion: SSM高效特性和基于可靠性的自适应融合机制结合，使3D检测更健壮、时序稳定且具可解释性，适用于实际自动驾驶场景。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [133] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: 本文介绍了“全球农田（FTW）”生态系统，包含大规模农田边界数据集、预训练分割模型和推断工具，旨在支持农田边界提取和作物分类等任务。


<details>
  <summary>Details</summary>
Motivation: 精确的农田边界地图是农作物监测、产量估算和病虫害预测等多种农业数据产品的基础，目前缺乏全球大规模、多样化的高质量农田边界标准数据和易用工具。

Method: 构建了横跨24个国家、包含160万个农田边界的FTW基准数据集；开发了预训练分割模型与命令行推断工具，并通过两个notebook演示本地与国家尺度的农田边界及作物类型提取。采用MOSAIKS特征及FTW农田边界进行作物类型分类，评估模型表现。

Result: 基于有限标签，作物类型分类的宏F1得分达0.65–0.75；展示了对5个国家（约4.76万平方公里）的预计算结果，不同国家田块面积中位值从0.06公顷到0.28公顷不等。

Conclusion: FTW提供了覆盖全球的大规模农田边界数据、工具与基准任务，能有效提升农田边界提取与作物类型分类的能力，为农业遥感等领域研究提供了重要基础设施。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [134] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 本文揭示了现有视觉-语言大模型（VLM）在面向拆分图片输入的安全对齐存在漏洞，通过提出新的拆分图片视觉越狱攻击（SIVA），能够绕过现有防护机制，并展现出较高的跨模型迁移能力。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM经由RLHF等方式进行了安全对齐，已有防护可应对直接/整体图片攻击，但未对由多个无害图片片段组合后产生的有害信息进行足够防护。因此，存在对拆分图片攻击的潜在脆弱性。

Method: 作者设计了逐步升级的拆分图片视觉越狱攻击（SIVA）：从基础图片分割，逐步演进至适应性白盒攻击，最终结合了一种创新的对抗性知识蒸馏（Adv-KD）算法，有效提升了攻击方案的黑盒迁移性。

Result: 实验证明，所提出的强力SIVA攻击在三个先进VLM及三套越狱数据集上，最高达到比现有基线高60%的跨模型攻击成功率。

Conclusion: 该项研究不仅发现了VLM安全对齐的新型短板，也提出了高效缓解该漏洞的方法，推动VLM安全性进一步完善。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [135] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

TL;DR: 本文提出了DAS-SK，一种高效且轻量化的高分辨率农业图像语义分割模型，兼顾准确性与计算效率，适用于边缘设备与实时应用。


<details>
  <summary>Details</summary>
Motivation: 高分辨率农业影像的语义分割实际应用中需兼顾高准确性和低计算消耗，以便于在无人机、机器人等边缘设备上部署。然而，现有方法普遍模型庞大、计算量高或泛化能力有限，难以满足上述需求。

Method: 提出DAS-SK结构，在DAS-Conv（双空洞可分离卷积）模块中引入了选择性卷积（SK-Conv）机制，增强多尺度特征学习能力，同时改进了ASPP模块以结合局部与全局信息。模型以改进的DeepLabV3为主干，采用MobileNetV3-Large与EfficientNet-B3双骨干，提升对小数据集、谱域泛化和边缘部署的适应性。

Result: 在LandCover.ai、VDD和PhenoBench三大基准上，DAS-SK均表现出色，刷新语义分割精度，与现有CNN、Transformer及混合模型相比，参数量减少最高21倍，计算量（GFLOPs）减少最高19倍。

Conclusion: DAS-SK在高效性和分割精度间达成良好平衡，具有广泛实用前景，适合用于无人机、农业机器人等对实时性和资源受限场景，并对其他计算机视觉领域也有推广潜力。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [136] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus 是一种能够基于几何和外观属性进行个性化3D形状生成的新框架，通过从参考形状中提取属性并与文本结合，可以灵活合成新形状。


<details>
  <summary>Details</summary>
Motivation: 当前3D形状生成领域缺乏同时针对几何和外观属性的个性化生成方法，且难以灵活地组合和迁移这些属性，不能很好地满足个性化和跨类别生成需求。

Method: 提出一种进阶式优化策略，将3D形状个性化建模为从参考形状中抽取可复用、与类别无关的几何与外观属性，并与文本描述组合生成新形状。进一步引入区域级的概念学习和上下文相关/无关损失，实现更加灵活与细致的属性提取和合成。

Result: PEGAsus 能够高效地从各种参考形状中抽取属性，并与文本灵活组合生成新形状，支持对生成结果的精细控制，实现多样化的个性化形状生成，并能在跨类别场景下表现出色。实验表明该方法优于现有最新方案。

Conclusion: PEGAsus 框架不仅实现了细粒度的个性化3D形状生成，提升了灵活性和生成质量，也为实际的跨类别3D设计和个性化应用带来了创新的解决方案。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [137] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的回归方法（MCSDR），以更准确地从心脏超声视频中估计左心室射血分数（LVEF），有效应对传统方法在多峰分布和高噪声情况下的局限。


<details>
  <summary>Details</summary>
Motivation: 传统用深度学习进行LVEF估计，大多采用均方误差回归方式，将问题简化为点估计，但在数据存在不确定性、多模态或异常时常产生误差，对临床分析带来风险。作者希望通过生成式回归的方法，更好地捕捉这种不确定性。

Method: 作者提出了多模态条件分数扩散回归模型（MCSDR），能够对超声视频和患者人口学信息联合建模，直接生成反映LVEF后验分布的结果，从而提高对复杂病理和高变异性的适应能力。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric以及CAMUS三个数据集上，MCSDR相较于现有方法取得了最优表现。特别是在高噪声和高生理可变性的病例中，MCSDR展示了其生成轨迹的独特性和更强的AI诊断解释力。

Conclusion: 利用生成式回归（扩散模型）可以显著增强基于超声的LVEF估计，对于处理医学图像的不确定性和多样性问题具有重要临床价值，同时提升了AI模型的可解释性。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [138] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出一种新方法GR-CoT，通过空间推理链提升遥感图像开放词汇分割的准确性，解决以往方法对地理语境理解不足导致的类别混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像开放词汇语义分割方法主要依赖视觉特征和文本嵌入的被动映射，缺乏地理空间上下文感知，导致语义歧义和分类错误，特别是在光谱特征接近而语义属性不同的地物类别间。

Method: 提出GR-CoT框架：包含1）离线知识蒸馏流用于建立细粒度类别解释标准、解决相似类别的语义冲突；2）在线实例推理流，以宏观场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理生成图像自适应词汇，引导下游模型实现像素级地理语义对齐。

Result: 在LoveDA和GID5两个常用遥感语义分割数据集上，通过大量实验证明了方法的优越性。

Conclusion: GR-CoT框架提升了多模态大语言模型对场景的理解能力，实现了更精确、语义混淆更小的开放词汇遥感图像分割，对相关任务具有推广价值。

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [139] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为Chain-of-Caption的免训练框架，通过工具引入额外文本或视觉上下文，显著提升多模态大模型（MLLMs）在指代表达理解（REC）任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在REC任务上已表现出色，但进一步提高准确率往往需要大规模数据和模型，且训练成本高。作者关注如何通过引入额外上下文，利用工具辅助模型，无需再训练也能提升效果。

Method: 系统分析了提供额外视觉和文本上下文的不同技术及其对MLLMs完成REC任务的影响。提出了称为Chain-of-Caption的新方法，该方法无需重新训练，通过组合多种上下文提升准确率。

Result: 在RefCOCO、RefCOCOg、RefCOCO+和Ref-L4等常用数据集上实验发现，无需微调，仅通过单一文本或视觉上下文即可提高REC表现。组合多种上下文后，免训练框架在各种IoU阈值的准确率上对比基线提升5%-30%。

Conclusion: 引入工具辅助的多种上下文，无需微调即可提升MLLMs在REC任务上的表现。所提出的Chain-of-Caption框架简单有效，为相关任务提供了实用路径。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [140] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

TL;DR: 本文提出Efficient-SAM2，以精简SAM2模型的冗余计算，显著提升视频目标分割的推理效率，并在精度损失极小的情况下实现了1.68倍加速。


<details>
  <summary>Details</summary>
Motivation: SAM2虽在视频目标分割上表现优异，但计算量大阻碍其实时应用。虽然已有提升效率的尝试，大多聚焦于轻量化主干网络再训练，缺乏对训练后模型的加速探索。作者注意到SAM2模型的感知存在稀疏性，为减少冗余计算和提升速度提供了新思路。

Method: 作者提出Efficient-SAM2，包括两大核心方法：1）图像编码器部分引入对象感知的稀疏窗口路由（SWR），根据前帧解码器输出，将背景区域路由到更简化、轻量的分支，显著减少无关计算。2）记忆库部分提出对象感知的稀疏记忆检索（SMR），仅让每帧中最显著的tokens参与记忆注意力计算，并复用首次检索出的显著性区域，从而去除许多不必要的全token运算。

Result: Efficient-SAM2在几乎不增加参数和训练成本的前提下，在主流数据集上（以SAM2.1-L为例），实现了1.68倍的推理速度提升，仅带来1%的精度降低。

Conclusion: Efficient-SAM2有效利用模型感知的稀疏性特点，通过结构与推理优化，大幅减少冗余计算，在保证精度的前提下显著加速了SAM2模型，适用于对实时性要求高的场景。

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [141] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: 本文提出了MA-ADV框架，首次利用点云表示生成事件相机的对抗样本，达到了100%攻击成功率且扰动最小。


<details>
  <summary>Details</summary>
Motivation: 事件相机广泛应用于安全关键领域，但基于事件的深度网络极易受到对抗攻击。然而，受限于事件表示的不可微特性，目前对事件攻击的研究非常稀缺。

Method: 作者提出了MA-ADV方法，通过点云表示生成事件对抗样本，巧妙考虑了事件中的高频噪声，并引入扩散方法平滑扰动，结合Adam优化、迭代细化与二分搜索，寻找最小成本扰动。该方法充分利用了事件的空间和时间关系。

Result: 在多项实验中，MA-ADV以最小扰动成功率达到100%，并在对抗防御下展现出更强鲁棒性。

Conclusion: MA-ADV不仅暴露了事件相机系统潜在的安全隐患，也为未来基于事件的感知系统对抗安全性提出了新的挑战。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [142] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 本研究提出直接对fMRI原始BOLD时序信号进行建模的方法，并开发了DeCI框架，在五个公开数据集上实现了优于传统静态FC方法的脑部疾病分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态功能连接（FC）分析，忽视了fMRI时序信号的动态信息，且仅捕获线性关系，导致对脑动力学刻画不足，本研究旨在挖掘时序特征提升脑部疾病分类性能。

Method: 首先系统性地评测了多种先进的时序模型（如PatchTST、TimesNet、TimeMixer）对原始BOLD信号的处理能力。然后，提出DeCI方法，核心包括：1) 周期-漂移分解（Cycle and Drift Decomposition），分别建模脑区的周期波动与漂移趋势；2) 信道独立性（Channel-Independence），分别建模每个ROI，增强鲁棒性并减轻过拟合。

Result: 大量实验结果表明，最新的时序模型在五个公开fMRI数据集上的表现均优于传统FC基线。所提DeCI框架进一步提升了分类准确率和泛化能力，超过了FC方法和时序基线。

Conclusion: 直接建模fMRI原始时序信号有助于更好地捕捉复杂脑动态，显著提升脑部疾病分类性能，建议fMRI分析领域向端到端时序建模方向发展。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [143] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO提出了一种视频扩散模型，针对专业影视制作中的高质量、精细视频实例插入需求，实现了任意稀疏关键帧精确控制，在多个指标和基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成依赖繁琐提示词设计和反复挑选，难以满足专业影视制作对“精细可控、高保真后期处理”的需求。特别是视频实例插入任务，要求在不损坏原视频时空一致性的前提下，将特定对象精准自然地插入原有视频，且操作尽量简便。现有方法尚无法高质量、直观且稳定地实现这一目标。

Method: 本文提出PISCO模型，基于视频扩散机制，并允许用户以任意稀疏关键帧（包括单帧、首尾帧或多个分散帧）指定插入实例。模型引入了“变信息引导”以提升对稀疏控制的鲁棒性、“保持分布的时序掩蔽”来提高生成时的时序稳定性，并用几何知识增强场景适应。还构建了PISCO-Bench，作为针对实例插入任务的新测试基准。

Result: 实验显示，在采用参考与无参考感知指标的评测下，PISCO在稀疏关键帧输入下持续优于现有强劲的修复和视频编辑基线，且随着输入控制点增多，生成效果有显著、单调提升。

Conclusion: PISCO为AI辅助影视制作提供了精确、灵活的视频实例插入解决方案，并通过创新模型设计和新基准验证，证明了其在实际业务中极高的应用潜力。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [144] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出了一种多模态融合框架，通过整合高质量但稀缺的Presence-Absence（PA）数据与大规模、但标签有噪声的Presence-Only（PO）数据，提升了大范围跨物种植物分布预测的表现，在GeoLifeCLEF 2025数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 大尺度、跨物种植物分布预测对生物多样性保护至关重要，但现有观测数据稀疏且偏倚，难以获得高质量训练标签。PA数据精度高但数量稀少；PO数据丰富但标签噪声大。两类数据各有优劣，亟需设计有效融合方法以解决实际应用中的数据约束问题。

Method: 提出多模态融合框架：1）基于卫星影像地理覆盖的伪标签聚合策略，提升PO数据标签空间与遥感特征空间的地理一致性；2）使用Swin Transformer Base提取遥感影像特征，TabM网络提取表格特征，Temporal Swin Transformer进行时序建模，并采用可堆叠的三模态跨注意力机制融合异质数据；3）借鉴专家混合范式，将测试样本按空间位置划分，分别用不同数据集训练的模型推理，缓解PO标签噪声带来的影响。

Result: 在GeoLifeCLEF 2025数据集上，所提方法在PA覆盖有限且分布存在显著偏移场景下，预测性能优于主流融合方法。直接混合PA与PO样本的模型易因PO标签噪声性能下降，而该方法有效解决了这一问题。

Conclusion: 本文提出的多模态融合及分区推断策略，能充分利用不同标签数据的互补优势，在现实场景中提升植物分布预测的准确性，对生物多样性保护具有实际应用价值。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [145] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出了CAE-AV框架，有效缓解了音视频学习中的模态错位问题，在多个基准测试上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 音视频学习常因声源位置不明确或背景干扰导致模态错位，现有方法往往放大无关内容，影响表现与泛化能力。

Method: 提出了包含CASTE和CASE两大模块的CAE-AV框架，分别通过跨模态一致性引导和语义对齐关注关键时空区域，并设计轻量化损失函数提升模态对齐。

Result: 在AVE、AVVP、AVS、AVQA等多项基准测试上达到最优，且在冻结预训练基础上的迁移表现突出。

Conclusion: CAE-AV框架显著增强音视频信息对齐与语义对齐能力，提高任务表现，并提升了对模态错位的鲁棒性。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [146] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出了一种利用语言信息引导的高效动作离散化方法（LG-Tok），提升动作生成的效率和表现。


<details>
  <summary>Details</summary>
Motivation: 传统动作离散化通过增加token数量提高重建质量，但这会加大生成模型学习难度。亟需在保证高质量重建的同时，简化生成复杂度。

Method: 提出了语言引导离散化（LG-Tok）框架，在tokenization阶段对齐语言和动作，借助Transformer Tokenizer提升语义对齐能力，并设计语言drop方案训练detokenizer，在训练过程中随机去除语言条件，增强模型的泛化能力。

Result: 在HumanML3D和Motion-X基准上，LG-Tok的Top-1和FID指标优于现有最优方法（Top-1: 0.542/0.582，FID: 0.057/0.088），精简版LG-Tok-mini用一半token也能维持竞争力表现。

Conclusion: LG-Tok能高效生成高质量动作，并有效利用语言引导，减少token数量同时保持表现，具备更强泛化与应用价值。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [147] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: 本文提出了UGData数据集，将街景图像与空间结构对齐，并开发了UGE训练策略与UGBench评测基准，有效提升了城市空间任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态城市环境理解缺少街景图像与城市空间结构的显式对齐，导致空间推理和城市感知任务效果受限。为了解决这一问题，作者提出了新的数据集和方法以增强多模态空间嵌入的可迁移性与空间性。

Method: 作者提出了UGData数据集，将街景图像锚定到结构化空间图，并通过空间推理路径和语境描述提供对齐监督。基于UGData，提出了UGE两阶段训练策略，将图像、文本与空间结构稳健对齐，并结合图编码和指令引导 对比学习。此外，设计了UGBench基准测试集，覆盖地理定位、图像检索、空间感知等城市任务。方法在多个主流VLM（视觉大模型）骨干网络上实现，包括Qwen2-VL等，并采用LoRA调优生成空间嵌入向量。

Result: UGE基于Qwen2.5-VL-7B主干在图像检索和地理定位排名上，在训练城市分别提升44%、30%，在新城市中也有超30%和超22%的提升。

Conclusion: 通过空间信息显式对齐和空间化训练策略，能显著提升多模态模型在需要空间推理的城市感知和理解任务上的能力，验证了方法与基准的有效性。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [148] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 本文提出了首个专为“大模型带图像推理”新范式（thinking with images）下的过程奖励模型（PRMs）而设计的评测基准，用于细致评估和推动PRMs发展。


<details>
  <summary>Details</summary>
Motivation: 随大视觉语言模型快速发展，模型实现了“带图像思考”能力，即能动态编辑和再编码视觉信息，模拟人类视觉推理过程。但此范式下推理中容易产生多样化错误，现有对PRMs的评测几乎都是以文本为中心，无法真实反映新范式下PRMs性能，需要新的评测基准与分析。

Method: 本文详细分析了推理过程轨迹和基于PRMs的指导搜索，定义了7类细粒度错误类型，构建了一个包含1206条手工标注推理轨迹的数据集，涵盖4大类16个子类，用于细粒度评测和驱动PRMs改进。

Result: 实验证明，当前LVLMs在作为PRMs时评估推理过程的能力有限，不同类型错误表现差异较大、有较强正向评价偏见，并对推理步骤位置高度敏感。

Conclusion: 本文建立的基准可有效发现和量化过程奖励模型在大模型图像推理过程中的不足，为后续相关研究和模型优化奠定了重要基础。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [149] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 本论文提出了电商短视频理解的新基准与新模型，显著提升了对电商视频中商业意图推理的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解模型及数据集主要关注通用任务，忽视了带有强商业目的的电商短视频，其多模态密度高且解释难度大，现有方法在此类视频上表现不佳。因此，亟需专门针对电商视频的评测基准及有效的推理模型。

Method: 1）提出多模态信息密度评估框架，量化电商短视频在视觉、音频、文本三模态下的信息复杂度；2）构建E-VAds基准，收集并标注3961条淘宝短视频，包含19785组开放式QA，覆盖感知、认知与推理等五项任务；3）提出E-VAds-R1推理模型，采用基于强化学习的多粒度奖励机制（MG-GRPO），为探索和精细任务分别设计奖励。

Result: 实验表明，E-VAds-R1模型在仅用几百条样本训练的情况下，在商业意图推理任务上比现有方法提升高达109.2%。

Conclusion: 电商短视频具有极高的多模态信息密度，理解难度高。新提出的E-VAds-E基准和E-VAds-R1模型为该领域的研究提供了新的基础和方向，对商业意图推理具有显著提升作用。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [150] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 该论文提出了GeoEdit框架，通过扩散Transformer模块和新建立的数据集，在复杂场景下实现更精准的几何编辑（位移、旋转、缩放）和更真实的光影效果。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型提升了图像编辑能力，但在对象几何变换和复杂光影效果处理上仍表现不佳，难以精准控制物体位置与外观且易生成“不真实”的图像。

Method: 作者设计了GeoEdit框架，利用扩散Transformer模块结合几何变换，实现物体精确编辑。同时提出了Effects-Sensitive Attention机制提升对复杂光影的建模能力。并构建了一个包含超12万高质量图片对的大规模几何编辑数据集RS-Objects用于训练。

Result: 在多个公开基准测试上，GeoEdit在视觉质量、几何精度和真实感上均优于现有主流方法。

Conclusion: GeoEdit有效提升了扩散模型在复杂场景下的几何编辑精度以及渲染真实度，为图像编辑带来新的突破。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [151] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种融合扩散模型和时间对齐的新颖视频修复方法D$^2$-VR，有效提升了处理速度和鲁棒性，同时保持高质量的视频修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型和时间对齐的视频修复方法，虽然在感知质量上表现优秀，但推理速度慢且在复杂实际退化情况下面临不稳定性，难以实用。

Method: 提出D$^2$-VR框架：1) 设计了退化鲁棒流对齐（DRFA）模块，借助置信度感知注意力过滤不可靠运动信息；2) 采用对抗蒸馏策略，将扩散采样压缩到极少步数内，实现快速推理；3) 提出协同优化策略，兼顾感知质量和时间一致性。

Result: 在大量实验中，D$^2$-VR显著提高了推理速度（加速12倍），并取得了当前最优的视频修复性能。

Conclusion: D$^2$-VR显著突破了扩散先验在视频修复中的推理瓶颈和时序不稳定难题，实现了更高效、更稳健的高性能视频修复，具有实际应用潜力。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [152] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: 本论文提出并构建了高度逼真的合成结肠镜数据集 RealSynCol，用于推动结肠内镜影像的深度学习研究。该数据集包括28,130帧高仿真影像，配有多种真实标签，显著提升了临床图像上的算法泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习有望通过3D重建提升结肠镜的诊断及导航能力，但大规模真实标注数据的稀缺限制了方法发展。因此，作者希望开发一种高仿真的合成数据集，解决数据不足的问题。

Method: 作者从10例CT提取结肠三维结构，导入仿真环境，模拟手术真实情况，并渲染逼真的血管纹理。每帧都配备深度图、光流、三维网格和相机轨迹等标签。数据集用于深度估计和位姿估计等基准测试。

Result: RealSynCol在真实感和变异性上均优于其他合成数据集。在深度和位姿估计等任务的基准测试中，该数据集显著提升了深度学习算法在临床影像上的泛化性能。

Conclusion: RealSynCol数据集极大丰富了用于结肠镜深度学习研究的训练资源，为开发辅助内镜诊断算法提供了有效工具，有望推动相关智能医疗应用的发展。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [153] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 作者分析了基于注意力机制的稀疏图像匹配模型训练过程中的关键设计选择，并提出一种新的无检测器依赖的微调方法，显著提升了模型的通用性和匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的图像匹配方法（如LightGlue）存在被忽视的关键设计问题，同时对于检测器（detector）和描述子（descriptor）在性能中的实际作用缺乏系统研究。作者希望提升模型泛化能力，并简化模型对特定特征点检测器的依赖。

Method: 1. 分析和重新审视了注意力图像匹配模型中的设计选择，对LightGlue等现有模型进行深入对比；2. 通过实验证明在匹配框架下，检测器对性能影响比描述子更大；3. 提出了一种基于多种检测器关键点联合微调现有匹配模型的方法，使其获得检测器无关性。

Result: 微调后的模型能够作为新检测器的zero-shot匹配器使用，并在多种特征类型下取得与专门训练模型相当或更好的匹配精度。

Conclusion: Transformer-based图像匹配模型的瓶颈多在检测器环节，多检测器微调方法可极大提升模型兼容性和部署灵活性，为后续局部特征与匹配模型设计提供新思路。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [154] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频理解任务——基于演示驱动的视频上下文学习，并配套推出了具有挑战性的基准Demo-ICL-Bench，以及相应的解决方法Demo-ICL。实验结果表明该基准具有挑战性，所提方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的视频理解基准主要考察模型的静态知识，而忽略了模型从新的、动态的上下文中通过少量示例学习和适应的能力。因此，作者希望通过新任务和基准促进模型的上下文学习能力。

Method: 作者提出了Demo-driven Video In-Context Learning任务，并开发了Demo-ICL-Bench基准，该基准基于1200个YouTube教学视频及相关问题，包含文本和视频两种演示示例。为应对该任务，还提出了Demo-ICL模型，通过视频监督微调和信息辅助的直接偏好优化两阶段训练提升上下文学习能力。

Result: 大量实验表明，Demo-ICL-Bench对现有顶尖多模态大模型具有很大挑战性，且所提的Demo-ICL方法提升了模型的上下文学习能力。

Conclusion: 新基准和任务揭示了现有模型上的不足，所提出的方法有效推动了视频上下文学习发展，并为未来研究指明了方向。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [155] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 该论文提出Vista框架，通过场景感知的方式实现高效且可扩展的流式视频问答模型，有效缓解了长时视频处理中的上下文丢失与内存溢出问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在处理流式视频问答时，因依赖固定大小内存或简单压缩，表现出上下文丢失或内存溢出，难以适应长时、实时场景，亟需一种既高效又能保持信息完整性的解决方案。

Method: Vista框架包含三大创新：1）场景感知分割，将连续帧动态聚类为时间和视觉连贯的场景单元；2）场景感知压缩，将每个场景编码为紧凑token存入GPU，实现高效索引，同时原始帧存入CPU；3）场景感知召回，接收到提问时仅召回相关场景及其内容，有效保证模型推理效率与信息完整性。框架通用，适用于多种视觉-语言模型。

Result: 在公开数据集StreamingBench上的大量实验表明，Vista取得了当前最优性能，显著优于以往方法，并成为流式视频理解的新基线。

Conclusion: Vista以创新（场景感知分割、压缩、召回）解决了流式视频问答中的效率和上下文完整性难题，兼顾推理效率与存储占用，具备很好的实际应用前景。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [156] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种创新的文本驱动动作生成框架TriC-Motion，实现了空间、时间和频率三域的联合建模并集成因果干预，从而提升了动作生成的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法大多仅关注时空建模或独立频域分析，缺乏将三者有效统一的框架，导致未能最大化利用各域信息，生成效果受限。此外，噪声与有用特征往往纠缠，影响动作生成质量。

Method: 作者提出TriC-Motion框架，结合扩散模型，分别构建时域编码、空间拓扑建模和混合频率分析模块。通过评分引导的三域融合模块整合三域信息，并设计基于因果推断的反事实动作解耦器以剔除动作无关信息，明确各域的实际贡献。

Result: 实验表明，TriC-Motion在HumanML3D数据集上性能优越，取得R@1为0.612，显著超越现有方法，生成的动作在逼真度、一致性、多样性以及与文本的匹配性方面表现优秀。

Conclusion: TriC-Motion达到高质量的文本到动作生成，能够有效整合三域信息并消除冗余噪声，为实现高保真度和丰富多样的人体动作生成提供了新的解决方案。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [157] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D姿态估计的手势分类框架，有效提升了自动驾驶车辆对行人手势的识别能力，分类准确率达87%。


<details>
  <summary>Details</summary>
Motivation: 在交通情境中，手势是行人与司机非语言交流的重要方式，而自动驾驶车辆对这些手势的识别存在困难。为提升自动驾驶系统的感知能力，有必要开发高效的手势分类方法。

Method: 作者使用WIVW数据集中真实世界视频，采用2D姿态估计算法，对手势分为停下、前进、致谢与打招呼以及无手势四类。通过提取76种归一化关键点的静态与动态特征，训练手势分类模型。

Result: 实验结果显示，手部位置和运动速度特征对手势区分效果显著，整体分类准确率达到87%。

Conclusion: 所提方法能显著提升自动驾驶车辆对关键交通手势的识别效率，有助于增强其交通环境理解能力，并促进对行人交通行为的深入研究。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [158] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 本文分析了照明变化带来的领域转移问题对食品识别系统的影响，并提出通过合成照明增强数据集提升模型鲁棒性。实验显示新方法显著增强了跨数据集的识别效果。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的食品识别系统（如自动化输送带检测）因不同照明条件容易出现识别准确率下降问题。以往研究多局限于单类别、受控场景或缺乏照明标注，难以推广到真实应用。

Method: 作者基于Food-101和Fruits-360两个多类别数据集，系统性地改变光温和强度，构建合成照明增强数据集，从而在无需额外标注的情况下分析照明对识别鲁棒性的影响。评估跨数据集迁移学习和领域泛化能力，聚焦对照明敏感的类别。

Result: 跨数据集测试显示，照明不匹配导致识别准确率大幅下降，引入照明感知增强后，模型鲁棒性明显提高，有效缓解领域转移影响且不损失实时性能。

Conclusion: 照明鲁棒性对食品识别系统在真实场景中的部署至关重要，合成光照增强是提升系统可靠性的重要手段，研究为今后食品自动检测系统的实际应用提供了有效路径和实践参考。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [159] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

TL;DR: 本文评估了视觉基础模型（VFMs）在不同类型电子显微镜图像（EM）上的迁移泛化能力，发现现有方法难以在异质数据集间建立一个鲁棒、通用的线粒体分割模型。


<details>
  <summary>Details</summary>
Motivation: 现有VFMs已广泛用于生物医学图像分析，但其潜在表征在不同类型显微镜数据上是否具有足够的泛化能力，以及如何实现高效迁移，尚不明确。作者以线粒体分割为任务，探究其可迁移性现状及挑战。

Method: 作者选用两个公开EM数据集（Lucchi++和VNC）与三种流行VFMs（DINOv2, DINOv3, OpenCLIP），在两种适应方式下评估其表现：一是“冻结骨干网络+轻量分割头”方案；二是基于LoRA的高效参数微调（PEFT）。此外，利用PCA、Fréchet距离和线性Probe等方法分析潜在特征空间的结构。

Result: 在单一EM数据集上，无论哪种网络骨干，模型都能取得较好分割性能，且LoRA微调能进一步提升效果。但对多个异质数据集联合训练时，模型性能严重下降，LoRA亦仅有微弱增益。特征空间分析显示，两个EM数据集合间尽管视觉上相似，但表征依旧存在明显、持久的领域差异。

Conclusion: VFMs在单一领域的EM图像轻量适配方面表现优异，但面对异构EM数据集时，现有PEFT策略难以构建一个通用鲁棒的模型，凸显了额外领域对齐机制的必要性。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [160] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: 作者提出了GeoFocus框架，通过关键局部感知器和紧凑的拓扑形式语言，有效提升了大规模多模态模型在几何问题解决上的表现。实验结果显示其准确率和鲁棒性均超过了现有专用模型。


<details>
  <summary>Details</summary>
Motivation: 大多现有的大型多模态模型在几何问题求解上存在挑战，尤其是在需要结合全局形状识别和复杂几何理论局部关系的任务中表现不佳。因此，作者希望提升多模态模型在复杂几何理解中的能力。

Method: GeoFocus框架包含两个主要模块：一是关键局部感知器（Critical Local Perceptor），利用13种基于几何理论的感知模板，自主检测和强化如角度、平行线、距离比较等关键局部结构，提高关键几何特征的覆盖率；二是VertexLang，一种利用顶点坐标和连通关系进行图形编码的紧凑拓扑形式语言，替代传统的冗余编码方式，并有效加快模型的训练速度。

Result: GeoFocus在Geo3K、GeoQA和FormalGeo7K数据集上的准确率比领先的专用模型提升了4.7%；并且在MATHVERSE数据集多种视觉条件下展现出更优的鲁棒性。VertexLang使全局感知训练时间减少了20%，同时提升了拓扑识别准确率。

Conclusion: GeoFocus框架通过强化局部感知与高效图形编码，有效改善了LMM在几何问题解决上的表现，为多人模型在几何智能领域提供了新的研究方向。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [161] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 本文提出了一种基于双重计算离散化和反馈控制算法的X射线层析图像重建正则化参数自动选择方法，在真实断层扫描数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: X射线层析中的图像重建属于病态逆问题，常依赖正则化技术缓解不适定性。而正则化参数的选取影响到数据拟合与先验信息的权衡，目前往往缺乏高效自动的选择方法。

Method: 方法通过让同一问题采用两种不同的计算离散化，实现基于反馈控制的参数调整。迭代地根据两种网格下重建结果的相似性自动调整正则化强度，直至找到最小且效果充分的参数。

Result: 在真实断层扫描数据上进行了测试，结果表明该自动参数选择方法能够获得良好的重建效果和平衡的数据-先验权重选择。

Conclusion: 提出的方法能够自动、有效地选取X射线层析逆问题的正则化参数，提升了重建质量及操作便捷性。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [162] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 本文提出了一种基于稀疏关键点的单目热成像SLAM系统，利用来自可见光训练数据的通用特征提取与匹配方法，并通过数据增强和改进的SLAM模块适应热成像场景，在公开数据集上表现出良好鲁棒性，无需热成像特定训练。


<details>
  <summary>Details</summary>
Motivation: 在低照度、烟雾或恶劣天气等视觉退化环境下，传统视觉SLAM难以应用，而热成像作为感知手段虽有应用潜力，但因图像纹理少、对比度低、噪声大，使基于特征的SLAM方法面临显著挑战。亟需开发适用于热成像的高效SLAM技术。

Method: 系统采用SuperPoint特征点检测器和LightGlue匹配器，二者均在大规模可见光图像上训练。为补偿热成像域差异，提出预处理流程以增强输入适应性，同时对SLAM核心模块做针对稀疏和异常匹配的调整，引入SuperPoint关键点置信度作为加权因子，提高建图与定位的鲁棒性。

Result: 在公开热成像数据集上进行实验，结果表明该系统无需针对热成像的专项训练或微调，依然能可靠完成SLAM任务，性能优于依赖特定数据集训练的方法。

Conclusion: 本文方法结合通用特征提取与特定适配技术，实现了在热成像数据上的高可靠、低门槛SLAM解决方案，具备实际部署价值并为少量高质量热成像数据局限下SLAM研究提供了新思路。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [163] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无需学习的4D高斯场物体级分割方法，通过双阶段迭代边界细化，实现了在动态场景下对物体的更准确分割。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯场分割在处理动态场景中复杂运动、遮挡和模糊边界时效果有限，且常用一次性阈值方法易丢失结构完整性与准确性。针对这一挑战，论文希望提高分割精度和效率，有效处理遮挡与边界问题。

Method: 核心方法为TIBR4D，包括两个阶段：第一步是IGIT，迭代追踪每个时间段中的高斯点到实例的概率，实现逐步优化分割并提取更完整的点云；第二步是RCC，针对每帧抑制对象边界附近不确定性高的高斯点，但保留主要贡献，以获得更准确的边界。此外，提出分时段合并策略，既保证身份一致性，又能及时感知动态变化。整个流程无需学习训练。

Result: 在HyperNeRF和Neu3D数据集上，该方法实现了比SOTA方法界限更清晰、高效且分辨准确的物体高斯点云分割效果。

Conclusion: 所提方法能有效提升动态4D高斯场分割准确性，尤其在复杂运动和遮挡场景下，对比现有方法更高效、边界更准确并保留结构完整性。

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [164] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 提出了一种融合几何感知的新型2D视频对象追踪方法GOT-Edit，显著提升了遮挡、干扰等复杂场景下的追踪鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的通用对象追踪方法大多只依赖目标及其周围的2D特征，忽视了3D几何信息，导致在部分遮挡、干扰物多或外观、几何变化显著的场景下表现不佳。而人类在追踪时天然会利用隐含的三维知识和语义推理，因此，作者希望通过引入几何线索，提升2D视频流中的对象追踪能力。

Method: 提出了一种叫做GOT-Edit的在线跨模态模型编辑方法，在2D视频追踪中注入几何感知能力。具体做法包括利用预训练的Visual Geometry Grounded Transformer从少量2D图像中提取几何相关特征，并通过一种null-space约束的在线模型编辑方式，将几何信息融合到追踪模型中，同时保留对语义的判别能力。该方法实现了几何信息和语义信息的有效协同。

Result: 在多个通用对象追踪基准测试（GOT benchmarks）上，GOT-Edit表现出更强的鲁棒性和精度，尤其是在遮挡和拥挤环境下，相较于传统方法取得了显著提升。

Conclusion: GOT-Edit首次在通用对象追踪中有效结合了2D语义与3D几何推理，确立了一种新的追踪范式，对遮挡和复杂场景具有更好的适应能力。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [165] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

TL;DR: FLAG-4D是一个新框架，通过重建3D高斯基元的时空变化，实现动态场景的高质量新视角生成，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有依赖MLP的动态场景新视角生成方法难以从稀疏视角捕捉复杂运动和细节，且时序一致性不足，因此亟需新的结构来提升建模效果。

Method: 提出FLAG-4D框架，利用双变形网络对3D高斯基元进行时空动态重建。该双网络包括本地精细动态建模的瞬时变形网络（IDN）和全局运动建模的全局运动网络（GMN），二者互相学习提升表达能力。此外，引入预训练光流骨干网络，利用时序帧间的稠密运动特征，通过变形引导注意力机制对齐3D高斯状态与流信息。

Result: 通过大量实验，FLAG-4D在重建精度、时序一致性和细节还原方面均超过了主流先进方法。

Conclusion: FLAG-4D实现了动态场景更高保真度和时序一致性的重建，显著优化了细节表现，是动态新视角生成领域的有效进展。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [166] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: 本文提出了一种基于Diffusion Transformer的照片真实感色彩修饰新方法SemiNFT，能够智能且美学地将参考图片的色彩迁移到源图片，优于现有方法，并能胜任零样本任务。


<details>
  <summary>Details</summary>
Motivation: 现有色彩修饰方法大多只基于像素统计，难以理解图片的语义和美学，特别对于非专家操作难度大，因此亟需兼顾结构理解与艺术审美的自动化解决方案。

Method: 提出SemiNFT 框架，先用配对三元组进行有监督训练获取基础技能，随后用强化学习在无配对数据上提升美学泛化能力，并设计混合奖励机制防止遗忘结构修饰能力。

Result: 在标准色彩迁移基准测试上，SemiNFT超越了现有最优方法，并能在黑白照片上色、二次元转照片等零样本任务下表现出强大智能。

Conclusion: SemiNFT不仅突破了传统方法的简单像素统计匹配，更展现了复杂的美学理解与创造力，有望推进图像智能修饰领域的发展。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [167] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文综述了中国音视频标准工作组（AVS）推出的第一代点云压缩标准（AVS PCC），重点介绍其采用的新编码工具、主要技术特点，以及与其他国际主流点云压缩标准（如MPEG G-PCC、V-PCC）的性能对比。


<details>
  <summary>Details</summary>
Motivation: 点云数据虽广泛应用于沉浸式媒体、自动驾驶等领域，但其庞大的数据量带来了传输与存储难题，制约了实际部署，因此高效的点云压缩成为亟需解决的关键问题。

Method: 本文着重回顾了AVS PCC标准，从技术层面对其编码工具与核心技术进行归纳，并与国际主流点云压缩标准在性能方面进行了对比分析。

Result: AVS PCC采用了区别于MPEG标准的新型编码工具，实验和标准评估显示其在压缩效率等维度取得了相较国际竞争标准的可观性能表现。

Conclusion: AVS PCC作为中国自主研发的点云压缩标准，不仅丰富了全球点云压缩标准体系，其创新的技术手段为点云数据高效存储与传输提供了新方案，在应用推广方面具有重要意义。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [168] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: 本论文提出了Inspiration Seeds框架，通过输入两张图像，生成多样且视觉一致的新图像，辅助创意探索，无需文字提示。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖精细的文本提示进行图像合成，对于灵感启发和前期视觉探索帮助有限；而设计师更倾向于依靠模糊的视觉灵感组合寻找新创意。

Method: 方法以两张图片为输入，基于CLIP稀疏自编码器提取CLIP潜空间的编辑方向和概念对，从而生成多样化、融合输入图像特征的合成图像。训练完全基于视觉方式构建的合成三元组，不依赖文本信息，模型为前馈结构，合成速度快。

Result: 实验显示所提出方法能有效生成揭示输入图片间潜在关系的高质量、多样化视觉合成，显著提升创意早期的视觉探索体验。

Conclusion: Inspiration Seeds无需文本提示即可快速生成创意性视觉组合，有力支持创作者在创作初期通过视觉探索激发新思路。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [169] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出LV-RAE方法，有效结合高层语义特征与低层细节特征，从而提升潜变量扩散模型在图像重建和生成质量上的表现，并针对高维潜空间敏感性引发的伪影提出抑制措施。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在编码端常用视觉基础模型抽取高层语义特征，虽然易于训练，但丢失了图像的低层信息（如颜色、纹理），导致重建精度不足，成为进一步提升LDM性能的主要瓶颈。

Method: 作者提出LV-RAE结构，通过表征自编码器将缺失的低层信息与语义特征结合，从而实现高保真重建。此外，分析了高维潜空间特征带来的解码器敏感性和伪影问题，并提出解码器鲁棒性微调及潜变量噪声平滑方法加以改进。

Result: 实验表明，LV-RAE能显著提升重建的清晰度和细节表现，同时保持语义表达能力，并且生成质量强。

Conclusion: LV-RAE兼顾高层语义抽象与低层细节，还通过增强解码器鲁棒性及控制潜变量生成过程，达到了高质量图像重建与生成，推动了扩散模型的进一步发展。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [170] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 本文提出了对Vision Transformer中class token与patch tokens进行专门化处理的方法，显著提升了密集预测任务的性能，并保持了分类准确率，几乎不增加算力开销。


<details>
  <summary>Details</summary>
Motivation: 虽然[CLS] class token和patch tokens在ViT中本质不同，但它们却被一视同仁地处理。作者认为这可能会妨碍模型充分学习全局与局部特征，希望通过深入分析其相互作用机制以优化表示效果，特别是对密集预测等下游任务。

Method: 分析归一化层对class和patch tokens的隐性区分机制后，作者提出在归一化层和前期QKV投影时采用分离处理路径（specialized processing paths），针对性地拆分class与patch token的计算流程。这样做以解耦全局与局部特征通路，提高特征表征能力。

Result: 在标准分割基准上，经过该specialization的ViT模型mIoU提升超2分，同时保持了原有的分类准确率，只增加了8%的参数量，计算量不变。消融实验也说明了哪些结构最能受益于这种处理方式，并证明了方法具有良好的泛化性。

Conclusion: 对class与patch token采用分离专门化处理可以带来显著的密集预测表现提升，且几乎无额外计算负担。该策略简单高效，为ViT结构设计与实际应用带来了新思路。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [171] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本论文提出了一个深度学习模型，利用低分辨率缩略图预测病理切片的固定类型，实现高效、大规模的质量控制。该方案无需高倍率全图像，有效提升了准确率和处理速度。


<details>
  <summary>Details</summary>
Motivation: 手工标注切片固定类型容易出错，影响诊断准确性，现有方法依赖高分辨率全切片图像，不利于高通量质控和快速流程。作者希望找到一种高效、自动化的新方法。

Method: 训练了一个深度学习模型，输入为低分辨率缩略图像，对切片固定类型（FFPE、FS）进行预测。模型在多个来源的病理切片数据集上进行训练和评估，并与当前主流方法比较。

Result: 在TCGA数据集上达到0.88的AUROC，优于其他预扫描方法4.8%；在其他数据集(Regensburg, Augsburg)上AUROC分别为0.72，但存在扫描仪领域偏移。单张切片处理速度达到21毫秒，比现有方法快约400倍。

Conclusion: 该方案可快速高效识别切片标签错误，适用于高通量病理质控流程。未来将优化模型泛化能力和适应更多扫描仪类型，预示可扩展至其他低分辨率注释问题。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [172] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: 本研究提出WiFlow，一个基于WiFi信号的连续人体姿态估计系统，通过时空特征提取及结构化注意力机制，显著提升精度与效率，适用于物联网场景。


<details>
  <summary>Details</summary>
Motivation: 当前WiFi姿态估计方法计算量大且难以应对连续动作，限制了其在智能感知和物联网领域的应用。为解决这些问题，本文提出新架构以提升效率与性能。

Method: 采用编码器-解码器结构。编码器利用时序卷积和非对称卷积捕获CSI信号的时空特征，并借助轴向注意力增强关键点结构建模。解码器将高维特征映射为人体关键点坐标。模型在自采3.6万组CSI-动作同步数据集上训练，并与传统2D残差网络方法对比。

Result: WiFlow在PCK@20上达97.00%，PCK@50达99.48%，平均关节定位误差仅为0.008米，且参数量4.82M，显著优于同类方法。

Conclusion: WiFlow实现了高精准、低复杂度的人体姿态估计，在实际物联网环境中具有很强的应用前景，为WiFi姿态估计树立了新基线。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [173] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 该论文探索了将机器学习方法用于具有限制条件的PDE（偏微分方程）求解，提出并测试了利用数据驱动离散化来提升低分辨率模拟准确性和稳定性的可行性。论文实现了经典的浅水方程和欧拉方程求解器，并提出了四种基于深度神经网络的ML方案，结果显示两种方法取得了较好效果。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习已在图像和自然语言处理等领域取得显著成功，但如何让其适用于有严格数学约束（如PDE求解）的领域仍是难点。提升传统PDE数值求解方法的精度和效率是主要动力。

Method: 论文首先基于不同框架实现了浅水方程和欧拉方程的经典数值解法作为对比基线。然后提出四种采用深度神经网络结构的数据驱动离散化方法，用于预测计算网格上的数值通量或系数，并对这些方法进行实验评估。

Result: 实验结果表明，采用数据驱动离散化的ML方法在低分辨率模拟时，准确性和稳定性均超过了传统有限差分或有限体积方法。与Pyclaw数值解算器相比，文中实现的经典解法效果更佳。在四种深度神经网络模型中，有两种产生了令人满意的PDE解。

Conclusion: 机器学习驱动的数据离散化在结构化网格下为PDE求解带来了更优的精度与稳定性，有望与传统数值方法互补。深度神经网络在特定问题上能提供高质量解，但模型选择和训练细节对结果有重要影响。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [174] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE是一款基于预训练文本生成视频（T2V）模型开发的音视频生成系统，支持生成高同步质量的视频和音频，性能优于其他开源和部分商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成领域正从单一的视频生成向音视频统一生成发展，市场和研究需求推动开发能够实现高音视频同步和动画能力的模型。现有T2V模型局限于仅能生成视频，缺少音视频统一表达，因此需要对其进行扩展。

Method: 本文提出ALIVE模型，在主流MMDiT架构基础上，增加联合音视频分支。引入TA-CrossAttn模块实现时序对齐的跨模态融合，采用UniTemp-RoPE实现精确的音视频对齐。同时设计了一套严谨的数据流程，包括音视频字幕生成、质量控制等，用于高质量数据微调，并构建了新基准测试全面评测模型能力。

Result: ALIVE模型经过大规模高质量数据的预训练与微调，在音视频生成任务上表现突出，无论与开源模型还是商业化模型对比，均达到或超越主流水平。

Conclusion: ALIVE为音视频生成提供了更高效、更一致的统一音视频生成能力，以及实际可用的公开基准，推动了音视频生成领域的发展。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [175] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 该论文提出视觉人工智能的本质在于信息压缩，并以此为出发点，设计了只关注高信息熵区域的视觉编码器OneVision-Encoder。实验显示，它以更少消耗获得了更好的表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉模型在处理图片和视频时，对所有像素采用均匀计算，未利用视觉信号中信息分布稀疏的特点，导致计算资源浪费。作者希望通过对齐模型结构与视觉信息理论原则，实现高效、精确的视觉理解。

Method: 提出了OneVision-Encoder，采用Codec Patchification 仅处理3.1%-25%高信息熵区域，无需均匀遍历所有像素。用统一的3D RoPE进行时空建模，基于百万级语义概念和大规模集群判别目标训练，实现空间-时间推理统一。

Result: OV-Encoder在16个多模态基准测试（图像、视频、文档）中，以更少的视觉token和预训练数据，整体优于强大的主流视觉模型（如Qwen3-ViT，SigLIP2），在视频理解任务上平均提升4.1%。

Conclusion: 对齐编解码器原理，从patch稀疏性出发，是视觉通用智能可扩展的关键。OneVision-Encoder证明了高效和精度可以兼得，为下一代视觉通用模型提供了有力方向。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [176] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频分解策略，结合视角无关和有观点的成分以提升低光照视频增强（LLVE）的表现，还引入了新结构和残差项，提升模型对复杂场景的适应能力和一致性。


<details>
  <summary>Details</summary>
Motivation: 低光照视频通常存在可见性差和噪声问题，现有方法对动态场景与复杂退化情况处理能力有限。作者希望通过改进视频分解和增强模型，提升对各种复杂、动态场景下视频的还原质量和一致性。

Method: 作者提出了VLLVE框架，将视频分解为视角无关（捕捉内在属性）和视角相关（描述光照条件）两部分，并设计了带有帧间交互机制的双结构增强网络，实现帧间分解特征的一致性。在此基础上，通过加入加性残差项，形成更完善的VLLVE++，增强对场景自适应退化的建模能力，并实现双向学习以提升特征相关性和纠正错误匹配。

Result: VLLVE++在包括真实动态场景在内的多项低光照视频增强基准测试中取得了优异表现，尤其能有效处理高动态和实际复杂场景。

Conclusion: 所提出的VLLVE++方法在低光照视频增强领域展现出极强的适应性和泛化能力，为复杂、真实场景下视频质量提升提供了新的解决思路。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [177] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新的音视频密集描述任务（Omni Dense Captioning），可生成带有明确时间戳的细粒度、结构化叙述，并设立相应的数据集与基准，提出新评价指标和强力基线模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音视频内容描述方法往往缺乏时间结构和细节，难以生成连贯、场景推进式的“剧本式”描述，因此需要更精细、更具结构化和时序信息的描述方法。

Method: 1. 提出了六维结构化标签体系实现脚本式密集描述任务；
2. 构建了高质量人工标注基准OmniDCBench；
3. 提出SodaM新评价指标，兼顾内容细节与场景边界模糊性；
4. 制作TimeChatCap-42K训练集，推出基于SFT与GRPO强化学习的TimeChat-Captioner-7B基线模型。

Result: 实验表明，TimeChat-Captioner-7B在OmniDCBench上的表现超过Gemini-2.5-Pro等先进模型；其密集描述能力还能大幅提升音视频推理（DailyOmni、WorldSense）及时间定位（Charades-STA）等下游任务表现。

Conclusion: Omni Dense Captioning任务及其配套基准、评价体系、模型可促进音视频理解发展，相关成果全面领先并开源，有望推动领域技术进步。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [178] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 本文通过分阶段模型对比分析，首次揭示了视觉—语言模型中语言模型如何通过多模态微调获得“看见”的能力，以及空间感知特征的出现与机制。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉—语言模型（VLMs）在多项任务上表现优异，但目前尚不清楚在多模态训练过程中，语言模型的表征如何适应，以及视觉能力何时、如何出现。因此，理解VLM的适应机制对于后续模型优化与理解十分重要。

Method: 作者提出了‘分阶段模型对比’（stage-wise model diffing）的方法，逐步追踪和区分在多模态微调期间语言模型表征的变化。通过检测微调过程中出现或重新定向的偏好视觉信息的特征，并用空间指令测试其空间关系编码能力，最后结合注意力头分析特征激活的因果关系。

Result: 研究发现：（1）一些新的或重新定向的视觉偏好特征在微调过程中产生；（2）部分关键特征能稳定地编码空间关系，并能被空间类提示激活；（3）空间特征的激活主要归因于极少数特定注意力头。

Conclusion: 分阶段模型对比法能有效揭示多模态模型空间性特征的源头和形成过程，提升了VLM训练过程的可解释性，为理解和优化多模态模型的视觉能力提供了理论基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [179] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本研究提出并评估了三种无需训练的自动人体解剖区域识别方案，实现了对CT和MRI体积图像的零样本检测，且性能优异，减少了对DICOM元数据和人工注释的依赖。


<details>
  <summary>Details</summary>
Motivation: 自动识别医学影像中的解剖区域对于后续分析和诊断非常重要，但当前方法严重依赖不可靠的DICOM元数据，并且多为有监督学习，实际应用受限。

Method: 作者提出并系统评估了三种训练自由的方法：（1）利用预训练多器官分割模型的规则驱动系统；（2）由放射科规则指导的多模态大语言模型（MLLM）；（3）结合视觉输入和分割信息的分割感知MLLM。三个方法均在887例人工标注的CT和MR影像上进行了测试。

Result: 分割驱动的规则系统表现最优且稳定，CT加权F1分数为0.947，MR为0.914，对多种成像和特殊扫描范围均有很强的鲁棒性。MLLM在视觉特征明显的区域表现良好，而分割感知型MLLM揭示了基本局限。

Conclusion: 训练自由分割驱动方法在不同成像模式下均有优异表现，为不依赖注释和元数据的自动解剖区域识别提供了有力方案，推动实际应用落地。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [180] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

TL;DR: 本文提出了一种名为RotLight的逆向渲染方法，通过在捕捉过程中多次旋转物体，配合代理网格和残差约束，有效提升了材质属性（如高质量反照率）的估算精度，并减少伪影。该方法在合成和真实数据集上均表现出优越的估算效果和高效的计算性能。


<details>
  <summary>Details</summary>
Motivation: 当前逆向渲染方法在估算物体反照率时，容易出现颜色不准和阴影烘焙等问题，主要原因在于材质和光照分离高度不适定，造成较大歧义。因此，研究如何简化采集设置、降低歧义并提升反照率估算精度显得尤为重要。

Method: 提出了RotLight采集设置，只需在捕捉过程中多次旋转物体（仅需两次旋转即可见效），即可显著降低光照与材质分离的歧义。同时，在2D高斯分布基础上引入代理网格，用于准确追踪入射光线，并利用残差约束增强全局光照效果。

Result: 在合成和真实场景数据集上，RotLight方法在反照率估算等多个逆向渲染指标上取得了优于现有方法的表现，并保持了计算的高效性。

Conclusion: RotLight通过简单的多次旋转采集和新颖的代理网格设计，有效缓解了传统方法在反照率估算上的关键难题，为逆向渲染在实际应用中提供了更高效和精确的解决方案。

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [181] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: 本文提出了FusionEdit，无需训练即可实现精准、可控的文本引导图像编辑，有效缓解编辑区域边界伪影问题，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于显式二值掩膜的文本图像编辑方法容易带来硬边界伪影并降低可编辑性，急需实现自然平滑且精准的局部编辑策略。

Method: 1）通过比较源与目标文本，自动识别需编辑和保留的图像区域；2）在区域边界采用距离感知的隐空间融合，生成软掩膜，并利用全变差损失获得平滑自然的过渡效果；3）结合DiT模型中AdaIN调制机制，在编辑区内进行统计注意力融合，提升编辑精度和整体一致性。

Result: FusionEdit在多个基准数据集和主流评测指标上均优于现有方法，编辑效果更加自然，边界过渡平滑，且全局一致性较好。

Conclusion: FusionEdit无需训练即可对图像进行精细、可控的文本驱动编辑，是克服硬掩膜边界伪影和提升编辑灵活性的有效新方法。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [182] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

TL;DR: 本研究提出了基于事件相机的眼动（主要为扫视和凝视）分类方法，利用合成数据集和脉冲神经网络（SNN）实现了高准确度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的摄像头存在运动模糊，难以高效、准确地捕捉人眼快速动态，限制了对认知与感知机制的研究。事件相机具备高时间分辨率和无运动模糊等优势，结合合成数据有望提升眼动研究效果。

Method: 使用Blender模拟合成真实感的扫视和凝视数据作为训练集，利用脉冲神经网络（SNN）训练及评估两种结构，并在真实事件数据上微调模型。

Result: 该方法的模型准确率最高达到0.83，并且在不同时间分辨率下依然表现稳定。与传统人工神经网络（ANN）相比，基于SNN的系统在计算效率上有显著提升。

Conclusion: 利用合成数据和SNN提高了基于事件流的眼动分类精度和效率，展现了合成数据扩增和神经拟态计算在事件视觉领域的巨大应用潜力。

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [183] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 本文提出了一种高效的混合深度学习框架，将2D和3D网络结合，用于减少稀疏采样CT图像中的伪影，提升成像质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏采样CT能够减少扫描时间和辐射剂量，但带来了伪影，影响成像质量和诊断价值。如何去除这些伪影，提升影像质量，是该领域的关键问题。

Method: 采用两阶段混合模型：首先用2D U-Net对每一层进行特征提取，然后将所有层的特征图叠加，输入到3D解码器中，利用层间的上下文信息恢复无伪影的3D CT图像。这样既兼顾了2D计算的高效性，也保证了3D体积的一致性。

Result: 该方法在相关方向上显著提升了层间一致性，同时计算资源占用较低，相较于只用2D或3D模型有明显优势。

Conclusion: 提出的混合深度学习框架为高质量3D CT后处理提供了高效且稳健的解决方案，兼顾成像质量与计算效率，具有实际应用前景。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [184] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CLIP-Guided Alignment（CGA），能够在不访问源数据的情况下，提升细粒度目标域迁移学习的表现，特别针对类别混淆问题进行了建模与缓解。


<details>
  <summary>Details</summary>
Motivation: 在无法获取源数据、只有目标域数据的情况下进行迁移学习（Source-Free Domain Adaptation, SFDA）越来越重要，尤其关注数据安全领域。现有方法在细粒度分类任务中面临类别间相似性高导致的模型预测混淆、伪标签噪声大等问题，影响适应效果。

Method: CGA包含三部分：（1）MCA模块通过分析源模型在目标域的预测，检测类别间的方向性混淆对；（2）MCC模块借助CLIP生成带有混淆上下文的文本提示，提高伪标签的语境敏感性；（3）FAM模块结合CLIP和源模型，建立混淆引导特征库，并通过对比学习进行特征对齐，减少表示空间中的模糊。

Result: 在多个数据集上的实验表明，CGA显著优于当前主流的SFDA方法，特别是在混淆多和细粒度分类任务中表现突出。

Conclusion: 显式建模和缓解类别间混淆对无源自适应任务极为重要。CGA为SFDA领域提供了有效的新思路，并在实际表现上有明显提升。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [185] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大语言模型训练框架HATCH，通过引入更接近人类认知机制的空间对齐和显式视角变换，提高模型多图像空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽在单图像空间推理上取得进展，但在多图像、多视角整合信息推理上表现不足。认知研究显示，人类依靠跨视角对应和逐步视角变换机制解决类似任务，而多数模型对此机制利用不充分。

Method: 提出HATCH训练框架，包括（1）Patch-Level Spatial Alignment，促使模型学习跨视角空间对应区域的表征对齐；（2）Action-then-Answer Reasoning，要求模型在最终回答前，先显式推理视角转换过程。

Result: 在三项基准任务上，HATCH训练的模型对比同尺寸基线有明显性能提升，对比更大规模模型也取得有竞争力的结果，同时保持了单图像推理能力。

Conclusion: HATCH框架能有效提升多模态模型的多图像空间推理能力，启发未来研究进一步结合人类认知机制改进模型空间推理表现。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [186] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 本文提出了一种新的注意力机制，提升了基于流的图像编辑模型在多实例编辑场景下的表现，实现了更精细、独立的实例级别编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流（flow-based）的图像编辑模型主要支持单条指令或全局编辑，在面对需要对参考输入的多个部分分别独立编辑的多实例场景时效果有限。本研究动机在于解决实例间语义相互干扰的问题，实现多实例下的精细编辑。

Method: 本文提出了实例解耦注意力（Instance-Disentangled Attention）机制。该方法将传统的联合注意力操作进行分割，使编辑过程中每个实例的文本指令与对应的空间区域强绑定，从而在速度场估计过程中实现实例间编辑的独立性。

Result: 作者在自然图像编辑任务和他们自建的、包含丰富文本说明的图像信息图编辑基准上进行了实验。结果表明，所提出方法有效提升了多实例编辑任务中的编辑解耦和局部性，同时仍能保证整体输出的一致性。

Conclusion: 本文方法实现了单次推理的一步多实例编辑，不仅解决了现有方法在多实例场景下的编辑干扰问题，还为实际应用提供了更高效、精细的编辑手段。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [187] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: 该论文提出了MVAnimate框架，通过结合多视角信息提升动画生成质量，同时对比当前方法展现了更高的空间一致性和时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前2D或3D建模的人体姿态动画生成方法存在输出质量低、训练数据不足的问题，难以生成高质量的动画视频。为应对这些挑战，需要引入新的框架提升输出视频的真实性和多样性。

Method: MVAnimate框架利用多视角先验信息，将2D与3D人体动态数据综合，生成时空一致的高质量动画。该方法还能进一步优化目标角色的多视角视频，实现不同视角下的一致性提升。

Result: 在多个不同的数据集上进行了实验，结果显示MVAnimate在处理不同运动模式与外观方面表现更为鲁棒，并提高了生成动画视频的整体质量。

Conclusion: MVAnimate有效提升了动画生成方法的视频质量和多样性，为高质量角色动画的生成提供了坚实的技术基础。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [188] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 本文提出了一种无需高性能GPU、可在普通CPU离线运行的轻量级拟人说话头像生成方法，适用于资源受限的教育场景。


<details>
  <summary>Details</summary>
Motivation: 当前许多说话头像生成技术依赖大规模数据、高算力GPU及复杂建模，在教育技术实际应用中，尤其是离线及低资源环境下难以部署，亟需低成本高效解决方案。

Method: 提出“符号吠陀计算”框架：先将语音转为时间对齐的音素流，再映射到紧凑的可视音素集合，通过受吠陀经启发的符号化协同发音生成平滑可视音素轨迹。结合轻量级2D渲染器，仅在关注区域进行扭曲与口型合成，并用抖动稳定技术支持普通CPU上的实时合成。

Result: 在CPU上测试表现出良好的口型同步、时间稳定性和身份一致性，并与相关CPU可行基线方法进行对比。

Conclusion: 该方法在低端硬件上显著降低计算负载和延迟，同时实现可接受的口型同步质量，为教育领域的虚拟说话人头像提供了实用解决方案。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [189] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 本文提出了一种融合高分辨率图像和力测量数据的多模态方法，以更为精准和稳健地检测电气化铁路受电弓-接触网界面的电弧。新算法在多种真实和合成数据集上表现优异，显著提升检测灵敏度，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网界面上的电弧现象会造成设备磨损、系统性能下降甚至铁路运营中断。目前受限于电弧事件瞬态性、环境噪声等原因，检测电弧事件存在挑战，亟需更准确、鲁棒的检测方法。

Method: 作者创建了两个同步视觉与力数据集（一个真实、一个合成），开发了多模态扩展算法MultiDeepSAD，并针对图像和力信号分别设计了伪异常数据生成方法，用于增强训练。

Result: 通过大量实验和消融分析，提出的方法在各种数据分布、领域转移及样本稀缺情况下，均实现了对电弧检测的高灵敏度和精确率，优于现有基线方法。

Conclusion: 所提多模态检测框架能更好识别铁路受电弓-接触网电弧事件，提升系统安全可靠性，有望实际应用于铁路供电系统监控。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [190] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源高质量音视频联合生成模型，首次支持图文到音视频的同步生成，并且公开了模型参数和代码。


<details>
  <summary>Details</summary>
Motivation: 现有音视频生成方法多依赖级联式流程，成本高、误差积累、整体质量下降；而典型系统又为闭源，限制了领域发展。

Method: MOVA采用了32B参数的Mixture-of-Experts（MoE）架构，推理时使用18B参数，支持IT2VA（图文到音视频）生成任务。模型及代码全面开源，并支持高效推理、LoRA微调和多种提示增强。

Result: MOVA能生成高质量、同步的音视频内容，包括拟真唇动语音、环境感知音效、与内容紧密结合的音乐等，效果优于级联系统。

Conclusion: MOVA开源推动了音视频联合生成领域发展，为学术与创作社区提供了重要资源和研究平台。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [191] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 本论文提出一种结合不确定性感知伪标签教师和基于置信度渐进学习的学生的半监督分割方法，在脑肿瘤MRI分割任务中，在仅有少量标注数据情况下取得了优异的表现。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI分割任务标注成本高昂，且不同扫描仪和医学中心带来数据异质性，因此亟需能利用未标注数据、鲁棒对抗伪标签噪声的分割方法。

Method: 提出教师-学生框架：教师网络为每像素生成概率分割与不确定性，并按图像等级别置信度分阶段引入未标注数据；学生网络采用双重损失，从高置信度区域学习、低置信度区域反向学习。引入基于一致性的伪标签细化增强。

Result: 在BraTS 2021数据集上，验证集DSC从使用10%数据的0.393提升到全数据的0.872，且前期提升最大；教师模型DSC达0.922，学生模型在肿瘤亚区分割上超越教师，尤其在增强区分割学生恢复了教师失效的表现。

Conclusion: 置信度驱动的课程学习以及有选择的反向学习提升了伪标签质量和分割鲁棒性，可在有限监督和伪标签噪声下实现高效准确的脑肿瘤分割。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [192] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2通过结合多模态大语言模型与视频扩散模型，实现了高效且可扩展的视频生成与编辑，表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成和编辑任务面临理解复杂用户指令和保证生成质量的挑战。作者希望利用多模态大语言模型强大的理解与推理能力，提升视频生成与编辑的复杂度和精度，并且实现模型的高效扩展与参数利用。

Method: 该方法将预训练多模态大语言模型（MLLMs）与视频扩散模型结合，利用MLLMs生成对用户指令的显式目标描述，直接指导生成过程，同时研发轻量化适配器将多模态条件信息注入文本到视频扩散模型，实现高参数效率和优质生成。模型扩展至14B参数，使用高质量数据训练，支持多种高难度视频编辑任务。

Result: 在FiVE细粒度视频编辑基准和VBench文本到视频生成基准上进行评测，Omni-Video 2在视频编辑的复杂指令遵循性和视频生成质量上表现出优异或领先水平。

Conclusion: Omni-Video 2有效提升了视频生成与编辑时对复杂合成性指令的理解与执行能力，为多模态视频生成和编辑提供了新的高效方案。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [193] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 论文提出了一种能够实现任意模态间MRI合成的基础模型，并在鼻咽癌放疗相关场景下表现出高度鲁棒和优质的性能。


<details>
  <summary>Details</summary>
Motivation: 鼻咽癌放疗对多模态MRI依赖性强，但现实中因扫描时间长、患者不适和高昂费用等问题，常常存在MRI模态不全，影响放疗计划的精度。现有合成方法局限于特定模态变换，难以适应复杂临床需求，且可解释性不足。亟需一种兼具泛化性、适用性和临床解释性的MRI合成方案。

Method: 作者开发了一个统一基础模型，将对比视觉表征学习与视觉-语言对齐（VLA）结合。通过对比编码器获得模态无关的表征，并采用基于CLIP的文本提示解码实现语义一致的合成，从而支持任意模态到任意模态的MRI合成。模型在13个机构共40,825张图像上训练，并在26个内外部验证站点（共15,748张图像）评估。

Result: 模型在所有验证站点表现出一致的高性能（平均结构相似性SSIM 0.90，峰值信噪比PSNR 27），对噪声和域偏移具备更强鲁棒性，同时合成结果更优于现有方法。其统一表征在下游如分割等放疗相关任务上亦有提升。

Conclusion: 该基础模型有效提升了鼻咽癌数字医疗方案，推动了MRI合成算法的技术与临床实用性结合。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [194] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 提出了一个新的视频生成真伪检测框架VideoVeritas，通过融合细粒度感知能力与基于事实的推理实现对生成视频的检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的发展，伪造视频数量和质量激增，传统检测方法难以应对，亟需更智能、更精准的检测方法。现有多模态大模型在推理上强但细节感知能力不足。

Method: 提出VideoVeritas框架，将细粒度感知和基于事实的推理结合。引入了联合偏好对齐和感知前置增强（PPRL），在强化学习阶段引入一般时空定位和自监督目标计数任务，提升感知能力。构建新数据集MintVid，覆盖主流生成模型和真实世界含事实错误样本。

Result: 实验表明，现有方法通常只侧重于粗粒度推理或机械分析，难以均衡两者，VideoVeritas在多个基准上表现出更好的均衡与检测效果。

Conclusion: 通过将细致的感知任务与推理任务结合，VideoVeritas能够更全面地识别生成视频，提升了伪造视频检测技术水平，为视频内容安全提供了更加可靠的技术支持。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [195] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的模型压缩方法FlattenGPT，通过将临近的transformer block合并，既减少网络深度，又有效检测移除冗余参数，从而提升了模型推理效率，并保持较高性能。


<details>
  <summary>Details</summary>
Motivation: 传统transformer模型存在层（block）间冗余，直接剪枝整个block容易损失有用信息，导致性能下降。通道剪枝虽然能保留更多性能，但不能减少模型深度且剪枝比例难以统一。本研究希望在压缩模型、加速推理的同时尽可能保留原有知识和结构。

Method: 提出FlattenGPT方法，将两个相邻transformer block‘变平’合并为一个，检测并删除冗余参数。此方式既压缩了深度，又比整块剪枝更好地保留信息，同时保持原transformer结构的一致性。

Result: FlattenGPT在LLaMA-2/3和Qwen-1.5等模型上实现了20%的压缩率，并保留了90-96%的零样本性能。实验证明其在不同模型和参数规模下的零样本准确率和WikiText-2困惑度均优于现有剪枝方法，推理加速效果显著。

Conclusion: FlattenGPT在提升transformer推理效率方面表现突出，在保证较小性能损失下实现了优于其他剪枝方法的模型压缩效果，具备广阔应用前景。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [196] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出了TiFRe框架，通过结合文本引导的视频帧采样和非关键帧信息融合，实现视频多模态大模型在减少计算成本的同时保持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频多模态大模型在处理大量视频帧时面临严重的计算和注意力开销，直接减少输入帧数量虽可降低成本，但常导致信息损失和性能下降。作者希望能在保证关键信息的前提下高效减少计算量。

Method: 提出Text-guided Video Frame Reduction (TiFRe)框架，其中包括两个核心方法：（1）Text-guided Frame Sampling（TFS）：以用户输入文本为引导，通过大语言模型生成CLIP风格的prompt，并用CLIP编码器计算每帧与prompt的语义相似度，挑选相关帧为关键帧；（2）Frame Matching and Merging（FMM）：通过匹配和融合，将非关键帧的信息合并到关键帧中，以尽可能减少信息损失。

Result: 实验结果显示，TiFRe框架在有效降低计算成本的同时，能够提升视频-语言相关任务的性能，优于仅用固定帧率采样的传统方法。

Conclusion: TiFRe能够通过文本引导和信息融合策略，在不损失甚至提升性能的前提下大幅降低视频多模态大模型的运算量，具有很强的实际应用价值。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [197] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 本论文研究了通过多视角优化得到的3D Gaussian Splatting(3DGS)解决方案中的内在结构，发现了混合结构和辐射度双峰等稳定模式。还提出密度感知训练方案和体系结构改进建议。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS方法在三维场景重建和渲染中效果良好，但其内在结构与参数分布机理研究不充分。本工作旨在挖掘3DGS模型经过多视角优化后，其参数会出现哪些统计特性，并探讨参数可预测性的成因。

Method: 提出“最优渲染参考（ROR）”概念，对经过标准多视角优化的3DGS参数进行统计分析。通过为ROR设计无渲染监督的预测器，并利用方差分解等方法，系统分析了稠密/稀疏区域间的密度分层与参数相关性。

Result: 发现3DGS参数自发展现出混合结构尺度和辐射度双峰现象。稠密区域的参数与几何强关联，可通过点云直接预测；稀疏区域预测失败且存在几何与表观参数耦合，需依赖多视角渲染约束。还提出了密度感知训练策略提升鲁棒性。

Conclusion: 3DGS内参数存在‘点云可预测的几何原语’与‘依赖多视角的渲染原语’双重属性，应针对不同稠密度区域采用自适应的前馈预测和渲染优化结合的体系结构设计。本文结论为3D重建与新视角合成方法的参数建模与系统架构提供了理论依据。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [198] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出一种新的3D高斯流场表示方法，用于建模植物生长过程中的时变三维外观，并在多视角时序数据集上获取了比现有方法更高的图像质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景建模方法难以处理植物生长过程中新几何结构的不断生成，而传统变形或高斯撒点方法不能很好地模拟植物生长的非线性和连续变化。

Method: 提出了一种3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色和透明度）的时变导数。通过重建成体植物并反向学习其生长过程，初始化足够的高斯基元，模拟植物的发育历史。

Result: 在多视角时序植株生长数据集上，所提方法在图像质量和几何准确性上均优于现有方法。

Conclusion: 新的3D高斯流场方法有效提升了对生长中三维结构的外观建模能力，为植物等生长场景的研究与应用提供了新的思路。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [199] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter 是一个基于视频扩散的系统，可以仅通过单摄像头视频同时重建4D几何体和估算稠密运动。通过创新性的联合表示和新颖的4D变分自编码器（VAE），该方法在多个数据集上取得了领先的几何重建和运动估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单视频中重建4D几何和估算稠密场景流任务中存在精度和效率的瓶颈，尤其是在联合建模和先验迁移方面。作者希望突破传统VAE对齐策略的局限，实现更高质量的重建和运动估计。

Method: 提出一种新的联合稠密3D点云和3D场景流的表示，并在此基础上设计新颖的4D VAE架构。同时创新性地采用数据归一化与改进的VAE训练方案，提高扩散模型先验的迁移能力。不同于传统方法，该方法不强制对齐3D与RGB潜变量，而是优化潜变量分布以获得更优表现。

Result: 在多个数据集上的实验表明，MotionCrafter 在几何重建和场景流重建任务上分别取得了38.64%和25.0%的提升，而且无需后处理优化，达到了当前最优水平。

Conclusion: 新方法打破了传统VAE潜变量对齐的限制，联合建模了4D几何和运动，实现了单目视频条件下的重建与估计的重大提升。MotionCrafter在精度和效率上均显著优于现有方法。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [200] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

TL;DR: 该论文针对现有视频大模型在体育反馈生成任务上的局限性，提出利用自由获取的网络数据辅助训练，并设计了更贴合任务的新评测指标，提高了模型在特定体育项目下的反馈生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频-大语言模型（video-LLMs）生成体育运动反馈能力有限，难以泛化到未见过的运动种类，且获取高质量标注数据成本高昂。此外，现有文本生成评测指标无法准确评价体育反馈的有效性和实用性。作者希望借此提升模型表现，降低数据依赖，并完善评测标准。

Method: 以攀岩为案例，提出集成来自目标领域（如比赛视频、教练手册等）的公开网络辅助数据，结合已有的非同类运动反馈数据，共同用于训练或提升体育反馈生成能力。同时，提出“specificity（针对性）”与“actionability（可操作性）”两项更适合评价体育反馈生成的新指标。

Result: 该方法有效提升了模型在目标领域（如攀岩）的运动反馈生成能力，尤其是在高质量标注样本受限的情况下依然有较好的表现；新提出的两项评测指标可更全面、客观地反映模型生成反馈的实际价值。

Conclusion: 结合自由开放的目标领域数据和创新评测标准，不仅提升了体育类反馈生成模型的实用性，也降低了人工注释成本，为相关领域应用和后续研究提供了新思路。

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [201] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出ArcFlow，一种利用非线性流轨迹进行蒸馏的新方法，仅需少步骤即可高效逼近扩散模型推理过程，在提升采样速度的同时性能够保持高生成质量，无明显质量损失。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生成质量高，但由于需要多步去噪，推理成本高。为此，近年兴起通过知识蒸馏将原始多步推理压缩到少步推理，但主流方法采用线性近似，无法准确匹配动态变化的速度方向，导致退化。

Method: ArcFlow使用非线性流轨迹显式地拟合扩散模型教师轨迹。方法上，将速度场参数化为连续动量过程的混合，从而精准描述速度的演化，并能解析求积分以避免离散误差。蒸馏实现时，结合轻量适配器在大模型上微调不到5%参数，从而得到少步推理学生模型。

Result: 在Qwen-Image-20B和FLUX.1-dev等大模型上，ArcFlow仅微调少量参数即可达到2步采样获得40倍推理加速，同时生成质量与原始多步教师模型接近。实验结果定量和定性地验证了效果与泛化。

Conclusion: ArcFlow在显著提升扩散模型推理速度的同时，高效保持了生成多样性和质量，突破了现有直线近似蒸馏的局限，为扩散模型高效部署开辟了新道路。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [202] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本论文提出了一种称为Raster2Seq的方法，将像素化户型图转化为结构化矢量图，通过序列到序列方式预测房间、门窗等多边形的几何和语义结构，实现复杂户型结构的高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以精准还原结构复杂、房间较多、顶点数量多变的户型图的结构和语义，影响了下游如自动化分析和CAD流程等应用。因此亟需一种能更好处理复杂户型的矢量重建方法。

Method: 提出将户型图重建建模为序列到序列任务，用自回归解码器逐步预测多边形顶点，并引入可学习锚点来辅助关注重要图像区域，实现对复杂结构多边形的有效处理。

Result: 该方法在Structure3D、CubiCasa5K和Raster2Graph等主流基准数据集上取得了最先进的表现，并能很好泛化到诸如WAFFLE等包含复杂房间结构和多样几何变化的挑战性数据集。

Conclusion: Raster2Seq方法在复杂户型图的结构和语义重建方面效果突出，具备很强的灵活性和泛化能力，为下游相关应用提供了更优质的数据基础。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [203] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass是一种针对基于视频的世界模型提升探索能力和一致性的RL后训练框架，能够明显提升互动准确性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 传统基于视频的世界模型在探索和交互准确性上存在不足，尤其是在长时间和复杂场景下难以高效获得有用环境反馈。现有方法缺乏在交互信号引导下精准探索的框架。

Method: 提出WorldCompass框架，包含：1）片段级回滚策略，提高采样效率并带来细粒度奖励信号；2）互补奖励函数，同时考虑交互准确性和视觉质量，抑制奖励作弊；3）高效RL算法，采用负样本微调和多项效率优化手段。

Result: 在SoTA开源世界模型WorldPlay上的评测显示，WorldCompass在不同场景下均大幅提升了交互准确率与视觉保真度。

Conclusion: WorldCompass作为RL后训练框架，有效增强了基于视频世界模型的交互探索能力，是提升虚拟环境中智能体表现的有力方案。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [204] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的离散生成方法（BAR），有效提升了离散编码方法在视觉生成任务中的表现，可超越连续方法，并大幅节省采样成本。


<details>
  <summary>Details</summary>
Motivation: 目前视觉生成领域中，连续方法占主导地位，普遍认为离散tokenizer表现不佳。作者发现这种劣势主要源于潜在空间比特数（压缩比），而不是离散方法本身。

Method: 作者提出BAR（masked Bit AutoRegressive modeling），允许用自回归Transformer逐步预测离散token的各个位，实现超大规模的codebook，并支持任意大小。

Result: BAR在ImageNet-256上取得了新的最先进gFID 0.99，超越了当前领先的连续和离散生成方法，同时显著降低了采样成本，收敛比连续方法更快。

Conclusion: 只要合理扩展codebook，离散方法的生成能力可与连续方法媲美甚至超越，BAR为大规模离散生成提供了高效可扩展的实践方案。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [205] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 本文评估了在像素化语言建模（将文本转为图像）和引入文本分词器（如DualGPT）后，分词器对低资源语言建模的影响，发现引入分词器会重新引入分词错配问题，影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前像素级语言建模试图通过图像化文本绕过分词瓶颈，但新型多模态模型（如DualGPT）为了提升自回归能力又重新引入了文本分词器，作者质疑这种做法是否真的消除了分词问题。特别是在低资源、非拉丁文字地方语言上，这一问题更加突出。

Method: 作者聚焦于印尼四种低资源的本地非拉丁文字（爪哇语、巴厘语、巽他语、Lampung语），在DualGPT架构下比较了标准分词器（如Llama 2分词器）与定制分词器的脚本对齐（script-tokenizer alignment）影响，对二者的降词率（OOV）与多样性（fertility）等指标进行测评。

Result: 即便使用视觉渲染，重新引入分词器依然带来与传统像素语言建模初衷相同的分词错配问题。Llama 2分词器即使降词率和多样性低，但性能却远不如定制分词器，定制分词器最高可带来30.15 chrF++的提升。

Conclusion: 研究表明，在多模态语言模型中，文本分词器依然是实现公平与高效建模的主要障碍。未来类似模型设计中应注意分词错配问题，避免影响模型在低资源语言上的性能。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [206] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: 本文介绍了BiomechAgent，这是一种可通过自然语言进行生物力学分析、生成代码的AI系统，其旨在让临床医生无需编程即可对动作捕捉数据进行分析。作者系统性地评估了该工具在多类任务中的表现，并分析了模型设计选择对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 动作捕捉正日益广泛应用于运动分析，但数据分析环节由于需要编程技能，阻碍了许多临床用户的使用。因此，开发一种不依赖编程即可分析动作捕捉数据的工具，帮助更多用户从中受益，成为亟需解决的问题。

Method: 作者开发了BiomechAgent—— 通过自然语言接口，使用户可查询数据库、生成可视化结果并进行数据解释，无需书写代码。为评估该系统，引入了数据检索、可视化、活动分类、时间分割和临床推理等五项系统性基准测试。同时，通过对不同设计的比较（如通用指令和领域定制指令、不同模型权重等）研究其对表现的影响。

Result: BiomechAgent在数据检索、可视化等任务上表现准确，并显示出初步的临床推理能力。领域定制的指令、集成专业工具（如步态事件检测）显著提升了高难度分析的准确性。而采用本地开源模型时，仅数据库检索表现良好，其他任务表现均明显下降。

Conclusion: BiomechAgent极大提升了动作捕捉数据的可用性和普及性，使非编程用户也能进行高质量的生物力学数据分析，并为AI在临床运动分析的应用奠定了基础。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [207] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: 作者提出了一种名为ILA-agent的新框架，使大语言模型（LLM）能够通过与文档和环境的动态交互，在有限资源下学习全新编程语言，并显著优于传统检索增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要依赖于海量数据的预训练，但在遇到未见过的新编程语言时表现较差。直接对所有新语言都大量微调不切实际，因此需要一种低资源下学习新语言的解决方案。

Method: 提出Inference-time Language Acquisition（ILA）范式，并据此设计了ILA-agent框架。ILA-agent为LLM配备了一套类人的行为原语，能够模仿人类通过查阅官方文档和实际运行代码，逐步探索和掌握新语言知识。在评估方面，作者构建了针对新型静态类型语言Cangjie的多任务基准Cangjie-bench，并在此测试ILA-agent在代码生成、翻译和修复等任务中的表现。

Result: 实验结果表明，ILA-agent在多个LLM和多项任务中表现优异，明显优于基于检索的增强系统。同时，还对ILA-agent的行为轨迹进行了分析，发现了其涌现的行为模式以及依然存在的提升空间。

Conclusion: ILA-agent证明了通过动态与外部信息交互的范式，LLM能够在低资源条件下习得全新编程语言，优于传统方法，但也存在改进空间。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [208] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: 本文提出了一种名为Anchored Decoding的新方法，用于在推理阶段抑制大语言模型对训练数据的逐字复制，降低版权或敏感信息泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现代大模型常常会逐字复制训练数据中的内容，特别是那些有版权或敏感的信息时，会引发合规和权益问题。现有方法难以在保障信息质量的同时有效抑制复制行为，因此需要一种可控、灵活的方法来权衡风险与效用。

Method: 论文提出Anchored Decoding方法，让待测模型的输出时刻受限于一个安全模型的输出范围，通过信息预算和逐步约束来控制与安全模型的接近度，并提供可调整的风险与效用折中。还提出了字节级Anchored$_{\mathrm{Byte}}$ Decoding以支持跨词表内容融合。此外，作者提供了一个新训练的小型安全模型TinyComma 1.8B，并在多种基线与安全模型组合上进行了实证评估。

Result: 在著作权风险和效用的长文生成任务中，Anchored Decoding及其字节级变体可在保持高流畅性和真实性的前提下，消除高达75%的复制程度（基于六项复制性指标），且计算开销较小。

Conclusion: Anchored Decoding方法能显著减少大模型逐字复制带来的版权风险，兼具实际效用和可调节性，是提升大模型合规性的新探索方案。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [209] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的注意力机制Free Energy Mixer (FEM)，通过log-sum-exp的能量函数实现按通道的选择性读取，提升了注意力机制的表现，且在复杂度不变的情况下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制在存储key/value时是无损的，但在读取时只能做整体的加权平均，无法细致到按通道选择。这阻碍了模型表达能力的提升，因此需要一种新的机制来实现更灵活的读取方式。

Method: FEM通过引入自由能（log-sum-exp）读取，将基于传统query/key的分布视作先验，再结合value信息进行按通道的推断式读取。随着可学习参数（逆温度）的提升，该机制可以在平均与强选择之间平滑切换，保持并行性和原有复杂度。FEM可以直接与标准/线性注意力、线性RNN和SSM模型集成，无需参数增加。

Result: FEM在NLP、视觉及时间序列等多个任务中，在参数预算相等的前提下，稳定地优于现有强基线方法。

Conclusion: FEM能够以不增加复杂度的方式，实现更精细的按通道选择性注意力，为注意力机制带来性能提升，适应多种主流模型结构。

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [210] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

TL;DR: 本文发现大规模语言模型（LLM）内部已经包含了与不同人格相关的参数子网络，通过挖掘模型内部结构可在无需外部知识或训练的情况下实现高效且具有解释性的人格特征定制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖外部提示、检索增强生成或微调来适应不同的人格和行为，作者想探索：LLM是否必须依赖外部上下文才能实现人格切换，还是说这些能力已内嵌于模型参数中？

Method: 通过使用小规模校准数据集，定位与不同人格相关的显著激活特征，并采用掩码策略隔离出特定人格的轻量级参数子网络。在面对二元对立人格（如内向-外向）时，引入对比剪枝方法分离导致人格差异的关键参数，全流程无需新的训练，仅基于已存在的参数空间操作。

Result: 实验表明，基于掩码与对比剪枝获得的人格子网络，无需额外知识即能实现比以往基线更准确、更高效的人格特征对齐效果。

Conclusion: LLM的参数空间本身蕴含了人类行为和人格的多样性，这为模型的可控个性化和可解释性开启了新的视角，并挑战了必须依赖外部知识定制行为的传统假设。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [211] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: 本文提出了名为Open TutorAI的开源智能辅导平台，结合大模型、生成式技术和3D虚拟形象，提供个性化、自适应的教学体验，并内置学习分析与多方协作界面，提升了学习过程的人性化和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 目前的教育类聊天机器人缺乏情境适应性、实时响应性和教学灵活性，导致学习者参与度和教学成效不佳。因此亟需融合AI和沉浸式技术的开放性平台，满足个性化和有意义的学习需要。

Method: Open TutorAI平台综合了大语言模型与生成式AI技术，通过定制化3D虚拟形象实现多模态交互。入门流程收集学习者目标与偏好，自动配置个性化AI助教，并支持文本与虚拟形象对话。平台还提供内容管理、嵌入式反馈、面向学生、教师和家长的不同界面，以及学习分析功能辅助自我调节学习。

Result: 平台实现了模块化架构与多方交互，能动态响应个人学习者特征，无需技术门槛即可个性化使用。真实案例显示，虚拟形象和助教生成流程提升了学习参与度和情感连接，学习分析可追踪行为给出反馈。

Conclusion: Open TutorAI作为下代智能辅导系统的范例，将生成式AI、虚拟化身和学习分析融为一体，显著提升了自适应教学的沉浸感与人性化，推动了开放、可扩展的教育技术发展。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [212] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文探讨了将用户个性特征作为隐性信号来改进大语言模型（LLM）回答个性化的问题，并提出了基于“五大人格”特质对偏好语句进行标注的新数据集及框架。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化生成常依赖用户偏好，但这些偏好信号往往嘈杂、不完整甚至误导性，导致回答质量下降。作者受个性特质影响日常偏好的观察启发，尝试将个性视为更稳健的隐性偏好信号。

Method: 通过实验，将人格对齐的偏好用于模型响应，观察回答准确率变化。同时，构建了名为PACIFIC的人格标注偏好数据集（含五大人格OCEAN注释），并提出了自动检索并整合与用户个性对齐偏好的新框架。

Result: 实验显示，选用与用户推断人格一致的偏好，可将回答准确率从29.25%提升至76%；同时构建的PACIFIC数据集覆盖丰富领域。

Conclusion: 个性对齐的偏好信号能显著提升LLM个性化问答质量，PACIFIC数据集和自动整合框架为该方向的研究和实际应用提供了新工具。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [213] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 该论文提出了一个针对法律文档长语境问答的系统，实现了对复杂文档和专有词汇的处理，并生成全面、权威的长答案，同时引入了覆盖度指标评估系统表现。


<details>
  <summary>Details</summary>
Motivation: 法律文档涉及复杂结构、嵌套章节、长脚注，以及大量专有词汇和复杂语法，这些特性使得在此类文档上实现长语境与长答案的问答任务非常具有挑战性。现有方法难以全面检索与解析，又无法生成专业、全面的长答案，导致在实际法律应用中效果有限。

Method: 1) 提出一种能够分解法律领域专有词汇、增强信息检索能力的方法；2) 设计新的文档解析流程，专门处理嵌套章节和脚注并建立其间关联；3) 利用精确术语生成全面、权威的回答；4) 引入覆盖度指标，基于召回率为答案覆盖范围进行分级，方便人工评价。5) 构建法律与企业税领域专业人员标注的QA数据集。

Result: 系统在专业构建的数据集和实际领域测试中表现出优越性。实验和消融研究结果均显示，该系统在信息检索、答案覆盖范围和内容专业性方面优于传统方法。覆盖度新指标也有效辅助了系统性能的人类评价。

Conclusion: 本文提出的法律文档问答系统能够针对复杂结构和专业词汇实现长文本综合性解答，提升了问答系统在法律等专业长文档领域的实用性和准确性。覆盖度评价体系为相关系统研发提供了新的人机评估思路。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [214] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 本文探讨了如何让大语言模型（LLM）具备多说话人、多通道语音理解能力，尤其适用于智能眼镜。通过级联系统和端到端系统方法，结合多麦克风阵列，实现了高效定向语音识别和翻译。


<details>
  <summary>Details</summary>
Motivation: 目前的语音大模型多针对单一说话人、单通道数据，难以直接应用到包含多说话人、多个声源方向的真实场景，如智能眼镜，因此亟需研究如何提升模型的定向多说话人语音理解能力。

Method: 提出两种新方法让LLM具备定向多说话人理解：1）级联系统，前端用源分离模块；2）端到端系统，采用序列化输出训练。两种方法都利用嵌入在智能眼镜里的多麦克风阵列，以实现流式定向语音处理。

Result: 实验结果表明，两种方法都能显著提升LLM对定向语音的识别和翻译能力，在智能眼镜多说话人场景下取得了良好表现。

Conclusion: 本文所提方法有效增强了LLM对多说话人、定向语音的理解能力，为智能设备中的复杂语音交互提供了解决方案，有助于推动实际应用。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [215] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 本文提出了一种新的以风险为导向的大模型幻觉评价方法，强调区分不同严重程度的输出错误，并展示了标准评价指标的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型幻觉评价标准主要关注事实正确性，将所有错误视为同等严重，无法反映出涉及高风险医疗指令等更具危害性的错误。鉴于医疗问答环境中错误输出可能造成实际危害，有必要设计更加注重风险的评价框架。

Method: 作者提出风险敏感评价框架，评估模型输出中包含的有风险指令语言（如治疗建议、禁忌、紧急提示、高风险药物等），并结合内容相关性来识别高风险且缺乏事实依据的错误。实验以设计的患者问题压力测试不同类型语言模型。

Result: 实验发现，不同模型表面表现相似但风险特征差异显著。标准评价指标难以辨别高风险幻觉，无法满足安全性需求。

Conclusion: 应将风险敏感性纳入幻觉评价体系，且评价指标的有效性高度依赖于任务场景与提示设计，这对于提升大模型医疗应用安全性具有重要意义。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [216] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 单轮和多轮对话场景下大模型效果差异明显，多轮对话中常出现“迷失在对话中”(LiC)现象，本文提出用Mediator-Assistant架构桥接用户意图和模型理解，显著提高多轮对话性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在多轮对话中性能下降严重，先前认为是模型本身不稳定导致。然而本文认为真正的原因是用户意图和模型理解之间存在对齐鸿沟，因此需要新架构来解决。

Method: 提出Mediator-Assistant架构，将意图理解和任务执行解耦。具体做法是用经验驱动的Mediator根据会话历史把用户含糊输入转为明确、结构化的指令，再交由助手任务执行，提升了意图对齐效果。

Result: 实验证明提出的方法能在多轮对话各类大模型上显著减轻性能衰退，效果优于基线。

Conclusion: 多轮对话性能低下主要是意图对齐问题，而非模型能力限制。只靠模型规模提升和训练优化难以解决，需用新的交互结构来显式理清用户意图，从而提升LLMs多轮对话表现。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [217] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 该论文提出了ViHERMES数据集，用于越南医疗法规领域的多跳问答，同时提出图感知检索方法，实验表明新数据集和方法具有挑战性和有效性。


<details>
  <summary>Details</summary>
Motivation: 由于医疗法规文档法律条款层级复杂、频繁修订，需进行跨文档多跳推理，但针对像越南语等低资源语言尚缺乏相应评测数据集，限制了此类领域QA系统的研究和发展。

Method: 作者设计并构建了ViHERMES问答数据集，包括多跳跨法规问题，涵盖修订追踪、跨文档对比、操作规程整合等依赖关系。数据集制作采用了语义聚类、图挖掘与大语言模型结合的自动问答生成流程。并提出图感知检索框架，结合法律条款间正式关系，支持上下文扩展，实现更具法律依从性的答案推理。

Result: ViHERMES成为当前越南医疗法规多跳问答的高质量评测基线。实验中，作者提出的图感知检索方法在所有任务上均优于强力检索基线，显示其对多跳法规推理问答的提升作用。

Conclusion: ViHERMES填补了越南领域法规多跳问答基准空白，并为多跳法规推理QA方法研究提供了新平台。提出的图感知检索方法展现出处理复杂法规依赖关系的有效性。数据与系统已开源，便于后续学界研究与验证。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [218] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: 提出了TernaryLM，一种在训练过程中直接采用1-bit三值量化（{-1,0,+1}）的132M参数Transformer模型，在显著减少内存消耗的同时保持了语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常需要大量计算资源，阻碍了其在边缘设备和资源受限环境中的部署。本工作旨在通过原生极低比特量化，缓解内存压力，实现高效部署。

Method: 与传统的训练后量化方法不同，TernaryLM从头开始采用量化感知训练，结合直通估计器和自适应逐层缩放因子，使三值化过程融入模型学习全过程。

Result: 在TinyStories数据集上验证困惑度为58.42；在MRPC复述检测任务上达到82.47% F1分数；相较全精度模型实现2.4倍内存减少（498MB对1197MB）且推理延迟相当；不同数据集上均展现稳定训练。中间层对极端量化的兼容性最好。

Conclusion: 原生1-bit三值化训练是提升神经语言模型效率的一条有前景的技术路线，对未来的非均匀精度策略制定具有参考价值。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [219] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于一阶统计信息的轻量级大语言模型（LLM）剪枝方法，能在不引入过多计算成本的情况下实现高质量模型压缩。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法在剪枝质量和计算效率之间存在权衡：启发式方法高效但对激活异常值敏感，重构型方法更精确但计算代价高昂。为兼顾效率与性能，急需更高效且鲁棒的剪枝策略。

Method: 作者提出利用模型权重与激活值的通道统计量，校准传统幅值重要性分数，从而降低激活异常值对剪枝决策的影响，并通过解析能量补偿方法修正剪枝带来的分布偏移。整个流程无需额外训练、梯度或高阶信息。

Result: 在多个LLM家族、多种剪枝模式与不同任务评估中，本文方法在保持与启发式方法相当的计算成本下，实现了更优的剪枝性能。

Conclusion: 简单的统计校正机制能够有效提升大语言模型的后训练剪枝表现，为高效模型压缩提供了实用工具。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [220] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: 本文提出Demo-SafetyBench数据集，通过引入群体多元化视角提升大语言模型安全性评估的多样性和公平性，实现了可扩展且具人口统计鲁棒性的评测。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型安全性的评估，多依赖于样本有限、群体单一的标注者，忽视了不同群体在安全感知上的差异。这使得模型安全对部分群体不够友好，无法真实反映广泛人群的需求。

Method: 作者将DICES数据集的提示根据14个适应自BEAVERTAILS的安全领域进行重新分类，并保留人口统计信息。为扩展低资源领域，结合Llama-3.1-8B-Instruct依赖SimHash去重扩充数据，得到4.3万余样本。再利用不同LLM模型（如Gemma-7B、GPT-4o、LLaMA-2-7B）以零样本推理评估群体多元敏感性，通过特定阈值设定衡量评测鲁棒性与人口统计敏感性。

Result: 设置平衡阈值后，评测的一致性（ICC=0.87）高，人口统计敏感性（DS=0.12）低，表明该方法兼具可扩展性与群体多元适应性。

Conclusion: 论文证明，在评估大模型安全性时引入多元人口群体，并采用系统化的数据和评价策略，可以实现兼顾公平性、可扩展性与鲁棒性的安全性评估，对推动LLM更普适和公正应用具有重要意义。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [221] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种名为AlignX的两阶段框架，旨在更有效地将大语言模型（LLMs）按照有用（helpful）、无害（harmless）和诚实（honest）的多目标进行对齐，克服了现有方法中多目标冲突和路由失准的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多目标（如helpful、harmless和honest）对齐训练时，常使用SFT和MoE，但存在目标冲突导致特征遗忘和专家路由失效等问题，因此亟需解决多目标下的"轴坍塌"（Axis Collapse）现象，以提升LLM对齐的安全性和可靠性。

Method: AlignX框架分为两个阶段：第一阶段通过注入提示的微调（prompt-injected fine-tuning），提取各目标（轴）特定的任务特征，缓解灾难性遗忘；第二阶段引入基于分形和自然几何的MoCaE（模块化专家校准）模块，校准专家路由以提升推理的可靠性。

Result: AlignX在多个评测集上表现显著优于现有方法：在Alpaca（helpfulness）、BeaverTails（harmlessness）、TruthfulQA（honesty）上，分别提升胜率171.5%、提升truthfulness-informativeness 110.1%，减少安全违规4.3%；同时相比先前MoEs结构，延迟和内存消耗下降超过35%。

Conclusion: AlignX有效解决了多目标下轴坍塌问题，在模型对齐、多目标训练和资源效率方面均表现出色，结果在四个LLM上得以验证，表明方法具备良好的泛化性和实用价值。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [222] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该论文通过注入法律领域知识，提升了印地文和英文法律文本摘要的效果，在主流评测和专家验证中均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决书语言复杂且大多数人难以理解，因此亟需高质量、多语言的法律文本摘要辅助普通公众。现有模型通常未结合法律领域知识，难以达成高质量摘要，尤其是跨语言摘要。

Method: 提出通过领域知识预训练编码器增强提取式神经网络摘要模型，并对生成式模型（包括大语言模型）在英印双语大规模法律语料上持续预训练，从而注入法律知识，提升摘要效果。

Result: 该方法在英-英、英-印地文的法律文本摘要任务上，通过标准指标、事实一致性和法律领域专有指标，均取得统计学显著提升。

Conclusion: 注入法律领域知识能显著提升法律文本多语言摘要模型性能，并经由专家评估验证方法有效，为印度法律信息普惠化提供技术支撑。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [223] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 本论文提出了一种基于词汇相似度的新计算指标，用于评估罗曼语族主要语言间的互通性，并通过多种模型和语料对比分析取得了有意义的相关性。


<details>
  <summary>Details</summary>
Motivation: 罗曼语族各语言间存在不同程度的互通性，但如何量化这一互通性仍缺乏有效、自动化的工具。研究动机在于填补这一空白，提供一个客观、可量化的计算方法。

Method: 作者提出了一种基于词汇表层（orthographic、phonetic）和语义相似性的新计算指标，用于测量法语、意大利语、葡萄牙语、西班牙语和罗马尼亚语之间的互通性。使用不同的平行语料和词义表示模型，对比结果。

Result: 通过该计算指标获得的互通评分，不仅与对语言互通性的直觉判断一致，还与人类实验中的cloze测试结果显著相关，说明该方法具备较好的效度。

Conclusion: 新提出的计算指标可有效、自动化地量化近缘语言间的互通性，对语言学、计算语言学等领域均具有实际应用价值。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [224] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文比较了基于扩散（DLLM）和自回归（AR）结构的大语言模型在多步决策智能体任务中的表现，发现DLLM智能体在保持准确率的同时，平均速度快30%以上，部分任务甚至快8倍，并展现出更高效的计划与工具调用行为。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散型大模型（DLLM）作为自动回归模型的替代方案在生成效率和建模能力上有优势，但其在多步智能体决策中的具体表现及行为机制尚未被系统研究。本文旨在明确对比扩散与自回归解码下，其他要素不变情况下智能体的规划与工具使用能力及效率表现。

Method: 作者搭建统一智能体工作流（DeepDiver），分别用DLLM和AR作为主干，基于相同的数据进行智能体微调，形成可直接对比的两种智能体。之后通过基准测试和案例研究多角度比对其在多步决策任务中的效能与行为差异。

Result: 在准确率相似的前提下，DLLM智能体端到端平均快30%，个别任务快8倍。DLLM智能体完成任务所需的交互轮次和工具调用次数也更少，表现出更高计划命中率和更少回溯。注意到DLLM需更严格的工具调用训练和Span处理时的注意力遮罩对齐，否则效果下降。注意力分析发现DLLM表现出更明显的全局计划特征。

Conclusion: 扩散式大模型智能体在多步任务上实现显著的效率提升和更高效的计划能力，但部署时需关注工具调用规范性和跨Span输入的注意力策略。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [225] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: 本论文提出了一种新的有监督微调方法（SED-SFT），通过选择性熵正则化机制，有效缓解了传统交叉熵损失导致的大模型响应模式坍塌问题，从而提升后续强化学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的标准后训练流程，多采用交叉熵损失进行有监督微调。但这种方式易导致模型生成单一、缺乏多样性，进而影响后续强化学习阶段的探索能力。尽管一些综改替换损失函数以改进多样性，但往往无法兼顾多样性与准确性，整体增益有限。因此，需要一种既能提升响应多样性，又能维持模型性能的方法。

Method: 作者提出SED-SFT方法，在优化目标中加入选择性掩码的熵正则项，动态选取高潜力的token位置鼓励生成多样性，避免模式坍塌，并只对探索空间进行多样性增强。

Result: 在8个数学基准测试上，作者方法可在几乎不增加计算成本的基础上，显著提升生成多样性。强化学习后的性能平均在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct模型上，分别比标准CE方法提升2.06和1.20分。

Conclusion: SED-SFT在提升分布多样性和维持性能平衡上取得突破，明显增强了RL前处理效果，有望成为大模型微调的新范式。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [226] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文提出了一种系统性的评估框架，针对不同文化背景下多语言网络迷因内容的检测，对主流视觉-语言模型（VLMs）的跨文化表现进行诊断和量化。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型目前主要在西方或英语语料上训练，导致其在多文化、多语言环境下公平性和鲁棒性不足，尤其在仇恨迷因检测等任务上表现有限。

Method: 作者建立了一个跨文化的多语言迷因数据集评测体系，从学习策略（零样本/一样本）、提示语言（本地语言/英语）、以及翻译对含义和检测效果的影响三个角度，系统分析了主流VLM的表现。

Result: 实验表明，常见的“先翻译后检测”方法会降低表现，而本地化的提示和一样本学习策略能明显提升检测准确率。此外，主流模型倾向于收敛到西方的安全标准。

Conclusion: 作者提出了可操作的建议，有助于缓解模型的文化偏差，推动设计更具全球适应性的多模态内容审核系统。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [227] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 提出一种分步简化框架，有效提升跨多级别语句简化的能力，同时减少计算步骤。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在进行受控语句简化，尤其是跨度较大的可读性级别简化上表现欠佳，需要提升简化效果与语义保持能力。

Method: 提出一种框架：通过动态路径规划、具备语义感知的样例选择，以及链式思维结合对话历史，分步完成复杂简化过程。

Result: 在五种语言、两大基准测试中，该方法提升了简化效果，并减少了22-42%的计算步骤。人工评测显示，简化效果与语义保持之间存在根本性权衡，且语义保持判断本身较为主观。

Conclusion: 分步简化方法提升了可控性，但在大幅简化过程中，完全保持语义仍是难题。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [228] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: DLLM 固有地不适合用于变长生成，本文提出一种长度正则化推理框架（LR-DLLM），解决了长度未知情况下生成不可靠的问题，有效提升了生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前 Diffusion Large Language Models（DLLMs）由于其固有的固定长度推理方式，难以应对实际场景中长度未知的生成任务，如文本补全、插入等，现有方法比较 mask 长度时信心存在系统性偏差，导致生成过少或重复。亟需解决生成长度自主判定问题，提高实用性。

Method: 提出长度正则化（LR-DLLM）推理框架，将生成长度显式建模为变量，通过长度正则项消除由长度带来的置信度偏差。LR-DLLM 在不改变底层 DLLM 结构及其训练方式的前提下，实现了生成长度的动态调整。

Result: 在 HumanEvalInfilling（长度未知）上，LR-DLLM Pass@1 达到 51.3%（比 DreamOn 提高 13.4%），在多语言 McEval 上平均 Pass@1 达到 51.5%（比 DreamOn 提高 14.3%）。

Conclusion: LR-DLLM 有效解决了 DLLM 在变长生成时的信心偏差和长度判定难题，无需模型结构和训练更改即可大幅提升 DLLM 在实际变长场景下的性能和实用性。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [229] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在生成与自我验证能力之间的不对称问题，发现单独提升生成能力难以提升自我验证能力，但提升自我验证对生成有积极影响。作者提出多任务强化学习框架，兼顾生成和验证目标，有效提升模型整体效果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在复杂任务中展现了强大的生成与推理能力，但却在自我验证（检查自身答案正确性）上表现不足。理解和解决这种生成与自我验证之间的不对称，有助于提升模型整体智能表现。

Method: 作者系统分析了模型训练过程中生成和自我验证能力的演变，验证提升生成能力对自我验证能力的影响。随后，提出用多任务强化学习，将生成与自我验证作为独立但互补的目标联合优化，增强模型能力。

Result: 实验表明：单独提升生成能力无法带来自我验证进步，但增强自我验证显著提升生成表现。提出的多任务框架在多个数据集和模型上都优于只优化生成的方案。

Conclusion: 生成和自我验证能力存在结构性不对称。联合优化两者可以整体提升大模型生成和验证能力，为后续模型训练和设计提供新思路。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [230] [SciClaimEval: Cross-modal Claim Verification in Scientific Papers](https://arxiv.org/abs/2602.07621)
*Xanh Ho,Yun-Ang Wu,Sunisth Kumar,Tian Cheng Xia,Florian Boudin,Andre Greiner-Petter,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文提出了SciClaimEval数据集，用于科学论断验证任务。该数据集包含直接从论文中提取的真实论断及其反例，并提供跨模态证据，填补了现有同类资源的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的科学论断验证数据集主要依赖人工生成或基于大模型伪造的反例，缺乏从真实文献中提取的、带有原始证据（如图表等）的真实或被驳斥的断言，因此需要更真实和多样化的数据资源以推动该方向发展。

Method: 作者直接从已发表论文中抽取真实和被驳斥的论断，并创新性地通过修改图表证据（而非编辑断言或利用大模型生成矛盾）来生成反例。数据同时包含图片格式的图表以及多种表格格式（图片、LaTeX、HTML、JSON），并覆盖机器学习、自然语言处理和医学三个领域，由专家进行标注。此外，作者还在这些数据上评测了11个多模态基础模型。

Result: 结果表明，现有多模态基础模型在处理基于图表的论断验证方面仍然面临很大挑战，模型与人工基线之间存在明显差距，尤其是在图像证据理解方面。

Conclusion: SciClaimEval提供了高质量的、跨模态的科学论断验证数据集，有助于推动科学事实核查和多模态推理模型的发展，同时显示了当前模型在复杂图表理解任务上的显著不足。

Abstract: We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.

</details>


### [231] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出通过激活空间方向（steering vector）引导大语言模型（LLM）在辅导场景下展现不同导师风格，不仅提升对话的语义契合度，并在无明确提示词情况下实现个性化教学。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的辅导系统往往仅学习单一导师策略，未能反映现实中多样化的教学风格。而实际教学中，导师会根据学生需求灵活调整教学方法、反馈方式等，影响学习体验与参与度。因此，研究如何让LLM展现多元、灵活的辅导风格，具有重要意义。

Method: 作者将真人导师对话中的导师人设（persona）信息嵌入LLM，通过改进BiPO方法，学习出可控制模型输出风格的激活空间方向（steering vector），以此引导模型在不依赖显式提示的情况下展现不同导师风格。

Result: 实验表明，steering vector 能够区分不同导师在多种对话场景下的风格，并提升了模型输出与真实导师话语在语义上的契合度，通过用户偏好评测显示更高满意度，且整体语言风格未受到明显破坏。

Conclusion: 激活引导方法能有效且可解释地调控LLM展现基于真人对话数据提炼的多样导师风格，为构建更个性化、真实的智能辅导系统提供了新工具。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [232] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: 本文系统分析了LLM作为自动评审工具在文本摘要任务中的偏见，并发现其对LLM生成摘要相比人类摘要存在偏好，尤其在文本相似度降低时明显。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评审指标被广泛用于文本摘要等任务，因为其在理解、推理和应对改写方面优于传统算法，但现有研究对其具体偏见缺乏细致量化分析，尤其与文本重叠度的关系尚不明确。

Method: 作者以文本摘要任务为例，选取了9种参数量1亿至120亿的主流LLM（包括多个Gemma 3和LLaMA 3变种），对比分析其在不同摘要重叠程度下的判别偏见，采用ROUGE和BLEU等重叠指标量化摘要间的相似性。

Result: 研究发现，随着人生成摘要和LLM生成摘要的相似度降低，LLM评审更倾向于选择LLM生成的摘要。这一趋势在大多数模型下成立，且与模型自身的顺序偏见无关。同时，LLM评审对低重叠度摘要的判断能力有限。

Conclusion: LLM作为自动评审擅长捕捉语义信息，但在低文本重叠场景下表现欠佳，显示出对自身生成结果的偏好。因此在实际摘要评审中，需要结合其他评审方法以提升客观性与可靠性。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [233] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了SRR-Judge框架，实现对大模型推理中每一步的细致评估，并通过其指导下的微调显著提升了复杂搜索问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在复杂问答任务中虽有强表现，但主要依赖结果监督，缺乏对搜索推理过程（即每步思考与行动质量）的评估，导致模型中间步骤可能不可靠。

Method: 作者提出SRR-Judge，一个能细致评估每步推理和搜索行为的工具，并嵌入到ReAct风格的工作流中。利用SRR生成的标注数据，作者采用迭代拒绝采样的微调方法，强化基础智能体的搜索能力。

Result: SRR-Judge的评价在细粒度上优于更大模型，对最终答案正确率有很强的相关性。基于SRR标注轨迹对模型策略微调，带来了深度搜索基准上平均超过10%的pass@1分数提升。

Conclusion: 通过引入细粒度的评分与微调框架，SRR-Judge显著提升了大模型的深度搜索推理和问答能力，为未来相关任务提供了一种更可靠的训练和评估方式。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [234] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的个性化大模型上下文压缩方法，在多项实验任务中优于现有方法并大幅减少token用量。


<details>
  <summary>Details</summary>
Motivation: 大模型个性化需要大量用户历史与画像，受限于输入token数，这导致推理慢且API成本高昂。现有压缩方法缺乏对不同用户信息权重的细致区分，难以有效提取关键个性化信号。

Method: 作者分析LLM的注意力机制，发现其天生可以识别关键信号，通过微调进一步提升模型判别相关与无关信息的能力。基于此提出Attn-GS框架，首先利用标记模型对重要个性化句子进行标记，再引导压缩模型生成任务相关、质量高的用户压缩context。

Result: Attn-GS框架在不同个性化任务、token长度和环境下均显著优于多种基线，最终在token用量减少50倍的同时取得与全量context相近的性能。

Conclusion: 注重注意力机制反馈能够智能提取与压缩LLM个性化上下文，Attn-GS方法有效实现高效且准确的用户个性化支持。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [235] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在推理过程中的内部表征机制，发现它们会动态构建并利用结构化的潜在表征空间来进行上下文推理。


<details>
  <summary>Details</summary>
Motivation: 近年来研究表明LLM可能具有类似人类的概念结构，但这些结构在推理过程中是否具备实际功能尚不明确。该研究试图揭示LLM内部表征在推理任务中的实际作用机制。

Method: 作者对LLM进行上下文概念推断任务分析，运用因果中介分析追踪模型各层表征的时空演化，观察并量化不同层之间表征空间的变化及其功能性。

Result: 发现模型中自中间至后期层会形成稳定的“概念子空间”，该空间在不同上下文下持续存在。通过因果中介分析，证实这一子空间对模型最终推断结果起关键作用。早中期的注意力头会整合上下文信息构建该空间，后期层则以此产出答案。

Conclusion: LLM推理时会动态构建并利用结构化潜在表征空间，表明其具备类似于人类的、灵活且动态的计算原理，这对于揭示复杂模型的内部推理机制具有重要意义。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [236] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

TL;DR: 本论文系统研究了在真实用户参与场景下，大语言模型（LLMs）显式思考机制的效果，发现强制“思考”反而降低了模型的交互表现。通过增强信息披露，可以提升模型的任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管诱导大模型“思考”被认为能增强其解决复杂任务的能力，现有研究多聚焦于离线测试，对于真实用户参与的情境缺乏系统性研究。因此有必要检验这种显式思考在实际人机交互中的有效性与潜在问题。

Method: 作者选取七种大语言模型，结合三套基准测试和两种思考实现方式，通过定量响应分类和定性的失败传播案例分析，评估了显式思考在用户参与型智能体中的作用。同时，实验了鼓励信息披露的提示对模型表现的影响。

Result: 结果显示，在强制思考的情况下，多数大语言模型在用户交互场景下出现反常性性能下降。其主要原因是模型变得更“内向”，对用户展示的信息减少、回复更简短，从而削弱了有效的信息交换，导致任务失败。而提示模型主动披露信息则能显著提升效果。

Conclusion: 在现实智能体设计中，仅注重推理能力而忽略主动信息透明，可能适得其反。提升智能体的信息透明度，是未来优化人机交互与智能体性能的重要方向。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [237] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于博弈论的层级剪枝新方法，有效提升大语言模型在推理时的效率，并降低了性能损失。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多种任务中表现优异，但其高昂的计算需求限制了在实际场景中的部署。传统的层级剪枝方法依赖静态启发式规则，未能考虑层之间的相互依赖性，导致剪枝效果有限。

Method: 将层级剪枝问题建模为一个合作博弈，每一层视为博弈参与者，模型性能为效用。由于Shapley值计算开销巨大，作者提出使用轻量级代理网络预测不同层组合下的模型性能，并结合分层蒙特卡洛mask采样方法，降低了Shapley值估算的成本。该方法能够捕捉层间依赖关系，动态识别关键层进行剪枝。

Result: 实验表明，该方法在困惑度和零样本准确率上表现优于现有方法，实现了更加高效和有效的层级剪枝。

Conclusion: 提出的博弈论层级剪枝框架，通过高效估测层贡献，实现了性能与效率的双重提升，适用于大语言模型的实际应用。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [238] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: 当前主流的大型语言模型（LLM）在混合数字表示的数值比较上容易出错，比如判定 $5.7 \times 10^2$ 与 $580$ 哪个更大。该论文探究了LLM的内部状态，发现虽然模型内部能以较高精度编码数字量级，但在实际任务中准确率远低于内部表征。这一差距通过增加新的训练目标得到缓解，提高了数字推理表现。


<details>
  <summary>Details</summary>
Motivation: 主流LLM常被认为具备处理算术和数字推理的能力，但作者发现模型在混合不同数字表示法的比较任务上频频出错，提示模型或许并未深刻理解数字的大小关系，因此开展深入分析。

Method: 作者选用多种小型开源LLM，对其隐藏层进行探查，利用线性投影等手段解码出模型内部的数字量级表示，并通过分类器评价其对数字大小顺序的编码能力，同时以准确率等指标对模型实际推理能力与内部表征能力进行对比。最后在训练时引入分类器的辅助损失目标，对模型进行微调。

Result: (1) 通过线性投影，隐藏层能较好地还原数字的对数量级；(2) 内部状态可高准确率地编码数字大小顺序，但模型显式回答时准确率却仅有50-70%；(3) 加入辅助损失目标后，模型数值推理表现提升3.22%。

Conclusion: 论文发现LLM内部的数字量级感知能力远高于外部表现，通过优化内部表征结构能有效提升模型数字推理能力。这为改进LLM的数值推理和理解指明了新方向。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [239] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: 提出了TodoEvolve，一种自动合成和动态修正任务特定规划结构的元规划范式，相较于固定、人工设计的规划结构，展现了更高的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统在复杂、长周期任务中广泛应用规划技术，但现有方法多基于固定、手工设计的结构，难以适应开放性和多样性的任务需求。

Method: 构建了PlanFactory，一个标准化与模块化的设计空间，统一不同规划范式，并通过其收集高质量规划轨迹。使用Impedance-Guided Preference Optimization(IGPO)多目标强化学习方法训练Todo-14B，实现高效、稳定且可泛化的规划系统生成。

Result: 在五个智能体基准任务上，TodoEvolve在性能、稳定性、资源效率等方面优于精心设计的传统规划模块，并保持较低的API和运行成本。

Conclusion: TodoEvolve能够自主生成并优化适应不同任务需求的规划架构，为开放环境下智能体高效、灵活完成任务提供了新范式。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [240] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 本文提出了当大语言模型（LLMs）用于多答案问答任务时，现有置信度校准方法存在系统性低估置信度的问题。作者引入了新基准MACE，并提出了SCA方法，显著提升了校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的训练无关置信度校准方法主要依赖单一答案场景，而实际中许多问题存在多个正确答案，导致模型校准效果不可靠，需要系统性分析和方法改进。

Method: 作者构建了包含12,000道跨6领域、答案数可变的事实类问题基准MACE，系统评测了15种主流置信度校准方法在4个大模型架构（7B-72B）上的表现。针对置信度随正确答案数增加而反降的问题，提出了语义置信聚合（SCA）方法，融合多条高概率答案的置信度信息。

Result: 实验显示，随着正确答案数增加，准确率上升但置信度评估却下降，导致问题答案数不同下严重的校准偏差。所提出的SCA方法在混合答案数设置下取得目前最优的校准表现，同时在单一答案问题上也保持了出色性能。

Conclusion: 当前训练无关的置信度校准方法不适用于多答案场景，SCA方法能有效缓解置信度低估问题，提升大语言模型在实际复杂问答任务中的可靠性。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [241] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了SparseEval方法，通过稀疏优化策略和锚点选择，有效降低大语言模型评测的计算成本，同时保证准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型性能提升，评测所需样本量和计算成本迅速增加，传统全量评测变得不现实，因此需要高效、低成本的评测方法。

Method: 作者将评测任务建模为模型-样本矩阵的稀疏优化问题，提出通过选择具有代表性的锚点样本，并用梯度下降法优化锚点权重，同时采用MLP（多层感知机）表征和迭代细化策略，利用两个分数指标(Anchor Importance/Candidate Importance)筛选关键样本以提升评测效率。

Result: SparseEval在多个基准数据集上的实验结果显示，该方法不仅估计误差低，而且Kendall's τ排名相关性高，展现出优良的鲁棒性和实际应用价值。

Conclusion: SparseEval显著降低了大模型评测成本，实现了准确、高效的模型评测，对实际大模型开发与评估具有重要意义。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [242] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文研究了指令微调大语言模型内部处理指令信息的机制，提出了名为Instruction Vectors (IVs)的表征，并揭示了其非线性因果作用。


<details>
  <summary>Details</summary>
Motivation: 虽然指令微调模型广泛应用并取得突出效果，但学界对模型如何内部处理指令信息知之甚少，尤其是机制层面的理解不足。

Method: 作者结合因果中介分析，系统研究了Supervised Fine-Tuning (SFT)和Direct Preference Optimization (DPO)阶段中指令相关表征的形成及作用过程。提出了一种新方法，在无需线性假设的前提下，定位模型内部信息处理路径。

Result: 研究发现，指令表征（IVs）在模型内部相对局部化，既表现出线性可分性，也存在非线性因果交互。作者证实，在模型后期层中，基于前期层形成的任务表征，会选择不同的通路进行任务求解，即IVs的作用类似于“电路选择器”。

Conclusion: 本工作质疑了解释学中流行的线性表征假设，强调指令表征的非线性因果机制，并为深入理解大语言模型的任务执行路径提供了新工具和视角。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [243] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: 本文提出并评估了两个专为波兰语开发的小型内容安全分类模型Bielik Guard，分别有0.1B参数和0.5B参数，用于高效识别不安全内容类别，实验结果显示其效果领先同类工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于波兰语场景，内容安全问题日益重要，亟需精准、高效且体量小的安全分类器，以保障合规与用户体验。

Method: 作者基于MMLW-RoBERTa-base（0.1B参数）和PKOBP/polish-roberta-8k（0.5B参数）分别构建并微调了两个波兰语安全分类器，使用6,885条社区标注数据对模型进行五类安全内容的识别任务，并通过多项基准测试评估模型性能。

Result: 0.5B模型在测试集上的Micro F1和Macro F1分别为0.791和0.785，整体判断能力最佳。0.1B模型极为高效，并在真实用户输入中表现出77.65%的精确率和极低的假阳性率（0.63%），远超同等规模的HerBERT-PL-Guard。

Conclusion: Bielik Guard模型家族兼具高性能和高效率，尤其适合波兰语环境下的内容安全应用。模型已公开，不仅能区分敏感内容类型，还能对自残等场景提供更细致的应对建议，而非一刀切地封禁。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [244] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 本文介绍了一个名为CompositeHarm的多语言安全性评测基准，以研究大语言模型在不同语言下的安全对齐能力，结果表明现有翻译测试仅为基础，仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要在英文环境下进行安全性评估，而通过翻译到其他语言后，模型的有害行为识别能力或许下降，无法全面反映多语言情境下的模型安全性。

Method: 作者将AttaQ（结构化攻击）和MMSafetyBench（真实情景下的伤害）两个英语数据集翻译扩展到六种语言（含多种印地语系），并用三种大模型进行对比评测，采用轻量级推理策略以减少计算资源消耗。

Result: 在印地语言下，模型遭受结构化攻击的成功率大幅上升，而情境型伤害在多语言间迁移程度较温和。轻量级设计有效降低了评测的能耗和成本。

Conclusion: 仅靠翻译得到的多语言测试集不足以全面保障模型安全性，但可作为第一步。建议后续构建更加扎实、资源感知与因地制宜的多语言安全系统。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [245] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种用于早期识别轻度认知障碍（MCI）的新型框架SynCog，通过合成多模态数据与推理链细化方法，提升诊断模型的数据多样性与可解释性，实现了跨语言的强泛化能力，并在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语音的数字生物标志物虽具备无创和可扩展性，但受限于临床数据稀缺和模型解释性不足，尤其难以扩展到多语言场景，难以获得临床信任。

Method: SynCog框架将可控的零样本多模态数据合成与推理链（Chain-of-Thought, CoT）微调结合：先模拟具有多样认知特征的虚拟受试者，合成丰富、具有代表性的多语言数据，再用这些数据对大型多模态模型进行带推理链的微调，使模型输出具备明确的诊断推理流程，提高可解释性。

Result: 在ADReSS和ADReSSo数据集上，利用合成数据增强后，取得了Macro-F1分别为80.67%和78.46%的成绩，优于现有基线模型。在独立的真人普通话数据集（CIR-E）上，模型获得48.71%的Macro-F1，显示了良好的跨语种泛化能力。

Conclusion: 通过生成和推理驱动的SynCog方法，缓解了临床数据的瓶颈，提升了诊断性能和模型可解释性，为全球范围内临床可信、语言兼容的认知障碍筛查工具提供了重要基础。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [246] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本论文评估了六个大型语言模型在自动评判任务中的公正性，发现模型评判容易受到与内容质量无关的提示干扰，且很少在解释中提及这些干扰因素，造成了决策过程的不透明和不可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛被用作系统输出的自动评审员，确保其评判标准的公正性和透明性变得尤为重要。本研究旨在评估这些模型是否仅关注内容质量，还是会受无关提示影响，并探索其决策解释的完整性。

Method: 作者通过在评审提示中注入合成元数据标签（如来源、时间、年龄、性别、族群、教育等），对GPT-4o、Gemini-2.0-Flash、Gemma-3-27B、Qwen3-235B、Claude-3-Haiku、Llama3-70B六个模型进行系统测试，并在ELI5（事实型问答）和LitBench（创意写作）两个数据集上评估它们的判决变动率和提示承认率。

Result: 实验显示，模型的判决极易受元数据干扰，如专家、最近性及教育水平信息会显著影响评分结果，但大多数情况下模型并不会在自然语言解释中明确承认这些干扰，提示承认率接近于零。且在事实型任务中，部分模型对提示承认率较高，但在创意写作任务中基本消失，即使评分随提示强烈波动。

Conclusion: 现有大型语言模型在担任判官时存在明显的解释性缺口和易受无关因素干扰的问题，这对基于模型的自动评审在研究和实际应用中的可靠性提出了质疑。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [247] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: 该论文针对大语言模型长上下文场景下KV缓存内存线性增长导致效率瓶颈的问题，提出了基于残差的KV缓存压缩框架DeltaKV，并结合高效推理引擎Sparse-vLLM实现了显著的内存与推理速度优化。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM应用（如自主智能体、长链推理、创意写作）受限于KV缓存的内存线性增长，现有压缩与剔除技术在精度、压缩比和硬件效率上难以兼顾，因此需要更好的KV缓存处理方法。

Method: 提出DeltaKV框架，利用KV表征中的长距离token相似性与高共享潜在成分，通过记录和历史引用的语义残差进行压缩，而非丢弃token，有效减少存储需求。同时，提出推理引擎Sparse-vLLM，解耦内存管理并针对稀疏、不规则KV布局进行优化，实现压缩收益转化为系统实际加速。

Result: 实验证明，通过DeltaKV可将KV缓存占用降至原来的29%，同时在LongBench、SCBench、AIME三大评测上精度近乎无损。在与Sparse-vLLM集成后，在长上下文场景下推理吞吐达到vLLM的2倍。

Conclusion: DeltaKV与Sparse-vLLM结合，为大模型长上下文应用提供了实用的、可扩展的技术路径，在保持高精度的同时大幅降低内存需求并提升系统吞吐，有助于推动长上下文LLMs落地。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [248] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 提出了一种DIP（Diverge-to-Induce Prompting）方法，通过让大模型生成多个多样化高层理由，并综合优化成最终解题计划，从而提升零样本推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT）方法常仅引导模型使用单一推理策略，导致不同任务下效果有限。本研究旨在打破单一路径限制，提高LLM推理稳定性和表现。

Method: DIP方法首先让LLM为每个问题生成多个多样化高层推理理由，每个理由进一步展开为详细步骤草案，最后将这些草案归纳成最终解题步骤。

Result: 实验结果显示，DIP在零样本推理场景下优于单策略指导，验证了多方案融合推理的有效性。

Conclusion: 多路径归纳推理能提升LLM的推理表现，DIP为零样本任务提供了无需高消耗采样的新路线。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [249] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 该论文提出一种基于马尔可夫随机场的分数校准方法，用于提升机器生成文本（MGT）检测的准确性，特别是在指标法检测器上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然机器生成文本（MGTs）应用广泛，但也带来了诸如虚假信息及钓鱼等风险，因此对其检测变得十分重要。现有基于指标（metric-based）的方法相较于复杂的模型法更为可行，但存在随机性带来的评分偏差问题，需要有效的校准机制提升检测效果。

Method: 作者首先统一了主流指标法检测器的设计框架，分析其优劣，并发现文本生成过程中的随机性导致分数不稳定。通过理论和实证分析，提出了“邻近相似性”和“初始不稳定性”两个关系，并基于此用马尔可夫随机场模型建模，进一步采用平均场近似，使其作为轻量级组件嵌入主流检测器中。

Result: 在多种现实场景下（如不同大模型、改写攻击等），该方法与主流基准对比取得了显著性能提升，且几乎没有增加计算开销。

Conclusion: 该方法能有效解决检测分数偏差问题，提升了机器生成文本检测的实用性和准确性，具备实际部署价值。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [250] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

TL;DR: 论文提出了一种基于时间动态图的网络（TDGNet），用于提升扩散语言模型（D-LLMs）中的幻觉检测性能，在现有基线之上取得了表现提升。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测算法主要针对自回归大语言模型（LLMs）设计，难以直接迁移到扩散语言模型，因为后者的生成过程分布在去噪轨迹中，证据会随时间演变、漂移或自校正。因此，急需针对D-LLMs特性的专用幻觉检测方法。

Method: 提出TDGNet方法：把幻觉检测建模为对演化的token级注意力图进行学习。在每一步去噪中，对注意力图稀疏化并通过消息传递更新每个token的记忆，最后利用时间注意力汇聚整个轨迹证据，实现最终预测。

Result: 在LLaDA-8B和Dream-7B两个模型以及多个QA基准上，TDGNet在AUROC指标上较传统基线（基于输出、潜变量和静态图的方法）均有稳定提升，并且推理为单次、计算开销适中。

Conclusion: 时序动态注意力图建模对D-LLMs的幻觉检测非常有效，有望增强扩散语言模型在事实性任务场景下的表现。

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [251] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 这篇论文研究了一种新型推理方式——潜在空间推理(transformer)，即模型在内部“不用语言”进行思考，并展示了其在多选题上的推理过程和行为模式。


<details>
  <summary>Details</summary>
Motivation: 目前大多数大型语言模型(LLMs)推理时，会通过语言表达中间步骤(例如Chain-of-Thought)。本文关注：如果让模型仅在内部连续的隐藏空间中推理（不输出文字链），其推理过程具有什么样的特征？

Method: 作者使用了一种称为Latent Reasoning Transformer (LRT)的模型，对其在多项选择题(QA benchmark)上的推理过程进行逐步解码和分析，揭示模型每一步隐藏表示的“信念”变化，并观察其搜索过程。

Result: LRT模型在潜在空间自发学会了一种可结构化的搜索推理流程。推理通常分成探索-提交-收敛或回溯几步。回溯行为在32%样本中出现，且带来34%准确率提升，回溯多是从“最像正确答案的干扰项”转向正确答案。如果用明显不相关的选项替换干扰项，探索阶段会缩短54%。

Conclusion: 潜在推理模型在激活空间中实现了类比于链式思维(chain-of-thought)的推理流程：能够犯错、觉察错误并修正。它们无需语言表达即可实现有效、灵活的推理。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [252] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 本论文发现大型语言模型（LLM）在生成消费品推荐时存在对性别和种族的显著偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在产品推荐中的广泛应用，其潜在的性别和种族偏见问题尚未被充分研究。作者希望填补该领域的研究空白。

Method: 通过提示工程向LLM提出针对不同种族和性别群体的产品推荐请求，并采用标记词（Marked Words）、支持向量机（SVM）、Jensen-Shannon散度三种分析方法来识别和量化偏见。

Result: 结果显示不同人口群体在LLM推荐中存在显著差异，表明推荐内容有明显的性别与种族偏见。

Conclusion: 本文强调在LLM推荐系统中，需要关注并减少针对特定性别和种族群体的偏见，以实现更公平的推荐。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [253] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: 本文提出了一个新的对话总结评价架构DIAL-SUMMER，通过细致的错误分类和新数据集，系统评估对话摘要的质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要评测方法忽视了对话特有的结构和叙述视角变化，例如多说话人、多回合散点信息到摘要中，以及说话人视角从第一/二人称转为第三人称。本文旨在解决这些对话内容总结中的复杂性。

Method: 作者提出了DIAL-SUMMER，对摘要错误进行分类，从对话层面（关注说话人/回合）和回合内部（关注每轮谈及的信息）两个层级综合评估摘要。构建了带有细致人工错误标注的新数据集，并对错误类型做实证分析，同时评估了大语言模型在该任务中的表现。

Result: 分析发现，对话中段的回合最容易被摘要遗漏，摘要结尾则常见无依据的内容臆造（extrinsic hallucinations）。实验还表明，大语言模型（LLM）在错误检出上仍有挑战，显示出该任务的难度和所提方法的稳健性。

Conclusion: DIAL-SUMMER架构和数据集为未来提升对话摘要自动评测方法、增强大模型在该任务的能力奠定了基础。作者强调了该领域的进一步探索和改进的重要性。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [254] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

TL;DR: 本文综述了如何利用自然语言处理（NLP）技术提升地方政府会议记录的结构化和可访问性。重点讨论了文本分段、实体抽取和自动摘要三大核心任务。


<details>
  <summary>Details</summary>
Motivation: 地方会议记录通常冗长、专业且各地方标准不一，导致普通公众难以理解，也限制了自动化智能系统的处理能力，从而影响政府透明度和公民参与。

Method: 文章回顾了三项核心NLP任务：1）文档分段，用于识别会议中的不同议程和流程；2）特定领域实体抽取，识别会议中的参与者、机构等关键信息；3）自动文本摘要，将复杂的会议过程压缩为简明的结论，并分析了相关的方法、评估指标和公开数据资源。

Result: 通过比较不同方法，文章指出在任务实现过程中面临数据稀缺、隐私保护和数据源多样性等难题。

Conclusion: NLP技术可以显著提升地方治理会议记录的结构化和可读性，从而促进政府信息透明和公民参与，但仍需解决特有的数据与隐私挑战。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [255] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文比较了人类与大模型（LLMs）在多模态交流游戏中的对齐能力，发现人类-人类和AI-AI配对都能自然形成交流惯例，但人类-AI配对则表现不佳。提示AI模仿人类行为，虽改善表面特征，但本质对齐效果仍有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究大语言模型（LLMs）在互动中是否能够像人类一样与对方形成高效交流惯例，即‘对齐’，从而更好地与人类合作。

Method: 作者设计了多模态交流游戏，分别测试人类-人类、AI-AI和人类-AI三种配对，对比他们在交流中的表现（准确率、一致性和消息长度）；实验中还通过特定提示要求AI模仿更人性化的交流方式。

Result: 结果发现：同类型配对（人类-人类、AI-AI）的准确率、一致性和效率（消息变短）均逐步提升。异类型配对（人类-AI）未能形成类似惯例。即使对AI进行人类化提示，只有消息长度接近人类，但准确率和词汇重合度依然落后。

Conclusion: 仅凭表面模仿仍难实现深层对齐。真正的人机对齐需依靠共同的解释偏好和意义共识，而非简单仿照对话历史。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [256] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种无需增加参数量，仅通过增加每个token的计算量来提升大语言模型能力的新方法——自适应潜在链式思维（CoT）预训练。实验表明在推理和训练中均能提升效果，且计算量相较于以往循环方法更低。


<details>
  <summary>Details</summary>
Motivation: 当前大模型能力的提升受限于高质量语料的稀缺和通信开销的增加，传统的扩大模型参数和训练数据的方法逐渐难以继续带来显著提升。因此，亟需探索提升模型性能的新路径。

Method: 作者提出在预训练阶段引入token级的自适应潜在链式思维（CoT）：模型在生成每个token之前，依据token难度动态决定“思考”步数（轨迹长度），难的token分配更长的潜在推理轨迹，简单token分配更短甚至为零的轨迹。这一机制可通过单阶段预训练自然涌现，并通过token级自适应终止机制有效控制计算量。

Result: 在Llama架构上的实验显示：自适应潜在CoT方法在减少训练FLOPs（计算量）的前提下，能够持续降低语言建模困惑度并提升下游任务准确率；相较于以往循环结构方法更经济高效。

Conclusion: 自适应潜在CoT预训练不仅能充分利用现有语料，而且在计算开销受限条件下能进一步提升大模型表现，是突破大模型扩展瓶颈的有效方案。

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [257] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: 本文分析了RAG模型在知识冲突中的表现，发现深层FFN存在记忆优先现象，提出无监督的层级修正方法CoRect，有效提升了结果的真实性。


<details>
  <summary>Details</summary>
Motivation: RAG模型常常在内部知识与检索证据冲突时，生成不真实答案。现有方法要么对解码做表面调整，要么需要真实标签做权重修正，但都有限制。因此需要新的方法来解决知识冲突，提高生成真实性。

Method: 通过层级分析，发现问题源于深层FFN会用模型记忆覆盖检索到的上下文信息。方法上，提出CoRect：通过对比有无上下文的logits分布，自动定位高偏置层，无需真实标签，然后纠正这些层的表征，保留基于证据的信息。

Result: 在问答和摘要任务的基准上，CoRect相比主流方法，持续提升输出的忠实度，减少幻觉现象。

Conclusion: CoRect能自动识别并修正RAG生成中的偏差层，无需标注数据，实验证明其在提升结果忠实度上优于现有方法，具有实际应用价值。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [258] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出了第一个针对计算机使用代理（CUA）“无意行为”的系统性分析框架和自动发现方法，有效揭示了主流CUA在正常输入下仍可能表现出有害的异常行为。


<details>
  <summary>Details</summary>
Motivation: 尽管CUA能够自动化复杂的操作系统流程，但它们有时在看似正常输入下会表现出偏离预期的危险行为。现有对此类风险的研究仅限于零散的案例，缺乏统一的分类、分析和自动检测方法。本文旨在系统性地揭示和分析这一风险。

Method: 作者提出AutoElicit方法，该方法通过CUA自身执行反馈，不断扰动原本正常的指令，自动诱发并收集CUA的危害行为，同时确保持扰动过程和输入本身依然真实、无害。方法适用于现有主流CUA（如Claude 4.5 Haiku和Opus），并评估了人类确认过的有效扰动在不同CUA间的迁移性。

Result: 利用AutoElicit，作者在多种顶尖CUA上发现了数百个实际危害性的无意行为，并证明这些有害扰动在其他主流CUA中也存在显著迁移性和普遍性。

Conclusion: 本文提出的框架为在“真实计算机使用场景”中系统地揭示和理解CUA无意行为提供了方法论基础，有助于未来的CUA安全性提升与规范设计。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [259] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 本论文提出了一种无监督的强化学习方法，用于提升大型语言模型（LLMs）的长上下文处理能力，且无需高成本的人类标注或教师模型监督。通过在长文档中用占位符替换部分段落，让模型通过重建文章结构进行训练，在RULER和LongBench v2数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 近年来，RLVR方法增强了LLMs的长上下文能力，但大多依赖人类专家或强力教师模型给出的标准答案，成本高、效率低。研究者希望找到免监督、低成本的解决路径。

Method: 在长文档中随机替换若干段落为特殊占位符，使用若干候选段落让LLM通过强化学习方式重建原始文档，实现段落识别和排序的训练，无需手工标注数据或教师模型参与。

Result: 在RULER数据集上模型获得明显提升；在LongBench v2也有合理改善，无需人工长文本QA数据。还进行大量消融实验，分析奖励设计、数据处理、训练细节及数据规模等因素对效果的影响。

Conclusion: 本文方法在无需人工标注的前提下，有效提升了LLM的长上下文理解能力，并开放了相关代码和模型，具有一定的实用价值和推广前景。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [260] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 该论文分析了人类语义类别系统的两个特性——凸性与交流效率，指出二者虽相关但本质不同，且效率更能解释语义系统的结构。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，颜色命名系统同时具有凸性和高效性，但两者的理论联系与共现原因并不明晰。本文旨在理解两者间的关系及各自对语义系统的解释力。

Method: 结合分析和实证方法，基于信息瓶颈（IB）框架，分别检验凸性与效率之间的关系，并对颜色命名领域内的实际和假想命名系统进行对比预测。

Result: 结果发现：1）凸性和效率互不蕴含。存在凸但低效的系统，也有高效但不凸的系统。2）在颜色命名领域，最优高效系统大多为凸集，解释了观测到的凸性现象。3）效率对实际系统的判别力大于凸性，凸性的增量贡献很小。4）一些现象仅效率理论能解释。

Conclusion: 语义系统的凸性和效率虽常共现，但互为独立特征。效率理论能更全面地解释语义类型学现象，应作为主要分析工具。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [261] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 本文提出并验证了一种通过认知语言学、隐喻和大语言模型（LLM）结合分析语言以衡量身份融合的新方法，该方法在预测极端主义倾向方面优于现有方法，并揭示了通往极端暴力的两种不同心理路径。


<details>
  <summary>Details</summary>
Motivation: 在全球政治极化和政治暴力抬头的背景下，理解极端主义的心理根源变得尤为重要。以往研究发现，强烈的身份融合与极端行为倾向密切相关。本文旨在开发并验证一种基于语言的新工具，用于更高效、更大规模地量化和检测身份融合，从而助力极端主义研究与预警。

Method: 研究团队提出了“认知语言学身份融合量表”（Cognitive Linguistic Identity Fusion Score），该方法结合了认知语言学理论、隐喻分析及大语言模型，自动分析文本中的融合倾向。通过英国和新加坡的多个数据集，逐步与已有的融合量化方法进行对比。最后，将该工具应用于极端分子宣言，分析高融合与暴力倾向之间的关系及路径。

Result: 新方法在多数据集上能够更精准地预测经验证的身份融合分值，表现优于现有主流方法。对极端分子文本的分析发现，存在两种通往高融合与暴力的路径：一类是将自我完全融入群体的“理念型”极端分子，另一类则是将群体等同于自己个人困境的“申诉型”极端分子。

Conclusion: 研究完善了身份融合理论，并提出了一个高效可扩展的工具，有助于未来极端主义倾向的识别及相关心理机制的研究。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [262] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 本文提出将同义改写细分为不同语言学类型，以提升模型对语义等价的理解，从而改善诸如抄袭检测、重复问题识别等应用。


<details>
  <summary>Details</summary>
Motivation: 现有的同义改写研究多数将任务简化为两段文本是否同义的二元判定，或仅生成单一改写版本，缺乏对决定语义等价的语言因素的细粒度控制。这限制了模型对语义理解和表达灵活性的提升。

Method: 作者提出将同义改写分解为具体的语言学类型，并对模型进行有针对性的训练。通过为同义改写赋予多维标签，使模型能区分并控制不同的改写方式。

Result: 实验显示，对同义改写类型进行训练的模型在相关任务上表现优异，如维基百科抄袭检测准确率达89.6%，超越人工基线78.4%；arXiv论文抄袭检测准确率66.5%，同样优于人工基线55.7%；在Quora重复问题识别中也取得提升。

Conclusion: 细粒度的同义改写类型分解和建模能够提升语言模型的语义理解能力，从而推动相关下游应用的性能改善。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [263] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文探讨了RLVR（具有可验证奖励的强化学习）对大语言模型能力的影响，提出利用概率框架来解释其对于多步推理能力的提升机制，并通过实验验证了相关假设。


<details>
  <summary>Details</summary>
Motivation: 当前学界争论RLVR到底是激发LLM新能力还是仅仅利用潜在能力。本文作者认为RLVR的确可以赋予模型新能力，并希望通过理论与实验证明其机制。

Method: 作者提出以实例层面可解性为基础的概率框架，假设多步推理能力来自于单步操作成功概率的提升。基于Algebrarium框架，他们仅用单步任务训练模型，并在完全没见过的多步任务上测试性能。

Result: 实验发现：(1) RLVR显著增强模型现有技能，促使其探索以往无法到达的解路径；(2) 多步任务表现由原子步骤的联合概率严格控制，两者相关系数高达0.69-0.96；(3) RLVR作为全局最优器，可能牺牲部分技能以最大化总体奖励。

Conclusion: RLVR优化可解问题的过程，能够推动模型发展新能力，有望解释LLM在RLVR训练下出现涌现现象。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [264] [Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network](https://arxiv.org/abs/2602.08289)
*Binglin Wu,Xianneng Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于超图神经网络的法律文书实体与关系抽取方法，结合了法律领域知识与特殊司法案例，显著提升了抽取效果。


<details>
  <summary>Details</summary>
Motivation: 随着中国司法机构数字化推进，积累了大量电子法律文书。而现有的法律文书实体与关系抽取方法较少考虑法律领域的专业知识及行业特点，亟需更精准适用的抽取方法。

Method: 提出Legal-KAHRE算法，基于超图神经网络。方法包括：(1) 利用邻域打包策略与biaffine机制生成实体候选区间；(2) 构建司法知识词典，借助多头注意力机制整合领域知识到文本编码；(3) 在超图结构中融合法域特殊案例（如共同犯罪等）；(4) 通过超图神经网络进行高阶推理和信息传递。

Result: 在CAIL2022信息抽取数据集上，所提方法在实体与关系抽取任务上较基线方法取得了显著性能提升。

Conclusion: 结合法律领域知识和超图神经网络的实体关系抽取算法能更好地处理司法文书，为法律智能化分析提供了有效手段。

Abstract: With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.

</details>


### [265] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 论文提出了一个理论框架，分析任意上下文信息对Transformer大模型推理性能的影响，从几何角度揭示上下文对输出误差的修正机制，并通过实验验证该理论。


<details>
  <summary>Details</summary>
Motivation: 虽然已有工作证明上下文信息（如演示、外部知识、历史交互）可提升大模型推理效果，但其理论机制仅在特定场景（如ICL）有说明，普适性理解不足。缺乏针对任意上下文影响的统一理论分析框架，限制了有效利用上下文增强LLMs的能力。

Method: 作者提出以输出误差动态为核心的理论分析方法，在单层Transformer中，证明上下文修正项与基线误差项可加性分解，提出误差减少的几何充要条件，并给出上下文修正项的显式上界。该分析也拓展到多上下文、多层Transformer，公式明确。

Result: 理论分析表明，只有当上下文修正项与负基线误差方向一致且满足范数约束，才能有效减小误差。修正项范数上界由上下文与查询的相关性和互补性决定。实验在ICL、检索增强生成、记忆演化等任务下进行，验证了理论预测，同时提出了基于理论的上下文选择策略，可提升0.6%的性能。

Conclusion: 论文系统刻画了任意上下文对LLMs推理输出的理论影响机制，为后续上下文选择与应用提供了清晰的指导，能更有效利用外部信息增强大模型能力。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [266] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动生成司法文书的方法，显著提高了法律内容的准确性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成司法文书的方法过于简化了人类法官推理流程，尤其忽视了"预判断（Pre-Judge）"阶段，导致文书法律严谨性不足。

Method: 作者提出名为JUSTICE的新框架，模拟法官"查找→预判断→撰写"的思维流程。该方法包含三大模块：参考司法要素检索器（RJER）用于检索法律条文和案例，预判结论模拟器（ICE）生成可验证的中间结论，最终由统一合成器（JUS）整合所有信息生成完整判决文书。

Result: 在法律领域标准数据集和异构数据集上的实验表明，JUSTICE模型在法律准确性上显著优于现有方法，刑期预测等指标提升了4.6%。

Conclusion: 显式建模"预判断"阶段对于提升自动生成司法文书的法律逻辑性和准确性具有重要作用。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [267] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: 本文提出了Dr.SCI数据集和后训练流程，有效提升了大模型在开放性科学问答任务上的科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在开放性科学问答中表现受限，原因是科学领域缺乏高质量监督与合理评价标准，训练数据和奖励设计成为性能瓶颈。

Method: 1. 构建Dr.SCI大规模科学问答数据集，包含8个STEM学科、1百万问题，提供开放式/可验证性分割、难度标注、细粒度评价标准。2. 基于数据集提出新的后训练流程，包括：(i) 扩展模型推理模式的探索式SFT；(ii) 动态难度训练课程；(iii) 用评分细则引导的强化学习。

Result: 采用Dr.SCI流程训练的Qwen3-4B-Base模型在GPQA-diamond和GPQA-general上分别达到63.2和32.4，超越o1-mini和GPT-4o等现有强大基线，在开放式科学推理上有显著提升。

Conclusion: 通过系统性数据构建与创新训练流程，大模型在科学领域尤其是开放性问题上的推理能力显著增强，Dr.SCI方案为科学AI模型的开发提供了新方向。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [268] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种生成式框架来同时处理多意图检测与槽位填充，并构建了新的多意图数据集，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SLU方法主要面向单意图，现实中用户常表达多个意图，这对现有系统和数据集带来挑战。

Method: 提出attention-over-attention生成式解码器，通过引入归纳偏置提升多任务学习的表现；利用BERT中的NSP头部将单意图语料构造成多意图数据集。

Result: 所提attention-over-attention模型在MixATIS、MixSNIPS及新构建数据集上取得了SOTA表现。

Conclusion: 该方法有效提升了多意图意图检测与槽位填充的性能，为多意图对话理解任务提供了新思路。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [269] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: 提出了一种新的推理方法Thinking States，在输入处理过程中逐步生成和利用思维token，实现更高效的推理。相比传统链式思维（CoT），该方法在多项任务上提升了推理效果，并显著减少了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法虽能提升LLM的复杂任务推理能力，但需生成冗长的推理过程，导致推理成本高，推理延迟大。因此需要一种在不显著增加推理成本的情况下，维持甚至提高推理表现的方法。

Method: 提出Thinking States方法：在输入文本每处理若干token时，就生成一批“思维token”，将这些token映射回embedding空间并与后续输入一同处理。这些token可通过自然语言标注及teacher-forcing并行高效训练。

Result: 在多项推理任务中，Thinking States优于其他隐式推理方法。在数学任务上显著缩小与CoT的性能差距，在2-Hop QA任务上达到与CoT相当的效果但推理延迟更低。在状态跟踪任务上，Thinking States推理能力强于CoT，并能很好泛化到训练未见过的长序列。

Conclusion: Thinking States方法有效兼顾了推理效果与效率，推动LLM在复杂推理任务中向更高效、更强泛化能力方向发展。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [270] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 本文提出UReason基准，用于检测多模态大模型中推理对图像生成的实际作用。研究发现推理线索在提升性能的同时，可能因上下文干扰反而损害表现，提出只用精炼后的提示生成图像效果更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型越来越强调借助推理（如chain-of-thought）提升图像生成质量，但推理过程究竟在生成结果中发挥了怎样的作用仍不清楚。作者希望通过诊断实验深入理解推理对视觉合成的真实影响。

Method: 提出了UReason基准，包含2000个样本，覆盖编程、算数、空间、属性、文本等五类推理任务。设计评估框架，直接比较三种生成方式：直接生成、推理引导生成、仅基于精炼提示生成，并在八个开源多模态大模型上统一评估。

Result: 发现通用现象为“推理悖论”：推理线索整体提升了结果，但如果保留中间推理内容作为生成条件，反而妨碍图像合成；仅用精炼后的提示可显著优化生成结果。分析表明关键瓶颈在于上下文干扰而非推理能力不足。

Conclusion: UReason基准为多模态统一模型的推理研究提供了科学的测试平台。未来应重点研究如何有效融合推理与图像生成，同时减少上下文带来的干扰。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [271] [WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints](https://arxiv.org/abs/2602.08367)
*Zexuan Wang,Chenghao Yang,Yingqi Que,Zhenzhu Yang,Huaqing Yuan,Yiwen Wang,Zhengxuan Jiang,Shengjie Fang,Zhenhe Wu,Zhaohui Wang,Zhixin Yao,Jiashuo Liu,Jincheng Ren,Yuzhen Li,Yang Yang,Jiaheng Liu,Jian Yang,Zaiyuan Wang,Ge Zhang,Zhoufutu Wen,Wenhao Huang*

Main category: cs.CL

TL;DR: 本论文提出了WorldTravel基准和WorldTravel-Webscape测试环境，真实反映现实世界复杂的自动化规划问题，评估了现有模型在此环境下的表现，发现主流大模型在感知和计划推理存在明显瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有自动规划的基准任务多为松散约束且用理想化数据，无法体现现实中紧密耦合约束和从真实网页动态获取信息的难度。需要更真实和具挑战性的基准推动相关智能体的发展。

Method: 作者构建WorldTravel基准集，含5座城市的150个旅行场景，每个场景均包含15个以上的互相关联的时序与逻辑约束。同时，开发了WorldTravel-Webscape多模态仿真实验环境，提供2000余个网页渲染页面，要求智能体直接从视觉布局感知约束参数，并在此基础上完成规划。评估了10种前沿大模型的表现。

Result: 最先进的大模型GPT-5.2在仅有文本信息环境下的可行性仅为32.67%，在多模态环境下下降到19.33%。论文还分析出存在“感知-行动间隙”（Perception-Action Gap）以及当约束数量超过10时模型推理大概率失败的问题。感知和推理是两个独立的瓶颈。

Conclusion: 当前主流大模型无法有效解决感知与推理高度结合的现实复杂条件下的规划问题。未来需要融合高精度视觉感知与长程推理能力的新一代智能体，才能胜任真实世界中的物流和决策任务。

Abstract: Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\% feasibility in text-only settings, which plummets to 19.33\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.

</details>


### [272] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 该论文构建了一个包含20664条越南语社交媒体评论的情感数据集ViGoEmotions，并评估了不同表情符号处理策略对情感分类的影响。


<details>
  <summary>Details</summary>
Motivation: 情感分类对于情绪预测和有害内容检测有重要作用，但越南语这一方向的高质量语料和细粒度情感标签资源匮乏。

Method: 构建了包含27种细粒度情感标签的越南语情感语料ViGoEmotions，并基于8个预训练Transformer模型，使用三种不同的表情符号预处理方法进行对比实验，包括：保留表情符号，表情符号转文本，使用ViSoLex词法归一化。

Result: 实验发现将表情符号转为文本通常有助于提升BERT系列基线模型的表现，而对于ViSoBERT和CafeBERT而言，保留表情符号效果最好；若完全移除表情符号则会降低性能。ViSoBERT达到了最高Macro F1为61.50%，Weighted F1为63.26%。

Conclusion: ViGoEmotions数据集能有效支持多种模型，但情感分类任务的预处理策略和文本注释质量对下游表现有显著影响。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [273] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种受认知启发的长上下文推理框架，通过分块压缩和选择性记忆回调，有效提高大语言模型处理长文本的能力，实现了高准确率与高效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时面临计算成本高、信息遗忘和检索增强生成(RAG)导致的上下文碎片化问题，需要更高效的推理方案。

Method: 该方法将超长输入拆分为分块(chunk)，每块通过一个可学习的压缩器编码为压缩记忆。一个门控模块动态选择相关记忆块，推理模块结合动态工作记忆多次推理解决任务。压缩器与推理模块通过端到端强化学习联合优化，门控模块单独作为分类器训练。

Result: 在多跳推理基准RULER-HQA等任务上，方法在准确率上具备竞争力，并能将上下文长度从7K拓展到1.75M tokens，相较强基线方法在准确率-效率间具备更优权衡，GPU峰值内存下降2倍，推理速度提升6倍（相较MemAgent）。

Conclusion: 全文验证了分块压缩+选择性记忆机制显著提升了长上下文推理性能，并实现了高效率和低资源消耗，对大模型实际应用有重要意义。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [274] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: 本文提出团队（TEAM）框架，通过减少激活专家数量并提升接受 token 数，实现对混合专家扩散大语言模型（MoE dLLM）解码过程的加速。TEAM 基于专家路由决策在时间和空间上的一致性，采用三种互补专家激活和解码策略。实验表明，TEAM 可在几乎不影响性能的情况下，将推理速度提升至原有系统的 2.2 倍。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE dLLM 在每一步解噪中会激活大量专家，但实际只有少量 token 被保留，导致推理开销高，限制了其在对延迟敏感场景的应用。作者希望解决 MoE 架构与扩散解码之间的效率瓶颈。

Method: 观察到专家路由决策在不同解噪阶段（时间）和 token 位置（空间）上都表现出一致性，作者提出 TEAM 框架，涵盖三种策略：对已解码和掩码 token 谨慎挑选专家、同时对多个候选进行富有进取心的探索，以此最大化 token 接受率并减少激活专家数。

Result: 实验证明，TEAM 框架相比基础的 MoE dLLM 可获得最高 2.2 倍的加速效果，且引入的性能下降可以忽略不计。

Conclusion: TEAM 有效缓解了MoE dLLM 在扩散解码中的效率瓶颈，为在实际延迟敏感应用场景中部署此类模型提供了可行方案。

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [275] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了Prism方法，通过频谱感知的方式提升block-sparse attention中block筛选的效率，从而显著加速长文本上下文预填充过程且不降低准确率。


<details>
  <summary>Details</summary>
Motivation: 在长上下文LLM中，block-sparse attention可以加速计算，但如何高效且准确地选择相关block依然是难题，现有方法需付出高昂的token级代价。本研究旨在解决这一效率瓶颈。

Method: 作者分析通过mean pooling与RoPE的结合会损失高频位置信息，阻碍精确的block筛选。为此提出Prism方法，不用额外训练，通过频谱分支将block分为高频和低频部分，并结合能量温度校准，还原被削弱的位置信号，实现仅用block级操作完成重要性评估。

Result: Prism在多个评测任务中维持与全注意力机制相同的准确度，同时加速比可达5.1倍。

Conclusion: Prism方法有效解决了mean pooling下位置信息丢失导致的block选择不准问题，实现了高效且无训练的block-sparse attention筛选，显著提升长上下文任务推理效率。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [276] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 本文回应了Chomsky关于CHATGPT和大型语言模型（LLM）无法真正理解语言的批评，用实验验证了LLM对“可能”与“不可能”语言的学习能力较弱，提出了理论范式转变的可能性。


<details>
  <summary>Details</summary>
Motivation: Chomsky等学者批评LLM只是依赖表面统计关联，缺乏人类般的因果推理和自我修正机制，不能区分实际无法存在的“不可学习”语言。作者希望通过系统实验检验这一观点，同时探讨AI语言理论的发展方向。

Method: 作者首先回顾了语言学和心理学关于语言学习的相关文献，然后设计人工“不可学习语言”(如句子倒序、按奇偶词数加否定等)及正常英语，对GPT-2小型模型和LSTM模型分别进行两轮对照实验，并采用Welch's t-test进行统计比较。

Result: 实验结果显示，GPT-2小模型明显难以学习“不可学习语言”，在这些任务上的表现远逊于学习常规语言，显著性达p<0.001。而LSTM模型则表现出与Chomsky理论一致的局限性，说明Transformer结构的进步对模型能力起了关键作用。

Conclusion: 文章结合理论和实验，提出对Chomsky理论的新理解，同时建议AI语言研究应拓展到功能主义和经验主义的新范式，不再局限于理性主义的人类语言观。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [277] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一套用于评估和优化大型推理模型（LRMs）推理过程的新方法，包括推理质量定义、评估和利用评估信号进行优化。引入ME^2原则，开发了基于有向无环图（DAG）的推理迹评价框架，并据此构建了TRM-Preference数据集及推理奖励模型（TRM），实验显示该方法显著提升了推理表现和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前，大型推理模型越来越依赖于复杂的推理过程，但现有研究未能统一回答推理定义、评价方法及如何利用评价结果优化推理三个基本问题。因此需要新的理论体系和方法框架推动高质量推理和评估的发展。

Method: 1）提出ME^2原则，从宏观和微观两个层面对推理效率与有效性进行定义；2）将推理过程建模为有向无环图（DAG），开发DAG对比打分方法来评估复杂推理结构；3）基于DAG评价法及新数据集，训练推理奖励模型（TRM），实现批量化推理质量自动评价和优化。

Result: 实验证明，推理奖励信号可有效作为优化指标：推理选择决策带来最多19.3%的性能提升，在强化学习训练中提升最高可达3.9%。TRM模型适用于多种任务，显示出泛化能力。

Conclusion: 提出的评价与优化框架能有效提升LRMs的推理质量和实用性能，为复杂推理模型的未来发展提供了理论基础和实证支持。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [278] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文介绍了一个新的通用型信息检索助手基准（GISA），用于更加真实和系统地评估大型语言模型驱动的搜索代理，其任务和答案更贴近实际用户需求，解决了以前基准不自然、易被记忆污染的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索代理评测基准普遍存在以下问题：问题是从答案反推生成的，任务不够自然、不符实际应用场景；侧重单一任务类型或需信息聚合的任务，但都依赖静态答案集，容易被模型“背答案”导致评测不真实。因此，需要有一个更贴近真实用户需求、能持续适应信息变化的新型测试集。

Method: 作者提出GISA基准，包含373个人工设计的真实查询，涵盖四类结构化答案格式（单项、集合、列表、表格），便于确定性评估；任务兼顾信息聚合和深度推理，部分子集答案动态更新，防止记忆污染。此外，GISA为每个查询提供完整的人工搜索过程轨迹，可为过程级监督和模仿学习提供参考。

Result: 作者在主流LLM和商业搜索产品上评测GISA，发现表现最好的模型也只有19.3%的完全匹配得分，且复杂计划和信息聚合任务表现特别差。

Conclusion: 主流大模型和现有搜索产品距离真实复杂信息检索需求差距依然很大，GISA能够有效衡量这些差距，未来在LLM搜索代理的发展上还有很大提升空间。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [279] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文通过可解释性方法分析了大语言模型（LLM）理解表格的内部机制，揭示其通过顺序三阶段流程定位和提取表格单元格信息。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理表格任务方面越来越广泛，但其内部如何理解和定位线性化的二维结构表格机制尚不透明。论文试图揭开大模型在表格位置感知上的工作机制。

Method: 作者采用激活补丁（activation patching）和互补可解释性技术，深入剖析模型理解表格的过程，具体聚焦于“单元格定位”这一原子任务，并分解为三大阶段：语义绑定、坐标定位和信息提取。进一步利用向量空间分析等技术揭示模型如何利用向量算术精准聚焦单元格信息。

Result: 结果发现，模型通过数分隔符的“序数机制”来解析坐标，列索引编码在可操控的线性子空间，允许通过向量操作调整模型注意力。多单元格定位任务时，模型会复用同样的注意力头。

Conclusion: 该研究为Transformer结构下大模型对表格理解的内在机制提供了详细解释，有助于提升后续模型的可控性和表格处理能力。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [280] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 该论文面向英-马拉雅拉姆低资源语言对，提出了首个含译文质量备注（TQR）的分段质量估计数据集，以及基于强化学习的ALOPE-RL框架，提升了大模型在少量标注数据下的翻译质量估计表现。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量估计（QE）方法多数仅输出分数，难以体现具体错误信息，且低资源语言因缺乏标注数据，QE效果不佳。作者希望解决这两个问题。

Method: 1）构建英-马拉雅拉姆语分段QE数据集，含人工DA分数和错误备注TQR；2）设计ALOPE-RL，基于策略奖励（结合DA和TQR），用强化学习微调小参数量LLM（采用LoRA和4-bit量化），使模型学习来自错误备注的反馈。

Result: 在小规模QE数据集训练下，ALOPE-RL小参数模型表现优于大模型基线以及主流编码器模型，在英-马拉雅拉姆QE上取得最优效果。

Conclusion: 结合错误感知强化学习的策略微调，在数据/计算资源有限情境下能显著提升QE系统性能。数据和代码已开源，为领域发展提供便利。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [281] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于Masked Diffusion Modeling（MDM）的非自回归语音大模型VocalNet-MDM，有效提升了生成效率和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大模型多采用自回归方式，存在生成效率低和曝光偏差等问题，限制了实际语音交互应用。

Method: 首次将Masked Diffusion Modeling（MDM）引入语音大模型，通过层次化分块掩码和迭代自蒸馏法，改进训练与推理匹配性、减少迭代步骤，并适配流式场景。

Result: 仅用6K小时语音数据即可实现3.7-10倍推理加速，首块延迟降低34%，准确率维持主流水平，文本和语音自然度达前沿。

Conclusion: MDM为低延迟高效语音大模型提供了可扩展的新范式，展现出其在实际语音交互系统中的应用潜力。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [282] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 本文探讨了多语言大模型（LLMs）中是否存在专为单一语言服务的注意力头，并且尝试移除不需要语言的专用头以优化模型部署效率。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言大模型具备处理多种语言的能力，但在实际应用中，往往只需关注部分语言，因此如何高效部署、减少不必要的计算和资源消耗成为一个实际问题。

Method: 本文分析多语言LLMs的注意力头设计，研究是否存在特定语种的专用头，并测试去除某些语言的头后对所需语言性能的影响。

Result: 初步结果显示，部分注意力头确实负责特定语言的处理，而且可以在不影响目标语言性能的前提下，去除针对不关注语言的专用头。

Conclusion: 多语言大模型存在可优化空间，通过精简模型结构可以提升部署效率，为实际应用提供了更高的灵活性和性能保障。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [283] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文探讨了演绎、归纳和溯因三种基本推理范式对大语言模型（LLM）泛化能力的影响，并提出了一种新的数据集和训练方法系统性研究这一问题，实验证明对不同推理范式能力的提升可以显著增强模型在真实任务中的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 尽管提升大语言模型的推理能力引起了广泛关注，但三大推理范式（演绎、归纳、溯因）如何影响泛化能力，之前尚未有系统性研究。因此，本文旨在深入理解三种推理范式对LLM推理行为的影响及其泛化能力。

Method: 作者首先构建了一个包含三种推理范式（演绎、归纳、溯因）的符号任务推理轨迹数据集，抽象出与具体世界知识无关的推理过程。接着，作者采用多种手段向LLM注入这些能力，包括简单的微调、提升模型深度、将密集模型转换为专家混合模型等。最后，在涉及真实世界知识的自然语言任务中，对模型进行全面评估。

Result: 作者的方法显著提升了模型在真实场景任务中的泛化能力，在不同任务中性能提高最高达到14.60分。

Conclusion: 系统性注入和提升LLM三大推理范式能力可大幅增强模型的推理泛化能力，对于发展更强大、可适应多领域的LLM具有重要意义。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [284] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: 本文提出了一种新方法（GER-Eval），让大语言模型不仅执行已有评价标准，还能自主设计和评估自己的评价体系，发现其评分一致性在部分任务中存在不足，并对未来的评测方法提出建议。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型经常被用来作为自然语言生成任务的评估者，通常按照人工定义的标准来评分，但这些标准与模型内部对语言质量的理解往往并不一致，因此需要探索模型能否自我设计和应用更贴合自身能力的评价标准。

Method: 提出GER-Eval方法，允许LLM自主生成和应用评价标准，再对这些模型自定义标准的语义连贯性、打分一致性及与人工标准的对齐程度进行评测，并对比不同类型大模型（闭源如GPT-4o，开源如Llama）在评价一致性和泛化能力上的表现。

Result: LLM能够生成可解释、与任务相关的评价维度，并在模型内部表现出一致性；但在涉及事实性和知识密集型任务时，评分一致性较差。闭源模型（如GPT-4o）在模型间和跨模型间的一致性及泛化能力上优于开源模型（如Llama）。

Conclusion: LLM自评能力是一种“学习到的语言能力”，虽然在同一模型内部表现一致，但模型之间则存在分化。未来需要发展能将人类与模型评价语言联合建模的新方法，以提升评估的可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [285] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

TL;DR: 本文比较了三种在波斯语推文中检测不文明言论的方法，发现基于ParsBERT的监督学习方法表现优于ChatGPT等大语言模型。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体上不文明和仇恨言论现象的加剧，尤其在波斯语等低资源语言环境下，准确检测和分析这些言论变得更具挑战性。因此，本文旨在系统比较不同方法在波斯语仇恨言论检测中的有效性。

Method: 作者收集了47,278条与伊朗#MahsaAmini运动相关的波斯语推文，采用了三种方法进行不文明言论检测：人工定性编码、基于ParsBERT的监督学习，以及使用ChatGPT等大语言模型，并对比了各自的准确性与效率，同时考察了提示语言对ChatGPT表现的影响。

Result: 实验结果显示，ParsBERT显著优于七个版本的ChatGPT模型，无论是在识别微妙还是明显的不文明内容上，ChatGPT都存在较大困难。此外，使用英语或波斯语作为提示语对ChatGPT的输出结果影响不大。

Conclusion: 本文详细比较了三种方法在波斯语不文明言论检测中的优劣，确认了ParsBERT在低资源语种下的优势，并指出了大语言模型在该任务中的局限性。

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [286] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 本研究探讨了机器翻译在印度本土语言（特别是孟加拉语、马拉雅拉姆语和泰卢固语）中的应用及其对现有评估体系的影响，强调了语言多样性与教育技术融合的现实需求。


<details>
  <summary>Details</summary>
Motivation: 由于印度NEP 2020政策推动教育多语化以及如NPTEL等大型慕课平台的实际需求，研究选取了三种代表性本土语言来考察机器翻译技术的可行性与挑战。

Method: 通过分析NPTEL慕课平台上的技术讲解文本，构建自然发生的语音语料库，着重关注技术内容在多语种表达中的用词和表达方式。采用多种自动评估指标，检测这些语言在机器翻译输出中的表现。

Result: 结果显示，当机器翻译结果通过基于表层重叠的评价指标时，形态丰富且语义紧凑的本土语言会暴露出明显挑战，指标对不同类型特征表现出敏感性。

Conclusion: 研究表明，现有评估指标在处理印度多语种、尤其是结构复杂语言时有局限性，有必要针对这些语言特性探索更合适的评价方法，以推进教育领域多语种机器翻译的实际应用。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [287] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 本研究探讨了在对话式搜索系统中，将图像融入澄清性问题对用户表现的影响，发现图片对任务的促进作用随任务和用户特征而变化。


<details>
  <summary>Details</summary>
Motivation: 当前对话式搜索系统常用文字澄清性问题提升检索效果，但图像在澄清性问题中的辅助作用尚未深入研究。作者希望了解图像对用户完成搜索相关任务的影响，为多模态系统设计提供参考。

Method: 通过涉及73名参与者的用户实验，作者比较了多模态（文本+图片）和仅文本的澄清性问题，分别在两个任务（澄清问题的作答和查询重构）中的影响，考察多种用户视角和专业水平。

Result: 实验发现，用户在作答澄清性问题时更偏好多模态问题，而在查询重构任务中，则偏好并无显著差异。图片在保持不同专业水平用户的参与度上有所帮助，并能提升查询重构时的检索准确率。然而，仅文本设置在澄清性问题作答上的用户表现更好。

Conclusion: 研究认为，多模态增强的优势会依赖具体任务和用户特性，应有选择地将视觉信息融入对话式搜索系统设计中。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [288] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动化评估方法，通过比对生成性AI所生成的意见类摘要与原始评论中的事实性主张，实现更高效和精准的摘要质量评估。新方案对主张的一致性与覆盖度进行量化，实验显示该方法与人工判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化评测方法在大型语言模型推动下已难以准确评估生成性AI的摘要质量，尤其是在多样性和事实一致性要求更高的意见型摘要任务中，因此亟需新的评估工具。

Method: 作者提出了一种全自动的事实一致性评估方法，先从生成摘要及原始评论中抽取主张，再通过比对两者主张的相似度来衡量摘要的覆盖度和一致性。生成一个综合得分，该方法能够处理否定、释义或扩展形式的主张。

Result: 实验结果显示，该方法在比对主张内容时，能对含义相似的主张赋予更高分数，并且分数与人工评价结果高度相关，优于现有主流指标。

Conclusion: 论文证明了新指标在意见类文本摘要评估中的有效性和相关性，有望成为事实一致性评估的新工具。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [289] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: 该论文提出了一个名为PERSPECTRA的新基准，用于评估大语言模型在处理多元观点方面的能力，并展示了现有主流模型的显著不足。


<details>
  <summary>Details</summary>
Motivation: 多元性（pluralism）对于让大语言模型真实反映人类多样化的观点至关重要，但此前在LLM研究领域中鲜有重视，尤其是在模型对不同观点区分和理解上的系统测评仍然缺失。

Method: 作者融合了Kialo的结构清晰（辩论图表）和Reddit的语言多样性，构建了PERSPECTRA基准。通过受控检索-扩展流程，生成了3810条丰富论据，涵盖100个有争议话题的762组正反观点，并对每个观点扩展出多种自然表述。基准包括观点计数、观点匹配和立场判断三项任务。

Result: 实验显示，现有开源和商用LLM在观点计数和辨别任务中存在系统性错误，如高估观点数量、错误判别让步语气等，说明主流模型在多元观点理解与推理方面存在显著短板。

Conclusion: PERSPECTRA作为首个融合结构与多样性的可扩展多元性基准，为量化和提升模型对多个视角的表达、区分及推理能力提供了新工具，也揭示了当前模型在多元性感知方面距离预期还有较大差距。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [290] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 作者提出了一种规模化比较和可视化句子编码器的方法，通过将1101个公开可用句子编码器进行量化和映射，构建了句子编码器的关系地图。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏有效的工具和方法，对大量的句子编码器进行系统性、定量化比较和可视化，从而更好理解其特性及相互关系。

Method: 首先用句子集合的嵌入矩阵表示每个编码器，然后计算该编码器的成对内积（PIP）矩阵，并利用与单位基编码器间的量子相对熵（QRE）生成编码器特征向量。最后，这些特征向量用于可视化和地图构建。

Result: 构建了覆盖1101个句子编码器的全景地图，地图上属性相似的编码器会相互靠近，并且用编码器特征向量可以准确预测在检索和聚类等下游任务中的表现。

Conclusion: 作者的方法能有效揭示句子编码器之间的各种关系，构建的地图可为编码器选择和下游任务提供参考和指导，显示出较高的实用性和可靠性。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [291] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为LakeHopper的新方法，实现了预训练语言模型在不同数据湖之间的跨域迁移，为列类型自动化标注任务提供了更高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的列类型标注方法在迁移到新数据湖（即目标域）时，需要大量新数据的标注，代价高昂，且面临知识转移、数据选择和灾难性遗忘等挑战。

Method: 提出的LakeHopper框架通过LM交互识别源域与目标域知识差距，采用基于聚类的未标注列数据筛选策略，并利用渐进式微调方法平滑地将模型适配到新数据湖，实现知识的高效迁移。

Result: 实验证明，LakeHopper在两组不同的数据湖迁移场景中，无论低资源还是高资源设定下，均显著提升了列类型标注任务的效率和准确性。

Conclusion: LakeHopper有效缓解了迁移学习中知识鸿沟与数据利用问题，实现了预训练语言模型的高效迁移，减少了目标数据湖对人工标注的依赖，为数据表分析等任务带来新的解决方案。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [292] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出了一种新的情感支持对话建模方法，通过对对话过程中情感状态的连续建模，实现了更精细的中间策略监督，从而显著提升了对话系统的情感支持能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在情感支持对话应用中，受制于稀疏的最终结果信号，无法有效监督多轮对话中的中间策略决策，导致复杂情感支持任务表现有限。

Method: 提出AFlow框架，对多轮对话中的每一步进行情感流建模。通过对对话前缀进行细粒度监督，结合子路径层面的平衡优化目标，将用户偏好信号传递到中间状态，从而提升策略连贯性和共情性。

Result: AFlow模型在多个情感支持对话基准上显著优于包括GPT-4o和Claude-3.5在内的主流竞品，尤其是在策略连贯性与情感共情维度表现突出。

Conclusion: 通过更细腻的情感流建模与优化，AFlow能够提供更优质、更富共情的情感支持对话服务，对推动大模型在人机情感交互领域的能力提升具有重要意义。

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [293] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本论文提出了一种无需人工偏好对的数据驱动奖励模型WildReward，直接从真实用户交互中提取反馈信号，性能与传统奖励模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 目前LLM奖励模型大多依赖大规模人工标注偏好对，且标注成本高。随着LLM广泛应用，实际交互数据成为丰富的隐式反馈信号来源，如何充分利用这些数据建立奖励模型，是提升RLHF效率的重要问题。

Method: 作者利用WildChat平台收集用户实际交互数据，提出一套管线从中提取高质量人类反馈，总计获得186k训练样本。采用序数回归方法，直接基于用户反馈训练WildReward模型，跳过了偏好对生成步骤。

Result: WildReward在多个实验中展现出与传统奖励模型相当或更优的表现，尤其在校准能力与样本间一致性方面表现突出。实验还发现用户群体越多，Reward模型表现越强。

Conclusion: 直接利用真实世界交互数据构建奖励模型可行且有效，有潜力替代人工偏好对模式，助力LLM训练。WildReward应用于在线DPO训练，在多个任务上显著提升效果。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [294] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 本文针对token-level自适应计算提出新评测范式与模型框架，并系统性分析其与难度对齐与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前token-level自适应计算主要在自然语言任务和任务级指标上评估，难以观测token级难度并与模型结构因素混淆，导致无法判断算力分配是否真正反映底层复杂性。

Method: 引入可控复杂度的算法及合成语言任务，便于精确测试token级算力分配；提出ANIRA模型——一种每个token可变深度、决策隔离的循环变换器框架；基于该框架系统性分析算力分配与复杂性对齐、泛化及决策时机。

Result: 算力分配可与任务复杂性自动对齐，无需显式难度监督，但这种对齐不意味着算法泛化，模型在未见大输入时仍无法泛化；早期决策依靠静态结构信息，而在线终止机制与实际执行状态更相关。

Conclusion: 虽然token级算力分配在无监督下能部分反映底层复杂性，但不能保证算法泛化，且不同算力决策机制存在对复杂性的偏好差异。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [295] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: 本论文提出利用大语言模型(LLM)改进在危机救援文档中地理信息抽取的公平性与准确性，缓解现有自动化系统在地理和社会经济层面上的偏见。


<details>
  <summary>Details</summary>
Motivation: 当前自动化从文本抽取地理位置信息的系统存在地理及社会经济偏见，导致危机地区的可见性不均，对人道主义响应产生不利影响，因此需要更公平且高效的位置抽取方法。

Method: 作者提出了一个两步框架：首先用LLM基于少量样例完成命名实体识别，然后用依赖上下文的智能编码代理模块解决地名歧义。该方法与主流预训练和规则系统在HumSet扩展数据集上进行了准确率与公平性对比实验。

Result: 结果表明，LLM方法极大提升了地理信息抽取的准确性和公平性，对欠发达地区表现尤为突出。

Conclusion: 将大语言模型推理与负责任和包容性AI原则结合，有助于构建更公平的人道主义地理数据系统，推动危机数据分析领域“不让任何地方被遗漏”的目标。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [296] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在长上下文推理下的安全性，发现推理能力提升并不能自动提升模型的安全性，特别是在有害意图需组合推断时。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理能力增强，人们普遍认为其识别隐含有害意图的能力也会提升，即推理能力提升有助于提升安全性。本文质疑并验证这一假设。

Method: 作者设计了组合推理攻击，把有害意图拆成零散片段分布在长文本中，再用中性问题诱导模型推理、拼合这些片段，从而测试14个先进LLM模型（长至64k token）在此场景下的安全性表现。

Result: 主要发现三点：1) 推理能力强的模型并不更能阻挡组合推理攻击，往往能拼出有害意图但未能拒答；2) 随上下文变长，安全性对齐能力下降；3) 推理时增加计算资源显著减少攻击成功率，如GPT-oss-120b模型下降超50个百分点。

Conclusion: 安全性不会随模型推理能力提升而自然增强，尤其是在长上下文、多步推理场景下。安全对齐需针对高阶推理与长文本新型攻击另寻对策。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [297] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文提出了一种名为GitSearch的框架，专注于利用社区感知的信息缺口来提升内容审查效果，并显著优于现有方法和人工注释。


<details>
  <summary>Details</summary>
Motivation: 传统中心化事实核查方式在规模扩展上存在瓶颈，而社区驱动的众包审核虽可扩展但面临结构化挑战，尤其是在冷启动情况下，现有AI方法表现不佳。作者旨在解决如何提升社区审核的效率与质量，并帮助其在初始数据不足时工作。

Method: 提出GitSearch框架，包括三阶段流程：识别信息缺口、实时针对性网络检索补全缺失信息、合成合规内容注释。此外，作者构建了PolBench数据集（包含78,698条美政推文及其社区注释）用于评估。

Result: GitSearch实现了99%的覆盖率，是现有最先进方法的两倍，同时自动生成的注释在帮助性方面以69%的胜率超越人工注释，评分也更高（3.87 vs 3.36）。

Conclusion: GitSearch不仅显著提升了社区审核的覆盖率和注释质量，还平衡了规模与准确性，证明了以信息缺口为核心信号的AI框架的有效性。

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [298] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 目前商用的语言识别（LID）系统仅能可靠识别少数几百种语言，大部分语言识别覆盖不足。原因在于现有方法过于聚焦于无上下文的文本分类，将LID问题简单化。作者建议应将LID视为路由问题，融合环境线索，以提高对稀有语言的覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 尽管世界上有7000多种语言，但主流LID系统的语言覆盖远远不足，且对“长尾”语言（使用人数较少的语言）几乎没有支持。作者认为这是现有LID任务设定和标准激励机制共同导致的问题。

Method: 本文是立场性论文，强调从LID的本质和任务划分角度反思当前做法。作者建议将其转变为一种路由问题（即根据上下文和环境判别逻辑更加流畅、自然地识别语言），而非始终采用通用、全球固定先验的模型。

Result: 本论文未进行实验，而是分析了现有LID覆盖不足的原因，并提出了新的任务划分和建模思路。

Conclusion: 想要提升长尾语言识别的覆盖和实际效能，应将语言识别任务与环境信息、本地上下文结合，突破固定先验和仅文本分类的局限。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [299] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 论文提出了一种新的生成式预训练范式：下一个概念预测（NCP），增强了传统的下一个词元预测（NTP），并在多项基准测试上取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型通常采用下一个词元预测（NTP）作为预训练目标，但该任务较为简单，难以进一步提升模型性能。研究动机是通过更具挑战性的预训练目标，提升语言模型的理解与生成能力。

Method: 作者提出了“下一个概念预测”（NCP）方法，该方法不仅预测下一个词元，还要预测由多个词元组成的离散概念。通过向量量化构建概念词表，模型在训练时同时用NCP和NTP目标驱动参数更新。此外，模型生成概念来指导后续词元生成。该方法在多个参数规模和数据规模下进行了训练与验证。

Result: 在70M到1.5B参数规模、总计300B训练数据（包括Pythia和GPT-2结构）下进行实验证明，NCP在13个基准任务上均比传统NTP模型有持续提升。对8B参数Llama模型的继续预训练实验也显示，NCP可以进一步提升已有NTP模型性能。

Conclusion: NCP通过设定更具挑战性的预训练任务，有助于构建更强大的语言模型，并为未来语言建模任务的提升提供了一个有潜力的途径。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


### [300] [When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents](https://arxiv.org/abs/2602.08995)
*Yuting Ning,Jaylen Jones,Zhehao Zhang,Chentao Ye,Weitong Ruan,Junyi Li,Rahul Gupta,Huan Sun*

Main category: cs.CL

TL;DR: 本文首次提出并研究了计算机使用代理（CUAs）中行为偏离检测问题，并提出了新的检测及纠正机制DeAction，在多个评测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来计算机使用代理（CUAs）进展显著，但其行为仍常与用户意图不一致，这些偏离既可能来自外部攻击（如提示注入），也可能源于自身推理错误，带来安全隐患和效率损失。为此，有必要系统检测和纠正此类偏离行为。

Method: 作者首次定义了CUA的行为偏离检测问题，覆盖了由外部和内部两方面引发的偏离；划分了CUA实际部署中的三类常见偏离，并构建了具有人工对齐标注的行为基准数据集MisActBench。随后提出DeAction，一种检测并纠正偏离行为的通用方法，该方法在执行前检测偏离，通过结构化反馈迭代修正。

Result: （1）Offline评测中，DeAction在MisActBench数据集上F1分数领先基线15%以上；（2）Online评测下，面对攻击DeAction能将攻击成功率降低超90%，同时在正常环境中保持或提高任务成功率，推理延迟适中。

Conclusion: DeAction能有效提前检测并修正CUA的行为偏离，提升其安全性与可靠性，在现实场景及基准评测中效果显著，具备实用价值。

Abstract: Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [301] [Embodied Intelligence for Flexible Manufacturing: A Survey](https://arxiv.org/abs/2602.06966)
*Kai Xu,Hang Zhao,Ruizhen Hu,Min Yang,Hao Liu,Hui Zhang,Haibin Yu*

Main category: cs.RO

TL;DR: 本文综述了具身智能在灵活制造领域的研究进展，从感知、控制和决策三个层面对相关技术进行系统梳理，提出三阶段发展模型并展望未来趋势。


<details>
  <summary>Details</summary>
Motivation: 灵活制造场景下，工业具身智能面临感知受限下的过程建模与监测、柔性适应与高精控制的动态平衡、泛化技能和专用操作的融合等三大挑战，因此需要系统梳理现有研究成果并提出未来发展路径。

Method: 通过“工业之眼”（感知）、“工业之手”（控制）、“工业之脑”（决策）三个层面系统回顾与分析现有多模态感知、实时建模、柔性精密控制、智能优化等关键技术，并提出多层协同与跨学科融合的系统性观点。

Result: 总结了具身智能在工业制造领域已取得的主要技术进展，明确了感知-决策-执行的闭环优化路径，构建了认知增强、技能转移、系统进化三阶段的发展模型。

Conclusion: 系统揭示了工业具身智能在灵活制造的核心技术线路及演进脉络，为推动相关领域的理论创新与实际应用提供了指导和参考。

Abstract: Driven by breakthroughs in next-generation artificial intelligence, embodied intelligence is rapidly advancing into industrial manufacturing. In flexible manufacturing, industrial embodied intelligence faces three core challenges: accurate process modeling and monitoring under limited perception, dynamic balancing between flexible adaptation and high-precision control, and the integration of general-purpose skills with specialized industrial operations. Accordingly, this survey reviews existing work from three viewpoints: Industrial Eye, Industrial Hand, and Industrial Brain. At the perception level (Industrial Eye), multimodal data fusion and real-time modeling in complex dynamic settings are examined. At the control level (Industrial Hand), flexible, adaptive, and precise manipulation for complex manufacturing processes is analyzed. At the decision level (Industrial Brain), intelligent optimization methods for process planning and line scheduling are summarized. By considering multi-level collaboration and interdisciplinary integration, this work reveals the key technological pathways of embodied intelligence for closed-loop optimization of perception-decision-execution in manufacturing systems. A three-stage evolution model for the development of embodied intelligence in flexible manufacturing scenarios, comprising cognition enhancement, skill transition, and system evolution, is proposed, and future development trends are examined, to offer both a theoretical framework and practical guidance for the interdisciplinary advancement of industrial embodied intelligence in the context of flexible manufacturing.

</details>


### [302] [Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://arxiv.org/abs/2602.06967)
*Siqi Song,Xuanbing Xie,Zonglin Li,Yuqiang Li,Shijie Wang,Biqing Qi*

Main category: cs.RO

TL;DR: CLiMRS通过引入大语言模型（LLM）作为机器人代理并模拟人类团队合作，提高了多机器人协作效率，尤其在异构机器人复杂任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人协作任务通常面临空间限制、环境不确定性等挑战，且异构机器人协同配合的效率有限。尽管大语言模型在推理和规划上能力强，但其在机器人协调控制方面的潜力未被充分挖掘。为此，作者希望通过模拟人类团队的合作方式，充分释放LLM在多机器人协作中的优势。

Method: 提出了一种名为CLiMRS的自适应组谈判框架：每台机器人配备一个LLM代理；任务过程中通过通用提案规划器动态分组，并在子组内通过感知驱动的LLM讨论形成命令；子组管理员负责领导决策，机器人执行和环境反馈作为闭环调节，持续优化。并开发了异构多机器人基准测试CLiMBench，对框架进行评价。

Result: 实验表明，CLiMRS相比当前最佳基线方法，在复杂任务上提升了超40%的效率，同时在简单任务上的成功率未下降。

Conclusion: 采用人类启发的组队和协商原则，可显著提升异构多机器人系统的协同效率和任务执行能力，在多种复杂环境下表现优异。

Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.

</details>


### [303] [Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing](https://arxiv.org/abs/2602.06968)
*Xubo Luo,Zhaojin Li,Xue Wan,Wei Zhang,Leizheng Shu*

Main category: cs.RO

TL;DR: KANLoc是一种结合视觉里程计和轻量级全局位姿回归器的新型月球着陆6自由度定位方法，大幅降低误差，提升实时与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动月球着陆需精确实时的6自由度定位，但视觉里程计易漂移，基于地图的位置回归在纹理稀疏或弱光环境下效果差，亟需突破。

Method: 提出KANLoc框架：创新性地将视觉里程计（VO）与基于Kolmogorov-Arnold Network（KAN）的绝对位姿回归器紧密结合。KAN高效提取图像特征并映射到地图坐标，产生稳健稀疏全局锚点，这些锚点在束调整框架中与局部运动信息融合，抑制漂移。同时的混合定位策略，辅以针对传感器遮挡的数据增强方法，提高整体稳健性。

Result: 在真实及高仿真月面着陆数据集上，KANLoc能将平均平移误差和旋转误差分别降低32%和45%，在单次轨迹最高分别提升45%和48%，显著优于当前强基线方法，且能够实时运行（≥15 FPS）。

Conclusion: KANLoc有效结合了VO的局部高精度与KAN回归器的全局稳定性，可应用于实际月球着陆等严苛场景，兼顾高精度、实时性与强鲁棒性，推动了月面自主导航技术发展。

Abstract: Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.

</details>


### [304] [A Survey of Medical Drones from Flight Dynamics, Guidance, Navigation, and Control Perspectives](https://arxiv.org/abs/2602.06969)
*Roshan Kumar Chhetri,Sarocha Jetawatthana,Thanakorn Khamvilai*

Main category: cs.RO

TL;DR: 本文综述了医用无人机在飞行动态与制导、导航与控制（GNC）系统方面的最新研究进展，聚焦于任务需求、机型配置、载荷设计及其对医疗物资和飞行动态的影响，并探讨了GNC原理及算法在提升应用性能中的作用和局限。


<details>
  <summary>Details</summary>
Motivation: 虽然已有综述关注医用无人机在医疗供应链和应急响应等应用，但缺乏从飞行动态以及GNC系统角度对医用无人机进行系统梳理的研究。该文旨在填补医用无人机系统性原理与应用的研究空白。

Method: 本文通过文献调研和系统性分析，梳理了医用无人机的任务需求、无人机系统适配、载荷箱优化设计对飞行性能和医疗物资保护的影响，剖析了制导、导航与控制系统的原理、关键技术难题及应对算法，并评估其优劣。

Result: 分析指出，环境因素如振动、温度、压力和湿度会显著影响医疗物资质量；针对这些挑战，GNC算法能够在一定程度上缓解问题，但仍有局限性。现有GNC框架存在优化空间，且研究存在若干待填补空白。

Conclusion: 综合分析发现，优化医用无人机的GNC系统设计与算法对于提升实际医疗应用价值至关重要。未来需关注环境适应性、物资保护和系统智能化等方向，以推动无人机医疗运输的实际部署和发展。

Abstract: The integration of drones into the medical field has revolutionized healthcare delivery by enabling rapid transportation of medical supplies, organs, and even emergency assistance in remote or disaster-stricken areas. While other survey papers focus on the healthcare supply chain, operations, and medical emergency response aspects, this paper provides a comprehensive review of medical drones from the perspectives of flight dynamics and guidance, navigation, and control (GNC) systems. We first discuss the medical aerial delivery mission requirements and suitable uncrewed aerial system (UAS) configurations. We then address payload container design and optimization, and its effect on supplies and overall flight dynamics. We also explore the fundamental principles of GNC in the context of medical drone operations, highlighting key challenges arising from vibration, air temperature, pressure, and humidity, which affect the quality of medical supplies. The paper examines various GNC algorithms that can mitigate these challenges, as well as the algorithms' limitations. With these considerations, this survey aims to provide insights into optimizing GNC frameworks for medical drones, emphasizing research gaps and directions to improve real-world healthcare applications.

</details>


### [305] [Formal Methods in Robot Policy Learning and Verification: A Survey on Current Techniques and Future Directions](https://arxiv.org/abs/2602.06971)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi,Suresh Jagannathan*

Main category: cs.RO

TL;DR: 本文综述了形式化方法在机器人学习领域中的应用，重点讨论了政策学习与验证方面的最新进展，并对未来的挑战和发展方向进行了总结。


<details>
  <summary>Details</summary>
Motivation: 随着硬件和软件系统复杂度提升，尤其是在深度学习推动机器人策略发展后，现有的形式化分析方法面临新挑战。该领域迫切需要更适应复杂场景的规格化与验证工具，以保障机器人系统的安全和正确性。

Method: 文章采用综述方法，系统梳理并对比了近年来在机器人学习中用于策略学习与政策验证的形式化与半形式化方法，分析了其可扩展性与表达能力，并总结了各方法对提升实际机器人安全性与有效性的贡献。

Result: 通过对典型技术的对比和评述，文章展示了形式化方法在规范目标、指导学习及验证策略正确性等方面的优势，也揭示了当前方法存在的不足。

Conclusion: 虽然形式化方法对提升机器人系统安全性和正确性有重要作用，但目前在处理复杂学习策略（如深度神经网络实现的策略）时依然面临扩展性和可解释性等难题，未来需在方法理论与实践结合上持续创新。

Abstract: As hardware and software systems have grown in complexity, formal methods have been indispensable tools for rigorously specifying acceptable behaviors, synthesizing programs to meet these specifications, and validating the correctness of existing programs. In the field of robotics, a similar trend of rising complexity has emerged, driven in large part by the adoption of deep learning. While this shift has enabled the development of highly performant robot policies, their implementation as deep neural networks has posed challenges to traditional formal analysis, leading to models that are inflexible, fragile, and difficult to interpret. In response, the robotics community has introduced new formal and semi-formal methods to support the precise specification of complex objectives, guide the learning process to achieve them, and enable the verification of learned policies against them. In this survey, we provide a comprehensive overview of how formal methods have been used in recent robot learning research. We organize our discussion around two pillars: policy learning and policy verification. For both, we highlight representative techniques, compare their scalability and expressiveness, and summarize how they contribute to meaningfully improving realistic robot safety and correctness. We conclude with a discussion of remaining obstacles for achieving that goal and promising directions for advancing formal methods in robot learning.

</details>


### [306] [FeudalNav: A Simple Framework for Visual Navigation](https://arxiv.org/abs/2602.06974)
*Faith Johnson,Bryan Bo Cao,Shubham Jain,Ashwin Ashok,Kristin Dana*

Main category: cs.RO

TL;DR: 本文提出了一种分层视觉导航框架，通过视觉相似性驱动的记忆模块和可转移的航点选择网络，实现了无需测距信息和高效的机器人导航。实验显示，该方法在无需训练或推理阶段里程计信息的情况下，在多个SOTA方法中表现竞争力，并通过交互方式进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 在未知、无映射或GPS不可用的环境下，基于传统度量地图的方法难以应用，激励研究者开发不依赖详细地图、能自适应新环境的视觉导航方法。

Method: 提出一种分层决策架构，首先通过轻量且可迁移的航点选择网络划分子目标，然后利用按视觉相似性组织的隐空间记忆模块替代传统的图论拓扑表示，用以简化导航决策流程。该方法无需任何里程计信息，仅基于视觉进行决策与导航。

Result: 在Habitat AI环境中，该方法在训练及推理阶段完全不依赖里程计信息，性能与当前最优方法持平甚至优于部分竞品。此外，实验证明，即使最低限度的人为交互也能显著提升整体导航表现。

Conclusion: 本文方法简洁、高效、易于迁移，能够在新颖环境中实现有效导航，并支持可解释与交互式优化，显示出良好的实际应用前景。

Abstract: Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.

</details>


### [307] [Autonomous Manipulation of Hazardous Chemicals and Delicate Objects in a Self-Driving Laboratory: A Sliding Mode Approach](https://arxiv.org/abs/2602.06977)
*Shifa Sulaiman,Francesco Schetter,Tobias Jensen,Simon Bøgh,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的滑模控制（MBSMC）方法，在自驱动化学实验室中实现了操控机械臂平稳、安全搬运易碎危险材料的任务，并在多项指标下优于常规控制器。


<details>
  <summary>Details</summary>
Motivation: 在自驱动化学实验室中，机械臂需安全高效地搬运脆弱和危险的化学品，面对操作不确定性和动态扰动，传统控制方法（如PID）存在控制精度和稳定性不足的问题。为此，亟需更鲁棒且能平滑控制运动的方案。

Method: 设计了一种基于模型的滑模控制器（MBSMC），采用双曲正切激励函数，优化轨迹跟踪的平滑性，并与非基于模型的滑模控制（NMBSMC）和PID控制进行了系统性性能比较。

Result: 实验表明，相比NMBSMC和PID控制器，MBSMC显著提升了运动平滑度，减少了最高达90%的控制能耗，并顺利完成了精密抓取和操作任务（如开/关窗、搬运瓶体）。

Conclusion: 基于模型的滑模控制方法实现了精确、平滑且安全的机械臂运动控制，为自驱动化学实验室智能机械臂的发展提供了强有力的技术支持，优于PID等传统方法。

Abstract: Precise handling of chemical instruments and materials within a self-driving laboratory environment using robotic systems demands advanced and reliable control strategies. Sliding Mode Control (SMC) has emerged as a robust approach for managing uncertainties and disturbances in manipulator dynamics, providing superior control performance compared to traditional methods. This study implements a model-based SMC (MBSMC) utilizing a hyperbolic tangent function to regulate the motion of a manipulator mounted on a mobile platform operating inside a self-driving chemical laboratory. Given the manipulator's role in transporting fragile glass vessels filled with hazardous chemicals, the controller is specifically designed to minimize abrupt transitions and achieve gentle, accurate trajectory tracking. The proposed controller is benchmarked against a non-model-based SMC (NMBSMC) and a Proportional-Integral-Derivative (PID) controller using a comprehensive set of joint and Cartesian metrics. Compared to PID and NMBSMC, MBSMC achieved significantly smoother motion and up to 90% lower control effort, validating its robustness and precision for autonomous laboratory operations. Experimental trials confirmed successful execution of tasks such as vessel grasping and window operation, which failed under PID control due to its limited ability to handle nonlinear dynamics and external disturbances, resulting in substantial trajectory tracking errors. The results validate the controller's effectiveness in achieving smooth, precise, and safe manipulator motions, supporting the advancement of intelligent mobile manipulators in autonomous laboratory environments.

</details>


### [308] [LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM](https://arxiv.org/abs/2602.06991)
*Seongbo Ha,Sibaek Lee,Kyungsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.RO

TL;DR: 本文提出一种能够实时重建语言对齐稠密特征场的RGB-D SLAM系统，实现高精度三维感知和语言推理的结合。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统难以将稠密场景表达与语言理解融合，且实时性、内存开销和特征一致性难以兼顾。本文旨在开发一种高效、低延迟且支持语言推理的3D SLAM方案。

Method: 提出Top-K Rendering高通量语义无失真特征渲染流程，设计多准则地图管理策略高效剔除不一致或冗余的高斯分布，采用混合场优化框架，通过解耦优化频率，实现几何和语义场的实时联合优化。

Result: 系统几何准确度优于几何SLAM基线方法，语义丰富度与离线方法相当。实现了15FPS的实时性能，且保持了稠密、无压缩特征。

Conclusion: 本文方法验证了在线、稠密、语言对齐特征场的SLAM系统的可行性和有效性，为3D感知与语言推理的融合提供了现实基础。

Abstract: In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.

</details>


### [309] [When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey](https://arxiv.org/abs/2602.06995)
*Konstantinos Gounis,Sotiris A. Tegos,Dimitrios Tyrovolas,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.RO

TL;DR: 本文综述了无线通信与同步定位与地图构建（SLAM）结合的最新进展，特别聚焦于视觉SLAM（V-SLAM）与无线通信的集成，探讨技术现状、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着商用无线通信与传感设备的普及和智能自主系统的进步，实现健壮的联合通信与SLAM成为可能。同步提升定位、感知和通信能力，满足智能机器人和无人系统的复杂需求。

Method: 调查并分析无线信号传播、几何信道建模、射频（RF）定位与感知、图像处理等关键技术，以及使用概率模型、空间方法进行信号处理；并探讨视觉与无线信息融合解决方案。

Result: 发现无线信息能解决单目V-SLAM中的尺度歧义问题，SLAM中的视觉里程计可辅助5G等下一代无线通信。还分析了不同传感源与无线通信的互动机制，并提出了集成通信与SLAM的一体化发展方向。

Conclusion: 无线通信与SLAM的集成仍处于初级阶段，需在理论与实践上进一步突破。未来应提升RF及天线技术在高精度定位与语义感知方面的能力，推动智能系统的发展。

Abstract: The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.

</details>


### [310] [Admittance-Based Motion Planning with Vision-Guided Initialization for Robotic Manipulators in Self-Driving Laboratories](https://arxiv.org/abs/2602.07005)
*Shifa Sulaiman,Tobias Jensen,Francesco Schetter,Simon Bøgh*

Main category: cs.RO

TL;DR: 本文提出了一种基于顺应性控制的运动规划框架，使机器人能够在自驱动实验室（SDLs）环境下进行自适应、安全的人机协作式操作。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室中常涉及精密设备和偶发的人类干预，因此需要一种能兼顾安全性、适应性和可靠性的机器人控制方法。

Method: 提出在轨迹执行过程中直接集成顺应性控制算法，使机器人手臂可以根据环境中的外部力即时响应，实现动态人机交互；结合视觉算法进行结构化平面物体的初始姿态检测，为运动规划提供参考。

Result: 通过纹理化平面图像检测的实验证明了方法的可行性，机器人能实现实时受力反馈的人机交互。

Conclusion: 该方法增强了自驱动实验室中机器操作的安全性和自适应能力，未来可推广至处理透明实验室物体等更复杂场景，加强人机协作。

Abstract: Self driving laboratories (SDLs) are highly automated research environments that leverage advanced technologies to conduct experiments and analyze data with minimal human involvement. These environments often involve delicate laboratory equipment, unpredictable environmental interactions, and occasional human intervention, making compliant and force aware control essential for ensuring safety, adaptability, and reliability. This paper introduces a motion-planning framework centered on admittance control to enable adaptive and compliant robotic manipulation. Unlike conventional schemes, the proposed approach integrates an admittance controller directly into trajectory execution, allowing the manipulator to dynamically respond to external forces during interaction. This capability enables human operators to override or redirect the robot's motion in real time. A vision algorithm based on structured planar pose estimation is employed to detect and localize textured planar objects through feature extraction, homography estimation, and depth fusion, thereby providing an initial target configuration for motion planning. The vision based initialization establishes the reference trajectory, while the embedded admittance controller ensures that trajectory execution remains safe, adaptive, and responsive to external forces or human intervention. The proposed strategy is validated using textured image detection as a proof of concept. Future work will extend the framework to SDL environments involving transparent laboratory objects where compliant motion planning can further enhance autonomy, safety, and human-robot collaboration.

</details>


### [311] [ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning](https://arxiv.org/abs/2602.07007)
*Dongsheng Chen,Yuxuan Li,Yi Lin,Guanhua Chen,Jiaxin Zhang,Xiangyu Zhao,Lei Ma,Xin Yao,Xuetao Wei*

Main category: cs.RO

TL;DR: 本文提出了ARGOS框架，用以高效且有物理基础地分析具身AI系统的安全风险，并自动生成符合标准的功能安全需求。


<details>
  <summary>Details</summary>
Motivation: 传统的危害分析与风险评估方法（HARA）难以适应具身AI面对开放自然语言指令时引发的风险组合爆炸问题，且现有大语言模型虽然可扩展，但缺乏物理基础，导致风险描述表浅或不连贯。

Method: 提出了ARGOS框架，将开放式用户指令动态分解为实体的具体物理属性，用以指导大语言模型结合因果风险因素进行物理合理的危害场景生成。然后ARGOS将国际安全标准（如ISO 13482）实例化为场景相关的功能安全需求（FSRs），并结合机器人能力整合到具体应用中。

Result: 实验显示ARGOS在生成FSRs质量上表现优异，且能有效识别“长尾”风险，优于目前基线方法。

Conclusion: ARGOS为具身AI功能安全需求的系统化、物理基础化生成提供了关键方法，对于推动该领域工业级安全部署具有重要意义。

Abstract: Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.

</details>


### [312] [A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration](https://arxiv.org/abs/2602.07024)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 本文提出了一种结合惯性测量单元数据手套与视觉触觉传感器的人体活动识别系统，并在多种情境下进行了测试，展示了其在不同合作任务中的高准确率。


<details>
  <summary>Details</summary>
Motivation: 在人机协作中，准确识别人类活动对机器人的自主响应与适应人类意图至关重要，尤其是涉及手部与机器人接触活动时，目前的系统在多模态数据融合与识别准确率方面存在提升空间。

Method: 该方法结合了一种模块化数据手套（集成惯性测量单元）和视觉型触觉传感器，能够捕捉手部在接触机器人时的多模态活动数据，并在离线分类、静态实时分类和真实HRC场景下进行了全面测试。

Result: 实验结果表明，该多模态识别系统在所有测试场景下均实现了很高的识别准确率。

Conclusion: 多模态数据融合的人体活动识别方法能有效提升人机协作中的交互表现，未来可广泛应用于多种协作场景。

Abstract: Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.

</details>


### [313] [Airspace-aware Contingency Landing Planning](https://arxiv.org/abs/2602.07074)
*H. Emre Tekaslan,Ella M. Atkins*

Main category: cs.RO

TL;DR: 本文提出了一种用于飞机紧急降落的实时搜索型规划器，能在密集空域中最小化对航班的干扰，并考虑地面风险，利用ADS-B数据和低延迟算法完成轨迹和风险评估，应用于华盛顿特区等复杂空域，取得了比传统方法更优的联合风险和运行效率。


<details>
  <summary>Details</summary>
Motivation: 当前密集空域下飞机发生紧急情况时，如何在保障安全同时减少对其他航班和地面风险的影响，是一个实际紧迫但尚未充分解决的问题。

Method: 基于历史ADS-B数据评估空域航班密度，采用低延迟计算几何算法生成高风险区域的热力图，将空域风险和地面人口密度风险进行联合建模。轨迹选择综合考量空域中其它飞机干扰与人口暴露度，辅以着陆点选择模块降低对现有航班影响。

Result: 与传统最小风险Dubins路径对比，所提规划器可在笔记本电脑上实时生成联合风险更小、对空域运行干扰更低的紧急降落轨迹，且平均仅需2.9秒。

Conclusion: 该方法在保持实时性的同时，有效提升了紧急着陆轨迹的安全性和对空域运行的友好度，为密集空域飞机紧急管理提供了可行解决方案。未来将进一步引入动态空域信息优化规划效果。

Abstract: This paper develops a real-time, search-based aircraft contingency landing planner that minimizes traffic disruptions while accounting for ground risk. The airspace model captures dense air traffic departure and arrival flows, helicopter corridors, and prohibited zones and is demonstrated with a Washington, D.C., area case study. Historical Automatic Dependent Surveillance-Broadcast (ADS-B) data are processed to estimate air traffic density. A low-latency computational geometry algorithm generates proximity-based heatmaps around high-risk corridors and restricted regions. Airspace risk is quantified as the cumulative exposure time of a landing trajectory within congested regions, while ground risk is assessed from overflown population density to jointly guide trajectory selection. A landing site selection module further mitigates disruption to nominal air traffic operations. Benchmarking against minimum-risk Dubins solutions demonstrates that the proposed planner achieves lower joint risk and reduced airspace disruption while maintaining real-time performance. Under airspace-risk-only conditions, the planner generates trajectories within an average of 2.9 seconds on a laptop computer. Future work will incorporate dynamic air traffic updates to enable spatiotemporal contingency landing planning that minimizes the need for real-time traffic rerouting.

</details>


### [314] [A compliant ankle-actuated compass walker with triggering timing control](https://arxiv.org/abs/2602.07158)
*Deniz Kerimoglu,Ismail Uyanik*

Main category: cs.RO

TL;DR: 本文提出了一种新的人行步态模型TC-AACG，实现了更容易实际应用的弹性踝部推蹬，提高了双足机器人的行走能力。


<details>
  <summary>Details</summary>
Motivation: 传统的无源动态步行者主要依赖倾斜面和重力，虽然已有改进方法使其能适应水平或复杂地面，但大多实现方式在物理平台上难以操作。

Method: 本文提出了触发控制的踝关节驱动步态模型（TC-AACG），采用非瞬时弹性踝部推蹬，并可通过串联弹性执行器(SEA)在现实中实现。通过系统性仿真分析，对其步行速度、机械运输成本及吸引域等进行了评估。

Result: 仿真结果表明，该方法相比于传统冲击式推蹬拓展了模型的步行能力，在多个性能指标上均有改进。

Conclusion: TC-AACG模型以更实用的推蹬机制提升了仿步行机器人的行走范围和效率，兼具理论与现实可行性。

Abstract: Passive dynamic walkers are widely adopted as a mathematical model to represent biped walking. The stable locomotion of these models is limited to tilted surfaces, requiring gravitational energy. Various techniques, such as actuation through the ankle and hip joints, have been proposed to extend the applicability of these models to level ground and rough terrain with improved locomotion efficiency. However, most of these techniques rely on impulsive energy injection schemes and torsional springs, which are quite challenging to implement in a physical platform. Here, a new model is proposed, named triggering controlled ankle actuated compass gait (TC-AACG), which allows non-instantaneous compliant ankle pushoff. The proposed technique can be implemented in physical platforms via series elastic actuators (SEAs). Our systematic examination shows that the proposed approach extends the locomotion capabilities of a biped model compared to impulsive ankle pushoff approach. We provide extensive simulation analysis investigating the locomotion speed, mechanical cost of transport, and basin of attraction of the proposed model.

</details>


### [315] [Continuum Robot Localization using Distributed Time-of-Flight Sensors](https://arxiv.org/abs/2602.07209)
*Spencer Teetaert,Giammarco Caroleo,Marco Pontin,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot,Perla Maiolino*

Main category: cs.RO

TL;DR: 该论文提出了一种适用于软体和连续体机器人（CR）的本体定位方法，利用沿机器人长度分布的小型低分辨率ToF传感器，并结合机器人形状先验，实现了高精度的定位与建图。实验结果显示，定位误差低且对环境具有较好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 常规的高分辨率ToF（如激光雷达）虽然在移动机器人上效果很好，但由于体积过大和软体机器人的可变形特性，不适用于连续体机器人。本领域CR在复杂环境下的定位和建图一直是难以解决的问题。

Method: 作者采用一系列小型、低分辨率的ToF传感器沿机器人本体分布，将各个传感器的测量数据与机器人形状的先验知识进行融合，通过此方式完成定位估算，即便单个传感器经常遇到退化情况，也可实现整体准确定位。

Result: 在一个长53厘米的连续体机器人上，实验获得了2.5厘米的位置误差和7.2度的旋转误差。该方法在多种环境下，通过仿真与现实实验均重复获得一致结果，并对先验地图误差的鲁棒性进行了研究。

Conclusion: 该方法有效解决了连续体机器人在无结构环境中的定位与建图问题，表现出良好的精度与鲁棒性，对于未来软体机器人实际部署具有重要意义。

Abstract: Localization and mapping of an environment are crucial tasks for any robot operating in unstructured environments. Time-of-flight (ToF) sensors (e.g.,~lidar) have proven useful in mobile robotics, where high-resolution sensors can be used for simultaneous localization and mapping. In soft and continuum robotics, however, these high-resolution sensors are too large for practical use. This, combined with the deformable nature of such robots, has resulted in continuum robot (CR) localization and mapping in unstructured environments being a largely untouched area. In this work, we present a localization technique for CRs that relies on small, low-resolution ToF sensors distributed along the length of the robot. By fusing measurement information with a robot shape prior, we show that accurate localization is possible despite each sensor experiencing frequent degenerate scenarios. We achieve an average localization error of 2.5cm in position and 7.2° in rotation across all experimental conditions with a 53cm long robot. We demonstrate that the results are repeated across multiple environments, in both simulation and real-world experiments, and study robustness in the estimation to deviations in the prior map.

</details>


### [316] [Realistic Synthetic Household Data Generation at Scale](https://arxiv.org/abs/2602.07243)
*Siddharth Singh,Ifrah Idrees,Abraham Dauhajre*

Main category: cs.RO

TL;DR: 本论文提出了一种新的生成框架，可以大规模生成包含长期人机互动和家庭环境的三维数据集，提升了智能体在环境推理和交互领域的数据基础。该框架通过自然语言配置，灵活生成符合需求的多样化数据。实验结果表明生成的数据集在多项统计指标上与真实数据高度一致，可有效支撑智能家庭设备的开发和测试。


<details>
  <summary>Details</summary>
Motivation: 现有用于长期人机互动的数据生成框架通常只提供单向的数据合成，未能有效建模人类行为与家庭环境之间的双向影响，导致生成的数据在用于开发和测试智能体时存在局限性。因此，亟需一种能够反映人类生活习惯与环境之间相互作用、易于规模化生成、且可自定义的数据生成工具。

Method: 作者提出了一个生成框架，将家庭环境的三维结构与长期人机互动进行松耦合联动生成。人设特征影响环境生成，环境语义结构反过来又塑造人机互动。工具支持用户通过自然语言描述环境和行为需求，自动生成多样化环境和活动数据。同时，采用多模态嵌入、余弦相似度、互信息增益、干预分析等多种统计方法对生成数据的真实性和多样性进行验证。

Result: 统计分析显示，该生成框架产出的数据与真实世界数据集（HOMER）在多项指标上表现良好，余弦相似度为0.60，显著高于其他合成数据集（0.27）。干预实验（如不同年龄、整理习惯、作息规律）均产生统计学上显著（p<0.001）、效果量较大的行为和环境差异，体现了框架对人设-环境双向影响的有效性。

Conclusion: 该生成框架有效填补了长期人机互动与环境联合建模的数据缺口，所生成数据高度与真实场景一致，对推动家庭智能设备与环境智能体的开发与测试具有重要意义。

Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.
  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.
  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.

</details>


### [317] [aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones](https://arxiv.org/abs/2602.07264)
*Jacopo Panerati,Sina Sajjadi,Sina Soleymanpour,Varunkumar Mehta,Iraj Mantegh*

Main category: cs.RO

TL;DR: 本文介绍了aerial-autonomy-stack，这是一套开源端到端框架，用于提升无人机从感知到控制的自主能力，并极大加快了开发迭代速度。


<details>
  <summary>Details</summary>
Motivation: 无人机在多个领域的应用迅速扩展，而提升自主性是提升其效率和可靠性的关键。当前物理AI面临从仿真到现实的鸿沟，硬件及软件高度异构的集成复杂性亟需解决。

Method: 作者提出了aerial-autonomy-stack，一套开源的端到端系统，支持ROS2、PX4和ArduPilot，并能进行GPU加速感知至飞控动作的开发仿真，提供统一接口，加速仿真与测试。

Result: 实验证明该框架能实现比真实时间快20倍以上的端到端仿真，涵盖了边缘计算和网络通信，极大压缩了自主系统的开发到部署周期。

Conclusion: aerial-autonomy-stack有效简化了无人机自主系统从开发、测试到部署的流程，有助于推动物理AI及无人机应用的实际落地。

Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term "simulation-to-reality gap". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.

</details>


### [318] [Action-to-Action Flow Matching](https://arxiv.org/abs/2602.07322)
*Jindou Jia,Gen Li,Xiangyu Chen,Tuo An,Yuxuan Hu,Jingliang Li,Xinying Guo,Jianfei Yang*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖的机器人控制策略——Action-to-Action flow matching（A2A），通过利用机器人的历史动作数据替代随机噪声初始化，大幅提升了推理速度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略需要多步去噪造成实时控制延迟，成为应用瓶颈。作者希望通过更有效的动作初始化方式，解决高延迟问题，提高控制效率。

Method: A2A方法用历史本体动作序列编码为高维空间向量，作为动作生成的起点，避免从无关高斯噪声采样。这种有信息的初始化同时保持了物理动态和动作时序连续性，减少了去噪迭代次数。

Result: 大量实验表明，A2A方法在训练效率、推理速度和泛化能力上都优于现有扩散模型。尤其是只需一次推理（0.56ms延迟）即可生成高质量动作，对视觉扰动和新配置具有优秀鲁棒性。

Conclusion: A2A大幅降低了机器人控制推理延迟，提高了效率和适应能力，还能扩展到视频生成等序列建模任务，展示出广泛的应用前景。

Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.

</details>


### [319] [Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing](https://arxiv.org/abs/2602.07326)
*Edgar Lee,Junho Choi,Taemin Kim,Changjoo Nam,Seokhwan Jeong*

Main category: cs.RO

TL;DR: 本文提出了一种在极简感知条件下实现多指抓取的机器人方案，仅依靠单轴指尖力反馈和关节本体感受而无需视觉或高分辨率触觉传感器，实现了可靠的抓取表现。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，视觉和高分辨率触觉传感器虽然能够提升抓取能力，但存在成本高、易损坏、集成复杂等问题。因此，如何在感知受限的条件下实现鲁棒的抓取，是机器人抓取研究中的瓶颈和挑战。

Method: 采用教师-学生训练体系：教师在仅限于仿真环境中，利用强化学习和更充分的观测信息生成示范动作；学生以仅限实际硬件能获取的部分观测（单轴力+本体反馈）为输入，通过模仿学习（transformer策略）学习教师策略能力，最终实现“不看”情况下的抓取。

Result: 在真实机器人平台和包含18种物体的实验中（覆盖训练内外分布样本），方法取得了98.3%的抓取成功率，展示了极强的鲁棒性和泛化能力。

Conclusion: 该方法大幅降低了对高成本感知的依赖，实现了高效且普适的机器人抓取，为实际部署提供了新思路。

Abstract: Grasping under limited sensing remains a fundamental challenge for real-world robotic manipulation, as vision and high-resolution tactile sensors often introduce cost, fragility, and integration complexity. This work demonstrates that reliable multifingered grasping can be achieved under extremely minimal sensing by relying solely on uniaxial fingertip force feedback and joint proprioception, without vision or multi-axis/tactile sensing. To enable such blind grasping, we employ an efficient teacher-student training pipeline in which a reinforcement-learned teacher exploits privileged simulation-only observations to generate demonstrations for distilling a transformer-based student policy operating under partial observation. The student policy is trained to act using only sensing modalities available at real-world deployment. We validate the proposed approach on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, achieving a 98.3~$\%$ overall grasp success rate. These results demonstrate strong robustness and generalization beyond the simulation training distribution, while significantly reducing sensing requirements for real-world grasping systems.

</details>


### [320] [UEREBot: Learning Safe Quadrupedal Locomotion under Unstructured Environments and High-Speed Dynamic Obstacles](https://arxiv.org/abs/2602.07363)
*Zihao Xu,Runyu Lei,Zihao Li,Boxi Lin,Ce Hao,Jin Song Dong*

Main category: cs.RO

TL;DR: UEREBot是一种四足机器人层次化控制框架，将规划与即时反应结合，实现更高效安全地在复杂环境中规避障碍并完成任务。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人在非结构化环境中无法兼顾目标进展、地形可通过性和对高速动态障碍的即时避让，纯规划或纯反应方案均有局限性。

Method: 提出了UEREBot层次化控制框架，将慢速全局规划与即时反射性规避分离，通过空间-时间规划器指引路线及威胁，威胁感知融合导航与反射行为，最终由控制屏障函数保障安全执行。

Result: 在Isaac Lab仿真和Unitree Go2实机测试中，UEREBot在复杂静态结构和高速动态障碍环境下展现出比主流方法更高的避障成功率和更稳定的行走，同时保持目标进展。

Conclusion: UEREBot有效地平衡了安全性与任务进展，优于现有四足机器人控制方案，适合在复杂非结构化环境下部署。

Abstract: Quadruped robots are increasingly deployed in unstructured environments. Safe locomotion in these settings requires long-horizon goal progress, passability over uneven terrain and static constraints, and collision avoidance against high-speed dynamic obstacles. A single system cannot fully satisfy all three objectives simultaneously: planning-based decisions can be too slow, while purely reactive decisions can sacrifice goal progress and passability. To resolve this conflict, we propose UEREBot (Unstructured-Environment Reflexive Evasion Robot), a hierarchical framework that separates slow planning from instantaneous reflexive evasion and coordinates them during execution. UEREBot formulates the task as a constrained optimal control problem blueprint. It adopts a spatial--temporal planner that provides reference guidance toward the goal and threat signals. It then uses a threat-aware handoff to fuse navigation and reflex actions into a nominal command, and a control barrier function shield as a final execution safeguard. We evaluate UEREBot in Isaac Lab simulation and deploy it on a Unitree Go2 quadruped equipped with onboard perception. Across diverse environments with complex static structure and high-speed dynamic obstacles, UEREBot achieves higher avoidance success and more stable locomotion while maintaining goal progress than representative baselines, demonstrating improved safety--progress trade-offs.

</details>


### [321] [Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2602.07388)
*Yuxuan Hu,Xiangyu Chen,Chuhao Zhou,Yuxi Liu,Gen Li,Jindou Jia,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的Trace-Focused Diffusion Policy (TF-DP)，通过将机器人历史执行轨迹与当前视觉观测结合，有效解决了长期任务中由于视觉相似性导致动作多模态歧义（MA2）的问题。实验表明，该方法显著提升了机器人操作的时序一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在长时序机器人操作任务中，相似的视觉观测可能对应不同的操作，需要更丰富的上下文来确定正确动作。以往策略仅依赖当前观测，导致动作预测存在多模态歧义（MA2），降低了操作的准确性和鲁棒性。

Method: 提出Trace-Focused Diffusion Policy (TF-DP)，该方法将历史运动轨迹编码为显式执行轨迹，并将其投影到视觉观测空间，与当前观测一起用于动作生成。同时，利用trace-focused field强化与历史运动相关的任务关键区域，提升对视觉干扰的鲁棒性。

Result: 实验证明，在存在较强多模态动作歧义和复杂视觉干扰的真实机器人操作任务中，TF-DP方法在时序一致性和鲁棒性方面均大幅优于基础的diffusion policy方法。在多模态歧义任务上性能提升80.56%，在视觉干扰下提升86.11%，仅带来6.4%的推理时长增加。

Conclusion: 通过引入与执行轨迹相关的条件，TF-DP能够以单一策略实现对长时序机器人操作任务的高鲁棒性和可扩展性，为解决长期任务中的动作歧义问题提供了有效而通用的方法。

Abstract: Generative model-based policies have shown strong performance in imitation-based robotic manipulation by learning action distributions from demonstrations. However, in long-horizon tasks, visually similar observations often recur across execution stages while requiring distinct actions, which leads to ambiguous predictions when policies are conditioned only on instantaneous observations, termed multi-modal action ambiguity (MA2). To address this challenge, we propose the Trace-Focused Diffusion Policy (TF-DP), a simple yet effective diffusion-based framework that explicitly conditions action generation on the robot's execution history. TF-DP represents historical motion as an explicit execution trace and projects it into the visual observation space, providing stage-aware context when current observations alone are insufficient. In addition, the induced trace-focused field emphasizes task-relevant regions associated with historical motion, improving robustness to background visual disturbances. We evaluate TF-DP on real-world robotic manipulation tasks exhibiting pronounced multi-modal action ambiguity and visually cluttered conditions. Experimental results show that TF-DP improves temporal consistency and robustness, outperforming the vanilla diffusion policy by 80.56 percent on tasks with multi-modal action ambiguity and by 86.11 percent under visual disturbances, while maintaining inference efficiency with only a 6.4 percent runtime increase. These results demonstrate that execution-trace conditioning offers a scalable and principled approach for robust long-horizon robotic manipulation within a single policy.

</details>


### [322] [Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity](https://arxiv.org/abs/2602.07413)
*Yunhai Han,Linhao Bai,Ziyu Xiao,Zhaodong Yang,Yogita Choudhary,Krishna Jha,Chuizheng Kong,Shreyas Kousik,Harish Ravichandar*

Main category: cs.RO

TL;DR: 提出了一种新的机器人操作技能学习框架Unified Behavioral Models (UBMs)，通过动力系统方法统一视觉与机器人自身状态流，解决了现有方法反应性和时序性的权衡问题，并提出基于Koopman算子的具体模型K-UBM，实现了高效、平滑、鲁棒和可在线重新规划的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 当前利用扩散模型和Transformer等方法学习高阶操作技能需要海量数据、算力，且多指灵巧操作中效果并不可靠，核心因为这些方法都是简单的反应式策略、用定长action片段来缓解抖动，难以兼顾时序一致与响应灵敏性。

Method: 作者提出UBM，将操作技能建模为视觉-本体感知状态联合流的耦合动力系统。引入Koopman-UBM，借助Koopman算子理论，将视觉与感知特征共同嵌入递推线性系统里，通过分析性的方式，根据初始状态直接计算机器人整个操作时域上的动作和预期视觉变化；还引入在线再规划机制，实时检测预测与真实视觉流的偏差，必要时自动再规划。

Result: 在7个仿真任务和2个实际任务上，K-UBM与最新方法相比，达到了相当或更优的表现，并且推断速度更快，动作更平滑，对视觉遮挡更鲁棒，重规划能力更灵活。

Conclusion: UBM框架尤其K-UBM模型能高效、统一、鲁棒地学习和执行复杂灵巧操作技能，兼顾时序一致与实时响应性，是当前复杂机器人操作任务中有前景的方法。

Abstract: There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.

</details>


### [323] [Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots](https://arxiv.org/abs/2602.07434)
*Songhua Yang,Xuetao Li,Xuanye Fei,Mengde Li,Miao Li*

Main category: cs.RO

TL;DR: 本论文提出了SeM^2框架，实现了情感丰富且协调的语音、面部表情及手势多模态表达，涵盖云端和本地高效部署。


<details>
  <summary>Details</summary>
Motivation: 当前多数仿人机器人在语音、面部表情和肢体动作的情感协调表达方面存在明显短板。同时，实际应用中需要机器人系统能脱离云端，独立高效运行。

Method: 提出基于视觉语言模型的SeM^2系统，包括多模态感知模块（感知用户上下文）、链式思维推理模块（生成互动计划）、以及新颖的语义序列对齐机制（协调语音和动作时序）。系统有云端和边缘两种版本，边缘版通过知识蒸馏提升本地推理效率。

Result: 综合评测表明，该方法在自然性、情感清晰度和多模态协调性方面，相较单一模态基线显著提升，并且边缘版保留了95%的云端性能。

Conclusion: SeM^2框架推进了社会化表达型机器人发展，为多场景下的自主情感交互提供了有效方案。

Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.

</details>


### [324] [TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control](https://arxiv.org/abs/2602.07439)
*Weiji Xie,Jiakun Zheng,Jinrui Han,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了TextOp，一个支持实时文本驱动的人形机器人运动生成与控制系统，实现了用文本指令动态操控机器人复杂动作。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人控制方式受限于预定义轨迹或人工即时操作，灵活性和自主性不足，难以满足交互式、多变的用户意图。

Method: TextOp采用两级架构：高级自回归运动扩散模型根据文本实时生成短时运动轨迹；低级运动追踪策略负责在真实机器人上执行这些轨迹，并支持流式文本指令和中途动态修改。

Result: 在大量真实机器人实验和离线测试中，TextOp实现了即时响应、动作平滑和精确控制，能够在连续运动中顺畅地切换多种复杂行为。

Conclusion: TextOp通过结合交互式指令生成与稳健全身运动控制，大幅提升了人形机器人运动表达的自由度和实用性，是推动机器人自然交互和应用的重要进展。

Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/

</details>


### [325] [VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots](https://arxiv.org/abs/2602.07506)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Yang Zhang*

Main category: cs.RO

TL;DR: VividFace系统实现了类人机器人对人类面部表情的实时、逼真模仿，在0.05秒内完成动作转移，并经过实测验证。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人表情模仿系统通常在实时性和表情真实度上表现不足，主要因为推理流程离线化和表情细节捕捉与迁移能力有限，影响了机器人在人机交互中的情感表达效果。

Method: 提出VividFace系统，基于优化的X2CNet++模仿框架，改进了人-机器人面部动作迁移模块，并引入特征适配训练策略，提升不同图像源的一致性。同时采用支持视频流推理的管线和异步I/O，提高系统整体实时性和效率。

Result: VividFace能在0.05秒内模仿复杂且细致的人类面部表情，对多样化面部结构有良好泛化能力，并通过大量实际应用演示验证其实用性。

Conclusion: VividFace有效实现了面部表情的高效实时迁移，显著提升了类人机器人的表情表达能力，为情感化人机交互奠定基础。

Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.

</details>


### [326] [Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning](https://arxiv.org/abs/2602.07541)
*Jingyi Hou,Leyu Zhou,Chenchen Jing,Jinghan Yang,Xinbo Yu,Wei He*

Main category: cs.RO

TL;DR: 论文提出了一种新的VLA模型增强框架iSTAR，通过在模型参数内嵌入任务语义结构，有效提升了机器人任务推理与执行能力。实验显示，iSTAR在多项操作基准任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在任务级推理方面存在短板，依赖于不稳定的prompt分解或高成本的端到端训练，难以实现高效的任务分解和泛化能力。研究动机是提升机器人模型在复杂任务中的结构化推理能力。

Method: 提出iSTAR框架，通过将任务级语义结构直接注入到模型参数中，结构化建模场景对象关系、子任务语义和依赖关系，实现参数空间内的功能化分化和任务级推理，无需外部planner或人工prompt。

Result: 在多种机器人操作基准上，iSTAR在任务分解稳定性和成功率方面优于依赖prompt的分解方法和端到端VLA系统，证明了参数空间结构化推理的有效性。

Conclusion: iSTAR能显著提升VLA模型在复杂任务中的推理能力和泛化性，为机器人的高层规划和多任务学习提供了更优秀的解决方案。

Abstract: As robots are expected to perform increasingly diverse tasks, they must understand not only low-level actions but also the higher-level structure that determines how a task should unfold. Existing vision-language-action (VLA) models struggle with this form of task-level reasoning. They either depend on prompt-based in-context decomposition, which is unstable and sensitive to linguistic variations, or end-to-end long-horizon training, which requires large-scale demonstrations and entangles task-level reasoning with low-level control. We present in-parameter structured task reasoning (iSTAR), a framework for enhancing VLA models via functional differentiation induced by in-parameter structural reasoning. Instead of treating VLAs as monolithic policies, iSTAR embeds task-level semantic structure directly into model parameters, enabling differentiated task-level inference without external planners or handcrafted prompt inputs. This injected structure takes the form of implicit dynamic scene-graph knowledge that captures object relations, subtask semantics, and task-level dependencies in parameter space. Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines, demonstrating the effectiveness of parameter-space structural reasoning for functional differentiation and improved generalization across task variations.

</details>


### [327] ["Meet My Sidekick!": Effects of Separate Identities and Control of a Single Robot in HRI](https://arxiv.org/abs/2602.07598)
*Drake Moore,Arushi Aggarwal,Emily Taylor,Sarah Zhang,Taskin Padir,Xiang Zhi Tan*

Main category: cs.RO

TL;DR: 本论文研究了机器人展示不同身份时，人类对其感知及信任的影响，尤其关注一个物理机器人不同部件同时作为独立机器人的情境。通过实验比较‘单一机器人’、‘双主体共控’和‘双主体分控’三种模式，分析人机任务过程中对机器人的身份归属和故障关联。结果显示人类可将机器人失效准确关联到各自的身份和部件，验证了多身份配置的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人多以一致身份出现，但机器人具备将不同功能或控制域呈现为独立身份的潜能。理解人类如何感知和信任带有多重身份（尤其是分别控制不同硬件部分）的机器人，有助于设计更易接受、更具协作力的多能力机器人。

Method: 采用混合实验设计，让参与者体验‘单一机器人’、‘共控双主体’和‘分控双主体’三种身份呈现。实验任务涵盖数据录入、物品分类（含机器人独立故障）、以及协作排列（含与人相关的机器人故障），记录参与者对身份的感知及对故障的归因。

Result: 参与者能准确识别机器人在不同部件的独立身份，并能有效将异常事件或故障归因于对应的身份与控制域。分控与共控模式在感知上均显示出较高的多身份辨识度。

Conclusion: 多身份、多主体配置可通过单一机器人体实现，并且用户能够对这种划分做出良好感知和归因。该结果表明未来机器人可以采用多身份化设计，在提升任务能力和灵活性的同时，减少多实体硬件负担，增强人机协作体验。

Abstract: The presentation of a robot's capability and identity directly influences a human collaborator's perception and implicit trust in the robot. Unlike humans, a physical robot can simultaneously present different identities and have them reside and control different parts of the robot. This paper presents a novel study that investigates how users perceive a robot where different robot control domains (head and gripper) are presented as independent robots. We conducted a mixed design study where participants experienced one of three presentations: a single robot, two agents with shared full control (co-embodiment), or two agents with split control across robot control domains (split-embodiment). Participants underwent three distinct tasks -- a mundane data entry task where the robot provides motivational support, an individual sorting task with isolated robot failures, and a collaborative arrangement task where the robot causes a failure that directly affects the human participant. Participants perceived the robot as residing in the different control domains and were able to associate robot failure with different identities. This work signals how future robots can leverage different embodiment configurations to obtain the benefit of multiple robots within a single body.

</details>


### [328] [LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation](https://arxiv.org/abs/2602.07629)
*Nitesh Subedi,Adam Haroon,Samuel Tetteh,Prajwal Koirala,Cody Fleming,Soumik Sarkar*

Main category: cs.RO

TL;DR: 提出了一种新的视觉-语言导航框架LCLA，通过将感知与控制部分解耦，提高了导航任务的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言导航任务中，感知和控制通常紧耦合，限制了模型在不同环境与模态下的泛化和重用能力。需要一种方法分离感知与控制，从而提升幻想能力，并简化系统设计和训练。

Method: 首先用有特权状态信息训练专家策略，得到可控性充足的潜在空间并冻结。然后，训练一个轻量级适配器，将由冻结的视觉-语言模型得到的视觉-语言观测映射到专家的潜在空间，将感知-控制问题转化为潜在空间对齐的监督学习。这样可以知识重用并提高模块泛化性。

Result: 在室内视觉-语言导航任务上，LCLA方法在分布内测试中表现优秀，并且在零样本测试下（包括不同环境、光照及视角）保持了较强的泛化能力。同时推理效率高，计算量小。

Conclusion: LCLA通过潜在空间对齐，有效地解耦了感知与控制，提升了视觉-语言导航任务的泛化能力和推理效率。

Abstract: We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.

</details>


### [329] [Affine Transformable Unmanned Ground Vehicle](https://arxiv.org/abs/2602.07677)
*Aron Mathias,Mohammad Ghufran,Jack Hughes,Hossein Rastgoftar*

Main category: cs.RO

TL;DR: 本文提出了一种新型可进行仿射变形的无人地面车辆（ATUGV），能够在携带多个载荷时安全地进行激进变形，并通过实验和仿真验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有无人地面车辆在携带多载荷和变形能力方面存在局限。为实现多载荷同时运输且能适应复杂环境，需要一种安全、可控、可变形的新型机器人结构。

Method: 提出一种多体系统ATUGV，包括可驱动的移动机器人单元、动力/无动力载荷单元及可变形结构。通过深度神经网络设计单元连接结构，使所有单元能共同行进实现仿射变换。采用移动机器人和步进电机来调节各单元连接，设计相应控制方法以安全实现目标仿射变形。

Result: 通过硬件实验和仿真，验证了ATUGV能够安全地跟踪所需仿射变换，实现了多载荷同时运输与结构变形的功能。

Conclusion: 所提出的ATUGV结构及控制方法有效实现了携载多载荷条件下安全、可控的仿射变形，为多功能无人地面车辆的应用奠定基础。

Abstract: This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.

</details>


### [330] [Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples](https://arxiv.org/abs/2602.07736)
*Omar Tahri*

Main category: cs.RO

TL;DR: 本文提出利用几何矩检测对象对称性，从而提高抓取操作的稳定性和效率。方法支持2D与3D场景，并结合优化方法提升多平面对称检测的准确性与计算速度。


<details>
  <summary>Details</summary>
Motivation: 检测对称性可以帮助选择更稳健的抓取方式，对对称轴的识别对于提升机械手抓取的平衡性和成功率至关重要。但现有方法在高维空间和效率上存在局限。

Method: 本文利用几何矩来识别对象的对称性和估计正交变换（如旋转与镜像变换），通过在n维空间推导moment n元组的方法，能同时检测旋转、反射及其组合。开展了2D和3D实验，验证方法的稳健性。

Result: 方法在检测对称性和估计正交变换方面效果可靠。与基于迭代优化的先进方法对比，结合两者能提升多对称面的检测数量和减少计算时间。

Conclusion: 提出的基于几何矩的方法能够在高维空间准确高效识别对象对称性，并与现有优化方法结合后进一步提高了性能，适合实际应用于复杂对象的抓取任务。

Abstract: Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \( n \)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.

</details>


### [331] [CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport](https://arxiv.org/abs/2602.07776)
*Joachim Yann Despature,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本研究提出了一种面向视觉-语言引导的多机器人协作搬运任务的方法，解决因机器人视角和语言模糊导致协作失效的问题。核心方法是引入Consistent Leader-Follower（CoLF）多智能体强化学习框架，通过领导者-跟随者分工，实现更稳定一致的协作搬运。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在根据自然语言指令进行协作搬运时，常常会因各机器人视觉观测角度不同及语言歧义，产生认知偏差，影响协作效果。现有对称角色多智能体方法容易导致协作不稳定，因此亟需一种能促使角色分化、提升协作一致性的新方法。

Method: 提出了CoLF多智能体强化学习框架。具体包括两个核心创新：（1）引入非对称策略设计，使智能体间自然分为领导者和跟随者；（2）采用基于互信息的目标，通过变分下界训练，引导跟随者依据自身观测预测领导者动作。整个系统采用集中训练、分布执行（CTDE）框架优化策略。

Result: CoLF框架在仿真和真实四足机器人协作搬运实验中均得到了验证，实验表明该方法能够有效提升多机器人系统在语言引导下的协作稳定性和搬运表现。

Conclusion: 引入明确角色分工并结合互信息训练目标，可以显著缓解多机器人因视角和语言理解偏差带来的协作障碍，为语言-视觉引导的机器人协作任务提供了一种高效的新思路。

Abstract: In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.

</details>


### [332] [RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI](https://arxiv.org/abs/2602.07837)
*Hongzhi Zang,Shu'ang Yu,Hao Lin,Tianxing Zhou,Zefang Huang,Zhen Guo,Xin Xu,Jiakai Zhou,Yuze Sheng,Shizhe Zhang,Feng Gao,Wenhao Tang,Yufeng Yue,Quanlu Zhang,Xinlei Chen,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出了USER，一个面向现实世界在线策略学习的统一且可扩展的系统平台，使多机器人和复杂模型在物理世界中高效协作训练。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的策略学习面临着数据收集效率低、异构部署难、长期训练不易等独特挑战，这些不仅是算法层面的问题，更是系统工程难题。作者希望通过构建系统来解决这些瓶颈。

Method: USER将物理机器人作为与GPU同等地位的硬件资源，通过统一的硬件抽象层管理异构机器人，实现自动发现、调度。提出适应性通信层，优化边缘-云通信，并使用持久、缓存感知的异步缓冲区进行高效长时实验。同时系统支持奖励、算法、策略的可扩展抽象，可在线训练多样模型。

Result: USER在仿真和真实环境中实现了多机器人协调、异构机械臂协作、大模型的边缘-云协同训练与长期异步训练，验证了系统具有强大的统一性、可扩展性和实用性。

Conclusion: USER为现实世界的在线策略学习提供了系统级基础支撑，使多机器人大模型能高效在实际环境中训练与协作，为智能体技术走向现实应用奠定基础。

Abstract: Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.

</details>


### [333] [Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning](https://arxiv.org/abs/2602.07845)
*Yalcin Tur,Jalal Naghiyev,Haoquan Fang,Wei-Chuan Tsai,Jiafei Duan,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的视觉-语言-动作模型RD-VLA，能够根据任务难度动态分配计算资源，大幅提升了复杂任务的表现和推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在处理简单和复杂任务时，均采用固定的计算深度，导致计算资源浪费且效率低下。Chain-of-Thought方法虽支持动态计算，但记忆占用高且不适用于连续动作空间。因此，需要一种更高效、可适应任务复杂度的推理方法。

Method: 本文提出RD-VLA架构，采用递归、权重共享的action head，实现任意推理深度且内存占用恒定。模型训练时使用截断反向传播（TBPTT）高效监督模型迭代推理过程，推理阶段通过检测潜在状态收敛动态决定停止迭代，实现自适应计算分配。

Result: 在复杂操作任务上，单迭代推理完全失败（0%成功率），而采用4次迭代后成功率超90%；对于简单任务，模型能够快速饱和。与基于token的推理方式相比，RD-VLA可实现高达80倍推理速度提升并保证恒定内存消耗。

Conclusion: RD-VLA通过潜在空间递归推理有效提升了VLA模型在复杂任务下的效率和表现，在保持低内存消耗的同时，实现了动态的计算资源分配，为机器人推理提供了可扩展的路径。

Abstract: Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/

</details>


### [334] [System-Level Error Propagation and Tail-Risk Amplification in Reference-Based Robotic Navigation](https://arxiv.org/abs/2602.07846)
*Ning Hu,Maochen Li,Senhao Cao*

Main category: cs.RO

TL;DR: 本文分析了在双平面X光图像引导的机器人导航系统中，由于安装产生的结构扰动如何在几何感知流程中被放大，影响系统的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在临床机器人导航等安全关键场景中，系统的空间定位精度至关重要。尽管当前多阶段感知流程具有实时性和几何可解释性，但很少关注由安装误差导致的系统级误差放大机制。作者希望揭示此类结构性误差源对整个系统的影响，以提升导航系统的安全性和设计合理性。

Method: 作者建立了一个统一的误差传播建模框架，分析了安装引起的结构扰动如何通过双平面成像、投影矩阵估计、三角测量与坐标映射，在涉及像素级噪声时进一步耦合和放大。具体采用一阶不确定性传播和蒙特卡洛仿真，定量分析系统最敏感的误差通道和极端误差情况，并通过实际实验验证。

Result: 研究发现，旋转型的安装误差是误差放大的主要驱动因素，而同等幅度的平移误差在典型的双平面几何下影响较小。实测结果与理论趋势一致，证实了理论模型的有效性。

Conclusion: 参考基、多阶段的几何感知流程普遍存在结构性误差放大的风险。本文的框架可用于系统级可靠性分析和风险感知设计，有助于提升安全关键型机器人导航系统的鲁棒性和可信度。

Abstract: Image guided robotic navigation systems often rely on reference based geometric perception pipelines, where accurate spatial mapping is established through multi stage estimation processes. In biplanar X ray guided navigation, such pipelines are widely used due to their real time capability and geometric interpretability. However, navigation reliability can be constrained by an overlooked system level failure mechanism in which installation induced structural perturbations introduced at the perception stage are progressively amplified along the perception reconstruction execution chain and dominate execution level error and tail risk behavior. This paper investigates this mechanism from a system level perspective and presents a unified error propagation modeling framework that characterizes how installation induced structural perturbations propagate and couple with pixel level observation noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Using first order analytic uncertainty propagation and Monte Carlo simulations, we analyze dominant sensitivity channels and quantify worst case error behavior beyond mean accuracy metrics. The results show that rotational installation error is a primary driver of system level error amplification, while translational misalignment of comparable magnitude plays a secondary role under typical biplanar geometries. Real biplanar X ray bench top experiments further confirm that the predicted amplification trends persist under realistic imaging conditions. These findings reveal a broader structural limitation of reference based multi stage geometric perception pipelines and provide a framework for system level reliability analysis and risk aware design in safety critical robotic navigation systems.

</details>


### [335] [Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model](https://arxiv.org/abs/2602.07888)
*Ning Hu,Shuai Li,Jindong Tan*

Main category: cs.RO

TL;DR: 本文提出了一种基于平行透视近似的相机位姿估计方法，通过显式建模误差传播机制，有效提升了近距离场景下的稳健性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在近距离场景下，由于强透视效应和异质测量噪声，传统解析PnP方法的相机位姿估计稳定性下降。本文旨在解决该类环境下相机位姿估计的准确性与鲁棒性问题。

Method: 提出了几何误差传播框架，基于平行透视近似，推导了误差传递模型，并结合误差感知加权和高斯-牛顿优化，开发了新的位姿估计算法。

Result: 在强光、手术照明、水下低光等多种环境下，通过大量合成和真实数据验证，所提方法在准确性和鲁棒性上与主流PnP方法相当，且保持了较高的计算效率。

Conclusion: 显式的几何误差建模对提升近距离复杂场景中的相机位姿估计具有重要意义。该方法兼具高效性和稳健性，有望应用于严苛场景中的视觉定位问题。

Abstract: Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.

</details>


### [336] [Incremental Mapping with Measurement Synchronization & Compression](https://arxiv.org/abs/2602.07901)
*Mark Griguletskii,Danil Belov,Pavel Osinenko*

Main category: cs.RO

TL;DR: 提出了一种可动态构建因子图的新方法，实现多传感器数据的高效融合，同时通过压缩节点数量，降低优化变量数量，保持地图精度。


<details>
  <summary>Details</summary>
Motivation: 现有因子图方法难以高效处理多传感器异步数据，刚性拓扑结构与高频率数据冗余导致优化效率低下，影响地图精度与定位性能。

Method: 提出一种根据外部评估标准，动态增量式地构建最优连通因子图的方法，能够整合所有可用传感器数据，并支持因子图压缩来减少节点数量。

Result: 新方法实现平均约30%的节点压缩，使得因子图所需优化变量减少，同时地图质量与传统方法相当。

Conclusion: 该方法提升了多传感器异步数据融合的效率与因子图优化的可扩展性，为实现更高效、精确的机器人与自动驾驶定位与建图系统提供了理论与实践支撑。

Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.

</details>


### [337] [Multi-Agent Route Planning as a QUBO Problem](https://arxiv.org/abs/2602.07913)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: cs.RO

TL;DR: 该论文研究了多智能体路径规划问题，旨在在最大化道路网络覆盖范围的同时，限制重复覆盖。


<details>
  <summary>Details</summary>
Motivation: 多智能体在城市道路覆盖任务中路径的选择，关系到资源的高效利用和覆盖效果最大化，但过度重叠导致资源浪费。需优化路径以提升网络覆盖效率。

Method: 首先形式化定义了该问题，并通过与加权集合打包问题的归约证明其NP难。然后，提出了二次无约束二元优化（QUBO）建模思路，通过唯一覆盖奖励和成对重叠惩罚实现目标，并设置惩罚参数调节覆盖与重叠间权衡。最后，设计了从实例生成到QUBO建模与多种数值求解器联合的完整求解流程。

Result: 在巴塞罗那城市实例、最多1万台车辆下测试，发现覆盖率与重叠存在明显拐点，最优解多在高惩罚参数下取得。D-Wave混合量子退火和Gurobi两种求解器在目标值上表现基本一致，随着问题规模增长仅在运行时间上略有差异。

Conclusion: QUBO建模与高惩罚参数有助于生成覆盖最大且重叠最小的近最优路径方案。不同求解器均能有效处理大规模问题，为城市级多智能体路径优化提供了实用工具。

Abstract: Multi-Agent Route Planning considers selecting vehicles, each associated with a single predefined route, such that the spatial coverage of a road network is increased while redundant overlaps are limited. This paper gives a formal problem definition, proves NP-hardness by reduction from the Weighted Set Packing problem, and derives a Quadratic Unconstrained Binary Optimization formulation whose coefficients directly encode unique coverage rewards and pairwise overlap penalties. A single penalty parameter controls the coverage-overlap trade-off. We distinguish between a soft regime, which supports multi-objective exploration, and a hard regime, in which the penalty is strong enough to effectively enforce near-disjoint routes. We describe a practical pipeline for generating city instances, constructing candidate routes, building the QUBO matrix, and solving it with an exact mixed-integer solver (Gurobi), simulated annealing, and D-Wave hybrid quantum annealing. Experiments on Barcelona instances with up to 10 000 vehicles reveal a clear coverage-overlap knee and show that Pareto-optimal solutions are mainly obtained under the hard-penalty regime, while D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with only minor differences in runtime as problem size grows.

</details>


### [338] [Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities](https://arxiv.org/abs/2602.07924)
*Nur Ahmad Khatim,Mansur Arief*

Main category: cs.RO

TL;DR: 本文提出了一种用于石油基础设施安全保障的人-机器人联合调度选址模型（HRCD-FLP），综合设施分级、人-机器人监管比例及最低使用率等实际需求，权衡了自主系统高效性与人工判断。实验表明提升机器人监管比大幅降低成本，同时保持对关键基础设施的覆盖。小规模问题可用精确算法快速获得最优解，大规模问题则提出启发式算法，在合理时间内获得近优解。优化人-机器人协同规划对提升成本效益及任务可靠性具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 传统设施选址模型假设资源同质，无法满足石油基础设施的安全需求，尤其是在需要兼顾自主系统效率与人工决策时。实际中存在设施分级、监管比例限制和使用率等复杂约束，这需要新的建模与优化方法。

Method: 构建了带容量限制的人-机器人联合调度设施选址问题（HRCD-FLP）模型，考虑设施重要性分层、人-机器人监管比例和最小利用率约束。针对不同技术成熟度设定多种监管比例，并采用精确算法和启发式方法分别解决不同规模问题。

Result: 实验结果显示，从传统1:3扩大到1:10的监管比可显著降低成本，并保证关键基础设施全覆盖。小规模问题精确法解优且快，大规模用启发式算法3分钟内收敛，与最优值比约有14%差距。

Conclusion: 结果表明，优化的人-机器人团队协同规划对于实现高性价比和高可靠性的石油基础设施安全保障方案至关重要。

Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.

</details>


### [339] [Feasibility-Guided Planning over Multi-Specialized Locomotion Policies](https://arxiv.org/abs/2602.07932)
*Ying-Sheng Luo,Lu-Ching Wang,Hanjaya Mandala,Yu-Lun Chou,Guilherme Christmann,Yu-Chung Chen,Yung-Shun Chan,Chun-Yi Lee,Wei-Chao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种可行性引导的规划框架，将多个特定地形的策略集成到腿式机器人在复杂地形上的路径规划中，并通过实验验证能高效、可靠地产生适应不同地形的规划。


<details>
  <summary>Details</summary>
Motivation: 针对传统路径规划无法整合技能策略，以及分层学习方法可解释性差且难以扩展的局限，亟需一种既能集成多策略，又能灵活适应新技能的规划方案。

Method: 为每个地形特定策略配对一个Feasibility-Net，基于局部高程图和任务向量预测可行性，然后用经典规划算法结合这些信息进行全局最优路径生成。

Result: 通过仿真和真实机器人实验，结果表明该方法能在各种复杂地形下高效生成兼容底层策略能力的可靠路径规划方案。

Conclusion: 文中方法突破了多技能集成和自动扩展的难题，提高了腿式机器人在非结构化地形上的规划适应能力，具有良好的实用价值。

Abstract: Planning over unstructured terrain presents a significant challenge in the field of legged robotics. Although recent works in reinforcement learning have yielded various locomotion strategies, planning over multiple experts remains a complex issue. Existing approaches encounter several constraints: traditional planners are unable to integrate skill-specific policies, whereas hierarchical learning frameworks often lose interpretability and require retraining whenever new policies are added. In this paper, we propose a feasibility-guided planning framework that successfully incorporates multiple terrain-specific policies. Each policy is paired with a Feasibility-Net, which learned to predict feasibility tensors based on the local elevation maps and task vectors. This integration allows classical planning algorithms to derive optimal paths. Through both simulated and real-world experiments, we demonstrate that our method efficiently generates reliable plans across diverse and challenging terrains, while consistently aligning with the capabilities of the underlying policies.

</details>


### [340] [Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control](https://arxiv.org/abs/2602.07984)
*Simon Sagmeister,Panagiotis Kounatidis,Sven Goblirsch,Markus Lienkamp*

Main category: cs.RO

TL;DR: 论文探讨了车辆动力学建模精度对自主驾驶轨迹跟踪控制器闭环行为的影响，并通过多种精度模型进行对比评估。


<details>
  <summary>Details</summary>
Motivation: 不同研究在车辆动力学建模精度上存在差异，导致控制算法评估难以横向对比。论文旨在明确动力学模型不同精度对实际控制性能的影响，为控制算法测试提供参考。

Method: 开发了一个兼容Autoware的高精度车辆模型，并推导出一系列精度不同的简化模型。通过超过550组仿真实验，将不同模型与实际赛车数据进行对比，量化各自的近似质量，并考察在不同加速裕度下模型简化带来的影响。

Result: 通过大量仿真实验，定量分析了各精度模型与真实数据间的差异，揭示了在不同应用需求下模型可以简化的程度及对控制算法评估的影响。

Conclusion: 为控制算法评估提供了模型精度选择的依据，提出了根据具体应用场景可接受的简化程度建议，有助于提升仿真效率且保证评估准确性。

Abstract: Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.

</details>


### [341] [From Ellipsoids to Midair Control of Dynamic Hitches](https://arxiv.org/abs/2602.08116)
*Jiawei Xu,Subhrajit Bhattacharya,David Saldaña*

Main category: cs.RO

TL;DR: 本文提出了一种基于椭球体的动力学建模和控制方法，实现了多飞行器协作下对电缆交缠（hitch）结构的动态操控，并利用CLF-HOCBF-QP控制器实现了高精度、稳定地跟踪期望目标。


<details>
  <summary>Details</summary>
Motivation: 多飞行器带缆机构因其增强的灵活性，在空中抓取和操作任务中备受关注。如何动态操控缆绳之间的互绕（如打结、缠绕），提升负载操作和安全约束能力，是提升系统应用价值的关键难题。

Method: 作者提出了一种椭球体几何模型，描述缆绳由四架飞行器驱动下形成的缠结结构。通过分析系统的控制仿射特性，采用基于二次规划的CLF-HOCBF-QP控制器，结合李雅普诺夫函数和高阶控制阻碍函数实现对缆结位置与形状的精准跟踪和安全约束（例如缆绳张紧约束）。

Result: 通过数值仿真验证，该方法可以实现对动态期望缆结位置的高精度、快速、稳定跟踪，有效维护系统的安全性和合作性能。

Conclusion: 本文的建模与控制方法为多飞行器协作操控缆绳结构提供了理论基础和有效工具，在空中负载运输、复杂结构组装等应用场景有良好应用前景。

Abstract: The ability to dynamically manipulate interaction between cables, carried by pairs of aerial vehicles attached to the ends of each cable, can greatly improve the versatility and agility of cable-assisted aerial manipulation. Such interlacing cables create hitches by winding two or more cables around each other, which can enclose payloads or can further develop into knots. Dynamic modeling and control of such hitches is key to mastering the inter-cable manipulation in context of cable-suspended aerial manipulation. This paper introduces an ellipsoid-based kinematic model to connect the geometric nature of a hitch created by two cables and the dynamics of the hitch driven by four aerial vehicles, which reveals the control-affine form of the system. As the constraint for maintaining tension of a cable is also control-affine, we design a quadratic programming-based controller that combines Control Lyapunov and High-Order Control Barrier Functions (CLF-HOCBF-QP) to precisely track a desired hitch position and system shape while enforcing safety constraints like cable tautness. We convert desired geometric reference configurations into target robot positions and introduce a composite error into the Lyapunov function to ensure a relative degree of one to the input. Numerical simulations validate our approach, demonstrating stable, high-speed tracking of dynamic references.

</details>


### [342] [Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning](https://arxiv.org/abs/2602.08167)
*Milan Ganai,Katie Luo,Jonas Frey,Clark Barrett,Marco Pavone*

Main category: cs.RO

TL;DR: 该论文提出R&B-EnCoRe方法，通过自监督优化避免现有VLA模型链式推理模板的固有限制，有效提升了机器人感知-语言-动作决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型中的链式推理方法依赖于严格的推理模板，但这些模板会引入无关信息，干扰关键的动作预测，影响推理质量和策略学习效果。因此，需要有更自适应、无模板依赖的推理机制。

Method: R&B-EnCoRe将推理视为一个隐变量，并通过重要性加权的变分推断实现自监督优化。模型可无需外部奖励、人类标注或者规则校验器，自动生成和精炼面向具体任务的推理数据集，使推理与实际控制任务紧密结合。

Result: 在多种场景（机械臂操作、腿式导航、自动驾驶）和多个模型规模（1B、4B、7B、30B参数）上验证，R&B-EnCoRe相比于传统方法，操作成功率提升28%，导航得分提升101%，碰撞率降低21%。

Conclusion: R&B-EnCoRe实现了面向实际控制表现的推理蒸馏，将网络规模知识有效应用于机器人物理任务，减少了对人工设计和标注的依赖，为提升VLA模型实际应用能力提供新方向。

Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>


### [343] [Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments](https://arxiv.org/abs/2602.08189)
*Seoyeon Jang,Alex Junho Lee,I Made Aswin Nahrendra,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种用于移动机器人在线变化检测和长期地图维护的双头网络，并通过数据增强策略解决了真实世界数据获取难题。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法在频繁变化和遮挡的动态环境（如施工现场或频繁变动的室内空间）下效果有限，难以准确检测变化或及时更新地图。实际手动收集和配准地标数据的过程也极为繁琐且不可扩展，因此有必要开发新的方法提升自动化水平。

Method: 提出采用双头网络结构同时实现变化检测和地图维护。针对现实中真实数据难以收集的问题，设计了一种数据增强策略，能够通过不同场景元素的引入合成结构变化，无需大量人工标注即可高效训练模型。

Result: 在真实世界的施工场景和室内办公环境下，实验结果显示所提方法具有良好的泛化能力和高效、准确的地图更新性能。

Conclusion: 本方法有效提升了动态环境下机器人变化检测和地图维护的自动化程度和准确性，有望广泛应用于多种实际场景。

Abstract: Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.

</details>


### [344] [STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction](https://arxiv.org/abs/2602.08245)
*Jinhao Li,Yuxuan Cong,Yingqiao Wang,Hao Xia,Shan Huang,Yijia Zhang,Ningyi Xu,Guohao Dai*

Main category: cs.RO

TL;DR: 本文提出了STEP方法，有效解决了扩散策略在机器人视觉运动控制中推理延时高的问题，在保证动作质量的同时大幅降低延时，并在多个基准和实际任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 扩散策略因能建模动作序列分布和多模态特性，在机器人操作中表现出色。但其逐步去噪导致推理延时高，限制实时闭环系统的控制频率，现有加速方法往往难以兼顾动作质量和低延时。因此需要新的方法提升推理效率并保持性能。

Method: STEP方法包括两个核心机制：一是空间-时间一致性预测，生成高质量且时序一致的初始动作，分布接近目标动作，同时不损失扩散模型的生成能力；二是速度感知扰动注入机制，根据动作变化自适应调节激励，防止执行停滞。并给出了预测机制收敛性的理论分析。

Result: 在9个模拟任务和2个真实任务中的大量评估显示，STEP只需2步即可比BRIDGER和DDIM分别在RoboMimic基准与真实任务上平均提升21.6%和27.5%的成功率。

Conclusion: STEP能够在保持高动作质量的同时，大幅降低推理延时，在现有方法中达到了推理延时与成功率的最优权衡，推进了该领域实际应用性能的前沿。

Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>


### [345] [Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control](https://arxiv.org/abs/2602.08251)
*Yuanzhu Zhan,Yufei Jiang,Muqing Cao,Junyi Geng*

Main category: cs.RO

TL;DR: 本论文提出了一套无人机自主完成复杂接触任务的全自主感知与控制系统，无需外部动作捕捉，实现精确稳定的操作。


<details>
  <summary>Details</summary>
Motivation: 现有的空中操作多依赖外部动作捕捉系统，且主要解决粗略位置控制，难以推广到真实环境中的精细、高接触任务。

Method: 提出了两大核心方法：（1）增强型视觉-惯导里程计（VIO），在接触时启用一致性因子提高定位精度；（2）基于图像的视觉伺服和混合力-运动控制器，实现精准的接触力和横向运动控制。

Result: 实验表明，该方法仅用机载传感器即可有效闭合“感知-作用力”回路，使接触时速度估计误差降低66.01%，能稳定实现目标靠近与持力操作。

Conclusion: 本系统实现了无需外部设备的高精度空中操作，为无人机在野外等实际环境中的高接触任务部署奠定基础。

Abstract: Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>


### [346] [Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes](https://arxiv.org/abs/2602.08266)
*Seunghoon Jeong,Eunho Lee,Jeongyun Kim,Ayoung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种基于对象感知的3D高斯溅射（3DGS）和新颖的下一最佳视角（NBV）策略，用于在杂乱、遮挡场景下高效选择信息量大的视角并提升三维重建质量，特别适应多对象任务和目标物体的操作需求，显著降低深度重建误差。


<details>
  <summary>Details</summary>
Motivation: 在多物体复杂场景中，遮挡和观察不完全常常影响三维重建的准确性。现有的NBV方法多基于几何信息，缺乏语义和对象相关性，难以针对目标物体或未被充分观测区域进行有效探索与重建，因此需要引入对象感知和更优的NBV策略以改善任务表现。

Method: 提出“对象感知的3DGS NBV策略”，利用对象级特征（一热矢量）及置信度加权信息增益来引导视角选择，有效锁定易错和不确定的高斯区域。方法可适配于场景级和对象级NBV，实现针对性重建并提升健壮性。

Result: 在合成数据集和真实GraspNet数据集上，该方法相较基线能分别降低77.14%和34.10%的深度误差。进一步针对特定物体实施NBV，相较全场景NBV，该物体的深度误差还能再降低25.60%。

Conclusion: 方法有效提升复杂场景中多对象三维重建与机器人操作任务的可靠性和精度，特别在物体遮挡和对象目标化操作方面优势明显，有较好的实际应用价值。

Abstract: In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.

</details>


### [347] [DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer](https://arxiv.org/abs/2602.08278)
*Ke Zhang,Lixin Xu,Chengyi Song,Junzhe Xu,Xiaoyi Lin,Zeyu Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: DexFormer是一种用于灵巧操作的端到端跨机器人手型策略模型，通过基于Transformer的网络和历史观测，能在不同手型间泛化和即时适应，提升通用操控能力。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作对控制高自由度机械手在复杂接触动力学环境中的能力要求极高，但现有方法面对不同机械手时需要单独训练、或依赖定制的解码器来适配结构差异，导致泛化性差、扩展性低。因此，需要一种能够跨不同手型自适应泛化的统一策略。

Method: 提出DexFormer模型，基于改进版Transformer架构，利用历史观测信息动态推断当前机械手的形态和动力学特征，实现对不同手型适应的动作预测。该方法在大量程序生成的灵巧机械手上进行训练，使模型具备广泛的泛化能力。

Result: DexFormer在Leap Hand、Allegro Hand和Rapid Hand等多种实际机械手上，实现了强大的零样本迁移能力（zero-shot transfer），无需专门针对每种手型单独训练，展现了单一策略跨不同手型的通用性和优良性能。

Conclusion: DexFormer证明了基于时序上下文和Transformer结构可以跨多种异构手型实现灵巧操作，建立了跨机器人手型操控的可扩展方法基础，有助于推动通用机器人操控的发展。

Abstract: Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.

</details>


### [348] [ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects](https://arxiv.org/abs/2602.08285)
*Josh Pinskier,Sarah Baldwin,Stephen Rodan,David Howard*

Main category: cs.RO

TL;DR: 本文提出了一种创新软体夹指设计方法ReefFlex，用于安全、高效地操作和再生易碎的珊瑚，为珊瑚礁生态修复提供新工具。


<details>
  <summary>Details</summary>
Motivation: 受气候变化、外来物种与人类活动影响，全球珊瑚礁正经历前所未有的破坏，目前缺乏可规模化、安全、高效的工具来支持脆弱珊瑚的再生和搬运工作，阻碍了生态修复的进展。

Method: 提出ReefFlex——一种生成式软体夹指设计方法，通过将异构抓取动作编码为简化的运动基元，从而转化成可行的多目标优化问题，并应用于设计用于珊瑚搬运的软体机器人。

Result: ReefFlex设计的软体夹指，在抓取成功率、抓取质量（抗干扰能力、定位精度）方面均优于传统参照设计，同时降低了操作中出现不利事件的概率。

Conclusion: ReefFlex为复杂物体操作提供了可泛化的软体末端执行器设计框架，为自动化解决此前难以实现的珊瑚操作和修复任务提供了可行路径。

Abstract: Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>


### [349] [Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework](https://arxiv.org/abs/2602.08298)
*Yuxin Zhang,Cheng Wang,Hubert P. H. Shum*

Main category: cs.RO

TL;DR: 本文提出开发驾驶员基础模型（DFM）为自动驾驶汽车（AVs）性能设定新基准，以提升其安全性、舒适性、效率与能源经济性，从而促进AVs的普及和市场渗透。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶汽车的安全性、舒适性、出行效率与能耗等表现仍不及有经验的人类驾驶员，这成为其推广应用的主要障碍，因此需要新的方法来系统地对自动驾驶技术进行规范、验证和提升。

Method: 作者提出了一种开发驾驶员基础模型（DFM）的框架，包含大规模数据集采集策略，用于训练DFM，同时讨论了该模型应具备的核心功能和实现这些功能的潜在技术方案。

Result: 本文展示了DFM在自动驾驶系统生命周期中的多种应用，包括定义以人为本的安全边界以及能耗经济性的基准等。

Conclusion: 通过正式提出DFM概念，本文为系统化规范自动驾驶汽车的设计、验证和性能评估提供了一种新的范式。

Abstract: Autonomous vehicles (AVs) are poised to revolutionize global transportation systems. However, its widespread acceptance and market penetration remain significantly below expectations. This gap is primarily driven by persistent challenges in safety, comfort, commuting efficiency and energy economy when compared to the performance of experienced human drivers. We hypothesize that these challenges can be addressed through the development of a driver foundation model (DFM). Accordingly, we propose a framework for establishing DFMs to comprehensively benchmark AVs. Specifically, we describe a large-scale dataset collection strategy for training a DFM, discuss the core functionalities such a model should possess, and explore potential technical solutions to realize these functionalities. We further present the utility of the DFM across the operational spectrum, from defining human-centric safety envelopes to establishing benchmarks for energy economy. Overall, We aim to formalize the DFM concept and introduce a new paradigm for the systematic specification, verification and validation of AVs.

</details>


### [350] [Personalized Autonomous Driving via Optimal Control with Clearance Constraints from Questionnaires](https://arxiv.org/abs/2602.08326)
*Yongjae Lim,Dabin Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种能显式融入用户期望与周围车辆保持安全距离偏好的规划框架，通过问卷调查收集用户清晰度偏好，并将其纳入最优控制问题。为解决多场景导致的实时计算不可行问题，将原始问题分解为多个子问题并并行求解。实验结果显示该方法能有效提升偏好对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶/辅助驾驶系统往往没有考虑驾驶者与周围车辆保持何种距离的个体偏好，这会导致乘客不适。为提升用户体验与安全感，需将个体偏好纳入规划环节。

Method: 设计专门的问卷，用于高效捕捉用户距离偏好（涵盖周围车辆的尺寸、速度、位置及动作等场景因子）；将用户对特定场景的距离选择作为最优控制问题的约束。考虑到所有可能场景带来的高计算复杂度，提出将原始问题拆解为多个定场景子问题，可并行求解并通过原有问题的代价函数择优输出。

Result: 通过仿真实验，针对不同用户问卷反应，分析所提规划器反映用户个性化偏好的能力。结果表明，该方法比未考虑用户偏好的基线方法在偏好对齐度上有明显提升。

Conclusion: 本文方法能有效融入并反映用户对车辆间隔的个性化偏好，提升了规划系统的人机友好度和潜在乘坐舒适性，对智能驾驶领域具有一定应用价值。

Abstract: Driving without considering the preferred separation distance from surrounding vehicles may cause discomfort for users. To address this limitation, we propose a planning framework that explicitly incorporates user preferences regarding the desired level of safe clearance from surrounding vehicles. We design a questionnaire purposefully tailored to capture user preferences relevant to our framework, while minimizing unnecessary questions. Specifically, the questionnaire considers various interaction-relevant factors, including the surrounding vehicle's size, speed, position, and maneuvers of surrounding vehicles, as well as the maneuvers of the ego vehicle. The response indicates the user-preferred clearance for the scenario defined by the question and is incorporated as constraints in the optimal control problem. However, it is impractical to account for all possible scenarios that may arise in a driving environment within a single optimal control problem, as the resulting computational complexity renders real-time implementation infeasible. To overcome this limitation, we approximate the original problem by decomposing it into multiple subproblems, each dealing with one fixed scenario. We then solve these subproblems in parallel and select one using the cost function from the original problem. To validate our work, we conduct simulations using different user responses to the questionnaire. We assess how effectively our planner reflects user preferences compared to preference-agnostic baseline planners by measuring preference alignment.

</details>


### [351] [Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation](https://arxiv.org/abs/2602.08328)
*Yi-Hsuan Hsiao,Quang Phuc Kieu,Zhongtao Guan,Suhan Kim,Jiaze Cai,Owen Matteson,Jonathan P. How,Elizabeth Farrell Helbling,YuFeng Chen*

Main category: cs.RO

TL;DR: 本文介绍了一种仅1.29克的微型飞行机器人，能够凭借机载传感和计算实现自主悬停、轨迹跟踪，并在无外界辅助下避障降落。


<details>
  <summary>Details</summary>
Motivation: 当前昆虫尺度的飞行机器人多依赖外部传感器与计算系统维持稳定飞行，无法在自然环境下自主作业，极大限制了其应用领域。

Method: 作者研制了一套集成传感器、状态估计器和低层控制器的微型机器人，实现了高精度的自主悬停和轨迹跟踪。同时，开发了分层控制系统，通过人类操作员下达高层指令。

Result: 在无动作捕捉系统的室外30秒飞行实验中，机器人能够自主避障，最终成功降落在向日葵上，表现出厘米级的定位精度。

Conclusion: 该成果大幅提升了微型飞行机器人自主感知与计算的能力，为实现完全自主的机载规划和能源自主打下了基础，拓展了其在实际任务中的应用前景。

Abstract: Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>


### [352] [Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving](https://arxiv.org/abs/2602.08334)
*Xuanjin Jin,Yanxin Dong,Bin Sun,Huan Xu,Zhihui Hao,XianPeng Lang,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出了一种面向CPU的并行POMDP规划器Vec-QMDP，显著提高了高维不确定性任务下的机器人规划效率，超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于POMDP不确定性规划的CPU-GPU混合求解器因同步延迟和分支发散等问题，导致实时性不足，并限制了实际机器人系统的部署，因此需要更高效的方案。

Method: Vec-QMDP以现代CPU的SIMD架构为基础，采用数据导向设计（DOD），将分散的数据重组为连续、缓存友好的内存结构。引入分层并行机制，在多个CPU核心与SIMD通道之间分配子树，实现树扩展和碰撞检测的完全矢量化；并通过UCB负载均衡和矢量化STR树进行高效的碰撞检测。

Result: 与当前最先进的串行规划器相比，Vec-QMDP在大规模自动驾驶基准上实现了227至1073倍的加速，达到了毫秒级时延。

Conclusion: Vec-QMDP实现了大规模、不确定性下高效的实时规划，将CPU平台确立为大规模不确定性规划的高性能计算平台，推动了实际机器人部署的可行性。

Abstract: Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\times$--$1073\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.

</details>


### [353] [Learning Human-Like Badminton Skills for Humanoid Robots](https://arxiv.org/abs/2602.08370)
*Yeke Chen,Shihao Dong,Xiaoyu Ji,Jingkai Sun,Zeren Luo,Liu Zhao,Jiahui Zhang,Wanyue Li,Ji Ma,Bowen Xu,Yimin Han,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种新的强化学习框架，实现类人机器人在羽毛球等高难度体育运动中的高水平表现，能够无缝结合全身协调与精准打击，并实现了首次的零样本、模拟到现实的技能迁移。


<details>
  <summary>Details</summary>
Motivation: 类人机器人在羽毛球等体育运动领域的运动能力远未达到人类水平，主要在于传统模仿方法难以兼顾爆发力、击球时机精准与运动自然协调。需要一种新方法来融合感知、运动控制与高层策略，实现人形的真实技能。

Method: 提出“Imitation-to-Interaction”渐进式强化学习框架：1）从人类数据中获得稳健的运动先验；2）通过模型实现状态表达的压缩；3）利用对抗性先验稳定动力学；4）创新性地提出流形扩展方法，将离散击球点泛化为连续交互空间，从而克服专家演示稀疏问题。

Result: 在仿真环境中，框架实现了多样技能（如挑球、搓球等高级动作）的掌握，并首次实现了从模拟到现实的人形羽毛球运动技能的零样本转移，使机器人具备与人类类似的运动优雅性与精准击球能力。

Conclusion: 本文提出的框架显著拓展了类人机器人在高要求运动任务中的能力，实现了运动自然性、功能性与物理一致性的统一，为实际机器人体育技能训练和应用提供了新范式。

Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>


### [354] [BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models](https://arxiv.org/abs/2602.08392)
*Xin Wu,Zhixuan Liang,Yue Ma,Mengkang Hu,Zhiyuan Qin,Xiu Li*

Main category: cs.RO

TL;DR: 本文提出了BiManiBench基准，用于多模态大语言模型（MLLMs）在双臂机器人任务中的评测，发现现有模型在空间协调和控制上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs已在智能体AI中取得显著进展，但现有评测主要集中在单臂操作，难以反映双臂任务中空间和时序协调的复杂性。因此需要针对双臂任务提出新的评测框架。

Method: 作者构建了BiManiBench分层基准，从空间推理、高层规划到末端执行器控制三个层级，系统性地评测试题，区分了传统感知错误与规划失误，并专门考察双臂的独特挑战，包括可达性和运动学限制。

Result: 评测超过30种前沿MLLMs后发现，尽管高层次推理能力成熟，但在双臂的空间绑定与协同控制方面表现较差，常出现干扰和动作排序错误。

Conclusion: 当前MLLM在理解双臂运动学约束存在不足，未来需加强对双臂碰撞规避和细粒度时序控制的研究。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>


### [355] [Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion](https://arxiv.org/abs/2602.08417)
*Wentao Zhao,Yihe Niu,Zikun Chen,Rui Li,Yanbo Wang,Tianchen Deng,Jingchuan Wang*

Main category: cs.RO

TL;DR: 提出了一种名为Graph-Loc的图结构地图先验下的基于激光雷达的定位方法，能高效、稳定地进行位姿追踪，适用于部分观测、遮挡、场景变化等实际复杂情况。


<details>
  <summary>Details</summary>
Motivation: 长时间自治操作要求位姿追踪方法能高效利用体积紧凑的地图先验，但实际激光雷达观测存在部分观测、重复及强遮挡等挑战。如何利用异构、稀疏且结构化的地图先验实现鲁棒定位是难点。

Method: 提出Graph-Loc框架，将结构化地图先验表示为轻量级的点线图，可来自多种数据源（如轮廓、CAD模型、楼层平面等）。每帧激光点云抽稀简为点线原语，生成观测图。采用激光模拟检索可见子图，通过不平衡最优传输和本地上下文正则进行扫描-地图关联，同时根据优化信息矩阵对弱约束方向延后更新，提高鲁棒性。

Result: 在公共基准、压力测试及真实部署中，Graph-Loc实现了KB级地图先验下的精准稳定追踪，即使在遮挡、结构退化及缓变场景中也表现良好。

Conclusion: Graph-Loc能以极其精简的异构结构先验支持鲁棒、高效的地图定位，适用于实际长期无人系统运作场景，对部分观测和场景变化具有强适应性。

Abstract: Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>


### [356] [Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric](https://arxiv.org/abs/2602.08421)
*Farhad Keramat,Salma Salimi,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种新型 LLM oracle 聚合方法和基于 Hyperledger Fabric 的去中心化多机器人平台，用于可靠地将自然语言意图转化为可执行的机器人任务，并在新建立的基准下验证了其实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）可便捷地将用户自然语言意图转为可执行动作，但带来了安全与隐私挑战，尤其是自我偏好（单一供应商支配市场）。现有 LLM oracle 聚合机制在机器人任务规划领域存在局限，主要因为未考虑任务的时序性。

Method: 作者提出了一种更适用于机器人任务规划的新型LLM oracle聚合方法，解决现有方法只基于语义相似性且忽略任务时间顺序的问题。此外，提出基于Hyperledger Fabric的去中心化多机器人基础设施以支持多供应商机器人间的任务分解与协作，并实现细粒度访问控制管理。还构建并公开了SkillChain-RTD基准以评测方法有效性。

Result: 实验结果显示，所提聚合方法在新基准下优于现有主流聚合方法，所提系统可有效分解和协调多机器人任务，验证了架构的可行性。

Conclusion: 本文提出的LLM oracle新聚合方法和去中心化多机器人平台能更安全、可靠、隐私保护地实现机器人任务自动化，有效缓解自我偏好等风险，对实际人机交互及机器人领域具有重要应用价值。

Abstract: Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>


### [357] [Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence](https://arxiv.org/abs/2602.08425)
*Jinxian Zhou,Ruihai Wu,Yiwei Liu,Yiwen Hou,Xunzhe Zhou,Checheng Yu,Licheng Zhong,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出了一种名为Bi-Adapt的新方法，用于实现机器人在双臂协同操作时的高效泛化，显著提升了对不同类别新物体的适应能力，且对数据量要求较低。


<details>
  <summary>Details</summary>
Motivation: 现有的双臂操作方法依赖高昂的数据收集与训练成本，且在处理新类别和未见过的物体时泛化能力有限。提升双臂机器人对于新任务和新物体的快速适应能力成为亟需解决的问题。

Method: 提出Bi-Adapt框架，通过语义对应关系实现跨类别的可用性映射（affordance mapping），借助视觉基础模型的强大能力，仅需少量新类别数据即可高效微调，在零样本的情况下实现对新物种的泛化。

Result: 在仿真和真实环境中进行了大量实验，验证了Bi-Adapt在不同基准任务中的高效率和高成功率，尤其是在数据有限的条件下跨类别泛化效果显著。

Conclusion: Bi-Adapt为双臂操作任务提供了一种高效且泛化能力强的解决方案，极大减少了对大量标注数据的需求，有望提升机器人在实际复杂任务中的应用能力。

Abstract: Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>


### [358] [SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios](https://arxiv.org/abs/2602.08440)
*Tian Gao,Celine Tan,Catherine Glossop,Timothy Gao,Jiankai Sun,Kyle Stachowicz,Shirley Wu,Oier Mees,Dorsa Sadigh,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出SteerVLA框架，将大型视觉-语言模型的推理能力引入自动驾驶系统，通过自然语言指引下游驾驶策略，有效提升复杂驾驶场景下的鲁棒性，实验证明其在难度较高的基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域面临将高层语义推理与低层驾驶控制结合的挑战。现有大规模视觉-语言模型具备强大的常识推理能力，但缺乏实际车辆控制经验；直接依赖感知动作端的模型在遇到复杂与长尾事件时又易失效。因此，如何将知识丰富的高层推理与可靠的低层控制有效衔接，是推动自动驾驶智能化发展的关键动力。

Method: 提出SteerVLA，通过利用VLM生成细粒度的语言指令，引导视觉-语言-动作策略进行决策。该方法核心创新点在于建立高层VLM到低层控制策略间的语言接口，并利用VLM为驾驶数据生成与车辆控制对齐的详细语言标注，实现高效的推理能力与控制能力结合。

Result: SteerVLA在挑战性的闭环自动驾驶测试平台上，整体驾驶得分较最先进方法提升4.77分，在长尾场景子集提升8.04分，展示出更强的泛化与复杂场景处理能力。

Conclusion: 引入自然语言中介层，使高层语义推理与低层驾驶执行实现更有效衔接，显著提升了自动驾驶系统的鲁棒性和智能化水平。该方法为融合世界知识与感知控制开辟了新路径。

Abstract: A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.

</details>


### [359] [Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions](https://arxiv.org/abs/2602.08444)
*Samsaptak Ghosh,M. Felix Orlando,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: 本文针对自动驾驶车辆发生碰撞后的轨迹恢复问题，提出一种联合控制转向和驱动力的启发式控制律，在更真实的车辆模型下实现有效恢复，并通过仿真验证其一致性。


<details>
  <summary>Details</summary>
Motivation: 碰撞会导致车辆出现横向偏移和偏航等复杂瞬态行为，若不能及时恢复，容易带来安全隐患。现有方法多忽略纵向速度变化及非线性耦合，难以应对实际碰撞后的动力学特性。因此，有必要提出能够更准确、有效处理这种复杂动态并实现安全恢复的新方法。

Method: 本文基于广义的单轨Ackermann车辆模型，建立考虑时变纵向速度和转向非线性耦合的横摆动力学模型，设计融合转向与驱动力的结构化启发式恢复控制律。在仿真中，将该方法分别应用于本文提出的车辆模型和MATLAB标准3自由度车辆模型进行对比测试。

Result: 仿真结果显示，在不同的初始碰撞后条件下，该方法均表现出一致且有效的轨迹恢复能力，优于忽略速度变化和非线性项的方法。

Conclusion: 提出的控制律能够针对碰撞后复杂动力学状态下的轨迹恢复问题实现鲁棒控制，提升自动驾驶车辆在极端工况下的安全性和实际适应能力。

Abstract: Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>


### [360] [UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials](https://arxiv.org/abs/2602.08450)
*Stefan Ivić,Luka Lanča,Karlo Jakac,Ante Sikirica,Stella Dumenčić,Matej Mališa,Zvonimir Mrle,Bojan Crnković*

Main category: cs.RO

TL;DR: 本文提出了一套集成流场重建、动态概率建模、搜索控制和机器视觉检测的自主海上搜寻系统，并在实际环境中进行了验证，结果表明该系统在复杂海况下能够有效识别漂浮目标。


<details>
  <summary>Details</summary>
Motivation: 当前海上搜救存在环境复杂、不确定性强、人工巡查成本高等问题，迫切需要高效的自主系统提升搜寻能力。

Method: 该系统整合了实时漂流仪数据采集、基于CFD和数值优化的流场重建与建模、多无人机协作搜索控制、视觉传感和基于深度学习的目标检测技术，并在克罗地亚Cres岛Valun湾实施了野外实验。

Result: 该方法实现了在实际环境与多重不确定性下，稳定、高效地检测和定位海上漂浮目标。

Conclusion: 文中紧密集成的多技术系统为未来自主海上搜寻与救援任务提供了实际可行和高可靠性的方案。

Abstract: This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>


### [361] [Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment](https://arxiv.org/abs/2602.08466)
*Ning Hu,Senhao Cao,Maochen Li*

Main category: cs.RO

TL;DR: 本文提出了一种新机制来提升视觉引导机器人系统在精密对准任务中的执行可靠性，即使位姿估计已较准确，执行层仍可能失效。作者通过实验证明，该机制能大幅提升任务成功率和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然位姿估计算法越来越精准，但在实际机器人精确对准任务中，任务执行失败仍频繁发生，说明高精度位姿估计并不等于高执行可靠性。因此，需要寻找新的执行层解决方案来弥补这一差距。

Method: 作者分析了执行失败是由于几何误差在系统结构和动作执行中的放大机制引起。提出了一种"可靠性感知的执行门控机制"，在实际执行前评估几何一致性及配置风险，对高风险位姿更新进行拒绝或调整，从而保障执行安全。该方法无需修改位姿估计算法，兼容多种主流估计方法。

Result: 在实际UR5机器人平台和多种视觉对准场景下验证，所提机制显著提升了任务成功率、降低了执行方差，并有效抑制了极端风险。而位姿估计平均精度无显著变化。该方法不仅普适性强，还能和传统及学习型估计方法无缝集成。

Conclusion: 仅靠精度提升无法保障机器人系统的执行稳定性，执行层的鲁棒建模非常重要。本文提出的执行门控机制为近场视觉引导机器人可靠性提升提供了有效工具和理论支持。

Abstract: Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>


### [362] [Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi](https://arxiv.org/abs/2602.08518)
*Kento Kawaharazuka,Kei Okada,Masayuki Inaba*

Main category: cs.RO

TL;DR: 本研究对仿生肌肉结构的多样特性进行了归类和分析，提出了针对控制与管理这些特性的系统方法。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多肌肉骨骼型拟人机器人和相关控制机制研究，但对其复杂结构本质特性的深入和统一的讨论比较缺乏，管理和利用方式也不明确。

Method: 以Kengoro和Musashi两个自主研发的肌肉骨骼机器人为基础，归纳肌肉结构的五类主要特性（冗余性、独立性、各向异性、可变力臂和非线性弹性），并分析不同特性组合下的优缺点。此外，针对身体图式学习与反射控制、肌肉分组、身体图式适应等进行了深入探讨，并集成到动作实现系统中。

Result: 归纳总结了肌肉骨骼型机器人的结构特性及其带来的优势与不足，提出了相关动作实现和控制策略，并指出不同管理和利用方法的效果。

Conclusion: 为肌肉骨骼机器人未来的控制机制和身体图式相关研究提供了理论基础和实践建议，展望了未来需解决的挑战和研究方向。

Abstract: Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.

</details>


### [363] [UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation](https://arxiv.org/abs/2602.08537)
*Haoming Ye,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出了UniPlan系统，将视觉-语言模型推理与符号规划结合，实现了大规模室内移动操作任务的长时规划，并显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM与符号规划结合的系统（如UniDomain）主要局限于桌面操作，难以应对需要导航、开门、双手协调等复杂移动操作任务。实际应用环境更复杂，需要扩展至大范围室内和多样化操作能力。

Method: UniPlan将场景拓扑、视觉信息和机器人能力统一编码到PDDL中，从UniDomain的桌面领域扩展支持导航、门穿越及双手协作。系统基于视觉-拓扑地图，检索任务相关节点并通过VLM识别物体及其PDDL状态，再重连形成压缩的高连通性拓扑图，利用现有PDDL求解器进行移动操作任务规划。

Result: 在包含真实环境图像的大规模室内地图和实际设定下，与VLM、LLM+PDDL等方法对比，UniPlan在任务成功率、规划质量和计算效率等方面均实现了显著提升。

Conclusion: UniPlan充分融合视觉、语言与符号知识，有效推动了移动操作机器人在复杂实际环境中的长时任务规划能力，突破了传统桌面操作场景的局限性。

Abstract: Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>


### [364] [Constrained Sampling to Guide Universal Manipulation RL](https://arxiv.org/abs/2602.08557)
*Marc Toussaint,Cornelius V. Braun,Eckart Cobo-Briesewitz,Sayantan Auddy,Armand Jordana,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种用模型驱动采样器辅助强化学习的方法，以训练能够从任意可行初态达到任意可行目标的通用操控策略，在复杂接触场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在复杂且奖励稀疏的操作任务中，传统强化学习由于探索效率低下而难以发现复杂的操作策略，因此需要引入额外的探索和策略引导机制来提高学习效率和策略表现。

Method: 作者提出Sample-Guided RL，将基于模型的约束求解器作为采样器，采样满足碰撞、接触和力学约束的低维可行状态，并结合黑盒轨迹优化及行为克隆损失，引导通用的目标条件RL。具体包括直接利用采样数据偏置状态访问，或者用最优开环轨迹在随机配置间引入状态偏差。

Result: 在简化的“双球”操作实验中，方法能学会复杂操作策略，达到高成功率。在更具挑战性的Panda机械臂实验中，方法远超基线（几乎为零），达成了多样的复杂全身接触操作策略和较高成功率。

Conclusion: 基于模型采样的引导方法能有效提升强化学习在稀疏奖励、高维接触操作任务中的能力，促进多样复杂策略的学习，明显超越现有基线表现。

Abstract: We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>


### [365] [Head-to-Head autonomous racing at the limits of handling in the A2RL challenge](https://arxiv.org/abs/2602.08571)
*Simon Hoffmann,Simon Sagmeister,Tobias Betz,Joscha Bongard,Sascha Büttner,Dominic Ebner,Daniel Esser,Georg Jank,Sven Goblirsch,Alexander Langmann,Maximilian Leitenstern,Levent Ögretmen,Phillip Pitschi,Ann-Kathrin Schwehn,Cornelius Schröder,Marcel Weinmann,Frederik Werner,Boris Lohmann,Johannes Betz,Markus Lienkamp*

Main category: cs.RO

TL;DR: 本文介绍了TUM Autonomous Motorsport团队为阿布扎比自动驾驶赛车联赛（A2RL）开发的算法及部署策略，展示了其在人车交互和极限操控下的软件与成绩。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车需要在极限性能和多车互动中实现安全与高效，极具挑战性，也有助于推动自动驾驶技术和交通安全的发展。

Method: 团队开发出能够模拟人类驾驶行为的算法，并设计了对应的软件与部署策略，应对赛车在极限状态下的操作和多车辆的竞速策略。

Result: 团队的软件在A2RL中成功实现了极限车辆操控和多车间交互，并凭借此技术赢得了比赛。

Conclusion: 成功的关键在于人类驾驶行为的模拟和对车辆极限性能的把控。文中总结了经验教训，为自动驾驶竞赛车辆和相关技术的发展提供参考。

Abstract: Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>


### [366] [MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation](https://arxiv.org/abs/2602.08594)
*Zhenguo Sun,Bo-Sheng Huang,Yibo Peng,Xukun Li,Jingyu Ma,Yu Sun,Zhe Li,Haojun Jiang,Biao Gao,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: MOSAIC是一个开源的人形机器人运动追踪和远程操作系统，通过新的训练和适应方法，实现了鲁棒的长期操作能力，并解决了仿真到实际部署中的接口差异。


<details>
  <summary>Details</summary>
Motivation: 现有通用运动追踪器在仿真环境表现优异，但在实体硬件上远程操作时易受接口和动力学误差影响，导致不稳定和脆弱。因此，亟需提升运动追踪在真实环境下的适应性和鲁棒性。

Method: MOSAIC系统首先通过强化学习（RL）在多源运动数据上训练通用运动追踪器，采用适应性重采样和奖励函数，重点强化真实世界坐标系下一致性。为解决仿真与现实的接口落差，MOSAIC引入残差适应机制，即用少量接口特定数据训练残差模块，并将其融合至通用追踪器，实现快适应且兼顾泛化能力。

Result: 通过消融实验、异常分布测试和真实机器人实验证明，MOSAIC相比简单微调或持续学习，能更鲁棒地进行高质量离线运动重放和长时在线远程操作，能有效应对实际延迟和噪声。

Conclusion: MOSAIC实现了面向多接口的人形机器人全身运动追踪和远程操作，在真实操作和仿真切换中表现出更强适应性和稳健性，有助于推动通用机器人运动控制的发展。

Abstract: Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>


### [367] [A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation](https://arxiv.org/abs/2602.08599)
*Kenghou Hoi,Yuze Wu,Annan Ding,Junjie Wang,Anke Zhao,Chengqian Zhang,Fei Gao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于六个低成本、灵敏皮肤式磁传感模块的空中操作抓取系统，实现了高精度三维力测量和力知觉操控，在实际多种场景测试中表现有效，并可完全自主运行，无需外部运动捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有空中机器人操作系统依赖笨重昂贵的力传感器，或完全无力反馈，无法安全抓取易碎物体，限制了实际应用。作者希望以低成本、高灵敏度力觉传感系统提升空载机器人操作能力。

Method: 设计了基于磁感应的三维力觉模块，集成六个皮肤式传感器，通过基准霍尔传感器消除地磁干扰，并简化校准流程；结合力知觉控制算法，实现精准抓取和实时称重，并在无人机平台全面测试。

Result: 系统在气球抓取、负载动态变化和消融实验等多项实际任务中得到验证。实验结果表明，该系统可安全操作易碎物体，精确感知重量，且全流程自主，无需外部定位设备。

Conclusion: 本研究显著提升了空中机器人的实用性和安全性，证明低成本磁基皮肤式力感传感技术适用于空中操作，扩展了空中机器人安全物理交互和抓取复杂任务的应用前景。

Abstract: Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>


### [368] [Mimic Intent, Not Just Trajectories](https://arxiv.org/abs/2602.08602)
*Renming Huang,Chendong Zeng,Wenjing Tang,Jingtian Cai,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: 该论文提出了一种新的模仿学习方法，通过显式解耦行为意图与执行细节，提高了机器人在动态环境中的适应与技能迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的模仿学习方法（如VLA模型）在环境变化与技能迁移时表现不佳，主要因为只模仿了轨迹而未理解背后的意图。

Method: 作者提出了MINT方法，通过多尺度频域分解，将动作表示分为粗粒度的“意图token”和细粒度的“执行token”。策略以自回归的方式递进生成动作，实现从意图到执行的推理。

Result: 在多个操作基准和真实机器人实验中，该方法在成功率、推断效率、泛化能力和一拍即会的技能迁移上都超过了现有方法。

Conclusion: 通过行为解耦，MINT显著提升了模仿学习的泛化与迁移能力，为机器人自主学习和适应新环境提供了更有效的解决方案。

Abstract: While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>


### [369] [High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning](https://arxiv.org/abs/2602.08653)
*Jiarui Zhang,Chengyong Lei,Chengjiang Dai,Lijie Wang,Zhichao Han,Fei Gao*

Main category: cs.RO

TL;DR: 提出了一种结合模型安全机制的端到端强化学习框架，用于四旋翼无人机的自主导航与避障，在高速度下兼顾性能与安全。


<details>
  <summary>Details</summary>
Motivation: 传统的无人机导航方法要么是模块化流程，导致累积延时；要么是单纯基于强化学习，但难以提供安全保障。需要一种兼具高效率与强安全性的导航与避障方法。

Method: 提出一个端到端的强化学习方法，并在训练与部署阶段融入物理知识：训练时通过引入物理先验的信息指导奖励结构，部署时在策略输出上增加实时安全过滤器，将输出投影到安全集合上，实现严格避障。

Result: 该方法在基准测试中优于传统规划方法和最新的基于可微物理的端到端障碍规避方法。大量实验表明，该方法在稠密杂乱和复杂林地环境下能以高达7.5m/s速度安全通行，并具备良好泛化性。

Conclusion: 所提混合架构能够实现在高速度下无人机导航的性能与安全的平衡，解决了传统和单纯RL方法的不足，在复杂环境下展现出优越的自主导航与避障能力。

Abstract: Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>


### [370] [Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch](https://arxiv.org/abs/2602.08776)
*Cuijie Xu,Shurui Zheng,Zihao Su,Yuanfan Xu,Tinghao Yi,Xudong Zhang,Jian Wang,Yu Wang,Jinchen Yu*

Main category: cs.RO

TL;DR: 本文提出了一种面向遥操作机器人控制的新行为克隆范式，不仅模仿操作轨迹，更重视操作意图（主设备命令），通过建模主从响应差异提升鲁棒性，实现无需力感知的高效控制。


<details>
  <summary>Details</summary>
Motivation: 传统的行为克隆方法仅模仿机器人的动作轨迹，忽视了操作员补偿系统硬件缺陷（如延迟、摩擦、无力反馈）的能力，难以应对实际中复杂的动态交互。如何在低成本、无传感器设备上实现对隐式力及系统动态的感知和补偿，是提升遥操作系统性能的关键。

Method: 提出“双状态条件（Dual-State Conditioning）”框架，将学习目标由模仿执行轨迹转为模仿操作意图（主命令），并利用主命令与从响应的差异（意图-执行不匹配）作为系统动态与外力的信号，将其输入策略网络，实现隐式阻抗控制。同时，利用历史不匹配信息进行隐式系统辨识，将跟踪误差视为外力以闭合控制回路。为应对推理延迟引起的时间差，采用轨迹补全模型确保控制连续性。在低成本双臂机器人平台上进行了传感器缺失条件下的实验。

Result: 在多项需要接触力和动态跟踪的任务中，新方法相比标准执行克隆取得了显著提升：后者因无法克服接触刚度和跟踪延迟而失效，而本文方法稳健完成任务。

Conclusion: 该方法为低成本机器人平台提供了不依赖于力传感器的最小化行为克隆框架，可感知隐式外力并实现动态补偿，显著提升遥操作控制的实际表现和适应性。

Abstract: Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>


### [371] [GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion](https://arxiv.org/abs/2602.08784)
*Santiago Montiel-Marín,Miguel Antunes-García,Fabio Sánchez-García,Angel Llamazares,Holger Caesar,Luis M. Bergasa*

Main category: cs.RO

TL;DR: 本文提出了GaussianCaR，一种融合摄像头与雷达数据、基于Gaussian Splatting的端到端BEV（鸟瞰视图）分割网络，用于提升自动驾驶场景下动态物体和地图要素的感知效果。其性能与现有最优算法持平甚至超越，并且推理速度大大提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要对复杂环境中的动态物体和道路元素实现高鲁棒性和高精度的感知。尽管视觉方法进步明显，但结合低成本雷达数据能进一步提升性能。目前BEV融合方法在统一不同传感器视角方面还有提升空间。

Method: 作者创新性地将Gaussian Splatting用于协调摄像头像素和雷达点云的视角差异，将二者投影到统一的鸟瞰视图（BEV）表征空间。提出了端到端的GaussianCaR网络，采用多尺度融合加Transformer解码器，高效提取BEV特征，从而实现高效的摄像头-雷达信息融合。

Result: 在nuScenes数据集上的BEV分割实验中，车辆、道路、车道线的IoU分别达到57.3%、82.9%、50.1%，与SOTA方法持平或超越，同时推理速度提升至3.2倍。

Conclusion: GaussianCaR展现了在摄像头-雷达BEV融合分割任务中的高效率与高精度，证明了Gaussian Splatting作为通用视角变换器的有效性，对多传感器融合领域有实际推动意义。

Abstract: Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>


### [372] [A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles](https://arxiv.org/abs/2602.08799)
*Robin Dehler,Michael Buchholz*

Main category: cs.RO

TL;DR: 本文提出了一个通用的功能卸载框架，可将自动驾驶车辆的计算任务根据不同位置在本地与远程设备间灵活分配，提高计算效率并保证服务质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（CAVs）与其他自主机器人在计算能力和能源方面存在局限性，需要一种高效的任务分配机制来优化资源利用。

Method: 设计了一个通用的功能卸载框架，支持不同的决策算法与QoS需求，并提出基于位置的决策方法，将轨迹规划任务卸载到边缘计算（MEC）服务器。框架在仿真和实际场景中进行了评估。

Result: 实验结果显示，该框架能保障轨迹规划的服务质量并提升CAVs的计算效率，同时在多车同时卸载请求时表现出良好的适应性。

Conclusion: 提出的功能卸载框架有效提升了自动驾驶车辆的计算效率和服务质量，具有泛化性并能适应多样化的应用场景。

Abstract: Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>


### [373] [Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems](https://arxiv.org/abs/2602.08821)
*Robin Dehler,Oliver Schumann,Jona Ruof,Michael Buchholz*

Main category: cs.RO

TL;DR: 本论文提出在分布式智能交通系统中，将面向服务架构（SOA）与功能卸载相结合，同时引入多阶段安全分析框架，以确保远程服务的数据可靠性并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着智能网联自动驾驶车辆对算力的需求不断提升，通过功能卸载到远程设备可有效降低本地计算负担，但面临远程服务数据易被攻击或篡改的安全隐患，亟需系统性安全性验证方法。

Method: 首先分析SOA在分布式环境下的应用，基于此提出可验证远程服务可靠性和本地接收数据的安全性框架。考虑到自动驾驶可能卸载多种服务，设计了针对本地与远程服务组合的多阶段安全分析框架，并将其整合到已有的SOFOF（服务导向功能卸载框架）中。

Result: 评估显示，扩展后的框架在实现卸载带来能耗与算力优化的同时，能够有效检测并识别远程服务数据被篡改的情况。

Conclusion: 该多阶段安全分析融合卸载服务框架，不仅提升了计算与能耗效率，也增强了对远程服务数据安全性的保障，适用于分布式智能交通系统中的自动驾驶车辆。

Abstract: The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>


### [374] [Finite-Time Teleoperation of Euler-Lagrange Systems via Energy-Shaping](https://arxiv.org/abs/2602.08845)
*Lazaro F. Torres,Carlos I. Aldana,Emmanuel Nuño,Emmanuel Cruz-Zavala*

Main category: cs.RO

TL;DR: 本文提出了一类针对全驱动非线性Euler-Lagrange系统的双向远程操作有限时间控制器，能够在无时延情况下实现全局有限时间收敛。


<details>
  <summary>Details</summary>
Motivation: 远程操作系统在主从操作（如机器人远程操控）中常因非线性、时滞和不确定性导致控制难题。现有方法难以在保证全球收敛性的同时实现有限时间收敛，且实现较为复杂。作者希望提出更为简单且高效的控制器。

Method: 基于能量整形框架，假定人与环境的交互为被动，构建了连续时间的比例加阻尼注入型控制器。通过分析闭环系统的齐次性证明其能实现有限时间内误差和速度的全球收敛。方法经过仿真和实验验证。

Result: 仿真和实验结果表明，该类控制器能确保主从系统的位置误差和速度在无时延情况下有限时间内收敛于零。

Conclusion: 新提出的控制器结构简单，理论上能够全局有限时间收敛，且验证效果良好，为实际非线性远程操作系统的快速精确控制提供了有效方法。

Abstract: This paper proposes a family of finite-time controllers for the bilateral teleoperation of fully actuated nonlinear Euler-Lagrange systems. Based on the energy-shaping framework and under the standard assumption of passive interactions with the human and the environment, the controllers ensure that the position error and velocities globally converge to zero in the absence of time delays. In this case, the closed-loop system admits a homogeneous approximation of negative degree, and thus the control objective is achieved in finite-time. The proposed controllers are simple, continuous-time proportional-plus-damping-injection schemes, validated through both simulation and experimental results.

</details>


### [375] [Reduced-order Control and Geometric Structure of Learned Lagrangian Latent Dynamics](https://arxiv.org/abs/2602.08963)
*Katharina Friedl,Noémie Jaquier,Seungyeon Kim,Jens Lundell,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的保持结构的降阶动力学潜在控制框架，用于高维拉格朗日系统，通过理论分析与实验验证其稳定性与收敛性。


<details>
  <summary>Details</summary>
Motivation: 高维机械系统如可变形物体或软体机器人缺乏准确物理模型，现有神经网络方法难以兼顾高维系统与强形式控制保证，需要新方法兼顾近似能力与物理结构。

Method: 采用结构保持的降阶动力学学习方法，提出基于Riemann几何的投影降阶思路，推导出降阶追踪律，并分析系统稳定性和收敛性的条件，对欠驱动系统引入学习的驱动模式，理论分析与实验相结合。

Result: 在仿真与真实系统上均证明提出控制器的准确性与理论稳定性，分析了建模误差的来源，并提出了易解释的收敛与稳定性条件。

Conclusion: 结构保持的潜在降阶控制方法能为高维系统带来良好的稳定性与收敛性理论保证，对实际高维系统具有良好应用前景。

Abstract: Model-based controllers can offer strong guarantees on stability and convergence by relying on physically accurate dynamic models. However, these are rarely available for high-dimensional mechanical systems such as deformable objects or soft robots. While neural architectures can learn to approximate complex dynamics, they are either limited to low-dimensional systems or provide only limited formal control guarantees due to a lack of embedded physical structure. This paper introduces a latent control framework based on learned structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems. We derive a reduced tracking law for fully actuated systems and adopt a Riemannian perspective on projection-based model-order reduction to study the resulting latent and projected closed-loop dynamics. By quantifying the sources of modeling error, we derive interpretable conditions for stability and convergence. We extend the proposed controller and analysis to underactuated systems by introducing learned actuation patterns. Experimental results on simulated and real-world systems validate our theoretical investigation and the accuracy of our controllers.

</details>


### [376] [CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion](https://arxiv.org/abs/2602.08999)
*Mouad Abrini,Mohamed Chetouani*

Main category: cs.RO

TL;DR: 这篇论文提出了CLUE方法，将视觉语言模型（VLM）的交叉模态注意力转化为空间显式信号，用于判断机器人在与人交互时何时需要提问澄清，进而优化人机交互的视觉指代消解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式视觉指代（IVG）方法缺乏明确机制判断何时应主动提问澄清用户意图，导致机器人在处理歧义时表现不佳。

Method: 该方法提取文本到图像的注意力图，并输入轻量级CNN来检测指代歧义性；然后由LoRA微调的生成式解码器执行对话并输出定位信号，整个模型在真实IVG与混合歧义数据集上进行训练。

Result: 在仅用InViG监督的情况下，CLUE取得了比现有方法更好的性能，并且参数更为高效。其歧义检测模块同样超越其他基线方法。

Conclusion: CLUE能够将VLM内部的交叉模态注意力有效转化为空间信号，显式用于判别何时提问，从而提升了机器人在复杂人机交互场景下的指代消解能力。

Abstract: With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>


### [377] [From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection](https://arxiv.org/abs/2602.09002)
*Zilin Fang,Anxing Xiao,David Hsu,Gim Hee Lee*

Main category: cs.RO

TL;DR: 本文提出了一套融合几何规划与社会语境推理的机器人社会导航框架，通过结合视觉语言模型实现路径选择与实时适应，显著提升了机器人在人类环境中的社交合规性和导航表现。


<details>
  <summary>Details</summary>
Motivation: 传统几何路径规划虽然能避免碰撞，但忽视了人类活动和社会规范，导致机器人路径可能妨碍人类或违背社交期望。因此，亟需将具备常识推理的社会化考量纳入机器人导航体系。

Method: 系统分两步：首先采集障碍物与人类动态，生成几何可行的候选路径；然后采用精调后的视觉语言模型（VLM），综合情境下的社会规范对路径进行评价和选择，最终交由控制器执行。该VLM对来自大型基础模型的社会推理能力进行了知识蒸馏，保证了高效与实时。

Result: 在四类社会导航情境中的实验表明，该方法在总表现上优于其他方案，在个人空间侵占持续时间、朝向行人时间等核心社交指标上均表现最佳，且无越界至人类社交区的现象。

Conclusion: 本文提出的社会导航框架能有效熔合几何与社会性信息，提升机器人与人类共处时的社交能力，并可在多样化环境中实时适应，具有较高的实际应用价值。

Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>


### [378] [Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction](https://arxiv.org/abs/2602.09013)
*Hongyi Chen,Tony Dong,Tiancheng Wu,Liquan Wang,Yash Jangir,Yaru Niu,Yufei Ye,Homanga Bharadhwaj,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 提出VIDEOMANIP框架，通过普通RGB视频学习多指机械手操作，无需穿戴设备或专用传感器。利用视觉与优化方法提取手物交互信息，合成多样化机器人演示，显著提升抓取与操作能力。


<details>
  <summary>Details</summary>
Motivation: 目前多指机械手操作需要高维动作空间和大量训练数据，现有方法主要依赖可穿戴设备或特殊传感器获取数据，难以大规模扩展。如何仅用常规视频实现灵巧操作学习，是提升机器人自主性的关键。

Method: 提出VIDEOMANIP，通过单目RGB视频估算人手姿态和物体形状，重构人手与物体的四维交互轨迹，并将其迁移至机器人手。引入手物接触优化和交互导向的抓取建模，以及单视频多样化演示合成，以增强数据多样性和训练泛化能力。

Result: 在仿真环境中，模型对20种物体的抓取成功率达70.25%；在实际环境中，基于RGB视频训练的策略在7项任务中的平均成功率为62.86%，比传统基于迁移方法高15.87%。

Conclusion: VIDEOMANIP能高效从普通视频中学习多指机械手操作技能，无需专用硬件，提升了机器人在现实环境中的泛化能力，具有良好的应用前景。

Abstract: Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>


### [379] [Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models](https://arxiv.org/abs/2602.09017)
*Zichen Jeff Cui,Omar Rayyan,Haritheja Etukuru,Bowen Tan,Zavier Andrianarivo,Zicheng Teng,Yihang Zhou,Krish Mehta,Nicholas Wojno,Kevin Yuanbo Wu,Manan H Anjaria,Ziyuan Wu,Manrong Mao,Guangxun Zhang,Binit Shah,Yejin Kim,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Contact-Anchored Policies（CAP）的机器人操作学习方法，用物理接触点代替语言提示，实现更加可靠的泛化能力，并在仿真与现实中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统利用语言指令进行泛化的机器人学习方法，受限于语言的抽象性，难以指导实际的物理操作，影响了复杂操作任务的可靠性。

Method: 作者提出了CAP方法，通过以空间内的物理接触点为条件输入，代替语言指令。此外，将任务分解为一组模块化的实用模型（utility models）而非单一泛化模型，并设计EgoGym仿真平台实现快速迭代失败分析及模型优化。

Result: 采用CAP，机器人仅通过23小时的示范数据，就能在三种基础操作技能上，对新环境和新实体表现出良好的泛化性能。零样本测试中，相较于大型VLA方法，性能提升56%。

Conclusion: 基于物理接触点进行条件输入以及模块化模型框架，有效提高了机器人操作任务的泛化性和可靠性，并在数据和计算量较低的条件下优于当前主流方法。所有相关资料和代码实现将开源。

Abstract: The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/

</details>


### [380] [Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving](https://arxiv.org/abs/2602.09018)
*Amir Mallak,Alaa Maalouf*

Main category: cs.RO

TL;DR: 本论文系统性分析了自动驾驶中模型在各种环境分布外（OOD）情况下的鲁棒性，并提出了更具洞见的分析方法和设计准则。


<details>
  <summary>Details</summary>
Motivation: 当前关于自动驾驶的OOD鲁棒性通常被简化为一个单一指标，无法揭示具体导致性能下降的因素。本文旨在通过解构环境属性，找出影响鲁棒性的关键因素，为鲁棒自动驾驶策略的设计提供更细致的指导。

Method: 作者将环境分解为场景、季节、天气、时间、交通参与者五个轴，利用VISTA平台对不同网络结构（FC、CNN、ViT及基于大模型特征的ViT）进行完整的闭环控制实验，系统考察了组合扰动（k因素变化）下的性能变化，并分析了数据规模、场景多样性、时序信息等对鲁棒性的影响。

Result: 1. ViT和基于大模型特征的策略在OOD下显著优于CNN/FC，但增加了延迟。2. 简单多帧输入并不优于最佳单帧基线。3. 城乡切换和昼夜变化是影响最大单因子，其他如参与者切换、降雨和季节切换也有明显影响。4. 基于大模型特征的ViT在三重变化下仍保持较高性能，而其他结构下降严重。5. 各因素组合影响非线性，有的叠加影响更严重。6. 在冬季训练最抗单因素变化，乡村+夏季训练对整体OOD最鲁棒。7. 增加轨迹视角数提升稳健性，也可用针对性强化弥补规模不足。8. 多环境联合训练提升广覆盖表现。

Conclusion: 本文提出的多维分析和实证结果揭示了OOD鲁棒性的瓶颈及提升套路，并为鲁棒自动驾驶策略的系统设计提出了有效建议。

Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>


### [381] [$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies](https://arxiv.org/abs/2602.09021)
*Checheng Yu,Chonghao Sima,Gangcheng Jiang,Hai Zhang,Haoguang Mai,Hongyang Li,Huijie Wang,Jin Chen,Kaiyang Wu,Li Chen,Lirui Zhao,Modi Shi,Ping Luo,Qingwen Bu,Shijia Peng,Tianyu Li,Yibo Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种面向长期、高可靠性的机器人操作框架χ₀，具有高效、稳健、实用的特点，能够显著提升多阶段任务下的机器人自主性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有长期机器人操作依赖大量数据与算力，但实际“瓶颈”主要源自人为演示、模型归纳偏置与测试执行分布之间的分布切换差异，这些系统性不一致易导致多阶段任务中误差累积和任务失败。

Method: 提出χ₀框架，包括：(1)模型算术，利用权重空间融合不同演示分布以提升分布多样性适应性；(2)阶段优势，采用阶段感知的优势估计器，提供稳定、密集的状态进展信号，克服以往方法的数值不稳定性；(3)训练-部署对齐，通过时空增强、启发式DAgger修正与时序分段平滑手段，减小训练与测试分布的落差。

Result: χ₀框架成功应用于双臂机器人协作完成长期复杂的服装整理任务，实现如展开、叠放、挂衣等全流程，且机器人可从任意初始状态连续自主运行24小时，实验表明χ₀在仅需20小时数据和8张A100显卡的情况下，成功率超过SOTA方法π₀.₅近250%。

Conclusion: χ₀极大提升了机器人在长期、多阶段任务下的鲁棒性和可靠性，在资源效率、通用性和实用性方面优于现有方法。后续将开放代码、数据和模型，促进学术社区发展。

Abstract: High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>


### [382] [TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation](https://arxiv.org/abs/2602.09023)
*Qinwen Xu,Jiaming Liu,Rui Zhou,Shaojun Shi,Nuowei Han,Zhuoyang Liu,Chenyang Gu,Shuo Gu,Yang Yue,Gao Huang,Wenzhao Zheng,Sirui Han,Peng Jia,Shanghang Zhang*

Main category: cs.RO

TL;DR: TwinRL通过结合数字孪生系统与现实机器人，实现高效扩展的探索，提高VLA模型在现实场景下的强化学习效率与成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型强大但依赖于昂贵的专家演示数据与有限的现实交互。现实中的RL因探索低效和空间受限，难以提升泛化能力，需要既经济又能拓展探索空间的有效方法。

Method: 提出TwinRL框架，首先利用手机快速重建高保真数字孪生体，实现现实与仿真的双向切换。在SFT阶段，利用数字孪生扩展探索空间。之后，通过虚实协同的探索策略，先在数字孪生体上大规模并行RL，筛选失败但有启发性的情境，再在实际机器人上进行有针对性的人机交互采样，缩小仿真与现实的鸿沟。

Result: TwinRL在真实任务实验中实现了约100%的成功率，不论是在已覆盖还是未覆盖的分布区域。整体RL速度提升至少30%，四个任务平均每个仅需约20分钟，明显优于现有方法。

Conclusion: TwinRL大幅提升了VLA模型在实际操作任务的效率和泛化能力，有效降低了数据和交互成本，为数字孪生与现实协同的强化学习提供了新思路。

Abstract: Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>
