<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 237]
- [cs.CL](#cs.CL) [Total: 76]
- [cs.RO](#cs.RO) [Total: 80]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MOTION: ML-Assisted On-Device Low-Latency Motion Recognition](https://arxiv.org/abs/2512.00008)
*Veeramani Pugazhenthi,Wei-Hsiang Chu,Junwei Lu,Jadyn N. Miyahira,Soheil Salehi*

Main category: cs.CV

TL;DR: 本文提出利用三轴加速度计与AutoML管道，在多传感器可穿戴设备（WeBe Band）上实现高效、低延迟手势识别的方法，特别适用于医疗监控等对响应速度和准确率要求高的场景。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互和医疗监控需求的增加，日常生活与医疗领域亟需能够低延迟、低误报地实现动作监测的嵌入式设备，如跌倒检测、康复跟踪及患者看护等。

Method: 通过仅利用三轴加速度传感器收集数据，采用AutoML管道自动提取运动片段的最优特征，并用这些特征训练多个轻量化机器学习模型。模型在硬件性能足够的WeBe Band设备端本地部署，实现端侧推理。

Result: 在测试的多种模型中，神经网络在准确率、延迟和内存占用三方面达到了最佳平衡。实验证明，该方法能在WeBe Band设备上实现可靠的实时手势识别。

Conclusion: 本研究验证了基于三轴加速度计和AutoML技术可在端侧设备上高效地进行手势识别，适合实时医疗监控，并且有较好的准确性与响应速度，显示了嵌入式运动识别在相关领域的广阔应用前景。

Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.

</details>


### [2] [Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions](https://arxiv.org/abs/2512.00042)
*Egemen Sert,Şeyda Ertekin*

Main category: cs.CV

TL;DR: 本文提出通过高质量多模态数据进行有监督微调，可使开源视觉-语言模型取得与专有模型相近的推理表现。作者构建了大规模多模态数据集并对Qwen-2.5VL-32B模型进行优化训练，模型在新基准测试上表现接近主流闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理研究多集中于算法提升，对数据本身的探索较少。作者希望证明，数据的质量与表达方式同样关键，优质的数据可大幅提升模型性能。

Method: 作者结合教科书题解对、课程配套图示和上下文材料，构建了1.61亿token的多模态数据集，用新设计的推理表达（QMSA）对Qwen-2.5VL-32B模型进行有监督微调，并建立了涵盖309个学科主题的多模态考试基准YKSUniform评测集。

Result: 微调后的模型在YKSUniform基准上取得了78.6%正确率，仅比Gemini 2.0 Flash低1%，显示了非专有模型也可通过数据优化接近最先进的闭源模型。

Conclusion: 多模态推理中的数据构成与表征语法至关重要。精心设计、与课程对应的数据可显著提升模型能力，为开源视觉-语言模型发展提供可推广的数据驱动路线。

Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.

</details>


### [3] [PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2512.00060)
*Abdolazim Rezaei,Mehdi Sookhak*

Main category: cs.CV

TL;DR: 本文提出PEFT-DML，一个高效参数的多模态3D物体检测框架，适用于自动驾驶，可处理传感器丢失和新组合，提升鲁棒性和训练效率，在nuScenes数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多模态3D检测框架通常假定传感器固定并始终可用，但实际自动驾驶环境中往往存在传感器异常或组合变化，导致原有方法鲁棒性不强。亟需一种对传感器情况变化鲁棒、参数高效、训练高效的模型。

Method: PEFT-DML通过将不同类型传感器（LiDAR、雷达、摄像头、IMU、GNSS）的输入，统一映射到共享隐空间，结合LoRA（低秩适配）和adapter层，使模型在面对新的模态组合或部分传感器不可用时依然能可靠检测。该方法显著提升了训练效率与模型泛化能力。

Result: 实验证明，PEFT-DML在主流自动驾驶多模态检测数据集nuScenes上相较于现有方法，取得了更高的检测精度和鲁棒性，尤其在传感器掉线、运动剧烈、天气变化及领域转移等复杂场景下表现优异。

Conclusion: PEFT-DML有效提升了多模态3D检测系统在自动驾驶应用中的鲁棒性和参数效率，表明以共享隐空间建模及高效适配结构是提升复杂环境下物体检测关键途径。

Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.

</details>


### [4] [DL-CapsNet: A Deep and Light Capsule Network](https://arxiv.org/abs/2512.00061)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种改进的深度胶囊网络（DL-CapsNet），通过多层胶囊结构和参数简化，有效提升分类精度并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理类别重叠和仿射变换图像时准确率受限，而现有CapsNet结构较浅，参数量大、训练效率低，难以扩展到大规模复杂数据集。作者旨在解决这些问题。

Method: 作者提出了包含多层胶囊的新型深度胶囊网络架构（DL-CapsNet），并引入Capsule Summarization层，用于减少参数数量和计算复杂度，从而兼顾深度与高效性。

Result: 实验结果表明，DL-CapsNet在保持高精度的同时，显著减少了模型参数，训练和推断速度更快，能够有效处理高类别复杂数据集。

Conclusion: DL-CapsNet在准确率、参数效率和训练速度方面均优于传统CapsNet和CNN，在复杂场景下具有更强的实用性和推广价值。

Abstract: Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.

</details>


### [5] [Satellite to Street : Disaster Impact Estimator](https://arxiv.org/abs/2512.00065)
*Sreesritha Sai,Sai Venkata Suma Sreeja,Deepthi,Nikhil*

Main category: cs.CV

TL;DR: 本文提出了一种用于灾害后卫星图像损害评估的深度学习框架，并在公共数据集上取得了比传统方法更好的损害检测效果。


<details>
  <summary>Details</summary>
Motivation: 手动分析卫星图像进行灾后损害评估速度慢、依赖主观判断且难以大规模应用。常规的深度学习分割模型虽然可以应用，但难以应对微小结构变化和类别不平衡的问题，导致高度损坏区域检测效果不佳。

Method: 作者提出了一种名为Satellite-to-Street: Disaster Impact Estimator的深度学习模型，采用改进的双输入U-Net架构，增强特征融合能力，联合处理灾前后图像以获取精细的像素级损害图。同时加入类别感知加权损失函数，以减轻未损坏像素的主导影响，提高对严重损坏区域的敏感性。

Result: 在公开的灾害数据集上，该方法在损伤区域的定位和分类上优于传统分割方法和主流变更检测基线模型。

Conclusion: 所提出的方法可以为专家提供快速、一致的数据驱动损害评估辅助，提升灾害管理的效率，但不是取代专家决策。

Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.

</details>


### [6] [ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN](https://arxiv.org/abs/2512.00073)
*Aswinkumar Varathakumaran,Nirmala Paramanandham*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProvRain的新型夜间车辆检测管道，通过集成降噪架构与高效神经网络，显著提升了雨天夜晚的车辆检测准确率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 夜间雨天车辆检测因强噪声干扰和车辆灯光等问题而变得极具挑战。仅依赖车灯特征会影响预判能力，因此作者亟需一种能有效降噪且提前探测车辆的车辆检测方法。

Method: 作者构建了一套基于MobileNet-U-Net的轻量级网络架构，融合curricula training理念，提升其在不同气象条件下的泛化降噪能力。数据方面采用合成数据与PVDN公开数据集混合训练，并与Faster RCNN基线模型做对比分析。

Result: 该系统在夜间雨天车辆检测中，准确率提升8.94%，召回率提升10.25%；降噪效果也优于其他方法，PSNR提升10-15%、SSIM提升5-6%，感知误差（LPIPS）降低67%。

Conclusion: ProvRain通过集成降噪与轻量化网络架构，有效提升了恶劣天气下夜间车辆检测的鲁棒性和性能，对实际智能交通应用具有重要意义。

Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.

</details>


### [7] [Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation](https://arxiv.org/abs/2512.00075)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Wangqiu Zhou,Jianbo Zhang,Linhan Cao,Dandan Zhu,Hua Yang,Xiongkuo Min,Wei Sun,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种名为Adapter Shield的防护方法，通过可逆加密技术保护个人图像，防止在零样本图像生成任务中被未授权使用。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的发展，零样本图像生成技术可以仅凭一张图像高保真地复制面部身份或艺术风格，这带来了知识产权和身份风险（如未授权克隆和风格模仿）。需要有效手段保护个人图像不被滥用。

Method: 研究现有零样本方法如何通过图像编码器提取嵌入，并注入UNet模型。基于此，设计了一种可逆加密系统，将原始嵌入按密钥加密为特殊表征。授权用户可用密钥还原嵌入，正常生成图像。并提出多目标对抗扰动方法，将嵌入主动移动到加密模式，提升防护效果。

Result: 实验表明，该方法有效阻止未授权的零样本图像合成，性能优于现有防御方法，同时为验证用户提供灵活安全的访问控制。

Conclusion: Adapter Shield能够为个人图像提供坚固的防护，兼顾安全与灵活性，是零样本生成风险下防护知识产权和个人隐私的重要方案。

Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.

</details>


### [8] [Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection](https://arxiv.org/abs/2512.00078)
*Mario de Jesus da Graca,Jörg Dahlkemper,Peer Stelldinger*

Main category: cs.CV

TL;DR: 论文提出使用扩散模型生成明场显微镜下的合成单细胞图像，并将其用于提升细胞检测模型性能。结果显示合成数据可提升YOLO等检测方法的准确性，且生成图像高度逼真。


<details>
  <summary>Details</summary>
Motivation: 明场显微图像的单细胞检测用于生物学研究，但深度学习方法受限于真实数据多样性不足与人工注释瓶颈。作者希望用合成数据解决数据稀缺和注释成本的问题。

Method: 作者训练了基于U-Net构架的无条件扩散模型生成高质量明场显微图像，并设计不同真实图像与合成图像占比的数据集。随后，分别采用YOLOv8、YOLOv9与RT-DETR等主流检测模型进行训练和检测效果评估。

Result: 使用合成数据联合真实数据训练，单细胞检测准确率提升，并且生成图片通过专家盲测无法与真实图像区分（只有50%辨别准确率），显示扩散模型生成图像的高度真实感。

Conclusion: 扩散模型生成的合成显微图像可有效增强真实数据集，为细胞检测任务带来性能提升，减少人工注释需求，未来有望推动生物医学图像分析的深入发展。

Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.

</details>


### [9] [Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels](https://arxiv.org/abs/2512.00080)
*André Dehne,Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本文介绍了欧洲XFEL的MARWIN机器人如何通过深度视觉立体里程计（DVSO）替代当前多传感器融合导航，实现隧道中自主辐射监测。DVSO具有无需标注、低成本、可扩展的数据采集等优点，但面临纹理不足、光照变化和计算负载等挑战。文章为今后在高安全需求基础设施中提升机器人自主导航能力制定了研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有MARWIN机器人导航依赖激光雷达、里程计和QR码，但在几何结构未知或有障碍时灵活性较差。基于视觉的纯感知导航可降低依赖、适应未知环境，因此需要研究DVSO在加速器隧道复杂场景中的适用性。

Method: 提出利用深度视觉立体里程计（DVSO），结合3D几何约束、立体视差、光流及自监督学习，估算深度与自运动，无需任何标签数据。DVSO还可融合绝对参考（如地标）保障全局一致性。并以欧洲XFEL隧道为案例，概念性地评估其应用。

Result: DVSO在理论上可减小比例漂移（因采用立体视觉），实现低成本传感和高效数据采集。显著优于传统依赖固定基准点的模式，但在隧道低纹理、光照变化大、辐射干扰和高计算负载下依然存在挑战。

Conclusion: DVSO为MARWIN机器人在复杂和未知几何的加速器隧道中自主导航提供了新方向，可提升其灵活性和安全性。未来需要针对实际应用场景解决视觉感知稳定性、计算负载和环境鲁棒性等关键难题。

Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.

</details>


### [10] [Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages](https://arxiv.org/abs/2512.00082)
*Divendar Murtadak,Yoon Kim,Trilokya Akula*

Main category: cs.CV

TL;DR: 本文探讨了诊断性提示是否能提升多模态大语言模型（MLLM）对亚马逊搜索结果页视觉复杂性的评估一致性。通过和标准群组原则法对比，诊断性提示显著提升了模型与人类专家判断的一致性，但绝对表现有待提高。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在理解网页视觉复杂性时与人类专家存在较大偏差，准确性不足。因此作者尝试通过“诊断性提示”来更好引导模型思考，提升其与人类评价的一致性，推动MLLM在人机协作中的实际应用。

Method: 作者构建了200个亚马逊搜索结果页数据集，并邀请人类专家标注复杂度。分别采用诊断性提示和基于格式塔原则的标准提示，对MLLM进行复杂度评估测试，并比较各自与人类专家一致性。同时通过特征重要性分析和失败案例分析深入理解模型推理过程及短板。

Result: 诊断性提示将F1分数从0.031提升到0.297，相对提升858%，但绝对分数依然较低。特征分析发现模型主要关注视觉设计（如徽章杂乱），而人类更关注内容相似度。模型在产品相似性和颜色强度等感知方面仍表现较差。

Conclusion: 诊断性提示推动了MLLM与人类视觉复杂性判断的对齐，是一个有前景的方向，但距离实用还有明显差距。未来需扩充标注数据集、改进提示策略以提升模型稳定性与可靠性。

Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.

</details>


### [11] [A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation](https://arxiv.org/abs/2512.00084)
*Venkata Siddharth Dhara,Pawan Kumar*

Main category: cs.CV

TL;DR: 该论文提出了FastTextDiff，一种高效利用文本标签的扩散模型，提升了医学图像分割的准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，像素级别的标注数据昂贵且耗时，限制了扩散模型的表现，因此研究者试图通过高效利用文本注释，减少对密集标注的依赖。

Method: 作者将能处理长文本的ModernBERT引入扩散分割模型，利用训练于大规模MIMIC临床数据集的ModernBERT替代原本常用的Clinical BioBERT，通过改良的跨模态注意力机制强化图像与文本的语义联系，并集成FlashAttention 2等新技术提升效率。

Result: FastTextDiff利用ModernBERT和新注意力机制，在分割准确率和训练效率上均优于传统的扩散分割模型和基于Clinical BioBERT的方案。

Conclusion: 用ModernBERT替代Clinical BioBERT不仅提升了效果，还为医学图像分析中的多模态学习提供了新思路，验证了多模态技术的巨大潜力。

Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.

</details>


### [12] [Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs](https://arxiv.org/abs/2512.00086)
*Davide Nadalini,Manuele Rusci,Elia Cereda,Luca Benini,Francesco Conti,Daniele Palossi*

Main category: cs.CV

TL;DR: 本文提出了一种针对物联网（IoT）设备的单目深度估计（MDE）在设备端自适应学习新环境的高效方法，实现能耗低、内存占用小下的高精度迁移。


<details>
  <summary>Details</summary>
Motivation: IoT设备资源有限，现有MDE模型在遇到数据分布变化（域偏移）时性能大幅下降，亟需能够在设备端自适应新环境、同时不损失精度和效率的解决方案。

Method: 构建了由GAP9 MCU、单目摄像头及可控低功耗深度传感器组成的系统。利用107k参数的μPyD-Net模型平时仅用摄像头推理；置于新环境时启动深度传感器辅助生成伪标签，在MCU上完全本地微调。同时提出稀疏更新方法，可将微调内存需求降低到1.2MB（为全量更新的2.2倍压缩），基本无损精度。

Result: 在KITTI和NYUv2标准数据集上精度仅下降1.5%-2%。实地部署中，使用3k自标记样本，ODL微调只需17.8分钟，RMSE误差大幅下降（4.9米降至0.6米），验证了本方法的实际可用性。

Conclusion: 本研究首次实现MDE任务在低功耗IoT设备上的高效本地在线微调，为低功耗空间感知应用落地提供了可行范式。

Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.

</details>


### [13] [Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data](https://arxiv.org/abs/2512.00087)
*Ivo Bueno,Ruikun Hou,Babette Bühler,Tim Fütterer,James Drimalla,Jonathan Kyle Foster,Peter Youngs,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 本文提出了利用AI自动分析教室录像和课堂文本记录的方法，有效替代了传统依赖人工标注的教师反馈手段。通过密集标注数据集并采用多种深度学习模型，实验证明细致调优的模型在活动和话语识别上优于提示式大模型，显示了自动化教室分析的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前课堂互动观测依赖大量人工标注，既耗时又难以大规模应用。为了让教师能够获得高效、可扩展的教学反馈，需要探索自动化、智能化的教室数据分析方法。

Method: 作者利用164小时视频和68份课堂文本，先后构建了面向视频和文本的分析管道。视频部分，评估零样本多模态大模型、微调后的视觉-语言模型及自监督视频Transformer，针对24种活动标签。文本部分，用上下文感知的Transformer分类器与基于提示的大模型对照，针对19种话语标签。为应对类别不均衡和多标签问题，采用了逐标签阈值、上下文窗口及不均衡损失函数。

Result: 实验结果显示，微调模型在视频分析上达到0.577的macro-F1分数，在文本分析上达到0.460，均优于基于提示的大模型。

Conclusion: 本文验证了自动化分析课堂视频和文本以为教师提供反馈的可行性，为今后可扩展的教学反馈系统奠定了方法基础。

Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.

</details>


### [14] [SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features](https://arxiv.org/abs/2512.00088)
*Mohammad Zare*

Main category: cs.CV

TL;DR: 提出了一种创新的方法SemImage，将文本转化为二维语义图像，利用卷积神经网络进行处理，实现了竞争性或更好的文本分类效果并提升了可解释性。


<details>
  <summary>Details</summary>
Motivation: 推动文本分类和文本可解释性研究，突破传统文本输入方式的限制，通过可视化手段将文本中的主题、情感等特征形象化展示，方便模型和人类理解。

Method: 设计了SemImage方法：将文本单词映射为二维图像像素，行对应句子，句间插入边界行，像素采用HSV向量（H代表主题，S代表情感，V代表强度）。提出ColorMapper网络实现词嵌入到HSV空间的投射，用多任务框架对主题和情感进行辅助监督。利用动态边界突出语义过渡，将生成图像输入2D CNN做分类。

Result: 在多标签（主题和情感）和单标签分类任务上，SemImage在准确率上达到或超过BERT、分层注意力网络等主流文本分类模型。消融实验证明了多通道HSV表征和动态边界对模型效果的贡献。

Conclusion: SemImage既提升了文本分类性能，又突出地增强了模型的可解释性，可有效揭示文本中的话题转变和情感变化，对文本可视化和解释具备现实应用前景。

Abstract: We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.

</details>


### [15] [TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts](https://arxiv.org/abs/2512.00089)
*Ioannis Prapas,Nikolaos Papadopoulos,Nikolaos-Ioannis Bountos,Dimitrios Michail,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 该论文提出了一种结合全球气候遥相关和局地火灾驱动因子的Vision Transformer模型（TeleViT），在全球尺度野火中长期预测任务中，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 野火对生态和社会有重大影响，但提前数周到数月的预测极为困难。准确的长期预测有助于火险管理和资源分配，而目前方法对全球气候模式和遥相关利用有限。

Method: 提出TeleViT模型，结合（1）局地火灾驱动信息，（2）粗化的全球场数据，（3）气候遥相关指数，通过异构token设计和融合，利用Vision Transformer结构进行统一建模，并映射回空间预测。

Result: 在全球SeasFire数据集（2001-2021）上进行测试，TeleViT在各预测提前期（最远到4个月）AUPRC优于U-Net++、ViT等主流模型。在地区性分析中，对季节性规律强的区域（如非洲稀树草原）效果最好，对苔原和干旱区效果较低。

Conclusion: 显式融合全球气候遥相关信息的视觉Transformer方法，能显著提升全球野火亚季节-季节尺度的可预测性，对相关资源管理和预警有实际意义。

Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.

</details>


### [16] [Deep Filament Extraction for 3D Concrete Printing](https://arxiv.org/abs/2512.00091)
*Karam Mawas,Mehdi Maboudi,Pedro Achanccaray,Markus Gerke*

Main category: cs.CV

TL;DR: 本文提出了一种适用于大规模3D混凝土打印中主要成型技术的自动化质量控制流程，用于检测和控制打印丝材（层）的几何质量。该流程可兼容多种传感器，无论材料新鲜或固化状态均可应用，实现线上和离线质量监测。


<details>
  <summary>Details</summary>
Motivation: 随着建筑行业对可持续和高效建造的需求增长，大规模3D混凝土打印技术迅速发展。打印过程中，丝材的成型质量对整体结构性能至关重要，但目前缺乏通用且自动化的质量控制方案。

Method: 提出了一套自动化流程，可对丝材几何形状进行质量控制，适用于挤出式和喷射式3D混凝土打印方法。该流程不依赖于特定的传感器，支持使用相机、结构光系统或激光扫描仪采集数据，可用于材料的新鲜或固化状态。

Result: 该流程能够实现打印过程中丝材几何形状的实时或后期自动监测与质量控制，并验证其兼容多种数据采集设备和材料状态。

Conclusion: 自动化、通用的丝材质量控制流程为3D混凝土打印提供了高效、可靠的质量保证，有助于提升构建质量与施工自动化水平，对建筑、工程和施工领域具有重要实际意义。

Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.

</details>


### [17] [Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images](https://arxiv.org/abs/2512.00103)
*Ifeanyi Okala*

Main category: cs.CV

TL;DR: 本文比较了三种基于图像的方法（VGG16、ViT-B/16 和 CoAtNet-Tiny）在用手腕活动记录图像识别抑郁症、精神分裂症和健康人群中的表现，发现CoAtNet-Tiny在准确性和稳定性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备在精神健康监测中的普及，如何有效利用其产生的时序数据进行精神障碍筛查成为重要课题。作者希望比较不同的深度学习方法，从中找出更适合用作精神健康评估的架构。

Method: 使用Psykose和Depresjon数据集的手腕活动信号，将其转换为30×48像素图像，分别用VGG16、ViT-B/16、CoAtNet-Tiny三种模型，在三折交叉验证的框架下进行训练和测试。通过分析训练数据和未见数据的性能（如准确率、精确率、召回率、F1分数），对比三种方法在精神健康状态识别中的表现。

Result: 所有方法在训练数据上拟合良好，但在未见数据上的表现不同。VGG16准确率提升稳健但总体较低，ViT-B/16表现波动大，CoAtNet-Tiny在准确率、稳定性以及对稀少类别（抑郁症、精神分裂症）的指标上均优于其他方法。

Conclusion: CoAtNet-Tiny在基于手腕活动图像的精神健康类别判别中表现最优，显示出混合型模型架构（如CoAtNet）更适合于此类任务。VGG16和ViT-B/16结果较为参差，表明架构选择对心理健康检测模型效能有重大影响。

Abstract: This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.

</details>


### [18] [TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening](https://arxiv.org/abs/2512.00117)
*Ishwaryah Pandiarajan,Mohamed Mansoor Roomi Sindha,Uma Maheswari Pandyan,Sharafia N*

Main category: cs.CV

TL;DR: 该论文提出了一种利用可见光图像和深度学习技术检测太阳能光伏面板故障的新方法，无需昂贵的多模态传感器，有效提升了大规模维护的可及性和性价比。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态成像方法检测光伏面板表面故障虽然有效，但实际在农场及大规模分布式场景下推广有很高的成本和部署难度。因此，需要一种仅依赖普通可见光相机即可准确检测和量化表面故障的解决方案。

Method: 作者提出了TinyViT，一套紧凑的管线，结合了Transformer为基础的分割、光谱-空间特征工程和集成回归分析，对消费级相机拍摄的五颜六色拼接图像进行处理，实现了七类故障的识别和故障严重程度的量化。该方法完全依赖于可见光图像，从而避免了对电致发光或红外传感器的依赖。

Result: 在真实的公开数据集上对该系统的分类和回归子模块进行了验证，结果显示其准确率和可解释性与专业化方法相当，表现竞争力强。

Conclusion: 本方法降低了检测太阳能光伏面板表面故障的经济和技术门槛，为资源有限的场景和普遍部署提供了可行路径，为太阳能健康监控领域的广泛应用迈出了重要一步。

Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.

</details>


### [19] [Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance](https://arxiv.org/abs/2512.00125)
*Ruo-Syuan Mei,Sixian Jia,Guangze Li,Soo Yeon Lee,Brian Musser,William Keller,Sreten Zakula,Jorge Arinez,Chenhui Shao*

Main category: cs.CV

TL;DR: 本文提出了一种混合型合成数据生成（SDG）框架，通过结合仿真渲染、域随机化和真实背景合成，实现了无需人工标注的工业零件质量检测。该方法有效解决了实际应用中样本稀缺及类别不平衡问题，显著提升了模型精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在工业质检领域应用广泛，但受限于高质量标注数据的获取难度和缺陷样本稀缺，导致模型鲁棒性和泛化能力有限，实用性受到很大影响。

Method: 作者构建了一个SDG流水线，通过变化零件结构、照明和表面属性合成大量标注好的图像，并将合成零件嵌入真实场景背景。训练使用YOLOv8n检测+MobileNetV3-small分类，两阶段架构仅用合成数据进行训练，并在真实的工业零件数据集上进行评估。

Result: 方法能在一小时内生成12,960张带标注的图像。只用合成数据训练的模型，在300个真实零件测试中，目标检测mAP@0.5为0.995，分类准确率96%，平衡准确率90.1%。在极端类别不平衡下，方法平衡准确率90-91%，而传统小样本基线仅为50%。

Conclusion: 所提SDG框架在无需人工标注的情况下，有效应对类别不平衡，实现了高效、可扩展且鲁棒的工业质检，为实际生产环境中深度学习系统的大规模应用提供了可行方案。

Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.

</details>


### [20] [Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation](https://arxiv.org/abs/2512.00129)
*Jayan Adhikari,Prativa Joshi,Susish Baral*

Main category: cs.CV

TL;DR: 本论文提出将基于ResNet50的OOD过滤器与YOLO系列（YOLOv8、YOLOv11、YOLOv12）结合，用于乳腺癌检测，有效提升系统对分布外（OOD）输入的鲁棒性与检测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在乳腺癌检测中的应用存在极大分布外（OOD）输入敏感性问题，如处理不同成像模态（CT、MRI、X-ray）或设备变化时，模型容易误检甚至误诊，严重影响实际临床应用的可靠性，因此亟需提升模型应对OOD输入的能力。

Method: 方法上，构建了基于ResNet50的OOD过滤模块，通过余弦相似度与域内图库比对，先 rigidly 排除非乳腺X线影像，再将确认合规输入送入YOLO（YOLOv8/v11/v12）检测，且ResNet50经过12种CNN骨干网络比对后被选优。通过Grad-CAM提升模型可解释性。

Result: 实验表明，该OOD检测模块在分布外测试集上准确率达100%，总体达到99.77%；检测任务在mAP@0.5指标下得到0.947，相较未过滤OOD的系统更能避免误报，并在乳腺摄影数据上保持高检测率。

Conclusion: 综合框架显著提升了基于AI的乳腺癌检测系统在多样化、异构临床环境下的鲁棒性和实用性，为实际临床部署奠定了可靠基础。

Abstract: Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.

</details>


### [21] [Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition](https://arxiv.org/abs/2512.00130)
*Fadi Dornaika,Danyang Sun*

Main category: cs.CV

TL;DR: 本文提出LGCOAMix，一种基于超像素的上下文与对象部分感知数据增强方法，解决了传统Cutmix方法在本地判别特征利用不足和生成样本与标签不一致性带来的局限性，在多个分类与弱监督定位任务上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Cutmix基于的是全局语义，忽略了类别的判别性本地上下文，且大多使用简单矩形区域，导致对象信息丢失。同时，为解决混合图片与标签不一致，需要双前馈或外部模型，效率低下。

Method: 提出了一种新颖的超像素注意力标签混合策略，在数据增强过程中，不再使用传统矩形切片，而是以超像素网格方式进行图像混合，并通过判别性超像素区与跨图像超像素对比信息，强化局部特征学习。该方法高效并兼顾上下文与对象局部结构。

Result: 在多个主流数据集分类任务及弱监督目标定位上，LGCOAMix显著超越了现有Cutmix类增强方法，无论在CNN还是Transformer架构下都表现优异。

Conclusion: LGCOAMix高效、对象局部特征感知强，是Cutmix类数据增强方法的新突破，为提升深度模型泛化能力提供了新的解决方案。

Abstract: Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.

</details>


### [22] [Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems](https://arxiv.org/abs/2512.00179)
*Mohamed Abdallah Salem,Nourhan Zein Diab*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级卷积神经网络（CNN），可高效识别激光散斑图像中的多类材料，在保证高识别准确率的同时显著减少模型参数量，适用于边缘设备的实时激光切割场景。


<details>
  <summary>Details</summary>
Motivation: 激光切割中准确识别材料类型对于切割质量和设备安全至关重要，但现有散斑识别方法依赖“大模型”或识别材料种类有限，不适用于资源约束设备和实际多样材料环境。

Method: 设计了一种专用于散斑图模式识别的轻量型CNN网络，在参数数量大幅减少的前提下具备高区分能力，使用SensiCut数据集训练，该数据集含59类各种材料。

Result: 新模型在59类材料测试集上取得95.05%的准确率和0.951的宏/加权F1分数，仅含34.1万个参数（约1.3MB），比ResNet-50小70倍，推理速度达295张/秒，可在树莓派和Jetson等嵌入式设备上流畅运行。聚合同类材料识别时召回率超过98%。

Conclusion: 该领域专用的紧凑CNN模型不仅性能优于大型通用骨干网络，还可直接支持激光切割设备自动选择加工参数，为材料自适应、边缘可部署的智能切割系统奠定基础。

Abstract: Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.

</details>


### [23] [AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI](https://arxiv.org/abs/2512.00194)
*Zag ElSayed,Grace Westerkamp,Gavin Gammoh,Yanchen Liu,Peyton Siekierski,Craig Erickson,Ernest Pedapati*

Main category: cs.CV

TL;DR: 本文提出了ICVision系统，将视觉和自然语言推理引入EEG ICA组分的自动分类，达到专家级水平，并提供可解释的输出。其在多个数据集上的表现优于现有主流方法，并已作为开源平台的核心模块。


<details>
  <summary>Details</summary>
Motivation: 传统EEG ICA组分分类方法依赖人工特征，无法充分利用医生通过图表可视化进行判断的专业能力。需要开发能像专家一样“看到”和“解释”脑电图组分的新型AI方法。

Method: ICVision采用多模态大型语言模型（GPT-4 Vision），直接分析ICA可视化图（地形、时间序列、功率谱、ERP图），并通过类人解释和推理将EEG组分分类到六类（脑、眼、心脏、肌肉、通道噪声、其他噪声），返回置信度和解释说明。

Result: 在124组EEG数据共3168个ICA组分上测试，ICVision与专家共识的kappa值为0.677，超越主流工具MNE ICLabel，并有效保留了临床相关脑信号；97%以上的AI解释被专家认为可理解且可用于决策。

Conclusion: ICVision实现了AI视觉认知和自然语言推理在神经生理学中的首次应用，是EEG分析领域的重要变革，为全球可解释和可复现脑电流程奠定了基础，同时标志着AI专家级决策能力的崛起。

Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.

</details>


### [24] [Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting](https://arxiv.org/abs/2512.00198)
*Shantanu Ghosh,Vedant Parthesh Joshi,Rayan Syed,Aya Kassem,Abhishek Varshney,Payel Basak,Weicheng Dai,Judy Wawira Gichoya,Hari M. Trivedi,Imon Banerjee,Shyam Visweswaran,Clare B. Poynton,Kayhan Batmanghelich*

Main category: cs.CV

TL;DR: Mammo-FM是一种专为乳腺X光影像设计的基础模型，预训练于迄今最大、最多样化的数据集上，并在多项临床关键任务中优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因之一。现有基础模型多为通用类型，难以满足乳腺影像诊断等特定领域的需求，因此亟需开发专门针对乳腺影像的基础模型，以提升诊断、风险评估等核心任务的效率和准确性。

Method: 提出Mammo-FM模型，基于来自四家美国机构、覆盖140,677名患者（821,326份乳腺X光片）的超大规模多样化数据集进行预训练，实现图像与文本对齐，支持多任务，包括癌症诊断、病灶定位、结构化报告生成和风险评估。

Result: Mammo-FM在内外部分布数据集上的诊断、预后、报告生成等任务中进行了严格评估。与当前主流通用基础模型相比，即便参数量只有其三分之一，Mammo-FM在多个公开和私有基准测试中均表现更佳。

Conclusion: 专为临床领域（如乳腺影像）设计的基础模型在任务表现和效率上均具有显著优势，强调了针对领域任务进行严格评估的重要性，对实际临床应用具有更高价值。

Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.

</details>


### [25] [ReactionMamba: Generating Short &Long Human Reaction Sequences](https://arxiv.org/abs/2512.00208)
*Hajra Anwar Beg,Baptiste Chopin,Hao Tang,Mohamed Daoudi*

Main category: cs.CV

TL;DR: ReactionMamba是一种新的人体3D反应动作生成框架，结合了运动VAE和基于Mamba的状态空间模型，能高效生成短/长、不同行为序列，其性能优于现有方法，同时提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前3D人类动作生成技术在生成复杂、连续、长动作序列时面临效率和一致性挑战。论文旨在突破现有方法在这些方面的局限，提升动作生成的多样性、真实感和速度。

Method: 该方法将动作变分自编码器（VAE）与Mamba状态空间模型结合。先用VAE高效编码动作特征，再通过Mamba模型进行解码，保证时间序列上的一致性。该框架可灵活适应不同长度和复杂程度的动作序列生成。

Result: 在NTU120-AS、Lindy Hop和InterX三个数据集上进行了实验，结果显示该方法在动作真实感、多样性和长序列生成方面与其他先进方法（InterFormer、ReMoS、Ready-to-React）相当甚至更优，并大幅提升了推理速度。

Conclusion: ReactionMamba实现了高效且多样化的3D人体动作生成，尤其适合长、复杂动作场景。方法在业内主流数据集上展现了优异的性能，证明其广泛适用性和实用价值。

Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.

</details>


### [26] [DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation](https://arxiv.org/abs/2512.00226)
*Zirui Wang,Tao Zhang*

Main category: cs.CV

TL;DR: 本文提出了DenseScan，一个带有多层次丰富语义注释的3D场景理解数据集，通过自动化流程生成并辅助多模态大模型提升3D任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解数据集通常只包含几何和实例信息，缺乏细致的语义标注，限制了视觉-语言任务的发展。为解决这一不足，作者希望提供兼具几何与语义细节的数据。

Method: 作者构建了DenseScan数据集，利用多视角2D图像和多模态大模型自动生成详细的对象级场景描述，并基于场景内容自动生成与场景相关的高阶问题，注重对象属性、空间关系和上下文语义。

Result: DenseScan显著提升了3D环境下目标理解能力和问答任务表现，优于传统标注手段。作者提供了完整数据集及注释流水线，获得更丰富的对象级和上下文语义。

Conclusion: DenseScan为视觉-语言导航、交互式问答等3D下游任务提供了新的基石，有望推动机器人、增强现实等领域的场景理解研究走向更高语义层次。

Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.

</details>


### [27] [Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views](https://arxiv.org/abs/2512.00255)
*Kunwar Maheep Singh,Jianchun Chen,Vladislav Golyanik,Stephan J. Garbin,Thabo Beeler,Rishabh Dabral,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 论文提出了一种创新的人体再光照与自由视角渲染方法——RHC（Relightable Holoported Characters），利用稀疏视角RGB视频，仅靠单次网络推理即可实现高动态全身人物在新光照下的高拟真渲染，且无需传统的繁琐数据采集流程。


<details>
  <summary>Details</summary>
Motivation: 传统真人再光照技术依赖于一次只点亮一个光源（OLAT）的采集方法，操作复杂且效率低下，难以满足动态人体、自由景别和多样光照的实际需求，因此迫切需要更高效、灵活的人物再光照与渲染方案。

Method: 作者提出了基于Transformer架构的RelightNet网络，利用新设计的数据采集策略，在多视角光舞台下采集交替的环境光和统一光追踪帧，同时生成物理启发的几何和外观特征；RelightNet结合这些特征与新的光照条件，通过3D高斯点云在单次前向推理中回归再光照外观，无需繁琐的光照基底采集与重建。

Result: 实验表明，该方法在视觉效果和光照还原准确性等方面领先于当前最先进的方法，实现了高保真、细腻、动态的人体再光照与渲染。

Conclusion: RelightNet及其配套采集与物理特征设计，极大提升了动态人体在新环境光照下的自由视角渲染效率和质量，为虚拟人物、影视特效、AR/VR等领域提供了高效实用的新工具。

Abstract: We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/

</details>


### [28] [UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations](https://arxiv.org/abs/2512.00261)
*Yuzhen Hu,Saurabh Prasad*

Main category: cs.CV

TL;DR: 提出了UniDiff框架，用于在标注稀缺情况下，将ImageNet预训练扩散模型有效适配到多种遥感模态，实现了多模态数据的有效融合和特征提取。


<details>
  <summary>Details</summary>
Motivation: 当前多模态遥感任务中的标注数据稀缺，严重限制了监督学习模型（如MSFMamba）的实际应用，即使这些方法架构上有了进步；而直接将ImageNet预训练模型迁移到异构遥感模态（如HSI和SAR）在缺乏大规模标注的情况下依然困难。

Method: 提出UniDiff方法，通过仅调整约5%的参数，将单一ImageNet预训练的扩散模型自适应于多种遥感模态。其核心机制包括：基于FiLM的步长-模态条件设定、参数高效的适配和伪RGB锚定以保持预训练表示能力、防止遗忘。该框架完全依赖目标域数据，无需大量标注。

Result: 在两个主流多模态遥感基准数据集上，UniDiff展现出较强的无监督适配能力，能有效缓解由于标注稀缺带来的瓶颈，实现多模态遥感数据的有效融合。

Conclusion: UniDiff作为一种高效的参数自适应框架，能在标注稀缺情形下跨模态迁移和融合预训练扩散模型，为多模态遥感应用带来更强的灵活性和实际应用价值。

Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.

</details>


### [29] [HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction](https://arxiv.org/abs/2512.00264)
*Zhengda Ma,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出了HeartFormer，这是第一个利用点云表示的几何深度学习框架，用于从cine MRI数据中重建3D心脏四腔结构。并发布了大规模公开数据集HeartCompv1。模型在多个实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的cine MRI只能获得心脏的2D切片图像，无法全面理解心脏结构及生理机制。本研究旨在克服这一限制，实现三维、精细的心脏重建。

Method: 提出了点云补全网络HeartFormer，包括两个变换器模块：SA-DSTNet用于生成包含全局和子结构信息的初步点云，SA-GFRTNet在语义和几何引导下精细化输出。还构建了包含17,000个高分辨率三维心脏网格和点云的新数据集HeartCompv1。

Result: 在HeartCompv1和UK Biobank的大量跨领域实验中，HeartFormer展示了强鲁棒性、准确性和泛化能力，表现持续优于当前主流方法。

Conclusion: 通过首个基于点云的深度学习框架和数据集，为三维心脏重建提供了新思路和基准，有望推动该领域研究与临床应用的发展。

Abstract: We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.

</details>


### [30] [USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing](https://arxiv.org/abs/2512.00269)
*Jun Wang,Peirong Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种统一的脑影像生成与编辑框架（USB），可在健康与病理脑影像之间进行双向生成和编辑，并在多个公开数据集上取得了优秀效果。


<details>
  <summary>Details</summary>
Motivation: 健康与病理脑结构之间的关系对于神经影像学具有基础性意义，但缺乏成对的病理-健康脑影像数据，导致现有方法多专注于单一领域，无法同时建模二者。

Method: 提出USB框架，通过配对扩散机制（paired diffusion mechanism）联合建模病变与脑解剖结构，实现健康与病理影像的双向生成；引入一致性引导算法，确保在编辑过程中解剖结构和病变的一致性。

Result: 在包括健康、卒中和阿尔茨海默症患者的六个公开脑MRI数据集上进行实验，USB框架生成了多样且真实感强的脑影像。

Conclusion: USB首次实现了脑影像统一生成与编辑的基准，推动了数据集扩展与神经影像学分析的新方向。

Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.

</details>


### [31] [HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention](https://arxiv.org/abs/2512.00275)
*Yi Liu,Yi Wan,Xinyi Liu,Qiong Wu,Panwang Xia,Xuejun Huang,Yongjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种适用于遥感图像的轻量级超分辨率框架HIMOSA，实现了高效推理与优异重建性能的有机结合。


<details>
  <summary>Details</summary>
Motivation: 遥感应用如灾害检测与响应需要实时、高效和轻量级模型，但现有方法在模型性能和计算效率之间存在权衡问题。

Method: 提出HIMOSA框架，利用遥感图像固有冗余，引入内容感知稀疏注意力机制，并采用分层窗口扩展与注意力稀疏度调整，提升多尺度重复模式利用与计算效率。

Result: 在多种遥感数据集上进行了大量实验，结果显示该方法在保持计算效率的同时，实现了当前最优的性能。

Conclusion: HIMOSA框架有效解决了遥感图像超分任务中的性能与效率权衡问题，具有广泛应用潜力。

Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.

</details>


### [32] [Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth](https://arxiv.org/abs/2512.00281)
*Sylvain Bodard,Pierre Baudot,Benjamin Renoust,Charles Voyton,Gwendoline De Bie,Ezequiel Geremia,Van-Khoa Le,Danny Francis,Pierre-Henri Siot,Yousra Haddou,Vincent Bobin,Jean-Christophe Brisset,Carey C. Thomson,Valerie Bourdes,Benoit Huet*

Main category: cs.CV

TL;DR: 作者提出了一种AI系统，能够在低剂量CT上直接对肺结节进行检测和恶性肿瘤诊断，准确率高于放射科医师和现有AI模型。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节筛查极度依赖结节的大小与生长速度，导致诊断延迟。此外，AI应用面临数据集规模有限和解释性差的双重挑战。

Method: 作者开发了一个结合浅层深度学习和基于特征的专用模型的集成AI系统，并在包含超25,000次扫描和近70,000个结节的大数据集上训练和评估。

Result: 该系统在内部数据集上AUC达到0.98，在独立队列上AUC为0.945，均超越放射科医生、Lung-RADS及多个领先AI模型。其灵敏度达99.3%、假阳性率低，在所有结节尺寸和各分期均优于人类，尤其是早期和生长缓慢的肿瘤。

Conclusion: 该AI系统可有效克服当前肺癌筛查的主要难点，有望提前诊断，提升筛查效果，加速AI在临床的采纳。

Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.

</details>


### [33] [Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR](https://arxiv.org/abs/2512.00294)
*Lixing Guo,Tobias Höllerer*

Main category: cs.CV

TL;DR: 该论文提出了一种结合多模态大语言模型（MLLMs）与坐标感知视觉模型的模块化AR智能体系统，以增强AR对自然语言空间关系查询和交互的能力。


<details>
  <summary>Details</summary>
Motivation: 现有AR系统依赖固定类别检测器或标记，难以理解和响应复杂开放式自然语言查询，特别是在涉及空间关系和多目标推理时，因此有必要突破AR对场景理解和空间推理的能力。

Method: 该系统采用自适应任务智能体协调MLLMs和视觉感知工具，支持从简单物体识别到多物体关系推理的多样化查询，并以米级精度返回3D锚点。通过动态构建包含九种关系类型的AR场景图，实现对象的空间、结构语义和因果功能等丰富关系表达。系统还实现了区域高亮和空间信息检索，引导用户关注关键信息，支持人机交互和查询精细化。采用模块化架构，可灵活集成新视觉语言模型，无需重新训练。

Result: 系统能够有效处理不同复杂度的空间查询，实现物体及其空间关系的多模态推理与定位；支持复杂查询如选择、测量、对比及动作驱动的实际操作。提出了GroundedAR-Bench，可用于多样环境下基于语言的本地化与关系推理性能评估。

Conclusion: 论文展示了将MLLMs与空间智能相结合的AR智能体新路径，实现了AR场景交互和理解能力的显著提升，突破了传统系统依赖固定检测的局限，为现实场景中基于语言的空间推理和人机交互提供了创新解决方案。

Abstract: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.

</details>


### [34] [TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion](https://arxiv.org/abs/2512.00300)
*Rui Qian,Haozhi Cao,Tianchen Deng,Tianxin Hu,Weixiang Guo,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为TGSFormer的时序高斯投影新方法，有效提升了3D语义场景补全的精确度与可扩展性，并节约了计算和存储资源。


<details>
  <summary>Details</summary>
Motivation: 现有高斯方法由于随机初始化冗余原语，导致在处理大范围场景时效率低下且不具备良好扩展性。即使是深度引导的方法也局限于局部范围，随规模增大带来延迟和显存负担。为此，作者亟需一种高效且适用于长时、广域场景的3D场景补全框架。

Method: TGSFormer持续维护一个时序高斯内存，实现历史与当前观测联合预测，无需依赖逐帧缓存或图像连贯性。其提出的“双时序编码器”通过置信度感知交叉注意力机制融合历史与当前高斯特征。之后，“置信体素融合模块”将重叠原语聚合为体素对齐表现，调控密度并提升紧凑性。

Result: TGSFormer在本地和具身3D场景补全主流基准上验证了优越表现：用更少的原语实现更高准确率，并在大场景下展现更优的可扩展性及长时场景一致性。

Conclusion: TGSFormer有效解决了大范围、长期3D语义场景补全中的冗余和可扩展性难题，在保持高准确率和紧凑表达的同时，推进了具身感知场景理解技术的实用化。

Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.

</details>


### [35] [Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation](https://arxiv.org/abs/2512.00308)
*Xiao Cui,Yulei Qin,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于最优传输（Optimal Transport, OT）的新型数据集蒸馏方法，能更好地保持全局和实例级别的数据分布特征，并在多个数据集和架构上大幅提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集的蒸馏方法主要关注于全局统计信息匹配（如均值和方差），但忽视了实例级特征和类内差异，导致蒸馏后模型泛化能力不足。作者旨在解决这一局限，实现更细致、几何结构保真的数据蒸馏。

Method: 将数据集蒸馏问题重构为最优传输距离最小化，通过OT引导的扩散采样、标签与图像对齐的软重标记和基于OT的logit匹配三个模块，全流程在全局与实例级别进行分布对齐，保留了复杂高维分布的局部结构和类内模式。

Result: 在ImageNet-1K等大型数据集和多种网络架构下，所提方法在IPC=10设定下，比当前最优方法至少提升了4%的精度，并表现出稳定高效的优势。

Conclusion: 引入最优传输理论对数据集蒸馏进行细粒度分布对齐，有效提升了蒸馏数据的表达能力和模型的泛化性能，在大规模场景下具有广泛的实用价值。

Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.

</details>


### [36] [ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays](https://arxiv.org/abs/2512.00310)
*Qinyi Cao,Jianan Fan,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种针对胸部X光片的无监督异常检测新方法，通过合成更符合解剖结构特征的异常，并提供像素级掩码，以提升异常分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成异常的方法在异常检测领域难以生成与实际病理现象相似且符合解剖结构的异常，限制了训练时的监督效果。

Method: 提出了ART-ASyn框架，通过真实纹理增强结合自主提出的分步二值阈值分割（PBTSeg）算法对肺部进行分割后，生成逼真的、与解剖结构一致的异常。对每一个正常样本，产生合成异常及其精确像素级标注，实现显式监督。

Result: ART-ASyn框架不仅在常规一类分类任务中优于以往方法，并且能够在零样本异常分割任务中于未见数据集上展现出较强的泛化能力。

Conclusion: ART-ASyn极大提升了合成异常的真实性与结构一致性，为无监督异常检测与分割提供了新思路和可迁移工具。

Abstract: Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.

</details>


### [37] [Odometry Without Correspondence from Inertially Constrained Ruled Surfaces](https://arxiv.org/abs/2512.00327)
*Chenqi Zhu,Levi Burner,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种基于从图像序列中直线轨迹生成的光滑曲面（ruled surface），结合IMU惯性测量，用于提升视觉里程计与三维重建效果的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉里程计依赖点特征匹配和光流计算，但特征匹配计算量大且容易受噪声影响，影响位姿估计精度。部分研究尝试用线特征或多传感器融合，但依赖对应问题仍突出。因此需要新的方法绕过点对应的难题，同时提升算法鲁棒性。

Method: 核心方法是分析相机观察到运动中直线在图像-时间空间中生成的ruled surface（光滑曲面），通过分析其形状推导出里程计信息。算法基于event camera对边缘的灵敏检测能力，结合IMU惯性约束降低解空间维度，利用点到直线的微分更新实现高效估算。

Result: 算法可有效同步重建3D场景结构，并显著提升了视觉里程计的效率和准确性，避免了复杂的点对点特征对应。实验表明新方法在鲁棒性和实时性上具优势。

Conclusion: 研究提出了结合ruled surface与IMU的新视觉里程计框架，解决了特征对应依赖问题，为高效准确的三维重建和运动估计提供了新思路。

Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.

</details>


### [38] [MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection](https://arxiv.org/abs/2512.00336)
*Mengxue Hu,Yunfeng Diao,Changtao Miao,Jianshu Li,Zhe Li,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了首个用于检测AI生成多模态视频-音频内容的大型数据集MVAD，以应对现有数据集仅关注视觉或仅限于人脸伪造的问题，并提升检测系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的多模态（视频+音频）内容快速发展，带来了信息安全和内容真实性的担忧。然而，已有伪造数据集主要集中在视觉模态，极少涉及音频，且大多只覆盖人脸伪造，无法满足对广泛多模态AI内容的检测需求。

Method: 作者构建了MVAD数据集，特点包括：针对三种真实常见的视频-音频伪造模式生成样本；采用多种先进生成模型确保高感知质量；数据涵盖写实与动漫风格，四大内容类型（人类、动物、物品和场景）以及四种视频-音频多模态数据形式。

Result: MVAD成为首个覆盖广泛内容和伪造类型、且具高感知质量的多模态视频-音频伪造检测数据集。相关数据发布在GitHub，供研究和检测系统开发者使用。

Conclusion: MVAD数据集填补了AI生成多模态内容检测数据集的空白，将促进更加可靠和全面的伪造检测方法的发展，是推进内容安全研究的重要资源。

Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.

</details>


### [39] [Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models](https://arxiv.org/abs/2512.00343)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需先验知识即可检测视觉-语言预训练模型（如CLIP）后门的新方法AMDET，能高效识别和区分自然与注入型后门。


<details>
  <summary>Details</summary>
Motivation: VLP模型在实际应用中易被第三方注入后门，现有检测方法依赖于训练数据、触发器或下游分类器信息，难以适用于实际无先验场景，因此亟需无需这些先验的通用后门检测机制。

Method: 提出AMDET框架，无需训练集、触发器或目标等先验，仅依赖模型本身。其关键点在于揭示后门样本中文本编码器的特征同化效应，通过梯度反演恢复可激发后门行为的隐含特征，并结合loss landscape区分自然后门和注入后门。

Result: 在三种常用VLP结构、两种攻击范式下，对3600个后门和良性模型实验，AMDET取得了89.90%的F1分数。单次检测仅需约5分钟，并对自适应攻击表现出较强鲁棒性。

Conclusion: AMDET首次为VLP后门检测提供了无需先验的新思路，验证了方法通用性、效率和安全性，有望为相关模型部署提供实用保障。

Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET

</details>


### [40] [mmPred: Radar-based Human Motion Prediction in the Dark](https://arxiv.org/abs/2512.00345)
*Junqiao Fan,Haocong Rao,Jiarui Zhang,Jianfei Yang,Lihua Xie*

Main category: cs.CV

TL;DR: 本文提出利用毫米波雷达进行人体运动预测，有效解决了RGB-D相机在隐私保护和鲁棒性方面的问题，通过新颖的扩散模型框架提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB-D相机的人体运动预测方法容易受光照等环境影响，并存在隐私泄露隐患，无法在消防、医疗等实际场景中广泛应用。毫米波雷达具有鲁棒且保护隐私的优势，因此引入其作为新型感知手段进行人体运动预测。

Method: 本文提出了mmPred，这是首个针对毫米波雷达信号定制的扩散生成式预测框架。mmPred利用时域姿态细化分支（TPR）学习人体细致动作，频域主动作分支（FDM）捕捉整体运动趋势，解决雷达反射和多路径导致的噪声与时序不一致问题，并设计了全局骨架关系Transformer（GST）作主干网络，实现多关节协同恢复。

Result: 在mmBody与mm-Fi数据集测试中，mmPred分别比现有方法提升8.6%与22%，展现了显著的性能提升。

Conclusion: mmPred为雷达驱动的人体运动预测开创了新方向，不仅克服了传统RGB-D方案的现实局限，还在实际数据集上取得了最先进的结果，具有良好应用前景。

Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.

</details>


### [41] [SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction](https://arxiv.org/abs/2512.00355)
*Junqiao Fan,Pengfei Liu,Haocong Rao*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于Spatial Mamba的单阶段扩散模型SMamDiff，用于人体运动预测，实现了高空间-时间一致性、低延迟和低内存消耗。模型在主流数据集上获得了同行领先的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的运动预测方法存在要么预测结果单一、缺乏不确定性体现，要么在采用概率建模时牺牲动力学可行性的问题。扩散模型虽可平衡准确性和多样性，但多为多阶段实现，难以部署在边缘设备。本文旨在提出一种既准确、多样，又高效、易部署的单阶段人体运动预测方法。

Method: 作者提出SMamDiff模型，包含两个创新设计：(1) 残差-离散余弦变换（residual-DCT）运动编码，提升了关键信息的识别能力；(2) stickman-drawing空间mamba模块，实现了关节间顺序建模，增强关节间的关联和时空一致性。

Result: 在Human3.6M与HumanEva数据集上，SMamDiff在单阶段概率预测方法中达到了最优性能，并且比多阶段扩散基线方法在延迟和内存消耗方面更具优势。

Conclusion: SMamDiff模型能够实现高效、空间-时间一致且动力学合理的人体运动预测，为实际边缘智能场景的部署提供了有力支持。

Abstract: With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.

</details>


### [42] [MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters](https://arxiv.org/abs/2512.00363)
*Jianhong Han,Yupei Wang,Yuan Zhang,Liang Chen*

Main category: cs.CV

TL;DR: 本文提出了MM-DETR框架，实现高效且轻量的多模态遥感目标检测，兼顾性能与模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态遥感目标检测方法在准确性和轻量化之间难以平衡：共享主干网络导致模态特征建模能力不足，双流结构则使模型参数成倍增加，影响实际应用。

Method: 提出MM-DETR框架，包括基于Mamba的双粒度融合编码器，采用通道动态门控与1D选择性扫描提升跨模态融合效率；将多模态融合重新定义为模态补全，通过区域感知2D扫描分支恢复模态特征，实现细粒度双向金字塔融合；在共享主干中插入轻量频率感知模态适配器，以空间-频率联合结构和像素级动态路由实现高效空间频率融合。

Result: 在四个多模态遥感数据集上进行大量实验，验证了方法的有效性和泛化能力。

Conclusion: MM-DETR兼顾检测性能和轻量化设计，为多模态遥感目标检测提供了实用解决方案。

Abstract: Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.

</details>


### [43] [Towards aligned body representations in vision models](https://arxiv.org/abs/2512.00365)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 本文探讨了人类对物体的粗略、体积型“身体”表征如何支持直觉性物理推断，并分析了图像分割模型是否会形成类似的表征。实验发现小型分割模型更容易形成类似人类的粗表征。


<details>
  <summary>Details</summary>
Motivation: 心理物理学研究表明人类在物理推理时使用物体的粗略内部表征，但具体结构尚不清楚。作者希望通过机器视觉模型来探索这种表征的本质。

Method: 作者设计了一个基于50名人类参与者的心理物理实验，并将其适配为语义分割任务，对七种不同规模的语义分割神经网络进行测试和分析。

Result: 实验显示，小型分割模型更容易自动学习到人类类似的粗略“身体”表征，大型模型则倾向于细致过度的编码。

Conclusion: 有限计算资源下的分割模型可以自发形成粗略表征，为理解大脑中物理推理的结构提供了新视角，机器模型有望帮助解析人类物理推理过程。

Abstract: Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.

</details>


### [44] [THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2512.00368)
*Jian Zhu*

Main category: cs.CV

TL;DR: 提出了一种针对多视图聚类（MVC）数据融合信任性问题的新方法，通过引入层次化对比表征学习（THCRL）提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多视图聚类可提升数据分类效果，但现有方法融合视图信息时易受噪声和片面相似性计算影响，造成融合不准确和性能降低。该文旨在解决噪声干扰和单一实例对比忽略结构信息的问题。

Method: 提出THCRL方法，包括两个核心模块：（1）DSHF模块，结合UNet结构和多重去噪机制，实现高可信的多视图特征融合；（2）AKCL模块，将融合后的表征与单视图表征对齐，并通过同簇样本的邻近关系增强正向对比，改进对比学习过程。

Result: 在多个深度多视图聚类任务上，大量实验表明该方法优于现有同类方法，达到了最新最优表现。

Conclusion: THCRL有效提升了多视图数据融合的信任性和聚类效果，为深度多视图聚类研究提供了新方向。

Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.

</details>


### [45] [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)
*Wenshuo Chen,Haosen Li,Shaofeng Liang,Lei Wang,Haozhe Jia,Kaishen Yuan,Jieming Wu,Bowen Tian,Yutao Yue*

Main category: cs.CV

TL;DR: 论文提出POLARIS方法，通过动态调整指导系数，有效减少扩散模型在反演过程中的噪声近似误差，从而提升图像编辑与恢复的表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像反演和去噪任务中，因每一步噪声近似而累积大量误差，影响重建质量。以往方法多通过优化隐变量来补偿误差，而忽略了误差来源本身。

Method: 提出POLARIS框架，将反演过程中的误差来源问题数学化，并引入分步可变的指导系数ω，通过正交最小二乘法在每一步最小化近似误差。方法实现简单，仅需“一行代码”即可集成。

Result: POLARIS能显著抑制噪声近似造成的误差积累，提升反演潜变量的质量，提高图像编辑与恢复等下游任务的准确性，且计算开销极小。

Conclusion: POLARIS为基于扩散模型的图像反演提供了坚实的理论支持与高效实用的落地方案，显著改善现有方法在反演任务中的表现。

Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.

</details>


### [46] [Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features](https://arxiv.org/abs/2512.00381)
*Dong Li,HuaLiang Lin,JiaYu Li*

Main category: cs.CV

TL;DR: 本文提出了一个高质量毛孔级别的人脸图像块数据集（PorePatch）和配套评测基准，并利用端到端深度学习模型进行性能验证。虽然在特征匹配任务中大大优于传统方法，但在三维重建任务上的提升有限，显示深度特征在弱纹理区域仍有明显局限。


<details>
  <summary>Details</summary>
Motivation: 人脸皮肤区域的弱纹理特性使得局部描述符难以匹配，影响如面部动作分析和三维人脸重建等任务。当前缺乏大规模高分辨率的毛孔级训练数据集，限制了深度学习方法在这一领域的发展与应用。

Method: 作者提出了PorePatch毛孔级图像块数据集，并设计了Data-Model Co-Evolution（DMCE）框架从高分辨率人脸图像中逐步生成高质量图像块数据。用该数据集训练并评估已有的最优深度模型，进行详细实验分析。

Result: 深度学习SOTA模型在图像块匹配任务中取得FPR95为1.91%的结果，比传统PSIFT（22.41%）高出20.5%。但在三维重建任务中，深度方法的性能优势并不显著，仅与传统方法相当。

Conclusion: 尽管深度学习描述符在拼接匹配任务上表现突出，但在解决人脸弱纹理区的三维重建挑战方面仍有明显不足。未来在特定场景下需要改进深度特征方法。

Abstract: The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.

</details>


### [47] [EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation](https://arxiv.org/abs/2512.00385)
*Louis Geist,Loic Landrieu,Damien Robert*

Main category: cs.CV

TL;DR: 本文提出了一种基于superpoint的全GPU分割算法EZ-SP，在3D语义分割中实现了更快的几何和语义分割，同时模型小巧，推理速度快，且与最新point-based方法精度相当。


<details>
  <summary>Details</summary>
Motivation: 现有superpoint方法在分割阶段高度依赖CPU，导致处理速度成为瓶颈，限制了3D分割在大规模场景和实时应用中的推广。

Method: 作者设计了一个可学习的、全GPU执行的superpoint分割模块（小于6万参数），采用可微分的代理损失函数训练，无需手工特征。分割后结合轻量级的superpoint分类器，整个流程内存占用极低，支持大场景和实时推理。

Result: EZ-SP分割速度比以往方法快13倍，整体推理速度快72倍，参数量减少120倍，并在3个不同领域的数据集上（室内、自动驾驶、航空LiDAR）达到与SOTA point-based方法相似的准确率。

Conclusion: EZ-SP大幅提升了superpoint方法的速度和效率，保留高精度的同时极大减小了模型规模和运算负担，适合大规模数据和实时3D语义分割任务。

Abstract: Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\times$ faster inference and 120$\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.

</details>


### [48] [WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing](https://arxiv.org/abs/2512.00387)
*Kaihang Pan,Weile Chen,Haiyi Qiu,Qifan Yu,Wendong Bu,Zehan Wang,Yun Zhu,Juncheng Li,Siliang Tang*

Main category: cs.CV

TL;DR: 提出WiseEdit，一个针对认知和创造性图像编辑模型评价的新型基准，具有更全面和深入的任务设置。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑基准过于狭隘，无法全面评估新一代具备认知和创造能力的编辑模型，需要更具挑战性的评价体系。

Method: 通过类比人类认知创作过程，将图像编辑任务分为“感知、解释、想象”三个层次，每层单独设定挑战任务，并设计复杂任务涵盖多层难点。覆盖声明性、程序性和元认知三种知识类型，总计设计1220个测试用例。

Result: 通过WiseEdit测试，客观揭示了先进模型在知识推理和创新合成方面的不足。

Conclusion: WiseEdit为评估具认知和创意能力的图像编辑模型提供了更全面的基准，有助于推动该领域模型能力的提升。

Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.

</details>


### [49] [Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction](https://arxiv.org/abs/2512.00395)
*Jiazhen Liu,Mingkuan Feng,Long Chen*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的多模态大语言模型分割方法（STAMP），能够同时兼顾对话能力、高分割性能与推理速度。论文通过非自回归遮罩预测解耦对话生成与分割任务，突破了现有方法无法三者兼得的困境。


<details>
  <summary>Details</summary>
Motivation: 当前将图像分割任务引入多模态大语言模型时，会面临三难困境：难以在保持对话能力、高分割性能和高推理速度三者间达成平衡。传统方法容易导致对话性能下降，或者分割准确率降低，或推理速度不理想。因此，亟需新的方案来解决这一难题。

Method: 作者提出了一种全新的“All-mask prediction”范式，在完成文本对话生成后，通过非自回归方式一次性预测整张分割遮罩。具体做法是将分割视为图像块的并行“填空”任务，从而利用空间上下文信息，规避传统像素级自回归或embedding损失带来的问题。并基于该范式实现了STAMP模型。

Result: 实验结果显示，STAMP在多个分割基准数据集上都显著优于当前最新方法，在对话流畅性、分割准确性和推理速度三方面均表现卓越。

Conclusion: STAMP方法有效解决了多模态大模型在分割任务中的三难困境，实现了无损对话能力、高效分割和快速推理，为该领域提供了新的解决思路。

Abstract: Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.

</details>


### [50] [Low-Bitrate Video Compression through Semantic-Conditioned Diffusion](https://arxiv.org/abs/2512.00408)
*Lingdong Wang,Guan-Ming Su,Divya Kothandaraman,Tsung-Wei Huang,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.CV

TL;DR: 传统视频编解码器在极低比特率下易产生严重失真，原因在于像素精度与人类感知的不一致。DiSCo采用多模态描述并结合生成式模型，实现极低码率下的视频高感知质量重建。实验显示，该方法在感知指标上大幅超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编码方法注重像素级还原，但在压缩极限下（超低比特率）画面遭受严重损失。这是因为像素精度与实际人类观看体验不一致，因此亟需方案只保留真正影响感知的信息，从而提升超低码率下的观感。

Method: 提出DiSCo语义视频压缩框架。首先低比特率下分离出三种高信息密度的模态（文本描述、时空降质视频和可选草图/姿势），分别用于传达语义、外观和运动。利用条件扩散生成模型，通过多种创新（时序前向填充、token交错编码、模态自适应编解码）从这些信息紧凑的模态恢复结构和细节，从而重建高质量并且时序连贯的视频。

Result: DiSCo在低比特率下实现了极高的感知质量提升，在多种感知指标上，相比传统和现有语义压缩方案提升2-10倍。

Conclusion: 人类感知引导的视频编码方案能够在极低带宽场景下显著提升观看体验。通过多模态语义表示与生成式重建模型，DiSCo有效解决了传统像素还原方案感知失真的弊端，具有重要实际应用价值。

Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.

</details>


### [51] [SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control](https://arxiv.org/abs/2512.00413)
*Ji Gan,Lingxu Chen,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了SplatFont3D，一种结构感知的3D艺术字体生成方法，能够根据文本提示精确控制字体结构和风格，在效率和质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的艺术字体生成主要集中在2D平面，针对3D艺术字体的研究较少。然而，3D艺术字体在游戏、动画等沉浸式环境中具有广泛应用，同时3D字体生成还可以提升2D字体视角的多样性。3D字体需要精确的语义和结构控制，并且对细粒度部位风格有高要求，这对生成方法提出了新的挑战。

Method: 作者提出了SplatFont3D框架，利用3D高斯点渲染（Gaussian splatting）实现从文本描述到3D艺术字体的生成。方法包括：1）Glyph2Cloud模块，增强2D字形并生成3D点云作为高斯初始化；2）通过与预训练2D扩散模型的互动优化3D高斯；3）动态组件划分策略，实现3D高斯的部位分割和风格控制，同时解决优化过程中的漂移等问题。

Result: SplatFont3D在部位级风格控制、文本一致性、视觉质量及渲染效率上均显著优于现有3D字体生成方法（如NeRF等）。

Conclusion: SplatFont3D能够以更高的效率和控制粒度生成高质量3D艺术字体，为3D字体生成和相关应用提供了优越的新方法。

Abstract: Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.

</details>


### [52] [PhysGen: Physically Grounded 3D Shape Generation for Industrial Design](https://arxiv.org/abs/2512.00422)
*Yingxuan You,Chen Zhao,Hantao Zhang,Mingda Xu,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出了一种可结合物理知识生成具备高物理真实性3D形状的新方法，显著提升了工业设计场景下的三维模型质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 目前的3D形状生成模型虽然能产出高保真和视觉合理的模型，但对诸如汽车等受工程设计影响显著的物体，形状的“真实感”与其物理属性密切相关。现有方法通常不了解这些物理规律，因此无法有效提升生成形状的物理真实性。

Method: 作者提出了一个基于物理统一建模的3D形状生成管道，核心方法为“显式物理引导的流匹配模型”，包括基于速度的更新与基于物理的修正交替进行。过程中加入了物理感知正则项以进一步强化物理有效性，并设计了可联合编码形状与物理属性的变分自编码器（SP-VAE）来支撑上述流程。

Result: 在三个基准数据集上的实验表明，所提方法能生成在物理真实性上优于依赖纯视觉判断的传统方法的3D形状。

Conclusion: 结合物理知识的3D形状生成为工业设计等领域提供了更加真实且功能性更强的新工具，验证了模型将物理原理与深度生成方法融合的有效性。

Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.

</details>


### [53] [Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali](https://arxiv.org/abs/2512.00424)
*Nthenya Kyatha,Jay Taneja*

Main category: cs.CV

TL;DR: 本论文提出了一套基于CCTV的视频分析流水线，用于自动化提取非洲撒哈拉以南公交乘客的起终点流量（OD Flow），相比人工计数在理想条件下准确率较高，但在实际高压场景下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲公共交通通常过度拥挤，现有的自动化系统难以可靠获取客流数据。由于已大量部署用于安保的CCTV，如何利用这些现有设备提升客流统计精度，成为本研究的核心动机。

Method: 方法上，研究者结合了YOLOv12目标检测、BotSORT多目标跟踪、OSNet行人重识别、基于OCR的时间戳识别，以及基于车载遥感数据的上下车点判别，形成一整套CCTV视频流数据处理流程，并在肯尼亚与卢旺达公交的标注视频上进行评估。

Result: 在光线良好、乘客稀疏的视频片段中，该系统乘客计数的召回率约为95%、准确率91%、F1约93%，输出的OD矩阵与人工计数高度吻合。然而在高强度实际工况，如高峰期拥挤、黑白模式切换、乘客姿态变化以及非标准车门上下车等情况下，系统性能大幅下降，如高峰期上下客严重低估（约少算40%），黑白场景召回率减少17个百分点。

Conclusion: 本研究展示了利用现有CCTV提升公交客流自动采集的可行性，但也揭示了其在撒哈拉以南非洲真实复杂环境下的局限性，强调了亟需面向部署场景进行更健壮的行人重识别技术研发。

Abstract: Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.

</details>


### [54] [What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards](https://arxiv.org/abs/2512.00425)
*Minh-Quan Le,Yuanzhi Zhu,Vicky Kalogeiton,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出了NewtonRewards，一个通过物理约束奖励提升视频生成模型物理真实性的后训练框架，并在新基准NewtonBench-60K上显著提升了物理合理性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在视觉上虽表现优秀，但经常违背基础物理规律（如物体漂浮、加速度异常等），显示视觉真实感与物理真实感之间存在鸿沟，因此需要让视频生成更符合物理定律。

Method: 提出NewtonRewards，使用冻结的工具模型从生成视频中提取可验证代理（光流作为速度代理，高层外观特征作为质量代理），基于这些代理引入牛顿动力学约束奖励（维持常加速度动力学）和质量守恒奖励（防止退化解），在后训练阶段显式加强生成视频的物理结构。

Result: 在包含五种牛顿运动基本类型（自由落体、水平/抛体抛掷、斜坡上下滑）的大规模新数据集NewtonBench-60K上，NewtonRewards在视觉和物理指标上均优于以往方法，表现出更高的物理合理性、运动流畅性和时间连贯性；在高度、速度、摩擦等OOD设置下也表现稳健。

Conclusion: 基于物理约束和可验证奖励的方法可扩展并有效提升视频生成的物理意识，对物理感知的高质量视频生成具有重要意义。

Abstract: Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.

</details>


### [55] [Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana](https://arxiv.org/abs/2512.00428)
*Jiachuan Peng,Kyle Lam,Jianing Qiu*

Main category: cs.CV

TL;DR: 该论文评估了用Nano Banana（一种由Google发布的AI图像生成模型）合成的胸部X光片训练AI分类器，其在真实数据集上的表现表现出了良好的可行性。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据稀缺且存在隐私保护难题，采用AI生成合成数据是否能有效训练医学AI系统具有重要意义。

Method: 作者使用Nano Banana生成大量合成胸部X光片，仅用这些合成数据训练肺炎识别分类器，并在两个真实公共数据集（2018 RSNA和Chest X-Ray数据集）上进行性能测试。

Result: 用纯合成数据训练的分类器在真实数据集上取得了较高的AUROC和AUPR分数，分别为0.923/0.900（RSNA）及0.824/0.913（Chest X-Ray数据集），显示出可行性。

Conclusion: 用AI生成的合成医学影像可以在实际任务中训练出效果较好的模型，证实合成数据在医学AI开发中的潜力。但目前仍存在合成数据多样性控制和后处理等技术挑战，以及临床应用前需严密验证和监管等问题。

Abstract: We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.

</details>


### [56] [FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal](https://arxiv.org/abs/2512.00438)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（FR-TTS），解决了在下一个token预测（NTP）场景下，多样化采样及奖励模型难以有效指导生成过程的核心问题。通过创新性地引入填充型奖励（FR），显著提升了中间样本的评价相关性，从而实现更高质量的生成输出。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放（TTS）在图像生成中表现出色，但难以迁移到基于NTP的生成任务，主要因为中间token序列所生成的图像奖励与最终完整图像奖励相关性低，导致指导采样裁剪效果差。因此，亟需一个能准确衡量中间样本质量的指标。

Method: 作者提出填充型奖励（Filling-Based Reward, FR），即通过为中间token序列寻找合理填充方案，估计其可能的最终输出状况，以提升奖励的相关性。基于FR，作者设计了FR-TTS方法，通过高效搜索填充方案，并结合多样性奖励与动态权重，实现对中间样本的准确、全面评价。

Result: 实验显示，FR作为指标，其相关性和多种内在信号（如token信心）优于其他方法。FR-TTS在多种基准任务和奖励模型上均优于现有方法，验证了其优越性。

Conclusion: FR-TTS提供了有效的测试时缩放方法，用于提升NTP类生成模型的输出质量。填充型奖励创新性强，具有广泛应用前景。

Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.

</details>


### [57] [RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications](https://arxiv.org/abs/2512.00450)
*Amit Kumar Gupta,Farhan Sheth,Hammad Shaikh,Dheeraj Kumar,Angkul Puniya,Deepak Panwar,Sandeep Chaurasia,Priya Mathur*

Main category: cs.CV

TL;DR: 本文提出了RecruitView数据集，包含2011段视频面试片段，并在此基础上提出了一种新的跨模态几何深度学习方法CRMF，在性格与面试表现分析任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化性格与软技能评估任务受限于数据集规模小和方法未能有效捕捉人类特征的几何结构，因此亟需新的数据集和能反映性格复杂结构的新方法。

Method: 1）构建RecruitView数据集，包含超过300名参与者的2011个自然视频面试片段，支持12项维度标注（五大性格、整体性格及6项面试表现）。2）提出CRMF方法，基于超球面、双曲空间和欧氏空间，将输入行为数据投射到3种流形，分别用特定网络捕捉层级结构、方向性和连续性，再通过自适应动态权重融合。3）用切空间原理进行多流形特征的有效融合。

Result: CRMF方法在所提数据集上显著优于主流基线，Spearman相关提升最多11.4%，一致性指数提升6%，参数量相比大模型减少40-50%。

Conclusion: RecruitView丰富且公开的数据集及新颖的几何融合方法CRMF为自动性格和面试能力评估提供了有力工具，具备较强泛化能力与实际应用潜力。

Abstract: Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView

</details>


### [58] [CausalAffect: Causal Discovery for Facial Affective Understanding](https://arxiv.org/abs/2512.00456)
*Guanyu Hu,Tangzheng Lian,Dimitrios Kollias,Oya Celiktutan,Xinyu Yang*

Main category: cs.CV

TL;DR: 提出CausalAffect框架，实现面部行为中的因果图谱发现，提升AU检测和表情识别的效果，并能解释与心理学理论一致的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注表情与动作单元（AUs）的识别，但很少从数据中自动推断AUs与表情之间的因果关系，缺乏心理学上合理的因果解释能力。

Method: 提出CausalAffect框架，采用两级因果层次结构建模AU与AU、AU与表情之间的因果依赖，引入特征级反事实干预机制，以抑制虚假相关性，提升因果效果，无需人工先验或联合注释数据。

Result: 在六个基准数据集上大量实验表明，CausalAffect在AU检测和表情识别两个任务中都优于现有方法，并揭示了与心理学理论一致的因果结构及新型依赖关系。

Conclusion: CausalAffect不仅提升了人脸行为识别的准确性，还实现了可解释的因果关系发现，为面部行为分析提供了新范式，相关代码和模型将在论文录用后开放。

Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.

</details>


### [59] [RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards](https://arxiv.org/abs/2512.00473)
*Junyan Ye,Leiqi Zhu,Yuncheng Guo,Dongzhi Jiang,Zilong Huang,Yifan Zhang,Zhiyuan Yan,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的文本到图像生成框架 RealGen，极大提升了生成图像的真实感和细节，显著优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流文本到图像生成模型虽然在一致性和知识覆盖方面表现优异，却在真实感上存在明显短板，经常生成带有AI痕迹的"假"图片，难以达到以假乱真的效果。作者希望解决图像逼真度不足的问题。

Method: RealGen 框架结合了大语言模型优化提示词及扩散模型生成逼真图像。其核心创新为引入"Detector Reward"机制，通过语义级和特征级伪造图像检测器定量衡量图像伪造痕迹和真实感，并借助GRPO算法优化整个生成流程。同时，提出 RealBench 基准，实现自动化、免人工的写实评测。

Result: 实验表明，RealGen 在真实感、细节和美学方面，明显优于GPT-Image-1、Qwen-Image等通用模型以及FLUX-Krea等专门化写实模型。

Conclusion: RealGen 利用创新检测奖励机制和强化学习显著提升了文本到图像生成的写实效果，并为自动化客观评测提供了新工具，为后续相关研究奠定坚实基础。

Abstract: With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.

</details>


### [60] [Structured Context Learning for Generic Event Boundary Detection](https://arxiv.org/abs/2512.00475)
*Xin Gu,Congcong Li,Xinyao Wang,Dexiang Hong,Libo Zhang,Tiejian Luo,Longyin Wen,Heng Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的通用事件边界检测方法——结构化上下文学习，通过结构化序列划分（SPoS）有效提升了时序信息学习和检测效果，并在多个基准数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的事件边界检测方法依赖特定的时序模型，灵活性和效率有限，且对长视频序列的处理计算复杂度较高。为此，作者希望提出一种通用、高效且准确的解决方案。

Method: 提出了结构化序列划分（SPoS）机制，将输入帧序列结构化划分，赋予后续时序模型有组织的上下文信息；计算组间相似性来捕捉帧之间的差异，并通过轻量级全卷积网络基于相似性图检测事件边界。为缓解边界标注的模糊性，引入高斯核预处理标签。方法可端到端训练，灵活支持不同时序模型。

Result: 在Kinetics-GEBD、TAPOS及镜头转场检测多个挑战性数据集上进行充分实验，本文方法在检测准确率和效率上均优于当前主流方法。

Conclusion: 结构化上下文学习方法能有效提升通用事件边界检测的表现，具备良好的精度速度权衡，并具有高度通用性和实用价值。

Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.

</details>


### [61] [Learning What Helps: Task-Aligned Context Selection for Vision Tasks](https://arxiv.org/abs/2512.00489)
*Jingyu Guo,Emir Konuk,Fredrik Strand,Christos Matsoukas,Kevin Smith*

Main category: cs.CV

TL;DR: 本文提出了TACS框架，通过学习选择最能提升任务性能的配对示例，使得视觉Transformer（ViT）模型不仅仅依赖外观相似，而能主动发现有助于任务表现的参考样本，在18个图像识别/分割数据集上超越了传统的相似度检索方法。


<details>
  <summary>Details</summary>
Motivation: 人类解决视觉不确定性时会参考相关的例子，但现有的视觉Transformer（ViT）模型无法识别哪些例子能真正提升任务表现，现有方法多依赖表面相似，而非实际任务益处。作者希望解决这一局限。

Method: 提出Task-Aligned Context Selection（TACS）框架，采用一个选择器网络与主任务模型联合训练，混合使用梯度监督和强化学习，使得检索目标align到任务表现上，而不是仅仅选择相似例子。

Result: 在细粒度识别、医学图像分类与分割等18个数据集上，TACS在总体和困难/数据稀缺场景下均显著优于基于相似度的检索。

Conclusion: TACS框架有效将例子检索与任务表现对齐，为判别模型自动发现有帮助的参考例子，具备更强泛化和实用价值。

Abstract: Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.

</details>


### [62] [CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration](https://arxiv.org/abs/2512.00493)
*Boshi Tang,Henry Zheng,Rui Huang,Gao Huang*

Main category: cs.CV

TL;DR: 本文提出了CC-FMO方法，实现了从单张图片自动生成高质量且结构一致的3D场景，大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的单图3D场景生成方法受限于小规模数据集和模型泛化能力弱。虽然大规模3D基础模型在单个物体生成上有进步，但在整体场景生成方面存在物体姿态推断和空间一致性不足的问题。

Method: CC-FMO提出了一种融合语义向量集和高细节潜在特征的混合实例生成器，用于生成语义合理且高质量的物体几何。同时通过相机条件缩放算法，将基础姿态估计模型引入场景生成，实现整体空间一致性。

Result: 实验显示，CC-FMO生成的3D场景在物体布局一致性和细节保真度方面显著优于现有主流方法，能够高效生成相机对齐、高质量的结构化3D场景。

Conclusion: CC-FMO有效提升了单图3D场景生成的质量和一致性，对AR/VR和具身智能领域有重要实际意义。

Abstract: High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.

</details>


### [63] [Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching](https://arxiv.org/abs/2512.00514)
*Tanaka Nobuaki*

Main category: cs.CV

TL;DR: 本论文提出了一种基于智能手机结构光的地面起伏重建系统，通过投影网格并配合新型2D动态时间规整（2D-DTW）算法，实现了对移动平台稳定性有帮助的地形感知。


<details>
  <summary>Details</summary>
Motivation: 低成本的移动机器人在凹凸不平的地形上行驶时，小的地面起伏难以通过视觉感知，但却严重影响其运动稳定性，因此需要简单高效的本地地形感知方法。

Method: 作者设计了一套基于智能手机的结构光投影系统，将网格状光栅投射到地表，然后通过从手机拍摄的图片中识别网格的畸变来重建地形。为此，提出了一种新的拓扑约束二维动态时间规整（2D-DTW）算法，能够在有视角畸变和遮挡的情况下，准确将畸变后的网格与原始网格进行匹配。该算法计算轻量，适用于资源有限的平台。

Result: 实验结果表明，所提出的2D-DTW算法在地面感知任务中表现良好，实现了高效且可靠的网格模式匹配，并可推广应用于其他图像处理情境中的结构化网格匹配任务。

Conclusion: 论文展示了基于结构光与2D-DTW算法的小型地形重建系统，有助于低成本移动机器人对不平整地形的可靠识别，提高其行驶稳定性，并扩展了网格匹配算法的应用领域。

Abstract: Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.

</details>


### [64] [Image Generation as a Visual Planner for Robotic Manipulation](https://arxiv.org/abs/2512.00532)
*Ye Pang*

Main category: cs.CV

TL;DR: 本文提出利用图像生成模型，通过少量微调，使其能够生成与机器人操作相关的连贯视频，实现视觉规划。实验显示该方法能在多数据集上生成平滑且符合条件要求的机器人操作视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型需要大量特定领域数据，且泛化能力有限；而预训练图像生成模型已展现一定组合及空间-时间相关能力，本文探究这种模型是否能成为机器人操作视觉规划器。

Method: 提出两种生成模式：（1）基于文本条件生成，即输入指令和首帧生成操作视频；（2）基于轨迹条件生成，即输入2D运动轨迹和首帧生成视频。利用LoRA微调少量数据，将图像生成模型适配到机器人操作任务。

Result: 在Jaco Play、Bridge V2和RT1等数据集上，二者均能生成平滑、连贯且满足条件要求的机器人操作视频，取得良好效果。

Conclusion: 预训练图像生成模型隐式学习到了可迁移的时间先验，在少量微调下即可作为视频式机器人操作视觉规划器，显著降低数据和监督需求。

Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.
  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.
  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.

</details>


### [65] [Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update](https://arxiv.org/abs/2512.00534)
*Zeyuan An,Yanghang Xiao,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Cross-Temporal 3D Gaussian Splatting（Cross-Temporal 3DGS）的新型3D场景重建与更新方法，实现了用稀疏视角照片和历史场景信息对不同时间点的3D场景进行高效重建和动态更新。


<details>
  <summary>Details</summary>
Motivation: 在城市规划、灾后评估和历史遗址保护等场景中，由于缺乏密集扫描，如何基于稀疏观测数据对3D场景实现长时间一致和高效更新成为计算机视觉领域亟需解决的问题。

Method: 方法包含三个阶段：（1）跨时相相机对齐，估算并对齐不同时间点的相机位姿；（2）基于干扰的置信度初始化，自动识别时间轴上保持不变的区域，引导后续的更新；（3）渐进式跨时空优化，将历史先验信息迭代融入当前场景优化，提升重建质量。该方法支持非连续采集和历史场景恢复。

Result: 实验表明，所提出的方法在重建质量和数据效率方面相比基线方法有显著提升，能够以更少的视角达到更好的3D场景复原效果。

Conclusion: Cross-Temporal 3DGS为多时间点场景版本管理、数字孪生和长期空间档案提供了有效解决方案，在稀疏采集条件下实现了高效、灵活的3D重建与更新。

Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.

</details>


### [66] [SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning](https://arxiv.org/abs/2512.00539)
*Yongkang Hu,Yu Cheng,Yushuo Zhang,Yuan Xie,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 论文提出了一种新框架SAIDO来检测AI生成的图像，强调对不同场景的自适应和防遗忘能力，并在广泛实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法在面对新的生成技术和表现内容时泛化能力差，难以适应实际应用，存在安全隐患，因此亟需一种具备更好泛化和持续学习能力的检测方案。

Method: 提出了SAIDO检测框架，主要包括：1）基于场景感知和VLLMs的专家模块（SAEM），可动态识别和融合新场景，针对每个场景分配独立模块，从而更好捕捉场景特定的伪造特征，提升跨场景泛化能力；2）引入了重要性引导的动态优化机制（IDOM），通过重要性引导的梯度投影策略来优化神经元，平衡模型的可塑性与稳定性，缓解持续学习中的灾难性遗忘问题。

Result: 在持续学习任务上，SAIDO在模型稳定性和可塑性上均超过当前SOTA方法，平均检测错误率和遗忘率分别降低44.22%和40.57%；在开放世界数据集上，平均检测精度提升9.47%。

Conclusion: SAIDO框架有效提升了对AI生成图像的检测性能，兼具良好的泛化和防遗忘能力，为应对复杂和变化多端的生成图像检测挑战提供了新的思路和工具。

Abstract: The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.

</details>


### [67] [Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions](https://arxiv.org/abs/2512.00547)
*Sandika Biswas,Qianyi Wu,Biplab Banerjee,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本论文提出了一种结合3D生成模型、语义感知变形和3D高斯投影优化的混合方法，有效提升了多人体多物体动态场景的3D重建质量，尤其能处理严重遮挡情况下的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 多人体多物体的动态场景由于动作多样、遮挡频繁等挑战，3D几何建模一直很困难，现有大部分高斯投影（GS）方法很少能处理此类复杂场景。

Method: 作者提出结合：（1）3D生成模型对场景中各元素生成高保真网格；（2）语义感知变形（刚体的刚体变换+人体的LBS变形），确保高质量网格能对应到动态场景中恰当的位置和变形；（3）对各元素进行GS优化进一步提升对齐精度。这一混合方法强化了结构一致性（特别在遮挡严重时）。

Result: 在多人体多物体交互的HOI-M3数据集上评测，该方法在表面几何重建精度方面明显优于当前主流方法。

Conclusion: 该方法能高效、稳定地对动态、多元素场景实现高保真的3D表面建模，尤其在面对复杂交互和遮挡时具有显著优势。

Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.

</details>


### [68] [NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives](https://arxiv.org/abs/2512.00557)
*Haomiao Chen,Keith W Jamison,Mert R. Sabuncu,Amy Kuceyeski*

Main category: cs.CV

TL;DR: 本文提出了一种新的脑引导生成框架NeuroVolve，用于合成满足单区或多区脑部激活模式的图像，并能解释不同大脑区域之间的互动关系。


<details>
  <summary>Details</summary>
Motivation: 以往工作大多关注单一区域（如FFA区）对已知类别的选择性，缺乏对区域间如何互动以产生复杂神经表征的深入理解。当前方法对多区域联合加工复杂视觉信息的解释力有限。

Method: 提出NeuroVolve框架，结合预训练视觉-语言模型，通过一个可编程的神经目标函数对嵌入空间进行优化，实现‘以脑为引导’的图像合成。该方法可针对单一区域、多个区域的激活/抑制，甚至定制个体化神经目标，追踪优化过程揭示语义变化轨迹。

Result: NeuroVolve不仅能还原已知大脑区域的选择性，还可按多区激活约束生成结构化场景，揭示脑区间的协作或对抗关系。该方法对个体的偏好也有较强捕捉能力。

Conclusion: NeuroVolve为理解和模拟多脑区复杂互动提供了新工具，有助于解析大脑表征视觉信息的机制，也支持基于神经目标的个性化视觉合成与可解释映射。

Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.

</details>


### [69] [Describe Anything Anywhere At Any Moment](https://arxiv.org/abs/2512.00565)
*Nicolas Gorlo,Lukas Schmid,Luca Carlone*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的时空记忆框架DAAAM，实现了对大规模4D场景的实时、高精度语义理解和动态场景图构建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时性与丰富语义表达间存在权衡，难以同时实现高度语义细节和三维空间定位，限制了增强现实、机器人自主导航等应用。

Method: 提出Describe Anything, Anywhere, at Any Moment(DAAAM)框架，结合优化前端与本地化描述模型(DAM)，通过批量处理加速推理，并用层次化4D场景图实现场景的空间和时间一致表示，支持推理与工具调用。

Result: 在NaVQA和SG3D等基准上全面评测，DAAAM在大规模、长时序评测中显著优于最强基线，包括提高OC-NaVQA准确率53.6%、位置误差降低21.9%、时间误差降低21.6%、SG3D任务准确率提升27.8%。

Conclusion: DAAAM实现了实时高效的四维语义场景理解和场景图构建，推动了语言与视觉的深度结合，对增强现实和机器人领域具重要意义，数据和代码已开源。

Abstract: Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.
  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.

</details>


### [70] [Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models](https://arxiv.org/abs/2512.00572)
*Mohammed Mohiuddin,Syed Mohammod Minhaz Hossain,Sumaiya Khanam,Prionkar Barua,Aparup Barua,MD Tamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一个新的瑜伽姿势数据集“Yoga-16”，并系统性评估了三种深度学习模型与三种输入方式在瑜伽动作识别任务中的表现。结果显示，基于骨架的输入优于原始图像。


<details>
  <summary>Details</summary>
Motivation: 目前瑜伽姿势自动识别研究多依赖单一模型或原始图像，缺乏系统对比和高质量数据集。同时，为减少人类专家依赖并降低错误姿势导致的伤害，亟需改进和系统化的研究。

Method: 作者构建了Yoga-16数据集，并对VGG16、ResNet50和Xception三种深度学习架构，分别采用原始图像、MediaPipe Pose骨架图像和YOLOv8 Pose骨架图像三种输入方式进行对比实验。模型性能通过分类准确率和Grad-CAM可解释性分析进行评估。

Result: 实验表明，骨架图像输入优于原始图像，VGG16结合MediaPipe Pose骨架效果最佳，分类准确率达96.09%。Grad-CAM分析提供了模型判别的可解释性。

Conclusion: 系统性基准测试显示，基于骨架的深度学习输入能显著提升瑜伽动作识别准确率，Yoga-16数据集和可解释性分析为相关研究提供了坚实基础。

Abstract: Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.

</details>


### [71] [SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension](https://arxiv.org/abs/2512.00582)
*Yue Jiang,Haiwei Xue,Minghao Han,Mingcheng Li,Xiaolu Hou,Dingkang Yang,Lihua Zhang,Xu Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的框架SatireDecoder，显著提升了视觉讽刺理解的准确性，对视觉-语言模型的高级推理任务有重要贡献。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在理解纯视觉讽刺时，难以兼顾局部实体关系与全局上下文，常产生误判、理解偏差与幻觉，显著影响实际应用效果。

Method: 作者提出SatireDecoder框架，采用多智能体系统对图像进行视觉级联解耦，提取细粒度的局部与全局语义表示，并引入基于不确定性分析的chain-of-thought推理，将讽刺理解过程分解为一系列降低不确定性的子任务。该方法无需额外训练。

Result: 实验结果表明，SatireDecoder在讽刺图像理解任务上超越了现有基线，能有效提升解释准确率并减少幻觉现象。

Conclusion: SatireDecoder为视觉-语言领域高层次语义推理问题提供了新的解决思路，有助于推动复杂视觉任务的研究与实际应用。

Abstract: Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.

</details>


### [72] [Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models](https://arxiv.org/abs/2512.00597)
*Thuraya Alzubaidi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了一种名为MedCT-VLM的医疗CT视觉-语言模型，使用高效参数适应方法提升CT影像多标签疾病分类能力，尤其在零样本（zero-shot）场景数据稀缺时表现突出。


<details>
  <summary>Details</summary>
Motivation: 当下的视觉-语言基础模型虽在各类图像任务中表现优异，但在体积型医学影像（如CT）应用受限。亟需一种有效方法将这些强大模型迁移应用到具体医疗任务，且要兼顾参数效率，适应医学领域标注样本少的实际情况。

Method: 基于CT-CLIP（用25692个胸部CT训练的对比视觉-语言模型），通过引入Low-Rank Adaptation (LoRA)技术，将低秩分解矩阵插入视觉和文本编码器的注意力层，仅需训练1.67M参数（约占总参数的0.38%），实现模型参数高效适应，而无需全部微调。

Result: 在18种胸部病理的零样本多标签分类评测中，经LoRA训练后的模型其平均AUROC由61.3%提升至68.9%，准确率由67.2%升至73.6%，macro-F1由32.1%升到36.9%，均显著优于原始模型。

Conclusion: 参数高效的LoRA方法可以有效实现大型医学影像视觉-语言基础模型向下游临床任务的迁移，尤其适用于标签稀缺的零样本场景，具有实际应用推广价值。

Abstract: Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\% to 68.9\% (+7.6 pp), accuracy from 67.2\% to 73.6\% (+6.4 pp), and macro-F1 from 32.1\% to 36.9\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.

</details>


### [73] [Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning](https://arxiv.org/abs/2512.00625)
*Tzu-I Liao,Mahmoud Fakhry,Jibin Yesudas Varghese*

Main category: cs.CV

TL;DR: 本文比较了多种深度学习模型（如YOLOv9、U-Net、Swin Transformer、DeepLabV3、Mask R-CNN）在检测树木截面髓心任务上的表现，Swin Transformer精度最高，论文强调了模型选择需结合数据集特性和应用需求。


<details>
  <summary>Details</summary>
Motivation: 树木截面髓心检测对林业和木材质量分析至关重要，但现有方法多为人工处理且容易出错，因此亟需开发高效自动化的检测方法。

Method: 作者选用582张标注图片构建数据集并进行数据增强，分别用YOLOv9、U-Net、Swin Transformer、DeepLabV3、Mask R-CNN五种深度学习模型进行训练和评估，并将表现最差的模型再用新增64张标注图片微调测试泛化能力，同时尝试在奥勒冈州立大学的橡树数据集上验证模型可迁移性，过程中针对技术难点调整超参数和采用增强策略。

Result: Swin Transformer模型取得最高准确率0.94，表现优异；YOLOv9在框选中表现出色但边界处理一般；U-Net擅长结构化模式；DeepLabV3对多尺度特征把握良好但边缘略有误差；Mask R-CNN原本表现差，经NMS后IoU从0.45提升到0.80。

Conclusion: 深度学习方法在树木截面髓心自动检测中具有强大潜力，最佳模型依赖于具体数据集及实际需求，且通过合理的数据增强和参数调整可进一步提升效果。

Abstract: Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.

</details>


### [74] [XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance](https://arxiv.org/abs/2512.00626)
*Kim Gerard A. Villanueva,Priyanka Kumar*

Main category: cs.CV

TL;DR: 本研究提出了一种结合数据增强和可解释AI的皮肤病多分类诊断系统，在公开数据集上获得高准确率，并提升了模型的临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病多分类诊断受限于方法主观性、数据集类别不平衡以及深度学习模型黑箱特性，影响准确性与可靠性。亟需开发既高效又可解释的辅助诊断系统。

Method: 采用每类DCGAN生成式对抗网络进行数据增强以解决类别不平衡，并用微调后的ResNet-50分类器对增强后的数据集进行训练。系统集成了LIME和SHAP等可解释AI技术，提供诊断决策过程的透明性。

Result: 系统分类七种皮肤病的整体准确率为92.50%，Macro-AUC为98.82%，超过以往的多种主流模型。

Conclusion: 该工作验证了一个高性能且具备临床可解释性的诊断框架，并指出未来应加强对恶性黑色素瘤等关键类别的识别能力。

Abstract: Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).

</details>


### [75] [Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation](https://arxiv.org/abs/2512.00639)
*Mahmoud El Hussieni*

Main category: cs.CV

TL;DR: 本文利用YOLOv5系列算法对超声图像中的甲状腺结节进行实例分割，发现加入多普勒图像能显著提高分割性能，尤其是YOLOv5-Large模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 甲状腺癌发病率上升，推动了基于AI的计算机辅助检测方法的发展。准确分割甲状腺结节是实现AI辅助临床决策系统的重要前提。

Method: 本研究将YOLOv5不同变种（Nano, Small, Medium, Large, XLarge）应用于带有和不带多普勒图像的超声数据集，对甲状腺结节进行实例分割，并对比不同模型与数据输入版本的表现。

Result: 在含多普勒图像的数据集上，YOLOv5-Large模型取得了91%的dice分数和0.87的mAP。所有模型在加入多普勒图像后性能提升；如YOLOv5-Small未包含多普勒时dice分数为79%。

Conclusion: YOLOv5实例分割为甲状腺结节检测提供了一种高效实时的方法，结合多普勒图像能显著提升性能，具有自动化诊断系统的临床应用潜力。

Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.

</details>


### [76] [Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition](https://arxiv.org/abs/2512.00641)
*Razieh Ghaedi,AmirReza BabaAhmadi,Reyer Zwiggelaar,Xinqi Fan,Nashid Alam*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的跨域人脸表情识别方法GAT-ADA，通过结合Graph Attention Network和对抗域对齐手段，大幅提升了不同数据集间的适应与识别性能。


<details>
  <summary>Details</summary>
Motivation: 跨域人脸表情识别经常受到源域与目标域分布差异（域偏移）困扰，导致实际应用效果下降。论文希望通过增强特征关系建模与多重域对齐，有效缓解这一难题。

Method: 提出GAT-ADA：以ResNet-50为主干，加入批级Graph Attention Network，使每个小批量样本形成稀疏环状图，提取跨样本注意力信息以帮助表征域间关联。同时，融合Gradient Reversal Layer的对抗学习和CORAL/MMD的统计分布对齐，提升模型域适应能力。

Result: 在不带标注的多目标域适应实验中（RAF-DB到CK+、JAFFE、SFEW 2.0、FER2013、ExpW），GAT-ADA平均跨域准确率达74.39%，在RAF-DB到FER2013的情况下准确率高达98.0%，比同配置下最好基线提升约36个百分点。

Conclusion: GAT-ADA能够显著缓解人脸表情识别中的跨域泛化问题，实现了对传统方法的突破，具备良好的实际应用前景。

Abstract: Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.

</details>


### [77] [MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba](https://arxiv.org/abs/2512.00647)
*Shanhui Liu,Rui Xu,Yunke Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CF-ViM的自适应视觉推理框架，通过粗到细的处理方式动态分配计算资源，实现更高效率且不损失关键信息。


<details>
  <summary>Details</summary>
Motivation: 现有Vision Mamba模型虽然相比Vision Transformers具有更高效的表现，但其效率依然受限于输入token的数量。主流的token缩减方法（如剪枝、合并）会导致信息损失，特别是在对所有图片均采取统一、细粒度处理时，这一问题更加突出。因此，作者希望设计出一种能够根据图像复杂度自适应分配计算的高效方案。

Method: 提出CF-ViM方法：首先以较粗粒度（大patch）对输入图像进行推理，大幅减少token数量和计算量。如果模型预测信心较低，则只对部分区域以更高分辨率（细粒度）进行重新处理，以较小的额外计算成本恢复重要细节。该策略可根据图像复杂度自适应动态调整分辨率与计算分配。

Result: 在ImageNet数据集上的实验显示，CF-ViM在准确率和效率方面均优于基础的Vision Mamba模型和现有SOTA token缩减方法。

Conclusion: CF-ViM通过粗到细、动态调整分辨率推理的策略，实现了高效且信息保真的视觉处理，为视觉Transformer家族带来新的高效推理方案。

Abstract: Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.

</details>


### [78] [From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings](https://arxiv.org/abs/2511.21428)
*Jiajie Zhang,Sören Schwertfeger,Alexander Kleiner*

Main category: cs.CV

TL;DR: 本论文提出了一套全自动端到端系统，可从工业视频流中无监督地提取并组织适用于视觉-语言-动作（VLA）模型预训练的数据，实现了大规模VLA训练数据的高效构建。


<details>
  <summary>Details</summary>
Motivation: 工业视频数据大量未标注，人类操作流程结构复杂，传统需要大量人工标注的方法无法实现大规模、经济高效的数据获取，制约了VLA模型的发展。

Method: 提出方法包括：1）自监督训练轻量级运动分词器以编码运动动态，2）基于新颖的“潜在动作能量”指标无监督发现和分割语义一致的动作原语。最终输出结构化的视频片段和对应的动作序列，直接用于VLA模型预训练。

Result: 在公共基准与专有电机装配数据集上评估，表明方法能够有效分割工作站人类关键任务，动作原语通过Vision-Language模型进一步聚类与定量分析，展示了较强的语义一致性。

Conclusion: 这是首个可对工业视频流自动化萃取VLA预训练数据的端到端方案，为制造业智能体AI的规模化集成和发展提供了可行的基础设施。

Abstract: We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.

</details>


### [79] [Generalized Medical Phrase Grounding](https://arxiv.org/abs/2512.01085)
*Wenjun Zhang,Shekhar S. Chandra,Aaron Nicolson*

Main category: cs.CV

TL;DR: 提出了通用医学短语定位（GMPG）任务和首个模型MedGrounder，能够处理零、一个或多个区域定位，更贴合实际医疗报告需求，并在相关数据集上取得优秀表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学短语定位方法只支持每个短语对应一个区域，但实际医学报告常涉及多个区域、不适于定位的短语（如否定、正常结构等）或无需定位。为提升系统实用性和解释性，需打破这一限制。

Method: 将任务重新定义为GMPG，即句子可对应零、一个或多个带分数的区域。提出MedGrounder模型，采用二阶段训练：先用“句子-解剖区域”自动数据预训练，再用“句子-人工标注区域”微调。比较了medical REC和自动生成定位报告等基线方法。

Result: MedGrounder在PadChest-GR和MS-CXR数据集上实现了很强的零样本迁移效果，对多区域及不可定位短语的表现优于现有方法，同时极大减少了人工标注的需求。

Conclusion: MedGrounder更贴合实际医学场景，提升了定位准确性、灵活性和数据效率，可与已有报告生成器无缝组合，助力生成可解释的医学影像报告。

Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.

</details>


### [80] [Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges](https://arxiv.org/abs/2512.00676)
*Kiri L. Wagstaff*

Main category: cs.CV

TL;DR: 本文提出更贴合实际场景的手写多数字数据集（MDW），发现单独数字分类方法在多数字识别上表现有限，呼吁更有针对性的方法创新。


<details>
  <summary>Details</summary>
Motivation: 尽管手写数字识别已经取得长足进步，但现实生活中数字通常不是孤立出现，而是作为多位数出现，如邮政编码、支票金额等。因此，建立更符合实际的新基准数据集和评估方法变得十分重要。

Method: 作者利用NIST数字图片的书写者信息，构建了多数字同一个人书写、更加接近真实应用场景的数据集（MDW），并设计了更契合实际应用的性能评估指标。

Result: 结果显示，传统孤立数字分类模型在MDW多数字任务中的表现远不及在单独数字任务，说明当前方法尚不能满足真实需求。

Conclusion: 仅靠现有的单数字识别技术不能很好解决实际的多数字识别问题，需要结合任务背景开发更高效的方法，MDW数据集为相关研究提供了新的挑战和机遇。

Abstract: Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.

</details>


### [81] [Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation](https://arxiv.org/abs/2512.01242)
*Zirui Zhao,Boye Niu,David Hsu,Wee Sun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种结合几何推理与神经语义的受约束视觉组合生成方法，能在严苛规则（如Tangram组装任务）下显著优于现有生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前在视觉组合生成领域，受限于部件位置排列的组合复杂性与稀疏可行解，仅凭像素级统计模型难以取得高效、有效的结构生成。尤其是在需求兼具几何约束（不重叠、允许角度等）和语义目标（如文字描述）时，现有方法局限突出。

Method: 作者提出了一种融合蒙特卡洛树搜索（MCTS）与vision-language模型的框架：利用MCTS通过策略网络在组合方案中搜索可行解，同时视语言模型对生成结果进行语义奖励打分。搜索路径反向用于网络微调。另借鉴GAN思想，通过生成样本对奖励模型进行“对抗强化”。

Result: 在Tangram组装任务测试中，所提方法在生成的有效性与语义相关性方面，均显著优于diffusion模型和自回归模型，优势在约束更严格时更明显。

Conclusion: 结合显式几何推理与神经网络语义判别，该生成框架能有效攻克稀疏、约束下的结构组合难题，有望推广至其它结构生成领域。

Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.

</details>


### [82] [Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer](https://arxiv.org/abs/2512.00677)
*Dong In Lee,Hyungjun Doh,Seunggeun Chi,Runlin Duan,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 本文提出Dynamic-eDiTor，一种无需训练的文本驱动4D场景编辑框架，通过整合多模态扩散Transformer和4D高斯点，实现空间与时间一致且流畅的多视角视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有4D场景编辑方法难以在编辑过程中同时保持多视角与时序一致性，且多依赖于独立逐帧的2D扩散模型，导致运动失真、几何漂移和编辑不完整。本文旨在解决这些4D场景编辑中的一致性与完整性难题。

Method: 采用Multimodal Diffusion Transformer (MM-DiT)与4D Gaussian Splatting (4DGS)作为核心，并提出空间-时间子网格注意力（STGA）机制，用于局部、多视角与时序信息融合；同时提出上下文Token传播（CTP），结合token继承与光流引导的token替换，实现全局一致性。无需重新训练，直接对已有4DGS进行优化。

Result: 在多视角视频数据集DyNeRF上的实验表明，该方法在文本驱动编辑的保真度、多视角一致性和时序一致性方面均优于现有方法。

Conclusion: Dynamic-eDiTor有效突破了文本驱动4D场景编辑过程中的一致性难题，无需额外训练即可提升编辑效果，为4D内容可控性带来了新的解决方案。

Abstract: Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/

</details>


### [83] [StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707)
*Daeun Lee,Subhojyoti Mukherjee,Branislav Kveton,Ryan A. Rossi,Viet Dac Lai,Seunghyun Yoon,Trung Bui,Franck Dernoncourt,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出了StreamGaze，这是首个评估多模态大模型（MLLMs）在流式视频中利用人类视线进行时序和前瞻性推理能力的基准，填补了相关领域空白。结果显示现有MLLMs在这一任务上与人类有显著差距。


<details>
  <summary>Details</summary>
Motivation: 目前的流式视频理解基准主要专注于时序推理，缺乏对于模型能否理解或利用人类视线信号的评测，而现实应用如AR眼镜对此能力有实际需求。

Method: 作者提出StreamGaze基准，设计了基于视线引导的过去、现在和前瞻性任务，评估模型能否利用实时视线，追踪注意力变化并仅依赖已观察帧推理用户意图。为此，作者开发了一条视线-视频QA生成流程，包括注视点提取、区域视觉提示和视线路径构建，生成贴合人类感知的QA对。

Result: 在所有StreamGaze任务上，最先进的多模态大模型与人类表现存在很大差距，暴露了模型在视线驱动的时序推理、意图建模、前瞻预测等方面的根本局限。

Conclusion: 当前MLLMs在利用视线信号理解流式视频方面存在明显不足。论文深入分析了失败原因、视线提示策略等，指出未来模型需要提升相关推理与建模能力。论文数据和代码都将公开，推动社区继续研究。

Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.

</details>


### [84] [Silhouette-based Gait Foundation Model](https://arxiv.org/abs/2512.00691)
*Dingqiang Ye,Chao Fan,Kartik Narayan,Bingzhe Wu,Chengwen Luo,Jianqiang Li,Vishal M. Patel*

Main category: cs.CV

TL;DR: 该论文提出了FoundationGait，这是首个可扩展的、自监督的步态预训练框架，能够在多个步态相关任务和不同输入条件下实现强健泛化能力，刷新了现有步态识别的性能基准。


<details>
  <summary>Details</summary>
Motivation: 步态识别在身份识别及健康分析领域具有重要应用，但现有模型规模有限、泛化能力差，难以整体提升步态建模的性能，需要解决扩展性和多任务泛化两个核心难题。

Method: 作者提出了FoundationGait框架，采用自监督预训练方式，在12个公开步态数据集、超过200万条步态序列上进行训练，并构建了近1.3亿参数的大规模模型，在多个任务和输入模态下通过微调或无微调进行评估。

Result: FoundationGait无论在有无微调的情况下，在各类步态数据集、应用任务（如身份识别、脊柱侧弯筛查、抑郁预测等）及不同输入模态下均表现出强大的泛化能力。在Gait3D和OU-MVLP两个具有挑战性的数据集上分别获得了48.0%和64.5%的zero-shot rank-1准确率，刷新了步态识别的新纪录。

Conclusion: FoundationGait首次实现了大规模、跨任务、跨数据集的步态表示学习，展现出强劲的可扩展性和泛化能力，为后续步态感知及相关健康分析提供了基础模型和研究范式。

Abstract: Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

</details>


### [85] [Affordance-First Decomposition for Continual Learning in Video-Language Understanding](https://arxiv.org/abs/2512.00694)
*Mengzhu Xu,Hanzhi Liu,Ningkang Peng,Qianyu Chen,Canran Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种持续学习的视频-语言理解方法Affordance-First Decomposition (AFD)，能够在数据分布变化和内存受限的条件下明确区分模型的稳定部分和可塑性部分，并取得了多项基准任务下的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频-语言多模态持续学习方法在应对非静态数据、不同领域和查询风格时，难以平衡模型的稳定性（固有知识保持）和可塑性（新知识获取），且大多依赖数据回放，受限于隐私与内存约束。

Method: AFD方法将视频映射为缓慢变化的“可供性”令牌（affordance tokens），形成可对齐、共享的底层表示；同时设计了轻量的、按查询路由、冲突感知的调度器，实现对底层表示的有针对性适应，仅在需要时扩展容量。底层表示通过弱对齐和教师一致性进行稳定，训练仅需“问题回放”，无需原视频数据。

Result: AFD方法在多个持续学习基准上取得SOTA（最佳）成绩。例如，在端域增量VideoQA任务中平均准确率51.6%，遗忘降至-1.8%；在ViLCo和iVQA等其他任务上也展现出较低的遗忘与较高的准确率。

Conclusion: AFD方法在稳定性和适应性间实现了清晰、可解释的分离，兼顾了持续学习任务中的效率、准确率与隐私保护，为多模态处理提供了新思路。

Abstract: Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.

</details>


### [86] [TrajDiff: End-to-end Autonomous Driving without Perception Annotation](https://arxiv.org/abs/2512.00723)
*Xingtai Gui,Jianbo Zhao,Wencheng Han,Jikai Wang,Jiahao Gong,Feiyang Tan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出了一种全新的不需要感知标注的自动驾驶方法TrajDiff，实现了端到端地从传感器原始输入直接生成可行的轨迹，并在NAVSIM基准集上取得最新最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶方法依赖感知任务，但感知数据标注成本高，因此亟需无需感知标注的规划方法。

Method: 提出TrajDiff框架，利用传感器数据和未来轨迹，生成BEV高斯热力图作为目标，通过轨迹导向的BEV编码器与扩散变换器（TB-DiT），无需感知监督地生成轨迹，实现了无手工先验与标注的轨迹规划。

Result: TrajDiff在NAVSIM基准集上获得87.5 PDMS，数据增大后提升到88.5 PDMS，超越现有全部无标注方法，并接近带感知标注方法的表现。

Conclusion: TrajDiff有效突破了对感知标注的依赖，实现了高性能、可扩展的端到端自动驾驶规划，对自动驾驶系统的开发具有重要意义。

Abstract: End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.

</details>


### [87] [CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty](https://arxiv.org/abs/2512.00700)
*Ka Chung Lai,Ahmet Cetinkaya*

Main category: cs.CV

TL;DR: 本文提出了一种名为CAR-net的新型神经网络架构，专门用于去除图像中的旋转运动模糊，能够在模糊角度信息不完全准确的情况下高效去模糊。


<details>
  <summary>Details</summary>
Motivation: 旋转运动模糊场景中，传统去模糊方法在模糊参数未知或不准确时难以获得优异表现。因此，亟需一种在旋转模糊参数不完美已知情况下仍能有效还原图像细节的方法。

Method: 提出CAR-net架构，其核心为级联渐进细化流程。从频域逆变换获得初始去模糊估计后，多级细化模块预测并逐步校正残差，抑制伪影、恢复细节。此外，网络可选配端到端可训练的模糊角度检测模块以应对参数不确定性。

Result: 在合成和真实图像实验中，CAR-net展示出较高的去模糊效率和重建细节能力，实验效果优于传统方法。代码与模型已开源。

Conclusion: CAR-net针对旋转运动模糊、模糊角度不准等半盲场景表现优异，系统设计高效且灵活，为实际图像去模糊任务提供了新思路。

Abstract: We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net

</details>


### [88] [SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead](https://arxiv.org/abs/2512.00903)
*Chaojun Ni,Cheng Chen,Xiaofeng Wang,Zheng Zhu,Wenzhao Zheng,Boyuan Wang,Tianrun Chen,Guosheng Zhao,Haoyun Li,Zhehao Dong,Qiang Zhang,Yun Ye,Yang Wang,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: SwiftVLA是一种高效的视觉-语言-动作（VLA）模型，能够用更小的体积实现与大模型相媲美的性能，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型虽然性能强大，但参数量大导致实用性不足。采用轻量VLM虽能减轻负担，但会严重损失时空推理能力，且现有的一些3D融合方法依赖大模型且缺乏时间特征理解。

Method: SwiftVLA采用预训练的4D视觉几何变换器（视觉-空间-时间）配合时序缓存，从2D图像中提取4D特征。通过引入融合Token（Fusion Tokens）联合学习2D及4D语义表示，同时预测未来动作。此外，提出先掩码后重建机制，使模型在推理阶段可以仅用主干网络，减少推理资源开销。

Result: SwiftVLA在真实与模拟环境实验中表现优异，超越各类轻量基线，在仅为对手1/7参数量下达到甚至媲美成绩，推理速度提升18倍、内存占用减少12倍。

Conclusion: SwiftVLA显著提升了小模型的4D理解与动作生成能力，实现高效、低资源消耗且易于边缘部署的VLA系统。

Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.

</details>


### [89] [Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation](https://arxiv.org/abs/2512.00706)
*Chengzhi Yu,Yifan Xu,Yifan Chen,Wenyi Zhang*

Main category: cs.CV

TL;DR: 本文针对大规模视觉语言模型（LVLMs）的幻觉问题，提出了更高效、可靠的偏好注释和训练方法，显著降低了模型幻觉率。


<details>
  <summary>Details</summary>
Motivation: 多模态任务中的大视觉语言模型经常出现“幻觉”现象（即生成与事实或图像不符的信息），影响其广泛应用。作者发现已有数据注释和偏好标注方法存在引入额外幻觉、难以从根本上缓解幻觉的问题，因此需要开发更干净、有效的幻觉缓解方案。

Method: 作者分析了幻觉缓解数据的生成过程，证明了on-policy（依赖模型当前策略生成）数据比off-policy数据效果更佳。为此，作者设计了训练幻觉分类器，生成二元注释用于挑选干净样本，并提出了动态样本重加权的健壮迭代直接偏好优化（DPO）算法，对on-policy数据进行高效利用。

Result: 在三个标准基准上与8个先进方法比较，该方法将LLaVA-1.5-7B在MMHalBench上的幻觉率降低50.8%，在Object HalBench上的平均幻觉率减少79.5%；开源模型LLaVA-1.5-13B甚至超过了GPT-4V。

Conclusion: 作者提出的注释和优化方法能有效利用on-policy数据，极大降低了LVLMs的幻觉现象，并进一步提升了开源模型的性能和竞争力。

Abstract: Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.

</details>


### [90] [MM-ACT: Learn from Multimodal Parallel Generation to Act](https://arxiv.org/abs/2512.00975)
*Haotian Liang,Xinyi Chen,Bin Wang,Mingkang Chen,Yitian Liu,Yuhao Zhang,Zanxin Chen,Tianshuo Yang,Yilun Chen,Jiangmiao Pang,Dong Liu,Xiaokang Yang,Yao Mu,Wenqi Shao,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了一种通用的视觉-语言-动作（VLA）模型MM-ACT，将文本、图像和动作三种模态通过统一的token空间进行整合，并在所有模态上实现生成，显著提升了机器人策略泛化与执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人政策往往只专注于单一模态，缺乏综合的语义理解与环境互动预测能力，难以实现通用性和高效任务规划。作者希望通过统一多模态信息提升机器人在复杂环境下的通用操作能力。

Method: 提出MM-ACT模型，将文本、图像、动作编码到共享的token空间。采用re-mask并行解码提升文本和图像生成效率，对动作则采用一步并行解码。提出Context-Shared Multimodal Learning，实现三模态在共享语境下共同监督训练，以实现跨模态增强学习。

Result: 在LIBERO仿真环境上成功率96.3%，实际Franka机器人上三任务平均成功率72.0%，RoboTwin2.0八项双臂任务平均成功率52.38%。跨模态学习策略带来约9.25%的绩效提升。

Conclusion: MM-ACT实现了统一、泛化、跨模态的机器人策略，显著提升了实际操作中的认知与执行能力。跨模态共训有效改善了动作生成表现，对提升通用机器人智能具有重要意义。

Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.

</details>


### [91] [Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks](https://arxiv.org/abs/2512.00714)
*Emmanuella Avwerosuoghene Oghenekaro*

Main category: cs.CV

TL;DR: 本文综述了深度学习在多模态医学影像分析中对早期癌症检测的突破性进展，特别是计算机视觉模型和影像组学技术在实现更加精准和无创的癌症诊断中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有癌症筛查方法对早期诊断仍有较大局限，延迟发现会显著降低患者生存率。因此，开发更高效、可靠、非侵入性的早期检测技术成为当务之急。

Method: 利用深度学习的计算机视觉模型（如CNN、Transformer及其混合注意力结构），自动提取包括MRI、CT、PET等多模态医学影像的复杂特征。同时，结合影像组学与基因组、转录组、表观遗传组等生物标志物，实现多源信息融合。

Result: 这些模型不仅能够检测传统影像中难以分辨的微妙异常，还能识别肿瘤基因型、免疫反应、分子亚型和耐药性，提高了诊断的准确性和个体化程度。

Conclusion: 深度学习和多模态融合推动了无创、个性化癌症早筛新范式，有望显著提升筛查效果和患者生存率。

Abstract: Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.

</details>


### [92] [Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation](https://arxiv.org/abs/2512.01165)
*Mohamed Abdallah Salem,Ahmed Harb Rabia*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO模型的实时数据集标注方法，可在图像采集时即时标注，大幅提升数据准备效率，显著减少人工成本。


<details>
  <summary>Details</summary>
Motivation: 在农业等实际应用中，传统的目标检测数据标注方式费时费力，影响模型快速部署和生产决策。作者希望通过改进标注流程，提高效率和准确性。

Method: 通过将YOLO系列模型（YOLOv5、YOLOv8、YOLOv12）部署于边缘设备，实现采集图像时自动标注。研究分别比较了单类别与多类别、预训练与从零训练等多种配置，包含详细统计分析和学习动态。

Result: 结果显示预训练和单类别配置在模型收敛速度、检测性能和鲁棒性方面均具有明显优势。所提实时标注框架表现出高效性和高标注质量，大幅减少了数据集准备时间。

Conclusion: 本文提出的方法能够在保证标注质量的同时，大幅提升数据标注效率，推动目标检测模型在实际环境中的快速应用，尤其适用于对时效性要求高的农业领域。

Abstract: Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.

</details>


### [93] [RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images](https://arxiv.org/abs/2512.00718)
*Deliang Wang,Peng Liu*

Main category: cs.CV

TL;DR: 提出了一种针对遥感图像的高效交互式分割方法RS-ISRefiner，相较现有方法显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有交互式图像分割方法主要针对自然图像，缺乏对遥感图像的适应性，且因标注数据稀缺和计算开销大，难以应用于遥感场景。

Method: 采用基于适配器（adapter-based）的微调策略，以保持视觉基础模型的通用表征，并高效学习遥感影像独有的空间及边界特征；引入卷积-Transformer混合注意力机制提升对尺度和场景复杂性的适应；改进概率图调制方案，增强用户交互信息融合，提升分割边界质量和迭代稳定性。

Result: 在六个典型遥感数据集上，RS-ISRefiner在分割精度、效率和用户交互成本等方面均优于主流交互式分割方法。

Conclusion: RS-ISRefiner具备良好的泛化性与高精度，适用于实际遥感实例分割任务，可显著提升高质量标注效率。

Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.

</details>


### [94] [SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge](https://arxiv.org/abs/2512.01629)
*Yumeng He,Ying Jiang,Jiayin Lu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: SPARK框架可以从单张RGB图像自动重建具有良好物理一致性和关节结构的3D可动物体，缓解了创建仿真资产的高门槛和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D可动物体对AI与机器人等领域很重要，但传统建模需要大量人工与专业知识，因此急需更自动且高效的解决方案。

Method: SPARK框架分为两个阶段：首先利用视觉语言模型（VLM）提取URDF参数并生成部件参考图像，然后结合推理得到的结构图和参考图片，通过扩散生成变换器合成一致的部件与整体形状。最后利用可微前向运动学和可微渲染，在VLM监督下优化关节类型、轴和原点等关键参数。

Result: 大量实验结果表明，SPARK可在多种类别下生成高质量、可直接用于仿真的可动3D资产，并能支持下游如机器人操作、交互建模等应用。

Conclusion: SPARK显著降低了3D可动物体建模门槛，为仿真、机器人等领域的交互式场景理解提供了强有力的工具。

Abstract: Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.

</details>


### [95] [Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching](https://arxiv.org/abs/2512.01850)
*Yue Pan,Tao Sun,Liyuan Zhu,Lucas Nunes,Iro Armeni,Jens Behley,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 文章提出了一种新的点云配准方法，将其视为条件生成任务，通过学习连续的点速度场将噪声点云变换为配准后的场景，无需传统的点对点匹配。该方法在多组基准测试中表现出色，尤其适合重叠率低的点云，并适用于不同传感器和尺度。支持机器人重定位、多机器人SLAM、多会话地图合并等任务。


<details>
  <summary>Details</summary>
Motivation: 当前点云配准方法主要依赖点对点特征匹配，再通过优化配准变换实现多视角注册。这些方法在低重叠等复杂场景表现有限，且存在对特征鲁棒性和对应关系的依赖。作者希望提出一种不依赖于点对点匹配、可自适应不同场景和传感器的方法。

Method: 作者将点云配准视为条件生成任务，利用网络学习一个点到点的连续速度场，实现点云的直接配准。该方法包含轻量级局部特征提取器，并在测试时加入刚性约束，保证点云变换的物理合理性。无需两两点云之间的特征匹配来推断变换，而是直接生成配准结果。

Result: 新方法在点云两两和多视角配准基准测试中达到了最新最好水平，特别是在低重叠、不同尺度和不同传感器下表现突出。实验还验证了在重定位、多机器人SLAM以及多会话地图合并等下游任务中的应用能力。

Conclusion: 本文方法无需依赖传统对应点匹配，能高效、准确地实现点云配准，并具有很好的泛化能力。支持多种应用场景，是点云配准领域的一个新突破。代码已公开，便于进一步研究与应用。

Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.

</details>


### [96] [Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards](https://arxiv.org/abs/2512.00743)
*Qiang Lyu,Zicong Chen,Chongxiao Wang,Haolin Shi,Shibo Gao,Ran Piao,Youwei Zeng,Jianlou Si,Fei Ding,Jing Li,Chun Pong Lau,Weiqiang Wang*

Main category: cs.CV

TL;DR: 本文提出了Multi-GRPO框架，通过改进credit assignment和多目标奖励处理，提高了文本到图像生成模型的对齐稳定性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在分配奖励时对所有时间步统一处理，且多目标奖励混合导致梯度不稳定、优化冲突，因此需要更精细、有针对性的奖励设计。

Method: 设计了Multi-GRPO框架：（1）采用树结构的trajectory，将早期去噪步骤分组，对早期步骤进行更准确的优势估计；（2）将每个奖励函数独立分组，分别计算优势后再聚合，从而减少奖励冲突。并引入包含颜色约束的OCR-Color-10数据集进行多目标评测。

Result: 在单一目标（PickScore-25k）和多目标（OCR-Color-10）基准下，Multi-GRPO均表现出更好的稳定性和对齐效果，能有效兼顾并权衡多种目标。

Conclusion: Multi-GRPO能够解决以往GRPO共享奖励分配和多目标奖励冲突的问题，在文本到图像生成模型的多目标对齐方面具有较大优势。

Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.

</details>


### [97] [GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment](https://arxiv.org/abs/2512.01952)
*Haoyang He,Jay Patrikar,Dong-Ki Kim,Max Smith,Daniel McGann,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei,Sebastian Scherer*

Main category: cs.CV

TL;DR: 提出了一种叫RLWG的自监督后训练框架，通过几何和感知奖励将生成式视频世界模型更好地对齐到现实世界的几何结构，大幅提升了模型在导航任务中的空间一致性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频世界建模虽然能够生成高视觉保真度的环境模拟，但缺乏几何地基，导致在需要空间一致性和长期稳定性的导航任务中表现受限。

Method: 提出RLWG框架，采用自监督的奖励函数（比如姿态循环一致性、深度重投影、时序连贯性）对预训练世界模型进行对齐。具体实现为GrndCtrl方法，基于Group Relative Policy Optimization（GRPO）进行奖励对齐后训练。

Result: 实验证明，经过GrndCtrl奖励对齐后的视频世界模型，在户外场景下的空间一致性、导航稳定性等方面均优于传统的有监督微调方法。

Conclusion: 通过奖励对齐的自监督后训练，可以有效弥补生成式世界模型的几何和空间缺陷，使其更加适用于具身智能的导航和控制任务。

Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

</details>


### [98] [Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression](https://arxiv.org/abs/2512.00744)
*Zhengxin Chen,Xiaohai He,Tingrong Zhang,Shuhua Xiong,Chao Ren*

Main category: cs.CV

TL;DR: 本文提出了一种结合多尺度门控Transformer与先验引导卷积的神经网络，用于图像压缩任务，能够在性能与计算复杂度之间实现更优平衡，超过了当前先进算法。


<details>
  <summary>Details</summary>
Motivation: 传统图像编解码器如VVC已被基于学习的方法在性能上超越，神经网络中的卷积层和Transformer对特征表达能力至关重要，但仍存在提取局部和非局部特征的能力不足、计算开销较大等问题。本研究的动机在于提升卷积层和Transformer提取图像特征的能力，同时降低运算复杂度，以获得更优的图像压缩性能。

Method: 1. 提出先验引导卷积（PGConv），引入非对称卷积和差分卷积，分别用于强化骨架元素与提取高频信息，同时通过重新参数化策略降低计算复杂度。
2. 提出多尺度门控Transformer（MGT），采用不同扩张率的多头自注意力机制及多种核尺寸的深度卷积，结合门控机制提升非线性和多尺度特征提取能力。
3. 综合上述两个模块，构建了联合多尺度门控Transformer与先验引导卷积网络（MGTPCN）。

Result: 实验结果表明，MGTPCN在性能与计算复杂度的权衡上超过了现有主流方法，实现了更好的压缩效率和解码质量。

Conclusion: 本文提出的方法能够更好地捕捉图像的局部和非局部特征，在保持较低计算复杂度的前提下，实现卓越的图像压缩表现，有望在实际应用中替代现有压缩算法。

Abstract: Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.

</details>


### [99] [Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion](https://arxiv.org/abs/2512.02017)
*Shaowei Liu,David Yifan Yao,Saurabh Gupta,Shenlong Wang*

Main category: cs.CV

TL;DR: VisualSync是一种利用多视角动态优化的新方法，可在消费级摄像头拍摄的多视频流间实现毫秒级自动同步。方法基于3D点的极线约束，通过联合优化极线误差，显著提升同步准确率。实验显示其性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头视频同步依赖人工校准、高成本硬件或对环境/对象有特殊要求，这些都限制了实际应用。人们日常拍摄活动丰富，迫切需要一种能自动、高效处理真实场景下视频同步的新方法。

Method: 提出VisualSync优化框架，通过3D重建、特征匹配与稠密跟踪提取多视角短轨(tracklets)，确定相机相对姿态与跨视角对应关系。基于任一3D点共视时满足极线约束的原理，对所有流联合优化极线误差，从而精确估算每台相机的时间偏移，实现毫秒级同步。

Result: 在四组多样化、具有挑战性的数据集上测试，VisualSync的同步误差中位数低于50毫秒，比多种现有基线方法表现更优。

Conclusion: VisualSync突破了现有多视频流自动同步的精度与适用性瓶颈，尤其适合实际、多变、无标定环境下的视频同步需求，对消费级多摄像头内容对齐具有重要推动作用。

Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.

</details>


### [100] [Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization](https://arxiv.org/abs/2512.00748)
*Ke Liu,Shangde Gao,Yichao Fu,Shangqi Gao,Chunhua Shen*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ProSeg的概率模型，用于实现多标注者医疗图像分割任务，兼顾分割结果的多样性与标注者个性化。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割任务常受到数据不确定性影响，如图像边界模糊与不同专家的标注差异。现有方法无法同时兼顾专家间多样性与个性化分割结果，因此亟需更高效的方法来建模这些差异。

Method: 本文提出了ProSeg方法，设计了两个潜变量分别建模专家标注偏好和图像边界的不确定性。通过变分推断获取条件概率分布，分割输出通过从这些分布中采样得到，兼具多样性和个性化。

Result: 在鼻咽癌（NPC）和肺结节（LIDC-IDRI）等数据集上的实验表明，ProSeg在多标注者分割任务中达到了新的最优性能，实现了高多样性且个性化的分割结果。

Conclusion: ProSeg方法能够有效结合分割多样性和专家个性化，提升了多标注者医疗图像分割的实际应用价值。

Abstract: Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.

</details>


### [101] [Data-Centric Visual Development for Self-Driving Labs](https://arxiv.org/abs/2512.02018)
*Anbang Liu,Guanzhong Hu,Jiayi Wang,Ping Guo,Han Liu*

Main category: cs.CV

TL;DR: 本论文提出一种结合真实与虚拟数据生成的混合管道，用于提升自驱动实验室中移液过程气泡检测的鲁棒性和效率，实现高精度和低人工成本的数据获取。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室中的生物实验流程对高精度有严格要求，但训练鲁棒模型所需的大量标注数据（尤其是负样本）难以获得，成为实际部署的瓶颈。

Method: 作者针对实验室关键步骤——移液操作，设计了一个混合数据采集管道：一方面使用人机协同的自动数据采集和人工验证，另一方面利用参考条件和提示引导的虚拟图像生成，并对生成数据进行筛选和验证。最终两类数据合成为均衡数据集，用于气泡识别模型训练。

Result: 在独立真实测试集上，仅用自动采集真实图像训练的模型气泡检测准确率达99.6%；混合真实与虚拟数据训练的模型准确率为99.4%，同时减少了数据采集和审核工作量。

Conclusion: 提出的混合数据生成和筛选策略有效补充了罕见事件检测所需的高质量数据，提高了实验室视觉反馈自动化的可扩展性和性价比，为数据稀缺场景下的计算机视觉任务提供了可行思路。

Abstract: Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.

</details>


### [102] [Charts Are Not Images: On the Challenges of Scientific Chart Editing](https://arxiv.org/abs/2512.00752)
*Shawn Li,Ryan Rossi,Sungchul Kim,Sunav Choudhary,Franck Dernoncourt,Puneet Mathur,Zhengzhong Tu,Yue Zhao*

Main category: cs.CV

TL;DR: 本文提出了FigEdit，一个包含3万多个样本的大规模科学图表编辑基准，用于评估和推动结构感知的图表编辑技术。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式图像编辑模型（如扩散和自回归）主要基于像素级操作，假设图像只是像素排列，但科学图表本质上是结构化数据的视觉表现，需要结构化的转换，因此像素级方法不适用于科学图表的有效编辑。

Method: 论文构建了FigEdit基准，涵盖10种图表类型和丰富的复杂编辑指令，并将任务分为五类：单步编辑、多步编辑、对话式编辑、基于视觉引导的编辑和风格迁移。同时，作者评测了多种主流生成模型在这类任务上的表现，并分析了传统评估指标在该任务中的不足。

Result: 实验结果显示，现有主流模型在科学图表编辑中表现较差，无法处理有效的结构化变换。传统的像素级评估指标（如SSIM、PSNR）也无法有效评估编辑的语义正确性。FigEdit作为基准能更好地反映模型在结构感知编辑上的能力。

Conclusion: FigEdit揭示了现有像素级图像编辑技术在科学图表编辑领域的局限性，为结构感知模型的研究和评估提供了标准平台，有助于推动该领域进一步发展。

Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.

</details>


### [103] [Seeing the Wind from a Falling Leaf](https://arxiv.org/abs/2512.00762)
*Zhiyuan Gao,Jiageng Mao,Hong-Xing Yu,Haozhe Lou,Emily Yue-Ting Jia,Jernej Barbic,Jiajun Wu,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端可微逆图形框架，可以从视频中恢复看不见的物理力场（如风力），通过联合建模物体几何、物理属性和交互，实现了根据物体运动推断力场的能力。方法在合成和真实场景中验证有效，并具备基于物理的视频生成与编辑应用潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然计算机视觉领域在视频运动建模上已有很多成果，但运动背后的物理交互（即不可见的力场）很少被直接建模和探索。了解这些物理过程可以更本质地理解图像和视频中的物体运动。

Method: 作者提出了一种可微分的逆图形（inverse graphics）框架，该框架端到端地联合建模物体几何、物理属性和交互，并通过反向传播从视频中的物体运动中恢复出背后的力场表示。

Result: 实验在合成和真实世界的视频场景上进行了验证，证明该方法可以从视频中合理推断不可见的力场（如风场）。

Conclusion: 该方法首次实现了通过视频自动推测物理交互力场，促进了视觉与物理的结合，并展示了在物理驱动的视频生成和编辑领域的应用前景。

Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{https://chaoren2357.github.io/seeingthewind/}{project page}.

</details>


### [104] [The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches](https://arxiv.org/abs/2512.00765)
*Haojie Jia,Te Hu,Haowen Li,Long Jin,Chongshi Xin,Yuchi Yao,Jiarui Xiao*

Main category: cs.CV

TL;DR: 该论文提出了一种针对交通标志分类的高隐蔽性物理对抗攻击方法TESP-Attack，能有效逃避人眼检测并高效误导智能驾驶系统，并在多种模型和现实场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有物理对抗攻击虽然能误导交通标志识别模型，但大多缺乏隐蔽性，攻击痕迹容易被人类观察者察觉，影响了其在现实世界的实用性。

Method: 论文基于人类视觉注意力主要集中在标志中央的观察，利用实例分割生成与标志边缘对齐的攻击掩码。使用U-Net生成器，根据颜色、纹理与频域分析对补丁进行优化，使其与背景环境无缝融合，从而提升视觉隐蔽性和攻击有效性。

Result: 所提出方法在多种交通标志分类模型（架构不同）下均获得90%以上的攻击成功率，在查询预算有限条件下依然有效。此外，该方法对不同模型拥有较强的迁移能力，并在不同角度和距离的现实世界场景下，攻击效果依然鲁棒。

Conclusion: TESP-Attack展现了优越的隐蔽性和攻击性能，提升了物理对抗攻击的现实威胁水平，为增强智能驾驶安全机制、检测与防御对抗样本提出了新的挑战。

Abstract: Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.

</details>


### [105] [EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes](https://arxiv.org/abs/2512.00771)
*Xiaoshan Wu,Yifei Yu,Xiaoyang Lyu,Yihua Huang,Bo Wang,Baoheng Zhang,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: EAG3R通过结合事件流和RGB图像实现了更稳健的3D几何估计，特别是在动态低光环境下，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的3D重建方法在动态场景和极端光照（如低光）条件下表现不佳，限制了真实环境应用的鲁棒性。

Method: EAG3R在MonST3R骨干网络上，引入了Retinex风格的图像增强模块和轻量级事件适配器，并利用带有信噪比感知融合机制自适应融合RGB和事件特征。同时提出事件流支持的光度一致性损失，增强时空一致性。

Result: EAG3R在无需夜间数据重新训练的情况下，在深度估计、姿态追踪及动态重建等任务上，均显著优于最先进的RGB-only方法。

Conclusion: EAG3R有效提升了不同光照和动态场景下的3D几何重建鲁棒性，验证了RGB和事件流联合优势。

Abstract: Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.

</details>


### [106] [DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering](https://arxiv.org/abs/2512.00773)
*Toshiki Katsube,Taiga Fukuhara,Kenichiro Ando,Yusuke Mukuta,Kohei Uehara,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出了一个高质量大规模日语视觉-语言资源自动构建流程，并据此构建了DEJIMA-Cap和DEJIMA-VQA两个包含388万图文对的数据集，显著提升了日语多模态建模的资源基础。


<details>
  <summary>Details</summary>
Motivation: 目前日语视觉-语言(V&L)模型缺乏高质量的大规模资源，现有数据集规模有限、质量参差、缺乏文化代表性，这严重制约了相关模型的发展。作者希望通过自动化流程，低成本构建高质量、具日本文化特征的大型V&L数据集，推动日语多模态AI的发展。

Method: 作者提出了可扩展且可复现的数据构建流水线，包括：大规模网页爬取、严格的数据过滤和去重、基于目标检测的图像证据抽取、以及大模型（LLM）在语义约束下进行文本精炼。最终生成了两个数据集：DEJIMA-Cap（图像-描述）和DEJIMA-VQA（视觉问答），达到388万高质量图文对。该流程所有数据来源和模块均为可商用。

Result: 人工评估显示，DEJIMA数据集的“日本性”与语言自然性远高于翻译或人工标注数据集，并且保持了与人工标注语料相当的事实正确性。定量分析表明，其视觉域覆盖广泛、具有代表性。基于DEJIMA训练的模型，在多项主流日语多模态基准上取得了持续提升。

Conclusion: 作者证明了大规模、深具文化与语言代表性的日语视觉-语言数据集能够显著提升多模态模型性能，并为该领域后续研究和工业应用提供了基础资源。数据集已公开，有望推动日语及跨文化多模态AI发展。

Abstract: This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.

</details>


### [107] [PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery](https://arxiv.org/abs/2512.00794)
*Bo Guo,Sijia Wen,Yifan Zhao,Jia Li,Zhiming Zheng*

Main category: cs.CV

TL;DR: 论文提出PolarGS方法，通过引入偏振信息辅助现有基于RGB的3D高斯溅射表面重建模型，显著提升了在高光和无纹理区域的几何精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射重建方法在反光表面与无纹理区域的几何恢复精度较差，主要因为RGB信息在这些区域的光照信息不可靠，而偏振光能够提供额外的表面取向线索。

Method: 提出PolarGS方法，扩展传统3DGS框架，利用偏振度和偏振角度信息。具体包括两个模块：（1）反射区域通过线偏振度DoLP识别，并用颜色优化图精修；（2）高斯点云扩密模块在无纹理区域结合DoLP和偏振角信息，采用PatchMatch深度补全，提升形状还原。

Result: PolarGS在多个基准数据集下显著提升了高难度区域的重建几何精度，效果优于现有最高水平方法。

Conclusion: 将偏振线索引入3DGS是解决RGB信息受限场景下重建可靠性的有效途径，PolarGS能够框架无关地提升几何重建精度，有潜力广泛应用于各类3D重建场景。

Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.

</details>


### [108] [CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target](https://arxiv.org/abs/2512.00796)
*Jiajian He,Enjie Hu,Shiqi Chen,Tianchen Qiu,Huajun Feng,Zhihai Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为CircleFlow的高保真点扩散函数（PSF）估计框架，通过引导式边缘定位实现了精确的模糊特性描述，实现了在仿真和真实数据上领先的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确估计相机的点扩散函数对于光学表征和计算视觉任务至关重要，但受限于去卷积问题的不适定性和信息歧义，传统方法很难获得高质量的PSF估计。

Method: CircleFlow框架采用有结构的采集方式，利用圆形网格靶标编码具有局部各向异性和空间变化的PSF。方法中通过目标的二值先验实现图像与核的解耦，在亚像素级别上使用光流引导的边缘定位对二值结构进行对齐，PSF建模为能量约束的神经隐式表达，并在区分马赛克处理的可微框架中联合优化上述两个部分。

Result: 在大量仿真与真实数据实验中，CircleFlow在PSF估计的准确性与可靠性方面均达到了最新水平。

Conclusion: CircleFlow方法有效提升了PSF的物理一致性与鲁棒性，适用于实际应用中的高精度PSF标定。

Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.

</details>


### [109] [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805)
*Pengfei Hu,Meng Cao,Yingyao Wang,Yi Wang,Jiahua Dong,Jun Song,Yu Cheng,Bo Zheng,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的推测性时间推理框架SpecTemp，显著提升了视频理解模型的推理效率，并保持了较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的视频多模态大模型采用thinking-with-frames范式，通过全局与局部推理提升推理能力，但存在随着推理进展多模态上下文冗余与效率瓶颈的问题。

Method: SpecTemp采用合作式双模型设计，将时间感知与推理分离。轻量级草稿模型从密集采样的视频帧中快速筛选出重要帧，高能力目标模型负责深入推理与草稿验证，并迭代优化注意力分布，直到收敛。此外，作者还构建了包含粗粒度与细粒度证据标注的新数据集SpecTemp-80K。

Result: 在多个视频理解基准上，SpecTemp的准确率与竞争方法持平，但推理速度得到大幅提升。

Conclusion: SpecTemp框架通过分工协作式推理路径，兼顾推理效率和准确率，有望为长视频理解提供更高效的人类式智能方案。

Abstract: Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.

</details>


### [110] [IRPO: Boosting Image Restoration via Post-training GRPO](https://arxiv.org/abs/2512.00814)
*Haoxuan Xu. Yi Liu,Boyuan Jiang,Jinlong Peng,Donghao Luo,Xiaobin Hu,Shuicheng Yan,Haoang Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的后训练范式IRPO，用于图像复原任务，突破现有方法的泛化性与过度平滑问题，并在多项基准测试上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 目前低层视觉任务领域，大多图像复原方法依赖于像素级对齐，这造成过度平滑与泛化能力不足。尽管高层生成任务后训练技术已成功应用，低层视觉尚未充分探索。此论文旨在将后训练范式引入图像复原，并提升性能与泛化能力。

Method: 作者提出IRPO后训练范式：1）创新性地挑选预训练表现不佳的样本用于后训练，提高效果与效率；2）设计了由结构保真度、知觉一致性（基于Qwen-VL的专家评价）与复原质量三者组成的奖励标准体系，综合平衡客观指标与人类感受。

Result: 在六个同分布与五个异分布低层视觉基准上，IRPO超越主流AdaIR基线，分别在同分布任务与异分布设置上提升0.83 dB与3.43 dB，表现出更强的泛化和复原能力。

Conclusion: IRPO有效扩展了后训练范式在低层视觉领域的应用边界，显著提升了图像复原的表现，并为后续研究提供了新的方向与方法。

Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.

</details>


### [111] [PanFlow: Decoupled Motion Control for Panoramic Video Generation](https://arxiv.org/abs/2512.00832)
*Cheng Zhang,Hanwen Liang,Donny Y. Chen,Qianyi Wu,Konstantinos N. Plataniotis,Camilo Cruz Gambardella,Jianfei Cai*

Main category: cs.CV

TL;DR: 本论文提出了一种用于全景视频生成的新方法PanFlow，实现了更精确的运动控制并提升了视频质量。


<details>
  <summary>Details</summary>
Motivation: 当前全景视频生成方法难以有效控制运动，尤其是在大幅度和复杂运动场景下表现不佳，限制了其在虚拟现实等领域的应用。

Method: 作者提出PanFlow方法，利用全景图像的球面特性，将高度动态的摄像机旋转与输入的光流条件分离，实现对大幅运动的更精确控制。同时，设计了球面噪声扰动策略，提高全景边界上的运动一致性。此外，作者还构建了一个大规模、运动丰富的全景视频数据集，提供帧级位姿和光流标注。

Result: PanFlow在运动转移、视频编辑等多个应用场景中展示了优异表现。在运动保真度、视觉质量和时序一致性方面显著优于现有方法。

Conclusion: PanFlow为全景视频生成领域提供了新思路，解决了运动控制和跨边界一致性难题，适用于多种实际应用场景，推动了全景视频生成技术的发展。

Abstract: Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.

</details>


### [112] [AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent](https://arxiv.org/abs/2512.00846)
*Neeraj Anand,Rishabh Jain,Sohan Patnaik,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 本文提出AFRAgent，一种基于instruct-BLIP的多模态架构，用于提升移动UI自动化的性能，并在保持小模型规模（仅为主流模型1/4大小）的同时达到了新的业界最优。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的发展，现有移动UI自动化方法虽能根据屏幕内容独立于设备API进行操作，但模型在识别控件、确定操作时因空间信息受限而表现不佳，且模型参数庞大，推理延迟高。

Method: 提出AFRAgent架构，基于instruct-BLIP，并创新性地采用自适应特征再归一化（token-level affine transformation）方法丰富视觉编码器的空间信息，将低分辨图像嵌入增强融合高分辨细节。

Result: 在Meta-GUI和AITW基准数据集上完成实验，AFRAgent成功刷新了业界在智能手机自动化任务上的最新基线。

Conclusion: AFRAgent在大幅缩小模型规模的同时，有效提升GUI自动化的准确率与效率，验证了自适应特征再归一化技术在多模态任务中的价值。

Abstract: There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.

</details>


### [113] [Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting](https://arxiv.org/abs/2512.00850)
*Haishan Wang,Mohammad Hassan Vali,Arno Solin*

Main category: cs.CV

TL;DR: 提出了Smol-GS方法，将3D Gaussian Splatting编码为空间和语义高度压缩的表征，在保证渲染质量的情况下大幅度提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting在表达3D场景时表征效率不高，难以实现大规模压缩且在泛化和语义利用上有局限，急需更高效的编码方式以支持存储、传输或下游应用。

Method: 使用递归体素层次结构对splats坐标编码，并为每个splat引入抽象特征（颜色、不透明、变换、材料属性等）。该组合兼顾了空间结构与语义信息，提升了表征能力与压缩效率。

Result: 在标准测试基准上实现了当前最好的3D场景压缩率，同时保持高质量渲染效果。

Conclusion: Smol-GS不仅能高效压缩3D场景和保障可视化质量，其离散化表征对场景理解、导航及规划等下游任务也具有应用潜力。

Abstract: We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.

</details>


### [114] [TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models](https://arxiv.org/abs/2512.00872)
*Tim Veenboer,George Yiasemis,Eric Marcus,Vivien Van Veldhuizen,Cees G. M. Snoek,Jonas Teuwen,Kevin B. W. Groot Lipman*

Main category: cs.CV

TL;DR: 本文提出了一种适用于医学CT影像的通用基础模型TAP-CT，能够无需大量微调即可在多任务中取得良好表现。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的基础模型往往需要大量微调或依赖资源消耗大的解码器，且大多编码器训练目标偏向特定任务，缺乏任务无关的强大表征。

Method: 作者基于ViT与DINOv2设计了一套适用于3D CT体数据的自监督预训练方法。通过调整patch embedding、位置编码和体素增强，使模型对体积数据深度感知得到增强，并在大规模CT数据(105K volumes)上进行3D预训练。

Result: 模型在多种下游医学任务中展现出高度稳健且泛化能力强的冻结表征，无需大量微调即可取得优异效果。

Conclusion: TAP-CT为医学影像领域提供了一组透明、可复现且资源需求低的基础模型基线，模型及代码将开源，推动相关研究。

Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.

</details>


### [115] [Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation](https://arxiv.org/abs/2512.00873)
*Haoshen Wang,Lei Chen,Wei-Hua Zhang,Linxia Wu,Yong Luo,Zengmao Wang,Yuan Xiong,Chengcheng Zhu,Wenjuan Tang,Xueyi Zhang,Wei Zhou,Xuhua Duan,Lefei Zhang,Gao-Jun Teng,Bo Du,Huangxuan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为DeepPriorCBCT的深度学习三阶段框架，可以在仅使用传统剂量六分之一射线的情况下，实现诊断级质量的锥形束CT（CBCT）重建，且其图像质量和诊断性能与标准方法相当，同时显著降低患者受射线风险。


<details>
  <summary>Details</summary>
Motivation: 现有CBCT引导穿刺诊疗虽然有效，但患者因射线暴露风险显著增加，尤其是多次检查后二次肿瘤发生风险升高。目前低剂量CBCT方案缺乏大规模、多中心验证和临床前瞻性评估，急需安全且可临床推广的低剂量CBCT重建方法。

Method: 提出DeepPriorCBCT，一个三阶段深度学习重建框架，在多中心4102名患者8675次CBCT扫描回顾队列与前瞻性交叉试验中开发和验证。模型所重建的低剂量图像，经11位医师主观评分与标准算法无明显差异。

Result: 模型重建的图像在主观评价、诊断准确性及总图像质量上与常规技术不相上下。前瞻性试验中五位放射科医生及25位介入医生均无明显偏好，充分验证了模型在实际临床场景的可靠性。射线剂量降至传统疗法的六分之一。

Conclusion: DeepPriorCBCT实现了在低剂量（约1/6传统剂量）条件下的高质量CBCT重建，能有效降低介入过程中的射线风险，同时保证临床诊断和手术导航的图像质量，是推动低剂量CBCT临床应用的重要进展。

Abstract: Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P>0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa<0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.

</details>


### [116] [Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling](https://arxiv.org/abs/2512.00877)
*Zhening Liu,Rui Song,Yushi Huang,Yingdong Hu,Xinjie Zhang,Jiawei Shao,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的前馈式3D高斯点云压缩框架，极大提升了压缩率，并能有效建模长距离空间相关性，达到当前最优效果。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点云作为新兴的三维场景表示方式，在重建质量上表现突出，但其庞大的数据量严重阻碍了其广泛应用。尽管前馈式压缩方法提高了实用性，然而现有方法难以抓住长距离空间依赖，压缩效果有限。因此迫切需要新方法提升3DGS的压缩效率和泛化能力。

Method: 作者提出基于Morton编码序列的超大上下文结构，结合精细空间-通道自回归熵模型，以更好利用数千个高斯点的相关性。此外，引入基于注意力机制的变换编码网络，能从大量邻近高斯聚合特征，提取更有信息量的潜在先验。所有流程均为前馈式推理，提升了效率。

Result: 本文方法实现了3D高斯点云20倍压缩比，且在前馈推理条件下在通用压缩方法中取得了业界最佳的重建性能。

Conclusion: 该方法大幅提升了3DGS压缩率和泛化能力，在不影响高效推理的基础上有效建模了长距离相关性，有望推动3DGS在实际应用中的普及。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.

</details>


### [117] [Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning](https://arxiv.org/abs/2512.00880)
*Haijian Shao,Wei Liu,Xing Deng*

Main category: cs.CV

TL;DR: 本文提出了一种量子启发的几何方法来衡量神经网络算子的功能冗余，为多模态智能系统在异构硬件上的高效部署打下了理论基础。


<details>
  <summary>Details</summary>
Motivation: 多模态智能在资源受限且异构的家用硬件上部署时，面临特征异质、实时性与硬件算子冗余等难题，亟需有效算子度量与冗余裁剪机制。

Method: 提出用Bloch超球面上的归一化奇异值谱表述神经网络算子的特征，通过量子几何距离（如Fubini–Study/Wasserstein-2）推导功能等价性，并构建基于该度量的功能冗余图和一键结构化剪枝算法（QM-FRG）。

Result: 仿真实验证明提出的度量优于以往的幅值和随机基线。大规模多模态transformers和异构硬件上的实验结果将在后续期刊扩展版报告。

Conclusion: 量子度量驱动的功能冗余建模为多模态算子在跨模态、跨硬件架构的可替代性和高效部署奠定了理论基础。

Abstract: The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.

</details>


### [118] [Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints](https://arxiv.org/abs/2512.00882)
*Xisheng Feng*

Main category: cs.CV

TL;DR: 该论文针对多模态大模型在精准农业等专业领域出现的推理型幻觉现象，提出了一种“看、复述、再答”新框架，在模型参数冻结条件下，通过自生成知识线索，大幅提升了视觉-语言模型的推理能力和领域适应性，显著优化了农业场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在像精准农业这样高专业性的领域上表现不足，主要原因是模型更依赖语言先验而非真实视觉线索，导致推理过程中出现“幻觉”问题，即视觉嵌入无法有效激活已存储的专家知识，成为模型性能瓶颈。

Method: 提出“看、复述、再答”三阶段推理流程：（1）Look阶段生成客观的视觉描述及候选集合；（2）Recite阶段用轻量路由器将视觉线索转化为可激活候选领域知识的查询；（3）Answer阶段并行比对视觉描述与知识内容，选取最一致的标签。全流程实现参数高效，不需更改主干模型。

Result: 在农业多模态基准AgroBench上，所提方法取得当前最佳表现。尤其是在除草识别任务上，准确率比Qwen-VL提高23.6%，同时优于不依赖外部检索的GPT-4o。

Conclusion: 模块化设计有效降低了模型推理时的幻觉现象，将被动的表征感知转化为主动的知识检索，大幅提升专业领域多模态推理可信度和准确率，对后续专业VLM研究具有重要借鉴意义。

Abstract: Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval

</details>


### [119] [HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics](https://arxiv.org/abs/2512.00885)
*Masatoshi Tateno,Gido Kato,Hirokatsu Kataoka,Yoichi Sato,Takuma Yagi*

Main category: cs.CV

TL;DR: 本文提出了HanDyVQA基准数据集，实现了对手与物体交互（HOI）中操作和结果两方面的细粒度动态推理评测。


<details>
  <summary>Details</summary>
Motivation: 现有的语义HOI基准主要关注粗粒度的操作或效果，缺乏对HOI底层动态的细致时空推理，限制了对真实复杂交互场景的理解和模型发展。

Method: 作者构建了HanDyVQA视频问答数据集，涵盖操作、过程、对象、地点、状态变化和对象部件六种问题类型，共11100组多选题。数据集还包括10300个用于对象及其部件分割的问题掩码，支持对视频中对象/部件推理能力的评估。

Result: 在该基准上测试了最新视频基础模型，发现最好表现（Gemini-2.5-Pro模型）平均准确率仅为73%，远低于人类表现的97%。分析表明模型在空间关系、运动、局部几何理解等方面仍有明显挑战。同时，结果显示引入显式HOI相关视觉特征有助于提升模型表现。

Conclusion: HanDyVQA数据集推动了对HOI动态的细粒度理解，现有模型尚远未达人类水平。引入HOI相关信息对模型推理有促进作用，为未来系统设计和研究指明了方向。

Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.

</details>


### [120] [Multilingual Training-Free Remote Sensing Image Captioning](https://arxiv.org/abs/2512.00887)
*Carlos Rebelo,Gil Rocha,João Daniel Silva,Bruno Martins*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强提示、无需训练的数据驱动多语种遥感图像描述方法，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像描述方法依赖大规模人工标注英语数据，限制了其在多语种和全球范围的应用。为降低数据需求并提升多语言适应性，作者提出全新训练自由、可多语的描述系统。

Method: 方法基于SigLIP2视觉编码器检索与输入图像相关的描述和少量示例，然后将这些信息输入给多语种的大语言模型(LLM)或视觉-语言模型(VLM)。系统分为两种模式：一种仅依赖文本提示(LLM)，另一种结合图像和提示(VLM)。为提升检索内容连贯性，设计了基于PageRank的图结构重排序机制。

Result: 在4个基准数据集、10种语言上的实验证明，该方法与完全监督的英语系统性能相当，且具备良好语言泛化能力。引入PageRank重排序后性能提升显著，可达35%。VLM生成视觉相关且多样描述，LLM在BLEU和CIDEr指标上表现更好，直接生成目标语言描述优于先翻译后生成。

Conclusion: 本工作首次系统性评估了多语种、训练自由的遥感图像描述方法，为建设包容性与可扩展的地球观测多模态系统迈出了重要一步。

Abstract: Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.

</details>


### [121] [Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)
*Yiyu Wang,Xuyang Liu,Xiyan Gui,Xinying Lin,Boxue Yang,Chenfei Liao,Tailai Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本论文提出了STC（Streaming Token Compression）框架，通过缓存和裁剪视频帧视觉token，有效提升流式VideoLLM处理效率，并在保持高准确率的同时显著降低延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前流式视频大模型（VideoLLM）在实际应用中，由于需要连续处理大量高密度视觉token，特别是在ViT编码阶段对相似帧的重复处理，导致实时性差且计算消耗巨大，影响部署效率。因此，急需降低延迟和内存消耗，同时维持模型性能。

Method: 作者提出STC框架，包括两个主要模块：STC-Cacher用于缓存和重用相似帧的特征，减少ViT编码重复计算；STC-Pruner根据时空相关性裁剪即将输入LLM的visual token，只保留最具代表性的部分，从而减少序列长度和计算负担。STC为层级式、可直接集成的加速组件。

Result: 作者在四个主流VideoLLM与五项基准数据集上验证了STC的有效性。结果显示，STC能在保持99%准确率的基础上，将ViT编码延迟降至24.5%、LLM预填延迟降至45.3%，均优于其他压缩方法。

Conclusion: STC框架能大幅提升流式VideoLLM的实时推理效率且易于集成，为视频理解场景下大模型的实用化部署提供了有效方案。

Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.

</details>


### [122] [Hierarchical Semantic Alignment for Image Clustering](https://arxiv.org/abs/2512.00904)
*Xingyu Zhu,Beier Zhu,Yunfan Li,Junfeng Fang,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练图像聚类方法，结合了层次化的语义对齐策略，通过引入描述层级和名词层级的文本语义，有效提升聚类性能，并在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部语义（如名词）提升图像聚类的方法，往往忽视了名词语义的歧义性，导致聚类表现下降。因此需要引入更细粒度和多层次的语义信息，提升图像与语义的对齐与聚类效果。

Method: 提出了一种称为CAE的层次化语义对齐方法，结合名词级别（高层类别）和描述级别（细粒度属性）的文本信息，通过WordNet选取相关名词与描述数据集选取描述，构建一个与图像特征对齐的语义空间。随后利用最优传输方法对齐图像特征与名词、描述，最后融合增强后的语义特征和图像特征实现聚类，全程无需训练。

Result: 在8个公开数据集上进行了广泛实验证明了提出方法的有效性。尤其在ImageNet-1K上，相较于现有最佳无训练方法，精度提升4.2%，ARI提升2.9%。

Conclusion: 通过引入多层次、细粒度的文本语义并优化其与图像特征的对齐关系，在无监督训练下实现了更优的图像聚类效果，方法具有实用性和推广潜力。

Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.

</details>


### [123] [TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model](https://arxiv.org/abs/2512.00909)
*Alireza Javanmardi,Pragati Jaiswal,Tewodros Amberbir Habtegebrial,Christen Millerdurai,Shaoxiang Wang,Alain Pagani,Didier Stricker*

Main category: cs.CV

TL;DR: 提出了一种名为TalkingPose的扩散模型框架，可从单张RGB图像和驱动姿势生成长时、时序一致的人体上半身动画。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在角色动画生成方面取得进展，但现有方法只能处理较短视频片段，难以生成长时间、时序连贯的动画，主要受限于计算和内存瓶颈。

Method: 提出TalkPose框架，通过利用驱动帧捕捉表情和手部动作，并使用稳定扩散主干网络实现精确动作迁移。同时，提出基于反馈机制的图像级扩散方法以提升运动连续性和时序一致性，无需额外计算或增设训练阶段。

Result: TalkingPose能够生成时序连贯、任意时长的人体上半身动画。还提出了一个大规模数据集，成为该类动画生成的新基准。

Conclusion: TalkingPose突破了现有方法在长时动画生成上的瓶颈，提升了动画生成的连贯性与实用性，对人类动作生成任务和数据集建设具有重要推动作用。

Abstract: Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.

</details>


### [124] [Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision](https://arxiv.org/abs/2512.00911)
*Yuhao Shan,Qianyi Yuan,Jingguo Liu,Shigang Li,Jianfeng Li,Tong Chen*

Main category: cs.CV

TL;DR: 提出了一种新的双流角度感知生成网络，同时预测全景相机的倾斜角度并生成端正的全景图像，在SUN360和M3D数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人视觉中全景相机常因姿态不稳获取非端正图像，影响后续任务。传统基于IMU的方法易受漂移和干扰，视觉方法更具潜力。

Method: 网络包含CNN（自等矩投影提取局部结构信息）和ViT（由立方体投影提取全局上下文信息）双分支，通过双投影自适应融合模块对齐空间特征。同时引入高频增强块、循环填充和通道注意力机制，确保连续性和几何敏感性。

Result: 在SUN360和M3D数据集上，该方法在倾角估计与端正全景生成两方面均优于同类方法。消融实验验证各子模块的重要作用及两任务协同提升。

Conclusion: 所提方法有效改善非端正全景图像自动校正问题，提升全景相机在机器人视觉任务中的实用性。

Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360° continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.

</details>


### [125] [ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices](https://arxiv.org/abs/2512.00912)
*Abdelghafour Halimi,Ali Alibrahim,Didier Barradas-Bautista,Ronell Sicat,Abdulkader M. Afifi*

Main category: cs.CV

TL;DR: 本文提出了一个基于深度学习的自动分类流程，利用2D micro-CT切片对12种有孔虫进行分类，最终模型准确率达95.64%，AUC为0.998，并开发了实时交互式分类仪表盘，推进了地质学辅助AI识别的研究。


<details>
  <summary>Details</summary>
Motivation: 有孔虫分类在地质和古生物学研究中非常重要，但传统方法依赖人工，耗时且易错。随着3D micro-CT扫描技术普及，急需自动化、高效且准确的计算机辅助分类工具。

Method: 作者整理了97份有孔虫标本的3D micro-CT数据，通过严格的数据分割避免泄露，选取12种样本量充足的物种，截取109,617张2D切片用于训练/验证/测试。利用7种主流2D卷积神经网络结构，基于迁移学习对分类效果进行对比。最终采用ConvNeXt-Large和EfficientNetV2-Small集成模型，并开发了一套支持实时分类与3D匹配的交互式仪表盘。

Result: 集成模型在测试集上的准确率为95.64%，Top-3准确率达99.6%，AUC高达0.998，显著优于以往方法。仪表盘可进行实时切片分类和基于多种相似性指标（SSIM、NCC、Dice系数）的3D切片匹配。

Conclusion: 本研究建立了有孔虫自动分类的新基准，提出了可复现的深度学习流程并开发了实际应用工具，为地质学与深度学习的结合提供了先进范例。

Abstract: This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.

</details>


### [126] [LAHNet: Local Attentive Hashing Network for Point Cloud Registration](https://arxiv.org/abs/2512.00927)
*Wentao Qu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云配准方法LAHNet，通过引入局部注意力机制和哈希策略，有效提升了特征的区分能力，在室内外基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重局部感知，忽视了更大感受野下特征差异性的提升。合理且更广泛的感受野对提升点云特征区分度至关重要。为弥补这一短板，作者设计了能兼顾局部与全局信息的网络结构。

Method: 提出LAHNet，在点云描述子中引入带有卷积操作归纳偏置的局部注意机制。设计了Group Transformer和基于局部敏感哈希的线性邻域检索，将点云划分为不重叠窗口，并采用跨窗口策略扩展感受野；提出Interaction Transformer，通过计算重叠区域的Overlap Matrix提升不同配准点云间窗口级别全局特征的交互与匹配。

Result: 在真实室内外数据集上经过大量实验，证明LAHNet学到的特征更加稳健且具有良好区分性，配准精度领先现有方法。

Conclusion: LAHNet通过合理利用局部与全局结构信息，提升了点云配准性能，为点云配准特征学习提供了有效方法。

Abstract: Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.

</details>


### [127] [SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding](https://arxiv.org/abs/2512.00936)
*Keita Otani,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出了一种新方法SceneProp，通过最大后验推断将场景图定位 formulated为MRF推理，有效提高了多物体复杂关系的视觉-语言定位任务准确率，且复杂度越高表现越佳。


<details>
  <summary>Details</summary>
Motivation: 以往的视觉-语言模型在处理包含多个物体和关系的复杂、组合性视觉查询（如场景图定位）时表现不佳，尤其是随着查询描述变得更复杂时性能反而下降。因此，亟需寻找一种能有效解析并利用复杂关系信息的结构化方法。

Method: SceneProp方法将场景图定位问题重构为马尔可夫随机场（MRF）中的最大后验推断问题，通过全局推断为图中每个结点最优分配图像区域，满足所有约束。此外，该方法采用可微分的信念传播（Belief Propagation）算法，在端到端框架下实现联合优化。

Result: 在四个基准数据集上，SceneProp方法在复杂度提升时表现优异，相比现有方法显著提升了场景图定位的准确率，并首次实现了随着查询图结构的复杂性增加，性能随之提升。

Conclusion: SceneProp有效地解决了以前方法难以处理复杂场景图查询的问题，验证了结构化关系信息的利用可以带来更好的定位效果，对于多关系视觉-语言理解任务具有重要意义。

Abstract: Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.

</details>


### [128] [Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation](https://arxiv.org/abs/2512.00944)
*An Yang,Chenyu Liu,Jun Du,Jianqing Gao,Jia Pan,Jinshui Hu,Baocai Yin,Bing Yin,Cong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新型3D高斯泼溅（3D-GS）语义分割方法，通过二进制编码和渐进式训练，实现精细分割和高效内存利用。


<details>
  <summary>Details</summary>
Motivation: 当前3D-GS分割方法依赖高维分类特征，导致内存开销大，且在细粒度分割上表现有限，缺乏多粒度控制。

Method: 提出基于高斯点的类别二进制编码方案，将每个特征压缩为一个整数以降低内存；采用渐进式子任务分解训练策略以提升精细分割能力；并在训练阶段调整不透明度，缓解前景与背景混淆问题。

Result: 在多个基准测试上，方法在保持甚至提升分割性能的同时，显著降低了内存消耗和推理时间。

Conclusion: 该方法有效兼顾了分割精度、推理速度及内存效率，为3D-GS分割任务提供了更优的解决思路。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.

</details>


### [129] [Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval](https://arxiv.org/abs/2512.00953)
*Haojian Huang,Kaijing Ma,Jin Chen,Haodong Chen,Zhou Wu,Xianghao Zang,Han Fang,Chao Ban,Hao Sun,Mulin Chen,Zhongjiang He*

Main category: cs.CV

TL;DR: 本文提出了一种名为DEMR（Debiased Evidential Learning for Moment Retrieval）的新方法，以提升基于自然语言查询的视频片段检索表现，显著增强了不确定性估计和跨模态对齐能力。


<details>
  <summary>Details</summary>
Motivation: 原有视频片段检索方法在处理复杂或歧义时刻时，对细粒度信息和确定性推理能力不足，导致不确定性高误判和适应性弱。因此，亟需更高鲁棒性、更好适应复杂场景的技术。

Method: 论文提出DEMR框架，设计了Reflective Flipped Fusion块以提升跨模态对齐，并引入查询重建任务加强文本敏感性以降低不确定性偏置。同时，提出Geom-regularizer进一步优化不确定性估计，使系统能更好地识别困难片段。

Result: 在ActivityNet-CD和Charades-CD等标准及偏置数据集实验表明，DEMR在检索效果、鲁棒性和可解释性等方面达到或超过现有方法。

Conclusion: DEMR框架显著提升了复杂场景中时刻检索任务的准确性和可靠性，在实际应用中具有较强的推广潜力。

Abstract: In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.

</details>


### [130] [Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction](https://arxiv.org/abs/2512.00960)
*Boran Wen,Ye Lu,Keyan Wan,Sirui Wang,Jiahong Zhou,Junxuan Liang,Xinpeng Liu,Bang Xiao,Dingbang Huang,Ruiyang Liu,Yong-Lu Li*

Main category: cs.CV

TL;DR: 本文提出了4DHOISolver框架，通过结合人工标注和物理约束，从单目互联网视频中高效重建大规模4D人-物交互数据，并发布了开放数据集Open4DHOI，同时指出了现有3D基础模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人需要学习丰富的人-物交互，但真实世界中的大规模、多样化数据难以高效、准确地获取。互联网视频资源丰富但重建三维/四维人-物交互过程存在挑战。

Method: 提出4DHOISolver优化框架，利用人工参与标注接触点，将其作为约束，提高4D人-物交互的重建准确性和物理合理性。同时推出包含144种对象和103种动作的大规模Open4DHOI数据集。还应用强化学习验证了重建动作可被代理模仿。

Result: 新方法能在保证时空一致性和物理真实性的前提下，显著提升4D人-物交互重建质量；制作了丰富的新数据集；验证了动作可迁移性。同时发现现有3D基础模型在自动识别精准接触点方面存在明显短板。

Conclusion: 4DHOISolver与Open4DHOI为推动真实世界机器人研究提供关键支撑，当前人机协作式标注仍不可或缺，为社区提出挑战方向。数据与代码将开源。

Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/

</details>


### [131] [PhotoFramer: Multi-modal Image Composition Instruction](https://arxiv.org/abs/2512.00993)
*Zhiyuan You,Ke Wang,He Zhang,Xin Cai,Jinjin Gu,Tianfan Xue,Chao Dong,Zhoutong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了PhotoFramer，一个多模态照片构图指导系统，能针对构图不佳的照片生成文字建议和重构的示范图片。


<details>
  <summary>Details</summary>
Motivation: 普通用户在拍照时常常难以得到良好的构图效果，因此需要自动化工具来提供构图建议，从而帮助用户拍摄更美观的照片。

Method: PhotoFramer通过构建大规模数据集，将构图改进任务细分为移动、缩放和视角变换等子任务。利用裁剪数据集和多视角数据集生成训练样本，并训练一个模型，可以对输入的构图差图片生成自然语言改进建议及改进后的示例图片。

Result: 实验表明，PhotoFramer生成的文本指导可以有效引导用户进行照片构图，将文本指导与图片示例结合能显著优于仅用图片示例的方式。

Conclusion: PhotoFramer是一种实用的照片构图辅助方法，可以让普通用户获得专家级的拍照经验和建议，提升日常摄影作品质量。相关代码和数据已开源。

Abstract: Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.

</details>


### [132] [S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud](https://arxiv.org/abs/2512.00995)
*Han Su,Tianyu Huang,Zichen Wan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本论文提出了一种结合2D分割先验和3D一致监督的新方法S2AM3D，有效提升了点云分割的泛化性与一致性，并构建了大规模高质量数据集，实验验证了其领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云分割存在泛化性差和2D先验引入后分割结果不一致等问题。需要一种既能利用2D预训练知识、又能保证3D一致性的点云分割方法。

Method: 提出S2AM3D方法：1）设计点一致的部件编码器，通过3D对比学习聚合多视角2D特征，获得全局一致的点特征；2）提出尺度感知的提示解码器，可实时调整分割粒度；3）构建了超10万样本的高质量部件级点云数据集。

Result: 实验表明，S2AM3D在多项主流评测下表现优异，相比现有方法在复杂结构和尺寸变化大的部件分割中展现出更强的鲁棒性和可控性。

Conclusion: S2AM3D有效融合了2D和3D知识，解决了点云分割中的泛化与一致性难题，并凭借新数据集和创新模型在实际应用中展示出优越效果。

Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.

</details>


### [133] [Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints](https://arxiv.org/abs/2512.00999)
*Mohsin Rasheed,Abdullah Al-Mamun*

Main category: cs.CV

TL;DR: 该论文提出了一个融合了高阶语义信息和区块链可追溯性的医学影像重建框架，有效提升了结构一致性与溯源安全性。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据常因噪声、损坏或篡改导致AI辅助解读不可靠。以往方法注重像素级重建，容易损失关键解剖结构，影响临床判读。医学影像的可信恢复与可追溯机制亟需改进。

Method: 作者提出一种语义感知的医学影像重建框架，结合高阶潜在语义嵌入与混合型U-Net架构，突出恢复临床相关结构。同时，加入基于尺度无关图设计的轻量区块链溯源层，实现重建过程的可验证记录而低开销。

Result: 在多数据集、多种损坏条件下，该方法相比现有方案在结构一致性、恢复精度及溯源完整性上表现更优。

Conclusion: 该框架通过将语义引导重建与安全溯源结合，增强了医学影像AI的可靠性，为临床诊断信心和医疗合规提供保障。

Abstract: Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.

</details>


### [134] [LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency](https://arxiv.org/abs/2512.01008)
*Zhongbin Guo,Jiahe Liu,Wenyu Gao,Yushan Li,Chengzhi Li,Ping Jian*

Main category: cs.CV

TL;DR: 本文提出了LISA-3D，一种将文本驱动的语言-图像分割扩展至三维重建的系统，通过两阶段架构实现无需3D文本标注的跨视角一致性分割和3D重建，提升了任务表现且无需大量参数和数据。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的3D重建任务需要模型不仅能理解开放词汇的文本指令，还要能保证不同视角下的分割一致性。现有方法在跨视角一致性或通用性方面存在不足，因此亟需一种能有效融合语言与视角信息，且无需大规模新增3D-文本标注的数据高效方案。

Method: 提出LISA-3D框架，将文本-图像指令模型LISA通过结构感知的LoRA层融入三维信息，并与冻结的SAM-3D重建器结合。训练时，利用RGB-D序列及相机位姿设计可微重投影损失，实现分割在不同视角间保持一致，无需额外3D-文本标注。分割结果与原图拼接，作为SAM-3D输入，直接输出高斯点云或有纹理网格模型，无需重训练。

Result: 在ScanRefer和Nr3D数据集上，LISA-3D的语言到3D重建准确率相较单视角基线有高达15.6个百分点的提升，且参数更新量仅为11.6M，显示出优越的性能和高效的数据利用能力。

Conclusion: LISA-3D具备模块化、数据高效、支持零样本泛化等优点，为基于自然语言的3D内容创作提供了实用的解决方案。

Abstract: Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.

</details>


### [135] [Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model](https://arxiv.org/abs/2512.01030)
*Jing He,Haodong Li,Mingzhi Sheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种两阶段确定性框架Lotus-2，从单张图片中实现高质量像素级几何属性预测，并以极少的数据训练量在深度和表面法线预测任务上达到了新纪录。


<details>
  <summary>Details</summary>
Motivation: 传统的单图像几何预测任务受限于2D与3D之间的模糊和映射问题，判别式回归方法需大量多样数据，且缺乏物理推理能力。扩散模型虽展现出丰富世界先验，但直接用作确定性几何推理存在限制。

Method: Lotus-2包括两阶段：第一阶段用单步确定性预测器结合局部连续性模块（LCM）获得无网格伪像的全局结构；第二阶段在上一阶段输出的流形内，利用约束多步整流流动细化几何细节，实现无噪声细化。

Result: 仅用5.9万训练样本（不足主流数据集的1%），Lotus-2在单目深度估计任务取得了新的最优性能，并在表面法线预测上表现极具竞争力。

Conclusion: Lotus-2展示了将预训练生成先验确定性利用，突破了传统判别式与生成式模型局限，为高质量几何推理提供了新框架。

Abstract: Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.

</details>


### [136] [TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models](https://arxiv.org/abs/2512.01048)
*Maya Varma,Jean-Benoit Delbrouck,Sophie Ostmeier,Akshay Chaudhari,Curtis Langlotz*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TRoVe的新方法，自动发现视觉-语言模型（VLMs）在处理时序任务中学到的导致错误的静态特征偏置，并量化偏置检测的准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VLMs在时序视觉理解任务上虽表现优秀，但常常依赖图像中的静态特征（如背景或物体属性）来做决策，而非真正利用动态变化，这会导致模型预测出现系统性错误。因此，识别和分析模型的静态特征偏置，对于实际应用前的模型评估和优化至关重要。

Method: 作者提出TRoVe方法，针对已训练好的VLM和相关下游分类任务的标注数据集，自动提取候选静态特征，并通过两重评分机制量化每个特征导致预测错误的影响及模型依赖度。为评估TRoVe效果，作者构建了含101个VLM及静态特征偏置标注的基准框架，并与主流方法对比。

Result: TRoVe能准确识别VLM所学到的易导致错误的静态特征偏置，在精度上比最近的强基线高28.6%。进一步，TRoVe在7个开源VLM和2个时序理解任务上揭示了此前未被注意到的静态特征偏置。

Conclusion: TRoVe能够作为实用工具辅助分析和理解VLM模型的错误来源，并有助于通过调整应对这些偏置来提升实际测试中的模型性能。

Abstract: Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.

</details>


### [137] [Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction](https://arxiv.org/abs/2512.01059)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CV

TL;DR: 本论文研究了在视觉Transformer（ViT）中，减少MLP模块参数对模型表现的影响。提出了两种参数高效变体：GroupedMLP和ShallowMLP，它们在减少约32.7%参数量的情况下，反而提升了ImageNet-1K任务的准确率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然理论和经验上增大模型规模通常会提升表现，但实际中模型性能并非总是随参数量单调提升。作者想探讨通过参数共享和宽度减小，ViT模型是否可以既减少参数又稳表现。

Method: 以ViT-B/16为基线，作者设计了两种MLP模块精简方法：1）GroupedMLP：在相邻block间共享MLP参数；2）ShallowMLP：将MLP的隐层宽度减半。两者都让MLP模块参数减少约32.7%。在ImageNet-1K上进行实验证明有效性。

Result: GroupedMLP模型在不增加计算量的情况下，top-1准确率为81.47%；ShallowMLP模型准确率81.25%，推理吞吐提升38%。这两个模型准确率都超过了基准ViT-B/16（81.05%）。训练过程也更稳定，峰值和最终准确率间差距显著缩小。

Conclusion: 在标准训练下，ViT-B/16在ImageNet-1K上出现过参数化现象，MLP部分参数量可以安全减少，甚至有助于提升准确率和训练稳定性。参数共享和宽度收缩是有效的结构归纳方法，参数分配在ViT模型设计中极为关键。

Abstract: Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.

</details>


### [138] [Accelerating Inference of Masked Image Generators via Reinforcement Learning](https://arxiv.org/abs/2512.01094)
*Pranav Subbaraman,Shufan Li,Siyan Zhao,Aditya Grover*

Main category: cs.CV

TL;DR: 本文提出了一种名为Speed-RL的方法，通过强化学习加速生成高质量图像的Masked Generative Models（MGM），在保持图像质量的前提下，实现了推理速度提升3倍。


<details>
  <summary>Details</summary>
Motivation: 当前MGM在生成高质量图像时需要较多采样步骤，导致推理速度慢。加速生成过程已成为亟需解决的问题，以提升实际应用效率。

Method: 与传统用蒸馏方法以分布匹配为目标不同，作者首次将加速问题视为强化学习问题，将图像质量和生成速度综合为奖励函数，并用强化学习对模型进行微调优化。

Result: 实验表明，该方法在保持可比图像质量的基础上，使基础模型的推理速度提升3倍。

Conclusion: Speed-RL方法有效地加快了MGMs的生成过程，并保持高质量输出，为高效图像生成模型的研究和应用提供了新思路。

Abstract: Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.

</details>


### [139] [CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions](https://arxiv.org/abs/2512.01095)
*Simon Kohaut,Daniel Ochs,Shun Zhang,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.CV

TL;DR: 本文提出了一个名为CycliST的新基准数据集，用于评估视频语言模型理解和推理周期性状态变化的能力，并发现现有模型在周期性动态理解方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型（VLM）在处理依赖于时间和周期性变化的场景时仍表现有限，缺乏对周期性运动和属性变化的深层理解。因此，作者希望通过构建专门的数据集，推动这一领域的发展。

Method: 作者构建了CycliST数据集，通过合成生成具有丰富结构、呈现周期性模式的场景视频，并开发分层评价体系，逐步提升测试难度（包括增加周期性物体数量、场景杂乱度以及照明条件变化）。作者还利用这一框架，系统评测了多种最先进的VLM模型。

Result: 实验结果显示，当前VLM模型在识别并利用周期性模式和时序关系方面表现不佳，难以准确理解物体数量等定量信息。没有单一模型能在所有测试上表现领先，模型规模和架构与最终表现无明显相关性。

Conclusion: CycliST数据集为视频语言模型理解周期性与时间相关视觉模式提供了有针对性的挑战和评测手段，有助于推动模型在周期性视觉推理能力上的进步。

Abstract: We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.

</details>


### [140] [Learning Eigenstructures of Unstructured Data Manifolds](https://arxiv.org/abs/2512.01103)
*Roy Velich,Arkadi Piven,David Bensaïd,Daniel Cremers,Thomas Dagès,Ron Kimmel*

Main category: cs.CV

TL;DR: 提出了一种利用深度学习直接从无结构数据中学习光谱基的新框架，不再依赖传统算子选取和特征分解，适用于高维和无结构点云等数据。


<details>
  <summary>Details</summary>
Motivation: 传统几何处理需要选取和离散化微分算子（如拉普拉斯），并进行特征分解，这对于无结构数据和高维数据非常困难且计算量大。作者希望绕开这些步骤，构建数据驱动的、端到端学习的分析方法。

Method: 以最优近似理论为基础，用神经网络近似一个隐式算子的分解，通过在选定分布上的探测函数重建误差最小化来训练网络。方法无需已知算子、网格或流形维度假设，对无结构高维数据均适用。

Result: 在三维表面点云和高维图像流形上，方法学习到的光谱基与拉普拉斯算子的特征基类似，还能统一恢复采样密度和本征值，无需明确构造算子。

Conclusion: 该框架为几何处理提供了一个全新的端到端学习方式，适用于无结构、高维数据，摆脱了传统流程对算子选取与分解的依赖，具有很强的推广能力，为高维空间中的几何分析提供了新手段。

Abstract: We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.

</details>


### [141] [Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis](https://arxiv.org/abs/2512.01116)
*Yilan Zhang,Li Nanbo,Changchun Yang,Jürgen Schmidhuber,Xin Gao*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SlotSPE的新方法，实现了对癌症生存预测中病理切片和基因组数据的有效整合，显著提升了预测准确性和模型解释性。


<details>
  <summary>Details</summary>
Motivation: 当前在癌症生存预测领域，整合组织学图像与基因组数据虽然前景广阔，但现有方法难以高效捕捉关键的、影响预后的重要结构事件，原因在于输入数据维度高且复杂，且这些事件本身稀疏且难以标注。

Method: 作者提出了SlotSPE，该方法借鉴了factorial coding理念，利用slot attention机制，将各模态输入压缩为可以代表关键预后事件的小规模、互异的slot表示，并能引入生物学先验以增强相关性。通过这些slot对复杂的多模态相互作用建模，实现高效有效的信息整合。

Result: 在10个癌症数据集上的大量实验表明，SlotSPE在8个队列上优于现有方法，总体提升2.9%；在基因组数据缺失情况下依然表现稳健，且通过结构化事件分解，模型解释性显著提高。

Conclusion: SlotSPE为癌症多模态预后建模提供了一种结构清晰、有效可解释的新途径，对生存预测和临床应用具有重要推动作用。

Abstract: The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.

</details>


### [142] [OmniFD: A Unified Model for Versatile Face Forgery Detection](https://arxiv.org/abs/2512.01128)
*Haotian Liu,Haoyu Chen,Chenhui Pan,You Hu,Guoying Zhao,Xiaobai Li*

Main category: cs.CV

TL;DR: OmniFD 是一个统一的面部伪造检测框架，能够同时完成图像/视频分类、空间定位和时间定位，显著提高检测效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸伪造检测方法通常针对不同任务采用独立模型，导致计算冗余且未能利用任务间的相关性，亟需一种统一且高效的多任务检测方案。

Method: OmniFD 框架包含：1）基于 Swin Transformer 的共享编码器，提取统一的4D时空特征；2）跨任务交互模块，通过可学习查询与注意力机制动态建模任务间依赖；3）轻量级解码头，将特征转化为各伪造检测任务的预测结果。

Result: 在多项基准测试中，OmniFD 相较于任务特定模型取得更优性能，参数量减少63%，训练时间减少50%。通过多任务学习可有效迁移细粒度知识，如引入图像数据后，视频分类精度提升4.63%。

Conclusion: OmniFD 实现了图像、视频及四项任务的高效统一检测，在实际应用中表现出优良的可扩展性和泛化能力，是面部伪造检测领域一种实用且通用的解决方案。

Abstract: Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.

</details>


### [143] [Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network](https://arxiv.org/abs/2512.01145)
*Riyadh Mohammed Almushrafy*

Main category: cs.CV

TL;DR: 该论文提出了一种仅需微表情时序关键点（onset、apex、offset）弱标签的连续微表情强度估计方法。其核心思想是用三角先验从稀疏标签生成密集的伪强度轨迹，并基于ResNet18+GRU的轻量网络进行帧级强度预测，实验在SAMM与CASME II数据集取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 以往大多数微表情研究仅关注于分类不同的表情类别，而忽视了表情强度随时间的连续演变。受限于缺乏逐帧强度标注，现有的强度回归方法难以完全监督进行。因此，急需一种能够仅凭稀疏、弱时序标签实现表情强度连续估计的新方法。

Method: 作者提出了一个统一框架：首先利用onset、apex、offset这三个稀疏时序标签，通过简单的三角先验函数，生成每一帧的伪强度轨迹。然后，结合ResNet18特征编码器和双向GRU时序建模的轻量回归网络，实现图像序列到帧级强度的直接预测。该方法无需帧级人工标注，整体流程只需一次预处理与时序对齐。

Result: 在主流微表情数据集SAMM和CASME II上，所提方法与伪强度轨迹在时间上高度一致，其中Spearman相关系数分别达到0.9014和0.9116，Kendall相关系数达到0.7999和0.8168，均显著优于基线。消融实验显示，时序建模和伪标签策略是抓住表情动态变化的关键因素。

Conclusion: 该方法首次实现了仅凭稀疏时序标注对微表情连续强度进行统一、高效估计，为微表情强度分析带来可行的全新方向。

Abstract: Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical.
  We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline.
  Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements.
  To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.

</details>


### [144] [SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models](https://arxiv.org/abs/2512.01148)
*Hamza Tahboub,Weiyan Shi,Gang Hua,Huaizu Jiang*

Main category: cs.CV

TL;DR: 当前的视觉-语言预训练模型（VLM）在多个社会感知任务上表现不佳，出现负迁移。作者发现这种现象是由“社会降解”引起，即VLM在通用预训练过程中削弱了视觉编码器对社会信息的表达能力。提出了新的SocialFusion框架，显著提升了多社会任务表现，并呼吁未来VLM训练应增强社会感知能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模预训练的视觉-语言模型具备很强的泛化能力，但它们在同时执行多种社会感知任务（如识别互动、情绪等）时出现负迁移现象，性能下降。作者希望揭示产生此问题的原因，推动VLM在社会智能方向的应用。

Method: 作者首先通过线性探测（linear representation probing）和梯度冲突分析（gradient conflict analysis）双重视角，系统分析了VLM在社会信息表征方面的退化根源。随后，提出了一种新框架SocialFusion，将冻结的视觉编码器与语言模型之间建立最小连接，通过联合学习提升模型在五项社会任务上的整体表现。

Result: SocialFusion在五个社会感知任务中实现了正迁移，任务间产生良性协同，综合表现与针对单任务训练的最先进模型相当，同时远超现有VLM的表现。实验证明了原VLM预训练方式会导致社会信息表征能力下降。

Conclusion: 当前VLM的预训练方法不利于模型社会智能的习得。SocialFusion框架有效缓解了社会降解问题，提升了多任务社会感知能力，表明未来VLM应采取更注重社会感知能力的训练范式。

Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term "social degradation," whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.

</details>


### [145] [DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling](https://arxiv.org/abs/2512.01153)
*Han-Jin Lee,Han-Ju Lee,Jin-Seong Kim,Seok-Hwan Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散采样指导方法DPAC，通过最小化路径空间的KL散度，同时提升对抗目标达成率和图像质量，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性指导的扩散采样方法在增强目标类别生成能力的同时，常常导致图像质量下降。作者希望找到一种理论原理来解释并克服这种质量损失，实现目标达成和生成质量的平衡。

Method: 作者将扩散过程建模为随机最优控制问题，引入路径空间KL散度作为质量损失的度量，推导出能量最小化等价于最小化路径KL。由此出发，设计了DPAC方法，将对抗梯度投影到生成分数的切空间，从而减少分布偏移。进一步分析了离散求解器下的误差项，并验证了方法的稳健性。

Result: 理论证明DPAC方法能收紧与2-Wasserstein距离和FID相关的上界，显著降低质量损失，并在离散求解时具有更高阶的误差收敛性。实验证明在ImageNet-100上，DPAC在达到同等攻击成功率时，获得更低的FID和路径KL。

Conclusion: DPAC为对抗性可控扩散采样带来了显著的质量提升和理论保证，将能量-质量权衡与实际生成性能结合，推动了扩散生成模型的可控性和实际应用。

Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(Δt) leading error term in the Wasserstein distance, achieving an O(Δt^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.

</details>


### [146] [VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering](https://arxiv.org/abs/2512.01178)
*Zihua Liu,Hiroki Sakuma,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 本文提出了一种全新的弱监督框架VSRD++，无需3D标注，仅用2D监督实现单目3D目标检测，并大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前单目3D目标检测主要依赖大量3D标注，这些标注通常由LiDAR点云获得，标注代价高昂且费时。因此，研究如何在更弱监督下进行单目3D检测，降低对高强度3D标注的依赖，有重要现实意义。

Method: VSRD++框架分为两阶段：首先，多视角自动标注，通过神经场体积渲染技术，以带有实例感知的轮廓生成3D伪标签（包含静态和动态目标），目标表面用带有残差的SDF建模，动态物体还引入速度信息和置信度。然后，这些优化过的3D框体伪标签用于训练单目3D检测器。

Result: 在KITTI-360数据集上，VSRD++在静态和动态场景下均明显优于现有的弱监督单目3D检测方法。

Conclusion: VSRD++能够在无需人工3D标注的情况下，显著提升单目3D目标检测性能，为弱监督3D检测提供了新的思路和高效实现方案。

Abstract: Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus

</details>


### [147] [TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image](https://arxiv.org/abs/2512.01204)
*Ziqian Wang,Yonghao He,Licheng Yang,Wei Zou,Hongxuan Ma,Liu Liu,Wei Sui,Yuxin Guo,Hu Su*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练免疫、全自动3D桌面场景生成框架TabletopGen，大幅提升了场景的真实感、布局准确性和物理可交互性。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法主要针对大场景，难以表达桌面这种高密度和复杂空间关系的场景，限制了机器人等具身智能的研究与应用。

Method: TabletopGen以一张参考图片（可来自文本到图片模型）为输入，进行实例分割和补全，将每个对象重建为3D模型，统一坐标系后，通过两阶段方法（微分旋转优化器和顶视空间对齐机制）完成姿态和尺度估计，最终拼装成无碰撞的、物理可交互的桌面场景。

Result: 大量实验和用户研究表明，TabletopGen在视觉逼真度、场景布局准确性和物理可行性等方面显著优于现有方法，能生成具有丰富风格和空间多样性的桌面场景。

Conclusion: TabletopGen为具身智能和机器人等领域提供了高保真、物理互动的3D桌面场景生成新工具，有望推动相关方向研究进展。

Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.

</details>


### [148] [Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations](https://arxiv.org/abs/2512.01213)
*Yangbangyan Jiang,Qianqian Xu,Huiyang Shao,Zhiyong Yang,Shilong Bao,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了两种简洁实例级最小最大重构方法，有效地优化部分AUC（PAUC），提升了计算效率与泛化性能，用于解决类别不均衡和决策约束下的实际问题。


<details>
  <summary>Details</summary>
Motivation: 部分AUC（PAUC）常用于类别不均衡和有决策约束的实际场景，但现有求解方法存在近似误差不可控或可扩展性差的问题，影响其实用价值。

Method: 作者提出两种实例级最小最大重构形式：一种近似误差随样本增大可忽略，另一种对期望无偏但变量更多。具体做法包括将原复杂选样过程转化为等价实例级问题，再通过阈值学习和不同平滑技术简化优化，并采用高效求解器使迭代复杂度与样本线性相关。还推导了泛化性能界，展示TPR/FPR约束对泛化的影响。

Result: 所提方法可达迭代线性复杂度及O(ε^{-1/3})收敛速率，并首次给出了PAUC近似优化的精确泛化界限。

Conclusion: 提出的新方法在多个基准数据集上展现出优异性能，为PAUC优化提供了更高效、理论可控的实用工具。

Abstract: As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(ε^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $α$/$β$ on the generalization and exhibits a sharp order of $\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.

</details>


### [149] [M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis](https://arxiv.org/abs/2512.01214)
*Hang Wu,Ke Sun,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: 本论文提出了M4-BLIP框架，结合了BLIP-2模型和脸部局部信息，通过特定的融合模块提升多模态媒体操控检测准确性，并可与大语言模型结合以增强可解释性。实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态媒体操控检测方法往往忽视了局部信息，尤其是面部区域经常成为被操控目标，这导致检测结果不准确。因此，研究者希望通过更好地利用局部信息提升检测性能。

Method: 提出了M4-BLIP框架，以BLIP-2模型为特征提取基础，结合局部面部信息作为先验，通过专门设计的对齐与融合模块实现局部与全局特征的融合，并与大语言模型结合提升检测结果的可解释性。

Result: 在大量定量和可视化实验中，M4-BLIP展现出比最新方法更高的检测准确率。

Conclusion: M4-BLIP能够有效融合局部与全局特征，提升多模态媒体操控检测的准确性，并增强了检测结果的可解释性，对该领域具有推动作用。

Abstract: In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.

</details>


### [150] [S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/abs/2512.01223)
*Beining Xu,Siting Zhu,Zhao Jin,Junxian Li,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为S$^2$-MLLM的高效3D视觉基础定位框架，通过空间引导和结构增强模块，使多模态大模型具备更强的三维空间推理能力，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLM）主要侧重于2D视觉输入，对3D场景的空间结构理解能力有限，通过点云渲染等方法进行空间引导效率低且空间推理表现不足，亟需更高效、更强空间理解能力的3D视觉定位方案。

Method: 提出S$^2$-MLLM框架，通过隐式空间推理提升MLLM的三维空间推理能力。设计了空间引导策略，利用前馈3D重建过程获取三维结构性的认知；并设计了结构增强（SE）模块，采用视图内和视图间的注意力机制捕捉不同视角的依赖关系，并引入多层次位置编码，将视觉表征与空间位置信息和视点结合，实现对结构的准确建模。

Result: 在ScanRefer、Nr3D和Sr3D等主流3D视觉定位数据集上，S$^2$-MLLM在性能、泛化能力和效率方面均显著优于现有方法。

Conclusion: S$^2$-MLLM无需低效的显式点云渲染，即可提升MLLM的三维空间推理能力，实现高效、泛化性强且性能卓越的3D视觉基础定位，为三维场景下AI理解和机器人感知奠定了新基础。

Abstract: 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

</details>


### [151] [PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards](https://arxiv.org/abs/2512.01236)
*Shulei Wang,Longhui Wei,Xin He,Jianbo Ouyang,Hui Lu,Zhou Zhao,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种扩展个性化生成模型到多主体情境的方法，包含高质量多主体数据生成流程及强化学习提升一致性与文本可控性。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型在单一主体（如一个人、宠物等）任务中表现优秀，但难以在多主体场景中保持主体一致性和严格遵循文本指令，限制了应用空间。主要原因在于缺乏高质量多主体数据和有效的后训练策略。

Method: (1) 设计了一种可扩展的数据生成流程，利用单主体强模型自动合成多主体高质量训练数据；(2) 利用这些数据预训练模型，使其学习多图像、多主体合成能力；(3) 在强化学习阶段引入成对主体一致性奖励和通用奖励，提升主体间一致性和对文本的可控性；(4) 构建了三大维度、七大子集的多主体新基准评测。

Result: 大量实验表明，所提方法显著提升了多主体个性化图像生成在主体一致性与文本可控性方面的性能，相对现有方法取得了更优结果。

Conclusion: 该工作有效解决了多主体场景下主体一致性与文本控制的挑战，有望推动个性化生成模型的实用化和更丰富应用场景的发展。

Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR

</details>


### [152] [TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition](https://arxiv.org/abs/2512.01248)
*Junyuan Zhang,Bin Wang,Qintong Zhang,Fan Wu,Zichen Wen,Jialin Lu,Junjie Shan,Ziqi Zhao,Shuya Yang,Ziling Wang,Ziyang Miao,Huaping Zhong,Yuhang Zang,Xiaoyi Dong,Ka-Ho Chow,Conghui He*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督表格识别方法TRivia，可使预训练视觉语言模型在无需标注数据的情况下直接从表格图片中学习，并发布了领先的开源模型TRivia-3B。


<details>
  <summary>Details</summary>
Motivation: 当前表格识别任务主要依赖于大量标注数据进行监督学习，而取得领先性能的专有模型通常不可公开，开源模型因受限于标注资源表现落后。由于获取大规模标注数据成本高昂且受隐私限制，研究者亟需无需人工标注即可提升开源模型表格识别能力的新方法。

Method: 作者提出TRivia，一种基于自监督学习的微调方法。该方法利用Group Relative Policy Optimization从无标注表格图片中自动筛选最具学习价值的样本，并通过基于问答的奖励机制实现无需人工标注。具体做法是由注意力引导模块为每张表格图片生成多样化问题，根据模型对识别结果的解读及回答正确性为模型提供优化反馈，实现一个闭环式自监督学习过程。

Result: 基于TRivia方法，作者训练并发布了TRivia-3B模型。实验表明，TRivia-3B在三项主流公开基准测试上超越包括Gemini 2.5 Pro和MinerU2.5在内的现有系统，成为开源领域的领先表格识别模型。

Conclusion: TRivia方法显著缩小了开源与专有表格识别模型的性能差距，有效推动了无监督、自主学习领域的发展，具备实际落地与推广价值，实现了无需标注数据的表格识别能力提升。

Abstract: Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia

</details>


### [153] [ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process](https://arxiv.org/abs/2512.01268)
*Jongwon Sohn,Juhyeon Moon,Hyunjoon Jung,Jaewook Nam*

Main category: cs.CV

TL;DR: 本论文提出一种基于计算机视觉的非接触粘度计，通过分析搅拌液面对背景图案的光学畸变，准确推断液体粘度，突破了传统粘度计对实验环境和操作的限制，具备自动化适用性。


<details>
  <summary>Details</summary>
Motivation: 传统粘度计量仪器操作繁琐，需接触样品，并要求受控实验环境，这与实际工业过程条件有较大差异，限制了在线监测和自动化实验室的实现。因此，亟需发展一种能在复杂现实环境下、无需接触即可准确测量粘度的新方法。

Method: 作者提出了一种基于计算机视觉的粘度计。该方法让液体搅拌表面连续变形，将固定的背景图案通过液体折射产生畸变，用计算机视觉算法分析图案畸变程度与粘度的关系。在多种光照条件下测试了回归和分类性能，并通过多图案策略增强鲁棒性，同时引入不确定性定量评估模型预测的可靠性。

Result: 该系统在多光照下实现了0.113 log m2 s^-1的平均绝对误差，粘度分类准确度最高可达81%。对于粘度值相近的类别，性能有所降低。引入多背景图案后，系统鲁棒性和区分类别能力得到提高，并可对预测结果给出置信度估计。

Conclusion: 本文方法实现了非接触、适于自动化场景的粘度测量，具有实际应用潜力，可作为传统粘度计的自动化替代方案。

Abstract: Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.

</details>


### [154] [nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis](https://arxiv.org/abs/2512.01273)
*Xin Li,Wenhui Zhu,Xuanzhao Dong,Hao Wang,Yujian Xiong,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 提出了一种改进的轻量级网络架构nnMobileNet++，结合了卷积和Transformer技术，实现了更准确高效的视网膜图像分析。


<details>
  <summary>Details</summary>
Motivation: 当前基于CNN（如nnMobileNet）的视网膜图像分析方法虽然高效，但难以捕捉图像中重要的长距离依赖和不规则血管、病灶特征，而这些特征对疾病精准诊疗至关重要。

Method: 提出nnMobileNet++混合架构：1）采用动态snake卷积进行边界感知特征提取；2）在特定阶段加入Transformer模块以捕捉全局上下文；3）利用视网膜图像预训练提升泛化能力。

Result: 在多个公开视网膜数据集上，通过分类实验和消融研究，nnMobileNet++表现出领先或高度竞争力的准确率，并保持较低的算力消耗。

Conclusion: nnMobileNet++是一种兼具高效和高准确率的视网膜图像分析框架，为相关领域提供了新的轻量级方法选择。

Abstract: Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.

</details>


### [155] [Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI](https://arxiv.org/abs/2512.01291)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过目标对比去遗忘（TCU）模块与可解释声呐框架（UESF）结合，减少声呐图像解读时模型对海底背景的依赖，从而提升了模型泛化能力、鲁棒性及解释性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型虽能高精度识别声呐图像目标，但往往过度依赖海底特征，导致泛化能力差。现实与合成数据集的背景差异加剧了这一问题。为提升模型实用性，有必要减少模型对无关背景的依赖。

Method: 提出了两大模块：（1）目标对比去遗忘（TCU）模块，通过扩展三元组损失，对模型输出进行约束，刻意削弱与海底背景相关的特征表达；（2）可解释声呐框架（UESF）利用LIME算法改进，实现可视化分析模型‘遗忘’了哪些特征，更准确地归因于模型调整中的关键部分。

Result: 在多个真实与合成声呐数据集上进行大量实验，验证了方法能有效减少模型对背景的依赖，提升模型的泛化性、鲁棒性和可解释性。

Conclusion: 结合TCU与UESF的新框架在提升声呐图像分析模型性能的同时，还能显著增强模型的可解释性与实用价值，有助于应用于更复杂且环境变化更大的场景。

Abstract: Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.

</details>


### [156] [Diffusion Model in Latent Space for Medical Image Segmentation Task](https://arxiv.org/abs/2512.01292)
*Huynh Trinh Ngoc,Toan Nguyen Hai,Ba Luong Son,Long Tran Quoc*

Main category: cs.CV

TL;DR: 提出了一种高效的医学图像分割方法MedSegLatDiff，结合VAE和潜变量扩散模型，可生成多样且高质量的分割结果，并在多个医学数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像分割只输出单一分割结果，难以体现分割的不确定性，而现有生成模型虽然可以输出多个合理假设，但计算成本高。因此需要一种高效且能表达不确定性的分割方案。

Method: MedSegLatDiff结合了变分自编码器（VAE）和潜变量扩散模型：VAE将医学影像编码至低维潜在空间，降低噪声并加速训练；扩散模型在该紧凑空间中实现生成。VAE分割重建路径采用加权交叉熵损失，提升对小结构如微小结节的捕捉能力。

Result: 在ISIC-2018、CVC-Clinic和LIDC-IDRI三个公开医学影像分割数据集上，MedSegLatDiff实现了与当前最优方法相当或更优的Dice和IoU分数，同时能生成多样化分割假设及置信图。

Conclusion: MedSegLatDiff能输出多种分割结果和不确定性信息，兼具高效性与优秀性能，较传统单一输出方法具有更好可解释性和临床应用前景。

Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.

</details>


### [157] [EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly](https://arxiv.org/abs/2512.01296)
*Xiaokun Pan,Zhenzhe Li,Zhichao Ye,Hongjia Zhai,Guofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EGG-Fusion的实时3D重建系统，结合了鲁棒的稀疏到稠密摄像机跟踪和几何感知的高斯surfels映射，通过信息滤波方法增强对传感器噪声的鲁棒性，实现了高精度、高效率的三维重建，在基准数据集上精度和速度均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微渲染的SLAM系统在实时性能和对传感器噪声的鲁棒性方面存在不足，几何重建精度有限，实际应用受限。需要一种既能保证高速处理又能提升对噪声鲁棒性和几何精度的新方法。

Method: 提出EGG-Fusion系统，核心包括：1) 鲁棒的稀疏到稠密摄像机跟踪模块；2) 基于几何的高斯surfels映射模块；3) 信息滤波融合方法，显式建模传感器噪声。系统采用可微高斯surfels映射，实现多视角一致表面建模和高效参数优化，保障实时性和精度。

Result: EGG-Fusion在Replica和ScanNet++等标准数据集上实现0.6cm的表面重建误差，比当前SOTA的GS类方法精度提升超过20%；同时系统可实时运行，处理帧率达24FPS。

Conclusion: EGG-Fusion实现了目前精度最高的基于可微渲染的实时三维重建，显著提升了对传感器噪声的鲁棒性和实时性，具有良好的工程与应用前景。

Abstract: Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/

</details>


### [158] [TBT-Former: Learning Temporal Boundary Distributions for Action Localization](https://arxiv.org/abs/2512.01298)
*Thisara Rathnayaka,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了TBT-Former，一种针对视频动作时序定位任务的新型Transformer架构，能更精确地处理动作模糊边界与多尺度特征融合，在多个权威数据集上取得了新的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前领先的单阶段、无锚点模型如ActionFormer虽然效果强大，但在处理复杂、边界模糊的动作分割以及多尺度上下文特征融合方面，仍存在精度和灵活性不足的问题。

Method: TBT-Former提出三大创新：（1）提升Transformer主干网络容量；（2）引入横向连接的跨尺度FPN结构实现更丰富多尺度特征融合；（3）借鉴广义Focal Loss思想，设计概率分布式的边界回归头，从不确定性分布角度建模模糊时序边界。

Result: TBT-Former在THUMOS14、EPIC-Kitchens 100等权威数据集上，都超过前人基线方法，并在ActivityNet-1.3上保持了竞争力。

Conclusion: TBT-Former提升了动作时序定位的精度与鲁棒性，特别是在处理模糊边界与复杂时序上下文方面，推动Transformer架构在视频理解领域的新表现。

Abstract: Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding

</details>


### [159] [DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy](https://arxiv.org/abs/2512.01302)
*Jaewoo Song,Jooyoung Choi,Kanghyun Baek,Sangyub Lee,Daemin Park,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出了DCText，一种无需额外训练的分而治之视觉文本生成方法，有效提升长文本和多文本的图像生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在单文本上表现优秀，但面对长文本或多文本场景时，由于全局注意力稀释，文本渲染质量下降，因此需要改进以提升多文本和长文本的生成精度。

Method: DCText采用分而治之策略，首先分解输入文本并将目标文本分配到特定图像区域。利用多模态扩散变换器对短文本的可靠生成能力，通过两种序贯应用的注意力掩码（Text-Focus与Context-Expansion）在去噪过程中分别聚焦文本与图像整体。同时引入Localized Noise Initialization来提升文本准确度和区域对齐性，且无额外计算开销。

Result: 在单句与多句基准测试上，DCText在文本准确度上取得最佳，同时图像质量无损，并且生成延迟最低。

Conclusion: DCText显著提升了多文本或长文本的图像生成可控性和效率，无需额外训练即可大幅优化文本渲染，适合实际应用场景。

Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.

</details>


### [160] [Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians](https://arxiv.org/abs/2512.01306)
*Hongru Yan,Xiang Zhang,Zeyuan Chen,Fangyin Wei,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于3D高斯表面的空气动力学模拟方法，能高效、真实地还原自然界物体（如树枝、旗帜、船只）的运动，并兼顾渲染和动力学计算。


<details>
  <summary>Details</summary>
Motivation: 为了在视觉和图形领域实现更真实的自然运动，目前主流的基于网格或粒子的空气动力模拟方法存在计算复杂、效率低或表面表达有限等问题，因此需要一种新的高效且可扩展的模拟框架。

Method: 本文提出了Gaussian Swaying框架，该方法用3D高斯连续建模物体表面，通过高斯patches同时实现力的计算（用于动力模拟）和表面法线的获得（用于渲染），避免了繁琐的网格化和离散粒子处理，仅用同一表面表达即可统一动力学模拟和渲染。

Result: 在合成数据和真实世界数据集上，Gaussian Swaying在多项指标上表现出色，达到了当前最优的性能和效率。

Conclusion: Gaussian Swaying为场景空气动力学模拟提供了一种高效、可扩展且真实的解决方案，具有良好的模拟与渲染统一效果。

Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.

</details>


### [161] [Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction](https://arxiv.org/abs/2512.01310)
*Yanteng Zhang,Songheng Li,Zeyu Shen,Qizhen Lan,Lipei Zhang,Yang Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 本文系统性探讨了大规模脑影像数据中数据质量对模型预训练及下游任务表现的影响。结果显示数据质量显著影响模型性能，强调了专业领域内数据整理与筛选的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前脑影像数据集在规模上具备发展基础模型的潜力，但与自然图像数据不同，脑影像存在质量参差不齐的问题。作者关注：低质量或有噪声的数据究竟对于模型预训练是有益还是有害？

Method: 作者分别采用不同质量水平的数据进行预训练，并在外部数据集上通过微调实现脑龄预测。系统对比各质量水平数据对预训练效果的影响。

Result: 不同数据质量下，模型在下游脑龄预测任务上表现有明显差异。高质量数据有助于提升模型泛化能力，而低质量甚至有噪声的数据可能带来负面影响。

Conclusion: 作者认为，尽管大数据量重要，但脑影像领域需兼顾严格的数据筛选与质量控制，仅依赖大而全的数据集无法保证模型泛化与临床信任度。呼吁借鉴计算机视觉经验的同时，加强针对临床影像的领域自适应数据管理。

Abstract: Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.

</details>


### [162] [IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval](https://arxiv.org/abs/2512.01312)
*Ning Han,Yawen Zeng,Shaohua Long,Chengqing Li,Sijie Yang,Dun Tan,Jianfeng Dong,Jingjing Chen*

Main category: cs.CV

TL;DR: 本文提出了互动式视频库检索（IVCR）任务，并发布了IVCR-200K数据集与多模态大模型检索框架，实现多轮对话、支持个性化和动态需求的互动视频检索。


<details>
  <summary>Details</summary>
Motivation: 当前视频检索方法多为单向检索，无法满足绝大多数用户个性化和动态的搜索需求，缺乏与用户的深入互动。

Method: 1. 提出IVCR任务，允许用户与检索系统进行多轮、对话式互动。
2. 构建高质量、双语、多轮、对话、抽象语义的IVCR-200K数据集。
3. 基于多模态大模型设计框架，实现多种交互方式，并提供更可解释的解决方案。

Result: 实验表明所提出的数据集和框架均有效提升了视频检索的互动性和个性化能力。

Conclusion: 互动式视频检索为真实世界应用提供更强支持，IVCR-200K和多模态框架为后续研究和实际应用奠定了坚实基础。

Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.

</details>


### [163] [TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance](https://arxiv.org/abs/2512.01314)
*Pei Yang,Yepeng Liu,Kelly Peng,Yuan Gao,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出了TokenPure，一个基于扩散Transformer的新型水印去除框架，能高效且一致地移除数字水印，并在去除水印后保持图像细节和结构，同时明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数字经济时代，数字水印对于证明虚拟内容（如AI生成内容）的所有权至关重要，但随着内容的易复制性增强，设计能抗攻击的鲁棒水印和高质量去除水印的算法变得更加重要。

Method: TokenPure通过将带水印图像分解为视觉token（纹理信息）和结构token（几何信息）两部分，实现基于token的有条件重建。扩散过程利用这两个token集合作为条件，绕过原有噪声，视图像内容进行一致的高保真重构。这样既能有效去除水印，又能保持图像内容的一致性和细节。

Result: TokenPure在去除水印和图像重建的一致性方面取得了最先进的效果，不论在感知质量还是内容一致性上均显著优于现有水印去除基线方法。

Conclusion: TokenPure为鲁棒、高质量的水印去除提供了优越解决方案，可广泛应用于虚拟内容的所有权管理等数字经济场景。

Abstract: In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.

</details>


### [164] [FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection](https://arxiv.org/abs/2512.01315)
*Ashish Vashist,Qiranul Saadiyean,Suresh Sundaram,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 本文提出了首个专用于飞机油箱内FOD（异物碎片）检测的真实与合成混合图像数据集FOD-S2R，并验证合成数据能有效提升检测模型在真实条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 飞机油箱内的异物会导致燃油污染、系统故障和维护成本增加，但目前针对油箱内复杂封闭环境的FOD检测数据集极为稀缺，限制了自动化检测技术发展。

Method: 研究者构建了FOD-S2R数据集，包括3114张真实环境下拍摄的高清图片及3137张基于虚幻引擎生成的合成图像，涵盖不同视场、物体距离、光照、颜色和尺寸。利用该数据集，对比评测多种现有目标检测模型，并考察合成数据对模型检测性能及泛化能力的作用。

Result: 实验证明加入合成数据能有效提升基于真实图像训练的FOD检测模型精度，并加强模型对现实环境的泛化能力，实现“模拟-现实”领域差距（Sim2Real Gap）的缩小。

Conclusion: FOD-S2R数据集为飞机油箱FOD自动检测提供了基础，合成数据助力提升模型性能，推动航空维修智能化发展。

Abstract: Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.

</details>


### [165] [Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications](https://arxiv.org/abs/2512.01319)
*Feiyang Xiao,Yichi Zhang,Xigui Li,Yuanye Zhou,Chen Jiang,Xin Guo,Limei Han,Yuxin Li,Fengping Zhu,Yuan Cheng*

Main category: cs.CV

TL;DR: 本文提出了IAVS数据集，这是首个针对颅内动脉瘤及血管段细致分割的多中心3D MRA数据集，并结合血流动力学分析数据，弥补现有分割数据集在CFD应用方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有颅内动脉瘤分割方法多以影像指标为主，缺乏对后续血流动力学（CFD）分析有效性的考量，且缺乏包含CFD相关信息和完整拓扑结构的高质量公开数据集。

Method: 构建并公开了包含641例3D MRA影像和587个动脉瘤及血管分割注释的IAVS数据集，配套详细CFD分析结果。设计了两阶段评测基准（动脉瘤定位及精细分割），并提出了一个简单有效的两阶段分割框架作为基线方法。建立了标准化的CFD可用性评估体系，实现分割掩码到CFD模型的自动转换与评价。

Result: IAVS数据集和方法能更好地支持有临床意义的分割算法开发和评估，通过标准化流程，可以系统考察分割结果在CFD分析的适用性。所有数据、代码和模型均会开源。

Conclusion: IAVS数据集为CFD相关颅内动脉瘤分割任务提供了高质量数据和完善的评测体系，为今后相关研究和临床应用奠定基础。

Abstract: The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.

</details>


### [166] [Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI](https://arxiv.org/abs/2512.01333)
*A S M Ahsanul Sarkar Akib,Raduana Khawla,Abdul Hasib*

Main category: cs.CV

TL;DR: 本论文提出了一种结合集成学习与可解释性人工智能（XAI）技术的中风风险预测模型，准确率高达99.09%，并能解释三个关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要健康问题，导致高死亡率和永久残疾，因此需要早期风险评估以便及时干预和预防。

Method: 作者采用特征工程和数据预处理（包括随机过采样解决类别不平衡），比较10种机器学习模型，并通过5折交叉验证优化集成模型（随机森林+ExtraTrees+XGBoost）。此外，利用LIME模型解释技术，提升模型可解释性。

Result: 优化后的集成模型在中风预测数据集上获得了99.09%的高准确率，并通过LIME方法确定了年龄、高血压和血糖是影响中风最重要的三个变量。

Conclusion: 集成学习结合可解释AI不仅可实现高准确率的中风风险预测，还能帮助理解影响因素，从而有潜力提升个体化临床决策和疾病预防策略。

Abstract: Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.

</details>


### [167] [AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation](https://arxiv.org/abs/2512.01334)
*Yexin Liu,Wen-Jie Shu,Zile Huang,Haoze Zheng,Yueze Wang,Manyuan Zhang,Ser-Nam Lim,Harry Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法AlignVid，通过调节注意力机制提升文本引导下图像生成视频任务中的语义遵从性，并用新数据集OmitI2V进行评测，实验结果显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的图像生成视频方法在物体添加、删除或修改时对文本细节遵从性表现不佳，容易忽略细粒度语义（称为semantic negligence）。作者发现高斯模糊输入图像能提升语义遵从性，分析注意力机制后，希望采取更有效的低干预手段提升该能力。

Method: 提出AlignVid，包括两部分：(1)注意力缩放调控（ASM），对transformer的Q或K进行轻量化缩放调整注意力；(2)引导调度（GS），在不同transformer层与去噪步骤有选择性地应用ASM，以权衡画质和语义。此外，提出OmitI2V数据集，用于评估不同语义变动下的模型表现。

Result: 丰富实验表明，AlignVid在多个涉及物体添加、删除和修改的场景下明显提升了语义遵从性，且对生成画质影响有限。与现有TI2V模型相比取得更佳的语义一致性。

Conclusion: AlignVid作为一种训练无关的轻量框架，能有效提升TI2V任务中的文本语义遵从性而不显著影响美观度，为相关生成模型的实用性和细节控制带来了进步。

Abstract: Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.

</details>


### [168] [EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans](https://arxiv.org/abs/2512.01340)
*Yingjie Zhou,Xilei Zhu,Siyu Ren,Ziyi Zhao,Ziwen Wang,Farong Wen,Yu Zhou,Jiezhang Cao,Xiongkuo Min,Fengjiao Chen,Xiaoyu Li,Xuezhi Cao,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 本文提出了THQA-MT数据集和EvalTalker评测框架，解决多主体语音驱动的虚拟人动画质量评价难题。


<details>
  <summary>Details</summary>
Motivation: 现有“Talking Human”技术多为单一主体驱动，扩展到“多主体”会带来更丰富的互动体验，但现有方案质量不佳，缺乏有效的质量评价方法及标注数据集。

Method: 作者构建了首个大规模多主体虚拟人（Multi-Talker）生成视频质量评测数据集THQA-MT，通过主观实验分析了不同系统生成的失真类型。又提出EvalTalker自动评价框架，敏感感知动画全局质量、人特征、一致性并引入Qwen-Sync感知多模态同步性。

Result: 实验表明，EvalTalker与主观评价具有较高相关性，并优于现有评价基线。数据集和方法为多主体虚拟人生成与评价提供了新基础。

Conclusion: THQA-MT数据集和EvalTalker评测系统有效提升了多主体虚拟人生成领域的质量评价能力，为后续相关技术研究提供了坚实支撑。

Abstract: Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.

</details>


### [169] [InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision](https://arxiv.org/abs/2512.01342)
*Chenting Wang,Yuhan Zhu,Yicheng Xu,Jiange Yang,Ziang Yan,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的大规模视频表征自监督预训练方法InternVideo-Next，结合物理世界知识与语义学习，提升了无标签视频表征的效果，取得了领先的基准测试成绩。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频-文本预训练方法依赖合成或噪声文本，难以涵盖隐性世界知识（如运动、空间结构、物理线索），而纯掩码视频建模虽利用时空结构，但泛化效果不如文本监督方法，主要受限于架构不足。

Method: 作者提出Encoder-Predictor-Decoder（EPD）架构，将世界建模解耦为三个模块。创新性地采用条件扩散解码器和两阶段预训练流程：在第一阶段，引入条件扩散解码器和语义先验，兼顾细节和高层语义，改善像素级还原与语义抽象的矛盾；第二阶段，冻结第一阶段目标，对其预测，减少捷径学习。

Result: 仅用公共的无标签视频预训练，InternVideo-Next在多个视频任务和基准上均达到SOTA（state-of-the-art）性能，验证了方法的有效性和泛化能力。

Conclusion: InternVideo-Next同时获得了像素保真与语义一致性，突破了原有掩码视频建模的限制，为可扩展的、通用的视频表征学习提供了新路径。

Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.

</details>


### [170] [Handwritten Text Recognition for Low Resource Languages](https://arxiv.org/abs/2512.01348)
*Sayantan Dey,Alireza Alaei,Partha Pratim Roy*

Main category: cs.CV

TL;DR: 本文提出了一个无分割步骤的段落级印地语和乌尔都语手写文本识别系统——BharatOCR，采用ViT-Transformer Decoder-LM结构，在多个数据集上取得了领先的字符识别率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 印地语、乌尔都语等低资源语言的手写文本识别依然难以取得高准确率，主要因为这些语言缺乏丰富的语言资源和大规模训练数据，尤其在段落级别识别场景下更为困难，因此需要设计更有效的方法提升OCR性能。

Method: 提出了一种无分割（segmentation-free）段落级识别架构，采用ViT作为视觉特征提取器，Transformer解码器生成文本序列，并利用RoBERTa语言模型优化结果。模型融合了DeiT用于视觉特征建模，并将其与经优化的RoBERTa语言模型结合，最终通过Transformer Decoder实现视觉到文本的转换，并采用隐式行分割对段落图片逐行处理。

Result: 在自构建的Parimal Urdu、Parimal Hindi以及公开的NUST-UHWR、PUCIT-OUHL数据集上进行了评估，在NUST-UHWR、PUCIT-OUHL、Parimal-Urdu上字符识别率分别达到96.24%、92.05%、94.80%，在Hindi数据集上识别率为80.64%，均优于现有主要方法。

Conclusion: BharatOCR架构在低资源语言（如印地语、乌尔都语）的手写段落文本识别上显著优于当前方法，特别适用于缺少大规模标注数据的场景，对相关语言的OCR技术发展具有重要推动作用。

Abstract: Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.

</details>


### [171] [OpenBox: Annotate Any Bounding Boxes in 3D](https://arxiv.org/abs/2512.01352)
*In-Jae Lee,Mungyeom Kim,Kwonyoung Ryu,Pierre Musacchio,Jaesik Park*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自动化3D物体检测标注方法OpenBox，能在无需自训练的情况下生成高质量标注，并提升效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测方法需要大量标注成本，且经常忽视物体的物理状态，同时自训练方法又带来高昂的计算开销与低标注质量。因此需要更高效、更智能的无监督和开放词表的3D标注方法。

Method: OpenBox为两阶段自动标注流程：第一阶段利用2D视觉大模型处理后的图像与点云进行跨模态实例对齐；第二阶段根据实例的刚性与运动状态分类，并依据类别自适应生成边界框。这样可避免繁琐的自训练迭代。

Result: 在Waymo、Lyft Level 5和nuScenes等公开数据集上，OpenBox相比基线方法展现了更高的准确性和更高的效率。

Conclusion: OpenBox无需自训练即可实现高质量3D标注，显著降低计算和人工成本，在自动驾驶等实际应用中很有潜力。

Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.

</details>


### [172] [BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud](https://arxiv.org/abs/2512.01366)
*Yunzhe Li,Jiajun Yan,Yuzhou Wei,Kechen Liu,Yize Zhao,Chong Zhang,Hongzi Zhu,Li Lu,Shan Chang,Minyi Guo*

Main category: cs.CV

TL;DR: 本文提出了BlinkBud系统，通过单只耳机与手机协作，实时检测从身后接近的危险车辆等目标，提高行人与骑行者的道路安全。该系统利用耳机上的摄像头少量采集图像并结合智能跟踪算法，实现低功耗并高精度检测身后危险目标。


<details>
  <summary>Details</summary>
Motivation: 行人和骑行者在道路上容易因无法察觉从身后高速接近的车辆等危险目标遭遇事故，因此亟需一种能在不额外分心的情况下实时感知身后威胁的技术。

Method: BlinkBud利用耳机与手机联动，耳机以低频率采集后方图像。系统采用结合卡尔曼滤波的三维轨迹估计方法，并用强化学习优化图像采集时机，既提升功耗效率又保证跟踪准确性。同时通过推算头部俯仰和偏航角动态校正目标深度估计与相机坐标系，提高系统在用户自由活动时的可靠性。

Result: 实测显示，BlinkBud系统在耳机和手机上的平均功耗分别仅为29.8 mW和702.6 mW，且危险目标检测准确率高，误报率与漏报率分别为4.90%和1.47%。

Conclusion: BlinkBud系统无需额外设备，能高效低功耗地为步行者和骑行者实时感知身后危险目标，为道路交通安全提供了有力技术支持。

Abstract: Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.

</details>


### [173] [SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation](https://arxiv.org/abs/2512.01373)
*Sheng Liu,Tianyu Luan,Phani Nuney,Xuelu Feng,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型的3D形状真实感评估新方法，无需依赖真实参考模型，并构建了新数据集，在与人类感知对齐方面效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着3D内容应用的普及，对高真实感3D形状的需求快速增长。然而，现实应用中常无可用的真实参考模型，现有评估方法难以全面评价形状真实感，亟需新方法突破。

Method: 提出Shape-Realism Alignment Metric，利用大语言模型通过mesh编码将3D形状信息转换为语言token，再用专门设计的真实感解码器使模型输出与人类对真实感的感知对齐。同时，建立RealismGrading数据集，含16种生成算法产生的形状及人工真实感分数。

Result: 所提评价指标在k折交叉验证下表现良好，与人类真实感评估高度相关，且通用性强，优于传统方法。

Conclusion: 本研究提出的基于LLM的评估指标为3D真实感评价提供了新思路，可以有效提升评估效率与主观感知一致性，有望在实际内容生产中得到广泛应用。

Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.

</details>


### [174] [Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network](https://arxiv.org/abs/2512.01380)
*Tianyu Luan,Xuelu Feng,Zixin Zhu,Phani Nuney,Sheng Liu,Xuan Gong,David Doermann,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的针对带纹理3D模型的真实性评价方法TGE，能更准确反映人类的主观评价，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前用于3D模型真实性评价的指标（如Chamfer Distance等）与人类主观感受存在偏差。基于2D渲染图像的现有学习型指标受视角选择和结构覆盖度限制，还普遍只在合成失真上训练，无法很好对应真实失真，存在域间差距。迫切需要更贴合实际和人类感知的评价方法。

Method: 作者提出Textured Geometry Evaluation (TGE)方法，直接基于带纹理3D网格（无需渲染），联合利用模型的几何和颜色信息，衡量待测模型与参考模型的接近程度。为训练和评估这一指标，作者还构建了含真实失真的人工标注数据集。

Result: TGE在真实失真数据集上的实验结果优于基于渲染和仅几何的评价方法。

Conclusion: TGE方法能够更有效地对带纹理3D模型进行真实性评价，符合人类主观评价，适合应用于实际场景。

Abstract: Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.

</details>


### [175] [Reversible Inversion for Training-Free Exemplar-guided Image Editing](https://arxiv.org/abs/2512.01382)
*Yuke Li,Lianli Gao,Ji Zhang,Pengpeng Zeng,Lichuan Xiang,Hongkai Wen,Heng Tao Shen,Jingkuan Song*

Main category: cs.CV

TL;DR: 本文提出了一种高效的示例引导图像编辑方法ReInversion，无需大规模预训练，编辑效果优异且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有的示例引导图像编辑方法往往需要大规模预训练，计算成本高。无训练的倒置技术虽然灵活，但编辑质量和效率不佳，因此需要一种新型、兼具高效与高质量的编辑方案。

Method: 提出了可逆倒置（ReInversion），采用双阶段去噪：第一阶段以源图像为条件，第二阶段以参考图像为条件。并引入了掩码引导选择性去噪（MSD）策略，仅对目标区域进行编辑，保护背景结构一致性。

Result: 实验显示ReInversion方法在编辑质量和速度上均取得了领先，且消耗的计算资源更低，分别通过定性和定量对比得以证实。

Conclusion: ReInversion无需复杂训练即可实现高效且优质的示例引导图像编辑，兼具较低的计算成本和良好的编辑效果，可作为该领域的先进方法。

Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.

</details>


### [176] [PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications](https://arxiv.org/abs/2512.01383)
*Yunze Liu,Zifan Wang,Peiran Wu,Jiayang Ao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的4D骨干网络PointNet4D，能够在流式点云视频下实现实时处理，并在多项任务和多个数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前4D环境感知系统大多依赖算力消耗大的时空卷积或Transformer结构，不适合真实机器人或交互系统中的实时和资源受限需求，亟需一种轻量、高效兼具时序理解能力的新方法。

Method: 提出PointNet4D骨干网络，核心设计为结合Mamba高效状态空间建模和Transformer双向建模优点的混合式时序融合模块；同时，设计了一种帧级mask自回归预训练（4DMAP）策略，增强跨帧运动线索建模能力。

Result: 在7个数据集上的9项任务中系统性评测，PointNet4D在各类应用和挑战场景中均获得一致性的性能提升。并基于该网络构建了4D扩散策略与4D模仿学习两类机器人应用，在RoboTwin与HandoverSim基准测试取得显著进步。

Conclusion: PointNet4D兼具高效性与优越的时序理解泛化能力，有效支撑流式4D点云实时处理及各类机器人实际应用，为4D环境感知领域提供了新方向。

Abstract: Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.

</details>


### [177] [FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution](https://arxiv.org/abs/2512.01390)
*Seungho Choi,Jeahun Sung,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出FRAMER训练方案，通过频域分解和对比损失，提升扩散模型在真实图像超分辨中的高频细节重建能力，无需改变推理结构，在主流骨干上效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统GAN方法难以处理混合、未知退化的低分辨率图像，扩散模型虽在感知质量优越但高频细节重建不足，因此需要新方法改善高频信息还原能力，同时保持易于应用。

Method: FRAMER以不变更扩散模型主干或推理方式为前提，通过频域分解和分层特征蒸馏，在每一步去噪中用最终层特征监督中间层，分别施加低频IntraCL和高频InterCL对比损失，并通过FAW与FAM模块自适应权重和门控高低频蒸馏信号。

Result: FRAMER在U-Net和DiT等不同骨干（如Stable Diffusion 2、3）上，提升了超分任务的PSNR/SSIM和主观感知指标（LPIPS、NIQE、MANIQA、MUSIQ），消融实验证明核心机制（最终层教师与随机层负样本）的有效性。

Conclusion: FRAMER无需更改主结构即可有效提升扩散模型在真实图像超分辨中的细节恢复和感知质量，方法泛化性强，为相关任务提供了实用训练范式。

Abstract: Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.

</details>


### [178] [Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries](https://arxiv.org/abs/2512.01419)
*Tushar Pranav,Eshan Pandey,Austria Lyka Diane Bala,Aman Chadha,Indriyati Atmosukarto,Donny Soh Cheng Lock*

Main category: cs.CV

TL;DR: 该论文提出了RICE-VL基准和新的评测指标，以评估现有视觉-语言模型（VLM）对东南亚多国文化内容的理解，揭示了当前模型在文化多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs多以西方为中心，缺乏对东南亚等多文化背景下的适应性，导致其在多元文化语境任务中的表现不佳，需要一套新基准和评测方式来反映这种问题。

Method: 构建RICE-VL基准集，涵盖东盟11国、28000+人工标注的VQA样本和1000个视觉定位对，并设计SEA-LAVE指标专门评估文本准确性、文化契合度及国家识别能力。对六种VLM进行评测，分析其在文化多样性场景下的表现。

Result: 实验证明，当前主流VLMs在资源较少国家和抽象文化领域有显著性能不足；视觉定位任务显示模型在复杂场景中特定文化元素的识别和定位能力有限。

Conclusion: RICE-VL基准有效揭示了现有VLMs在文化多样性理解方面的短板，强调未来需开发更具包容性、更能够代表全球多元文化的视觉-语言模型。

Abstract: Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.

</details>


### [179] [MDiff4STR: Mask Diffusion Model for Scene Text Recognition](https://arxiv.org/abs/2512.01422)
*Yongkun Du,Miaomiao Zhao,Songlin Fan,Zhineng Chen,Caiyan Jia,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该论文首次将Mask Diffusion Models (MDMs)应用于场景文本识别（STR），提出了MDiff4STR，通过两项关键改进策略克服MDM在STR中的不足，实现了比最新自回归模型更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管MDMs在视觉-语言任务中因兼具效率与准确性而受到关注，但在场景文本识别任务上，原始MDM模型准确率跟不上自回归模型，优缺点不均衡，亟需专门设计解决方案。

Method: 作者分析了MDMs应用于STR时面临的两个核心问题：训练与推理的噪声差异，以及推理时过度自信的预测。为此，提出六种新的噪声策略对齐训练和推理行为，并引入了一种用于推理阶段的token替换噪声机制，帮助模型纠正过于确定但错误的预测。

Result: 在标准和复杂STR基准（含不规则、艺术化、遮挡、中文等）上广泛测试，MDiff4STR在所有场景下均优于主流STR模型，不仅准确率超过最新自回归方法，同时推理速度更快（仅需三步去噪）。

Conclusion: MDiff4STR通过创新性地优化MDM用于STR任务，有效突破以往的准确率瓶颈，实现了更好的性能和推理效率，为场景文本识别提供了新的高效解决方案。

Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.

</details>


### [180] [\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models](https://arxiv.org/abs/2512.01424)
*Xusen Hei,Jiali Chen,Jinyu Yang,Mengchen Zhao,Yi Cai*

Main category: cs.CV

TL;DR: 论文提出了ViRectify基准，用于系统性评估多模态大模型（MLLMs）在视频推理错误纠正方面的能力，构建了3万条以上的视频推理纠错数据，并提出了基于证据的纠错方法。实验发现即使是顶尖模型如GPT-5，纠错准确率也较低，框架显著提升了小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在复杂视频推理任务中经常出错，而现有基准未能系统化评估模型发现和纠正推理错误的能力，因此亟需开发新的评价标准和纠错框架来发现模型弱点并推动性能提升。

Method: 作者构建了ViRectify数据集，包含3万多条动态感知、科学推理和具身决策等多领域的视频推理错误样例。通过AI辅助和人工校验标注流程，要求模型分步找出错误并生成有证据支撑的推理理由。还提出基于证据驱动的纠错框架，引入分步错误轨迹与奖励建模，指导模型关注错误传播及关键时间片。

Result: 对16个主流MLLMs进行评测，发现ViRectify极具挑战性，即使GPT-5纠错准确率仅31.94%。所提纠错框架下，小型模型Qwen2.5-VL-7B在ViRectify上表现超过多款大模型（如72B参数型号），显示该方法有效。还发现不同模型在纠错中存在系统性差异。

Conclusion: ViRectify为评估与提升多模态大模型视频推理能力提供了新方向和工具，相关数据集也为后续的反思学习等研究奠定了坚实基础。

Abstract: As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.

</details>


### [181] [ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers](https://arxiv.org/abs/2512.01426)
*Yiyang Ma,Feng Zhou,Xuedan Yin,Pu Cao,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本论文提出ResDiT，一种无需重新训练即可实现高分辨率图像生成的方法，解决了以往Diffusion Transformer（DiT）高分辨率生成时空间结构崩溃和纹理失真问题。作者通过位置编码缩放和局部增强机制，有效提升了生成图像的结构和细节质量。


<details>
  <summary>Details</summary>
Motivation: 当前利用DiT进行高分辨率图像合成时，常出现空间布局破坏和细节丢失等问题，且以往解决方法依赖复杂、多阶段的工作流，效率低下。本文旨在理解和改善DiT本身在分辨率扩展时的内在机制，简化流程并提高生成效果。

Method: 作者首先分析高分辨率下位置编码失效的根源，并提出位置编码缩放技术以纠正位置信息，从而避免空间布局崩溃。此外，通过局部注意力增强和补丁级融合模块加强细节表现，并采用高斯加权拼接减少网格伪影，实现高质量无缝拼接。整个方法不依赖额外训练，为无训练扩展分辨率提供新思路。

Result: ResDiT方法在多项评测中显示出优于现有方法的高保真度和高分辨率图像合成能力，并成功适配了空间控制等下游任务，兼具灵活性与实用性。

Conclusion: ResDiT为DiT高分辨率生成提供了一种简洁、高效、不需重新训练的解决方案，有效解决了空间布局和纹理问题，为相关应用和后续研究奠定基础。

Abstract: Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.

</details>


### [182] [Language-Guided Open-World Anomaly Segmentation](https://arxiv.org/abs/2512.01427)
*Klara Reichard,Nikolas Brasch,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Clipomaly的开创性方法，实现了自动驾驶场景下对已知和未知对象的分割，并能对未知区域进行语义命名。该方法无须特定的异常训练数据，动态扩充词汇表，兼顾了开集分割与异常区域识别命名，无需重新训练。实验结果显示在相关基准上达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有开集和异常分割方法无法对未知区域赋予有语义的标签，且识别与学习未知类别的表达具有挑战。此外，现有开放词汇分割依赖于固定推理词表，不适用于类别未受限的异常分割场景，因此需要新的方法提升自动驾驶系统对环境中未知物体的感知和解释能力。

Method: 提出了Clipomaly，一种基于CLIP的开源世界与异常分割方法。该方法为零样本方案，无需异常类训练数据，利用CLIP的图像-文本联合特征空间识别并分割未知对象，并能为其动态赋予人类可解释的标签。与传统开放词汇方法不同，Clipomaly可在推理时动态扩充词汇表，无需重新训练，提高了灵活性。

Result: Clipomaly在多个主流异常分割数据集上实现了最新的最优性能，同时能够对异常区域进行命名，提升了解释能力。

Conclusion: Clipomaly方法为自动驾驶开集世界和异常分割带来了可扩展性、可解释性和实用性的新突破，能够在无需特定训练数据的情况下灵活应对环境中的未知对象识别和解释问题，具有实际部署应用潜力。

Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.

</details>


### [183] [FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation](https://arxiv.org/abs/2512.01444)
*Jian Shu,Nanjie Yao,Gangjian Zhang,Junlong Ren,Yu Feng,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的学习型框架，实现高效且高质量的3D人体动画，解决了模板构建与变形中的多种弊端，效果优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体动画存在模板构建需大量骨骼绑定、特定姿态下产生伪影，以及线性混合蒙皮（LBS）导致的结构畸变等问题，严重影响动画质量与效率。

Method: 提出两阶段统一学习框架：第一阶段采用U-Net架构，将纹理和姿态信息解耦，实现快速模板生成；第二阶段利用数据驱动的变形细化方法，提高变形后的结构完整性。

Result: 实验表明，该方法在不同姿态下表现出一致、优良的平衡效率与质量，优于现有最优（SOTA）技术。

Conclusion: 所提方法同时解决了模板构建和目标姿态变形中的核心难题，在3D动画生成领域具有更高的效率和真实感。

Abstract: 3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.

</details>


### [184] [CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball](https://arxiv.org/abs/2512.01478)
*Omer Sela,Michael Chertok,Lior Wolf*

Main category: cs.CV

TL;DR: 该论文提出了CourtMotion，一种用于分析和预测职业篮球赛事中事件和战术发展的时空模型框架。通过结合骨骼追踪数据和深度学习网络，能够比仅使用球员位置的方法取得更优的预测效果。


<details>
  <summary>Details</summary>
Motivation: 传统依赖球员位置的数据分析方法，无法捕捉如身体朝向、防守姿态和投篮准备等关键动作信息，致使赛事预测和理解存在局限。针对这些不足，研究者希望开发一个能够理解运动细节与战术关系的更全面模型。

Method: 提出两阶段建模方法：首先将骨骼追踪数据输入图神经网络以捕捉细腻的动作模式，再利用具有特殊注意力机制的Transformer建模球员之间的互动。为实现动作与篮球事件的显式关联，引入事件投影头，将物理动作与具体战术事件（如传球、投篮、抢断）建立联系，并进行端到端训练。

Result: 在NBA追踪数据上，CourtMotion相较仅依赖位置的最先进模型，在轨迹预测误差方面降低了35%，并在多个篮球分析任务中均取得持续提升。预训练模型可迁移用于下游如掩护识别、投篮手识别、助攻预测、投篮位置分类和投篮类型识别等任务，均显著优于现有方法。

Conclusion: CourtMotion通过细粒度动作与事件的建模，极大提升了篮球比赛关键事件的预测与分析性能，为后续多种篮球智能分析任务提供了强有力的模型基础。

Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.

</details>


### [185] [ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling](https://arxiv.org/abs/2512.01481)
*Qisen Wang,Yifan Zhao,Peisen Shen,Jialu Li,Jia Li*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新方法，可生成高保真、3D一致且时间同步的多视角视频，解决了现有模型扩展到4D世界的困难。


<details>
  <summary>Details</summary>
Motivation: 现有相机控制视频生成模型难以直接扩展至高保真、三维一致且时间同步的多视角视频，这是4D世界建模的关键能力。已有方法如数据增强和测试时优化都有泛化性和可扩展性的问题。

Method: 提出ChronosObserver方法，包括两个核心模块：（1）世界状态超空间，用于表达4D场景的时空约束；（2）超空间引导采样，通过该超空间在扩散模型采样过程中多视角同步，无需额外训练或微调。

Result: 实验表明，该方法能无需训练或微调扩散模型，即可实现高保真和三维一致的时间同步多视角视频生成，效果优于现有方案。

Conclusion: ChronosObserver为无需训练生成高质量4D（多视角、时序一致）视频提供了全新途径，显著提升了现有方法的泛化性和可扩展性。

Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.

</details>


### [186] [A variational method for curve extraction with curvature-dependent energies](https://arxiv.org/abs/2512.01494)
*Majid Arthaud,Antonin Chambolle,Vincent Duval*

Main category: cs.CV

TL;DR: 本文提出了一种基于能量离散和向量场分解定理的变分方法，用于从图像中自动提取曲线和一维结构，且方法几乎无需监督，并进一步扩展到可处理曲率特征。


<details>
  <summary>Details</summary>
Motivation: 准确从图像中提取曲线和一维结构在计算机视觉、医学图像等领域有重要意义，现有方法可能在自动化、复杂结构或曲率处理上存在不足。

Method: 提出了基于能量离散化和Smirnov向量场分解定理的变分方法，采用双层最小化框架自动从图像中提取曲线和一维结构；对曲率敏感场景，将曲线提升到位置-方向空间，并施加合适的亚黎曼或芬斯勒度量进行优化。

Result: 提出的方法可自动、近乎无监督地从图像有效提取曲线和一维结构，并可处理曲率依赖情况。

Conclusion: 该方法提升了曲线提取的自动化与适应性，尤其在处理复杂含曲率结构时表现突出，有望在多种图像分析任务中应用。

Abstract: We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.

</details>


### [187] [ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark](https://arxiv.org/abs/2512.01495)
*Joanne Lin,Ruirui Lin,Yini Li,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: ELVIS提出了针对低光视频实例分割的有效自适应框架，通过无监督合成、降质建模和增强技术，有效提升了现有VIS模型在低光场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 低光内容下的视频实例分割面临噪声、模糊、低对比度等成像劣化，且缺乏大规模数据集和有效的合成退化建模，现有VIS方法在低光情境下鲁棒性差，应用受限。

Method: 提出ELVIS框架，包括：1）无监督的低光视频合成管线，能建模空间和时间退化；2）VDP-Net用于无须校准的降质特征合成；3）增强解码器分离内容与降质特征，实现更强的降质鲁棒性。整个框架用于域自适应以提升现有VIS模型的低光适应能力。

Result: 在合成的低光YouTube-VIS 2019数据集上，ELVIS可将性能提升最高达+3.7AP，明显优于现有方法。

Conclusion: ELVIS能够有效提升VIS模型在低光视频分割场景下的表现，是低光VIS领域的重要进展。论文计划开源代码，促进后续研究。

Abstract: Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \textbf{ELVIS} (\textbf{E}nhance \textbf{L}ow-light for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.

</details>


### [188] [Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2512.01510)
*Franz Thaler,Martin Urschler,Mateusz Kozinski,Matthias AF Gsell,Gernot Plank,Darko Stern*

Main category: cs.CV

TL;DR: 提出一种新方法SRCSM，实现单一源领域医疗图像分割的领域泛化显著提升，无需新领域数据就能直接应用于不同模态，综合实验结果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 目前大多数医疗图像分割模型对领域分布变化非常敏感，训练数据和实际应用场景（如不同成像模态或中心）存在“领域偏移”，影响模型泛化能力，尤其是在无法获取新领域标注数据时。因此，提升单一源领域下模型直接泛化到未知领域的能力非常重要。

Method: 提出SRCSM方法：训练时通过“基于语义的随机卷积”多样化源域，即依据信息标签对图像不同区域采用差异化增广。测试时通过“目标域图像强度映射”使新域图像的分布更接近源域，提升模型适应性。

Result: 在多种跨模态、跨中心的医学部位分割任务（腹部、全心脏、前列腺）上分组实验，SRCSM方法在大多数实验中优于现有域泛化方法。此外，在更困难的全心脏CT/MR数据训练、不同硬件MR心动周期测试场景下，也显著缩小了域差距。

Conclusion: SRCSM可视为医疗图像分割领域泛化任务的新SOTA，并在多个任务中达到了与源域（理想）基线相当的分割性能。

Abstract: We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.

</details>


### [189] [QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions](https://arxiv.org/abs/2512.01519)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.CV

TL;DR: 本文提出了QuantumCanvas，一个专为原子对量子系统设计的大规模多模态基准数据集，并表明通过利用物理基础图像和多模态学习，有助于提升材料与分子的量子属性预测的可转移性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有分子与材料机器学习大多依赖全分子/晶体级相关性，缺乏对原子对间本质量子作用的物理解读和可迁移性；而现实中的键合、电子耦合等现象都源自两体作用。为解决这一科学与可解释性难题，作者希望建立以原子对为基础的可物理迁移基准。

Method: 作者构建了覆盖2850种元素对、包含18项电子、热力学与几何属性的多模态数据集，每对元素都配备经物理推导、具空间与电荷对称性的10通道图像表示（如轨道密度、场变换、共占据图等），并对8种架构做了系统评测。

Result: GATv2模型在能隙预测上达到0.201 eV的平均绝对误差（MAE）；EGNN对HOMO和LUMO预测的MAE分别为0.265 eV和0.274 eV；DimeNet在全能量和斥能预测上的MAE分别为2.27 eV和0.132 eV，多模态融合模型在Mermin自由能上的MAE达到2.15 eV。QuantumCanvas预训练可显著提升模型在更大数据集（如QM9等）上的收敛稳定性和泛化性能。

Conclusion: QuantumCanvas通过将轨道物理与视觉表征深度融合，为学习可迁移的量子相互作用提供了结构化、可解释、物理驱动的基础框架。该数据集和模型为材料与分子机器学习领域的物理可转移性及可解释性做出了突破性贡献。

Abstract: Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.

</details>


### [190] [Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling](https://arxiv.org/abs/2512.01533)
*Hailong Yang,Te Zhang,Kup-sze Choi,Zhaohong Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Diffusion Fuzzy System (DFS) 的新型扩散模型，能更高效地生成高质量且特征异质性更强的图像，并有效降低多路径扩散的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型虽然能生成高分辨率的真实感图像，但在处理特征差异显著的图像集合时表现不佳。传统多路径扩散虽然试图通过不同路径分别学习不同区域/特征，但存在协调低效和计算开销大的问题，因此亟需一种既能充分提取异质特征又能高效训练的新方法。

Method: DFS采用基于模糊规则引导的多路径扩散机制：每一路负责学习特定类别的图像特征，并通过规则链动态协调路径间的信息流。为提升效率，DFS在潜空间引入模糊隶属度压缩机制，显著降低计算成本。

Result: 在LSUN Bedroom、LSUN Church和MS COCO三个公开数据集上，DFS在训练稳定性、收敛速度方面均优于现有单路径和多路径扩散模型。同时，其生成的图像在质量、文本-图像对齐度以及与目标参考图的相似度上也取得了更好表现。

Conclusion: DFS不仅解决了传统多路径扩散模型难以高效提取图像特征的弊端，还大大提升了扩散模型的训练效率和生成效果，具有显著的实际应用价值。

Abstract: Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.

</details>


### [191] [Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis](https://arxiv.org/abs/2512.01534)
*Alexander Frotscher,Christian F. Baumgartner,Thomas Wolfers*

Main category: cs.CV

TL;DR: 本文建立了一个大规模、多中心的脑部MRI深度无监督异常检测基准，系统对现有方法的性能和局限性进行了全面评估，为今后临床应用和相关研究提供了标准与方向。


<details>
  <summary>Details</summary>
Motivation: 目前脑部MRI的无监督异常检测方法在数据集、评估标准等方面高度碎片化，影响了临床转化和研究进展，因此需要权威的大规模性能基准来厘清当前方法的实际效果及难点。

Method: 收集来自6台扫描仪、不同年龄段的大量T1和T2加权健康人脑MRI作为训练集，并用92份数据调参确定阈值，用近3500份健康及临床队列数据作为测试集，系统评估主流的重建法和特征法在不同扫描器、病灶类型、年龄、性别下的分割性能与鲁棒性。

Result: 所有算法的Dice分割性能差异很大（0.03到0.65）；重建类方法（尤其是扩散模型）在分割效果最优，而特征法对分布漂移更稳健；但大多数算法对扫描仪、病灶类型大小、年龄性别等因素存在显著系统性偏差，小而低对比病灶易被遗漏，误报与人口属性相关；扩大健康训练集仅带来适度提升，说明算法瓶颈不在于数据规模。

Conclusion: 目前的无监督异常检测主要受限于算法本身，未来研究应关注原生预训练、合理偏差衡量、公平性建模和鲁棒域自适应，完善标准化评估，为临床应用铺路。

Abstract: Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.

</details>


### [192] [FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention](https://arxiv.org/abs/2512.01540)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出了FlashVGGT，一种高效的多视角图像三维重建方法，通过基于描述符的注意力机制大幅提升了可扩展性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的三维重建方法虽然效果好，但由于全局自注意力机制导致计算复杂度高，难以处理长序列和大规模图像。

Method: FlashVGGT用紧凑的描述符代替密集的全局注意力，将每帧的空间信息压缩成少量的描述符token，并通过与全部图像token的交叉注意力减少计算量。此外，利用描述符的紧凑性，实现了块递归机制，支持长序列在线推理。

Result: FlashVGGT在三维重建精度上与VGGT相当，但在1000张图片上的推理时间仅为VGGT的9.3%，并能高效扩展到超过3000张图片的长序列。

Conclusion: FlashVGGT显著提升了多视角三维重建任务中的效率和可扩展性，在保证精度的同时大幅度降低了推理时间，适用于大规模和长序列场景。

Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.

</details>


### [193] [MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration](https://arxiv.org/abs/2512.01563)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Nguyen Chi Thanh,Truong Hoang Viet,Trong-Le Do,Mai-Khiem Tran,Minh-Khoi Pham,Trung-Nghia Le,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 本文发布了 MasHeNe，这是一个包含肿瘤和囊肿的头颈部肿块像素级标注CT数据集，并提出了新的分割方法WEMF，提供了基准测试和开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集多聚焦头颈恶性肿瘤，较少涉及其他占位性病变，这限制了算法的泛化和诊断研究。因此需要一个包含更丰富病变类型的数据集。

Method: 1. 构建 MasHeNe 数据集—3779 张增强CT切片，标注有肿瘤和囊肿。2. 提出 WEMF 模型，结合三视窗增强和多频率注意力机制的 Mamba U型骨干，对空间和频率信息进行融合提升分割能力。3. 与标准分割方法进行对比，用 Dice、IoU、NSD、HD95 等指标进行评测。

Result: WEMF 模型在 MasHeNe 数据集上取得最佳分割表现：Dice 70.45%、IoU 66.89%、NSD 72.33%、HD95 5.12mm，优于对比方法。

Conclusion: MasHeNe 数据集为头颈部占位性病变分割建立了完整基准。WEMF 模型验证了新方法有效性，但该任务仍具挑战性，需要持续研究。数据及代码已开源，便于社区发展和比较。

Abstract: Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.

</details>


### [194] [RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions](https://arxiv.org/abs/2512.01582)
*Junran Peng,Yiheng Huang,Silei Shen,Zeji Wei,Jingwei Yang,Baojie Wang,Yonghao He,Chuanchen Luo,Man Zhang,Xucheng Yin,Wei Sui*

Main category: cs.CV

TL;DR: 论文提出了RoleMotion，一个包含丰富角色扮演和功能性动作的大规模人体动作数据集，并系统性地解决了现有数据集数据割裂、动作质量不一、文本注释粗糙等问题。


<details>
  <summary>Details</summary>
Motivation: 当前的人体动作数据集大多由不同子集拼接而成，数据功能性差且彼此割裂，难以覆盖多样化的社会场景，同时动作质量参差不齐，文本注释缺乏细致描述。作者希望通过精细化设计数据集，改善这些问题。

Method: 作者精心设计并采集了RoleMotion数据集，涵盖25个经典场景、110个功能角色、500多种行为和10296条高质量人体与手部动作序列，每条数据配有精细文本描述。还构建了比现有更强的数据集评估器，并用其评测多种文本到动作生成方法。

Result: RoleMotion数据集的数据和注释质量高，能够支持多样场景下的功能性动作生成，同时实验结果也表明该数据集在文本驱动的全身动作生成任务上具有较好效果。

Conclusion: RoleMotion极大提升了动作数据集的功能性、细致性和评估能力，为更真实和多样的文本驱动人体动作生成奠定基础。

Abstract: In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

</details>


### [195] [Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation](https://arxiv.org/abs/2512.01589)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Trong-Le Do,Truong Hoang Viet,Nguyen Chi Thanh,Huynh Nguyen Thuan,Do Vo Cong Nguyen,Minh-Khoi Pham,Mai-Khiem Tran,Viet-Tham Huynh,Trong-Thuan Nguyen,Trung-Nghia Le,Vo Thanh Toan,Tam V. Nguyen,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 本文介绍了一套名为AbscessHeNe的头颈脓肿CT公开数据集，并评估了多种主流分割模型在该任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 头颈部脓肿可导致严重感染甚至死亡，及时准确识别和界定对临床决策至关重要。现有公开数据集稀缺，限制了自动分割模型的发展和临床应用。

Method: 作者构建并标注了包含4,926张对比增强CT的AbscessHeNe数据集，所有病例均有临床确诊脓肿，逐像素标注病灶轮廓及相关临床元数据。采用CNN、Transformer和Mamba等主流分割架构进行基线性能评价。

Result: 最佳模型Dice系数仅0.39，IoU为0.27，表明自动分割该任务具备挑战性，现有方法尚有较大提升空间。

Conclusion: AbscessHeNe作为首个细致标注的头颈脓肿CT数据集，为后续分割、内容检索与智能临床支持系统搭建奠定基础，并已公开，促进相关研究进步。

Abstract: Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.

</details>


### [196] [Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager](https://arxiv.org/abs/2512.01611)
*Fengfeng Li,Zhou Feng,Hongliang Wu,Hao Zhang,Han Tian,Peng Liu,Lixin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于ShapeDTW算法的井下油基泥浆微电阻率成像数据深度配准方法，通过结合一维梯度方向直方图（HOG1D）与原始信号进行特征提取，实现了成像数据的精准对齐。


<details>
  <summary>Details</summary>
Motivation: 传统油基泥浆成像仪在进行井下测井成像时，尽管有速度校正，仍存在上下探头成像图像深度不对齐问题，特别是在地层结构复杂、纹理明显和局部尺度变化时影响显著，迫切需要一种更加鲁棒与准确的深度对齐方法。

Method: 采用Shape Dynamic Time Warping（ShapeDTW）算法，通过提取局部形态特征（HOG1D加原始信号形成组合特征），构建形态敏感的距离矩阵，使成像序列在对齐过程中更好地保持结构相似性，实现复杂图像的深度精准配准。该框架还支持灵活扩展其他特征描述子以适应不同地质特征需求。

Result: 现场测试表明，该方法对存在复杂纹理、深度偏移或局部缩放的成像数据实现了高精度的深度对齐，表现优于传统对齐方法。

Conclusion: 基于ShapeDTW的成像深度对齐方法在油基泥浆微电阻率成像测井中表现出较高的精度与适应性，并可扩展融合不同特征以适配各类地质特征需求，为井下成像深度配准问题提供了有效方案。

Abstract: In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.

</details>


### [197] [Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2512.01636)
*Xin Wang,Haipeng Zhang,Mang Li,Zhaohui Xia,Yueguo Chen,Yu Zhang,Chunyu Wei*

Main category: cs.CV

TL;DR: 提出Fusion-Diff框架，有效提升零样本组合图像检索（ZS-CIR）性能，且数据需求低。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法依赖昂贵的三元组标注，零样本方法受制于视觉和文本模态鸿沟，效率和准确性待提升。

Method: 提出Fusion-Diff方法，在联合视觉-语言空间内进行多模态特征融合编辑，并加入轻量级Control-Adapter，在仅需20万合成样本的小规模数据上进行微调。

Result: 在CIRR、FashionIQ和CIRCO等标准CIR基准上，Fusion-Diff明显超越现有零样本方法。

Conclusion: Fusion-Diff有效缩小了模态鸿沟，实现了高效、高性能的零样本CIR，并具有良好解释性。

Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.

</details>


### [198] [ViT$^3$: Unlocking Test-Time Training in Vision](https://arxiv.org/abs/2512.01643)
*Dongchen Han,Yining Li,Tianyu Li,Zixuan Cao,Ziming Wang,Jun Song,Yu Cheng,Bo Zheng,Gao Huang*

Main category: cs.CV

TL;DR: 本文系统性地研究了视觉领域中的测试时训练（TTT）结构，并提出了一种新模型ViT^3，在多个视觉任务中表现优越，实现了计算高效与性能兼顾。


<details>
  <summary>Details</summary>
Motivation: 测试时训练（TTT）近来在高效序列建模上表现突出，但在视觉任务上的结构设计和优化还缺乏系统理解和实践指导。如何为视觉序列建模选择高效且有效的TTT设计，成为该领域亟需解决的问题。

Method: 作者对视觉TTT的内模块选择和训练方式进行了大规模实证分析，总结了六条实用的设计原则，并据此开发了全新的Vison Test-Time Training（ViT^3）模型，实现了线性复杂度和高并行计算能力。

Result: ViT^3在图像分类、生成、目标检测、语义分割等多种视觉任务上，稳定优于Mamba和线性注意力等先进线性复杂度模型，并大幅缩小与视觉Transformer的性能差距。

Conclusion: 本文不仅为视觉TTT模型设计提供了系统指南和经验，还推出了高效且强劲的基线模型ViT^3，有助于未来视觉序列建模方法的研发和推广。

Abstract: Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.

</details>


### [199] [DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation](https://arxiv.org/abs/2512.01657)
*Hongyu Xu,Panpan Meng,Meng Wang,Dayu Hu,Liming Liang,Xiaoqi Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的自适应双分支Kolmogorov-Arnold UNet（DB-KAUNet）用于视网膜血管分割，结合CNN和Transformer优势，通过多种新颖模块提升分割精度和鲁棒性，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN无法有效捕捉长距离依赖和复杂非线性关系，限制了视网膜血管分割的表现，因此需要新方法来提升特征表达能力，进而提升分割效果。

Method: 提出DB-KAUNet，主干为异构双分支编码器（HDBE），即并行的CNN和Transformer通道。设计KANConv和KAT模块提升特征学习能力，引入通道交互模块（CCI）加强特征交互，利用空间特征增强模块（SFE）融合两分支输出，进一步提出带有几何自适应融合能力的SFE-GAF模块，提高对真实血管形态的感知，并降低噪声与计算量。

Result: 在DRIVE、STARE和CHASE_DB1数据集上大量实验表明，DB-KAUNet在视网膜血管分割任务上取得了领先的分割性能和极高的鲁棒性。

Conclusion: DB-KAUNet综合了CNN和Transformer的优势，引入新模块显著提升了血管分割的精度和鲁棒性，展示了在医学图像分割领域的应用潜力。

Abstract: Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.

</details>


### [200] [Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.01665)
*Zhicheng Zhao,Yin Huang,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本论文提出了专为遥感图像中微小目标检测设计的先进检测框架ScaleBridge-Det，能在多种尺度下实现均衡检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中存在极端的尺度变化和高密度分布，既有密集的微小目标又有大目标，当前方法难以兼顾多尺度目标的检测性能。尽管大型基础模型在通用视觉任务中取得了突破，但尚未针对如此大尺度和密度变动的场景进行优化。

Method: 作者提出了ScaleBridge-Det检测框架。核心方法包括：1）REM（Routing-Enhanced Mixture Attention）模块，通过自适应路由动态选择融合多尺度专家特征，弥补标准MoE模型对主导尺度的偏好；2）DGQ（Density-Guided Dynamic Query）模块，预测目标密度以自适应调整检测query的位置与数量，使框架资源更加有效分配于不同尺度目标。

Result: ScaleBridge-Det在AI-TOD-V2和DTOD数据集上取得了最新最好的检测性能，并在VisDrone跨域数据集上表现出优异的鲁棒性，充分展示了其多尺度和跨领域的强适应能力。

Conclusion: 该工作首次实现通过大模型在遥感场景中对微小及大目标的同步优化，兼顾多尺度目标的检测性能，为后续研究提供了新的框架范式和技术路径。

Abstract: Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.

</details>


### [201] [GRASP: Guided Residual Adapters with Sample-wise Partitioning](https://arxiv.org/abs/2512.01675)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出一种名为GRASP的新方法，通过解决主流与稀有类别之间的梯度冲突，提升了长尾场景下的文本生成图像模型在罕见类别（如医学影像罕见病理）上的质量、多样性与实用性，大幅超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像扩散模型在常见类别表现出色，但对数据中出现频率低的罕见类别（如医学影像中的稀有病理）生成效果较差，导致数据增强、辅助诊断等目标无法实现。追踪问题根源，作者发现主要是频繁类别与罕见类别间存在梯度冲突，当前方法对此无有效解决。

Method: 提出GRASP方法：首先，用外部先验对样本静态划分成梯度冲突较小的集群，然后在变换器前馈层中为每个集群注入特定的残差适配器进行微调，避开复杂的动态门控机制，提升效率与稳定性。

Result: 在长尾分布的MIMIC-CXR-LT医学影像数据集上，GRASP在生成质量（FID）、多样性等指标上，尤其是在稀有类别表现显著优于标准微调和MoE等强大基线；下游分类任务稀有标签的指标也有明显提升，并可推广到ImageNet-LT等一般长尾任务。

Conclusion: GRASP是一种轻量、可扩展、易集成的新范式，有效缓解生成模型在长尾场景下对稀有类别的忽视问题，并普遍适用于多种长尾分布数据生成任务。

Abstract: Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.

</details>


### [202] [Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation](https://arxiv.org/abs/2512.01677)
*Haodong Yan,Hang Yu,Zhide Zhong,Weilin Yuan,Xin Gong,Zehang Luo,Chengxi Heyu,Junfeng Li,Wenxuan Song,Shunbo Zhou,Haoang Li*

Main category: cs.CV

TL;DR: 本论文提出一种新的结构与接触感知表示方法，无需3D注释即可生成物理逼真的手-物交互视频，并且方法具有很强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有手-物交互视频生成方法在2D和3D表示之间存在兼容性难题，无法兼顾扩展性和交互的真实性，且难以准确建模手与物体的物理约束。

Method: 作者提出了一种无需3D标注的结构和接触感知表示方案，可以有效捕捉手-物接触、遮挡关系以及整体结构信息。配合联合生成范式及共享-专用策略方法，对交互表征和视频同步生成，强化模型在不依赖3D注释下对细粒度物理交互的建模能力。

Result: 在两个真实世界数据集上，提出的方法在生成具有物理现实感和时序一致性的手-物交互视频任务上超越了现有先进方法，并表现出很好的对开放世界场景的泛化能力。

Conclusion: 论文方法为复杂交互视频生成领域提供了一种可扩展又高保真度的解决方案，为手-物交互建模和后续相关研究带来新的可能性。

Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.

</details>


### [203] [Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies](https://arxiv.org/abs/2512.01681)
*Farzaneh Seyedshahi,Francesca Damiola,Sylvie Lantuejoul,Ke Yuan,John Le Quesne*

Main category: cs.CV

TL;DR: 该研究提出了一种可以从胸膜间皮瘤活检样本中准确分类肿瘤亚型和预测患者预后的AI方法。


<details>
  <summary>Details</summary>
Motivation: 传统计算病理模型多依赖于大块切除组织样本进行训练，而现实临床中常见的是小型活检样本，这种差异限制了AI模型的广泛实际应用。如何在小样本条件下获得可靠的病理信息尤为重要。

Method: 研究人员使用了自监督方式训练的编码器，先在切除组织大样本上学习形态特征，然后将该编码器应用到活检样本中，从而自动提取有代表性的病理形态学模式。

Result: 该模型能够利用提取到的形态特征有效预测患者生存期，并准确进行肿瘤亚型分类。

Conclusion: AI驱动的模型能够帮助医生在活检样本有限的情况下，辅助诊断和治疗方案选择，推动胸膜间皮瘤等疾病的精准医疗。

Abstract: Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.

</details>


### [204] [DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models](https://arxiv.org/abs/2512.01686)
*Patrick Kwon,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出DreamingComics框架，通过结合视频扩散Transformer（DiT）和新的区域位置编码方法，实现对故事视觉化任务中角色一致性和艺术风格一致性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前故事可视化方法只依赖文本进行主体定位，导致很难实现角色一致性和艺术风格的一致性，因此需要一种能够结合版面布局信息且效果更佳的方案。

Method: 1. 基于预训练的视频扩散Transformer（DiT）模型，引入其时空先验以增强一致性；2. 提出RegionalRoPE，区域感知的位置编码机制，通过目标布局重建特征嵌入索引，实现对布局的精确控制；3. 设计Masked Condition Loss，进一步约束角色视觉特征在其指定区域内表达；4. 利用LLM训练布局推理器，可从自然语言脚本生成漫画风格布局，实现灵活的布局条件约束。

Result: 与现有方法相比，DreamingComics框架角色一致性提升29.2%，风格相似度提升36.2%，同时空间准确性也较高。

Conclusion: DreamingComics能够有效提升故事可视化任务中的角色和风格一致性，并能够实现高精度的空间布局控制，是基于视觉故事生成领域的重要进展。

Abstract: Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/

</details>


### [205] [SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation](https://arxiv.org/abs/2512.01701)
*Xiuli Bi,Die Xiao,Junchao Fan,Bin Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种新的语义与空间校正方法（SSR），用于提升基于CLIP的弱监督语义分割性能，显著减少了非目标区域的过激活现象，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于CLIP的弱监督语义分割方法存在对非目标前景和背景区域过激活的问题，影响分割精度。

Method: 方法包括两个部分：1）跨模态原型对齐（CMPA）通过对比学习机制实现跨模态特征空间对齐，增强类别区分性并提升语义相关性，解决非目标前景的过激活；2）超像素引导校正（SGC）利用超像素先验，在亲和力传播时过滤非目标区域干扰，有效抑制背景过激活。

Result: 在PASCAL VOC 和 MS COCO数据集上实验，SSR方法分别取得了79.5%和50.6%的mIoU性能，优于所有单阶段及多数多阶段方法。

Conclusion: SSR方法能够有效消除基于CLIP的弱监督语义分割中的过激活问题，提升分割精度，在主流数据集上表现优越，有望推动领域发展。

Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.

</details>


### [206] [FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing](https://arxiv.org/abs/2512.01755)
*Yucheng Liao,Jiajun Liang,Kaiqian Cui,Baoquan Zhao,Haoran Xie,Wei Liu,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: 本文提出FreqEdit框架，解决多轮指令图像编辑中高频信息丢失导致的画质下降问题，实现连续10次以上高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的图像编辑方法在多轮编辑时图像质量严重下降，主要原因是高频细节逐步丢失，急需方法稳定细节并提升多轮编辑表现。

Method: 提出FreqEdit框架，无需重新训练，包含三大核心：1）从参考速度场注入高频特征，恢复细节；2）自适应注入策略，空间精准调节不同区域注入强度；3）路径补偿机制，周期性矫正编辑轨迹，防止过拟合约束。

Result: 大量实验表明，FreqEdit在身份保持及指令遵循上均优于7种最新方法，支持10次以上连续高质量编辑。

Conclusion: FreqEdit有效克服多轮编辑高频信息损失问题，大幅提升多轮指令图像编辑质量，为自然语言图像编辑应用提供新思路。

Abstract: Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.

</details>


### [207] [HiconAgent: History Context-aware Policy Optimization for GUI Agents](https://arxiv.org/abs/2512.01763)
*Xurui Zhou,Gongwei Chen,Yuquan Xie,Zaijing Li,Kaiwen Zhou,Shuai Wang,Shuo Yang,Zhuotao Tian,Rui Shao*

Main category: cs.CV

TL;DR: 提出了一种高效利用历史信息的GUI智能体HiconAgent，通过历史上下文感知的策略优化方法（HCPO），在保证效果的同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统GUI智能体在处理顺序导航任务时，若利用完整历史信息虽能提高决策能力，但带来大量冗余和计算负担。因此，研究如何高效地利用有用历史信息是提升智能体效率和性能的重要问题。

Method: 提出历史上下文感知的策略优化方法（HCPO），包括两大创新：（1）动态上下文采样（DCS），让智能体在采样时自适应选择不同长度的有效历史。（2）锚点引导式历史压缩（AHC），通过双分支结构，在策略更新阶段压缩历史观测，同时保留动作信息作为锚点，并通过历史增强对齐损失，确保高效且一致的历史利用。

Result: 在主流GUI导航基准上，所提HiconAgent表现优越。体积更小的HiconAgent-3B在GUI-Odyssey任务中，地面准确率和步骤成功率较GUI-R1-7B分别提升+8.46%和+11.32%；在AndroidControl和AITW上也取得类似表现，并实现最高2.47倍提速及60% FLOPs的减少。

Conclusion: HiconAgent有效解决了历史信息冗余和计算效率低下的问题，实现了更高效、更精确的GUI顺序导航，是GUI智能体设计的有力进展。

Abstract: Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.

</details>


### [208] [VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis](https://arxiv.org/abs/2512.01769)
*Hafsa Billah*

Main category: cs.CV

TL;DR: 本文提出了一种通用的视频情境分析（VSA）框架，通过结合关系模型和图模型，实现跨不同领域的视频情境自动检测，解决了现有方法针对性强、扩展性差的问题。其方法在多领域视频中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 目前视频内容分析技术虽然在对象识别、跟踪等方面已取得进展，但对有意义情境的理解（如对象互动）仍十分困难，多依赖人工分析或定制算法，无法适应不同领域和复杂情景，亟需可通用且自动化的方法。

Method: 作者提出先采用先进的视频内容提取技术获得结构化信息，再用扩展关系模型（R++）与图模型进行内容表达。R++模型支持数据流式连续查询，图模型则擅长发现复杂情境。两者结合实现了灵活且强大的情境检测。不同领域情境通过参数化模板抽象以支持通用性。

Result: 方法在辅助生活、城市监管和监控等三大领域的多种实际视频场景中进行评估，实验证明该框架在检测准确率、效率和鲁棒性上均表现优异。

Conclusion: 框架具有通用性与高效性，可自动、准确地识别多领域下的视频情境，显著减少人工干预需求，对实际应用具有广泛潜力。

Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

</details>


### [209] [Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels](https://arxiv.org/abs/2512.01771)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 本文提出了一种结合可学习边缘核的新型医学图像配准方法，在多个数据集和任务设置中均优于当前主流技术。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像配准方法在应对对比度差异、空间畸变及多模态间差异时表现有限，难以兼顾不同临床和科研具体需求。

Method: 本文提出在学习型刚性与非刚性配准框架中引入可学习的边缘检测核，初始采用预定义的边缘核，通过噪声扰动并在训练阶段自适应更新。设计了四种刚性及四种非刚性模型变种，分别在有/无颅骨去除与多模态配准等不同设置下评估方法表现。

Result: 在医学大学自有数据集的三种设置及两个公开数据集上的实验表明，作者方法在所有情况下均优于最先进的配准技术。

Conclusion: 所提基于可学习边缘特征的医学图像配准框架，有效提升了多模态图像对齐及解剖结构分析的准确性，在实际应用中具有广阔前景。

Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.

</details>


### [210] [Evaluating SAM2 for Video Semantic Segmentation](https://arxiv.org/abs/2512.01774)
*Syed Hesham Syed Ariff,Yun Liu,Guolei Sun,Jing Yang,Henghui Ding,Xue Geng,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文探索将强大的Segmentation Anything Model 2（SAM2）从视觉对象分割扩展到视频语义分割（VSS），并提出两种主要方案提升复杂多对象、时序一致性的分割效果。实验表明SAM2有助于VSS的性能提升，尤其是在边界预测上。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM2在视频对象分割表现优异，但面向更加复杂且要求高的密集视频语义分割时，仍面临空间精度、时序一致性以及多对象复杂边界处理等新挑战，亟需进一步方法探索。

Method: 提出两条方案：一，利用SAM2从图像中提取唯一对象掩码，结合并行的分割网络提升初始预测和精细化分割；二，利用预测掩码提取唯一特征向量，通过简单网络进行分类，再将分类结果与掩码结合获得最终输出。

Result: 实验证明，借助SAM2显著提升了VSS任务中的整体表现，尤其体现在对象边界精确预测和整体分割精度上。

Conclusion: SAM2作为基础视觉分割模型，不仅在传统视频对象分割任务优异，在适当方法加持下，也能有效迁移并提升视频语义分割任务表现，尤其适合高精度和复杂对象边界的应用。

Abstract: The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.

</details>


### [211] [Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks](https://arxiv.org/abs/2512.01788)
*Christian Mollière,Iker Cumplido,Marco Zeulner,Lukas Liesenhoff,Matthias Schubert,Julia Gottfriedsen*

Main category: cs.CV

TL;DR: 本文比较了传统压缩方法（JPEG2000）与基于任务的深度学习压缩方法（Discretized Mixed Gaussian Likelihood）在卫星遥感图像分割任务中的表现。学习型压缩在多通道光学大尺度图像重建和分割上明显优于传统方法，但在小型单通道热红外数据集上表现接近。此外，端到端联合优化模型并未带来超越单独优化的性能提升。


<details>
  <summary>Details</summary>
Motivation: 卫星遥感数据量巨大，带来传输和存储压力，研究如何在压缩后保留关键任务相关信息以满足分割等下游应用需求。

Method: 在火灾、云层和建筑检测三类分割任务上，分别测试传统JPEG2000压缩和深度学习的概率模型压缩（Discretized Mixed Gaussian Likelihood）方法，综合数据重建和分割效果评估二者表现，并考察联合端到端优化对性能的影响。

Result: 学习型压缩在多通道光学影像的重建质量（PSNR）与分割准确度上均明显优于JPEG2000。但在数据量有限与单通道的热红外影像场景下，传统方法依然有竞争力。联合优化并未优于独立模型优化。

Conclusion: 基于任务学习的压缩算法可显著提升大规模、高维卫星影像压缩和下游分割任务的效果，但在特定受限场景下传统方法仍有价值。同时，端到端联合训练并非总能带来额外性能提升。

Abstract: The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.

</details>


### [212] [SAM3-UNet: Simplified Adaptation of Segment Anything Model 3](https://arxiv.org/abs/2512.01789)
*Xinyu Xiong,Zihuang Wu,Lei Lu,Yufa Xia*

Main category: cs.CV

TL;DR: 本文提出了SAM3-UNet，一种基于Segment Anything Model 3（SAM3）的轻量级变体，具备更低的训练成本和出色的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的SAM模型在下游任务上适应性有限，尤其在资源受限环境下。作者希望通过简化模型结构，提升SAM系列在具体任务上的实用性和适用范围。

Method: SAM3-UNet包含三部分：SAM3图像编码器、高效参数微调的简单适配器、轻量级U-Net式解码器。通过这种结构，既保持了强大的特征提取能力，又大幅降低了计算消耗。

Result: 在镜像检测和显著性目标检测等多项任务中，SAM3-UNet不仅超过了之前的SAM2-UNet，也优于其他先进方法，同时训练时显存消耗低于6GB（batch size为12）。

Conclusion: SAM3-UNet在保证高性能的同时极大降低了资源消耗，提升了SAM模型在多种下游视觉任务中的应用潜力。

Abstract: In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.

</details>


### [213] [Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos](https://arxiv.org/abs/2512.01803)
*Xavier Thomas,Youngsun Lim,Ananya Srinivasan,Audrey Zheng,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 本论文提出了一种用于评估生成视频中复杂人体动作视觉与时序正确性的全新指标。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成评测方法过于关注外观，缺乏对动作时序与动态机制的理解，难以判断生成动作的合理性与真实性。

Method: 作者提出结合无外观依赖的人体骨架几何特征与外观特征，构建真实人体动作的联合特征分布，通过测量生成视频特征与该分布的距离来量化动作合理性。

Result: 在新构建的评测基准和已有外部基准上，该指标较当前最好方法提升68%，且与人类主观评价相关性更强。

Conclusion: 本文指标有效暴露了现有生成模型的缺陷，为复杂动作视频生成研究提供了新标准。

Abstract: Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.

</details>


### [214] [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816)
*Juanxi Tian,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.CV

TL;DR: 现有多模态模型多通过文本生成单一静态图像来校准语义一致性，但这限制了模型对动态过程的建模能力。本文提出了Envision基准，通过链式文本到多图像生成任务评测模型在时空因果建模上的能力，并引入Envision-Score多维度评价指标。实验发现，当前的专用T2I模型虽然美学表现好，但缺乏真实世界知识；统一多模态模型虽更优于前者，但依然难以获得时空一致性，显露出现有方法的核心局限。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在理解和生成任务中大多依赖单一的静态图像生成，无法模拟现实世界中事件的动态演变过程，导致对真实世界知识的内化有限。因此，亟需一种新的评测方式，能促进模型在事件因果与时空链条上的推理和生成能力提升。

Method: 作者提出Envision基准，包括1,000个跨越六大领域、由四阶段组成的事件链文本提示，用于驱动多图像生成，并以时空因果链为基础重组评价维度。同时，提出Envision-Score综合指标，从一致性、物理合理性与美学等多维评价模型输出。通过对15个主流生成模型系统性评测，分析其在因果叙事连贯性和时空一致性等方面的表现。

Result: 实验结果显示，T2I专用模型在美学质量上表现突出，但缺乏对世界知识的深层理解。统一多模态模型在因果叙事连贯性上超越专用模型，展现更好的知识整合能力。然而，无论哪类开源模型，都未能实现理想的时空因果一致性，与闭源模型仍存在性能差距。

Conclusion: 聚焦于单一静态图像的模式训练导致模型倾向静态语义匹配，抑制了其动态世界建模及知识内化能力。只有注重因果-时空链条的生成与评测，才能推动多模态模型向着更强的世界知识推理与动态过程理解发展。

Abstract: Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.

</details>


### [215] [Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling](https://arxiv.org/abs/2512.01821)
*Meng Cao,Haokun Lin,Haoyuan Li,Haoran Tang,Rongtao Xu,Dong An,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种新范式MILO，通过视觉生成器与相对位置编码提升多模态大模型的三维空间推理能力，并构建了大规模几何数据集GeoGen。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型空间推理能力薄弱，过度依赖文本符号，缺乏与视觉几何的结合，导致无法像人类一样理解和想象三维空间。

Method: 提出MILO范式，在符号推理过程中引入视觉生成器，提供几何感知反馈，实现感知与符号的隐式绑定；同时设计了新型相对位置编码（RePE），用于建模相对相机位姿变化。并自建GeoGen数据集，含丰富的几何观测-动作-结果视频样本。

Result: 实验表明，MILO结合视觉模拟反馈与RePE，在多个主流基线和空间推理基准上，显著提升空间理解与推理能力。

Conclusion: 通过引入感知反馈和创新编码方式，MILO有效缩短多模态大模型与真实空间理解之间的差距，为三维空间推理提供了更完整的解决方案。

Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.

</details>


### [216] [CauSight: Learning to Supersense for Visual Causal Discovery](https://arxiv.org/abs/2512.01827)
*Yize Zhang,Meiqi Chen,Sirui Chen,Bo Peng,Yanxi Zhang,Tianyu Li,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文提出了视觉因果发现的新任务，并创建了大规模数据集VCG-32K，开发了创新模型CauSight，在视觉因果发现上大幅领先GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统大多只能感知视觉实体的存在，但无法推断其因果关系，而人类则具备通过观察推断因果的能力。为让AI具备类似的因果推理能力，有必要研发相关任务与模型。

Method: 作者构建了包含3万多张带有实体级因果图的VCG-32K数据集，并提出基于视觉-语言的CauSight模型，其训练包括：利用VCG-32K数据采集，采用Tree-of-Causal-Thought（ToCT）合成推理轨迹，以及基于因果奖励的强化学习优化推理策略。

Result: CauSight在视觉因果发现任务上表现突出，绝对提升21%，性能为GPT-4.1的三倍以上。

Conclusion: 论文证明了专门为视觉因果推理设计的模型（CauSight）在该任务上优于传统大模型，相关数据集和代码已开源，促进领域进一步发展。

Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.

</details>


### [217] [OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic](https://arxiv.org/abs/2512.01830)
*Songyan Zhang,Wenhui Huang,Zhan Chen,Chua Jiahao Collister,Qihang Huang,Chen Lv*

Main category: cs.CV

TL;DR: 本论文提出OpenREAD，一个基于视觉-语言模型的端到端自主驾驶框架，通过开放式推理强化学习，显著提升从高层推理到低层轨迹规划的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的二阶段微调策略（SFT+RFT）虽然推动了知识驱动的自主驾驶，但SFT的学习方式限制了推理泛化能力，同时RFT难以应用于场景理解等开放性问题，亟需一种新方法提升整体驾驶性能。

Method: 作者首先构建了包含大量思维链（Chain-of-Thought）标注的开源驾驶知识数据集，并采用大型语言模型Qwen3作为RFT中的评价者，对开放式问题的推理质量进行奖励建模，实现从高层到低层的端到端强化微调。

Result: 综合实验表明，该方法在上游（推理等）和下游（规划等）任务均带来显著提升，OpenREAD系统在同行基准测试中取得了最新的最优性能。

Conclusion: OpenREAD实现了端到端的开放式推理强化自主驾驶，突破了以往SFT和RFT的局限，推动知识驱动自主驾驶系统的推理和规划水平向前发展。

Abstract: Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.

</details>


### [218] [PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models](https://arxiv.org/abs/2512.01843)
*Zeqing Wang,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了物理不合逻辑检测数据集（PID），并基于此数据集微调视听语言模型（VLM），旨在评估和提升文本生成视频模型（T2V）在物理合理性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然T2V生成模型在视频质量、长度和指令跟随等方面取得了巨大进步，但能否遵循物理规律、生成物理合理的视频仍存疑，目前通用VLM难以识别物理不可能的视频内容。因此，亟需相关数据集和方法来自动检测与解释物理不合理的视频内容。

Method: 1. 构建PID数据集，包括500个测试视频和2,588个训练视频对，通过改写真实视频的描述来诱导T2V模型生成物理不合理内容。2. 提出轻量级VLM微调方法，使模型能检测物理不合理事件并给出物理原理解释。3. 微调后的模型命名为PhyDetEx，对多种主流T2V模型进行了物理合理性基准测试。

Result: 通过PhyDetEx，发现现有T2V模型虽然在物理合理性方面有所进步，但完全理解和遵循物理规律仍很困难，开源模型问题尤为突出。

Conclusion: 本文提出的新数据集和微调方法显著提升了VLM对视频物理合理性的检测与解释能力，为未来T2V模型物理合理性评测与优化提供了重要工具和基准。

Abstract: Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.

</details>


### [219] [COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis](https://arxiv.org/abs/2512.01853)
*Tsz-To Wong,Ching-Chun Huang,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出了一种可重构的多智能体系统（MAS），作为体育视频理解的基础框架，以提升对时间层次结构的建模能力，并实现任务通用性、低开发成本和良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端模型在体育视频分析中的时间层次理解能力有限，难以泛化到新任务，开发成本高且可解释性差，需要新的系统性方法进行突破。

Method: 设计了一个多智能体系统（MAS），每个智能体作为“认知工具”专注于特定分析任务。系统可灵活配置、递归调用和组合不同智能体，适应不同的时间粒度和分析任务。通过自适应管道，既能支持短时的分析推理，也能实现长时生成式摘要。

Result: 通过在羽毛球两大代表性任务（细粒度事件检测与全局语义组织）上的实验，验证了所提出MAS框架的适应性和有效性。

Conclusion: 该工作为体育视频智能分析提供了一种灵活、可扩展且高可解释性的系统范式，有望推动多任务、跨领域的体育视频理解发展。

Abstract: Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page

</details>


### [220] [TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals](https://arxiv.org/abs/2512.01885)
*Florian Bürger,Martim Dias Gomes,Nica Gutu,Adrián E. Granada,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TransientTrack的深度学习方法，用于显微镜多通道视频中带有瞬时荧光信号的细胞追踪，能够检测包括细胞分裂和死亡在内的关键细胞事件，并在各种条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统细胞追踪方法主要针对单信号、信号恒定的视频，对瞬时信号变化以及关键事件（如细胞死亡）检测能力有限。研究人员亟需能处理多通道、信号随时间波动并监测细胞命运的追踪方法，以改进对细胞群体动力学和相关生命过程的理解。

Method: 本文提出的TransientTrack方法基于深度学习，融合Transformer网络，通过直接对细胞检测嵌入进行匹配，无需专门提取追踪特征。方法采用多阶段匹配策略、利用Kalman滤波器对缺失轨迹进行插值，并能自动检测细胞分裂和死亡等事件。该方法轻量高效，适用于不同条件下的细胞追踪。

Result: TransientTrack在多种实验条件和数据集下均表现出色，能够有效追踪细胞及捕捉细胞分裂与死亡事件，并可用于分析化疗药物对单细胞水平细胞群体的影响。

Conclusion: TransientTrack为癌细胞动力学等领域的定量研究提供了新工具，有助于深入揭示药物治疗响应与耐药机制，实现对细胞命运的精准量化和追踪。

Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.

</details>


### [221] [KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM](https://arxiv.org/abs/2512.01889)
*Zaid Nasser,Mikhail Iumanov,Tianhao Li,Maxim Popov,Jaafar Mahmoud,Malik Mohrat,Ilya Obrubov,Ekaterina Derevyanka,Ivan Sosin,Sergey Kolyubin*

Main category: cs.CV

TL;DR: 本文提出了KM-ViPE，一个实时、开放词汇的单目视觉SLAM系统，可在无需深度传感器和离线标定的条件下，直接处理动态场景中的RGB视频流，实现高效的定位与语义建图。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统通常依赖深度传感器、离线标定或对动态环境适应性差，限制了在无人机、AR/VR等实际应用中的实用性。为此，作者希望开发一种可直接基于普通摄像头且适用于动态复杂场景的即时SLAM解决方案。

Method: KM-ViPE通过融合DINO视觉特征与几何约束，并基于高级特征构建自适应鲁棒核，有效处理动态及可移动静态物体。同时，系统将视觉、几何特征与语言嵌入对齐，实现对场景的定位与开放词汇语义建图。

Result: KM-ViPE在动态环境下实现了与现有最先进方案相当的定位与建图性能，并克服了需深度数据或依赖离线处理的局限性，展示了优异的在线与动态场景鲁棒性。

Conclusion: KM-ViPE能够结合互联网规模训练，实现了在线、无需标定、单目输入和动态环境下的鲁棒SLAM，适用于自动机器人和AR/VR等多种场景，提升了具身AI的空间智能水平。

Abstract: We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.

</details>


### [222] [StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data](https://arxiv.org/abs/2512.01895)
*Avirup Dey,Vinay Namboodiri*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨域人脸重定向方法StyleYourSmile，无需精心构建的多风格配对数据，即可实现对身份、表情和风格属性的解耦控制，同时保持高保真度的人脸重定向效果。


<details>
  <summary>Details</summary>
Motivation: 当前跨域人脸重定向难以在不同风格域中准确保留身份信息，且常常需求昂贵的多风格配对数据或测试时优化，应用受限。

Method: 提出一种高效的数据增强策略并结合双编码器框架，分别提取身份特征与域内风格特征，然后将这些解耦的控制信号用于调控扩散模型，实现表情等属性在不同域之间的迁移。

Result: StyleYourSmile方法在多种视觉风格域上都展现出优于现有方法的身份保真和重定向效果，无需特殊配对数据集。

Conclusion: 该方法显著提升了跨域人脸重定向的实用性和效果，对身份、表情及风格属性进行有效解耦控制，有较强的通用性，无需额外数据收集和繁琐优化。

Abstract: Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.

</details>


### [223] [SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception](https://arxiv.org/abs/2512.01908)
*Gurmeher Khurana,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: 提出SARL，一种空间感知自监督学习框架，可处理融合视觉-触觉数据的机器人操控问题，在下游任务上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-触觉自监督学习往往丢失空间结构信息，不符合实际机器人操作对局部几何信息的需求。作者希望设计一种更有效保留空间结构的方法，提升融合视觉-触觉数据的表达能力。

Method: 提出SARL框架，基于BYOL结构，增加了三个作用于中间特征图的空间层面损失（显著性对齐、区域原型分布对齐、区域亲和匹配），以保持不同视角间的空间一致性。同时仍保留全局目标，增强整体表达能力。

Result: 在六项下游任务中，SARL在融合视觉-触觉数据上均优于九种主流自监督方法。在最具挑战性的几何敏感edge-pose回归任务中，SARL的MAE为0.3955，比第二名方法提升30%，接近有监督上限。

Conclusion: 空间结构等变性对于融合视觉-触觉数据的表达至关重要，SARL显著提升了机器人操控的感知能力，为相关任务带来新的高效解决方案。

Abstract: Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.

</details>


### [224] [Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2512.01922)
*Zahra Mahdavi,Zahra Khodakaramimaghsoud,Hooman Khaloo,Sina Bakhshandeh Taleshani,Erfan Hashemi,Javad Mirzapour Kaleybar,Omid Nejati Manzari*

Main category: cs.CV

TL;DR: 本文提出了一种新的稀疏视觉对比解码方法（Med-VCD），可在不增加计算时间的情况下，减少医学大规模视觉-语言模型（LVLMs）的幻觉输出，提高生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的视觉-语言模型在应用（如医学视觉问答和报告生成）中易出现幻觉现象，即输出看似合理但实际错误的内容。现有方法虽然一定程度上能减少幻觉，但多数涉及二次解码等额外步骤，降低了推理速度，并存在跨模态或与真实内容不匹配等问题。因此，需开发高效且通用的新方法以抵抗幻觉。

Method: 作者提出Med-VCD方法，采用创新的稀疏化策略，实时选择与视觉信息高度相关的token，减少冗余，保留关键视觉上下文，无需二次解码，从而兼顾高效性和可靠性。

Result: 在八个涵盖眼科、放射科、病理等视觉问答、报告生成及幻觉基准测试的医疗数据集上评估，Med-VCD相比基础医学LVLMs提升了平均13%的事实准确率，以及6%的幻觉准确率。

Conclusion: Med-VCD在减少医学视觉-语言模型幻觉、提升事实准确性的同时，也保证了推理效率，有望为医疗AI应用带来更高的安全性和实用性。

Abstract: Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.

</details>


### [225] [Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory](https://arxiv.org/abs/2512.01934)
*Chenyi Wang,Yanmao Man,Raymond Muller,Ming Li,Z. Berkay Celik,Ryan Gerdes,Jonathan Petit*

Main category: cs.CV

TL;DR: 提出了一种针对多目标跟踪（MOT）系统的新型物理空间在线ID操控攻击AdvTraj，可以用对抗性轨迹实现ID转移，迷惑跟踪系统。


<details>
  <summary>Details</summary>
Motivation: 现有对MOT的攻击多局限于对个别目标劫持或只攻击集成检测模块，且攻击方式特定、鲁棒性差、仅适用于离线数据集。亟需揭示在真实物理和在线场景下，MOT系统的身份分配环节的脆弱性。

Method: 提出AdvTraj攻击，攻击者通过设计对抗性轨迹，将自身ID转移到目标对象，实现对ID分配的干扰，无需直接攻击目标检测模块。攻击在CARLA仿真环境中实施，并评估白盒和黑盒场景下不同MOT算法的攻击成效，分析形成人工可操作的通用轨迹模式。

Result: 在白盒攻击SORT算法下，ID分配混淆的成功率达到100%；对多种先进MOT算法具有高攻击迁移性，最高达93%。提出了两种可由人实现的通用对抗运动。

Conclusion: AdvTraj揭示了当前MOT系统在目标关联阶段存在的未被充分关注的安全隐患，强调了提升这类系统鲁棒性的必要性，并为未来安全防护措施提供了理论与实践依据。

Abstract: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.

</details>


### [226] [Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2512.01949)
*Zhongyu Yang,Dannong Xu,Wei Pang,Yingfang Yuan*

Main category: cs.CV

TL;DR: 提出了一种无需重训练、可泛化于多种多模态大模型（MLLMs）的可插拔式可视令牌剪枝方法Script，可显著提升推理效率并减少内存消耗，同时保持较高任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型，在处理高分辨率图像和视频时视觉令牌数量激增，导致显存占用和延迟严重。已有剪枝方法要么忽略了与用户查询的相关性，要么受限于注意力机制，限制了适应性和实用性，因此亟需一种更高效、泛用且与查询强相关的剪枝方案。

Method: Script包含两个模块：1）图结构剪枝模块，去除视觉冗余令牌；2）查询条件化语义剪枝模块，保留与用户查询密切相关的信息。整个方法为可插拔式，无需对大模型额外训练或微调，并可适配不同的MLLM架构。

Result: 在十四个跨图像和视频理解的基准上，Script较现有剪枝方法在模型推理效率和预测准确性上都取得了更优结果。例如在LLaVA-NeXT-7B模型上，Script实现了最高6.8倍预填速度提升和10倍FLOP减少，同时保持了96.88%的原始性能。

Conclusion: Script有效解决了大模型高效处理高分辨率视觉输入时的存算瓶颈，可广泛提升多模态大模型的适用性和推理效率，且无需重新训练，实用性强。

Abstract: The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.

</details>


### [227] [SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation](https://arxiv.org/abs/2512.01960)
*Zisu Li,Hengye Lyu,Jiaxin Shi,Yufeng Zeng,Mingming Fan,Hanwang Zhang,Chen Liang*

Main category: cs.CV

TL;DR: SpriteHand提出了一种可实时生成复杂手-物交互视频的自回归方法，能真实并连贯地模拟多种物体的动态交互，显著优于现有生成或物理引擎基线。


<details>
  <summary>Details</summary>
Motivation: 当前主流物理引擎难以精准模拟手与非刚性或复杂结构物体的动态交互，如织物、弹性体、毛类表面等，缺乏灵活性和真实性。

Method: 提出SpriteHand框架，通过自回归视频生成和因果推断架构，以及混合后训练增强视觉效果和时序一致性。输入为静态物体图像和手部假想动作视频流，实时生成虚拟手与物体的交互视频。

Result: 1.3B参数的模型在单张RTX 5090 GPU上实时生成，18FPS、640x368分辨率，150ms延迟，支持持续1分钟输出。实验显示在视觉质量、物理合理性和交互真实度上优于生成式方法和传统引擎。

Conclusion: SpriteHand为跨多种物体类型和动作模式的手-物交互建模与合成提供了高效而真实的新方案，具有较好的现实应用前景。

Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.

</details>


### [228] [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](https://arxiv.org/abs/2512.01975)
*Xu Zhang,Jin Yuan,Hanwang Zhang,Guojin Zhong,Yongsheng Zang,Jiacheng Lin,Zhiyong Li*

Main category: cs.CV

TL;DR: 本文提出了一项新任务——图像协作分割与描述（SegCaptioning），不仅用简单提示（如包围框）生成多个语义丰富的文字与分割掩码对，还提出了场景图引导扩散模型（SGDiff）实现此目标。新方法能更灵活地满足用户需要，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像语义理解任务（如图像描述、分割）要求用户输入提示且输出单一且信息有限，提示输入成本高，结果不灵活，难以满足多样化需求。为此，作者希望设计一种能用简单提示生成多样化分割-描述结果的任务和方案。

Method: 1. 定义新任务SegCaptioning：输入简单提示（如包围框），输出多个（描述，分割掩码）对。2. 构建场景图引导的扩散模型（SGDiff）：
- 使用Prompt-Centric Scene Graph Adaptor将用户提示转为场景图，精准捕捉意图。
- 采用Scene Graph Guided Bimodal Transformer在扩散预测过程中捕捉掩码和描述之间的关联。
- 引入多实体对比损失确保图像与文本实体对齐。

Result: 在两个数据集上大规模实验，所提SGDiff模型在SegCaptioning任务上显著超越现有分割、描述方法，能够用极少提示生成高质量的描述和掩码输出，表现突出。

Conclusion: SGDiff创新性地解决了简单提示高效生成多样语义输出的问题，为交互式视觉理解任务开辟新方向。在图像分割与描述场景下具普适价值，也为相关多模态任务研究提供启发。

Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.

</details>


### [229] [Artemis: Structured Visual Reasoning for Perception Policy Learning](https://arxiv.org/abs/2512.01988)
*Wei Tang,Yanpeng Sun,Shan Zhang,Xiaofan Li,Piotr Koniusz,Wei Li,Na Zhao,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的感知-策略学习框架Artemis，该方法通过结构化、基于提议的视觉推理替代纯语言中间推理，有效提升了视觉感知任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习视觉感知方法越来越采用自然语言表达的中间推理链，但实验证明这类纯语言推理常常导致感知任务性能下降。作者认为，问题不在于推理本身，而在于推理的表达形式：视觉任务本质上需要空间和对象中心化的推理，而不是无结构的语义推理。

Method: 作者提出Artemis框架，引入结构化、基于提议的推理方式，将每一步中间状态表示为(label, bounding-box)对，捕捉可验证的视觉状态。每一步都可显式追踪、可直接监督，提高提议质量，规避了语言推理的不确定性。Artemis基于Qwen2.5-VL-3B实现。

Result: Artemis在grounding和检测任务上表现突出，同时在计数与几何感知任务上表现出强泛化能力。整体上比纯语言中间推理方法有持续提升。

Conclusion: 空间结构化推理优于仅语义推理，能促进大规模、通用视觉感知策略学习。Artemis也在多模态大模型(MLLM)基准测试中展现强竞技力，验证了其视觉推理效果。

Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.

</details>


### [230] [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989)
*Fengzhe Zhou,Jiannan Huang,Jialuo Li,Deva Ramanan,Humphrey Shi*

Main category: cs.CV

TL;DR: 提出了一个统一的基准（PAI-Bench）来系统性评估物理智能方向感知与预测能力，发现现有多模态大模型和视频生成模型在物理一致性和预测任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型和视频生成模型快速发展，但它们在感知和预测现实物理动态方面能力尚不清晰，需要一个全面评估的标准。

Method: 作者设计了PAI-Bench，涵盖视频生成、条件视频生成、视频理解三大任务，基于2808个真实世界案例，并使用度量物理可行性和领域推理能力的任务对齐指标对模型进行评估。

Result: 实验证明现有视频生成模型虽然视觉效果好，但难以保持物理动态一致性；多模态大模型在预测和因果解释方面表现有限。

Conclusion: PAI-Bench为物理智能的评测奠定了现实基础，同时揭示了当前模型在感知和预测物理世界任务中的关键差距，指出了今后改进的方向。

Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.

</details>


### [231] [Learning Visual Affordance from Audio](https://arxiv.org/abs/2512.02005)
*Lidong Lu,Guo Chen,Zhu Wei,Yicheng Liu,Tong Lu*

Main category: cs.CV

TL;DR: 本论文提出音频-视觉可供性定位（AV-AG）任务，通过分析动作声音来分割物体的交互区域，展现了音频在线、丰富的交互理解优势，并提出新数据集和融合模型，效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖文本或视频，易受表达歧义和视觉遮挡影响，缺乏对物体交互区域直观、实时的识别手段。音频信号具备独立于视觉且语义丰富的特点，能够弥补上述不足。

Method: 构建了首个AV-AG数据集，包含大量动作声音、物体图像及像素级交互区注释，并设计涵盖未见类别用于零样本泛化评测。提出AVAGFormer模型，通过语义条件跨模态混合器和双头解码器高效融合音频视觉特征，实现掩码预测。

Result: AVAGFormer在AV-AG任务上获得最优效果，显著超越相关任务的基线模型。实验全面分析了AV-AG与现有音视分割任务（AVS）的区别、端到端建模优势及各组件贡献。

Conclusion: AV-AG任务和新数据集推动了音频驱动的物体可交互区理解，AVAGFormer模型为音视融合分割提供了有竞争力的框架，显著提升了基于音频的可交互性理解水平。

Abstract: We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.

</details>


### [232] [MV-TAP: Tracking Any Point in Multi-View Videos](https://arxiv.org/abs/2512.02006)
*Jahyeok Koo,Inès Hyeonsu Kim,Mungyeom Kim,Junghyun Park,Seohyun Park,Jaeyeong Kim,Jung Yi,Seokju Cho,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了MV-TAP，一种基于多视角信息的点追踪方法，在多视角视频中实现更准确和全面的点轨迹估计，并在多项基准测试上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着多视角相机系统的广泛应用，如何在动态场景下准确追踪目标点成为核心挑战，现有方法在跨视角和动态性的处理上存在不足，亟需更强大的多视角追踪技术作为支持。

Method: 提出MV-TAP点追踪器，结合摄像机几何信息与跨视角注意力机制，对多视角视频的时空信息进行聚合，实现点在多视角、多时刻之间的准确追踪，并构建了专门的合成训练集和真实评估集。

Result: 大量实验表明，MV-TAP在具有挑战性的点追踪基准上表现优越，优于当前主流点追踪方法。

Conclusion: MV-TAP为多视角点追踪研究领域建立了一个有效的基线，有助于推动相关研究和实际应用的发展。

Abstract: Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.

</details>


### [233] [AirSim360: A Panoramic Simulation Platform within Drone View](https://arxiv.org/abs/2512.02009)
*Xian Ge,Yuling Pan,Yuhang Zhang,Xiang Li,Weijun Zhang,Dizhe Zhang,Zhaoliang Wan,Xin Lin,Xiangkai Zhang,Juntao Liang,Jason Li,Wenjie Jiang,Bo Du,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: 该论文提出了AirSim360，一个面向无人机视角的全景仿真平台，能够进行多场景采样和多种任务实验，填补了大规模、异质全景数据不足的短板。


<details>
  <summary>Details</summary>
Motivation: 当前360度全景感知领域缺乏大规模且多样化的数据，限制了空间智能的进一步发展。

Method: 提出AirSim360平台，涵盖：（1）对齐渲染的数据标注方式，支持像素级几何、语义、实体理解；（2）可交互的行人行为建模系统；（3）自动化的无人机轨迹生成体系。

Result: 采集了6万多组全景样本，并在多项任务上进行了实验，展示了平台的有效性和广泛适用性。同时，首次实现了全景下对真实世界4D系统系统性建模。

Conclusion: AirSim360在全景仿真、数据采集与任务测试上均可创新，对推动空间智能领域发展具有重要意义，并将开放全部工具与数据集。

Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.

</details>


### [234] [Improved Mean Flows: On the Challenges of Fastforward Generative Models](https://arxiv.org/abs/2512.02012)
*Zhengyang Geng,Yiyang Lu,Zongze Wu,Eli Shechtman,J. Zico Kolter,Kaiming He*

Main category: cs.CV

TL;DR: 本文对MeanFlow（MF）一步生成模型进行改进，提出了iMF方法，在ImageNet 256x256上取得了1.72 FID分数，性能优于同类方法，且无需蒸馏。


<details>
  <summary>Details</summary>
Motivation: 原始MeanFlow模型在训练目标和引导机制上存在挑战，比如训练目标依赖网络自身，且在引导时缺乏灵活性。

Method: 1. 重新表述训练目标，将其转化为对瞬时速度v的损失回归，并用网络预测平均速度u，提高训练稳定性。2. 在引导机制上，通过将条件作为显式变量来保留推理时的灵活性，实现了高效的条件控制。3. 采用in-context conditioning减少模型规模，提高性能。

Result: 提出的iMF方法在ImageNet 256x256数据集上，通过一次函数评估（1-NFE）即可实现1.72的FID分数，大幅超过其他同类模型，并接近多步生成方法。

Conclusion: iMF方法显著提升了一步生成模型的性能和灵活性，有望推动快速生成范式成为主流。

Abstract: MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.

</details>


### [235] [TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models](https://arxiv.org/abs/2512.02014)
*Zhiheng Liu,Weiming Ren,Haozhe Liu,Zijian Zhou,Shoufa Chen,Haonan Qiu,Xiaoke Huang,Zhaochong An,Fanny Yang,Aditya Patel,Viktar Atliha,Tony Ng,Xiao Han,Chuyan Zhu,Chenyang Zhang,Ding Liu,Juan-Manuel Perez-Rua,Sen He,Jürgen Schmidhuber,Wenhu Chen,Ping Luo,Wei Liu,Tao Xiang,Jonas Schult,Yuren Cong*

Main category: cs.CV

TL;DR: TUNA提出了一种统一的多模态模型结构，通过级联VAE编码器和表征编码器，实现了端到端的图像和视频理解与生成，在多个任务上获得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）通常采用分离的表征，导致在理解与生成任务间表征格式不匹配，影响性能。作者希望构建一种真正统一且高效的表征空间，实现图像与视频的无缝理解与生成，并提升模型的整体表现。

Method: TUNA通过级联变分自编码器（VAE）编码器和表征编码器，构建出统一的连续视觉表征空间，实现端到端的图像和视频多模态任务。在该体系下，同时训练理解与生成数据，使两类任务互相促进。

Result: TUNA在图像、视频的理解、生成及编辑等多模态任务基准上取得了最新的最优结果，并且实验显示更强大的预训练表征编码器能进一步提升多模态任务的性能。

Conclusion: TUNA的统一表征结构在理解与生成任务中均展现了优越性与可扩展性，双任务联合训练下任务无冲突且相互提升，证明了统一空间设计的有效性。

Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.

</details>


### [236] [Generative Video Motion Editing with 3D Point Tracks](https://arxiv.org/abs/2512.02015)
*Yao-Chih Lee,Zhoutong Zhang,Jiahui Huang,Jui-Hsien Wang,Joon-Young Lee,Jia-Bin Huang,Eli Shechtman,Zhengqi Li*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D轨迹的V2V方法，实现了精确且一致的视频中相机与物体运动的联合编辑。


<details>
  <summary>Details</summary>
Motivation: 虽然相机和物体运动对视频叙事至关重要，但复杂场景下对这类运动的精确编辑一直非常困难。目前主流的方法要么缺乏整体场景一致性，要么只能做基础的位置变化，难以精细操控对象运动。

Method: 作者提出track-conditioned V2V框架：用源视频和配对的3D点轨迹（代表源和目标运动）作为模型输入。3D轨迹提供稀疏对应，能够传递丰富上下文并保证空间和时间上的一致性，同时还可利用深度信息处理遮挡等复杂情况。模型通过分阶段在合成和真实数据上训练。

Result: 模型不仅能实现相机与物体运动的联合操控，还支持运动转移与非刚性变形等多样化编辑，且空间和时间一致性良好，能产生更有创意的视频编辑效果。

Conclusion: 利用3D点轨迹为条件输入的视频生成模型，相较已有方法，在精细运动编辑、遮挡处理和创新性编辑等方面展现出明显优势，有望推动视频编辑技术向更精确、更灵活的方向发展。

Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.

</details>


### [237] [Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now](https://arxiv.org/abs/2512.02016)
*Varun Varma Thozhiyoor,Shivam Tripathi,Venkatesh Babu Radhakrishnan,Anand Bhattad*

Main category: cs.CV

TL;DR: 论文评估了视频生成模型在物理世界建模能力方面的表现，发现其在表现重力定律时存在偏差，并提出了一种新方法对其进行更精确测试，同时通过轻量微调取得了物理规律学习的提升。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成器越来越多地被视为潜在的世界模型，其是否能编码和理解物理定律成为关键能力。尤其是重力这样基本的物理规律，是衡量其世界建模能力的重要指标。然而，目前这种能力的评测往往受度量尺度模糊性的影响，导致难以判断模型的真实物理建模水平。

Method: 作者首先分析了视频生成模型在物体下落情境中表现出的物理偏差，并用单位无关的双球方案$t_1^2/t_2^2=h_1/h_2$来规避传统测试中尺度、机位等变量带来的干扰，从而更严谨地检测模型是否学会了伽利略等效原理。随后对模型进行了针对性轻量微调，利用100个单球下落视频，通过低秩适配器提升模型表现，并测试其迁移泛化能力。

Result: 实验证明，主流视频生成模型在“默认”状态下未能有效再现现实中的重力，且标准的时间重标定无法消除这一偏差，新方案下依然存在对伽利略等效原理的违反。通过轻量微调后，模型学习到的“有效重力”显著提升，并能零样本泛化到其他物理场景。

Conclusion: 作者证明了当前视频生成模型对基础物理规律学习存在缺陷，但这种缺陷可以用极少量物理场景样本和轻量参数调整显著缓解。这为后续研究如何让世界模型精准捕捉物理规律提供了实证和技术路径。

Abstract: Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [238] [Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis](https://arxiv.org/abs/2512.00046)
*Angelina Parfenova,Andreas Marfurt,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本研究探讨如何通过大语言模型（LLMs）来自动化定性数据分析中的归纳编码流程，并与人类专家进行了比较。研究揭示人类与LLMs在编码难易度上表现出不同趋势。


<details>
  <summary>Details</summary>
Motivation: 以往的定性数据分析多依赖于有预设标签的演绎方法，缺乏对标签自数据中归纳生成（归纳编码）自动化的研究，有望提升分析效率与客观性。

Method: 对六种开源LLMs在归纳编码任务中的表现进行评估，并邀请人类专家对同一数据进行编码与难度打分，分析两者的差异。同时比较两者与测试集标准答案（golden standard）之间的偏差。

Result: 人类编码者在处理复杂句子时表现优异，而在处理简单句子时表现较弱；LLMs 则相反。此外，人类标签有时偏离标准答案但更受专家好评，部分LLMs则更接近标准答案但得分较低。

Conclusion: LLMs可用于自动化数据归纳编码，但在表现和专家评价上与人类存在差异，未来需进一步改进模型以平衡标签准确性与可解释性。

Abstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.

</details>


### [239] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本文通过大规模模拟多个LLM协作讨论任务，研究了其在黑箱设定下的协调和互动机制，发现了群体协作中的语义压缩、协同趋同、以及影响力不对称等现象，并提出了一套新的行为分析指标。此工作展示了分析黑箱互动有助于理解大模型群体的协作策略。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）越来越多地被用于协作环境，但对于这些“黑箱”代理如何在群体中协调和达成共识的过程仍然知之甚少。本研究旨在填补LLM群体协作机制理解的空白。

Method: 作者模拟了7500场多智能体、多轮次的讨论任务，累积生成超12.5万条讨论话语，并以归纳编码任务为场景。提出并使用一系列过程级度量指标，如代码稳定性、语义自洽、词汇信心、情感分析和收敛性等，以追踪LLM之间的协调动态。此外，分析了LLM输出的向量嵌入几何结构在协作过程中的演变过程。

Result: 结果显示，随着多轮协作的进行，LLM群体在词汇和语义层面上趋同，逐步产生影响力不对称以及协商式行为；嵌入空间的本征维数逐渐降低，提示群体讨论过程中语义信息被压缩。整个过程没有对代理分配明确角色，协商和影响力模式仍然自发出现。

Conclusion: 本文证明，黑箱交互行为分析能有效揭示LLM群体中的协作与协调策略，并为理解和利用大模型间合作提供了一种可扩展的、区别于模型内探针的解释性工具。

Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

</details>


### [240] [Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees](https://arxiv.org/abs/2512.00204)
*Jason Lunder*

Main category: cs.CL

TL;DR: 提出了基于依存句法树的Tree Matching Networks (TMN)，在NLI任务上比BERT有更高效率和更低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer如BERT在自然语言推断（NLI）任务中表现优异，但参数量大、计算资源消耗高。它们从头学习词语间关系，未利用现有的语言结构知识。该文试图证明引入显式的语言结构（如依存句法树）可以提升学习效率及效果。

Method: 作者将Graph Matching Networks (GMN)方法适配到依存句法树，提出Tree Matching Networks (TMN)模型，用于表示句子结构，并在SNLI和SemEval任务上与BERT模型对比。针对结构聚合的可扩展性问题，提出multi-headed attention聚合方法。

Result: 在SNLI蕴含任务上，TMN在显著降低内存和训练时间的情况下，效果优于BERT；在SemEval相似性任务上两者均表现一般。显式结构信息在相同规模下优于纯序列模型。

Conclusion: 显式结构建模（如依存句法树）在NLI等任务上相较于单纯序列模型更高效、表现更佳，但当前聚合方式限制了模型的可扩展性，多头注意机制是未来改进方向。

Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.

</details>


### [241] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen,Tjaša Arčon,Luka Terčon,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文提出利用具代理能力的大型语言模型（LLM），结合代码生成和数据驱动推理，自动化分析带标注语料库，简化语法研究流程，并在多语言语序任务上取得了有希望的初步效果。


<details>
  <summary>Details</summary>
Motivation: 当前的语法实证研究高度依赖数据，但对带标注语料库的系统性分析仍需要大量方法和技术层面的投入，亟需更高效、可解释的自动化研究工具。

Method: 作者提出了一种面向语料库的语法分析代理框架，将自然语言任务解释、代码生成以及数据推理结合，通过大型语言模型对带注释的语料库进行推理分析。以Universal Dependencies多语言语料库和WALS灵感的语法任务为例，测试系统在13类语序特征、170多种语言上的表现。综合评估了主导语序识别准确率、语序覆盖的完整性以及分布保真度三个维度。

Result: 系统在上述多维度评测下，证明LLM可结合结构化语言学数据实现解释性强的自动化语法分析，且效果可行。

Conclusion: 将LLM推理与语料库语言学分析结合是可行的，能够实现解释性、可扩展的自动化语法研究，这为未来语料库语法分析的自动化和规模化实现提供了新的途径。

Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

</details>


### [242] [Minimal-Edit Instruction Tuning for Low-Resource Indic GEC](https://arxiv.org/abs/2512.00219)
*Akhil Rajeev P*

Main category: cs.CL

TL;DR: 本文提出了一种无需数据增强的印地语系语言语法错误纠正方法，基于大语言模型指令微调和保守生成，在官方评测上取得优异成绩，并为未来研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 现有印地语系（Indic languages）语法纠错任务面临语料稀缺、文字多样、形态复杂等挑战。同时，依赖大量数据增强的方法计算量大、可复现性差，因此急需一种高效、易复现的替代方法。

Method: 作者采用12B GEMMA 3大语言模型，通过bnb 4-bit参数高效微调（PEFT），结合Alpaca风格指令格式进行增广无关的指令微调。推理阶段，设计了基于轻量归一化器的保守解码方法，实现尽量少、但有意义的编辑。关键地，使用由确定性错误分类器驱动的语言专属prompt，显式指导模型纠错。

Result: 在未经调优的官方GLEU评测中，方法在马拉雅拉姆语上取得92.41分（第六名），在印地语上获81.44分（第三名），显示出较高的纠错能力和较强的计算效率。

Conclusion: 基于分类器提示设计、适配器式指令微调与确定性保守解码，相较于传统数据增强方案，该方法在印地语语法纠错任务中实现了可复现、高效、效果突出的表现。今后可进一步研究更强的形-句法约束和以人为中心的编辑评估。

Abstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.

</details>


### [243] [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)
*Sai Koneru,Matthias Huck,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出了一种将多模态基础模型（MMFM）与专用翻译大语言模型（LLM）融合的端到端多模态翻译新方法——OmniFusion，用于高效的端到端语音、图像到文本翻译任务。


<details>
  <summary>Details</summary>
Motivation: 现有开源文本翻译LLM虽然提高了语言覆盖率和质量，但只能以串联管线形式用于语音翻译，带来额外延迟且无法利用多模态信息（如图像）。现有MMFM具备多模态感知与推理能力，但缺乏多语种翻译能力。为此，亟需一种方法结合多模态理解与高质量翻译能力。

Method: 提出一种新颖的融合策略，将预训练MMFM多个层的隐藏状态连接到翻译LLM，实现端到端联合训练。具体实现为，将Omni 2.5-7B（MMFM）与SeedX PPO-7B（LLM）融合，构建能够处理语音、图像和文本输入的OmniFusion模型。

Result: 实验结果表明，OmniFusion能够有效利用语音和视觉信息，在SimulST（同步语音翻译）任务中比串联管线方法延迟降低1秒，并显著提升翻译质量。

Conclusion: 通过创新的模型融合机制，OmniFusion兼具多模态感知与高质量翻译能力，显著改善了延迟和最终译文表现，对多模态翻译场景具有重要意义。

Abstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.

</details>


### [244] [Lost without translation -- Can transformer (language models) understand mood states?](https://arxiv.org/abs/2512.00274)
*Prakrithi Shivaprakash,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 该研究评估了大语言模型对印度多种语言下四类心境状态的理解能力，发现直接处理本地语言表现极差，翻译为英文或中文后再处理效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要以英语为中心，且不同语言表达心理健康的方式（“苦难习语”）各异，尚不清楚模型是否能理解印度地区多语种下的心境状态。亟需解决多语言下心理健康识别的能力不足问题。

Method: 收集了11种印度语言下、代表抑郁、心境正常、欣快型躁狂和烦躁型躁狂四种心境状态的247个短语。通过原生/罗马化脚本的直接嵌入、多语种及印度本地模型，以及翻译成英文或中文的嵌入，对比k均值聚类效果。以复合分数（包括ARI、NMI、同质性、完整性）评价表现。

Result: 直接对印度本地语言短语进行嵌入聚类效果极差（分数仅0.002）；通过高质量英文或中文翻译转化后，用Gemini或中文模型聚类效果显著提升（最高分达0.67）。专用的本地模型IndicBERT和Sarvam-M表现也较差。

Conclusion: 现有模型无法直接理解印度地区的多种本地语言表现出的心境状态，严重阻碍在心理诊断和治疗中的实际应用。虽然高质量翻译可部分弥补，但依赖复杂和专有工具不可持续，未来应优先发展面向本地多样语言的模型以推动全球心理健康应用。

Abstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.

</details>


### [245] [EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education](https://arxiv.org/abs/2512.00290)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Jiawei Shen,Zilong Li,Yidan Liang*

Main category: cs.CL

TL;DR: 本文提出了EduEval，一个用于评价大语言模型（LLMs）在中国K-12教育任务中的综合层次化基准数据集，系统评估了多种LLM在不同认知任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育领域展现出巨大潜力，但未经严格评估的直接应用可能损害教育质量，因此亟需针对中国K-12教育制定系统性评测标准和方法。

Method: 提出了EduEval基准，包括：1）设计结合Bloom与Webb知识深度的六维认知能力分类体系（EduAbility）；2）选取真实考试题、课堂对话、学生作文和专家设计问题，保证任务真实性；3）涵盖小学到高中24类任务、11,000多道题。评测主流14个LLM，分别采用zero-shot和few-shot设置。

Result: LLMs在事实性任务表现出色，但在课堂对话分类和创意内容生成方面效果不佳。部分开源模型在复杂推理任务上优于专有模型。few-shot提示对不同认知任务提升效果不一。

Conclusion: EduEval为LLM在中国教育场景的应用提供了有针对性的评价指标，有助于指导后续专用于教育任务的LLM优化和开发。

Abstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.

</details>


### [246] [Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets](https://arxiv.org/abs/2512.00323)
*Muhammad Muneeb,David B. Ascher,Ahsan Baidar Bakht*

Main category: cs.CL

TL;DR: 本文基准测试了Hugging Face平台上的47个基于上下文的问题回答（CBQA）模型，在8个数据集上评估其表现。


<details>
  <summary>Details</summary>
Motivation: 用户在实际应用中常用CBQA模型进行有上下文的信息检索和问答，避免每次针对不同数据集重复微调模型，提高实现效率。作者希望找出综合表现最好的CBQA模型。

Method: 直接测试47个主流CBQA模型（未做进一步微调），在八个不同任务和领域数据集上对比性能。此外，尝试用遗传算法融合多个模型的结果优化准确率。

Result: 最佳模型为ahotrod/electra_large_discriminator_squad2_512，在所有数据集上平均准确率为43％，部分领域数据集甚至达到96.45％。此外，计算时间与上下文长度和模型大小相关，答案越长表现通常越差，复杂上下文会影响模型表现。

Conclusion: 选用合适预训练模型可提高CBQA任务的适用性，无需频繁微调。模型融合有助于进一步提升精度。结果为实际部署提供有意义的模型选择依据。

Abstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.

</details>


### [247] [Evidence-Guided Schema Normalization for Temporal Tabular Reasoning](https://arxiv.org/abs/2512.00329)
*Ashish Thanga,Vibhu Dixit,Abhilash Shankarampeta,Vivek Gupta*

Main category: cs.CL

TL;DR: 本论文提出了一种面向SQL的临时推理方法，通过改进表模式设计，显著提升了对维基百科信息框数据的问答系统精度。


<details>
  <summary>Details</summary>
Motivation: 现有QA系统在处理随时间变化的半结构化表格时表现有限，需要探索更有效的处理和推理机制，尤其是针对实体随时间演变的复杂问题。

Method: 作者设计了三步流程：1）从维基百科信息框自动生成3NF数据库模式；2）基于这些模式生成SQL查询；3）执行SQL查询得出答案。研究中特别对比了不同数据库模式设计、命名方式及时间锚定的效果，形成三项基于实验的数据表结构设计原则。

Result: 在WikiTable数据集实验中，采用最佳结构（Gemini 2.5 Flash模式+Gemini-2.0-Flash查询），问答精度（EM）达到80.39，比传统基线高出近17%。结果证明了表结构设计的决定性作用。

Conclusion: 优良的数据表结构设计，比大模型扩容更能提升临时推理类问答系统的效果。规范化、语义清晰命名与一致的时间锚定是提升精度的关键。

Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).

</details>


### [248] [Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents](https://arxiv.org/abs/2512.00332)
*Daud Waqas,Aaryamaan Golthi,Erika Hayashida,Huanzhi Mao*

Main category: cs.CL

TL;DR: 本文提出了一种针对多轮工具调用大模型的全新评测范式A-CC（Assertion-Conditioned Compliance），并揭示了此类模型在面对误导性断言时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用大模型广泛应用于AI助手，但在实际安全关键场景中，模型在多轮会话的稳健性和抗干扰能力仍受质疑，缺乏精细的鲁棒性评测方法。标准基准主要关注单轮而非多轮对话，不能有效检测真实风险。

Method: 提出A-CC评测框架，分别注入两类误导断言：一是来源于用户的偏见性断言（USAs），二是来源于系统的矛盾政策断言（FSAs），并设计综合性指标评估模型在这些情况下的表现。

Result: 实验发现，当前主流多轮工具调用大模型对于用户导向的偏见性断言和系统导向的政策性矛盾都高度脆弱，容易出现迎合谬误或错误服从现象。

Conclusion: A-CC有效揭示了多轮工具调用LLM在现实部署中隐藏的脆弱点，未来安全关键应用需重视此类风险，并采用更细致的鲁棒性评估与提升措施。

Abstract: Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.

</details>


### [249] [IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages](https://arxiv.org/abs/2512.00333)
*Ayush Maheshwari,Kaushal Sharma,Vivek Patel,Aditya Maheshwari*

Main category: cs.CL

TL;DR: 论文提出并发布了IndicParam多项选择题基准，涵盖11种低及极低资源印度语言，对现有19种大语言模型在这些语言上的能力进行系统评估，发现整体准确率普遍较低。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在高资源多语言任务上表现优异，但对低和极低资源的印度语言的评估严重不足。为促进低资源语言的研究与应用，需要建立高质量且具有挑战性的基准。

Method: 作者人工整理了包含13,000余道多项选择题的IndicParam数据集，覆盖11种低与极低资源印度语言，并包含Sanskrit-English混合样本。对19个主流语言模型（包括开源和商业模型）在该基准上进行评测，细分知识性与语言性问题，并考察列表匹配、断言-理由、顺序排序等多种题型。

Result: 即使是表现最好的GPT-5平均准确率也仅为45%，其他模型如DeepSeek-3.2和Claude-4.5分别为43.1%和42.7%。所有模型在低资源印度语言上的整体表现有限，展示出跨语言迁移的挑战。

Conclusion: IndicParam深刻揭示了当前大语言模型在低资源印度语言学习和推理方面的不足，为未来模型优化、资源补充和跨语种迁移提供了重要参考和基准。

Abstract: While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.

</details>


### [250] [CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA](https://arxiv.org/abs/2512.00360)
*Vsevolod Kovalev,Parteek Kumar*

Main category: cs.CL

TL;DR: 本文提出了一个针对教育讲座视频的时间标注问答系统，并发布了CourseTimeQA数据集。同时，提出了CrossFusion-RAG轻量级跨模态检索器，在单GPU下提高了检索准确率并保证低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着教育视频内容的增加，用户希望能高效地针对讲座视频提出自然语言问题并获得时间精确的相关回答，然而受单GPU资源（延迟、显存限制）困扰，现有方案难以高效检索并生成基于视频内容的回答。

Method: 作者提出了CourseTimeQA数据集，包含六门课程、52.3小时视频和902个带查询的数据。构建了CrossFusion-RAG检索系统，特征包括冻结编码器、学习的视觉投影、浅层跨模态注意力结合ASR及视频帧、时间一致性正则项，以及体积很小的跨注意力再排序器，实现了高效视频查询与答案生成。

Result: 在CourseTimeQA任务上，CrossFusion-RAG在单A100显卡下，nDCG@10提升0.10、MRR提升0.08（对比BLIP-2），中位延迟仅1.55秒。在零样本CLIP、文本混合、晚期融合等多基线下实现最优性能，同时报告了针对ASR噪音和时间定位的稳健性分析。

Conclusion: CrossFusion-RAG兼顾高效率与准确率，适用于单GPU教育视频问答任务。论文还提供完整的实现细节和调优说明，有助于后续工作的可复现性与对照。

Abstract: We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.

</details>


### [251] [Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing](https://arxiv.org/abs/2512.00390)
*Nuo Chen,Hanpei Fang,Jiqun Liu,Wilson Wei,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在相关性标注任务中易受前置判断（priming）影响，并探讨通过模拟不同大五人格特质来减缓这种偏差。结果显示，高开放性和低神经质人格能够有效减轻priming效应，且不同模型、任务下最佳人格配置可能不同，提出采用人格提示法（personality prompting）的新方法。


<details>
  <summary>Details</summary>
Motivation: LLM正被用于数据标注工作，但在相关性标注中易受到“priming”影响，也就是前面的判断会影响后续判断。心理学表明人格会影响偏见，但尚不清楚LLM模拟出的人格是否也影响priming。因此，研究动机在于结合心理学理论，探索通过调整人格提示，提升LLM标注的客观性和可靠性。

Method: 作者使用TREC 2021和2022 Deep Learning Track数据集，对多个LLM分别应用大五人格中的不同特质提示（如高开放性、低神经质等），分析模拟出的人格对相关性标注中priming偏差的影响，并比较不同模型、不同任务下减弱偏差的最有效人格。

Result: 实验证明，一些人格特质如高开放性和低神经质能持续有效地降低LLM在相关性标注任务中的priming倾向。但不同模型和任务中，最优的人格特质可能不一样。

Conclusion: 作者提出了通过人格提示（personality prompting）来减弱LLM标注中的priming偏差，这一方法在缓解认知偏见方面有效。该研究将心理学理论与LLM应用结合，为提升自动化评测的客观性提供了新思路。

Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.

</details>


### [252] [A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction](https://arxiv.org/abs/2512.00392)
*Damian Heywood,Joseph Andrew Carrier,Kyu-Hong Hwang*

Main category: cs.CL

TL;DR: 本研究开发了一个基于大语言模型的英语写作错误分析系统，能识别、分类并纠正多种写作错误，并实现了比传统评分更详细的反馈。


<details>
  <summary>Details</summary>
Motivation: 当前英语写作评估工具反馈类别有限，难以为学习者提供深入的错误分析；利用AI可提升评估效率和精细度。

Method: 系统通过调用Claude 3.5 Sonnet和DeepSeek R1等大语言模型，结合Corder、Richards、James等学者的语言学理论，构建细致的错误分类体系，并以Python实现API调用，对单词和句子层面的拼写、语法、标点等错误进行自动识别和反馈。先在孤立错误样本上优化分类体系，再用具有真实错误的文本进行全面测试。

Result: 系统能自动识别多种常见和复杂的写作错误，给出细致的反馈，但在理解上下文和面对新类型的错误时有不足，有时会新创未定义类别。

Conclusion: AI能够提升英语写作教学的自动化与细致化，对EFL（以英语为外语）教学有变革意义，但仍需增强上下文判断能力并完善错误分类，以支持更高层次的写作分析。

Abstract: This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.

</details>


### [253] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo,Suozhi Huang,Zixin Yao,Yifan Zhang,Yifu Lu,Jiashuo Liu,Zihao Li,Yanyan Deng,Qixin Xiao,Jia Tian,Kanghong Zhan,Tianyi Li,Xiaochen Liu,Jason Ge,Chaoyang He,Kaixuan Huang,Lin Yang,Wenhao Huang,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文提出了CryptoBench，这是首个由专家策划、动态更新，用于严格评估大语言模型（LLM）在加密货币领域实际能力的基准。该基准包括每月50个高专业性问题，分为数据检索和预测四个种类，能细致评估模型的检索与分析能力。评测发现，目前LLM在信息检索强于预测，揭示了“检索-预测不平衡”问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准多为通用场景，不能反映加密货币分析领域的独特挑战，如信息高度时效性、对抗复杂性和多源数据融合需求。因此，有必要设计专门针对加密领域、能够动态反映实战需求的评测基准。

Method: 作者构建了CryptoBench基准，模拟真实分析师工作流，由行业专家设计月度问题，并将任务细分为‘简单检索、复杂检索、简单预测、复杂预测’，以全面覆盖数据获取和深度分析两类能力，用这些任务评测多种LLM及其代理系统。

Result: 在对10种LLM及其代理系统的测试中，发现模型在检索任务上表现较好，但在预测任务上普遍较弱，尤其是在需要信息综合与深度分析时表现不足，形成了显著的‘检索-预测不平衡’现象。

Conclusion: CryptoBench为评测和提升LLM在加密货币等高强度领域的能力提供了更严苛、更系统性的试金石。实验表明，当前LLM虽能检索事实，但对复杂分析和预测的能力仍明显不足，有赖后续进一步优化。

Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

</details>


### [254] [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)
*Yang Xiao,Chunpu Xu,Ruifeng Yuan,Jiashuo Wang,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出了一种推理时动态分配算力的新方法SCALE，有效提升了大模型数学推理能力，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理时均匀分配算力，导致复杂子问题得不到足够关注，而简单操作反而浪费资源，进一步增加算力难以带来效果提升。作者希望通过更智慧的资源分配方式提升性能和效率。

Method: 提出SCALE框架：1）将问题分解为顺序子问题；2）评估各子问题难度；3）将简单子问题分配给高效的“System 1”，复杂子问题交由深度处理的“System 2”；4）顺序执行并传递上下文，以实现资源的动态优化分配。该方法受到双系统认知理论启发。

Result: SCALE在充分利用资源的同时，提升了推理准确率。实验证明，在AIME25等数据集上，准确率较均匀分配基线提升了最多13.75个百分点（57.50%提升到71.25%），同时计算成本降低33%-53%。

Conclusion: SCALE通过针对子问题难度动态分配算力，显著突破了现有推理均匀扩展方式的效率与能力瓶颈，代表了推理时算力分配的重要进步。

Abstract: Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.

</details>


### [255] [CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning](https://arxiv.org/abs/2512.00496)
*Diego A. B. Moreira,Alef I. Ferreira,Jhessica Silva,Gabriel O. dos Santos,Gustavo Bonil,João Gondim,Marina dos Santos,Helena Maia,Simone Hashiguti,Nádia da Silva,Carolina Scarton,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 提出了CACARA，一种高效扩展多模态和多语言能力的新架构，通过单语对齐学习，无需全量重训即可整合新模态，并支持100多种语言，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型扩展新模态或新语言时需大量重训，计算资源消耗大，效率低，亟需一种资源友好且高效的模型扩展方法。

Method: 提出CACARA架构，采用emergent alignment learning方法，仅在与英文对齐的数据上微调新模态，无需对文本编码器做多语言预训练或微调，实现新模态和多语言无缝集成。

Result: 使用提出的方法，在100多种语言上支持多模态任务，且在R@1音频-文本检索上提升了最大14.24个百分点，超越现有SOTA，同时大幅减少训练成本。

Conclusion: CACARA实现了高效、低成本的多模态和多语言扩展方案，显著降低了训练资源需求，为多模态、多语言模型的广泛应用提供了新思路。

Abstract: As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.

</details>


### [256] [G-KV: Decoding-Time KV Cache Eviction with Global Attention](https://arxiv.org/abs/2512.00504)
*Mengqi Liao,Lu Wang,Chaoyun Zhang,Zekai Shen,Xiaowei Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Huaiyu Wan*

Main category: cs.CL

TL;DR: 本文提出G-KV方法，通过全局评分机制优化LLM的KV缓存压缩，有效提升推理效率，并利用后训练技术进一步优化模型。


<details>
  <summary>Details</summary>
Motivation: 当下大模型长序列推理面临巨大的计算和内存压力，现有KV缓存压缩方法过于局部，无法充分衡量token长远重要性。

Method: 作者设计了G-KV，一种结合局部和历史注意力分数的全局评分机制，用于更精准地清除KV缓存中的低价值token。同时，引入了强化学习和知识蒸馏的后训练方案，使模型更适应被压缩后的KV缓存状态。

Result: G-KV方法在多个推理任务和数据集上均显著提升了计算效率和内存利用率，并保持或提升了模型性能。

Conclusion: G-KV为LLM推理中的KV缓存管理提供了一种更优解决方案，有效缓解了长序列带来的计算瓶颈，为实际大规模部署提供了可行路径。

Abstract: Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.

</details>


### [257] [Developing a Comprehensive Framework for Sentiment Analysis in Turkish](https://arxiv.org/abs/2512.00515)
*Cem Rifki Aydin*

Main category: cs.CL

TL;DR: 本论文提出了面向土耳其语和英语的情感分析综合框架，并在多个方面取得了创新方法和显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析研究主要集中在英语等资源丰富语言，针对土耳其语等资源稀缺并形态复杂的语言，相关研究和工具较为稀少，因此需要开发更有效的方法和工具来提升对这些语言的情感分析能力。

Method: 1）整合无监督、半监督和有监督指标生成新型特征集，并将其输入到传统机器学习模型中，与神经网络模型进行了对比；2）提出并应用半监督领域特定方法构建情感极性词典，首次应用于土耳其语语料；3）通过语素级极性判定进行细粒度形态分析；4）为英语构建了结合递归与循环神经网络的新型网络结构；5）开发了结合情感、句法、语义和词汇信息的新型词向量，并创新性将上下文窗口重定义为子句单位。

Result: 上述方法在土耳其语和英语的多个数据集上取得了超越现有神经网络模型的最新效果，显著提升了情感极性判定和相关任务的准确性。

Conclusion: 本论文是截至2020年对土耳其语情感分析最为详细和系统的工作，同时在英语情感分类领域也有显著贡献，相关方法和成果具备较强的泛化能力和拓展应用价值。

Abstract: In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.

</details>


### [258] [Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity](https://arxiv.org/abs/2512.00552)
*Subramanyam Sahoo,Vinija Jain,Saanidhya Vats,Siddharth Mohapatra,Rui Min,Aman Chadha,Divya Chaudhary*

Main category: cs.CL

TL;DR: 该论文发现通过答案准确率评估大语言模型数学推理能力存在局限，提出了一套新的诊断性评估框架，揭示了模型表面表现与实际推理能力的脱节。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型数学推理的评估主要依赖答案准确率，这可能掩盖了模型在逻辑计算上的本质失误。论文旨在提供更细致区分真正推理能力与表面模式匹配的方法。

Method: 作者提出四个互补的评估维度：正向-反向一致性、传递性覆盖、反事实敏感性和扰动鲁棒性，并对Qwen3-0.6B模型在MenatQA数据集上进行案例分析。

Result: 案例表明，尽管模型答案准确率较高（70%+），但其反向一致性（15%）、传递性覆盖（32.2%）和对扰动的鲁棒性均表现较差，表明其多依赖模式匹配而非真正的数学推理。

Conclusion: 传统准确率指标难以发现模型推理中潜在的根本性问题。所提出的诊断框架是模型无关且可泛化的，有助于推动数学推理能力的可验证评估。

Abstract: Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.

</details>


### [259] [Slovak Conceptual Dictionary](https://arxiv.org/abs/2512.00579)
*Miroslav Blšták*

Main category: cs.CL

TL;DR: 本文提出了首个斯洛伐克语概念性词典工具，以解决该语言机器可读词典数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语作为一种资源稀缺语言，其自动化自然语言处理任务受限于没有大规模机器可读词典或知识库，因而在诸多任务上表现不佳。

Method: 开发了一种新的斯洛伐克语概念性词典，作为首个面向该语言的词典类语言学工具。

Result: 为斯洛伐克语构建了第一个可用于机器处理的大规模概念性词典，填补了相关空白。

Conclusion: 该词典的建立将极大推动斯洛伐克语的自动化及自然语言处理研究，为后续多项任务提供数据保障。

Abstract: When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.

</details>


### [260] [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)
*Alla Chepurova,Aydar Bulatov,Yuri Kuratov,Mikhail Burtsev*

Main category: cs.CL

TL;DR: 本文提出了Wikontic，一种高效的知识图谱（KG）构建流水线，显著提升了KG质量和信息留存率，同时大幅降低了生成成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）主要将知识图谱作为文本检索的辅助工具，忽略了KG本身的质量优化。作者希望探索如何从开放领域文本中高效、准确地构建高质量、结构化的知识图谱，为LLM提供更可靠和精粹的知识支撑。

Method: Wikontic是一种多阶段流水线，包括从文本中抽取含有限定符的候选三元组，基于Wikidata强制类型和关系约束，并对实体进行归一化以减少重复。整个流程保证了KG的紧凑性、一致性和良好的连接性。

Result: Wikontic生成的KG在MuSiQue数据集上，正确答案实体出现在96%的三元组中。仅用三元组信息，模型在HotpotQA上达到76.0分F1，在MuSiQue上59.8分F1，优于或媲美多种需要原始文本检索的生成基线。同时，在MINE-1基准上Wikontic信息保留率达86%，是目前最优，并且构建效率高，输出token数远少于主流方法。

Conclusion: Wikontic极大提升了知识图谱的表达质量和信息覆盖，且构建高效，是从开放文本自动生成高质量、结构化知识的有效方案，为LLM大规模利用KG结构性知识提供了新路径。

Abstract: Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.

</details>


### [261] [Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior](https://arxiv.org/abs/2512.00611)
*Franck Binard,Vanja Kljajevic*

Main category: cs.CL

TL;DR: Prism是一种用于指定工具软件代理行为的小型组合金属语言，强调固定核心与领域可扩展性，便于表达和分析智能体策略。


<details>
  <summary>Details</summary>
Motivation: 现有智能代理语言通常包含零散的控制结构，缺乏统一、可组合的底层机制，不易扩展或分析。Prism旨在用一个固定且最小的核心，结合可扩展的领域语法，提供清晰且可分析的策略描述方式。

Method: Prism围绕固定的最小核心Core1设计，内含基本范畴和组合子，并通过单一抽象操作编写策略。各应用领域通过自定义上下文扩展语法和工具。作者以家庭自动化、电商推荐、医疗监控等实例展示了如何用自然语言决策规则生成可执行且可检查的策略。

Result: Prism能够把不同领域的自然语言决策规则系统性地转化为结构化策略，并且保持了分析性与可执行性。

Conclusion: 在语言学上，Prism区分核心语法与领域词汇，工具作为内外部桥梁；在工程上，Prism提供紧凑且可分析的接口，有助于策略空间的明确、安全和可验证。

Abstract: Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.

</details>


### [262] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

Main category: cs.CL

TL;DR: 本文提出了ART（自适应回应调优）框架，通过多模型竞赛和协作机制，显著提升了大语言模型（LLM）的回答质量。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型给出的回答经常存在不一致、虚构内容（幻觉）及跨领域质量不稳定等问题。

Method: ART框架引入了基于锦标赛风格的ELO排序和多代理推理机制，让多个LLM代理通过竞赛、批判和协作等流程，借助可配置参数、动态代理选择和多种融合策略达成高质量共识输出。

Result: 实验显示，ART在回答准确性、一致性和可靠性上，相较于单模型方法有显著提升；整体质量指标提高了8.4%，ELO收敛R22值超过0.96。

Conclusion: ART框架为需要高质量、经过严格筛选的LLM回答的实际应用提供了可扩展、可生产的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

</details>


### [263] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner,Volker Stocker,Stefan Schmid,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本论文综述了大语言模型（LLMs）中谄媚反应的研究方法，指出了现有方法的不足，并提出未来改进建议。


<details>
  <summary>Details</summary>
Motivation: 近期文献中频繁提及LLM出现谄媚式回答的现象，但相关测量方法存在挑战且对人类感知的评价缺失。作者希望系统梳理现有方法并推动更加科学的研究。

Method: 作者回顾并分析了测量LLM谄媚性的主流方法，总结出五种核心操作方式，并探讨了区分谄媚反应与AI对齐中相关概念的难点。

Result: 研究指出，尽管谄媚性本质上应以人为中心，但目前研究未能有效纳入人类主观评价，对相关概念的区分也面临困境。作者提出了有针对性的改进建议。

Conclusion: 本文强调未来研究应注重人类感知的评估，并理清谄媚性与相似概念的界限，以促进LLM可靠性和可控性方面的发展。

Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

</details>


### [264] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一种将大语言模型生成内容与企业专有知识可视化为交互式知识图谱的方法，以直观识别和缓解模型幻觉，提高其可靠性和质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业应用中常因上下文窗口限制及预训练与现有知识冲突而导致幻觉产生，这些幻觉往往难以被人工察觉和纠正，现有缓解措施成本高且难以确定性保障效果。因此需要更有效直观的幻觉识别和干预手段。

Method: 提出一种将企业专有知识和模型输出有机组织为可视化的互动知识图谱，展示模型结论与知识依据及其置信度。用户可通过该图谱识别模型结论出处、潜在幻觉区域和推理薄弱环节，并可直接给予反馈，形成反馈闭环。

Result: 用户能通过界面对模型输出进行更有效诊断、识别幻觉和错误、并提出针对性反馈，从而实现人机协作提升模型输出质量。

Conclusion: 该框架不仅增强了大模型在企业应用中的可解释性和可靠性，也为持续优化模型表现提供了结构化的反馈通道。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [265] [A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data](https://arxiv.org/abs/2512.00673)
*Breanna E. Green,Ashley L. Shea,Pengfei Zhao,Drew B. Margolin*

Main category: cs.CL

TL;DR: 本文评估了GPT-4在处理含有细微差别语言的数据集分类和注释任务中的表现，并与人类注释者进行对比，发现GPT-4对于复杂语言的分类仍存在明显困难，提示使用时需谨慎。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具如ChatGPT被越来越多的计算社会科学家利用，但其在处理复杂、具有细微语言差别的数据集上的表现尚不清楚，因此需要系统评估其在此类任务中的能力。

Method: 作者选取了一个包含细微差别语言特征的数据集，让GPT-4完成分类和注释任务，并与人类标注者的表现进行对比。实验考察了ChatGPT 3.5、4和4o三个版本，并设计了四种不同的提示方式，通过精确率、召回率和F1分数评价表现，并进行了定量和定性分析。

Result: 实验发现，包含标签定义的提示能够一定程度提升模型表现，但整体来看，GPT-4及此前版本在分类细微差别语言时表现不理想。定性分析进一步揭示了四个具体问题点，说明模型未能准确把握细微语义。

Conclusion: 研究表明，在涉及细微语言差别的分类任务中，GPT-4等大语言模型的表现需持谨慎态度，实际应用中有必要充分评估其适用性。

Abstract: Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.

</details>


### [266] [FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case](https://arxiv.org/abs/2512.00745)
*Md Abdullah Al Kafi,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文提出了一种无需依赖特定语言的Transformer结构的词性标注框架，主要针对资源稀缺语言，已在孟加拉语和印地语上的实验取得了高准确率。只需极少量代码修改即可跨语言迁移，展现了极强的可移植性。


<details>
  <summary>Details</summary>
Motivation: 许多资源稀缺语言在NLP任务中受到现有工具和数据集的限制，缺乏灵活高效的解决方案，因此亟需一种简单可迁移的标注框架以促进这些语言的处理能力提升。

Method: 采用了基于Transformer的模型架构，通过模块化设计，仅需极少代码调整（如仅更改3行），即可将Bangla的模型快速迁移到Hindi，实现不同语言间的高效适配。

Result: 在Bangla和Hindi的词性标注任务中，该框架分别取得了96.85%和97%的token级别准确率，并在数据类别不平衡及语言交叠的情况下保持了较高的F1分数，但在特定的词性类别上仍存在性能差异。

Conclusion: 提出的框架架构易于迁移、开源、模块化，极大降低了模型设计和调参成本，适合低资源语言快速扩展应用，未来重点应聚焦于数据集的精细化和预处理以进一步提升表现。

Abstract: This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.

</details>


### [267] [Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation](https://arxiv.org/abs/2512.00789)
*Xiaodong Cai,Hai Lin,Shaoxiong Zhan,Weiqi Luo,Hong-Gee Kim,Hongyan Hao,Yu Yang,Hai-Tao Zheng*

Main category: cs.CL

TL;DR: 这篇论文提出了一种无需额外超参数的文本生成采样方法（EES），简化了大语言模型的部署，提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文本生成采样策略通常需要引入超参数，这带来了繁琐的参数调优过程，增加了实际部署难度，因此亟需一种无需超参数且表现良好的新方法。

Method: 提出了一种基于信息论的“熵平衡采样（EES）”方法。通过动态平衡归一化熵和概率质量来自动调整候选集合，无需任何新的超参数。

Result: 在不同的推理和生成任务及多种模型架构上进行了实验，结果显示在不同temperature设置下，EES都表现出良好的准确性、一致性和多样性，且具有竞争力。

Conclusion: EES通过消除超参数调优需求，极大简化了大语言模型文本生成的部署，同时在多种任务上提升了生成性能。

Abstract: Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES

</details>


### [268] [Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy](https://arxiv.org/abs/2512.00829)
*Md Mehrab Hossain Opi,Sumaiya Khan,Moshammad Farzana Rahman*

Main category: cs.CL

TL;DR: 本文探讨了通过自动混合精度（AMP）训练提升Bengali（孟加拉语）NLP模型训练效率的方法，实现了训练速度和内存消耗的显著提升，同时不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: Bangla（孟加拉语）NLP发展受限于计算硬件资源匮乏，模型训练开销大、时间长，亟需方案降低门槛，提升开发效率。

Method: 引入自动混合精度训练（AMP），即在训练过程中动态结合16位与32位浮点计算，用于四个Bangla NLP任务（情感分析、命名实体识别、错误分类、问答），并基于四种transformer模型（BanglaBERT、BanglishBERT、XLM-R、mBERT）进行实证评估。

Result: AMP方案使模型训练加速44.5%，GPU内存消耗降低17.6%，F1分数基本保持与全精度（32位）训练一致（99.7%以上）。

Conclusion: 结果表明AMP能在不牺牲性能的同时，显著降低NLP模型训练对计算资源的要求，为硬件受限环境下的孟加拉语NLP研究与应用提供了有效解决方案，推动NLP能力的普及。

Abstract: Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.

</details>


### [269] [WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](https://arxiv.org/abs/2512.00837)
*Yukang Lin,Jiahao Shao,Shuoran Jiang,Wentao Zhu,Bingjie Lu,Xiangping Wu,Joanna Siebert,Qingcai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的文本水印方法WaterSearch，通过控制种子池实现高质量且易检测的水印嵌入。相比于现有方法，在检测能力与文本质量之间取得更优平衡，并在多种攻击下展现出强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型文本水印方法通常依赖于概率操作，难以在保证水印信号强度的同时维持文本质量，尤其在短文本和低熵场景下效果下降。因此，需要一种能够兼顾可检测性与文本自然性的水印嵌入技术。

Method: 设计了一种基于种子池控制的句子级平行生成方案，并提出了WaterSearch水印框架，可以结合多种现有水印方法，联合优化分布拟合性与水印信号特性。此外，还提出了健壮的句子级水印检测方法。

Result: 在三种主流大模型和十类任务上的实验显示，该方法在水印可检测性达到95%强度时，性能较最优基线平均提升51.01%。在短文本和低熵输出等场景下，性能分别提升47.78%和36.47%。在多种攻击（如插入、同义替换、复述）下仍具高检测能力。

Conclusion: WaterSearch能有效提升大模型生成文本的水印质量与可检测性，并在各种攻击下保持鲁棒性，优于现有主流方法。

Abstract: Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.

</details>


### [270] [Less is More: Resource-Efficient Low-Rank Adaptation](https://arxiv.org/abs/2512.00878)
*Chunlin Tian,Xuyang Wei,Huanrong Liu,Zhijiang Guo,Li Li*

Main category: cs.CL

TL;DR: EffiLoRA 提出了一种更高效的模型微调方法，显著减少资源消耗，同时提升性能，适用于多种任务和模型类型。


<details>
  <summary>Details</summary>
Motivation: 尽管 LoRA 作为高效参数微调方法已广泛应用于大模型，但在复杂数据集上的资源开销依然较大，并且存在参数干扰问题，亟需一种既高效又能提升鲁棒性的改进方法。

Method: 本文提出 EffiLoRA 方法，从参数冗余视角分析 LoRA，提出全层统一的 A 矩阵，并引入运行时选择性 B 矩阵更新，能够动态平衡资源消耗与模型性能，适用于语言、跨模态及扩散模型。

Result: EffiLoRA 在常识推理、视觉指令微调、图像生成等多模态任务上均优于传统 LoRA，展现出更好的效率和强健性。

Conclusion: EffiLoRA 是一种轻量高效、通用性强的低秩微调方法，广泛提升多类型模型在多样场景下的性能与资源利用率。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.

</details>


### [271] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang,Yongda Wei,Ruxue Bai,Shiyu Jiang,Nijia Mo,Binhong Li,Qiang Sun,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出Reward Auditor框架，用于评估大语言模型奖励模型（RM）在现实场景下的可靠性和脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型评估方法只关注在特定场景下的偏好感知准确率，难以发现真实世界中RMs的脆弱点和可靠性问题。现实需求是保障大模型的安全和对齐，因此亟需更全面、严谨的评估维度与方法。

Method: 作者提出了“Suitability”（适用性）的新维度，定义为在具体真实世界扰动下的条件可靠性。为此，设计了Reward Auditor框架，通过科学审计与统计检验，分析RMs在真实世界扰动场景下偏好感知置信度分布的退化，量化统计显著性和影响强度，从而综合评估RMs的系统性脆弱性。

Result: Reward Auditor能够在多种真实世界扰动场景下，有效推断RMs的脆弱性和置信度退化的严重性，揭示现有RMs的现实可靠性短板。

Conclusion: 该方法为大语言模型的安全对齐与健壮性评估提供了科学、系统性手段，有助于推动新一代更安全、可信和鲁棒的对齐系统研发。

Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

</details>


### [272] [Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study](https://arxiv.org/abs/2512.00931)
*Imane Jaaouine,Ross D. King*

Main category: cs.CL

TL;DR: 该论文研究了通过提示工程方法缓解大语言模型（LLM）在科学文献零样本摘要任务中出现语境不一致幻觉的有效性，结果显示合理设计提示确实能提升模型输出与原文对齐度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成科学文本摘要时，经常会出现输出内容与用户提示或原始文献不一致的幻觉现象。作者关注如何通过系统设计提示语，减少这种语境不一致，提高摘要的准确性和可靠性。

Method: 作者基于8篇酵母生物技术领域的科研论文摘要，选取6款指令微调过的LLM，分别应用7种不同的提示策略，包括基础提示、不同复杂度的说明性提示、不同级别（重复1句或2句）和内容（重要语句或随机语句）的重复提示，总共生成336个模型摘要。用ROUGE、BERTScore等6项指标对输出摘要进行词汇和语义一致性测评，并结合假设检验（BCa区间和Wilcoxon秩和检验）验证不同提示策略对结果的影响。

Result: 结果显示，通过重复关键信息（CR策略）或重复随机挑选的句子（RA策略）可以明显提升LLM摘要与原文在词汇层面的对齐度，各项统计指标均有显著提升。

Conclusion: 提示工程，尤其是通过策略性地重复原文内容，对减少零样本科学摘要任务中LLM幻觉现象具有积极作用，有助于提高输出可靠性。

Abstract: Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

</details>


### [273] [DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics](https://arxiv.org/abs/2512.00938)
*Ahmed Mustafa Younes*

Main category: cs.CL

TL;DR: 本文提出了DeformAr框架，专门用于分析和调试基于Transformer的阿拉伯语命名实体识别（NER）系统，旨在解释其与英文系统之间的表现差距。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的大型模型在英文中NLP任务表现优异，但在阿拉伯语的NER任务中效果有限。导致这一差距的原因包括分词、数据集质量和标注不一致等多方面因素，且这些问题往往被孤立分析，缺乏联合考察。作者希望通过系统性工具揭示影响性能的各个成分间的交互，推动阿拉伯语NER的发展。

Method: 作者提出DeformAr，这是一个包含数据提取库和交互式仪表盘的新型分析平台。该框架分为两个分析阶段：（1）跨组件分析，系统性地诊断数据和模型子组件，定位性能差异的成因及机制；（2）行为分析，结合可解释性技术、token级指标、可视化和表征空间分析，追踪和解释模型行为。

Result: DeformAr成功实现了面向阿拉伯语NER任务的组件分解和诊断分析，可以有效检测并解释模型性能瓶颈以及数据和表征层面的影响，为阿拉伯语这一低资源语言的模型提升提供了重要工具。

Conclusion: DeformAr是首个针对阿拉伯语NER的、基于组件的可解释性分析工具，填补了模型诊断领域的空白，对于低资源语言的系统分析和性能提升具有重大意义。

Abstract: Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.

</details>


### [274] [Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data](https://arxiv.org/abs/2512.00946)
*Alvaro Paredes Amorin,Andre Python,Christoph Weisser*

Main category: cs.CL

TL;DR: 该论文研究了轻量级开源大语言模型（LLM）在金融文本情感分析中的表现，发现其在低资源条件下也能取得与主流模型相媲美的效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融文本分析中表现突出，但通常需要大量算力和专有数据，这对大多数研究者和实践者来说成本高且不易获取，论文旨在探索开放、低门槛模型的可行性。

Method: 采用公开的五个金融文本数据集，评估轻量级开源LLM（DeepSeek-LLM 7B、Llama3 8B Instruct、Qwen3 8B）与FinBERT在不同条件下的情感分析能力，并测试其在训练数据有限的情况下的表现。

Result: Qwen3 8B和Llama3 8B在大多数场景下表现最佳，即便只用5%的训练数据也有很好的效果，且在zero-shot和few-shot学习条件下依然表现出色。

Conclusion: 轻量级开源LLM能够在有限资源下实现对金融异质文本的有效理解和情感分析，是一种性价比高、适用性强的方案。

Abstract: Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.

</details>


### [275] [Table as a Modality for Large Language Models](https://arxiv.org/abs/2512.00947)
*Liyao Li,Chao Ye,Wentao Ye,Yifei Sun,Zhe Jiang,Haobo Wang,Jiaming Tian,Yiming Zhang,Ningtao Wang,Xing Fu,Gang Chen,Junbo Zhao*

Main category: cs.CL

TL;DR: 该论文提出了TAMO模型，通过将表格数据作为独立模态处理，并结合超图神经网络和主流LLM，实现对表格推理任务的大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理表格数据时，通常仅将表格线性序列化，忽略了结构信息，导致性能欠佳。本研究动机是解决结构信息的损失问题，使LLM更好地适配表格推理任务。

Method: 提出TAMO模型，将表格视为独立模态，使用超图神经网络作为全局表格编码器，并与主流LLM无缝集成，构成多模态表格推理框架。

Result: 在HiTab、WikiTQ、WikiSQL、FeTaQA和StructQA等多个基准数据集上实验，模型平均相对提升42.65%。

Conclusion: 保留原始结构信息并融合结构编码器，能显著提升LLM在表格推理任务的泛化能力。TAMO为表格与LLM结合提供了有效新范式。

Abstract: To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.

</details>


### [276] [Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent](https://arxiv.org/abs/2512.00986)
*Zhihan Guo,Feiyang Xu,Yifan Li,Muzhi Li,Shuai Zou,Jiele Wu,Han Shi,Haoli Bai,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出Dr.Mi-Bench——一种科学领域深度研究智能体的模块化评测基准，并配套Dr.Mi-Eval评估范式，填补现有评测专注于检索且忽视规划、推理的不足。实验发现现有智能体在多领域评测表现碎片化，高阶规划能力提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着学术文献的激增，自动化深度研究（DR）智能体需求大增，而科学领域内缺乏全面评估其高阶能力的有效基准，这限制了相关技术发展。

Method: 作者设计了Dr.Mi-Bench——涵盖10个科学领域、200个人工标注实例的全新评测基准，并提出Dr.Mi-Eval模块化集成评估范式，分别以整体和模块化方式对智能体进行规划、检索和推理等能力的深入评测。

Result: 实验显示，当前智能体在多领域任务上表现碎片化，常在多源信息检索及多任务一致性上出现失分，尤其是审稿式任务中。同时，高阶规划能力对模型推理能力提升起关键作用。

Conclusion: Dr.Mi-Bench揭示了现有DR智能体的具体劣势和可改进路径，有助于推动更可靠学术研究助手的迭代升级。

Abstract: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.

</details>


### [277] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero,Francesca Salute,Daniel Hardt*

Main category: cs.CL

TL;DR: 本研究不仅比较了两种检索增强生成（RAG）策略对问答任务的影响，还探讨了大模型生成非传统学术内容（如演示文稿和播客脚本）的能力。结果显示GPT 4o mini搭配Advanced RAG效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大模型评估主要集中在传统任务上，未能涵盖实际应用中更复杂和新颖需求，如辅助学术创新产出，因此有必要扩展评估范围和深度。

Method: 一方面，比较Graph RAG（基于知识图谱）和Advanced RAG（关键词及语义混合检索）两种RAG方法的效果；另一方面，将评估对象拓宽至生成幻灯片和播客脚本，主流模型采用Meta LLaMA 3与GPT 4o mini，结合人工质量评审及大模型自动评判。

Result: GPT 4o mini结合Advanced RAG在问答任务中表现最佳；Graph RAG对改善准确性有限且易引入幻觉，原因在于其结构复杂及需手动设置。生成非传统学术内容时，GPT 4o mini依然表现优异，LLaMA 3则在叙事连贯性方面有一定优势。人工评审对于捕捉文本布局和风格问题不可或缺。

Conclusion: 为评估学术创新型大模型输出，需融合人类与AI自动评判。高级RAG方案（以GPT 4o mini为代表）优于知识图谱型方案；非传统内容输出评测为未来LLM评估应考虑的重要维度。

Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

</details>


### [278] [When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals](https://arxiv.org/abs/2512.01037)
*Riad Ahmed Anonto,Md Labid Al Nahiyan,Md Tanvir Hassan,Ch. Md. Rakin Haider*

Main category: cs.CL

TL;DR: 本文提出衡量大语言模型局部不一致性的"语义困惑"概念，以及相关度量和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐的语言模型在判别无害请求时存在误拒（false rejection）的问题，但目前评估方法只能得出整体误拒率，无法揭示模型对类似语义内容表达的本地不一致性问题。

Method: 提出了"语义困惑"这一衡量模型对等价意图不同表述判定不一致的失效模式，设计了度量该现象的评估框架。构建了ParaGuard数据集，包含1万个保持意图不变，仅表述变化的受控同义改写簇。提出三项模型无关的Token级指标：Confusion Index、Confusion Rate、Confusion Depth，并利用Token嵌入、下一个Token概率、困惑度等信号进行评估。

Result: 在不同模型及守卫机制上实验证明：整体误拒率掩盖了模型内部的局部不一致，有的情况下是边界整体不稳定，有的则是具体语句局部不一致。此外，更严格的拒绝策略未必增加不一致性。

Conclusion: 引入的困惑感知审计指标可分离模型拒绝的频率和合理性，为开发者优化模型的安全性和误拒率提供实际指引，有助于减少误拒同时保证安全。

Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

</details>


### [279] [ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages](https://arxiv.org/abs/2512.01077)
*Neha Joshi,Pamir Gogoi,Aasim Mirza,Aayush Jansari,Aditya Yadavalli,Ayushi Pandey,Arunima Shukla,Deepthi Sudharsan,Kalika Bali,Vivek Seshadri*

Main category: cs.CL

TL;DR: 本论文提出了一个涵盖10种濒危语言、包含1060条食谱的多模态数据集，用于促进濒危语言的自然语言处理研究，并公开发布该数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大多数NLP资源主要集中在主流语言，导致濒危以及低资源语言的文化、知识和技术发展严重滞后。作者希望通过构建具有文化和语言多样性的多模态数据集，促进濒危语言研究，同时推动相关技术发展。

Method: 作者通过在东印度偏远农村社群众包收集了涵盖10种濒危语言的1060条传统食谱，并采用适合低数字素养用户的移动端界面采集数据。然后，作者评估了多种主流大语言模型（LLM）在将这些食谱翻译成英文上的表现，进一步测试针对性的上下文提示（如背景信息、翻译示例和文化保护指导）对翻译质量的提升作用。

Result: 主流大语言模型在处理这些低资源、文化性强的语言数据时表现不佳，但在提供针对性上下文提示后，翻译质量有显著改善。这说明当前模型在此类任务上有一定潜力，但远未达到理想水平。

Conclusion: 作者强调，需要针对弱势与低资源语言和领域建立更具代表性的基准数据集，以推动公平和具有文化意识的NLP技术发展。ELR-1000 数据集的发布为濒危语言的技术研究和保护提供了重要资源。

Abstract: We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.

</details>


### [280] [How do we measure privacy in text? A survey of text anonymization metrics](https://arxiv.org/abs/2512.01109)
*Yaxuan Ren,Krithika Ramesh,Yaxing Yao,Anjalie Field*

Main category: cs.CL

TL;DR: 本文通过系统性综述梳理了文本隐私保护评估中的不同度量标准，指出了现有工作中的不足，并为今后更合规和可比性的评估方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 文本匿名化对于敏感数据领域的NLP研究和模型开发至关重要，但如何评估匿名化方法能否充分保护隐私，仍然缺乏标准化方法。

Method: 作者手动回顾了47篇有关文本隐私度量的论文，归纳出6种不同的隐私概念，比较了相关度量如何反映多样的隐私风险，并将这些度量与国际法律标准（如HIPAA和GDPR）及用户视角（HCI研究）进行对齐分析。

Result: 解析了各种隐私度量捕捉风险的能力，发现学界常用的度量与法律、用户期望存在一定脱节，同时为如何选择和设计度量标准提供了具体建议。

Conclusion: 本文对文本隐私评估方法现状进行了梳理并指出不足，强调未来评估应增强法律意识与用户导向，以提升匿名化技术的实际可行性和合规性。

Abstract: In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

</details>


### [281] [DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](https://arxiv.org/abs/2512.01174)
*Hyunjun Kim,Sooyoung Ryu*

Main category: cs.CL

TL;DR: 本文提出了DrawingBench，一个用于评估大模型（LLM）代理可信度的透明化验证框架，通过空间推理任务和可审计的GUI操作，提升代理系统的可监督性、可审核性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 随着具备自主性的AI代理系统日益普及，验证其行为可靠性和建立信任成为关键。现有基准测试缺乏透明性和可审计性，难以有效评估系统的真实表现，因此需要全新的验证框架。

Method: DrawingBench设计了一套基于空间推理任务的评测体系，涉及250个多样化提示、20个任务类别及4个难度等级，通过规则化的8项客观标准实现可复评分数，并支持细粒度的操作级别行为审核；同时引入外部多轮反馈机制以实现对代理行为的人工监督和持续优化。

Result: 对四个主流大模型（Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash）在1000次测试中的评估显示：模型结构化外部反馈提升平均准确率3.2%（复杂场景提升至32.8%），但部分模型在工具状态管理和长任务规划上仍有系统性错误。输入规范性比任务复杂度更重要，显式标准下模型表现可达100%。

Conclusion: 透明、可审核的评估框架可有效提升代理AI系统的信任度；而外部监督比模型自我纠错更有助于行为改进。DrawingBench作为开源工具为可信代理评测提供了新模板。

Abstract: As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench

</details>


### [282] [TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness](https://arxiv.org/abs/2512.01183)
*Yongxin Zhou,Philippe Mulhem,Didier Schwab*

Main category: cs.CL

TL;DR: 本文系统分析了RAG系统中检索噪声与生成参数（如temperature）之间的相互作用，并提出了一个分析框架，揭示在不同扰动和温度下性能下降的规律。


<details>
  <summary>Details</summary>
Motivation: 以往RAG系统的评估大多将检索质量和生成参数分开考察，忽视了二者的协同效应，特别是在检索过程存在噪声扰动时。

Method: 作者设计了RAG干扰-温度分析框架，对三类文本扰动结合不同temperature设置进行系统实验，利用HotpotQA数据集和多种LLM，分析表现。

Result: 高温度会放大对扰动的脆弱性，不同扰动类型在温度范围内表现出非线性敏感性。提出了诊断基准和分析框架，并给出了参数调优实用指导。

Conclusion: RAG系统鲁棒性受扰动和temperature双重影响，建议实际部署和参数选择时充分考量这一交互作用，以提升系统可靠性。

Abstract: The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.

</details>


### [283] [Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks](https://arxiv.org/abs/2512.01191)
*Krithik Vishwanath,Mrigayu Ghosh,Anton Alyakin,Daniel Alexander Alber,Yindalon Aphinyanaphongs,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本论文比较了两款主流临床AI助手与三款先进通用LLM（大语言模型）在医学相关任务上的表现，发现通用模型整体优于临床专用工具。


<details>
  <summary>Details</summary>
Motivation: 随着专用的临床AI助手越来越多地被应用于医疗实践，并被认为比通用LLM更安全可靠，但这些工具缺乏独立、量化的评估，存在证据空白。因此，论文旨在系统评估其实际表现。

Method: 作者选取了OpenEvidence和UpToDate Expert AI两款临床AI系统，以及GPT-5、Gemini 3 Pro和Claude Sonnet 4.5三款通用LLM。通过结合MedQA（医学知识）和HealthBench（临床一致性）任务，设计了一个包含1000题的mini-benchmark，量化测试它们的表现。

Result: 测试结果显示，通用LLM整体优于专用临床AI助手，以GPT-5表现最佳。OpenEvidence和UpToDate Expert AI在答案完整性、沟通质量、上下文理解、系统性安全推理等方面均存在短板。

Conclusion: 临床AI决策支持工具当前可能落后于顶尖通用LLM，因此在投入临床应用前，亟需进行透明、独立的性能评估，以确保患者安全和医疗质量。

Abstract: Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.

</details>


### [284] [Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework](https://arxiv.org/abs/2512.01198)
*Jiatong Han*

Main category: cs.CL

TL;DR: 本文提出了一种基于人机协作（HITL）的方法，通过引导大模型识别和传达中医经典文本中的隐喻和转喻，提升中医理论在英译中的认知表达效果。实验结果显示，经提示优化的大模型译文在多项认知指标上均优于人工译文和未优化机译。


<details>
  <summary>Details</summary>
Motivation: 中医理论以形象思维为核心，诊疗逻辑依赖隐喻和转喻。但英语译本多为直译，难以还原和应用中医深层概念网络，影响其国际传播和实际应用。解决译者对隐喻转喻处理不足的问题，提升中医理论的可读性与实用性。

Method: 采用人-机协作（HITL）框架，从《黄帝内经》中选取四段理论核心原文。通过提示词引导DeepSeek V3.1大模型识别文本中的隐喻和转喻，并在译文中传达理论内涵。评估时，利用ChatGPT 5 Pro与Gemini 2.5 Pro模拟三类真实读者，对人工译文、基础模型译文和优化译文在五个认知维度下进行打分，并结合结构化访谈和现象学分析（IPA）。

Result: 经提示优化的大模型译文在五个认知维度评分中表现最佳，并在不同模型和读者角色间表现出高度一致性。访谈揭示了人机译文差异、隐喻转喻传递的有效策略和读者对译文的认知偏好。

Conclusion: 人机协作式大模型方法为古代高概念密度文本，尤其是中医文献的翻译，提供了认知导向、高效且可复制的方法论路径，可提升理论阐释与国际传播效果。

Abstract: Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.

</details>


### [285] [Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language](https://arxiv.org/abs/2512.01256)
*Ekha Morang,Surhoni A. Ngullie,Sashienla Longkumer,Teisovi Angami*

Main category: cs.CL

TL;DR: 本论文首次针对Nagamese语言进行情感分析和情绪分类，填补了该语种在自然语言处理领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有的情感分析多集中于资源丰富语言（如英语、印地语），而Nagamese语言此前并无相关工作。作者希望推动Nagamese在自然语言处理领域的研究进展。

Method: 作者构建了包含1195个Nagamese词汇的情感极性词典，并结合其他特征，采用朴素贝叶斯和支持向量机两种监督式机器学习方法进行情感分类和情绪分类实验。

Result: 利用自建词典和机器学习方法，在Nagamese文本中实现了情感极性（正面、负面、中性）与基本情绪的识别。

Conclusion: 实验结果显示该方法在Nagamese文本情感分析上可行，首次为该语种构建了基础的情感分析资源和方法，对今后Nagamese及类似小语种NLP研究具有重要意义。

Abstract: The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na"ive Bayes and Support Vector Machines.
  Keywords: Nagamese, NLP, sentiment analysis, machine learning

</details>


### [286] [SUPERChem: A Multimodal Reasoning Benchmark in Chemistry](https://arxiv.org/abs/2512.01274)
*Zehua Zhao,Zhixian Huang,Junren Li,Siyu Lin,Junting Zhou,Fengqi Cao,Kun Zhou,Rui Ge,Tingting Long,Yuexiang Zhu,Yan Liu,Jie Zheng,Junnian Wei,Rong Zhu,Peng Zou,Wenyu Li,Zekai Cheng,Tian Ding,Yaxuan Wang,Yizhao Yan,Tingru Wei,Haowei Ming,Weijie Mao,Chen Sun,Yiming Liu,Zichen Wang,Zuo Zhang,Tong Yang,Hao Ma,Zhen Gao,Jian Pei*

Main category: cs.CL

TL;DR: 论文提出了SUPERChem基准，用于深入评估大语言模型（LLM）在化学领域的推理能力，并发现主流模型距离专家水平仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 现有化学领域用于评估LLM推理能力的基准存在任务过于简单、缺乏过程性评价，且与专家技能不符等问题，不能全面反映模型的真实推理能力。

Method: 作者提出了SUPERChem基准，包含500道专家精心筛选、推理密集的化学题，涵盖不同子领域，并提供多模态与纯文本两种形式。每道题都配有专家编写的解题过程，用于量化推理路径的保真度（RPF）而非只看最终答案对错。题目和解题步骤通过原创和迭代流程清洗，有效避免了数据污染。

Result: 以人类专家成绩（准确率40.3%）为基准，当前最优的GPT-5（高配）准确率为38.5%，Gemini 2.5 Pro为37.9%，DeepSeek-V3.1-Think为37.3%，均未超越人类专家。实验还证明了多模态信息对推理的影响及模型间在推理过程中表现出的不同特征。

Conclusion: SUPERChem作为一个挑战性强且可复现评测流程的基准，有助于揭示现有LLM在化学推理中的短板，为推动LLM在化学智能方面达到专家水平提供了工具和标准。

Abstract: Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.

</details>


### [287] [Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning](https://arxiv.org/abs/2512.01282)
*Jiahao Yuan,Zhiqing Cui,Hanqing Wang,Yuansheng Gao,Yucheng Zhou,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文提出了KardiaBench数据集和Kardia-R1框架，以提升对话系统的个性化情感推理能力，专注于人格一致性和可解释的情感反馈。实验表明其显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统在个性化和情感复杂性的支持方面有限，主要由于缺乏具有持续用户身份的数据集，以及现有训练方法中反馈信号粗糙且不可解释。因此，亟需支持用户身份感知和可验证推理的数据与方法，提升情感推理能力。

Method: 构建了KardiaBench大规模、多轮、包含真实用户画像的情感对话数据集，并设计了以可解释rubric为核心的情感推理与训练流程（Rubric-ERL），实现逐步、可解释的共情推理，强化了对用户理解及反应的个性化、一致性要求。

Result: 在四种主流大型语言模型上验证，Kardia-R1在情感准确性、共情度、相关性、画像一致性及安全性等指标均优于已有系统。

Conclusion: KardiaBench与Kardia-R1为实现身份感知、可解释的共情推理提供了有效数据基础与方法，对下一代个性化智能对话系统的研发具有重要推动作用。

Abstract: As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.

</details>


### [288] [Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding](https://arxiv.org/abs/2512.01316)
*Koki Natsumi,Hiroyuki Deguchi,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种新的翻译解码方法AC-PMBR，在减少计算成本的同时提高了译文质量。


<details>
  <summary>Details</summary>
Motivation: MBR解码能生成高质量译文，但其评估所有候选两两之间的得分，计算复杂度高。PMBR通过采样减少计算，但质量受损。作者旨在进一步改善译文质量与计算成本之间的权衡。

Method: 提出了AC-PMBR方法，利用知识蒸馏模型指导分数矩阵补全，从而提升矩阵补全的近似精度，减少对utility function的调用。

Result: AC-PMBR在WMT'23英德互译任务上，矩阵补全近似误差降低了3倍，在计算成本相近的情况下获得了更高的译文质量。

Conclusion: AC-PMBR能有效改善译文质量与计算成本之间的折中，是提升MBR解码效率和质量的有效方法。

Abstract: Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\leftrightarrow$De translation tasks.

</details>


### [289] [MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis](https://arxiv.org/abs/2512.01369)
*Md. Rafiul Biswas,Firoj Alam,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: MARSAD是一个面向阿拉伯语世界、支持多功能实时社交媒体监测与分析的自然语言处理平台。它适合研究人员和非技术用户，能分析实时及历史的社媒内容，输出多维度的可视化与报告。包括情感分析、情绪分析、宣传侦测、事实核查和仇恨言论侦测等。平台支持安全数据抓取，并有高效的数据处理和用户友好界面。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体在阿拉伯语地区使用量的大幅提升，研究和管理社会舆情变得极为重要。现有工具要么不支持阿拉伯语，要么不便于非技术用户操作。因此需要一个专门服务于阿拉伯世界、多功能且易用的NLP平台。

Method: MARSAD通过前后端分离架构实现：后端集成灵活的文档存储与结构化管理，能高效处理大规模多模态数据集。前端设计为用户友好式界面，支持简单数据上传和互动。平台利用API密钥实现安全的数据抓取，融合多种NLP技术（情感、情绪、宣传、核查、仇恨言论等分析模块）。

Result: 平台能够让技术和非技术用户都能便捷获取阿拉伯语社交媒体的情绪、立场、信息真伪等可视化分析结果，数据处理高效，报告详实。

Conclusion: MARSAD为阿拉伯语社交媒体内容分析和监控提供了强大、易用、高效的综合解决方案，促进了阿拉伯地区社会舆情与信息传播的研究与管理。

Abstract: MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.

</details>


### [290] [DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis](https://arxiv.org/abs/2512.01410)
*Ruohan Zhou,Jiachen Yuan,Churui Yang,Wenzheng Huang,Guoyan Zhang,Shiyao Wei,Jiazhen Hu,Ning Xin,Md Maruf Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种名为DyFuLM的多模态情感分析模型，通过动态融合和门控特征聚合模块，实现了对复杂文本情感的有效识别，取得了当前领先的情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 复杂文本表述中的情感理解仍是情感计算领域的难点，现有方法在处理多层语义和细粒度情感表达方面存在不足。

Method: 提出动态融合学习模型（DyFuLM），包含一个分层动态融合模块用于自适应融合多层特征，和一个门控特征聚合模块用于调控跨层信息流，从而实现均衡的特征表示学习。

Result: 在多任务情感数据集上的实验表明，DyFuLM在粗粒度情感分类准确率为82.64%，细粒度为68.48%；回归误差（MAE=0.0674，MSE=0.0082）最低，决定系数R^2=0.6903最高。消融实验显示两个关键模块和动态损失机制对提升模型性能均有显著贡献。

Conclusion: DyFuLM通过有效的层次化特征融合，增强了情感表示能力和整体性能，各模块在特征交互与任务平衡中均有重要作用。

Abstract: Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.

</details>


### [291] [PromptBridge: Cross-Model Prompt Transfer for Large Language Models](https://arxiv.org/abs/2512.01420)
*Yaxuan Wang,Quan Liu,Zhenting Wang,Zichao Li,Wei Wei,Yang Liu,Yujia Bao*

Main category: cs.CL

TL;DR: LLMs快速发展造成模型更替频繁，但提示词在不同模型间兼容性差，PromptBridge无需重新训练即可实现高效的提示词迁移。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用（如代码生成、数学推理等）常因性能、成本、部署等原因更换模型，然而不同模型间对同一提示词的响应差异大，迁移受限，优化成本高。

Method: 提出PromptBridge框架，无需单独为每个任务或模型重新优化提示词。通过MAP-RPE方法，基于一小部分校准任务，自动生成源模型与目标模型的最佳提示词对，并学习其迁移映射。新任务时，可将源模型提示词直接映射为目标模型优化版。

Result: 实验证明，在单智能体与多智能体环境下，PromptBridge既提高了准确性，又显著减少了模型迁移时的人工和计算成本。

Conclusion: PromptBridge有效缓解了模型更替带来的提示词迁移难题，为跨模型系统部署提供了高效、实用的新思路。代码即将开放。

Abstract: Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.

</details>


### [292] [Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech](https://arxiv.org/abs/2512.01439)
*Bharatdeep Hazarika,Arya Suneesh,Prasanna Devadiga,Pawan Kumar Rajpoot,Anshuman B Suresh,Ahmed Ifthaquar Hussain*

Main category: cs.CL

TL;DR: 本文提出了一种多语种会话式AI系统，面向印度金融助理场景，特别支持像Hinglish这样的混合语言，显著提升了用户参与度，并保持了系统低延迟。


<details>
  <summary>Details</summary>
Motivation: 印度拥有极为丰富的语言多样性，且大多数人不懂英语，这对金融科技平台的普惠性带来挑战。本文动机在于解决语言障碍，提高金融服务的覆盖和可达性。

Method: 系统采用多智能体架构，包含语言分类、功能管理和多语种响应生成模块，并支持代码混合语言的自然对话。作者对多种语言模型进行了对比分析，并进行了实际部署测试。

Result: 实验和实际部署表明，系统能够显著提升用户的使用参与度，且系统的响应延时增加非常小（仅4-8%）。

Conclusion: 本研究成功降低了印度等新兴市场数字金融服务的语言门槛，为多语种普惠金融提供了有效解决方案。

Abstract: India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\%). This work contributes to bridging the language gap in digital financial services for emerging markets.

</details>


### [293] [MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification](https://arxiv.org/abs/2512.01443)
*Xabier de Zuazo,Ibon Saratxaga,Eva Navas*

Main category: cs.CL

TL;DR: 本文提出了基于Conformer结构的解码器，应用于LibriBrain 2025 PNPL大赛的两项MEG任务（语音检测与音素分类），通过多种针对MEG数据的改进，成绩显著超越了基线，跻身前十。


<details>
  <summary>Details</summary>
Motivation: 脑磁图（MEG）信号的解码对脑机接口、语音识别等领域有重要价值，而现有模型在高维、稀疏MEG信号上的效果有限。本文旨在结合新型神经网络结构提升MEG语音相关任务表现。

Method: 采用紧凑型Conformer模型处理306通道原始MEG信号，配以轻量卷积投影层和专用任务头。语音检测任务引入了适用于MEG的SpecAugment增强方法。音素分类任务则使用逆平方根类别加权与动态分组加载器，并实现了实例级归一化以应对分布变化。

Result: 在比赛官方标准划分和F1-macro评估下，提出方法在语音检测任务上取得88.9%分数，在音素分类任务上取得65.8%，分别显著优于基线，均进入两项任务的前十名。

Conclusion: 基于Conformer的模型结合多项针对MEG数据的技巧，可以在多个脑磁图语音任务上大幅提升表现，为后续脑—语音解码方法的研究提供了有价值的思路和实现参考。

Abstract: We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.

</details>


### [294] [Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages](https://arxiv.org/abs/2512.01460)
*Jozef Kubík,Marek Šuppa,Martin Takáč*

Main category: cs.CL

TL;DR: 本文提出将主动学习（Active Learning, AL）与聚类和动态数据选择调度器相结合，提升低资源语言模型微调效果，在斯洛伐克语、马耳他语、冰岛语和土耳其语实验中，能减少30%标注工作并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言受限于数据稀缺，导致语言模型效果较弱。现有的预训练需大量计算资源，因此作者专注于在微调阶段提升表现，寻找更实用的方法。

Method: 作者提出一种微调流程，将主动学习、数据聚类及动态数据选择策略相结合，称为Active Learning schedulers。具体做法是利用聚类分析辅助主动选择训练样本，实现科学的数据采样顺序。

Result: 在斯洛伐克语、马耳他语、冰岛语、土耳其语四种低资源语言上，该方法不仅能使标注量减少30%，还能带来F1分数最高提升4分，并增强微调稳定性。

Conclusion: 结合主动学习和聚类策略可有效提升低资源语言模型微调表现，并降低数据标注成本，是提升低资源模型性能具备实用意义的新方案。

Abstract: Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.

</details>


### [295] [MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages](https://arxiv.org/abs/2512.01512)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Bo Yang,Keqi Deng,Xie Chen,Yang Xiang,Ming Liu,Bin Qin,YaoWei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种高效、覆盖多语言的语音到文本翻译（S2TT）框架MCAT，实现了高效扩展至70种语言的互译，并大幅提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在语音到文本翻译任务中受限于数据集以英语为中心，难以扩展到多语言互译，同时模型在处理长语音序列时推理效率低。

Method: MCAT框架提出了两项创新：一是采用课程学习和数据均衡策略，扩展模型支持至70种语言的互译；二是设计了优化的语音适配器，将语音序列长度压缩至30个token以内以提升推理效率。

Result: 在不同规模（9B、27B）的多模态大模型上，大量实验显示MCAT在FLEURS数据集70x69种语言对中超越了现有最优模型，并显著提升批量推理效率，仅需约1亿可训练参数和每种语言10小时数据。

Conclusion: MCAT框架显著提升了多语言语音翻译的覆盖范围与效率，支持大规模互译，且开源推动领域发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.

</details>


### [296] [Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces](https://arxiv.org/abs/2512.01557)
*Edward Ajayi,Eudoxie Umwari,Mawuli Deku,Prosper Singadi,Jules Udahemuka,Bekalu Tadele,Chukuemeka Edeh*

Main category: cs.CL

TL;DR: 本研究分析了非洲语言在数字领域的表现及其对现有语言检测工具的挑战，评估了Yoruba、Kinyarwanda和Amharic三种语言在不同网络平台的数据表现及检测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管Yoruba、Kinyarwanda和Amharic等非洲语言有大量使用者，但这些语言在网上的使用频率有限，且常受英语影响，缺乏真实、地道的语料，导致训练相关语言模型面临数据稀缺问题。

Method: 研究收集了这三种语言的Reddit和本地新闻媒体数据，对比二者的数据量、纯净度和用户互动，并使用专业模型（AfroLID）和通用LLM对数据进行语言检测性能评估。

Result: Reddit数据极少且存在较多语码转换；新闻媒体则提供了丰富且纯净的单语语料，并带动了社交媒体上的本地语言交流。语言检测模型在新闻数据上几乎完美，但在Reddit上表现不佳。

Conclusion: 本地新闻媒体是训练非洲语言AI模型更可靠的语料来源，提升了模型表现；同时强调未来模型需兼容纯净和语码转换文本，以进一步提升非洲语言检测的准确率。

Abstract: This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.

</details>


### [297] [MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark](https://arxiv.org/abs/2512.01603)
*Yuezhang Peng,Chonghao Cai,Ziang Liu,Shuai Fan,Sheng Jiang,Hua Xu,Yuxin Liu,Qiguang Chen,Kele Xu,Yao Li,Sheng Wang,Libo Qin,Xie Chen*

Main category: cs.CL

TL;DR: 该论文提出了一个新的多意图汽车舱内口语理解数据集MAC-SLU，并系统性评测了最新的LLMs和LALMs在该任务上的表现，结果显示现有模型表现仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有SLU数据集缺少多样性和复杂度，且缺乏统一的最新LLM/LALM基准。为推动领域发展，亟需一个更真实复杂的数据集以及完整的模型评测基准。

Method: 作者构建了MAC-SLU——包含更真实复杂多意图的汽车舱口语理解数据集，并基于该数据集，系统测评了主流开源LLM和LALM，涵盖in-context learning、SFT、端到端与pipeline范式。

Result: 实验发现：LLM和LALM虽然具备一定in-context学习能力，但在SLU任务中的性能远不及SFT；同时E2E的LALM性能与pipeline方法接近，并且有效规避了声学识别误差传播。

Conclusion: MAC-SLU为复杂对话场景提供了新基准；当前LLM/LALM尚不能完全取代传统SFT方法；端到端LALM展示了解决声学误差传播的潜力。作者还开源了代码和数据。

Abstract: Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.

</details>


### [298] [Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems](https://arxiv.org/abs/2512.01661)
*Dengyun Peng,Qiguang Chen,Bofei Liu,Jiannan Guan,Libo Qin,Zheng Yan,Jinhao Liu,Jianshu Zhang,Wanxiang Che*

Main category: cs.CL

TL;DR: 该论文提出了UnsolvableQA数据集和UnsolvableRL学习框架，使大语言模型更好地区分不可解（内在矛盾）与超出能力范围的问题，提升了模型拒答和检测不可解问题的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型容易混淆客观不可解（有内在矛盾）和主观能力有限的问题，导致产生幻觉和过度自信，缺乏可靠的拒答机制。

Method: 作者构建了UnsolvableQA数据集，通过程序化生成逻辑谜题和“逆向构造”法为数学题注入矛盾，包含可解与不可解实例；并提出UnsolvableRL强化学习框架，综合精度、不可解、难度三重奖励以训练模型。

Result: 实验证明提出的方法几乎完美地检测出不可解问题，并在可解任务上提升了准确率。还发现仅训练在可解数据上会导致“能力崩溃”，必须让模型接触不可解任务才能显著抑制过度自信。

Conclusion: 让大模型暴露于不可解的数据是防止系统性过度自信和提升其可靠性的必要手段。方法有效提升了大模型对不可解问题的识别和恰当拒答能力。

Abstract: Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

</details>


### [299] [MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications](https://arxiv.org/abs/2512.01710)
*Stefano Zeppieri*

Main category: cs.CL

TL;DR: 本文提出了MMAG混合记忆增强生成框架，将不同类型的记忆融入大语言模型代理，提升多轮交互中的连贯性、个性化与持久性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能够生成连贯文本，但在长时间多轮交互中难以持续保持相关性、个性化和上下文连续性。而人类交流依赖多种记忆，这为改进LLM提供了启发。

Method: 提出MMAG（混合记忆增强生成）模式，将记忆划分为五层：会话记忆、长期用户记忆、情节/事件记忆、感知/情境记忆和短时工作记忆。借鉴认知心理学，将这些记忆映射到技术组件，并提出协调、优先级和冲突解决机制。在Heero对话系统中实现该方法，采用加密用户档案和会话历史。

Result: 系统实现后，提高了用户参与度与留存率。分析了在实现过程中涉及的存储、检索、隐私和延迟等问题，并指出了仍待解决的挑战。

Conclusion: MMAG为打造具备丰富记忆、更加连贯与以人为本的语言智能体奠定了基础。

Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

</details>


### [300] [Self-Supervised Borrowing Detection on Multilingual Wordlists](https://arxiv.org/abs/2512.01713)
*Tim Wientzek*

Main category: cs.CL

TL;DR: 本文提出了一种完全自监督、多语言词表借词检测方法，通过结合PMI相似度和基于音素特征向量的轻量化对比模块，并自动选定决策阈值，无需标注数据，实现了优于现有字符串相似度方法的效果，且性能可与有监督基线相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有多语言借词检测方法通常依赖人工标注和有限的相似度计算方式，效率低且泛化能力不足，因此亟需一种无需人工标注、适用于大规模数据集且检测精度高的新方法。

Method: 方法结合了两类特征信息：1）基于全局对应模型的PMI（点互信息）相似度；2）基于音素特征向量的对比学习组件。此外，提出了无需标注数据的自动化阈值选择机制。

Result: 在多个基准数据集上实验表明，单独使用PMI已优于传统字符串相似度方法（如NED和SCA），而两种特征结合后表现等同或优于有监督方法。消融实验验证了编码、温度和数据增强策略的重要性。

Conclusion: 该方法适用于不同规模的数据集，完全无需人工监督，并提供了命令行工具支持研究者自主实验，具有良好的实用性和推广价值。

Abstract: This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.

</details>


### [301] [Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks](https://arxiv.org/abs/2512.01725)
*Jiannan Guan,Qiguang Chen,Libo Qin,Dengyun Peng,Jinhao Liu,Liangyu Huo,Jian Xie,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文发现大型语言模型在需要生成多解答的任务中表现不佳，提出了MuSoBench基准，并分析了思维过度自信问题及其可能原因。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在单一标准答案的推理任务中表现优异，但在要求输出多样化全面答案的多解答任务中效果不佳。作者希望找出造成这一现象的原因，并推动相关评估与方法的改进。

Method: 作者设计了多解答基准MuSoBench，并通过实验比较了短链思维（Short-CoT）和长链思维（Long-CoT）的提示范式。此外，他们提出了“认知僵化假说”并通过注意力熵分析对其进行了初步验证。

Result: 实验结果显示，常规的Short-CoT提示会显著导致模型推理中过度自信，而Long-CoT通过迭代探索和自我反思能减轻这一问题。注意力熵分析为认知僵化假说提供了初步证据。

Conclusion: 本文的研究为检测LLM推理完整性提供了新工具，并强调评价应超越单一答案准确率，更注重答案的全面性和多样性探索。

Abstract: Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

</details>


### [302] [Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference](https://arxiv.org/abs/2512.01728)
*Zhengjia Wang,Danding Wang,Qiang Sheng,Jiaying Wu,Juan Cao*

Main category: cs.CL

TL;DR: 本文提出OmiGraph框架，首次从信息遗漏的角度检测虚假信息，通过构建可发现被遗漏内容的图并对其关系建模，有效提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测主要关注捏造内容，对通过省略关键信息造成误导的手法关注较少，而这种隐蔽的误导容易让读者产生错误的全面认知。

Method: OmiGraph框架首先根据新闻报道的上下文环境构建出具备遗漏意识的图，挖掘同一事件的补充视角并暴露潜在被遗漏内容；然后通过对图中关系建模，识别上下文依赖和动态遗漏意图，形成遗漏关系表示；最后，引入遗漏感知的消息传递与聚合机制，融合遗漏内容与关系，实现整体误导认知。

Result: 在两个大规模基准上，OmiGraph在F1值和准确率上分别平均提升5.4%和5.3%。

Conclusion: 从遗漏角度进行建模能显著提升虚假信息检测的效果，OmiGraph验证了利用信息省略进行误导的新型检测范式的有效性。

Abstract: This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.

</details>


### [303] [InnoGym: Benchmarking the Innovation Potential of AI Agents](https://arxiv.org/abs/2512.01822)
*Jintian Zhang,Kewei Xu,Jingsheng Zheng,Zhuoyun Yu,Yuqi Zhu,Yujie Luo,Lanning Wei,Shuofei Qiao,Lun Du,Da Zheng,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了一个新的测评基准InnoGym，专门用于系统评估AI代理的创新能力，通过引入性能提升和方法新颖性两大指标来补足现有测评只关注正确性的不足。


<details>
  <summary>Details</summary>
Motivation: 当前大模型与AI代理在代码生成、数学推理等领域进步明显，但主流基准只考察解答的正确性，而忽略了实现路径的多样性和创新性，无法衡量解决问题时方案的原创性。创新性是推动AI突破的重要因素，因此需要新的评价基准。

Method: 构建了InnoGym基准，设计了两项核心创新性指标：“性能提升”（与已知最佳方案的改进幅度）和“新颖性”（与现有方法的区别程度）；共包含18个真实工程和科学领域任务，统一资源、评估和解集标准；此外还提出了iGym统一运行环境，用于可复现和长期评测。

Result: 大规模实验发现，虽然部分AI代理可以提出新颖方法，但这些方法普遍缺乏鲁棒性，难以大幅提升性能，创新性和实际效果之间存在显著差距。

Conclusion: 目前AI代理在创新方法与解题效果之间存在矛盾，表明需要能同时兼顾创新性和有效性的评测基准；InnoGym为评价AI创新潜力提供了新工具，有助于推动AI系统的全面发展。

Abstract: LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.

</details>


### [304] [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/abs/2512.01848)
*Jinghan Jia,Nathalie Baracaldo,Sijia Liu*

Main category: cs.CL

TL;DR: 本文关注于大型推理模型（LRM）在进行显式链式思维（CoT）推理时带来的安全风险。作者发现现有的以监督微调（SFT）为主的安全对齐方法效果有限，提出用强化学习（RL）来改进，对比实验结果证明RL在不损害推理能力的前提下显著提高了模型安全性。


<details>
  <summary>Details</summary>
Motivation: LRM通过生成显式的推理过程提升了数学和逻辑问题解决能力，但这些推理过程也带来了新的安全隐患——即使最终答案安全，过程中也易出现不安全行为。现有方法主要采用SFT进行安全性对齐，但效果不佳。因此，作者希望引入RL，探索更优的安全训练框架。

Method: 作者通过将强化学习引入LRM的安全训练流程，通过奖励反馈直接优化模型的策略。与单纯的SFT相比，RL方法期望得到更自适应、稳定的安全对齐效果。所有实验在多个模型家族和基准测试集上进行，通过定量评估安全提升和推理能力保留情况。

Result: 实验数据显示，RL方法相较于SFT能更强、更一致地提升模型安全性，同时不损害或更好保持推理能力。分析还表明，RL能有效抑制不安全的探索性推理，同时在保持推理深度方面优于基线方法。

Conclusion: 仅依赖监督方式进行安全对齐不足以应对LRM带来的新型安全风险。强化学习作为补充框架，能够更好地提升推理模型的安全性，且不会显著削弱其推理能力，是更为可行的安全训练方案。

Abstract: Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

</details>


### [305] [BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages](https://arxiv.org/abs/2512.01852)
*Hrishikesh Terdalkar,Kirtan Bhojani,Aryan Dongare,Omm Aditya Behera*

Main category: cs.CL

TL;DR: 本文提出了BHRAM-IL，这是一个面向多种印度语言（含英语）的LLM幻觉检测与评价基准，弥补了相关研究在印地语等资源稀缺语言上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多语言环境下应用广泛，但对于印地语等欠资源印度语言的幻觉检测研究不足。鉴于大模型在这些语言中的幻觉问题未被充分评估，亟需可靠的基准与方法。

Method: 作者构建了BHRAM-IL基准，包括覆盖5种语言、9类任务的36,047个精心设计问题。评测了14个多语言LLM，在10,265道题上分析模型在多语、事实等各类幻觉表现，使用归一化指标横向比较。

Result: 综合全部模型和类别，得到主指标为0.23，语言校正模糊指标为0.385，反映现有模型在印度主流语言幻觉检测任务中的表现及基准数据集的实用性。

Conclusion: BHRAM-IL为多语种LLM幻觉检测和抑制研究提供了重要工具。数据集与代码已开放，有助于推动印度语言环境下相关研究发展。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.

</details>


### [306] [Cross-Lingual Interleaving for Speech Language Models](https://arxiv.org/abs/2512.01865)
*Adel Moumen,Guangzhi Sun,Philip C. Woodland*

Main category: cs.CL

TL;DR: 提出了一种新的跨语言语音单元混合方法，并发布了双语语音数据集及评测基准，实现了更强的多语种口语语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有口语语言建模多以英语为主，受限于跨语言评测标准和训练数据的稀缺，难以推广到资源稀缺语种，需要耗时费力的人工标注和文本监督。作者希望让更多不具备良好书面资源的语言也能获得NLP技术支持。

Method: 作者提出了跨语言混合（interleaving）方法，在没有文本监督的情况下直接混合多种语言的语音token进行训练。此外，作者合成并公开了包含英文和法文的训练集TinyStories（约4.2万小时），并基于GPT-4自动生成了对应的跨语种语义评测基准，如StoryCloze和TopicCloze。

Result: 在360M和1B参数规模下，跨语言混合训练不仅提升了单语语义精度，还增强了跨语言生成和隐藏状态对齐能力。实验结果表明该方法对于提升模型多语种表现有效。

Conclusion: 跨语言混合是一种简单、可扩展的方案，有助于建立能够理解和生成多语言口语的模型。作者还将公开全部资源以促进复现和后续研究。

Abstract: Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.

</details>


### [307] [Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models](https://arxiv.org/abs/2512.01892)
*Heloisa Candello,Muneeza Azmat,Uma Sushmitha Gunturi,Raya Horesh,Rogerio Abreu de Paula,Heloisa Pimentel,Marcelo Carpinette Grave,Aminat Adebiyi,Tiago Machado,Maysa Malfiza Garcia de Macedo*

Main category: cs.CL

TL;DR: 本研究通过实验评估了生成式AI响应中缓解有害内容策略的人类感知差异，发现参与者对语言和语境高度敏感，并提出了新的评价指标。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的广泛应用，其输出信息虚假和有害内容的问题日益突出。尽管已经有多种防护措施，但人类对于这些防护策略的真实效果知之甚少。

Method: 本研究采用混合方法实验，邀请57名参与者在有害回答及其缓解和仅缓解后的两种条件下，对AI响应的可信度、公平性、去危害能力和相关性等多维度进行评估。采用组内设计，比较不同背景与经验下的感知差异。

Result: 结果显示，母语、AI相关工作经验和标注熟悉度显著影响受试者的评价。参与者更关注语言和语境，能识别和惩罚微小语法错误，并对语义完整的回复给予更高评价。这种偏好与LLM定量评价的处理方式形成对比。

Conclusion: 研究提出了新的缓解策略评测指标，并为未来的人机评价研究提供了见解，强调在人类评价中应高度重视语境与语言细节。

Abstract: With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.

</details>


### [308] [OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation](https://arxiv.org/abs/2512.01896)
*Jinzheng Yu,Yang Xu,Haozhen Li,Junqi Li,Yifan Feng,Ligu Zhu,Hao Shen,Lei Shi*

Main category: cs.CL

TL;DR: 本文提出了在线舆情报告自动生成任务，并构建了相应的数据集和评测工具，为危机管理的报告自动化提供了技术基础和研究平台。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型让自动化报告生成成为可能，但在在线舆情报告领域尚无系统性研究，缺乏正式的任务定义和评测基准。

Method: 作者定义了在线舆情报告自动生成（OPOR-GEN）任务，构建了涵盖463个危机事件的数据集OPOR-BENCH，包括新闻、社交媒体和专家摘要。同时提出了OPOR-EVAL框架，通过多智能体模拟专家对生成报告进行评估。

Result: 实验表明，OPOR-EVAL的评测与人类专家评判高度相关，验证了框架的有效性。

Conclusion: 本文系统性地提出了任务定义、基准数据集与评测框架，为自动化舆情报告研究提供了基础，推动了该领域的发展。

Abstract: Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.

</details>


### [309] [Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909)
*Lihu Chen,Xiang Yin,Francesca Toni*

Main category: cs.CL

TL;DR: 本文提出了“潜在辩论”（latent debate）框架，用于解释大语言模型（LLM）的预测过程，并可用于检测幻觉。该方法能揭示单一模型推理过程中的内部赞同和反对信号。实验证明该框架具有良好的一致性和解释性。


<details>
  <summary>Details</summary>
Motivation: 目前LLM的内部思维过程和幻觉成因难以解释，现有方法多依赖于显性多答案或多模型的外部辩论，而缺乏对单模型、单次推理中隐性内部争议的探索。

Method: 提出了一个与模型和任务无关的理论框架，将LLM内部赞成与反对信号形式化，对True/False预测任务进行了符号化实现，并用结构化代理模型来近似LLM思维过程。

Result: 潜在辩论框架与原始LLM的预测高度一致，同时，可作为检测幻觉的有力基线。进一步分析发现，幻觉产生与中间层高密度的潜在辩论强相关。

Conclusion: 潜在辩论为理解LLM内部机制提供了新视角，尤其适用于推理步骤中出现内部（不）一致性时的解释和幻觉检测。

Abstract: Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.

</details>


### [310] [Rectifying LLM Thought from Lens of Optimization](https://arxiv.org/abs/2512.01925)
*Junnan Liu,Hongwei Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新方法RePro，用于提升大语言模型的推理表现，通过修正其推理过程，减少过度推理和冗长链条，实验显示RePro有效改善了算法在各领域基准上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然长链式思维（CoT）能增强大语言模型推理能力，但也易导致“过度思考”和不必要的复杂推理流程，从而影响最终性能，亟需有效方法优化推理链条。

Method: 作者将推理过程视为梯度下降优化，将每一步类比为向问题解的更新，并提出RePro方法，设计双重评分机制衡量推理链的强度和稳定性，通过综合奖励整合至RLVR（可验证奖励的强化学习）训练流程中，对模型进行优化。

Result: 作者在数学、科学和编程等多个基准和多种强化学习算法下进行大量实验，结果表明RePro能稳定提升推理表现，并有效缓解模型的过度推理等问题。

Conclusion: 通过过程级奖励优化思路，本文的RePro方法为提高大语言模型链式推理质量提供了有效方案，对实际任务中的推理表现有显著促进作用。

Abstract: Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

</details>


### [311] [How Far Are We from Genuinely Useful Deep Research Agents?](https://arxiv.org/abs/2512.01948)
*Dingling Zhang,He Zhu,Jincheng Ren,Kangqi Song,Xinran Zhou,Boyu Feng,Shudong Liu,Jiabin Luo,Weihao Xie,Zhaohui Wang,Tianrui Qin,King Zhu,Yuqing Wang,Qianben Chen,Yuchen Eleanor Jiang,Wei Wang,Jiaheng Liu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一套新的细粒度深度研究基准（FINDER）和深度研究智能体失败模式分类体系（DEFT），用于更好地评估和分析深度研究智能体在自动生成分析报告中的表现和失败点。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究智能体（DRAs）主要在问答测试集上验证，缺乏生成全面分析报告的评估。同时，现有报告生成基准任务复杂、评价主观，难以满足用户需求，也不利于实际应用。

Method: 提出FINDER基准，涵盖100个人工设计的研究任务和419个结构化检查项，标准化报告结构和分析深度。同时，收集主流DRA生成的大约1,000份报告，提出基于理论和人机共注的DEFT失败分类体系，细致归纳了推理、检索、生成等环节的14类失败模式，并进行了可靠性验证。

Result: 实验表明，目前的DRA在任务理解上表现良好，但在证据整合、验证以及具备抗推理误差的规划能力方面存在较大挑战。

Conclusion: FINDER与DEFT为DRA的研究和评测提供了更具操作性和针对性的工具，为未来改进研究智能体的性能指明了方向。

Abstract: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.

</details>


### [312] [The Art of Scaling Test-Time Compute for Large Language Models](https://arxiv.org/abs/2512.02008)
*Aradhye Agarwal,Ayan Sengupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文首次系统地对多种推理任务下的大型语言模型（LLM）的推理时计算资源动态分配（Test-time scaling, TTS）策略进行了大规模比较分析，提出没有一种通用最优TTS，并给出了实际TTS选择建议。


<details>
  <summary>Details</summary>
Motivation: 虽然推理时动态分配计算资源（TTS）被认为能提升LLMs的推理能力，但目前缺少不同TTS策略在一致条件下的系统性对比研究，也不清楚模型类型和问题难度对推理表现的影响。该工作旨在填补这些空白，揭示最优TTS策略的选择依据。

Method: 作者在八个开源LLM模型（参数规模从7B到235B）上，使用四个推理数据集，生成超过三十亿个token，系统地实验和比较了多种知名TTS策略在推理任务下的表现，从而研究TTS表现随模型、问题难度和算力预算的变化。

Result: 1）没有任何单一的TTS策略在所有模型、任务和设置下始终最佳；2）推理模型根据任务难度和推断长度表现出两类明显的追踪质量趋势（短视野与长视野）；3）在同类模型下，随着分配更多算力，TTS表现会持续提升。

Conclusion: 研究为如何按需求选择TTS策略（综合考虑问题难度、模型类型和算力预算）提供了明确操作建议，对实际LLMs应用中的推理效率提升有较强指导意义。

Abstract: Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.

</details>


### [313] [Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling](https://arxiv.org/abs/2512.02010)
*Jack Cook,Junxian Guo,Guangxuan Xiao,Yujun Lin,Song Han*

Main category: cs.CL

TL;DR: 本文提出了一种改进的NVFP4量化算法4/6，显著提升了大语言模型在低精度（NVFP4）下的训练和推理表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的扩大，低精度数值格式（如NVFP4）受到关注，它们能带来速度和存储优势。但NVFP4在对所有矩阵运算操作数进行量化时，易在训练时发散、推理精度下降，需解决量化误差带来的性能问题。

Method: 提出4/6算法，每个数值块评估两种缩放因子，与标准NVFP4不同，针对FP4接近最大值时的量化误差进行优化，提高接近最大值的表示能力。此算法能高效在NVIDIA Blackwell GPU上实现，并可应用于多种后训练量化方法。

Result: 在transformer和混合架构的预训练实验中，4/6算法在多个场景下防止了发散，且训练损失与BF16接近，优于现有的NVFP4训练方案。同时，4/6可提升不同后训练量化策略的下游任务精度。

Conclusion: 4/6算法有效缓解了NVFP4下的性能瓶颈，为在低精度下训练和部署大语言模型提供了可行路径，并具备广泛适用性。

Abstract: As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [314] [DREAMer-VXS: A Latent World Model for Sample-Efficient AGV Exploration in Stochastic, Unobserved Environments](https://arxiv.org/abs/2512.00005)
*Agniprabha Chakraborty*

Main category: cs.RO

TL;DR: 提出了一种高效的基于模型的自动地面车（AGV）探索框架DREAMer-VXS，通过想象潜在轨迹进行规划，显著提高了学习效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的无模型强化学习方法在机器人实际应用中表现出样本效率低和策略不稳定，严重限制了其在现实场景中的部署。

Method: 构建了一个基于卷积变分自编码器（VAE）的世界模型，对高维激光雷达数据进行压缩表示，并结合递归状态空间模型（RSSM）捕捉环境时序动态，利用该模型作为高速模拟器，几乎全部在想象中训练导航策略；同时采用融合内在好奇心奖励的actor-critic方法，促进系统性探索。

Result: 与当前主流的无模型SAC基线相比，环境实际交互次数降低90%，探索未知环境效率提升45%，对动态障碍表现出更强的鲁棒性。

Conclusion: DREAMer-VXS极大提升了AGV探索任务的样本效率、泛化性与鲁棒性，为学习型移动机器人打开了现实落地的新局面。

Abstract: The paradigm of learning-based robotics holds immense promise, yet its translation to real-world applications is critically hindered by the sample inefficiency and brittleness of conventional model-free reinforcement learning algorithms. In this work, we address these challenges by introducing DREAMer-VXS, a model-based framework for Autonomous Ground Vehicle (AGV) exploration that learns to plan from imagined latent trajectories. Our approach centers on learning a comprehensive world model from partial and high-dimensional LiDAR observations. This world model is composed of a Convolutional Variational Autoencoder (VAE), which learns a compact representation of the environment's structure, and a Recurrent State-Space Model (RSSM), which models complex temporal dynamics. By leveraging this learned model as a high-speed simulator, the agent can train its navigation policy almost entirely in imagination. This methodology decouples policy learning from real-world interaction, culminating in a 90% reduction in required environmental interactions to achieve expert-level performance when compared to state-of-the-art model-free SAC baselines. The agent's behavior is guided by an actor-critic policy optimized with a composite reward function that balances task objectives with an intrinsic curiosity bonus, promoting systematic exploration of unknown spaces. We demonstrate through extensive simulated experiments that DREAMer-VXS not only learns orders of magnitude faster but also develops more generalizable and robust policies, achieving a 45% increase in exploration efficiency in unseen environments and superior resilience to dynamic obstacles.

</details>


### [315] [A Comprehensive Survey on Surgical Digital Twin](https://arxiv.org/abs/2512.00019)
*Afsah Sharaf Khan,Falong Fan,Doohwan DH Kim,Abdurrahman Alshareef,Dong Chen,Justin Kim,Ernest Carter,Bo Liu,Jerzy W. Rozenblit,Bernard Zeigler*

Main category: cs.RO

TL;DR: 本文综述了外科数字孪生（Surgical Digital Twins, SDTs）的最新进展、分类与挑战，提出了发展可信赖且合规SDT的未来研究方向，以加速其实验到临床应用的转化。


<details>
  <summary>Details</summary>
Motivation: 随着多模态外科数据和实时计算能力提升，SDTs作为虚拟镜像有望提升术前、术中与术后决策支持。当前SDT面临数据融合、效率、解释性、合规性等多方面挑战，为此需要系统梳理现状、问题和发展路径。

Method: 论文围绕术语定义、目的、模型精度和数据来源提出SDT的分类法，系统梳理了在形变配准与追踪、实时仿真、AR/VR导航、边缘-云协同及智能场景理解等关键领域的研究进展，并对非机器人及机器人协作SDT架构进行了对比。

Result: 文章总结了SDT领域主要成果，指出了验证、基准、安全与人因、数字线程集成及数据治理等待解决问题。提出组织术语、能力梳理和突出空白，助力SDT从实验室走向常规外科实践。

Conclusion: 统一术语、结构梳理能力及突出研究空白，有助于指导SDT设计与应用，推动其标准化发展，从而带来可衡量的临床价值。

Abstract: With the accelerating availability of multimodal surgical data and real-time computation, Surgical Digital Twins (SDTs) have emerged as virtual counterparts that mirror, predict, and inform decisions across pre-, intra-, and postoperative care. Despite promising demonstrations, SDTs face persistent challenges: fusing heterogeneous imaging, kinematics, and physiology under strict latency budgets; balancing model fidelity with computational efficiency; ensuring robustness, interpretability, and calibrated uncertainty; and achieving interoperability, privacy, and regulatory compliance in clinical environments. This survey offers a critical, structured review of SDTs. We clarify terminology and scope, propose a taxonomy by purpose, model fidelity, and data sources, and synthesize state-of-the-art achievements in deformable registration and tracking, real-time simulation and co-simulation, AR/VR guidance, edge-cloud orchestration, and AI for scene understanding and prediction. We contrast non-robotic twins with robot-in-the-loop architectures for shared control and autonomy, and identify open problems in validation and benchmarking, safety assurance and human factors, lifecycle "digital thread" integration, and scalable data governance. We conclude with a research agenda toward trustworthy, standards-aligned SDTs that deliver measurable clinical benefit. By unifying vocabulary, organizing capabilities, and highlighting gaps, this work aims to guide SDT design and deployment and catalyze translation from laboratory prototypes to routine surgical care.

</details>


### [316] [Foundation Models for Trajectory Planning in Autonomous Driving: A Review of Progress and Open Challenges](https://arxiv.org/abs/2512.00021)
*Kemal Oksuz,Alexandru Buburuzan,Anthony Knittel,Yuhan Yao,Puneet K. Dokania*

Main category: cs.RO

TL;DR: 本文综述了多模态基础模型在自动驾驶轨迹规划方面的最新进展，提出统一的分类法，评估了37种方法的设计、优缺点、代码和数据集的开放性，并提供了配套网页。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的出现，自动驾驶从传统依赖手工设计的方案转向了可直接从原始传感数据推断运动轨迹的统一基础模型方法。尤其是多模态（如视觉-语言-动作）模型的出现，带来了新的研究与实践挑战。作者动机在于总结和系统化该领域最新方法，帮助研究者和开发者理解现状、选择方案。

Method: 采用文献调研的方式，对37种近期提出的基于基础模型的自动驾驶轨迹规划方法进行系统梳理。引入统一的分类标准，对各方法的模型结构、优势、限制进行详细比较，评估其开源程度，并整理归档至网页供查阅。

Result: 整理了37个基于基础模型的轨迹预测/规划方法，提供了统一的归类、对比和评价。清晰呈现了不同方法在结构设计、能力、数据与代码开放性等方面的异同。

Conclusion: 多模态基础模型极大推动了自动驾驶技术的发展，但各方法在能力、开放性等方面存在差异。统一的分类框架和总结有助于推动该领域进一步发展，为业界和学界提供参考。

Abstract: The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories

</details>


### [317] [XFlowMP: Task-Conditioned Motion Fields for Generative Robot Planning with Schrodinger Bridges](https://arxiv.org/abs/2512.00022)
*Khang Nguyen,Minh Nhat Vu*

Main category: cs.RO

TL;DR: 本文提出一种新颖的生成式运动规划方法XFlowMP，通过薛定谔桥(Schrodinger bridges)将任务配置下的随机噪声与专家演示进行桥接，从而生成平滑、无碰撞且动力学可行的机器人运动轨迹，并在多个基准测试和实际机器人测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人运动规划方法难以兼顾高层语义与低层物理约束，尤其在任务目标与运动可控性之间的耦合建模方面存在不足，因此需要一种能够在任务条件下生成动力学可行、无碰撞且高效的运动轨迹的新方法。

Method: 通过引入薛定谔桥实现条件流匹配，结合得分函数(score function)学习多阶动力学运动场，编码起点-终点配置，有效桥接噪声与专家轨迹，形成一种任务条件下的生成式运动规划器XFlowMP。

Result: 在RobotPointMass基准上最大均值差下降53.79%、轨迹平滑度提升36.36%、能耗降低39.88%、短时规划时间减少11.72%；在LASA手写体数据集上长期轨迹最大均值差降低1.26%、平滑度提升3.96%、能耗下降31.97%。此外，在Kinova Gen3机械臂上实现了实地规划与验证。

Conclusion: XFlowMP能够在多任务和真实环境中生成高质量、动力学可行且高效的机器人运动轨迹，显著优于现有基线方法，具备良好的实用前景。

Abstract: Generative robotic motion planning requires not only the synthesis of smooth and collision-free trajectories but also feasibility across diverse tasks and dynamic constraints. Prior planning methods, both traditional and generative, often struggle to incorporate high-level semantics with low-level constraints, especially the nexus between task configurations and motion controllability. In this work, we present XFlowMP, a task-conditioned generative motion planner that models robot trajectory evolution as entropic flows bridging stochastic noises and expert demonstrations via Schrodinger bridges given the inquiry task configuration. Specifically, our method leverages Schrodinger bridges as a conditional flow matching coupled with a score function to learn motion fields with high-order dynamics while encoding start-goal configurations, enabling the generation of collision-free and dynamically-feasible motions. Through evaluations, XFlowMP achieves up to 53.79% lower maximum mean discrepancy, 36.36% smoother motions, and 39.88% lower energy consumption while comparing to the next-best baseline on the RobotPointMass benchmark, and also reducing short-horizon planning time by 11.72%. On long-horizon motions in the LASA Handwriting dataset, our method maintains the trajectories with 1.26% lower maximum mean discrepancy, 3.96% smoother, and 31.97% lower energy. We further demonstrate the practicality of our method on the Kinova Gen3 manipulator, executing planning motions and confirming its robustness in real-world settings.

</details>


### [318] [Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos](https://arxiv.org/abs/2512.00024)
*X. Hu,G. Ye*

Main category: cs.RO

TL;DR: 本论文提出了一种结合大规模视频理解基础模型和点跟踪技术的新方法，从网上人类操作视频中密集提取任务相关关键点轨迹，用于支持大规模机器人学习。实验表明该方法能准确跟踪操作过程的关键点，提升机器人学习的数据获取效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模机器人学习依赖真实机器人收集数据，过程繁琐且代价高昂。尽管不少方法试图利用网络上的人类操作视频，但多数只抓取手或物体的简单信息，忽略了视频中的丰富交互线索。因此，亟需更高效充分地从海量视频中提取有用信息，提高机器人的学习效率。

Method: 作者提出利用大规模视频理解基础模型与点追踪技术结合，从人类操作视频中自动获取所有任务相关关键点的密集轨迹。具体地，对视频先用基础大模型进行理解分割，再通过点跟踪方法提取完整操作过程中的关键点运动轨迹。

Result: 实验表明，该方法能够在整个操作过程中准确跟踪所有关键点，为后续的机器人训练和模仿学习等任务提供高质量、密集的数据基础。

Conclusion: 本方法大幅提升了从互联网规模人类操作视频中提取关键行为数据的能力，有助于提升机器人学习的可扩展性和数据效率，为数据驱动的机器人学习铺路。

Abstract: Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.

</details>


### [319] [A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation](https://arxiv.org/abs/2512.00027)
*Nivedan Yakolli,Avinash Gautam,Abhijit Das,Yuankai Qi,Virendra Singh Shekhawat*

Main category: cs.RO

TL;DR: 本文综述了视觉-语言导航（VLN）在机器人领域的最新进展，并展望了多机器人协作中的关键挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前VLN系统在机器人应用、尤其是多机器人系统中的高效协同与交流方面存在显著瓶颈，如双向沟通、歧义消解与协作决策等问题亟需突破。

Method: 作者系统回顾了约200篇相关文献，对VLN系统在机器人中的核心技术瓶颈、现有解决方案及未来潜力方向进行了全面评述。

Result: 调查显示现有VLN模型在自然语言交流、环境感知和多机器人任务协作上仍有限，强调了主动澄清、实时反馈和上下文推理等能力在提升系统智能化上的重要作用。同时提出分布式决策和动态角色分配框架对于实现大规模多机器人合作至关重要。

Conclusion: 未来VLN系统需结合更先进的自然语言理解技术及去中心化的协同决策，加强主动交流和上下文感知能力，推动机器人系统在医疗、物流与应急救援等实际场景的落地应用。

Abstract: Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.

</details>


### [320] [Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning](https://arxiv.org/abs/2512.00030)
*Zhaofan Zhang,Minghao Yang,Sihong Xie,Hui Xiong*

Main category: cs.RO

TL;DR: 该论文提出了DRIQN方法，结合分布式鲁棒优化和隐式分位数网络，以提升无人水面艇在异方差噪声条件下的鲁棒导航能力，显著超过现有技术。


<details>
  <summary>Details</summary>
Motivation: 无人水面艇在复杂、未知的海洋环境中面临多变的观测噪声，这对基于传感器的导航任务提出了巨大挑战。现有分布式强化学习方法忽视了不同环境下的噪声异质性，导致学习鲁棒性不足。作者旨在解决噪声模式变化对安全导航和价值函数学习的干扰问题。

Method: 论文提出DRIQN方法，将分布式鲁棒优化（DRO）与隐式分位数网络（Implicit Quantile Networks, IQN）结合，以优化不同环境条件下的最坏情况性能。具体做法包括在经验回放缓冲区中进行显式子群建模，从而纳入异质噪声和关键鲁棒性场景。

Result: 在风险敏感的仿真环境下，DRIQN方法相对于现有技术取得了明显提升：成功率提升13.51%，碰撞率降低12.28%，节省时间35.46%，节省能耗27.99%。

Conclusion: 实验结果表明，DRIQN方法显著提高了无人水面艇在面对异方差噪声环境下的鲁棒导航性能，为分布式强化学习在实际复杂环境中的应用提供了有力支撑。

Abstract: The robustness of Unmanned Surface Vehicles (USV) is crucial when facing unknown and complex marine environments, especially when heteroscedastic observational noise poses significant challenges to sensor-based navigation tasks. Recently, Distributional Reinforcement Learning (DistRL) has shown promising results in some challenging autonomous navigation tasks without prior environmental information. However, these methods overlook situations where noise patterns vary across different environmental conditions, hindering safe navigation and disrupting the learning of value functions. To address the problem, we propose DRIQN to integrate Distributionally Robust Optimization (DRO) with implicit quantile networks to optimize worst-case performance under natural environmental conditions. Leveraging explicit subgroup modeling in the replay buffer, DRIQN incorporates heterogeneous noise sources and target robustness-critical scenarios. Experimental results based on the risk-sensitive environment demonstrate that DRIQN significantly outperforms state-of-the-art methods, achieving +13.51\% success rate, -12.28\% collision rate and +35.46\% for time saving, +27.99\% for energy saving, compared with the runner-up.

</details>


### [321] [Intelligent Systems and Robotics: Revolutionizing Engineering Industries](https://arxiv.org/abs/2512.00033)
*Sathish Krishna Anumula,Sivaramkumar Ponnarangan,Faizal Nujumudeen,Ms. Nilakshi Deka,S. Balamuralitharan,M Venkatesh*

Main category: cs.RO

TL;DR: 本文综述了AI、机器学习与自主机器人技术在工程行业中的应用及其带来的影响。


<details>
  <summary>Details</summary>
Motivation: 工程行业亟需更高的效率、精度与适应性，智能系统和机器人正成为推动变革的关键。

Method: 通过最新研究和行业智能机器人系统的评估方法，结合实际案例和经验，分析其在相关工程领域中的应用成效和问题。

Result: 智能机器人技术提高了生产力，提升了安全性并降低了运营成本；尽管如此，仍存在一些待解决的难题。

Conclusion: 智能机器人不仅是技术上的变革，更为工程领域引入了创新的方法和理念。

Abstract: A mix of intelligent systems and robotics is making engineering industries much more efficient, precise and able to adapt. How artificial intelligence (AI), machine learning (ML) and autonomous robotic technologies are changing manufacturing, civil, electrical and mechanical engineering is discussed in this paper. Based on recent findings and a suggested way to evaluate intelligent robotic systems in industry, we give an overview of how their use impacts productivity, safety and operational costs. Experience and case studies confirm the benefits this area brings and the problems that have yet to be solved. The findings indicate that intelligent robotics involves more than a technology change; it introduces important new methods in engineering.

</details>


### [322] [Design And Control of A Robotic Arm For Industrial Applications](https://arxiv.org/abs/2512.00034)
*Sathish Krishna Anumula,SVSV Prasad Sanaboina,Ravi Kumar Nagula,R. Nagaraju*

Main category: cs.RO

TL;DR: 本文设计并实现了一款低成本、可扩展且可靠的六自由度工业机械臂，通过逆运动学和PID控制实现了高精度和高重复性的任务执行。


<details>
  <summary>Details</summary>
Motivation: 工业自动化需求快速增长，推动了机器人尤其是机械臂的应用。作者旨在设计一种适用于装配、焊接和物料搬运等工业任务的机械臂，以满足工业对精确性和高效性的要求。

Method: 设计并制造了基于伺服电机和微控制器接口的六自由度机械臂。本研究进行了运动学和动力学分析，以实现精确定位和有效负载，并采用逆运动学算法和PID控制器提升控制精度。

Result: 通过仿真和实验测试，系统能够高精度、高重复性地完成多种工业任务，验证了整体性能。

Conclusion: 所提出的机械臂方案兼具成本低、易扩展和高可靠性，为制造业中众多重复性流程自动化提供了有效解决方案。

Abstract: The growing need to automate processes in industrial settings has led to tremendous growth in the robotic systems and especially the robotic arms. The paper assumes the design, modeling and control of a robotic arm to suit industrial purpose like assembly, welding and material handling. A six-degree-of-freedom (DOF) robotic manipulator was designed based on servo motors and a microcontroller interface with Mechanical links were also fabricated. Kinematic and dynamic analyses have been done in order to provide precise positioning and effective loads. Inverse Kinematics algorithm and Proportional-Integral-Derivative (PID) controller were also applied to improve the precision of control. The ability of the system to carry out tasks with high accuracy and repeatability is confirmed by simulation and experimental testing. The suggested robotic arm is an affordable, expandable, and dependable method of automation of numerous mundane procedures in the manufacturing industry.

</details>


### [323] [ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM](https://arxiv.org/abs/2512.00037)
*Tali Orlev Shapira,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出了一种名为ICD-Net的新型神经网络框架，有效提升了视觉-惯性SLAM系统在无人机中的状态估计精度，显著优于标准VINS-Fusion。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-惯性SLAM系统在面对传感器误差、噪声、动态运动及光线不足等情况下，性能表现不佳，影响无人机等应用的安全自主导航。传统基于惯性导航积分的方法难以应对传感器的真实缺陷。因此，亟需一种更鲁棒且准确的状态估计算法。

Method: 提出ICD-Net神经网络，通过学习处理原始惯性测量，直接生成位移估计及其不确定性，同时输出作为额外残差约束引入VINS-Fusion优化，通过不确定性自适应调整网络对最终结果的贡献。方法既可正常运行也能够应对视觉信息丢失或退化。

Result: 在高动态无人机数据上，相比标准VINS-Fusion，ICD-Net将平均轨迹误差（APE）降低超38%，对估计的不确定性量化同样提升了系统鲁棒性。

Conclusion: ICD-Net展现了神经网络对视觉-惯性SLAM多种误差源的强大补偿效果，并能满足实时性要求，为复杂环境下的无人机自主导航提供了有效解决方案。

Abstract: Visual-inertial SLAM systems often exhibit suboptimal performance due to multiple confounding factors including imperfect sensor calibration, noisy measurements, rapid motion dynamics, low illumination, and the inherent limitations of traditional inertial navigation integration methods. These issues are particularly problematic in drone applications where robust and accurate state estimation is critical for safe autonomous operation. In this work, we present ICD-Net, a novel framework that enhances visual-inertial SLAM performance by learning to process raw inertial measurements and generating displacement estimates with associated uncertainty quantification. Rather than relying on analytical inertial sensor models that struggle with real-world sensor imperfections, our method directly extracts displacement maps from sensor data while simultaneously predicting measurement covariances that reflect estimation confidence. We integrate ICD-Net outputs as additional residual constraints into the VINS-Fusion optimization framework, where the predicted uncertainties appropriately weight the neural network contributions relative to traditional visual and inertial terms. The learned displacement constraints provide complementary information that compensates for various error sources in the SLAM pipeline. Our approach can be used under both normal operating conditions and in situations of camera inconsistency or visual degradation. Experimental evaluation on challenging high-speed drone sequences demonstrated that our approach significantly improved trajectory estimation accuracy compared to standard VINS-Fusion, with more than 38% improvement in mean APE and uncertainty estimates proving crucial for maintaining system robustness. Our method shows that neural network enhancement can effectively address multiple sources of SLAM degradation while maintaining real-time performance requirements.

</details>


### [324] [VISTAv2: World Imagination for Indoor Vision-and-Language Navigation](https://arxiv.org/abs/2512.00041)
*Yanjia Huang,Xianshun Jiang,Xiangbo Gao,Mingyang Wu,Zhengzhong Tu*

Main category: cs.RO

TL;DR: 本文提出了一种新的生成式世界模型VISTAv2，用于视觉-语言导航（VLN）任务，通过对候选动作序列进行想象预测，并与指令对齐，用于在线规划，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往基于图像想象的VLN方法局限于离散全景且缺乏在线、动作条件的预测，且未能提供显式的规划价值，存在规划效率低及鲁棒性差等问题。

Method: VISTAv2通过条件扩散变换器视频预测模型，结合动作历史、候选序列和语言指令，合成短期未来视图，并通过视觉-语言匹配打分，将多组想象结果融合到可微的价值映射头，生成自我中心的价值图，用于指导在线路径规划。为了高效性，在VAE潜在空间中进行辐射推理，并用稀疏解码实现低资源推理。

Result: 在MP3D和RoboTHOR两个VLN基准上，VISTAv2超过了强基线模型。消融实验表明，动作条件的想象、指令引导的价值融合以及在线价值图规划器都是性能提升的关键环节。

Conclusion: VISTAv2为VLN任务提供了一种高效、易解释且鲁棒的世界模型式路径规划方法，有望推进实际应用进展。

Abstract: Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.

</details>


### [325] [Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge](https://arxiv.org/abs/2512.00048)
*Wenzheng Zhao,Ran Zhang,Ruth Palan Lopez,Shu-Fen Wung,Fengpei Yuan*

Main category: cs.RO

TL;DR: 本论文提出了因果结构感知强化学习（CRL）框架，将因果发现与推理融入策略优化，用于提升医疗干预（如痴呆症关怀）中机器人的决策能力，该方法在模拟环境下展现了比传统RL更高的表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在自适应医疗干预场景中，传统强化学习方法面临数据稀缺、决策可解释性要求高、患者状态复杂且具有因果关系等多重挑战，因此需要能处理因果推断、提高决策透明度的新方法。

Method: 作者提出CRL方法，通过学习和利用描述人类行为状态与机器人行为之间因果依赖关系的有向无环图（DAG），将因果结构显式纳入策略优化流程，并应用于机器人辅助认知关护模拟环境。同时设计并测试了轻量级LLM部署，将策略嵌入系统提示以产生一致对话，无需微调。

Result: 实验证明CRL显著优于传统无模型RL基线，在累积奖励、状态维持及行为可解释性等方面表现更好，同时在不同权重及超参数设置下依然保持领先，且LLM嵌入策略可实现一致的支持性对话。

Conclusion: CRL框架展示了因果强化学习在以可解释性、自适应性和数据效率为核心要求的人机交互医疗应用中的前景，尤其适用于与人类行为状态紧密相关的复杂场景。

Abstract: Reinforcement Learning (RL) faces significant challenges in adaptive healthcare interventions, such as dementia care, where data is scarce, decisions require interpretability, and underlying patient-state dynamic are complex and causal in nature. In this work, we present a novel framework called Causal structure-aware Reinforcement Learning (CRL) that explicitly integrates causal discovery and reasoning into policy optimization. This method enables an agent to learn and exploit a directed acyclic graph (DAG) that describes the causal dependencies between human behavioral states and robot actions, facilitating more efficient, interpretable, and robust decision-making. We validate our approach in a simulated robot-assisted cognitive care scenario, where the agent interacts with a virtual patient exhibiting dynamic emotional, cognitive, and engagement states. The experimental results show that CRL agents outperform conventional model-free RL baselines by achieving higher cumulative rewards, maintaining desirable patient states more consistently, and exhibiting interpretable, clinically-aligned behavior. We further demonstrate that CRL's performance advantage remains robust across different weighting strategies and hyperparameter settings. In addition, we demonstrate a lightweight LLM-based deployment: a fixed policy is embedded into a system prompt that maps inferred states to actions, producing consistent, supportive dialogue without LLM finetuning. Our work illustrates the promise of causal reinforcement learning for human-robot interaction applications, where interpretability, adaptiveness, and data efficiency are paramount.

</details>


### [326] [Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches](https://arxiv.org/abs/2512.00049)
*Ibrahim Khalil Kabir,Muhammad Faizan Mysorewala*

Main category: cs.RO

TL;DR: 本文综述了基于深度强化学习（DRL）的社会意识导航方法，探讨了其在提升机器人与人类环境交互中的应用、优势及挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器人逐步进入人类日常生活，实现其在复杂社交环境中遵守隐性社交规范、保障人类舒适与安全成为亟需解决的问题。利用DRL为导航决策引入社会意识，是推动机器人实际落地的重要方向。

Method: 系统回顾各种DRL方法，包括价值型、策略型、Actor-Critic等，以及它们与不同神经网络结构（前馈、循环、卷积、图网络、Transformer）的结合，并分析社会相关的核心因素如人类亲近感、舒适性、轨迹及意图预测等。同时，探讨了相关评估机制、数据集、模拟环境以及从仿真到真实迁移的关键难题。

Result: DRL在安全性、人类接受度等方面明显优于传统方法，但在评估机制、标准化社交度量、计算负担、可扩展性和现实应用迁移等方面仍存在显著挑战。

Conclusion: 未来应采用混合方法融合多种技术优势，同时推动技术效率和以人为本的评测基准的建立，为社会意识导航实践化奠定基础。

Abstract: Socially aware navigation is a fast-evolving research area in robotics that enables robots to move within human environments while adhering to the implicit human social norms. The advent of Deep Reinforcement Learning (DRL) has accelerated the development of navigation policies that enable robots to incorporate these social conventions while effectively reaching their objectives. This survey offers a comprehensive overview of DRL-based approaches to socially aware navigation, highlighting key aspects such as proxemics, human comfort, naturalness, trajectory and intention prediction, which enhance robot interaction in human environments. This work critically analyzes the integration of value-based, policy-based, and actor-critic reinforcement learning algorithms alongside neural network architectures, such as feedforward, recurrent, convolutional, graph, and transformer networks, for enhancing agent learning and representation in socially aware navigation. Furthermore, we examine crucial evaluation mechanisms, including metrics, benchmark datasets, simulation environments, and the persistent challenges of sim-to-real transfer. Our comparative analysis of the literature reveals that while DRL significantly improves safety, and human acceptance over traditional approaches, the field still faces setback due to non-uniform evaluation mechanisms, absence of standardized social metrics, computational burdens that limit scalability, and difficulty in transferring simulation to real robotic hardware applications. We assert that future progress will depend on hybrid approaches that leverage the strengths of multiple approaches and producing benchmarks that balance technical efficiency with human-centered evaluation.

</details>


### [327] [Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control](https://arxiv.org/abs/2512.00050)
*Suzie Kim*

Main category: cs.RO

TL;DR: 该论文提出了一种基于非侵入式脑电信号（EEG）的隐式人类反馈强化学习（RLIHF）框架，用于无需人工干预地提升稀疏奖励下的强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在稀疏奖励环境下难以有效学习策略，通常需要人工设计复杂奖励函数。现有基于人类反馈的方法多依赖显式反馈，增加了用户负担，影响自然交互。本研究希望通过隐式反馈优化强化学习过程，减少对手工设计奖励和显式用户反馈的依赖。

Method: 采用非侵入式EEG技术获取用户的错误相关电位（ErrPs）信号，通过预训练解码器将原始脑电信号转化为概率化奖励信号，为智能体提供隐式反馈。并在MuJoCo物理引擎仿真环境下，对Kinova Gen2机械臂完成复杂操作任务进行实验验证。

Result: 实验结果显示，利用解码后的EEG隐式奖励训练的强化学习智能体，其表现可与使用人工密集奖励设计训练出的智能体媲美。

Conclusion: 基于隐式神经反馈的强化学习方法可行，能够推动人机交互中更加自然、高效、可扩展的强化学习范式，具有良好的应用前景。

Abstract: Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.

</details>


### [328] [Modeling and Control of Magnetic Forces between Microrobots](https://arxiv.org/abs/2512.00051)
*Amelia Fernández Seguel,Alejandro I. Maass*

Main category: cs.RO

TL;DR: 本研究提出一种新的级联控制方法，可仅通过全局磁场角度独立调节两个磁微机器人（微代理）之间的径向距离，并实现快速且平滑的控制，适用于实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有磁微机器人系统难以在共享全局信号下实现多机器人独立控制，限制了其在药物精准投送、微创手术等场景下的应用，因此亟需能区分驱动多个微机器人的控制方案。

Method: 论文通过建立磁偶极-偶极相互作用和粘性介质速度的物理模型，设计了一级PID径向距离控制器，并串联一个PD控制器用于平滑磁场角度变化。全部控制策略在MATLAB中仿真完成。

Result: PID控制器仅用约60%的原始时间即可收敛至目标半径，增加PD级后能在类似时间内使角度变动平滑，角度波动仅±5°，综合验证了控制方式的快速性、精确性和平滑性。

Conclusion: 级联PID+PD控制方案可以在不突变磁场角度的情况下，实现两台微机器人间的精确径向距离控制，适合实际应用。但模型目前仅限于二维及两代理，未来可扩展至多代理和三维空间。

Abstract: The independent control of multiple magnetic microrobots under a shared global signal presents critical challenges in biomedical applications such as targeted drug delivery and microsurgeries. Most existing systems only allow all agents to move synchronously, limiting their use in applications that require differentiated actuation. This research aims to design a controller capable of regulating the radial distance between micro-agents using only the angle ψof a global magnetic field as the actuation parameter, demonstrating potential for practical applications. The proposed cascade control approach enables faster and more precise adjustment of the inter-agent distance than a proportional controller, while maintaining smooth transitions and avoiding abrupt changes in the orientation of the magnetic field, making it suitable for real-world implementation. A bibliographic review was conducted to develop the physical model, considering magnetic dipole-dipole interactions and velocities in viscous media. A PID controller was implemented to regulate the radial distance, followed by a PD controller in cascade to smooth changes in field orientation. These controllers were simulated in MATLAB, showing that the PID controller reduced convergence time to the desired radius by about 40%. When adding the second controller, the combined PID+PD scheme achieved smooth angular trajectories within similar timeframes, with fluctuations of only \pm 5^\circ. These results validate the feasibility of controlling the radial distance of two microrobots using a shared magnetic field in a fast and precise manner, without abrupt variations in the control angle. However, the model is limited to a 2D environment and two agents, suggesting future research to extend the controller to 3D systems and multiple agents.

</details>


### [329] [An adaptive experience-based discrete genetic algorithm for multi-trip picking robot task scheduling in smart orchards](https://arxiv.org/abs/2512.00057)
*Peng Chen,Jing Liangb,Kang-Jia Qiao,Hui Song,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文旨在解决智能果园多机器人采摘任务调度（MTPRTS）中的优化问题，提出了一种自适应经验驱动的离散遗传算法（AEDGA），在负载平衡初始化、聚类的局部搜索和自适应选择上进行了创新，实验表明新方法优于现有八种主流算法。


<details>
  <summary>Details</summary>
Motivation: 随着智能机器人技术在果园自动采摘中的应用，对多机器人系统的调度优化需求激增，主要目的是应对劳动力短缺和成本上升，同时解决多机器人调度中的复杂优化难题（如负载、能耗、工期约束等交互影响）。

Method: 提出自适应经验驱动离散遗传算法（AEDGA），包含负载-距离集成初始化、基于聚类的局部搜索机制和基于经验的自适应选择策略；并增加了三种工期可行解修复策略，解决任务调度中的局部最优和计算效率瓶颈。

Result: 在18个自建实例和24个公开实例上的综合实验表明，AEDGA算法在解质量和效率上明显优于八种最先进的对比算法。

Conclusion: AEDGA方法能高效、稳定地解决多机器人采摘任务调度中的复杂优化问题，尤其适用于大规模场景，推动智能果园自动系统的发展。

Abstract: The continuous innovation of smart robotic technologies is driving the development of smart orchards, significantly enhancing the potential for automated harvesting systems. While multi-robot systems offer promising solutions to address labor shortages and rising costs, the efficient scheduling of these systems presents complex optimization challenges. This research investigates the multi-trip picking robot task scheduling (MTPRTS) problem. The problem is characterized by its provision for robot redeployment while maintaining strict adherence to makespan constraints, and encompasses the interdependencies among robot weight, robot load, and energy consumption, thus introducing substantial computational challenges that demand sophisticated optimization algorithms.To effectively tackle this complexity, metaheuristic approaches, which often utilize local search mechanisms, are widely employed. Despite the critical role of local search in vehicle routing problems, most existing algorithms are hampered by redundant local operations, leading to slower search processes and higher risks of local optima, particularly in large-scale scenarios. To overcome these limitations, we propose an adaptive experience-based discrete genetic algorithm (AEDGA) that introduces three key innovations: (1) integrated load-distance balancing initialization method, (2) a clustering-based local search mechanism, and (3) an experience-based adaptive selection strategy. To ensure solution feasibility under makespan constraints, we develop a solution repair strategy implemented through three distinct frameworks. Comprehensive experiments on 18 proposed test instances and 24 existing test problems demonstrate that AEDGA significantly outperforms eight state-of-the-art algorithms.

</details>


### [330] [SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning](https://arxiv.org/abs/2512.00062)
*Taewook Nam,Sung Ju Hwang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SpeedAug的机器人控制策略加速方法，通过对不同速度演示的预训练，结合强化学习微调，实现了在不增加额外示范的情况下大幅提升机器人的任务执行速度和采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略学习尽管在复杂操作取得很大进展，但受限于示范数据采集速度，导致执行速度远低于硬件上限。直接加速现有策略易引发分布偏移，强化学习虽然可以自适应加速，却样本效率低，难以实际应用。因此，亟需一种兼顾速度提升与采样效率的策略加速方案。

Method: 提出SpeedAug框架，先通过对不同速度的任务演示训练得到包含多种节奏信息的策略行为先验，然后以该先验为基础，用强化学习方法对策略进行微调，实现高效适应于更快的执行速度，无需额外人工采集加速演示。

Result: 在多项机器人操作基准测试中，SpeedAug展现出显著优势。其RL微调后的策略在保持高任务成功率的同时，大幅提升了强化学习及主流加速方法的样本效率。

Conclusion: SpeedAug有效解决了机器人策略加速过程中的示范不足与分布偏移问题，验证了通过速度增广预训练引入多样节奏行为先验，可以显著提升策略加速的效果与采样效率，对实际机器人部署具有重要价值。

Abstract: Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.

</details>


### [331] [Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals](https://arxiv.org/abs/2512.00069)
*Ohad Bachner,Bar Gamliel*

Main category: cs.RO

TL;DR: 本文通过结合大语言模型（LLM）和符号规划，提高了机器人在执行日常任务时的成功率和适应性。


<details>
  <summary>Details</summary>
Motivation: 机器人根据传统符号规划器执行任务时，由于指令中缺失常识性细节（如隐藏条件和子目标）较易失败，将全部细节显式写入又繁琐且难以穷尽。

Method: 作者将大语言模型与符号规划结合：输入自然语言任务后，LLM 生成可能的前置条件和子目标，将这些建议翻译成形式化规划模型，再在仿真中进行执行。

Result: 与传统（无LLM辅助）规划器相比，该系统生成的计划更有效、任务成功率更高，对环境变化适应更好。

Conclusion: 在传统规划中融入LLM常识，可以显著提升机器人在真实环境下的可靠性和表现。

Abstract: Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.

</details>


### [332] [Reconfigurable Auxetic Devices (RADs) for Robotic Surface Manipulation](https://arxiv.org/abs/2512.00072)
*Jacob Miske,Ahyan Maya,Ahnaf Inkiad,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文提出并演示了一种可重构的负泊松比（海绵状）材料格子结构，用于机器人表面的可变形操控，展示了其在表面顺应性和表面变形方面的新能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器人表面多采用正泊松比材料，限制了柔性和顺应性。负泊松比的海绵状材料能够多方向扩展，有望打造更顺应、多变的机器人界面，因此探究其可重构性和应用性能具有前沿意义。

Method: 设计并实现了带有锁定和嵌入伺服驱动的可重构海绵格子，通过单元间反冲结构实现局部变形和表面扩展，结合动力学模型对系统行为建模并实验验证。

Result: 实验表明，该结构实现了表面可变收缩和扩展，并能通过单元间反冲实现持续柔顺。实验数据与简化动力学模型良好契合，有效描述了动力学和变形机理。

Conclusion: 可重构的负泊松比材料结构为机器人表面操作开辟了新路径，具备自适应、表面顺应性强等优点，有助于拓展机器人在复杂操作中的应用前景。

Abstract: Robotic surfaces traditionally use materials with a positive Poisson's ratio to push and pull on a manipulation interface. Auxetic materials with a negative Poisson's ratio may expand in multiple directions when stretched and enable conformable interfaces. Here we demonstrate reconfigurable auxetic lattices for robotic surface manipulation. Our approach enables shape control through reconfigurable locking or embedded servos that underactuate an auxetic lattice structure. Variable expansion of local lattice areas is enabled by backlash between unit cells. Demonstrations of variable surface conformity are presented with characterization metrics. Experimental results are validated against a simplified model of the system, which uses an activation function to model intercell coupling with backlash. Reconfigurable auxetic structures are shown to achieve manipulation via variable surface contraction and expansion. This structure maintains compliance with backlash in contrast with previous work on auxetics, opening new opportunities in adaptive robotic structures for surface manipulation tasks.

</details>


### [333] [Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning](https://arxiv.org/abs/2512.00074)
*Qiwei Liang,Boyang Cai,Minghao Lai,Sitong Zhuang,Tao Lin,Yan Qin,Yixuan Ye,Jiaming Liang,Renjing Xu*

Main category: cs.RO

TL;DR: 本论文提出了AFRO框架，通过无监督方式学习能够理解动力学的3D表示，显著提升机器人操作任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉预训练方法在识别和分割任务上效果优异，但在机器人操作（操控）任务上表现欠佳。作者认为原因在于现有方法缺少状态-动作-状态动力学建模，以及过度关注几何重建带来的冗余。

Method: 提出AFRO，一种自监督无监督框架，不需要动作或重建监督即可学习动力学敏感的3D表征。其核心创新包括：将状态预测建模为生成扩散过程、在共享潜空间联合建模前向和逆向动力学、采用特征差分和逆向一致性监督，防止特征泄露与提升特征质量。

Result: AFRO结合Diffusion Policy后，在16项仿真及4项真实机器人操控任务上操作成功率远超现有预训练方法，并且该框架能够随数据量和任务复杂度扩展。可视化结果显示AFRO学到了有语义且判别性强的特征。

Conclusion: AFRO为机器人3D表征学习提供了有效的预训练方案，克服了以往纯视觉预训练在操控任务中的短板，对实际机器人场景有较好适应性。

Abstract: Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/

</details>


### [334] [Arcadia: Toward a Full-Lifecycle Framework for Embodied Lifelong Learning](https://arxiv.org/abs/2512.00076)
*Minghe Gao,Juncheng Li,Yuze Lin,Xuqi Liu,Jiaming Ji,Xiaoran Pan,Zihan Xu,Xian Li,Mingjie Li,Wei Ji,Rong Wei,Rui Tang,Qizhou Wang,Kai Shen,Jun Xiao,Qi Wu,Siliang Tang,Yueting Zhuang*

Main category: cs.RO

TL;DR: Arcadia提出了一种全闭环生命周期框架，通过紧密结合数据采集、场景重建、统一表示和仿真评估四个阶段，实现具身学习的持续改进和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前具身学习系统多仅优化数据收集、模拟、学习或部署某一环节，难以持续改进且泛化性弱。作者认为要解决具身学习长期、广泛适应的问题，需要从生命周期角度整体建模和优化。

Method: Arcadia框架包括：1）自主数据探索和采集，2）生成式场景重建与扩展，3）统一导航与操作的多模态共享表示，4）基于仿真的Sim-from-real评估与反馈，形成不可拆分的闭环。

Result: 在导航和操作任务的基准测试上，Arcadia带来持续性能提升，并能鲁棒迁移至真实机器人平台，展现了优秀的泛化能力。

Conclusion: 紧密闭环的具身学习生命周期框架能够支持终身改进和端到端泛化；Arcadia为通用具身智能体研究提供了可扩展的基础，已发布标准化接口促进复现与模型对比。

Abstract: We contend that embodied learning is fundamentally a lifecycle problem rather than a single-stage optimization. Systems that optimize only one link (data collection, simulation, learning, or deployment) rarely sustain improvement or generalize beyond narrow settings. We introduce Arcadia, a closed-loop framework that operationalizes embodied lifelong learning by tightly coupling four stages: (1) Self-evolving exploration and grounding for autonomous data acquisition in physical environments, (2) Generative scene reconstruction and augmentation for realistic and extensible scene creation, (3) a Shared embodied representation architecture that unifies navigation and manipulation within a single multimodal backbone, and (4) Sim-from-real evaluation and evolution that closes the feedback loop through simulation-based adaptation. This coupling is non-decomposable: removing any stage breaks the improvement loop and reverts to one-shot training. Arcadia delivers consistent gains on navigation and manipulation benchmarks and transfers robustly to physical robots, indicating that a tightly coupled lifecycle: continuous real-world data acquisition, generative simulation update, and shared-representation learning, supports lifelong improvement and end-to-end generalization. We release standardized interfaces enabling reproducible evaluation and cross-model comparison in reusable environments, positioning Arcadia as a scalable foundation for general-purpose embodied agents.

</details>


### [335] [A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs](https://arxiv.org/abs/2512.00077)
*Bowen Zhi*

Main category: cs.RO

TL;DR: 本论文提出了一种层次化控制架构，通过结合学习驱动的步态生成与模型控制的平衡机制，有效提升了带有超数肢体（SLs）的人形机器人在行走过程中的稳定性。


<details>
  <summary>Details</summary>
Motivation: 超数肢体（SLs）的集成为人形机器人带来了新的动态干扰，严重威胁其运动稳定性。因此，亟需新的控制方法以保证带有SLs的机器人在复杂任务下依然能够稳定行走。

Method: 论文提出分层解耦控制策略：底层使用模仿学习和课程学习生成Unitree H1的基本步态；高层则主动调控SLs，实现动态平衡。该系统在物理仿真中通过三种情形（基线、静态SL、动态平衡控制）进行对比评估。

Result: 动态平衡控制器明显提升了带有SLs机器人的稳定性：与仅静态负载SLs相比，步态更接近基线，质心轨迹的DTW距离下降了47%，并在步态周期中有更好的再平衡能力和更协调的地面反作用力模式。

Conclusion: 分层解耦的控制架构能够有效抑制超数肢体带来的内部动态扰动，从而实现装备功能型SLs的人形机器人的稳定运动。这验证了该方案的实用性和有效性。

Abstract: The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.

</details>


### [336] [Hyper-GoalNet: Goal-Conditioned Manipulation Policy Learning with HyperNetworks](https://arxiv.org/abs/2512.00085)
*Pei Zhou,Wanting Yao,Qian Luo,Xunzhe Zhou,Yanchao Yang*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于超网络的目标条件策略学习方法（Hyper-GoalNet），相比以往方法在复杂任务中表现更佳，尤其在高变化环境下展现出了显著性能提升，并且在真实机器人实验中展示了对噪声和物理不确定性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人目标条件策略学习方法在面对多样目标和环境时难以持续保持性能，主要因为目标解释与状态处理耦合，难以有效泛化和提升对变化环境的适应能力。

Method: 提出Hyper-GoalNet框架，利用超网络根据目标动态生成对应的策略网络参数，将目标解释与状态处理分离；同时在潜在空间引入正向动力学模型和距离约束，以强化策略生成的有效性和向目标状态的单调收敛性。

Result: 在多种机器人操作任务和环境随机性测试下，Hyper-GoalNet明显优于现有方法，尤其在高变化、复杂条件下表现突出，同时在真实机器人的实验中也验证了对噪声和物理扰动的鲁棒性。

Conclusion: Hyper-GoalNet有效提升了目标条件策略在复杂实际环境中的泛化力和鲁棒性，为机器人多目标任务学习和执行提供了新途径。

Abstract: Goal-conditioned policy learning for robotic manipulation presents significant challenges in maintaining performance across diverse objectives and environments. We introduce Hyper-GoalNet, a framework that generates task-specific policy network parameters from goal specifications using hypernetworks. Unlike conventional methods that simply condition fixed networks on goal-state pairs, our approach separates goal interpretation from state processing -- the former determines network parameters while the latter applies these parameters to current observations. To enhance representation quality for effective policy generation, we implement two complementary constraints on the latent space: (1) a forward dynamics model that promotes state transition predictability, and (2) a distance-based constraint ensuring monotonic progression toward goal states. We evaluate our method on a comprehensive suite of manipulation tasks with varying environmental randomization. Results demonstrate significant performance improvements over state-of-the-art methods, particularly in high-variability conditions. Real-world robotic experiments further validate our method's robustness to sensor noise and physical uncertainties. Code is available at: https://github.com/wantingyao/hyper-goalnet.

</details>


### [337] ["Why the face?": Exploring Robot Error Detection Using Instrumented Bystander Reactions](https://arxiv.org/abs/2512.00262)
*Maria Teresa Parreira,Ruidong Zhang,Sukruth Gowdru Lingaraju,Alexandra Bremers,Xuanyu Fang,Adolfo Ramirez-Aristizabal,Manaswi Saha,Michael Kuniavsky,Cheng Zhang,Wendy Ju*

Main category: cs.RO

TL;DR: 本文提出了一种创新性的通过颈部佩戴设备从下巴区域采集面部表情信息的方法，用于机器人检测和理解人类对机器人错误反应的研究，并展示其优于现有技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前机器人很难准确感知和理解人类对其行为的细微社交反应，而这些社交提示对机器人融入人类环境至关重要。现有感知方法（如常规摄像头数据或OpenFace等分析工具）尚不能充分利用隐藏或微妙的人类反应。

Method: 1) 设计了一种颈部佩戴设备，专门从下巴区域录制人类面部表情；2) 提出了NeckNet-18 3D面部重建模型，将采集到的数据映射为面部特征点和头部运动信息；3) 利用这些特征训练了机器人错误检测模型，并与现有主流方法进行比较。

Result: 所提出的基于颈部摄像设备和NeckNet-18的机器人错误检测模型，在检测和识别人类对机器人错误的反应时，尤其是在单个受试者内部泛化性能方面，优于OpenFace等传统方法。

Conclusion: 该方法开辟了以人为中心的机器人感知新路径，有助于机器人更好地理解和适应人类社会环境，提升社交线索检测水平，对未来可适应的社交机器人发展具有重要意义。

Abstract: How do humans recognize and rectify social missteps? We achieve social competence by looking around at our peers, decoding subtle cues from bystanders - a raised eyebrow, a laugh - to evaluate the environment and our actions. Robots, however, struggle to perceive and make use of these nuanced reactions. By employing a novel neck-mounted device that records facial expressions from the chin region, we explore the potential of previously untapped data to capture and interpret human responses to robot error. First, we develop NeckNet-18, a 3D facial reconstruction model to map the reactions captured through the chin camera onto facial points and head motion. We then use these facial responses to develop a robot error detection model which outperforms standard methodologies such as using OpenFace or video data, generalizing well especially for within-participant data. Through this work, we argue for expanding human-in-the-loop robot sensing, fostering more seamless integration of robots into diverse human environments, pushing the boundaries of social cue detection and opening new avenues for adaptable robotics.

</details>


### [338] [RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals](https://arxiv.org/abs/2512.00287)
*Yuzheng Gao,Yuxing Long,Lei Kang,Yuchong Guo,Ziyan Yu,Shangqing Mao,Jiyao Zhang,Ruihai Wu,Dongjiang Li,Hui Shen,Hao Dong*

Main category: cs.RO

TL;DR: 现有家电资产存在渲染差、机制不全和与手册对齐差等问题，影响家电操作开发。本文提出了RealAppliance数据集和RealAppliance-Bench基准，评测多模态大模型及具身操作规划模型的相关任务。


<details>
  <summary>Details</summary>
Motivation: 传统家电模拟资产在物理、机制和熏陶真实性方面存在缺陷，且与真实的用户手册不符，导致模拟与现实之间存在较大差距，进而阻碍了家电操作研究领域的进步。

Method: 作者构建了RealAppliance数据集，包含100种高保真家电资产，具备完整的物理结构、电子机制和程序逻辑，并与其说明手册一一对齐。在此基础上，提出RealAppliance-Bench基准，涵盖手册页检索、部件定位、开环操作规划及闭环操作调整四项评测任务，用于系统性评估多模态大语言模型和具身操作规划模型。

Result: 通过在RealAppliance-Bench上分析主流模型，揭示了当前模型在各项家电操作任务中的优势与不足，为后续改进提供了数据支持和研究线索。

Conclusion: RealAppliance数据集和评测基准的提出，为具身智能体在家电操作场景下的能力提升提供了关键支持，有望推动该领域的技术研究和应用进展。

Abstract: Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research

</details>


### [339] [MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation](https://arxiv.org/abs/2512.00324)
*Jinda Du,Jieji Ren,Qiaojun Yu,Ningbin Zhang,Yu Deng,Xingyu Wei,Yufei Liu,Guoying Gu,Xiangyang Zhu*

Main category: cs.RO

TL;DR: 本论文提出了MILE系统，一种从人体手、外骨骼到仿生机械手的全新同构远程操作与数据采集系统，有效提升灵巧手模仿学习的数据采集质量和效率。


<details>
  <summary>Details</summary>
Motivation: 灵巧手操作的模仿学习受到高质量大规模数据集匮乏的限制。现有数据采集流程在动作映射、效率与触觉分辨率上存在不足，亟需新的系统提升数据质量与采集效率。

Method: 设计了MILE系统，并协同开发了仿手外骨骼与同构机械手，保证一对一的关节位置映射。外骨骼结构以人的手为基础，机械手整合了高分辨率的指尖视觉触觉模块，能精确获取多模态数据（指尖触觉、RGB-D图像、关节位置）。无需非线性动作映射，实现自然精确的远程操作及数据采集。

Result: MILE外骨骼的多关节平均绝对角误差低于1度。远程操作流程平均成功率提升64%，集成触觉传感后较视觉基线再提升25%。高分辨率触觉数据验证了数据集的高保真度和实用性。

Conclusion: MILE系统显著提升灵巧手模仿学习的数据获取质量和效率，为进一步推动基于高保真数据的仿人手操作智能体研究奠定了基础。

Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.

</details>


### [340] [DPNet: Doppler LiDAR Motion Planning for Highly-Dynamic Environments](https://arxiv.org/abs/2512.00375)
*Wei Zuo,Zeyi Ren,Chengyang Li,Yikun Wang,Mingle Zhao,Shuai Wang,Wei Sui,Fei Gao,Yik-Chung Wu,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种将Doppler LiDAR与运动规划器结合的新方法，并设计了Doppler规划网络（DPNet），有效应对快速运动障碍物，实现高频高精度的轨迹跟踪和规划。


<details>
  <summary>Details</summary>
Motivation: 现有运动规划方法难以应对快速变化的障碍物，主要因为对环境动态变化理解不足，尤其缺乏对障碍物速度的实时感知和利用。

Method: 提出了Doppler Kalman神经网络（D-KalmanNet）用于跟踪障碍物状态，并基础上构建了Doppler调优的模型预测控制（DT-MPC）框架，实现自主运动规划。两者均结合了Doppler LiDAR的速度感知能力，以较少的数据适配环境变化。

Result: DPNet在高保真仿真和真实数据集实验中，均优于现有多种基线方法，在障碍物跟踪和路径规划精度、效率方面表现突出。

Conclusion: 将Doppler LiDAR融入运动规划极大提升了对快速障碍物的适应性，所提DPNet方法实现了兼具高精度与高频率的高效规划，具有较强的实际应用价值。

Abstract: Existing motion planning methods often struggle with rapid-motion obstacles due to an insufficient understanding of environmental changes. To address this limitation, we propose integrating motion planners with Doppler LiDARs which provide not only ranging measurements but also instantaneous point velocities. However, this integration is nontrivial due to the dual requirements of high accuracy and high frequency. To this end, we introduce Doppler Planning Network (DPNet), which tracks and reacts to rapid obstacles using Doppler model-based learning. Particularly, we first propose a Doppler Kalman neural network (D-KalmanNet) to track the future states of obstacles under partially observable Gaussian state space model. We then leverage the estimated motions to construct a Doppler-tuned model predictive control (DT-MPC) framework for ego-motion planning, enabling runtime auto-tuning of the controller parameters. These two model-based learners allow DPNet to maintain lightweight while learning fast environmental changes using minimum data, and achieve both high frequency and high accuracy in tracking and planning. Experiments on both high-fidelity simulator and real-world datasets demonstrate the superiority of DPNet over extensive benchmark schemes.

</details>


### [341] [Balancing Efficiency and Fairness: An Iterative Exchange Framework for Multi-UAV Cooperative Path Planning](https://arxiv.org/abs/2512.00410)
*Hongzong Li,Luwei Liao,Xiangguang Dai,Yuming Feng,Rong Feng,Shiqin Tang*

Main category: cs.RO

TL;DR: 该论文提出了一种创新的多无人机协同路径规划迭代交换框架，能够在提高执行效率和任务分配公平性之间实现更优权衡。


<details>
  <summary>Details</summary>
Motivation: 在多无人机系统任务执行中，既需要最小化整体任务成本（例如总飞行距离），又要避免某些无人机负担过重，实现任务公平分配。目前的方法难以兼顾效率和公平两者。

Method: 作者提出一个迭代交换框架，将总任务距离和最大任务时长（makespan）结合为复合优化目标。通过无人机之间的局部任务交换与路径细化，在可行性与安全性约束下不断改进方案。每台无人机的无碰撞轨迹基于地形感知A*算法生成。

Result: 在多个地形数据集上的实验显示，该方法在总距离和任务时长之间实现了比现有基线更优的折中，提升了路径规划的效率与公平性。

Conclusion: 该方法能够有效权衡多无人机协同任务中的效率和公平性，有望推广应用于实际复杂环境下的无人机路径规划。

Abstract: Multi-UAV cooperative path planning (MUCPP) is a fundamental problem in multi-agent systems, aiming to generate collision-free trajectories for a team of unmanned aerial vehicles (UAVs) to complete distributed tasks efficiently. A key challenge lies in achieving both efficiency, by minimizing total mission cost, and fairness, by balancing the workload among UAVs to avoid overburdening individual agents. This paper presents a novel Iterative Exchange Framework for MUCPP, balancing efficiency and fairness through iterative task exchanges and path refinements. The proposed framework formulates a composite objective that combines the total mission distance and the makespan, and iteratively improves the solution via local exchanges under feasibility and safety constraints. For each UAV, collision-free trajectories are generated using A* search over a terrain-aware configuration space. Comprehensive experiments on multiple terrain datasets demonstrate that the proposed method consistently achieves superior trade-offs between total distance and makespan compared to existing baselines.

</details>


### [342] [Hardware-Software Collaborative Computing of Photonic Spiking Reinforcement Learning for Robotic Continuous Control](https://arxiv.org/abs/2512.00427)
*Mengting Yu,Shuiying Xiang,Changjian Xie,Yonghang Chen,Haowen Zhao,Xingxing Guo,Yahui Zhang,Yanan Han,Yue Hao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于光子突触脉冲强化学习的新型计算架构，有效提升了机器人连续控制任务的能效和实时性。


<details>
  <summary>Details</summary>
Motivation: 针对机器人连续控制任务高维状态空间和实时交互需求，传统电子计算架构在计算效率和能耗方面存在瓶颈。

Method: 提出了一种光-电混合计算架构，将硅基光子Mach-Zehnder干涉仪（MZI）芯片用于线性矩阵运算，非线性脉冲激活在电子域完成。架构结合TD3算法和脉冲神经网络，实现了软硬件协同推理。

Result: 在Pendulum-v1和HalfCheetah-v2基准测试中实现了较高奖励（5831），减少了23.33%的收敛步数，动作偏差低于2.2%。系统能效达1.39 TOPS/W，计算延迟低至120皮秒。

Conclusion: 首次在机器人连续控制任务中应用可编程MZI光子芯片，展示了光子脉冲强化学习在实时自主和工业机器人系统中的潜力。

Abstract: Robotic continuous control tasks impose stringent demands on the energy efficiency and latency of computing architectures due to their high-dimensional state spaces and real-time interaction requirements. Conventional electronic computing platforms face computational bottlenecks, whereas the fusion of photonic computing and spiking reinforcement learning (RL) offers a promising alternative. Here, we propose a novel computing architecture based on photonic spiking RL, which integrates the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm with spiking neural network (SNN). The proposed architecture employs an optical-electronic hybrid computing paradigm wherein a silicon photonic Mach-Zehnder interferometer (MZI) chip executes linear matrix computations, while nonlinear spiking activations are performed in the electronic domain. Experimental validation on the Pendulum-v1 and HalfCheetah-v2 benchmarks demonstrates the system capability for software-hardware co-inference, achieving a control policy reward of 5831 on HalfCheetah-v2, a 23.33% reduction in convergence steps, and an action deviation below 2.2%. Notably, this work represents the first application of a programmable MZI photonic computing chip to robotic continuous control tasks, attaining an energy efficiency of 1.39 TOPS/W and an ultralow computational latency of 120 ps. Such performance underscores the promise of photonic spiking RL for real-time decision-making in autonomous and industrial robotic systems.

</details>


### [343] [Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction](https://arxiv.org/abs/2512.00453)
*Arad Firouzkouhi,Omid Mirzaeedodangeh,Lars Lindemann*

Main category: cs.RO

TL;DR: 本文提出了一种主动模仿学习（AIL）中的新采样方法CRSAIL，在减少大量专家标注成本的同时，保持甚至提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统的AIL方法需要在训练过程中频繁地请求专家标注动作，以应对协变量转移问题。但专家的标注成本极高，尤其是在需要GPU的大型仿真环境、人类参与或机器人等实际应用中，重复状态的标注浪费尤为突出。因此，如何在保证训练效果的前提下，显著减少专家标注次数，是该工作的核心动机。

Method: 作者提出CRSAIL方法，在模型仅在遇到未被专家数据充分覆盖的新状态时才请求专家标注。具体方法为：以每个状态到专家集的第K近邻距离衡量新颖性，并通过符合预测理论设定一个全局阈值（α分位数），使之可控地平衡标注率与训练效果。不同于基于安全门控的AIL，CRSAIL不需专家实时介入，而是先批量采集数据再选择性请求专家。

Result: 实验证明，CRSAIL在MuJoCo机器人任务中，大幅减少了专家查询次数，较DAgger减少高达96%，较已有AIL方法减少65%，同时能达到甚至超过专家级回报。此外，方法对α、K等参数具有鲁棒性，便于在未知新系统上部署。

Conclusion: CRSAIL有效降低了AIL场景下的专家标注需求，维持甚至提升了任务表现，参数调节简单，具有很好的实际落地潜力。

Abstract: Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-α)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $α$ to the expected query rate and makes $α$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $α$ and $K$, easing deployment on novel systems with unknown dynamics.

</details>


### [344] [LAP: Fast LAtent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving](https://arxiv.org/abs/2512.00470)
*Jinhao Zhang,Wenlong Xia,Zhexuan Zhou,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 提出了LAtent Planner (LAP)，通过在VAE学习的潜在空间中进行规划，实现更高效且多模态的自动驾驶行为建模，同时大幅减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在建模人类驾驶行为上表现出色，但存在采样延迟高和受限于低层级动力学特征的问题，亟需一种能高效表达高层语义意图并提升推理速度的方法。

Method: 1. 利用变分自编码器(VAE)学习区分高层意图和低层动力学的潜在空间。
2. 在该潜在空间中使用扩散模型规划，提升多模态策略表达能力。
3. 引入细粒度特征蒸馏机制，加强高层规划语义和场景上下文向量信息的交互与融合。
4. 创新性地实现单步去噪的推理流程，大幅减少计算资源消耗。

Result: 在大规模nuPlan基准上测试，LAP在学习型规划方法中闭环性能达SOTA，同时推理速度提升高达10倍。

Conclusion: LAP展示了扩散模型和潜在空间结合用于自动驾驶行为规划的巨大潜力和实用价值，显著提升性能和推理效率，对未来高效自动驾驶方案具重要参考意义。

Abstract: Diffusion models have demonstrated strong capabilities for modeling human-like driving behaviors in autonomous driving, but their iterative sampling process induces substantial latency, and operating directly on raw trajectory points forces the model to spend capacity on low-level kinematics, rather than high-level multi-modal semantics. To address these limitations, we propose LAtent Planner (LAP), a framework that plans in a VAE-learned latent space that disentangles high-level intents from low-level kinematics, enabling our planner to capture rich, multi-modal driving strategies. We further introduce a fine-grained feature distillation mechanism to guide a better interaction and fusion between the high-level semantic planning space and the vectorized scene context. Notably, LAP can produce high-quality plans in one single denoising step, substantially reducing computational overhead. Through extensive evaluations on the large-scale nuPlan benchmark, LAP achieves state-of-the-art closed-loop performance among learning-based planning methods, while demonstrating an inference speed-up of at most 10 times over previous SOTA approaches.

</details>


### [345] [HAVEN: Hierarchical Adversary-aware Visibility-Enabled Navigation with Cover Utilization using Deep Transformer Q-Networks](https://arxiv.org/abs/2512.00592)
*Mihir Chauhan,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: 提出了一种结合深度Transformer Q网络与低层模块控制的新型分层自主导航方法，能够在部分可观测环境下安全高效导航，优于传统方法和记忆缺失强化学习。


<details>
  <summary>Details</summary>
Motivation: 在复杂且部分可观测的环境中，机器人的导航需要考虑视野受限、遮挡和安全性等因素，传统路径规划和无记忆的强化学习方法难以同时兼顾效率和安全，因此亟需一种新方法来解决此类实际任务中的挑战。

Method: 采用分层结构：高层使用Deep Transformer Q-Network（DTQN）处理包含位姿、目标方向、障碍物距离与可见性等信息的短时历史特征，产生候选子目标的Q值并进行排序；同时引入基于可见性奖励的候选生成机制鼓励合理利用障碍物掩护与安全预判；低层采用势场法模块化控制器实现短程障碍物避让和子目标跟踪。方法在2D仿真和3D Unity-ROS环境中通过特征映射自然迁移，无需架构更改。

Result: 相比传统路径规划和强化学习基线方法，新方案在到达率、安全裕度、耗时等多项指标上均取得持续优异表现。消融实验验证了临时记忆和可见性机制的有效性。

Conclusion: 提出的分层导航框架在不确定环境下具备安全、通用、可迁移等优点，适用于多类机器人平台，推动了机器人自主导航在实际中的应用。

Abstract: Autonomous navigation in partially observable environments requires agents to reason beyond immediate sensor input, exploit occlusion, and ensure safety while progressing toward a goal. These challenges arise in many robotics domains, from urban driving and warehouse automation to defense and surveillance. Classical path planning approaches and memoryless reinforcement learning often fail under limited fields of view (FoVs) and occlusions, committing to unsafe or inefficient maneuvers. We propose a hierarchical navigation framework that integrates a Deep Transformer Q-Network (DTQN) as a high-level subgoal selector with a modular low-level controller for waypoint execution. The DTQN consumes short histories of task-aware features, encoding odometry, goal direction, obstacle proximity, and visibility cues, and outputs Q-values to rank candidate subgoals. Visibility-aware candidate generation introduces masking and exposure penalties, rewarding the use of cover and anticipatory safety. A low-level potential field controller then tracks the selected subgoal, ensuring smooth short-horizon obstacle avoidance. We validate our approach in 2D simulation and extend it directly to a 3D Unity-ROS environment by projecting point-cloud perception into the same feature schema, enabling transfer without architectural changes. Results show consistent improvements over classical planners and RL baselines in success rate, safety margins, and time to goal, with ablations confirming the value of temporal memory and visibility-aware candidate design. These findings highlight a generalizable framework for safe navigation under uncertainty, with broad relevance across robotic platforms.

</details>


### [346] [Fast, Robust, Permutation-and-Sign Invariant SO(3) Pattern Alignment](https://arxiv.org/abs/2512.00659)
*Anik Sarker,Alan T. Asbeck*

Main category: cs.RO

TL;DR: 本文提出了一种无需点对点对应的新方法，实现了SO(3)旋转集合的快速、鲁棒配准，对传统方法进行了显著提速（6-60倍），能处理高达90%的离群点。


<details>
  <summary>Details</summary>
Motivation: 在机器人校准及姿态配准中，常因时间未对齐、离群点、多种旋转轴标号导致旋转集合难以有效配准，且多需对应点、运算量大。

Method: 每个旋转用三个变换后基向量（TBV）反映为S^2球面点集，对每轴独立采用高效匹配算法（SPMC、FRS及混合方法）配准；为解决轴交换与正负翻转，引入PASI封装，对24种有符号排列遍历并相关评分，再通过Karcher均值求唯一旋转。整体复杂度O(n)，优于传统O(N^3 log N)方法。

Result: 在EuRoC仿真与ETH Hand-Eye真实数据集上，提出方法在保有高准确度时，较传统法提速6-60倍，能在高达90%离群点环境下稳定工作，无需事先建立点对应。

Conclusion: 该方法大幅提高了SO(3)配准的速度和鲁棒性，突破了传统方法对对应点的依赖，适用于高离群点和轴标号不确定等实际场景。

Abstract: We address the correspondence-free alignment of two rotation sets on \(SO(3)\), a core task in calibration and registration that is often impeded by missing time alignment, outliers, and unknown axis conventions. Our key idea is to decompose each rotation into its \emph{Transformed Basis Vectors} (TBVs)-three unit vectors on \(S^2\)-and align the resulting spherical point sets per axis using fast, robust matchers (SPMC, FRS, and a hybrid). To handle axis relabels and sign flips, we introduce a \emph{Permutation-and-Sign Invariant} (PASI) wrapper that enumerates the 24 proper signed permutations, scores them via summed correlations, and fuses the per-axis estimates into a single rotation by projection/Karcher mean. The overall complexity remains linear in the number of rotations (\(\mathcal{O}(n)\)), contrasting with \(\mathcal{O}(N_r^3\log N_r)\) for spherical/\(SO(3)\) correlation. Experiments on EuRoC Machine Hall simulations
  (axis-consistent) and the ETH Hand-Eye benchmark (\texttt{robot\_arm\_real})
  (axis-ambiguous) show that our methods are accurate, 6-60x faster than traditional methods, and robust under extreme outlier ratios (up to 90\%), all without correspondence search.

</details>


### [347] [MS-PPO: Morphological-Symmetry-Equivariant Policy for Legged Robot Locomotion](https://arxiv.org/abs/2512.00727)
*Sizhe Wei,Xulin Chen,Fengze Xie,Garrett Ethan Katz,Zhenyu Gan,Lu Gan*

Main category: cs.RO

TL;DR: 该论文提出了MS-PPO框架，通过将机器人运动学结构和形态对称性直接嵌入策略网络，提高了四足机器人运动任务中的训练效率、泛化能力和对称性表现，并在多个模拟和真实平台上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在四足机器人运动中表现优异，但主流策略网络通常忽略了机器人自身的结构信息和形态对称性，导致泛化能力弱、训练效率低和对称性差，严重制约了其实际应用。

Method: 作者提出了一种形态-对称等变（morphological-symmetry-equivariant）策略学习框架MS-PPO。具体做法是将机器人的运动学结构与对称性信息建模为图神经网络，并将此等变性嵌入到策略网络结构中，实现状态对称时策略输出也对称，并保持价值估计的不变性，无需繁琐奖励塑形或数据增强。

Result: MS-PPO在模拟环境下对Unitree Go2和Xiaomi CyberDog2机器人进行了多种复杂运动任务测试，包括小跑、跳跃、斜坡行走和双足转向，并在真实硬件部署。实验显示，MS-PPO在训练稳定性、对称泛化和采样效率上均优于现有最优基线方法。

Conclusion: 将运动学结构和形态对称性显式嵌入强化学习策略网络，可以显著提升四足机器人运动控制中的泛化能力和效率，为该领域带来强有力的先验引导。

Abstract: Reinforcement learning has recently enabled impressive locomotion capabilities on legged robots; however, most policy architectures remain morphology- and symmetry-agnostic, leading to inefficient training and limited generalization. This work introduces MS-PPO, a morphological-symmetry-equivariant policy learning framework that encodes robot kinematic structure and morphological symmetries directly into the policy network. We construct a morphology-informed graph neural architecture that is provably equivariant with respect to the robot's morphological symmetry group actions, ensuring consistent policy responses under symmetric states while maintaining invariance in value estimation. This design eliminates the need for tedious reward shaping or costly data augmentation, which are typically required to enforce symmetry. We evaluate MS-PPO in simulation on Unitree Go2 and Xiaomi CyberDog2 robots across diverse locomotion tasks, including trotting, pronking, slope walking, and bipedal turning, and further deploy the learned policies on hardware. Extensive experiments show that MS-PPO achieves superior training stability, symmetry generalization ability, and sample efficiency in challenging locomotion tasks, compared to state-of-the-art baselines. These findings demonstrate that embedding both kinematic structure and morphological symmetry into policy learning provides a powerful inductive bias for legged robot locomotion control. Our code will be made publicly available at https://lunarlab-gatech.github.io/MS-PPO/.

</details>


### [348] [SAGAS: Semantic-Aware Graph-Assisted Stitching for Offline Temporal Logic Planning](https://arxiv.org/abs/2512.00775)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 本文提出了一种在无模型、离线环境下满足线性时序逻辑（LTL）约束的机器人控制方法，能够仅依赖无关任务的碎片化轨迹数据高效实现复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有LTL机器人控制方法通常依赖准确动力学模型或代价高昂的实时交互，而许多实际场景只提供碎片化的离线数据，难以满足复杂逻辑约束，亟需解决如何在缺乏模型和在线环境下完成复杂任务的问题。

Method: 提出SAGAS框架：1）构建基于学习的时序距离潜在可达性图，通过锚点和软标签弥补语义鸿沟；2）将LTL规范转化为Büchi自动机，在隐式积空间中搜索最优前缀-后缀轨迹；3）利用子目标条件化低级策略，在潜在空间下执行路径。

Result: 在OGBench运动任务实验中，SAGAS能够针对多样的LTL任务综合出高效的轨迹，大幅提升离线数据下、碎片化轨迹到复杂逻辑任务要求间的转化效率和可靠性。

Conclusion: SAGAS在碎片化离线数据环境下实现了复杂LTL任务的高效规划和控制，显著缩小了碎片数据与复杂逻辑约束之间的鸿沟。

Abstract: Linear Temporal Logic (LTL) provides a rigorous framework for complex robotic tasks, yet existing methods often rely on accurate dynamics models or expensive online interactions. In this work, we address LTL-constrained control in a challenging offline, model-free setting, utilizing only fixed, task-agnostic datasets of fragmented trajectories. We propose SAGAS, a novel framework combining graph-assisted trajectory stitching with automata-guided planning. First, we construct a latent reachability graph from a learned temporal-distance representation. To bridge the semantic gap, we augment this graph with certified anchor nodes and probabilistic soft labels. We then translate the specification into a Büchi automaton and search the implicit product space to derive a cost-minimal prefix-suffix plan. Finally, a subgoal-conditioned low-level policy is deployed to execute these latent waypoints. Experiments on OGBench locomotion domains demonstrate that SAGAS successfully synthesizes efficient trajectories for diverse LTL tasks, effectively bridging the gap between fragmented offline data and complex logical constraints.

</details>


### [349] [Sign Language Recognition using Bidirectional Reservoir Computing](https://arxiv.org/abs/2512.00777)
*Nitin Kumar Singh,Arie Rachmad Syulistyo,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.RO

TL;DR: 本文提出了一种高效的手语识别系统，结合MediaPipe与基于回声状态网络（ESN）的双向水库计算（BRC）架构，能在资源受限设备上快速、准确地进行手语识别。


<details>
  <summary>Details</summary>
Motivation: 现有手语识别系统多依赖深度学习模型，计算资源消耗大，不适合运行在移动或边缘设备上。作者希望提出一种更轻量且高效的解决方案。

Method: 系统首先利用MediaPipe提取手部关节点坐标，将其作为输入送入基于ESN的双向水库计算架构，分别正向和反向处理，充分捕捉时序特征。最后，合并得到的状态进行分类识别。

Result: 在Word-Level American Sign Language (WLASL)数据集上，提出方法取得57.71%的准确率，训练时间仅需9秒，远低于Bi-GRU深度学习方法的55分38秒。

Conclusion: 该基于BRC的手语识别系统在保证识别准确率的同时大幅降低了训练时间和计算需求，非常适合边缘设备等资源受限场景。

Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing individuals. Deep learning is widely used to develop SLR-based systems; however, it is computationally intensive and requires substantial computational resources, making it unsuitable for resource-constrained devices. To address this, we propose an efficient sign language recognition system using MediaPipe and an echo state network (ESN)-based bidirectional reservoir computing (BRC) architecture. MediaPipe extracts hand joint coordinates, which serve as inputs to the ESN-based BRC architecture. The BRC processes these features in both forward and backward directions, efficiently capturing temporal dependencies. The resulting states of BRC are concatenated to form a robust representation for classification. We evaluated our method on the Word-Level American Sign Language (WLASL) video dataset, achieving a competitive accuracy of 57.71% and a significantly lower training time of only 9 seconds, in contrast to the 55 minutes and $38$ seconds required by the deep learning-based Bi-GRU approach. Consequently, the BRC-based SLR system is well-suited for edge devices.

</details>


### [350] [Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration](https://arxiv.org/abs/2512.00797)
*Nan Sun,Bo Mao,Yongchang Li,Chenxu Wang,Di Guo,Huaping Liu*

Main category: cs.RO

TL;DR: 本文提出了一种多智能体框架InteractGen，利用大模型分工协作，提升服务机器人在现实环境中的自主性、适应性和与人的协作能力。实验显示该方法优于单一大模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大模型尝试用单一模型统一机器人感知与规划，但在实际部署时表现出无法适应服务场景多样化和动态性的局限。单一模型难以实现可靠的人群环境自适应自治。

Method: 作者提出InteractGen，将机器人智能解构为多个专职Agent，分别负责持续感知、依赖感知规划、决策与验证、失效反思与动态人类协作，把LLM及基础模型作为受控组件，构成闭环式集体系统。

Result: InteractGen在异构机器人团队与为期三个月的开放使用实验中提升了任务成功率、适应能力和人机协作水平。

Conclusion: 与继续单一大模型扩展相比，多智能体联合编排为机器人服务自治提供了更可行的路径。

Abstract: Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.

</details>


### [351] [A Novel MDP Decomposition Framework for Scalable UAV Mission Planning in Complex and Uncertain Environments](https://arxiv.org/abs/2512.00838)
*Md Muzakkir Quamar,Ali Nasir,Sami ELFerik*

Main category: cs.RO

TL;DR: 提出了一种可扩展且容错的无人机任务管理框架，通过两阶段MDP分解方法，大幅提升决策求解效率，并保持全局最优。


<details>
  <summary>Details</summary>
Motivation: 在复杂和不确定环境下，大规模MDP求解存在计算瓶颈，限制了UAV在实际任务中的实时智能决策能力。

Method: 提出两阶段分解策略：第一阶段使用基于目标优先级、故障状态、空间布局和能量约束的因子算法将全局MDP划分为小的子MDP；第二阶段利用优先级重组算法独立求解各子MDP，并通过元策略合并解决冲突，形成统一全局策略。理论分析证明该方法在温和独立假设下能等价于最优全局MDP策略。

Result: 大量仿真结果显示，该方法在不损失任务可靠性和策略最优性的前提下，将计算时间缩短了几个数量级。

Conclusion: 本框架为实时无人机任务决策提供了可扩展、可靠的基础，显著提升了大规模AI序贯决策问题的求解能力。

Abstract: This paper presents a scalable and fault-tolerant framework for unmanned aerial vehicle (UAV) mission management in complex and uncertain environments. The proposed approach addresses the computational bottleneck inherent in solving large-scale Markov Decision Processes (MDPs) by introducing a two-stage decomposition strategy. In the first stage, a factor-based algorithm partitions the global MDP into smaller, goal-specific sub-MDPs by leveraging domain-specific features such as goal priority, fault states, spatial layout, and energy constraints. In the second stage, a priority-based recombination algorithm solves each sub-MDP independently and integrates the results into a unified global policy using a meta-policy for conflict resolution. Importantly, we present a theoretical analysis showing that, under mild probabilistic independence assumptions, the combined policy is provably equivalent to the optimal global MDP policy. Our work advances artificial intelligence (AI) decision scalability by decomposing large MDPs into tractable subproblems with provable global equivalence. The proposed decomposition framework enhances the scalability of Markov Decision Processes, a cornerstone of sequential decision-making in artificial intelligence, enabling real-time policy updates for complex mission environments. Extensive simulations validate the effectiveness of our method, demonstrating orders-of-magnitude reduction in computation time without sacrificing mission reliability or policy optimality. The proposed framework establishes a practical and robust foundation for scalable decision-making in real-time UAV mission execution.

</details>


### [352] [Magnetic Tactile-Driven Soft Actuator for Intelligent Grasping and Firmness Evaluation](https://arxiv.org/abs/2512.00907)
*Chengjin Du,Federico Bernabei,Zhengyin Du,Sergio Decherchi,Matteo Lo Preti,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种集成磁性感测的软体机器人驱动器（SoftMag），通过多物理场仿真建模和神经网络去耦合方法，实现了柔性机器人同时精准感知和灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 传统软体机器人在操作易损物体时缺乏集成触觉感应，且驱动时的形变会干扰感测信号，限制了其实用性。解决这两大难题，有助于推动软体机器人在实际应用中的发展。

Method: 本文设计了软体磁性感测驱动器（SoftMag），将感知与驱动统一架构，并针对机械寄生效应，用多物理仿真模拟形变与信号的耦合。再通过神经网络对感测信号进行去耦合校正，以消除形变干扰。通过压痕、静态和阶跃驱动及疲劳实验验证方法有效性。

Result: 实验表明，SoftMag能准确恢复触觉信息，实现对接触力和位置的实时预测。基于双指夹爪和多任务神经网络，系统还能在抓取时推断物体坚固度。测试结果显示估算坚固度与真实测量高度相关（Pearson r>0.8）。

Conclusion: 将集成磁性感测、学习校正和实时推理结合，SoftMag实现了可自适应操作和材料属性识别的软体机器人平台，推动了智能化、材料感知型软体机器人技术的发展。

Abstract: Soft robots are powerful tools for manipulating delicate objects, yet their adoption is hindered by two gaps: the lack of integrated tactile sensing and sensor signal distortion caused by actuator deformations. This paper addresses these challenges by introducing the SoftMag actuator: a magnetic tactile-sensorized soft actuator. Unlike systems relying on attached sensors or treating sensing and actuation separately, SoftMag unifies them through a shared architecture while confronting the mechanical parasitic effect, where deformations corrupt tactile signals. A multiphysics simulation framework models this coupling, and a neural-network-based decoupling strategy removes the parasitic component, restoring sensing fidelity. Experiments including indentation, quasi-static and step actuation, and fatigue tests validate the actuator's performance and decoupling effectiveness. Building upon this foundation, the system is extended into a two-finger SoftMag gripper, where a multi-task neural network enables real-time prediction of tri-axial contact forces and position. Furthermore, a probing-based strategy estimates object firmness during grasping. Validation on apricots shows a strong correlation (Pearson r over 0.8) between gripper-estimated firmness and reference measurements, confirming the system's capability for non-destructive quality assessment. Results demonstrate that combining integrated magnetic sensing, learning-based correction, and real-time inference enables a soft robotic platform that adapts its grasp and quantifies material properties. The framework offers an approach for advancing sensorized soft actuators toward intelligent, material-aware robotics.

</details>


### [353] [Constant-Time Motion Planning with Manipulation Behaviors](https://arxiv.org/abs/2512.00939)
*Nayesha Gandotra,Itamar Mishani,Maxim Likhachev*

Main category: cs.RO

TL;DR: 本文提出了一种面向操纵任务的常数时间运动规划器（B-CTMP），能在极短时间内（毫秒级）为机器人操纵任务提供安全高效的运动规划，并在模拟和实际机器人实验中有效验证。


<details>
  <summary>Details</summary>
Motivation: 现有接触丰富的机器人操纵取得了进展，但落地系统仍受限于简单、脚本化的流程。缺乏能为运动规划提供安全、高效、可靠性可验证保证的算法是主要障碍，尤其是在需要复杂物体操纵的场景。

Method: 该文扩展了常数时间运动规划（CTMP），提出B-CTMP，将两步操纵任务（先到达触发状态、后执行操纵行为如抓取或插入）融入到常数时间查询机制中。通过预先计算紧凑的数据结构，实现毫秒级查询与任务处理的一致性和有效性保证。

Result: 在模拟的货架分拣、插头插入任务以及真实机器人上评估B-CTMP，验证其能在半结构化环境中高效完成操纵任务，实现碰撞自由运动与操作行为的统一规划，且具有速度和成功率的理论保障。

Conclusion: B-CTMP在常数时间下将碰撞避免与物体操纵行为整合，针对复杂操纵任务，既保证了安全高速，也提供了一致的成功率，是半结构化环境下机器人操纵任务的重要进展。

Abstract: Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.

</details>


### [354] [H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer](https://arxiv.org/abs/2512.00971)
*Yunfeng Lin,Minghuan Liu,Yufei Xue,Ming Zhou,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为H-Zero的跨人形机器人步态预训练方法，可实现人形机器人控制策略的零样本和小样本迁移。该策略在未见过的新机器人上表现良好，并能在短时间内通过微调适应新的机器人。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人控制器通常针对特定机器人进行设计，需大量参数调优，导致迁移性差，开发效率低。作者希望解决如何快速让控制策略泛化到不同构型的人形机器人上的难题。

Method: 提出H-Zero预训练流程，在有限种类的人形机器人上进行步态策略预训练，获得可泛化的基础策略。然后将该策略迁移到新的机器人，仅需极少微调甚至无需微调（零样本/小样本迁移），即可达到较好效果。

Result: 在仿真实验中，H-Zero预训练策略在未见过的新型机器人上能维持最高81%的完整步态时长；同时在对新的人形机器人或直立四足机器人进行迁移时，仅需30分钟微调便能适应。

Conclusion: H-Zero显著提升了人形机器人控制策略的泛化能力和迁移效率，为通用型机器人控制器的设计提供了新思路。

Abstract: The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining on a limited set of embodiments enables zero-shot and few-shot transfer to novel humanoid robots with minimal fine-tuning. Evaluations show that the pretrained policy maintains up to 81% of the full episode duration on unseen robots in simulation while enabling few-shot transfer to unseen humanoids and upright quadrupeds within 30 minutes of fine-tuning.

</details>


### [355] [FOM-Nav: Frontier-Object Maps for Object Goal Navigation](https://arxiv.org/abs/2512.01009)
*Thomas Chabal,Shizhe Chen,Jean Ponce,Cordelia Schmid*

Main category: cs.RO

TL;DR: 本文提出FOM-Nav框架，通过联合空间前沿与细粒度目标信息的地图，并结合视觉-语言模型，实现了在未知环境中更高效的目标导航，在多个基准任务上刷新了性能。


<details>
  <summary>Details</summary>
Motivation: 现有隐式记忆方法在长时记忆和规划上存在短板，显式地图方法则缺乏丰富语义，两者难以兼顾。在复杂未知环境中，如何让机器人高效发现目标物体成为核心挑战。

Method: 提出模块化FOM-Nav框架，在线构建Frontier-Object Map，将空间可探索边界(前沿)和物体语义细节联合编码，配合视觉-语言模型实现多模态场景理解和目标预测，并由低阶规划器生成高效轨迹，同时自动构建大规模真实导航数据集进行训练。

Result: FOM-Nav框架在MP3D和HM3D两个权威基准数据集上性能领先，尤其在导航效率指标SPL上表现突出，实机测试同样取得积极效果。

Conclusion: 利用Frontier-Object Maps提升了导航场景建模的表达力，结合视觉-语言模型显著优化了未知环境下的目标导航效率，方法在仿真与实机均表现优异，具备应用潜力。

Abstract: This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.

</details>


### [356] [Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping](https://arxiv.org/abs/2512.01018)
*Adelina Giurea,Stijn Luchie,Dieter Coppens,Jeroen Hoebeke,Eli De Poorter*

Main category: cs.RO

TL;DR: 本论文提出了一种不依赖外部基础设施、基于超宽带（UWB）雷达的移动机器人障碍物检测与环境建图方法，适用于视觉受限环境，如黑暗、烟雾中等。实验评估了UWB雷达在不同材料和无线电通道下的表现，并设计了数据处理流程，实现了高精度的障碍物检测。


<details>
  <summary>Details</summary>
Motivation: 在黑暗、烟雾或反光环境等视觉受限情况下，传统相机或激光雷达等传感器容易失效。因此，亟需发展无需依赖视觉和固定基础设施的障碍物检测与环境建图方法。

Method: 作者将UWB雷达安装在移动机器人上，通过研究不同障碍物材料（如金属、混凝土、胶合板）和不同UWB通道（5和9）对信道脉冲响应（CIR）的影响。提出了三步数据处理流程：目标识别、信号过滤和聚类，分别基于CIR峰值检测、信噪比和相位差等特征，实现障碍物定位。

Result: 该系统在UWB通道9检测低反射材料（如胶合板）时，障碍物检测精度达82.36%，召回率达89.46%；还有效减少了噪声和多路径效应影响。

Conclusion: 结果表明，所提UWB雷达方法能够在无锚点、无视觉特征的动态环境中，实现可靠障碍物检测及环境建图，具备发展UWB SLAM系统的潜力，且无需依赖固定锚点，比传统UWB定位方案更具灵活性。

Abstract: This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform. Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces. In these visioned-impaired conditions, UWB radar offers a promising alternative. To this end, this work explores the suitability of robot-mounted UWB radar for environmental mapping in dynamic, anchor-free scenarios. The study investigates how different materials (metal, concrete and plywood) and UWB radio channels (5 and 9) influence the Channel Impulse Response (CIR). Furthermore, a processing pipeline is proposed to achieve reliable mapping of detected obstacles, consisting of 3 steps: (i) target identification (based on CIR peak detection), (ii) filtering (based on peak properties, signal-to-noise score, and phase-difference of arrival), and (iii) clustering (based on distance estimation and angle-of-arrival estimation). The proposed approach successfully reduces noise and multipath effects, resulting in an obstacle detection precision of at least 82.36% and a recall of 89.46% on channel 9 even when detecting low-reflective materials such as plywood. This work offers a foundation for further development of UWB-based localisation and mapping (SLAM) systems that do not rely on visual features and, unlike conventional UWB localisation systems, do not require on fixed anchor nodes for triangulation.

</details>


### [357] [CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding](https://arxiv.org/abs/2512.01022)
*Yi-Lin Wei,Haoran Liao,Yuhao Lin,Pengyue Wang,Zhizhao Liang,Guiliang Liu,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: 本文针对机器人周期性操作任务提出了新的方法和基准，显著提升了机器人按预定时间完成循环任务的能力。


<details>
  <summary>Details</summary>
Motivation: 周期性操作（如摇瓶、钉钉子）在实际生活中十分常见，但现有模仿方法利用历史信息效果不佳，难以保障按时完成。这一领域也缺乏公开基准和自动评测工具，制约了相关研究发展。

Method: 作者提出了CycleManip 框架，以端到端模仿学习实现周期操作，无需额外模型、分层结构或高算力。核心创新包括‘代价感知采样’提升历史感知，以及多任务学习加强历史理解。同时，构建了包含多样周期任务和自动评估工具的benchmark。

Result: 在模拟和现实平台上的实验表明该方法在周期任务中成功率高，并在通用操作任务（如VLA政策）、不同机器人（如双臂、灵巧手、人形机器人）上均表现出强适应性和可迁移性。

Conclusion: CycleManip 及其基准有效促进了机器人周期性操作领域的发展，方法高效、通用、可用性强，有望推动更多相关应用。

Abstract: In this paper, we explore an important yet underexplored task in robot manipulation: cycle-based manipulation, where robots need to perform cyclic or repetitive actions with an expected terminal time. These tasks are crucial in daily life, such as shaking a bottle or knocking a nail. However, few prior works have explored this task, leading to two main challenges: 1) the imitation methods often fail to complete these tasks within the expected terminal time due to the ineffective utilization of history; 2) the absence of a benchmark with sufficient data and automatic evaluation tools hinders development of effective solutions in this area. To address these challenges, we first propose the CycleManip framework to achieve cycle-based task manipulation in an end-to-end imitation manner without requiring any extra models, hierarchical structure or significant computational overhead. The core insight is to enhance effective history perception by a cost-aware sampling strategy and to improve historical understanding by multi-task learning. Second, we introduce a cycle-based task manipulation benchmark, which provides diverse cycle-based tasks, and an automatic evaluation method. Extensive experiments conducted in both simulation and real-world settings demonstrate that our method achieves high success rates in cycle-based task manipulation. The results further show strong adaptability performance in general manipulation, and the plug-and-play ability on imitation policies such as Vision-Language-Action (VLA) models. Moreover, the results show that our approach can be applied across diverse robotic platforms, including bi-arm grippers, dexterous hands, and humanoid robots.

</details>


### [358] [VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference](https://arxiv.org/abs/2512.01031)
*Jiaming Tang,Yufei Sun,Yilong Zhao,Shang Yang,Yujun Lin,Zhuoyang Zhang,James Hou,Yao Lu,Zhijian Liu,Song Han*

Main category: cs.RO

TL;DR: 本文提出了一种新的异步推理框架VLASH，用于提升视觉-语言-动作模型（VLA）在机器人任务中的反应速度和控制连续性，有效解决了推理与动作执行的时序错位问题。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在多样化机器人任务上的表现很强，但实际部署时往往反应迟钝，存在动作延迟和卡顿现象，亟需一种能同时兼顾平滑性、准确性和实时性的解决方案。

Method: VLASH框架通过利用已生成的动作序列向前预测机器人的执行状态，从而估计未来的动作时机，实现推理与动作的并行异步进行，克服时序对齐难题，无需对原模型结构做额外改动。

Result: 实验显示，VLASH在无需牺牲原有精度的前提下，能使机器人推理速度提升最高达2.03倍，反应延迟降低达17.4倍，并能胜任需高速反应和高精度的复杂任务，如乒乓球或打地鼠游戏，传统方法难以胜任。

Conclusion: VLASH有效提升了VLA模型在机器人领域的实时性与稳定性，实现了平滑、快速且准确的控制，对机器人在真实场景的部署具有显著促进作用。

Abstract: Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash

</details>


### [359] [Autonomous Grasping On Quadruped Robot With Task Level Interaction](https://arxiv.org/abs/2512.01052)
*Muhtadin,Mochammad Hilmi Rusydiansyah,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 本文提出了一种为四足机器人赋予自主抓取能力的系统，并验证其在现实环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前四足机器人多以移动为主，缺乏物体操作能力。手动远程控制机器人手臂和抓手在复杂场景下难度大，限制了其应用场景。

Method: 本研究将机械臂和抓手集成于四足机器人，基于ROS设计了分层控制系统，并开发了基于Web的人机交互界面。采用GraspNet实现自主导航、目标检测和抓取任务。

Result: 通过真实场景测试，机器人实现了自动导航、目标识别及抓取操作。在12次实验中，抓取成功率达75%。

Conclusion: 该系统有效提升了四足机器人在现实环境中的服务能力，展现出较大的应用潜力。

Abstract: Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.

</details>


### [360] [Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer](https://arxiv.org/abs/2512.01061)
*Haoru Xue,Tairan He,Zi Wang,Qingwei Ben,Wenli Xiao,Zhengyi Luo,Xingye Da,Fernando Castañeda,Guanya Shi,Shankar Sastry,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种新的教师-学生自举学习框架，用于基于视觉的人形机器人运动与操作任务，能够有效泛化并实现现实世界转移。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习方法在通用性和现实世界适应性上受限，尤其是基于视觉的人形机器人在与复杂关节物体交互时更具挑战，因此需要寻找高效且可扩展的训练方法。

Method: 采用GPU加速、基于仿真的光线追踪数据生成，通过大规模物理与视觉扰动提升策略泛化能力。提出了分阶段重置的探索策略稳定长期训练，引入GRPO微调程序提升局部观测及闭环一致性，实现从视觉输入到动作输出的端到端策略。整个训练流程在仿真环境中完成。

Result: 训练得到的策略能够在多种门类任务中实现零样本迁移，任务完成时间最多比人类遥操作快31.7%，展现出优异的鲁棒性和泛化能力，是首个仅用RGB视觉实现多样关节操控的人形机器人方案。

Conclusion: 该方法显著推进了基于视觉的人形机器人现实世界操作能力，为通用的机器人学习和本体能力迁移提供了新方向。

Abstract: Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.

</details>


### [361] [Reinforcement Learning for Gliding Projectile Guidance and Control](https://arxiv.org/abs/2512.01066)
*Joel Cahn,Antonin Thomas,Philippe Pastor*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的制导控制律，旨在提升光学引导滑翔机在动态环境中的自主导航与目标跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统的制导方法在动态环境中存在灵活性和自主性不足，难以实现对移动目标的高精度跟踪，因此本研究希望引入强化学习提升固定翼无人机的导航与控制能力。

Method: 设计并开发了一套应用于光学引导滑翔机的强化学习控制律，并探索其在所有轴向的适用性，通过摄像头检测目标后，引导滑翔机实现高精度跟踪。

Result: 通过该控制律，滑翔机能在复杂环境下自主且高精度地跟踪摄像头检测到的目标点，展示了强化学习方法对固定翼无人机的有效性。

Conclusion: 强化学习能够显著增强固定翼无人机在复杂环境中的导航灵活性和自主性，为未来无人机的自主制导控制提供了新思路。

Abstract: This paper presents the development of a control law, which is intended to be implemented on an optical guided glider. This guiding law follows an innovative approach, the reinforcement learning. This control law is used to make navigation more flexible and autonomous in a dynamic environment. The final objective is to track a target detected with the camera and then guide the glider to this point with high precision. Already applied on quad-copter drones, we wish by this study to demonstrate the applicability of reinforcement learning for fixed-wing aircraft on all of its axis.

</details>


### [362] [Estimation of Kinematic Motion from Dashcam Footage](https://arxiv.org/abs/2512.01104)
*Evelyn Zhang,Alex Richardson,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: 本文探讨使用行车记录仪视频来预测汽车实际运动状态的准确性。作者通过同步采集车载数据和视频，建立神经网络模型来量化预测车辆速度、偏航、前车存在及相对距离等指标的准确性，并介绍如何用开源工具和普通装备复现实验。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵/复杂的传感器，难以大规模部署。本文希望验证消费级行车摄像头和简单传感器能否准确预测车辆运动参数，为低成本智能驾驶提供依据。

Method: 利用18小时同步采集的车载网络（CAN）数据和行车记录仪视频，训练神经网络模型预测车辆速度、偏航、前车存在及距离速度等信息，并用真实数据量化预测准确性。

Result: 神经网络模型能够较为准确预测车辆速度、偏航及环境信息。实验展示了各项指标的预测精度，证明了方法的可行性。

Conclusion: 消费级行车记录仪结合神经网络能有效预测车辆运动状态和前方车辆信息，为低成本数据采集和自动驾驶感知提供实用方案，流程和工具也便于他人复现研究。

Abstract: The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.

</details>


### [363] [Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept](https://arxiv.org/abs/2512.01105)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本文提出并验证了一种将社会辅助机器人（SAR）作为高校生产力教练的新方法，旨在提升大学生的时间管理和任务优先级技能。实验结果显示系统具备优秀的可用性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统自助书籍和生产力应用无法提供个性化帮助，且可能不利于组织能力发展；传统教练虽然有效，但资源消耗大、难以普及。因此，急需一种既可个性化辅导、又易扩展的高校生产力提升解决方案。

Method: 设计并实现了一个SAR原型，作为教育型教练，围绕时间管理和任务优先级提供6节课程，通过对话界面互动，SAR能语音反馈。系统配备仪表盘，实时监控进度、情绪、信心等，提供个性化反馈。招募15名大学生进行实地测试，通过可用性和体验问卷进行评价。

Result: 系统可用性得分为79.2（SUS），用户总体体验和参与度评分均较高，反馈显示大学生认可该SAR系统在生产力提升方面的有效性。

Conclusion: SAR生产力教练具备良好可用性、用户体验和潜在可扩展性，是高校生产力提升的有效方案和传统教练、应用的有力补充。

Abstract: College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the development of key organizational skills. Traditional productivity coaching offers personalized support, but is resource-intensive and difficult to scale. In this study, we present a proof-of-concept for a socially assistive robot (SAR) as an educational coach and a potential solution to the limitations of existing productivity tools and coaching approaches. The SAR delivers six different lessons on time management and task prioritization. Users interact via a chat interface, while the SAR responds through speech (with a toggle option). An integrated dashboard monitors progress, mood, engagement, confidence per lesson, and time spent per lesson. It also offers personalized productivity insights to foster reflection and self-awareness. We evaluated the system with 15 college students, achieving a System Usability Score of 79.2 and high ratings for overall experience and engagement. Our findings suggest that SAR-based productivity coaching can offer an effective and scalable solution to improve productivity among college students.

</details>


### [364] [Tactile Robotics: Past and Future](https://arxiv.org/abs/2512.01106)
*Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文通过回顾近50年来约150篇综述，总结了触觉机器人领域的历史演变、现状及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 当前触觉机器人正处于快速扩展与多样化阶段，但领域发展存在波折与挑战。作者希望通过历史梳理，挖掘行业的演进原因、瓶颈与未来机遇，帮助界定该领域的未来。

Method: 以系统性文献回顾的方式，按年代划分触觉机器人发展的四个阶段，提取每一时期主题和关键技术，并结合专家观点分析领域沿革与挑战。

Result: 触觉机器人经历了起源、基础增长、“寒冬”与扩展多元化四个阶段。近期出现了电子皮肤、触觉手、视觉触觉传感、软体/仿生触觉及触觉互联网等新热潮。

Conclusion: 2025年后，触觉机器人有望实现商业化推广，与人类灵巧、智能理解和远程存在等深度结合。文章强调了领域中的反复难题和发展动力，预测其未来主要机遇和影响。

Abstract: What is the future of tactile robotics? To help define that future, this article provides a historical perspective on tactile sensing in robotics from the wealth of knowledge and expert opinion in nearly 150 reviews over almost half a century. This history is characterized by a succession of generations: 1965-79 (origins), 1980-94 (foundations and growth), 1995-2009 (tactile winter) and 2010-2024 (expansion and diversification). Recent expansion has led to diverse themes emerging of e-skins, tactile robotic hands, vision-based tactile sensing, soft/biomimetic touch, and the tactile Internet. In the next generation from 2025, tactile robotics could mature to widespread commercial use, with applications in human-like dexterity, understanding human intelligence, and telepresence impacting all robotics and AI. By linking past expert insights to present themes, this article highlights recurring challenges in tactile robotics, showing how the field has evolved, why progress has often stalled, and which opportunities are most likely to define its future.

</details>


### [365] [Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception](https://arxiv.org/abs/2512.01108)
*Gabriel Olin,Lu Chen,Nayesha Gandotra,Maxim Likhachev,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种在处理传感器噪声情况下拦截高速目标的新方法，利用运动原语在状态-时间空间中生成树状结构，实现了多目标实时可达性和决策优化。方法在6自由度工业机械臂上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 高速目标拦截需要在极短时间内做出决策，受限于传感器噪声导致的不完全信息，这给获取最佳拦截轨迹带来了极大挑战。作者希望实现机器人在信息仍不断获取时就可决策，解决目标状态不确定、多目标切换和实时响应等难题。

Method: 提出在状态-时间空间利用动力学一致的运动原语生成树状结构，该结构实现了从单一初始状态到多个目标的可达性编码，并支持在目标信念持续更新时及时价值评估和无缝切换目标。目标追踪和信念更新采用鲁棒创新自适应估计算法（RIAE-AKF）。在ABB IRB-1600工业机械臂及ZED 2i双目相机上进行实验测试。

Result: 实验证明，该方法能够在存在传感器噪声和目标状态不确定性的情况下，实现高速有效地拦截目标，并支持多目标无缝切换与实时决策优化。

Conclusion: 利用状态-时间树状结构与自适应滤波相结合，可以大幅提升机器人在传感器噪声环境下的高速拦截与多目标适应能力，具有良好的实际应用前景。

Abstract: Intercepting fast moving objects, by its very nature, is challenging because of its tight time constraints. This problem becomes further complicated in the presence of sensor noise because noisy sensors provide, at best, incomplete information, which results in a distribution over target states to be intercepted. Since time is of the essence, to hit the target, the planner must begin directing the interceptor, in this case a robot arm, while still receiving information. We introduce an tree-like structure, which is grown using kinodynamic motion primitives in state-time space. This tree-like structure encodes reachability to multiple goals from a single origin, while enabling real-time value updates as the target belief evolves and seamless transitions between goals. We evaluate our framework on an interception task on a 6 DOF industrial arm (ABB IRB-1600) with an onboard stereo camera (ZED 2i). A robust Innovation-based Adaptive Estimation Adaptive Kalman Filter (RIAE-AKF) is used to track the target and perform belief updates.

</details>


### [366] [Ethically-Aware Participatory Design of a Productivity Social Robot for College Students](https://arxiv.org/abs/2512.01111)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本文探讨了社交辅助机器人（SARs）如何帮助有注意力障碍（ADHD）等执行功能困难的大学生改善学业生产力。研究通过参与式设计方法，收集有效需求并提出面向高等教育生产力的SAR设计建议。


<details>
  <summary>Details</summary>
Motivation: 许多大学生面临学业和生活压力，生产力受影响，尤其是有ADHD等执行功能困难的学生。传统生产力工具需要高度自律和持续使用，难以满足这部分学生需要。因此，寻找更适合的支持方式成为动机。

Method: 研究采用了参与式设计（Participatory Design, PD）方法，邀请大学生及学生成功与健康教练共同参与设计过程。通过访谈和合作研讨会，系统收集学生实际面临的生产力挑战和对SAR功能的偏好。同时，从研究一开始就考虑了伦理问题。

Result: 研究收集并梳理了大学生关于生产力挑战的详细反馈，识别并总结了他们对SAR的设计需求和偏好，形成了具体的特征建议。此外，提出了利益相关者驱动的伦理守则，以指导后续SAR应用。

Conclusion: SAR具有支持大学生产力的潜力。研究为SAR在高等教育环境下的设计和应用提供了可行性建议和伦理准则，促进未来更负责任、更贴合用户需求的研发和实践。

Abstract: College students often face academic and life stressors affecting productivity, especially students with Attention Deficit Hyperactivity Disorder (ADHD) who experience executive functioning challenges. Conventional productivity tools typically demand sustained self-discipline and consistent use, which many students struggle with, leading to disruptive app-switching behaviors. Socially Assistive Robots (SARs), known for their intuitive and interactive nature, offer promising potential to support productivity in academic environments, having been successfully utilized in domains like education, cognitive development, and mental health. To leverage SARs effectively in addressing student productivity, this study employed a Participatory Design (PD) approach, directly involving college students and a Student Success and Well-Being Coach in the design process. Through interviews and a collaborative workshop, we gathered detailed insights on productivity challenges and identified desirable features for a productivity-focused SAR. Importantly, ethical considerations were integrated from the onset, facilitating responsible and user-aligned design choices. Our contributions include comprehensive insights into student productivity challenges, SAR design preferences, and actionable recommendations for effective robot characteristics. Additionally, we present stakeholder-derived ethical guidelines to inform responsible future implementations of productivity-focused SARs in higher education.

</details>


### [367] [Real-World Reinforcement Learning of Active Perception Behaviors](https://arxiv.org/abs/2512.01188)
*Edward S. Hu,Jie Wang,Xingfang Yuan,Fiona Luo,Muyao Li,Gaspard Lambrechts,Oleh Rybkin,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人学习方法AAWR，利用训练时可见的额外传感器数据帮助机器人高效学习主动感知策略，从而在部分可观测环境下更好地完成任务。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实环境中经常无法通过即刻的感知获得所有与任务相关的状态信息，主动获取信息是实现最优行为的关键，但现有学习方法难以实现有效的主动感知行为。

Method: 提出了非对称优势加权回归（AAWR）方法，在训练阶段利用额外的“特权”传感器指导策略学习，结合少量示范和粗略初始化策略，通过学习高质量价值函数来提升策略优势。

Result: 在三种机器人上的8个操控任务中，AAWR学得的主动感知策略均优于以往方法，即使初始策略表现较差，AAWR也能高效地生成信息获取行为，在极端部分可观测条件下表现突出。

Conclusion: AAWR是一个实用且高效的主动感知学习框架，在多机器人多任务场景下有效推动了机器人在部分可观测环境下的学习和决策能力。

Abstract: A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to "privileged" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a "generalist" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/

</details>


### [368] [RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments](https://arxiv.org/abs/2512.01194)
*Jaejin Jeon,Seonghoon Ryoo,Sang-Duck Lee,Soomok Lee,Seungwoo Jeong*

Main category: cs.RO

TL;DR: 本文提出了RoboLoc数据集，专注于室内外无缝切换环境下的机器人位置识别，并对多种最新模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的数据集多集中于室外场景，缺乏能够反映室内外复杂切换的无缝域转移场景。实际机器人导航任务中，可靠的位姿识别需应对室内外频繁转换。

Method: 设计并采集了RoboLoc数据集，包括真实机器人在室内外、不同比高和过渡场景下的轨迹信息。数据集兼顾结构化室内和非结构化室外环境。采用基于点云、体素和BEV视角的多种位置识别模型进行评测，分析域迁移下的泛化能力。

Result: RoboLoc验证了多种最先进模型在不同场景切换（室内-室外）的表现，对比显示各架构对域转换的适应性存在明显差异。

Conclusion: RoboLoc成为开发和评估多域机器人定位系统的真实测试平台，有助于推动机器人及自动导航领域对复杂环境下位置识别研究的进步。

Abstract: Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions. However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts. In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions. RoboLoc features real-world robot trajectories, diverse elevation profiles, and transitions between structured indoor and unstructured outdoor domains. We benchmark a variety of state-of-the-art models, point-based, voxel-based, and BEV-based architectures, highlighting their generalizability domain shifts. RoboLoc provides a realistic testbed for developing multi-domain localization systems in robotics and autonomous navigation

</details>


### [369] [COMET: A Dual Swashplate Autonomous Coaxial Bi-copter AAV with High-Maneuverability and Long-Endurance](https://arxiv.org/abs/2512.01246)
*Shuai Wang,Xiaoming Tang,Junning Liang,Haowen Zheng,Biyu Ye,Zhaofeng Liu,Fei Gao,Ximin Lyu*

Main category: cs.RO

TL;DR: 本论文提出了一种名为COMET的共轴双旋翼自主飞行器平台，采用双挥杆机构，解决了紧凑性、效率与机动性难以兼顾的问题。


<details>
  <summary>Details</summary>
Motivation: 共轴双旋翼具有结构紧凑和效率潜力，但在效率、机动性和紧凑性三者之间难以平衡，目前技术限制了其实用化。

Method: 设计了带有双挥杆机构的共轴双旋翼飞行器，并通过台架实验优化结构效率和紧凑性，飞行实验验证其在不同载荷下的效率和鲁棒性，通过轨迹跟踪测试评估机动性能。

Result: 实验表明，双挥杆结构相比单挥杆方案提升了轨迹跟踪性能与飞行效率，系统在多种场景下成功完成自主飞行。

Conclusion: COMET平台通过双挥杆设计显著提升了共轴双旋翼飞行器的性能，验证了其在实际应用中的潜力。

Abstract: Coaxial bi-copter autonomous aerial vehicles (AAVs) have garnered attention due to their potential for improved rotor system efficiency and compact form factor. However, balancing efficiency, maneuverability, and compactness in coaxial bi-copter systems remains a key design challenge, limiting their practical deployment. This letter introduces COMET, a coaxial bi-copter AAV platform featuring a dual swashplate mechanism. The coaxial bi-copter system's efficiency and compactness are optimized through bench tests, and the whole prototype's efficiency and robustness under varying payload conditions are verified through flight endurance experiments. The maneuverability performance of the system is evaluated in comprehensive trajectory tracking tests. The results indicate that the dual swashplate configuration enhances tracking performance and improves flight efficiency compared to the single swashplate alternative. Successful autonomous flight trials across various scenarios verify COMET's potential for real-world applications.

</details>


### [370] [Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms](https://arxiv.org/abs/2512.01280)
*Longji Yin,Yunfan Ren,Fangcheng Zhu,Liuyu Shi,Fanze Kong,Benxu Tang,Wenyi Liu,Ximin Lyu,Fu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种完全分布式的基于激光雷达（LiDAR）无人机集群自主协同跟踪系统，实现了多无人机在复杂环境下对目标的鲁棒协作跟踪，提升了可见性和多方向覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 虽然单机无人机跟踪已被广泛研究，但集群协同跟踪因其分布感知、冗余容错和多方向覆盖等独特优势，仍相对缺乏系统性研究。现有研究尚未充分解决复杂环境中多无人机协作下实时可见性保持和高效分布规划等挑战。

Method: 本文提出了基于LiDAR的分布式无人机集群跟踪框架。包括：（1）用球面有符号距离场（SSDF）表征3D环境遮挡，并配套实时更新算法；（2）设计通用视野对齐代价函数，兼容异构LiDAR配置；（3）提出基于类静电势分布的创新多方向包围协调度量，强化协同行为。所有创新集成到多层次规划系统中，包括具有动力学约束的前端搜索器和空间-时域$SE(3)$后端优化器。

Result: 该系统已在异构LiDAR无人机集群上实现完全分布式部署，具备协同感知、分布式规划和动态重构能力。通过实际复杂户外环境下对高速目标（无人机、人员）的实验，验证了系统对目标的稳健跟踪与优异的可见性维护能力。

Conclusion: 提出的方法能显著提升无人机集群在复杂遮挡环境下的协同跟踪和多方向可见性能力，展示了在安防、影视和工业检测等领域的广阔应用前景。

Abstract: Autonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal $SE(3)$ back-end optimizer to generate collision-free, visibility-optimized trajectories.Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance.

</details>


### [371] [Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning](https://arxiv.org/abs/2512.01336)
*Diyuan Shi,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 本文利用深度强化学习和课程学习，训练类人机器人自主学习跌倒保护策略，通过形成三角结构显著降低跌倒损害，并成功迁移至现实机器人平台。


<details>
  <summary>Details</summary>
Motivation: 类人机器人较容易跌倒，且跌倒时易造成硬件和环境损害，而现有基于控制的防跌倒方法应对多变场景效果一般，因此亟需更鲁棒、适应性强的跌倒保护方案。

Method: 采用大规模深度强化学习与课程学习，结合精心设计的奖励函数和多样化的训练环境，鼓励机器人自主探索最适合物理特性和形态的跌倒保护行为。

Result: 机器人学会通过身体三点接触地面（形成三角结构）来显著减小跌倒伤害，在多项指标和实验中优于已有方法，并在真实机器人平台验证了算法可行性。

Conclusion: 课程学习和深度强化学习能够引导类人机器人发掘其形态优化的跌倒保护方式，在降低损伤和实际环境迁移方面展现出巨大潜力。

Abstract: Humanoid robots have received significant research interests and advancements in recent years. Despite many successes, due to their morphology, dynamics and limitation of control policy, humanoid robots are prone to fall as compared to other embodiments like quadruped or wheeled robots. And its large weight, tall Center of Mass, high Degree-of-Freedom would cause serious hardware damages when falling uncontrolled, to both itself and surrounding objects. Existing researches in this field mostly focus on using control based methods that struggle to cater diverse falling scenarios and may introduce unsuitable human prior. On the other hand, large-scale Deep Reinforcement Learning and Curriculum Learning could be employed to incentivize humanoid agent discovering falling protection policy that fits its own nature and property. In this work, with carefully designed reward functions and domain diversification curriculum, we successfully train humanoid agent to explore falling protection behaviors and discover that by forming a `triangle' structure, the falling damages could be significantly reduced with its rigid-material body. With comprehensive metrics and experiments, we quantify its performance with comparison to other methods, visualize its falling behaviors and successfully transfer it to real world platform.

</details>


### [372] [Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1](https://arxiv.org/abs/2512.01358)
*Junsung Park,Hogun Kee,Songhwai Oh*

Main category: cs.RO

TL;DR: 提出了一种通过模态增强微调基础机器人策略以适应不同人形机器人平台的方法，验证其在GR1和Unitree G1两种机器人上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基础机器人策略在不同机器人实体上迁移时，通常性能下降，作者希望通过引入多模态信息提升策略泛化能力和迁移效果。

Method: 在GR1上，使用公开数据集并加入二值接触信号和深度信息等后处理模态；在Unitree G1上，构建包含多模态（包括运动规划、逆运动学、接触力测量等）的新数据集，采用多模态微调的方法提升策略。

Result: GR1机器人中加入接触信息和RGB-D融合后，在线成功率由51%提升到63%；在G1的“将苹果放入碗中”任务中，接触增强模型成功率达94%，远高于标准微调（48%）和零样本迁移基线（0%）。

Conclusion: 模态增强提高了多平台泛化迁移能力，轻量后处理可提升GR1策略，多模态高质量数据对Unitree G1可靠迁移至关重要，为机器人基础策略扩展提供统一的数据驱动路径。

Abstract: This paper presents a modality-augmented fine-tuning framework designed to adapt foundation robot policies to diverse humanoid embodiments. We validate our approach across two distinct settings: (i) the GR1 embodiment, utilizing public datasets where we introduce post-processed modalities, including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we contribute a novel multi-modal dataset incorporating cuRobo motion planning, inverse kinematics, and ground-truth contact-force measurements. Our experiments demonstrate that modality augmentation consistently enhances policy performance across different embodiments. Specifically, for the GR1, integrating contact-state cues and RGB-D fusion improves online success rates from 51% to 63%. Furthermore, in the G1 "Pick Apple to Bowl" task, our contact-augmented model achieves a success rate of 94%, significantly outperforming the 48% achieved by standard fine-tuning and the 0% baseline of zero-shot transfer. These results highlight that lightweight post-processing effectively strengthens policies for GR1, while high-quality multi-modal data is crucial for reliable transfer to the Unitree G1. Consequently, this work establishes a unified, data-centric pathway for extending foundation robot policies through targeted modality design and multi-modal fine-tuning.

</details>


### [373] [$\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering](https://arxiv.org/abs/2512.01446)
*Jiayi Li,Yuxuan Hu,Haoran Geng,Xiangyu Chen,Chuhao Zhou,Ziteng Cui,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人操作的材料泛化方法，通过物理特性驱动的光度再渲染，从单次演示生成多种材料效果的数据，有效提升了机器人在不同材料间的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实际机器人任务中，物体材质多样且可能具备透明、反光等特性，导致现有方法难以泛化。现有模拟-现实迁移方法受限于视觉域差，现实数据采集又昂贵、耗时且覆盖有限材料。为解决上述难题，需发展无需大量数据即可泛化到多种表面材料的新方法。

Method: 提出了Mutable Material Manipulation Augmentation（M$^3$A）框架，利用计算摄影学，通过真实演示的光传输特性进行光度再渲染，生成不同材料属性的高真实感演示数据，用以训练能够材料泛化的策略模型。

Result: 首次构建了多材料、多环境的操作基准，实验证明M$^3$A大幅提升了跨材料泛化能力，三项真实任务平均成功率提升了58.03%，且对新材料展现出良好鲁棒性。

Conclusion: 通过光度再渲染增强数据可有效解耦操作技能与表面外观，显著增强机器人在未知材料上的泛化与适应能力，减少额外数据收集需求。

Abstract: Material generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting diverse visual and physical properties. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials. To overcome these limitations, we resort to computational photography and introduce Mutable Material Manipulation Augmentation (M$^3$A), a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we photometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection. To systematically evaluate this capability, we construct the first comprehensive multi-material manipulation benchmark spanning both simulation and real-world environments. Extensive experiments show that the M$^3$A policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by 58.03\%, and demonstrating robust performance on previously unseen materials.

</details>


### [374] [NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction](https://arxiv.org/abs/2512.01550)
*Fei Liu,Shichao Xie,Minghua Luo,Zedong Chu,Junjun Hu,Xiaolong Wu,Mu Xu*

Main category: cs.RO

TL;DR: NavForesee提出了一种新的视觉-语言模型，通过结合高层次的语言计划与世界模型预测，实现复杂自然语言指导下的长期智能体导航任务，显著提升了在R2R-CE和RxR-CE基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前智能体在根据自然语言指令完成长时序导航任务时，面对陌生环境规划能力弱、失败率高，表明现有方法在高阶语言推理与环境预测上存在不足。

Method: NavForesee将高层次的语言任务分解和生成性环境预测融合于同一个视觉-语言模型中。模型在完整指令和历史观测条件下，具备分解任务、跟踪进度和提出子目标的能力，并通过生成性世界模型预测短期环境变化与长期导航里程碑。同时，结构化计划引导模型的有针对性预测，未来想象反过来优化导航动作，实现感知—规划/预测—行动的循环反馈。

Result: 在R2R-CE和RxR-CE复杂导航基准上，NavForesee在多个指标上取得了高度竞争性的结果，验证了融合语言计划与空间世界预测带来的效果提升。

Conclusion: 通过将语言高阶推理和时空预测能力统一进VLM，NavForesee显著提高了智能体在复杂自然语言引导导航任务上的智能与实力，为未来更高级的具身智能体导航系统指明了方向。

Abstract: Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.

</details>


### [375] [L2M-Calib: One-key Calibration Method for LiDAR and Multiple Magnetic Sensors](https://arxiv.org/abs/2512.01554)
*Qiyang Lyu,Wei Wang,Zhenyu Wu,Hongming Shen,Huiqin Zhou,Danwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为L2M-Calib的新型磁-激光雷达融合多传感器一键校准方法，能够高效估算两类传感器间的外参和磁传感器的内在畸变参数，实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态传感器融合能提高环境感知的鲁棒性，但高效、精确的多传感器校准是实现有效融合的关键，尤其针对磁传感器因缺乏校准方法导致其在多传感器系统中应用受限。

Method: 作者提出L2M-Calib框架，结合迭代高斯-牛顿法优化外参，并将磁传感器内参校准建模为加权岭正则化TLS（w-RRTLS）问题，以增强噪声和数据不适定情况下的鲁棒性。

Result: 在模拟数据集和真实机器人场景（如AGV装载传感器）下进行大量实验，表明该方法无论在不同环境和任务下都能实现高精度和鲁棒的校准效果。

Conclusion: L2M-Calib是首个有效解决磁-激光雷达多传感器融合中的自动高精度校准问题的方法，为磁传感器在复杂环境下的多模态感知应用提供了坚实基础。

Abstract: Multimodal sensor fusion enables robust environmental perception by leveraging complementary information from heterogeneous sensing modalities. However, accurate calibration is a critical prerequisite for effective fusion. This paper proposes a novel one-key calibration framework named L2M-Calib for a fused magnetic-LiDAR system, jointly estimating the extrinsic transformation between the two kinds of sensors and the intrinsic distortion parameters of the magnetic sensors. Magnetic sensors capture ambient magnetic field (AMF) patterns, which are invariant to geometry, texture, illumination, and weather, making them suitable for challenging environments. Nonetheless, the integration of magnetic sensing into multimodal systems remains underexplored due to the absence of effective calibration techniques. To address this, we optimize extrinsic parameters using an iterative Gauss-Newton scheme, coupled with the intrinsic calibration as a weighted ridge-regularized total least squares (w-RRTLS) problem, ensuring robustness against measurement noise and ill-conditioned data. Extensive evaluations on both simulated datasets and real-world experiments, including AGV-mounted sensor configurations, demonstrate that our method achieves high calibration accuracy and robustness under various environmental and operational conditions.

</details>


### [376] [A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics](https://arxiv.org/abs/2512.01598)
*Marek Vagas,Martin Varga,Jaroslav Romancik,Ondrej Majercak,Alejandro Suarez,Anibal Ollero,Bram Vanderborght,Ivan Virgala*

Main category: cs.RO

TL;DR: 本文提出了一套新的跨平台机械手夹持器基准评测方法（CEGB），可用于评估机械手在多种机器人平台（如工业机器人、协作机器人和空中机器人）间的可迁移性及其能耗表现。


<details>
  <summary>Details</summary>
Motivation: 目前常用的YCB和NIST基准仅在单一平台下评估夹持器的性能，无法满足现代移动和空中操作对“夹持器跨平台可迁移性”及“能效”评测的需求。

Method: 提出CEGB测试套件，补充了现有基准，包含三个新增测试：1）转移时间（跨平台更换夹持器所需时间）；2）能耗（夹持与保持过程的能效）；3）意图相关的理想载荷（根据设计反映操作能力）。以轻型自锁式夹持器为参考实例进行实验，评估迁移时间、能耗和夹持性能。

Result: 所提夹持器在不同用户组间迁移时间约为17.6秒，保持能耗低至每10秒1.5焦耳，夹持成功率超过90%，周期时间3.2~3.9秒，表现稳定。

Conclusion: CEGB为机械手夹持器的跨平台、能效自觉的统一评测提供了可重现的基础，有利于评估和优化空中与地面操作系统中的夹持器共用能力。

Abstract: Robotic grippers are increasingly deployed across industrial, collaborative, and aerial platforms, where each embodiment imposes distinct mechanical, energetic, and operational constraints. Established YCB and NIST benchmarks quantify grasp success, force, or timing on a single platform, but do not evaluate cross-embodiment transferability or energy-aware performance, capabilities essential for modern mobile and aerial manipulation. This letter introduces the Cross-Embodiment Gripper Benchmark (CEGB), a compact and reproducible benchmarking suite extending YCB and selected NIST metrics with three additional components: a transfer-time benchmark measuring the practical effort required to exchange embodiments, an energy-consumption benchmark evaluating grasping and holding efficiency, and an intent-specific ideal payload assessment reflecting design-dependent operational capability. Together, these metrics characterize both grasp performance and the suitability of reusing a single gripper across heterogeneous robotic systems. A lightweight self-locking gripper prototype is implemented as a reference case. Experiments demonstrate rapid embodiment transfer (median ~= 17.6 s across user groups), low holding energy for gripper prototype (~= 1.5 J per 10 s), and consistent grasp performance with cycle times of 3.2 - 3.9 s and success rates exceeding 90%. CEGB thus provides a reproducible foundation for cross-platform, energy-aware evaluation of grippers in aerial and manipulators domains.

</details>


### [377] [Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track](https://arxiv.org/abs/2512.01608)
*Mo Chen*

Main category: cs.RO

TL;DR: 该论文提出了一套面向非完整性差动移动机器人、实现实时自主轨迹导航的系统，核心是多任务视觉感知与具备收敛性分析的控制器的深度融合，并在嵌入式平台上经实验证明其效果。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术发展，移动机器人需要在没有高精度地图和全球卫星定位条件下，依靠视觉完成实时、自主、安全的轨迹导航，因此迫切需要一种稳健且高效的端到端系统。

Method: 设计了基于2D-3D相机投影的车道中心线重建流程，通过弧长均匀重采样和三次多项式拟合（QR最小二乘法）实现稳健感知；控制器基于Lyapunov稳定性原理调节机器人线速度与角速度，保证误差收敛性，适应动态、不完整感知场景。

Result: 在真实嵌入式平台下的实验表明，该系统具备高忠实度、实时性，轨迹平滑、闭环稳定，有效实现了可靠的自主导航能力。

Conclusion: 无须高精地图或GPS，只需视觉感知和理论保障的控制方法，即可实现非完整性移动机器人的安全自主导航，证实了方法的实用性和广泛应用价值。

Abstract: This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.

</details>


### [378] [Dynamic Log-Gaussian Process Control Barrier Function for Safe Robotic Navigation in Dynamic Environments](https://arxiv.org/abs/2512.01668)
*Xin Yin,Chenyang Liang,Yanning Guo,Jie Mei*

Main category: cs.RO

TL;DR: 提出了一种新颖的基于高斯过程的控制障碍函数（CBF）方法DLGP-CBF，用于在动态未知环境中实现机器人安全导航。


<details>
  <summary>Details</summary>
Motivation: 传统CBF在面对未知和动态障碍物时难以在线生成具有充分信息和对障碍物运动敏感的障碍函数，限制了机器人在实际场景的安全性和灵活性。

Method: 结合高斯过程回归与对数变换，提出DLGP-CBF，能够即使在数据稀疏区也生成平滑且信息量丰富的障碍值和梯度。将障碍物位置作为函数变量，预测并纳入障碍物的速度，实现对动态环境的前瞻性响应。

Result: 仿真结果显示，该方法在障碍物规避性能上优于基线方法，包括提升的安全裕度、更平滑的轨迹及更好的对障碍物动态的响应能力。

Conclusion: DLGP-CBF为动态未知环境下的机器人安全导航提供了一种高效且实用的解决方案，改善了现有方法的短板。

Abstract: Control Barrier Functions (CBFs) have emerged as efficient tools to address the safe navigation problem for robot applications. However, synthesizing informative and obstacle motion-aware CBFs online using real-time sensor data remains challenging, particularly in unknown and dynamic scenarios. Motived by this challenge, this paper aims to propose a novel Gaussian Process-based formulation of CBF, termed the Dynamic Log Gaussian Process Control Barrier Function (DLGP-CBF), to enable real-time construction of CBF which are both spatially informative and responsive to obstacle motion. Firstly, the DLGP-CBF leverages a logarithmic transformation of GP regression to generate smooth and informative barrier values and gradients, even in sparse-data regions. Secondly, by explicitly modeling the DLGP-CBF as a function of obstacle positions, the derived safety constraint integrates predicted obstacle velocities, allowing the controller to proactively respond to dynamic obstacles' motion. Simulation results demonstrate significant improvements in obstacle avoidance performance, including increased safety margins, smoother trajectories, and enhanced responsiveness compared to baseline methods.

</details>


### [379] [DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models](https://arxiv.org/abs/2512.01715)
*Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Yicheng Feng,Sipeng Zheng,Qin Jin,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了DiG-Flow框架，通过几何正则化提升视觉-语言-动作模型（VLA）在机器人任务中的鲁棒性，实验和理论结果均显示该方法提升了复杂任务和分布变化下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管使用流匹配训练的VLA模型在机器人操作任务中取得了优异的成绩，但在遇到分布偏移或复杂多步任务时，模型性能下降，说明其表征未能稳健捕捉任务语义。论文旨在解决VLA模型在实际复杂环境下鲁棒性不足的问题。

Method: 提出DiG-Flow框架，利用观测与动作嵌入的分布差异作为几何信号：差异小表示表征兼容，差异大则表征不一致。通过单调函数将此差异映射为调节权重，在流匹配前对观测嵌入进行残差更新。该方法不改变流匹配路径或目标向量场，只在表征层干预。理论分析保证训练目标下降，推理收敛。

Result: DiG-Flow可以无缝整合到现有VLA架构，几乎无额外开销。在实验中，无论是在有限数据还是复杂多步任务下，性能均有显著提升，尤其在分布变化时鲁棒性更好。

Conclusion: DiG-Flow通过几何正则化显著增强了VLA模型的鲁棒性和通用性，特别适合应对复杂任务和分布偏移，在理论和实验上均验证了其有效性。

Abstract: Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.

</details>


### [380] [AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields](https://arxiv.org/abs/2512.01753)
*Zhihao Zhan,Yuhang Ming,Shaobin Li,Jie Yuan*

Main category: cs.RO

TL;DR: 本文提出了AgriLiRa4D，一个面向复杂农田环境的多模态无人机（UAV）数据集，用于推动多传感器SLAM研究。


<details>
  <summary>Details</summary>
Motivation: 农业无人机在精准作业中（如喷洒、巡检等）对SLAM依赖极强，但现有真实多模态农田UAV数据集极为稀缺，限制了鲁棒性相关研究。

Method: 作者构建了一个包含三种典型农田地形（平地、丘陵、梯田）、两种作业模式（边界与覆盖）、六组飞行序列、采用高精度惯导与RTK定位、并同时采集3D激光雷达、4D雷达和IMU的多模态、不同比例、带完整标定信息的数据集。

Result: AgriLiRa4D支持对多种SLAM与定位算法的全面测试。作者还基准评测了四种主流多传感器SLAM算法，并展示了各类农业环境挑战下多传感器融合的重要性和现有方法的困难。

Conclusion: AgriLiRa4D填补了农用UAV SLAM领域数据集空白，将为鲁棒自主导航等相关研究提供强基准和推动。

Abstract: Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.

</details>


### [381] [IGen: Scalable Data Generation for Robot Learning from Open-World Images](https://arxiv.org/abs/2512.01773)
*Chenghao Gu,Haolan Kang,Junchao Lin,Jinghe Wang,Duo Wu,Shuzhao Xie,Fanding Huang,Junchen Ge,Ziyang Gong,Letian Li,Hongying Zheng,Changwei Lv,Zhi Wang*

Main category: cs.RO

TL;DR: IGen框架可以从开放世界图像大规模生成高质量视觉—动作数据，提升通用机器人策略训练的数据获取效率。


<details>
  <summary>Details</summary>
Motivation: 当前通用机器人策略对大规模训练数据需求爆发式增长，但现实中通过机器人自身采集数据成本高且场景有限。开放世界图片虽然丰富多样，但缺乏机器人动作信息，难以直接用于机器人学习。该论文旨在填补这一利用视觉大数据训练机器人的关键空白。

Method: 提出IGen框架，将未结构化的2D开放世界图像转化为可操控的3D场景表示。借助视觉-语言模型推理，将任务指令转为高层次计划，再转化为末端执行器的低层动作序列（SE(3)位姿）。通过这些动作合成动态场景变化并渲染时序连贯的视觉观测。

Result: 实验显示，IGen生成的视觉-动作数据质量高，仅依赖该数据训练的机器人策略，其表现与真实世界数据训练的策略相当。

Conclusion: IGen有效解决了开放世界图像难以用于机器人学习的问题，为通用机器人策略提供了可扩展的高质量数据生成方案，显著推动了相关领域的发展。

Abstract: The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.

</details>


### [382] [GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2512.01801)
*Yunfei Li,Xiao Ma,Jiafeng Xu,Yu Cui,Zhongren Cui,Zhigang Han,Liqun Huang,Tao Kong,Yuxiao Liu,Hao Niu,Wanli Peng,Jingchao Qiao,Zeyu Ren,Haixin Shi,Zhi Su,Jiawen Tian,Yuyang Xiao,Shenyu Zhang,Liwei Zheng,Hang Li,Yonghui Wu*

Main category: cs.RO

TL;DR: GR-RL是一种将通用视觉-语言-动作策略（VLA）转化为特定精通领域（如复杂长期灵巧操作）的机器人学习框架，专注于去除人类示范中的噪声和次优影响，并提升操控表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA策略过度依赖人类示范最优性假设，但在人类很难精准完成的灵巧复杂操作任务中，人类示范本身经常带有噪声与次优，影响机器人策略的最终表现。

Method: GR-RL提出多阶段训练流程：1）学习视觉-语言的任务进展函数，过滤并保留有益进展的演示轨迹；2）利用离线强化学习稀疏奖励的Q函数作为鲁棒进展评价；3）引入形态对称性数据增强提升泛化能力；4）通过学习潜空间噪声预测执行在线RL以提升策略与实际部署对齐。

Result: GR-RL能够自动完成复杂的“系鞋带”任务，需跨越多个鞋眼，上述任务涉及长期推理、毫米级高精度和柔体接触。实验中，GR-RL实现83.3%的成功率，显示其在长期灵巧操作方面的机器人能力显著增强。

Conclusion: GR-RL为通用型机器人基础模型向可靠、现实世界专家化迈进提供了关键技术手段，尤其在需要较高精度及复杂推理任务具有很大潜力。

Abstract: We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.

</details>


### [383] [Much Ado About Noising: Dispelling the Myths of Generative Robotic Control](https://arxiv.org/abs/2512.01809)
*Chaoyi Pan,Giri Anantharaman,Nai-Chieh Huang,Claire Jin,Daniel Pfrommer,Chenyang Yuan,Frank Permenter,Guannan Qu,Nicholas Boffi,Guanya Shi,Max Simchowitz*

Main category: cs.RO

TL;DR: 本文系统评估了生成式控制策略（GCP）在机器人仿真中的效果，发现其优越性并非因其能够捕获多模态分布或表达复杂观察—动作映射，而是来源于带中间步骤监督的迭代计算过程。


<details>
  <summary>Details</summary>
Motivation: 当前生成式模型（如流模型和扩散模型）在机器人策略参数化方面表现出色，但关于其成功机制存在诸多猜测，有必要系统分析其真正优势所在。

Method: 作者在常用行为克隆（BC）基准任务上，对多种主流生成式控制策略进行评估，并通过控制实验验证多模态、复杂性表征等假设，同时提出最小迭代策略（MIP）进行性能对比。

Result: 实验表明，生成式控制策略的性能提升主要来自训练时对每一步中间结果的监督和适度随机性引入，而不是分布拟合或复杂映射能力。MIP模型能与流GCPs表现相当，常优于捷径蒸馏模型。

Conclusion: 生成式策略的分布拟合能力并非决定性优势，未来可关注以控制性能为核心的新型模型设计空间。

Abstract: Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/

</details>


### [384] [Is Image-based Object Pose Estimation Ready to Support Grasping?](https://arxiv.org/abs/2512.01856)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: 本文提出了一个用于评估单RGB图像输入的6自由度物体位姿估计算法的框架，并通过仿真抓取实验深入比较了五种开源算法在实际机器人抓取任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前许多6-DoF物体位姿估计算法只用单张RGB图像，但尚缺系统性分析其在机器人抓取真实应用中的效果，尤其作为抓取任务唯一感知手段时的实用性未知。本文旨在填补这一空白。

Method: 设计了基于物理仿真的抓取实验流程，将评估对象的位姿估计结果直接用于控制平行夹爪和自适应机械手。本实验使用BOP数据集子集，比较了五种开源位姿估计算法，关注其用于抓取的实用表现。

Result: 实验揭示了不同算法在机器人抓取场景下的实际表现差异，并发现部分评测维度之前文献中未充分讨论，提供了新的实用洞见。

Conclusion: 单RGB图像位姿估算虽有潜力服务于实际机器人抓取任务，但具体算法差异大，本文工作为实际系统选型与研究提供参考，并指出了现有方法的局限和未来改进方向。

Abstract: We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.

</details>


### [385] [NeuroHJR: Hamilton-Jacobi Reachability-based Obstacle Avoidance in Complex Environments with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.01897)
*Granthik Halder,Rudrashis Majumder,Rakshith M R,Rahi Shah,Suresh Sundaram*

Main category: cs.RO

TL;DR: 本文提出了一种新框架NeuroHJR，利用物理信息神经网络（PINNs）近似 Hamilton-Jacobi Reachability (HJR) 解，实现了自动地面车辆在复杂环境下的高效避障。


<details>
  <summary>Details</summary>
Motivation: 传统 HJR 方法虽然拥有完备的安全性理论保障，但在面对大量障碍物和复杂环境时，计算效率低下，难以实时应用到自动车辆导航。

Method: 作者提出NeuroHJR框架，将系统动力学和安全约束直接融入到神经网络损失函数，通过PINN学习避免对状态空间的网格离散化，在连续状态空间中高效近似可达集。

Result: 在仿真实验中，NeuroHJR在密集障碍环境下的安全性表现可与经典HJR求解器媲美，但计算效率大幅提升。

Conclusion: NeuroHJR为基于可达性分析的实时、可扩展避障提供了新的可能，有助于机器人在复杂场景中的实际部署。

Abstract: Autonomous ground vehicles (AGVs) must navigate safely in cluttered environments while accounting for complex dynamics and environmental uncertainty. Hamilton-Jacobi Reachability (HJR) offers formal safety guarantees through the computation of forward and backward reachable sets, but its application is hindered by poor scalability in environments with numerous obstacles. In this paper, we present a novel framework called NeuroHJR that leverages Physics-Informed Neural Networks (PINNs) to approximate the HJR solution for real-time obstacle avoidance. By embedding system dynamics and safety constraints directly into the neural network loss function, our method bypasses the need for grid-based discretization and enables efficient estimation of reachable sets in continuous state spaces. We demonstrate the effectiveness of our approach through simulation results in densely cluttered scenarios, showing that it achieves safety performance comparable to that of classical HJR solvers while significantly reducing the computational cost. This work provides a new step toward real-time, scalable deployment of reachability-based obstacle avoidance in robotics.

</details>


### [386] [Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model](https://arxiv.org/abs/2512.01924)
*Kentaro Fujii,Shingo Murata*

Main category: cs.RO

TL;DR: 作者提出了一种新的深度主动推断框架，能让机器人在不确定环境下有效实现目标导向和探索行为，并通过三个模型协同工作，有效降低计算成本，在真实机器操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多数基于深度学习的控制方法在真实世界的不确定环境中难以平衡探索与目标导向行为，且在探索能力和不确定性下表现欠佳。作者希望突破深度主动推断方法在环境建模能力不足和动作选择计算成本高的瓶颈。

Method: 提出包含世界模型、动作模型和抽象世界模型的新型深度主动推断框架。世界模型将环境动态编码为不同时间尺度的隐状态，动作模型用向量量化将动作序列压缩为抽象动作，抽象世界模型在抽象动作条件下预测慢变量状态，从而高效进行动作选择。

Result: 在真实机器人的物体操作任务中验证了该框架，结果显示在多样任务下均能取得高成功率，并能在不确定环境下在目标导向与探索动作间灵活切换，同时显著降低了动作选择的计算复杂度。

Conclusion: 多时间尺度动力学建模及对动作与状态转移的抽象对于实现高效且灵活的机器人主动推断极为重要，新的框架很好地兼顾了性能与计算效率。

Abstract: Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.

</details>


### [387] [Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models](https://arxiv.org/abs/2512.01946)
*Paul Pacaud,Ricardo Garcia,Shizhe Chen,Cordelia Schmid*

Main category: cs.RO

TL;DR: 本文提出了一种自动生成机器人操作失败数据的方法，通过对成功的操作轨迹进行程序化扰动，生成多样化的规划与执行失败案例，并据此构建了新的故障检测数据集和基准，最终训练并验证了新模型Guardian在故障检测和恢复上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 可靠的机器人操作需要高效的故障检测和恢复机制，但现有视觉-语言模型在这方面的性能受限于失败数据的稀缺。为弥补该数据缺口，提高检测的准确率与泛化能力，作者希望自动化生成丰富且细致的失败数据。

Method: 首先，作者提出了一种自动生成机器人失败数据的方法，通过对成功操作流程进行程序化扰动，设计多样的失败情景，包括不同类型和程度。然后将这些数据用于建立三个全新故障检测基准数据集（RLBench-Fail、BridgeDataV2-Fail、UR5-Fail），并训练了一个多视角图像和视觉-语言结合的模型Guardian，用于细致的失败推理与检测。

Result: 生成的大规模多样化故障数据集极大丰富了现有研究资源。Guardian模型在现有和新引入的基准数据集上均取得了最优检测性能，并在仿真和真实机器人上的操作成功率得到显著提升。

Conclusion: 通过自动生成高质量失败数据，显著提升了视觉-语言模型在机器操作故障检测和恢复上的能力，为相关领域的研究和实际应用提供了新的方法和数据支持。

Abstract: Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.

</details>


### [388] [RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies](https://arxiv.org/abs/2512.01993)
*Guillermo Garcia-Cobo,Maximilian Igl,Peter Karkus,Zhejun Zhang,Michael Watson,Yuxiao Chen,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 提出了一种名为RoaD（Rollouts as Demonstrations）的方法，通过利用策略自身的闭环轨迹作为额外训练数据，有效缓解自动驾驶中的共变量偏移问题，并在两个大型仿真平台上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的策略通常通过人类演示的开放环路行为克隆进行训练，但在实际闭环部署时会遇到共变量偏移和错误累积的问题。论文希望解决这种训练与部署分布不一致导致的性能下降。

Method: RoaD方法在策略闭环轨迹生成过程中，结合专家的指导，将轨迹偏向高质量行为，生成更有信息且现实的演示数据，用以微调原有策略。这样可以减少对数据量的需求，同时避免以往闭环监督微调方法的限制性假设。

Result: 在大型交通仿真基准WOSAC上，RoaD表现与现有闭环监督微调（CL-SFT）方法相当或更好。在AlpaSim高保真神经重建仿真器中，相比原方法驾驶得分提高41%，碰撞减少54%。

Conclusion: RoaD方法实现了少量数据下的稳健闭环适应能力，兼具高效率和广泛适用性，有望大幅提升自动驾驶系统现实场景下的可靠性和安全性。

Abstract: Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.

</details>


### [389] [Learning Sim-to-Real Humanoid Locomotion in 15 Minutes](https://arxiv.org/abs/2512.01996)
*Younggyo Seo,Carmelo Sferrazza,Juyue Chen,Guanya Shi,Rocky Duan,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本论文提出了一种基于改进的离策略强化学习（RL）算法（FastSAC和FastTD3），能够在单块4090显卡上，仅用15分钟快速训练出拟人机器人行走控制策略，并实现在真实机器人（如Unitree G1、Booster T1）上的高效迁移。


<details>
  <summary>Details</summary>
Motivation: 虽然并行仿真极大加速了机器人强化学习的训练，但在高维、需要大规模域随机化（提升现实可迁移性）的拟人机器人控制上，依然面临算法不稳定、训练不可靠等难题。因此，亟需找到一种既能在大规模仿真环境下稳定工作、又能提升sim-to-real能力的高效方法。

Method: 提出基于off-policy强化学习的FastSAC与FastTD3算法，通过精心调整设计（如奖励函数极简化、参数优化），实现千级并行环境下的训练稳定性与效率提升，并采用大规模域随机化测试算法鲁棒性。

Result: 在Unitree G1与Booster T1等拟人机器人平台上，利用1张4090显卡，仅需15分钟即可训练出具备强鲁棒性的行走控制策略。同时，验证算法能在动态扰动、粗糙地形等多变环境下迁移与表现良好，并能实现整身仿真人体运动跟踪策略的快速训练。论文也放出了公开代码和演示视频。

Conclusion: 通过简洁实用的算法和训练策略，可极大提升高维机器人（尤其是拟人机器人）在仿真到现实（sim-to-real）领域的强化学习效率与可靠性，将训练时间由天数级降至分钟级，有望加速机器人复杂行为的研发。

Abstract: Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.

</details>


### [390] [LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation](https://arxiv.org/abs/2512.02002)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）静态仿真的机器人操作代码生成与修正框架，无需实际物理实验或定制仿真环境，即可实现高可靠性的机器人代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有依赖LLM生成机器人物理操作代码的方法，需要依赖昂贵且耗时的物理实验或定制仿真环境作为执行反馈，限制了广泛部署和应用。作者希望解决环境配置和执行的高成本问题，提升代码生成流程的可靠性和便捷性。

Method: 作者提出将LLM配置为静态模拟器，直接在文本层面对机器人操作代码的执行进行仿真。该框架可以解析动作、推理状态转移、分析执行结果并生成语义级观测，从而准确捕捉代码执行轨迹和动态。

Result: 在多种机器人（包括无人机和地面小车）上的多任务实验表明，该静态文本仿真框架在代码生成的准确性与可靠性上均达到甚至接近最先进的方法，而无需真实环境中的动态执行支持。

Conclusion: LLM静态仿真不仅提升了机器人代码生成的便捷性，且保持高准确性和可靠性，为未来无需物理实验的机器人智能开发提供了新思路和有效工具。

Abstract: Recent advances in Large language models (LLMs) have demonstrated their promising capabilities of generating robot operation code to enable LLM-driven robots. To enhance the reliability of operation code generated by LLMs, corrective designs with feedback from the observation of executing code have been increasingly adopted in existing research. However, the code execution in these designs relies on either a physical experiment or a customized simulation environment, which limits their deployment due to the high configuration effort of the environment and the potential long execution time. In this paper, we explore the possibility of directly leveraging LLM to enable static simulation of robot operation code, and then leverage it to design a new reliable LLM-driven corrective robot operation code generation framework. Our framework configures the LLM as a static simulator with enhanced capabilities that reliably simulate robot code execution by interpreting actions, reasoning over state transitions, analyzing execution outcomes, and generating se- mantic observations that accurately capture trajectory dynamics. To validate the performance of our framework, we performed experiments on various operation tasks for different robots, including UAVs and small ground vehicles. The experiment results not only demonstrated the high accuracy of our static text-based simulation but also the reliable code generation of our LLM-driven corrective framework, which achieves a comparable performance with state-of-the-art research while does not rely on dynamic code execution using physical experiments or simulators.

</details>


### [391] [Learning Dexterous Manipulation Skills from Imperfect Simulations](https://arxiv.org/abs/2512.02011)
*Elvis Hsieh,Wen-Han Hsieh,Yen-Jen Wang,Toru Lin,Jitendra Malik,Koushil Sreenath,Haozhi Qi*

Main category: cs.RO

TL;DR: 提出了一种新的sim-to-real（仿真到现实）框架，显著提升了多指灵巧操作任务（如螺母螺栓组装和螺丝刀操作）的泛化与鲁棒性，尤其在处理触觉信息方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习及sim-to-real方法受限于难以高保真地模拟复杂接触动力学和多模态感知（特别是触觉），导致现实任务泛化与精度有限。

Method: 三阶段框架：1）在仿真环境下用简化物体模型训练RL策略，使手指步态正确涌现；2）将RL政策作为技能原语，嵌入远程操作系统采集包含真实触觉和本体感觉的数据；3）用行为克隆（BC）方法训练融合触觉的控制策略，提升现实泛化能力。

Result: 提出的方法相比直接sim-to-real迁移，在螺母螺栓和螺丝刀任务中均展现更高的任务进度比，并对不同新形状和外部扰动具有良好鲁棒性和泛化能力。

Conclusion: 通过将仿真中的技能自举为现实演示，结合多模态感知（含触觉），有效解决了灵巧操作任务的sim-to-real难题，为实际复杂机器人操作任务的推广提供方案。

Abstract: Reinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose \ours, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations. Videos and code are available on https://dexscrew.github.io.

</details>


### [392] [ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation](https://arxiv.org/abs/2512.02013)
*Chenyang Gu,Jiaming Liu,Hao Chen,Runzhong Huang,Qingpo Wuwu,Zhuoyang Liu,Xiaoqi Li,Ying Li,Renrui Zhang,Peng Jia,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManualVLA通过引入“多模态操作手册”，提升了现有视觉-语言-动作（VLA）模型在复杂长序列任务中的规划与执行能力，显著提高LEGO组装和物体重排等任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在需要高层次规划与精确操作的长时序任务（如LEGO组装、物体重排）上表现不足，难以将目标状态（what）有效转化为可执行的操作流程（how）。因此，本文旨在增强VLA模型的任务分解与执行能力。

Method: 提出ManualVLA框架，基础为Mixture-of-Transformers架构。核心创新是引入“手册专家”生成中间多模态手册（包含图像、位置提示和文本指令），再通过手册链式推理(ManualCoT)逐步引导动作专家进行精准操作。为高效训练，开发基于3D高斯溅射的数字孪生工具，自动生成训练用手册数据。

Result: ManualVLA在真实场景下明显优于之前的分层SOTA基线，在LEGO组装和物体重排任务中，平均成功率提升32%。

Conclusion: ManualVLA有效地将长时序任务目标分解为可执行中间步骤，实现了高水平的多模态规划与精准操作融合，为复杂机器人任务提供了新的技术方案。

Abstract: Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.

</details>


### [393] [EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI](https://arxiv.org/abs/2512.02020)
*Jianlei Chang,Ruofeng Mei,Wei Ke,Xiangyu Xu*

Main category: cs.RO

TL;DR: 本文提出了EfficientFlow框架，通过引入等变性和加速正则化，提高了生成式策略学习的数据利用率和采样效率，使机器人操作任务在有限数据条件下能够高效且快速地推理和决策。


<details>
  <summary>Details</summary>
Motivation: 现有生成式策略方法在灵活控制任务表现出色，但需要大量演示数据且推理速度慢，影响了实际应用的数据与采样效率，亟需提升。

Method: 作者提出EfficientFlow框架：1）在flow-matching算法中引入等变性，理论上证明等变网络配合高斯先验能提升泛化和减少数据需求；2）为加速采样，设计了一项新的加速正则化策略，并推导出可以用条件轨迹训练的代理损失以实现稳定扩展的训练。

Result: 在多种机器人操作基准测试中，EfficientFlow在有限数据条件下达到了有竞争力甚至更优的性能，同时显著提升了推理速度。

Conclusion: EfficientFlow为高效能体AI提供了一种高性能且高效率的新范式，兼顾了更低数据需求与更快的在线推理速度。

Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.

</details>
