<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

TL;DR: 本文提出了一套名为GAZE的流水线系统，能够自动将原始长视频转化为高质量的、多模态的世界模型训练数据，极大提高标注效率并降低人工参与。


<details>
  <summary>Details</summary>
Motivation: 当前训练鲁棒的世界模型需要大规模、精细标注的多模态数据集，而人工标注由于低效与高成本成为瓶颈。如何提升多模态大数据的标注与处理能力，形成可直接用于模型训练的高质量数据集，是该研究的动机。

Method: GAZE流水线主要包括三步：1）将特定格式的360度视频规范化为标准视角并分片处理，提高并行效率；2）应用一系列AI模型（如场景理解、目标跟踪、语音转录、隐私和敏感内容检测）实现密集的多模态自动预标注；3）将多源信号汇总为结构化输出，方便人工快速复核。

Result: GAZE系统能够将人工复核总量减少80%以上，并能在每小时人工审核中节省约19分钟。通过自动跳过低价值片段，提高了标签密度和一致性，同时加入了隐私保护和数据追踪元数据，系统能生产高质量、合规的数据集。

Conclusion: GAZE流水线为生成高质量世界模型训练数据提供了可扩展的解决方案，在保障数据吞吐与治理的同时提升了自动化与效率，具有推广应用价值。

Abstract: Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>


### [2] [PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising](https://arxiv.org/abs/2510.14995)
*Yang Shi,Jingchao Wang,Liangsi Lu,Mingxuan Huang,Ruixin He,Yifeng Xie,Hanqian Liu,Minzhe Guo,Yangyang Liang,Weipeng Zhang,Zimeng Li,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了Poisson Consistent U-Net (PC-UNet)模型，并结合创新的Poisson方差与均值一致性损失（PVMC-Loss），在低剂量PET成像中提升物理一致性和图像保真度。


<details>
  <summary>Details</summary>
Motivation: PET医学成像在降低放射剂量时会显著增加噪声，现有去噪方法难以应对高泊松噪声，导致成像质量下降。因此亟需结合物理噪声机制的新方法以提升低剂量下的成像质量。

Method: 作者提出PC-UNet网络，并设计PVMC-Loss损失函数，将泊松噪声的物理性质纳入训练过程，通过统计上无偏的方差和梯度自适应，实现物理一致性的去噪训练。

Result: 在实际PET数据集上的实验证实，PC-UNet能有效提升图像的物理一致性和保真度，优于现有传统方法。

Conclusion: PC-UNet结合了物理噪声建模，显著提升低剂量PET成像效果，验证了物理信息融入深度学习在医学影像降噪领域的有效性。

Abstract: Positron Emission Tomography (PET) is crucial in medicine, but its clinical
use is limited due to high signal-to-noise ratio doses increasing radiation
exposure. Lowering doses increases Poisson noise, which current denoising
methods fail to handle, causing distortions and artifacts. We propose a Poisson
Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean
Consistency Loss (PVMC-Loss) that incorporates physical data to improve image
fidelity. PVMC-Loss is statistically unbiased in variance and gradient
adaptation, acting as a Generalized Method of Moments implementation, offering
robustness to minor data mismatches. Tests on PET datasets show PC-UNet
improves physical consistency and image fidelity, proving its ability to
integrate physical information effectively.

</details>


### [3] [DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.15015)
*Mor Ventura,Michael Toker,Or Patashnik,Yonatan Belinkov,Roi Reichart*

Main category: cs.CV

TL;DR: 本文提出DeLeaker方法，通过在推理时直接干预注意力图，降低文本到图像模型中的语义泄漏问题，并构建了首个专用于评测语义泄漏的数据集SLIM。实验表明，DeLeaker在不牺牲图像质量的情况下有效缓解了语义泄漏现象，效果优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型的发展，语义泄漏（即不同实体间特征的无意传递）成为模型生成中的突出问题，现有方法多依赖优化或额外输入，操作复杂且效果有限。为此，亟需更高效、通用的缓解策略。

Method: 提出DeLeaker，在扩散生成过程中动态重加权注意力图，抑制不同实体间的过度交互，突出各实体身份，无需额外优化或外部信息。同时发布SLIM数据集和自动化评价框架，用以系统评估语义泄漏。

Result: DeLeaker能够在不降低图像质量和生成保真度的前提下，有效抑制语义泄漏，并且在多场景下性能超越依赖外部信息的现有方法。

Conclusion: 直接控制注意力机制能够有效缓解T2I中的语义泄漏，推动生成模型向更高语义精度发展。SLIM数据集的发布也为社区带来统一评测标准。

Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.

</details>


### [4] [UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos](https://arxiv.org/abs/2510.15018)
*Mingxuan Liu,Honglin He,Elisa Ricci,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一个名为UrbanVerse的系统，可以将城市旅游视频自动转换为高保真、可物理交互的仿真城市场景，从而极大地提升训练城市内智能体的效率和真实性。


<details>
  <summary>Details</summary>
Motivation: 当前城市智能体（如送货机器人、四足机器人等）的仿真训练受限于现有环境的可扩展性与真实性。传统手工或程序生成的城市场景难以全面反映现实世界的复杂性，因此需要更自动化、真实感更强的仿真训练平台。

Method: 提出UrbanVerse系统，包括：1）建立UrbanVerse-100K，大型真实城市3D资产库，含语义与物理标签；2）UrbanVerse-Gen自动化流程，从群众采集的视频中提取场景布局，调用资产库生成物理感知的3D仿真场景；3）依托IsaacSim仿真平台实现高质量、多样化的城市模拟。

Result: UrbanVerse目前已生成来自24个国家的160个高质量城市场景，并有10个艺术家设计的测试基准。实验表明，UrbanVerse仿真场景在现实语义和布局上可媲美人工制作的场景。城市导航任务中，使用UrbanVerse训练的策略具有更好的泛化和扩展能力，在仿真和零样本迁移到实际场景中分别提升成功率6.3%和30.1%。

Conclusion: UrbanVerse极大提升了城市智能体仿真训练的真实性和可扩展性，为后续研究和大规模智能体部署奠定了坚实基础。

Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.

</details>


### [5] [NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks](https://arxiv.org/abs/2510.15019)
*Junliang Ye,Shenghao Xie,Ruowen Zhao,Zhengyi Wang,Hongyu Yan,Wenqiang Zu,Lei Ma,Jun Zhu*

Main category: cs.CV

TL;DR: Nano3D提出了一种无需训练、精确且一致的3D对象编辑新方法，并且首次推出大规模3D编辑数据集，提升了3D编辑的通用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前3D对象编辑方法存在效率低、编辑一致性差、未编辑区域容易被破坏等问题，常依赖多视角重建流程，易引入伪影并影响实际应用。

Method: Nano3D是一个无需训练的框架，将FlowEdit集成到TRELLIS中，通过正面渲染引导，实现局部、掩码无关的3D编辑；提出了Voxel/Slat-Merge区域感知合并策略，自适应保持编辑与未编辑区域的一致性和结构完整性。

Result: 实验表明，Nano3D在3D一致性和视觉质量上优于现有方法。同时，基于此框架，作者构建了包含10万余组高质量3D编辑对的大型数据集Nano3D-Edit-100k。

Conclusion: Nano3D有效解决了3D编辑中的算法设计和数据存储难题，提升了3D编辑的通用性、可靠性，为后续更高效的端到端3D编辑模型奠定了基础。

Abstract: 3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized
edits guided by front-view renderings, and further introduces region-aware
merging strategies, Voxel/Slat-Merge, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior 3D consistency and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of feed-forward 3D editing models. Project
Page:https://jamesyjl.github.io/Nano3D

</details>


### [6] [Constantly Improving Image Models Need Constantly Improving Benchmarks](https://arxiv.org/abs/2510.15021)
*Jiaxin Ge,Grace Luo,Heekyung Lee,Nishant Malpani,Long Lian,XuDong Wang,Aleksander Holynski,Trevor Darrell,Sewon Min,David M. Chan*

Main category: cs.CV

TL;DR: 本文提出了ECHO，一个基于社交媒体真实用例构建的图像生成模型评测新框架，有效弥补了现有基准测试与最新模型实际表现间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型的基准测试更新滞后，无法及时反映模型新能力和真实用例，导致业界评估与社区实际体验脱节。因此，需要一种更贴近实际使用场景的评测方法。

Method: 作者提出ECHO，通过收集和整理社交媒体上用户展示新颖提示词（prompts）和主观评价的帖子，构建了包含3.1万条提示词的新数据集，并据此评估GPT-4o Image Gen等模型。

Result: ECHO方法能够发现现有基准测试中没有覆盖的创新复杂任务，更好地区分主流模型表现，并基于社区反馈用于设计新的评价指标。

Conclusion: ECHO框架为图像生成模型的评测带来了更高的代表性和实用性，有助于缩小模型进展与实际应用评价之间的差距。

Abstract: Recent advances in image generation, often driven by proprietary systems like
GPT-4o Image Gen, regularly introduce new capabilities that reshape how users
interact with these models. Existing benchmarks often lag behind and fail to
capture these emerging use cases, leaving a gap between community perceptions
of progress and formal evaluation. To address this, we present ECHO, a
framework for constructing benchmarks directly from real-world evidence of
model use: social media posts that showcase novel prompts and qualitative user
judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset
of over 31,000 prompts curated from such posts. Our analysis shows that ECHO
(1) discovers creative and complex tasks absent from existing benchmarks, such
as re-rendering product labels across languages or generating receipts with
specified totals, (2) more clearly distinguishes state-of-the-art models from
alternatives, and (3) surfaces community feedback that we use to inform the
design of metrics for model quality (e.g., measuring observed shifts in color,
identity, and structure). Our website is at https://echo-bench.github.io.

</details>


### [7] [LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://arxiv.org/abs/2510.15022)
*Mert Sonmezer,Matthew Zheng,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的子模数框架，用于在大量 LoRA 微调适配器中自动选择最相关且多样的模型。实验表明，该方法能在多个领域生成多样化的输出。


<details>
  <summary>Details</summary>
Motivation: 当前已有超过10万个 LoRA 适配器，虽极大丰富了模型个性化能力，但由于数量庞大、分类混乱，用户难以高效筛选和有效利用合适的模型。因此，自动化、高效地在众多 LoRA 适配器中选择相关且多样的模型成为亟需解决的问题。

Method: 作者将 LoRA 模型选择问题视为组合优化问题，提出了一种基于子模数函数的新方法，对适配器进行高效去冗余、多样性选择。方法既考虑了相关性也兼顾多样性，通过定量和定性实验进行评估。

Result: 方法在实验中实现了跨领域的多样性输出，优于传统选择策略。同时，实验展示了选择结果与实际生成内容的多样化和相关性提升。

Conclusion: 本文的子模数选择方法能有效提升 LoRA 适配器库的应用效率和生成结果的多样性，对于实际大规模微调模型管理和内容生成有重要意义。

Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of
pre-trained diffusion models by enabling fine-tuning through low-rank,
factorized weight matrices specifically optimized for attention layers. These
models facilitate the generation of highly customized content across a variety
of objects, individuals, and artistic styles without the need for extensive
retraining. Despite the availability of over 100K LoRA adapters on platforms
like Civit.ai, users often face challenges in navigating, selecting, and
effectively utilizing the most suitable adapters due to their sheer volume,
diversity, and lack of structured organization. This paper addresses the
problem of selecting the most relevant and diverse LoRA models from this vast
database by framing the task as a combinatorial optimization problem and
proposing a novel submodular framework. Our quantitative and qualitative
experiments demonstrate that our method generates diverse outputs across a wide
range of domains.

</details>


### [8] [MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://arxiv.org/abs/2510.15026)
*Mattia Segu,Marta Tintore Gazulla,Yongqin Xian,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: 论文提出了MOBIUS，一种为实例分割设计的高效基础模型家族，可在保证性能的同时显著降低训练与推理计算量，适用于移动端和高端硬件。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型虽然在实例级感知任务上表现优异，但高昂的计算成本限制了其在资源受限设备（如移动端）的部署。作者希望解决高效部署与性能之间的平衡问题。

Method: 提出MOBIUS模型家族，采取三项新方法：（1）高效的瓶颈像素解码器实现多尺度多模态特征融合；（2）语言引导的不确定性校准损失用于自适应裁剪解码器；（3）统一精简的训练策略。同时设计模型以实现Pareto最优下采样，适应不同设备资源。

Result: MOBIUS相比高效基线方法，在保持最先进性能的同时，将像素及Transformer解码器的FLOPs分别降低至55%和25%，并在仅三分之一训练轮次下达成效果；在高性能和移动设备上都树立了新的高效分割基准。

Conclusion: MOBIUS实现了实例分割领域在效率与性能上的新突破，为基础模型在不同计算平台的推广部署带来积极影响。

Abstract: Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.

</details>


### [9] [Composition-Grounded Instruction Synthesis for Visual Reasoning](https://arxiv.org/abs/2510.15040)
*Xinyi Gu,Jiayuan Mao,Zhang-Wei Hong,Zhuoran Yu,Pengyuan Li,Dhiraj Joshi,Rogerio Feris,Zexue He*

Main category: cs.CV

TL;DR: 本文提出COGS方法，通过分解少量种子问题并与新图像系统性重组，生成多模态推理数据，从而有效提升多模态大模型在人工图像领域（如图表、网页等）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型MLLMs的推理能力受限，尤其是在难以收集人工标注的领域（如图表、渲染文档等），缺乏大规模推理数据，阻碍了其能力的提升与泛化。

Method: 提出COGS框架，将少量种子问题分解为基础感知和推理因子。这些因子与新图片系统性重组，生成大量合成问答对，每个问题配有子问题及中间答案，支持基于过程的强化学习。

Result: 在图表推理任务上，COGS显著提升了模型对未知问题的解答能力，特别是在推理复杂和组合性强的问题上，性能提升最大。此外，因子级多样种子数据训练能在多数据集间更好迁移，显示方法具备通用性。

Conclusion: COGS框架不仅能提升多模态模型在图表等人工图像领域的推理性能，而且具备良好的迁移性，对网页等其它领域同样有效，为多模态推理任务提供了数据高效、可推广的解决方案。

Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.

</details>


### [10] [Generalized Dynamics Generation towards Scannable Physical World Model](https://arxiv.org/abs/2510.15041)
*Yichen Li,Zhiyi Li,Brandon Feng,Dinghuai Zhang,Antonio Torralba*

Main category: cs.CV

TL;DR: 本文提出了GDGen框架，以统一建模刚体、关节体和软体动力学，通过潜在能量视角处理多种物理行为，在多样虚拟环境下实现高泛化和稳定的物理模拟。


<details>
  <summary>Details</summary>
Motivation: 在数字孪生世界和虚拟环境日益复杂的趋势下，亟需一种方法能够统一不同类型物理体的建模与交互，以支持多样化的机器人和虚拟体训练。

Method: 作者提出了GDGen框架，将物理系统的稳定性视为低潜能态，扩展了传统弹性动力学，通过引入方向性刚度参数，利用专门的神经网络和神经场来进行材质建模和几何无关的形变表达，从而统一处理不同动力体系。

Result: 实验表明，GDGen能够在各种动力学模拟场景下实现准确而一致的模拟，大幅提升了不同物理系统的建模和交互效率。

Conclusion: GDGen为构建高度互动、动态丰富的虚拟环境和训练通用机器人智能体提供了有效且统一的物理基础。

Abstract: Digital twin worlds with realistic interactive dynamics presents a new
opportunity to develop generalist embodied agents in scannable environments
with complex physical behaviors. To this end, we present GDGen (Generalized
Representation for Generalized Dynamics Generation), a framework that takes a
potential energy perspective to seamlessly integrate rigid body, articulated
body, and soft body dynamics into a unified, geometry-agnostic system. GDGen
operates from the governing principle that the potential energy for any stable
physical system should be low. This fresh perspective allows us to treat the
world as one holistic entity and infer underlying physical properties from
simple motion observations. We extend classic elastodynamics by introducing
directional stiffness to capture a broad spectrum of physical behaviors,
covering soft elastic, articulated, and rigid body systems. We propose a
specialized network to model the extended material property and employ a neural
field to represent deformation in a geometry-agnostic manner. Extensive
experiments demonstrate that GDGen robustly unifies diverse simulation
paradigms, offering a versatile foundation for creating interactive virtual
environments and training robotic agents in complex, dynamically rich
scenarios.

</details>


### [11] [Comprehensive language-image pre-training for 3D medical image understanding](https://arxiv.org/abs/2510.15042)
*Tassilo Wald,Ibrahim Ethem Hamamci,Yuan Gao,Sam Bond-Taylor,Harshita Sharma,Maximilian Ilse,Cynthia Lo,Olesya Melnichenko,Noel C. F. Codella,Maria Teodora Wetscherek,Klaus H. Maier-Hein,Panagiotis Korfiatis,Valentina Salvatelli,Javier Alvarez-Valle,Fernando Pérez-García*

Main category: cs.CV

TL;DR: 本文提出了一种利用额外归纳偏置（如报告生成目标和视觉预训练）来增强3D医学影像领域视觉语言编码器（VLE）的方法，并提出了COLIPRI编码器，显著提升了报告生成、分类和零样本分类等任务表现。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像领域的视觉-语言预训练（VLP）受限于配对影像与文本数据的稀缺，限制了VLE在检索和异常预测等实际应用中的潜力，亟需通过引入新方法拓展可用数据。

Method: 本文提出将报告生成目标引入训练，并结合视觉-语言预训练与视觉自监督预训练，使模型能够同时利用配对影像文本和仅影像数据，从而扩大训练数据总量。此外，综合了3D医学影像领域的最佳实践，形成COLIPRI编码器系列。

Result: COLIPRI编码器在报告生成、分类探测及零样本分类等任务上取得了最新最优性能，在语义分割任务上也保持了较强竞争力。

Conclusion: 引入报告生成目标及融合多源数据的预训练策略，能够有效缓解数据不足瓶颈，显著提升3D医学影像VLE的多项下游任务表现。

Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.

</details>


### [12] [Directional Reasoning Injection for Fine-Tuning MLLMs](https://arxiv.org/abs/2510.15050)
*Chao Huang,Zeliang Zhang,Jiang Liu,Ximeng Sun,Jialian Wu,Xiaodong Yu,Ze Wang,Chenliang Xu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 现有多模态大语言模型（MLLMs）推理能力落后于强大的文本模型，现有提升方法成本高。本文提出一种轻量级推理迁移方法DRIFT，在多模态微调时提升推理能力，且优于简单模型合并与传统微调。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型推理能力不足，而现有依赖大规模有监督数据或强化学习的方法资源消耗大。模型参数插值虽简单，但对不同模型家族效果不一。

Method: 提出DRIFT方法：先计算推理模型与多模态模型参数的差值，作为推理先验。在多模态微调中，用此先验方向引导梯度，实现高效推理能力迁移，且不会破坏原有多模态对齐能力。

Result: 多项多模态推理基准（如MathVista和MathVerse）实验显示，DRIFT在多模态推理任务上均优于直接模型合并和标准微调方法，效果与资源消耗大的高阶方法相当或更优，但计算成本更低。

Conclusion: DRIFT为提升多模态大模型推理能力提供了一种高效且简单可行的解决方案，在兼顾性能和资源消耗方面具有显著优势。

Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.

</details>


### [13] [A solution to generalized learning from small training sets found in everyday infant experiences](https://arxiv.org/abs/2510.15060)
*Frangil Ramirez,Elizabeth Clerkin,David J. Crandall,Linda B. Smith*

Main category: cs.CV

TL;DR: 该研究提出婴儿日常视觉体验的统计特性，即“聚类性”，解释了婴儿如何通过有限体验实现对物体类别的泛化。通过分析婴儿视角图片和机器学习实验，表明这种经验结构可促进泛化和高效学习。


<details>
  <summary>Details</summary>
Motivation: 尽管幼儿能够灵活地识别并泛化带有普通名词标签的视觉对象，但这些基本类别的形成机制尚不清楚。作者试图揭示婴儿在有限的视觉体验下，如何高效进行类别泛化。

Method: 1. 分析了14名7-11个月大婴儿的自我视角图片，统计其日常视觉输入结构；2. 识别输入集中存在“聚类”现象，即部分图像高度相似，而其他则更为稀疏和多样；3. 在模拟这种结构的机器学习实验中，评估聚类特性对小数据集泛化性能的影响。

Result: 实验显示，婴儿的视觉输入确实表现出聚类性结构，将该结构应用于机器学习算法后，能有效提升模型在小样本数据集上的泛化能力。

Conclusion: 婴儿经验中的“聚类性”视觉输入有助于早期类别学习和泛化，这一现象为高效学习原理以及不同学习体的设计和理解提供了新思路。

Abstract: Young children readily recognize and generalize visual objects labeled by
common nouns, suggesting that these basic level object categories may be given.
Yet if they are, how they arise remains unclear. We propose that the answer
lies in the statistics of infant daily life visual experiences. Whereas large
and diverse datasets typically support robust learning and generalization in
human and machine learning, infants achieve this generalization from limited
experiences. We suggest that the resolution of this apparent contradiction lies
in the visual diversity of daily life, repeated experiences with single object
instances. Analyzing egocentric images from 14 infants (aged 7 to 11 months) we
show that their everyday visual input exhibits a lumpy similarity structure,
with clusters of highly similar images interspersed with rarer, more variable
ones, across eight early-learned categories. Computational experiments show
that mimicking this structure in machines improves generalization from small
datasets in machine learning. The natural lumpiness of infant experience may
thus support early category learning and generalization and, more broadly,
offer principles for efficient learning across a variety of problems and kinds
of learners.

</details>


### [14] [SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images](https://arxiv.org/abs/2510.15072)
*Jiaxin Guo,Tongfan Guan,Wenzhen Dong,Wenzhao Zheng,Wenting Wang,Yue Wang,Yeung Yam,Yun-Hui Liu*

Main category: cs.CV

TL;DR: 提出SaLon3R框架，实现结构感知、长期泛化的3D高斯重建，有效移除冗余并提升几何一致性，支持在线高效处理超50视角的长序列。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯重建方法在处理长视频时，产生大量冗余和几何不一致问题，且效率低，难以实时高质量重建。

Method: SaLon3R以结构感知为核心，先通过3D重建主干提取密集高斯及区域复杂性显著性图，利用可微显著性高斯量化机制合并冗余高斯为稠密锚点，然后通过3D Point Transformer学习空间结构先验，细化锚点属性与显著性，实现自适应高斯解码，无须相机参数与测试时优化。

Result: 在多数据集上，SaLon3R在新视角合成和深度估计任务上表现出更优的效率、鲁棒性和泛化能力，支持10 FPS以上的高效在线重建，同时可移除50%~90%的冗余。

Conclusion: SaLon3R大幅提升了长期3DGS重建的效率与质量，特别适合无需已知相机参数、追求泛化和高效的实际应用场景。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.

</details>


### [15] [TGT: Text-Grounded Trajectories for Locally Controlled Video Generation](https://arxiv.org/abs/2510.15104)
*Guofeng Zhang,Angtian Wang,Jacob Zhiyuan Fang,Liming Jiang,Haotian Yang,Bo Liu,Yiding Yang,Guang Chen,Longyin Wen,Alan Yuille,Chongyang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本轨迹（TGT）的文本生成视频方法，使生成视频在多目标和复杂场景下具有更强的精确性与可控性，可通过点轨迹和文本描述实现对外观及运动的直观控制。


<details>
  <summary>Details</summary>
Motivation: 传统文本生成视频方法在多目标和复杂场景下，对场景中主体的控制能力有限，现有的基于区域信号（如bounding box或mask）方法在复杂场景控制精度低、目标数量增多时关联不明确，因此亟需更细粒度和直接的控制方式。

Method: 提出Text-Grounded Trajectories (TGT) 框架，核心包括：1）用带有局部文本描述的点轨迹来作为生成视频的条件；2）设计了Location-Aware Cross-Attention (LACA)机制，将轨迹与文本信息有效结合；3）采用双重CFG方案，分别调控局部和全局文本引导；4）开发数据处理流程，分离并生成有局部描述的目标轨迹，并注释了200万高质量视频用于训练。

Result: 实验结果显示，TGT方法相较于现有方法，在视觉质量、文本对齐度及运动可控性方面均取得显著提升。

Conclusion: TGT为文本到视频生成任务提供了更强的细粒度控制和高度可控的方法，尤其在复杂多目标场景下表现优异，推动了文本生成视频技术的进步。

Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.

</details>


### [16] [Deep generative priors for 3D brain analysis](https://arxiv.org/abs/2510.15119)
*Ana Lawry Aguila,Dina Zemlyanker,You Cheng,Sudeshna Das,Daniel C. Alexander,Oula Puonti,Annabel Sorby-Adams,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了利用扩散模型作为正则项，解决医学成像逆问题的新方法，尤其针对脑部MRI数据，在无需配对训练集的情况下实现了高质量成像任务。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型已成为强大的医学影像生成工具，但如何融入领域知识、提升对实际成像过程的建模，仍然是难题。传统贝叶斯逆问题虽可结合成像领域知识，但其解剖结构建模能力有限，难以充分表达大脑解剖的复杂性。

Method: 作者提出使用基于扩散模型的得分先验，将其广泛训练于多样化的脑MRI数据，并结合灵活的前向模型，解决如超分辨率、强度偏置校正、修补等多种图像处理任务。此外，该方法支持对其他深度学习方法输出的修正，提升解剖结构复现度。

Result: 在多种实际临床和科学脑MRI数据上，该方法无需成对训练集即可实现一致且高质量的成像表现，超越了现有方法。

Conclusion: 基于扩散模型的先验为医学脑影像逆问题提供了强大且通用的工具，对提升MRI分析结果具有显著意义和应用潜力。

Abstract: Diffusion models have recently emerged as powerful generative models in
medical imaging. However, it remains a major challenge to combine these
data-driven models with domain knowledge to guide brain imaging problems. In
neuroimaging, Bayesian inverse problems have long provided a successful
framework for inference tasks, where incorporating domain knowledge of the
imaging process enables robust performance without requiring extensive training
data. However, the anatomical modeling component of these approaches typically
relies on classical mathematical priors that often fail to capture the complex
structure of brain anatomy. In this work, we present the first general-purpose
application of diffusion models as priors for solving a wide range of medical
imaging inverse problems. Our approach leverages a score-based diffusion prior
trained extensively on diverse brain MRI data, paired with flexible forward
models that capture common image processing tasks such as super-resolution,
bias field correction, inpainting, and combinations thereof. We further
demonstrate how our framework can refine outputs from existing deep learning
methods to improve anatomical fidelity. Experiments on heterogeneous clinical
and research MRI data show that our method achieves state-of-the-art
performance producing consistent, high-quality solutions without requiring
paired training datasets. These results highlight the potential of diffusion
priors as versatile tools for brain MRI analysis.

</details>


### [17] [Fourier Transform Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2510.15138)
*Anthony Bilic,Guangyu Sun,Ming Li,Md Sanzid Bin Hossain,Yu Tian,Wei Zhang,Laura Brattain,Dexter Hadley,Chen Chen*

Main category: cs.CV

TL;DR: 研究提出了一种基于傅里叶变换的多实例学习（FFT-MIL）方法，通过引入频域特征来增强全景图像分类中的全局依赖建模，显著提升诊断预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例学习（MIL）方法主要依赖于空间patch特征，但由于WSI的超大尺寸和局部patch嵌入的局限性，难以有效捕捉全局依赖关系，影响了对粗结构的建模，从而制约了图像的诊断预测性能。

Method: 提出了FFT-MIL框架，在传统MIL方法中引入频域分支。方法包括：提取WSI的低频信息，使用快速傅里叶变换（FFT）；通过包含卷积层和Min-Max归一化的FFT-Block处理高方差的频率数据；将学到的全局频域特征与空间patch特征融合，并采用轻量级集成策略，实现与各类MIL架构的兼容。

Result: 在BRACS、LUAD、IMP三个公开数据集、六种先进MIL方法上评估，集成FFT-Block后宏F1分数平均提升3.51%，AUC提升1.51%，在不同架构和数据集上均取得了一致的效果提升。

Conclusion: 频域学习能有效高效地捕捉WSI分类中的全局依赖，在补充空间特征的同时，推动了基于MIL的数字病理学的可扩展性和准确率。

Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning
(MIL) with spatial patch features, yet existing methods struggle to capture
global dependencies due to the immense size of WSIs and the local nature of
patch embeddings. This limitation hinders the modeling of coarse structures
essential for robust diagnostic prediction.
  We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a
framework that augments MIL with a frequency-domain branch to provide compact
global context. Low-frequency crops are extracted from WSIs via the Fast
Fourier Transform and processed through a modular FFT-Block composed of
convolutional layers and Min-Max normalization to mitigate the high variance of
frequency data. The learned global frequency feature is fused with spatial
patch features through lightweight integration strategies, enabling
compatibility with diverse MIL architectures.
  FFT-MIL was evaluated across six state-of-the-art MIL methods on three public
datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1
scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains
across architectures and datasets. These results establish frequency-domain
learning as an effective and efficient mechanism for capturing global
dependencies in WSI classification, complementing spatial features and
advancing the scalability and accuracy of MIL-based computational pathology.

</details>


### [18] [XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models](https://arxiv.org/abs/2510.15148)
*Xingrui Wang,Jiang Liu,Chao Huang,Xiaodong Yu,Ze Wang,Ximeng Sun,Jialian Wu,Alan Yuille,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了一个大规模三模态基准XModBench，用于系统性评估和诊断通用型多模态大语言模型（OLLMs）的模态不变性推理能力，发现现有模型存在明显的模态偏差和推理薄弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然当前OLLMS试图融合音频、视觉和文本的理解，但缺乏细致诊断其是否真正具备模态不变性推理能力，也未能确定模型是否存在模态偏差。现有评测主要聚焦于一般的跨模态问答能力，难以深入揭示细致的推理能力和潜在问题。

Method: 作者提出XModBench基准，包含60,828个多选题，覆盖5大任务类型，涵盖音频、视觉、文本6种所有可能的问答模态组合，可以对OLLMS进行跨模态一致性、模态差异和方向性不平衡的细粒度诊断。

Result: 实验发现，即使是最强的Gemini 2.5 Pro模型，也在空间与时间推理方面表现不佳，准确率低于60%；同一语义通过音频而非文本表现性能明显下降，存在模态差异；以视觉为上下文时，模型一致性也较弱，呈现出系统性的方向性不平衡。

Conclusion: 现阶段OLLMS距离真正的模态不变性推理还有很大差距，XModBench为多模态能力的诊断和改进提供了关键工具，有助于推动跨模态推理研究发展。

Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.

</details>


### [19] [Train a Unified Multimodal Data Quality Classifier with Synthetic Data](https://arxiv.org/abs/2510.15162)
*Weizhi Wang,Rongmei Lin,Shiyang Li,Colin Lockard,Ritesh Sarkhel,Sanket Lokegaonkar,Jingbo Shang,Xifeng Yan,Nasser Zalmout,Xian Li*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的多模态数据过滤器UniFilter，可筛选高质量图文数据，从而提升多模态大模型的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在混合的图文数据和交错文档数据上预训练，但对高质量、交错的图文数据的筛选研究较少。高质量数据对于提升模型能力极为重要，因此需要更有效的过滤方法。

Method: 提出了统一多模态数据质量分类器UniFilter，能同时筛选出高质量图文描述与交错文档数据。创新地采用半合成方法，利用原始图片并自动生成不同质量等级的文本，迅速构建训练样本对，训练分类器后应用于实际大规模多模态数据的筛选。

Result: 经过UniFilter筛选的数据用于多模态大模型预训练后，模型在零样本推理和上下文学习能力上提升显著。进一步视觉有监督微调后，在多个基准测试中表现更优。

Conclusion: UniFilter可以有效提升多模态大模型预训练数据的质量，并带来下游任务性能的提升。作者公开了合成训练数据、UniFilter模型及高质量数据集，推动相关领域发展。

Abstract: The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

</details>


### [20] [Hyperparameter Optimization and Reproducibility in Deep Learning Model Training](https://arxiv.org/abs/2510.15164)
*Usman Afzaal,Ziyu Su,Usama Sajjad,Hao Lu,Mostafa Rezapour,Metin Nafi Gurcan,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 本论文系统性研究了基础模型在病理图像中的训练可复现性问题，并提出了提高复现性的具体实验规则。


<details>
  <summary>Details</summary>
Motivation: 在数字病理学领域，基础模型的训练面临可复现性不足的问题，软件随机性、硬件非确定性和超参数报告不一致等因素加剧了这一挑战。作者希望明确影响复现性的关键因素，并为社区提供改进方案。

Method: 作者采用CLIP模型在QUILT-1M数据集上进行训练，并在三个下游病理数据集（PatchCamelyon, LC25000-Lung, LC25000-Colon）上，系统评估了不同超参数设置与数据增强策略对性能的影响。重点分析了RandomResizedCrop参数、分布式训练方式及学习率选择等因素。

Result: 实验发现，当RandomResizedCrop参数在0.7-0.8区间时模型性能最佳；分布式训练中不使用局部损失可提升稳定性；学习率过低（<5.0e-5）会导致所有数据集上性能下降；LC25000(Colon)数据集则表现出良好的复现性。

Conclusion: 可复现性不仅依赖于实验报告透明，还需合理选择关键的实验配置。作者提出了切实可行的实践准则，以指导数字病理学领域后续的基础模型可复现性研究。

Abstract: Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.

</details>


### [21] [Salient Concept-Aware Generative Data Augmentation](https://arxiv.org/abs/2510.15194)
*Tianchen Zhao,Xuanbai Chen,Zhihua Li,Jun Fang,Dongsheng An,Xiang Xu,Zhuowen Tu,Yifan Xing*

Main category: cs.CV

TL;DR: 提出了一种新颖的个性化图像生成方法，可根据图像和文本双重条件进行高质量数据增强，显著提升下游分类模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前图文条件生成的数据增强方法难以兼顾图像细节保真与多样性，尤其在图像和文本提示的对齐方面存在冲突，影响了模型的泛化和鲁棒性。

Method: 提出了一种引入显著概念感知图像嵌入模型的个性化图像生成框架，通过降低图像中无关细节对合成结果的影响，实现图像与文本输入的更好对齐，同时生成既保留类别判别特征又具备受控变化的新样本，用于增强训练集。

Result: 在8个细粒度视觉数据集上进行实验，方法在常规和长尾设置下的分类准确率平均分别提升0.73%和6.5%，优于现有最先进数据增强方法。

Conclusion: 该方法能更有效提升训练数据多样性和下游模型鲁棒性，是图文条件数据增强任务的有力工具。

Abstract: Recent generative data augmentation methods conditioned on both image and
text prompts struggle to balance between fidelity and diversity, as it is
challenging to preserve essential image details while aligning with varied text
prompts. This challenge arises because representations in the synthesis process
often become entangled with non-essential input image attributes such as
environmental contexts, creating conflicts with text prompts intended to modify
these elements. To address this, we propose a personalized image generation
framework that uses a salient concept-aware image embedding model to reduce the
influence of irrelevant visual details during the synthesis process, thereby
maintaining intuitive alignment between image and text inputs. By generating
images that better preserve class-discriminative features with additional
controlled variations, our framework effectively enhances the diversity of
training datasets and thereby improves the robustness of downstream models. Our
approach demonstrates superior performance across eight fine-grained vision
datasets, outperforming state-of-the-art augmentation methods with averaged
classification accuracy improvements by 0.73% and 6.5% under conventional and
long-tail settings, respectively.

</details>


### [22] [CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records](https://arxiv.org/abs/2510.15208)
*Daniela Vega,Hannah V. Ceballos,Javier S. Vera,Santiago Rodriguez,Alejandra Perez,Angela Castillo,Maria Escobar,Dario Londoño,Luis A. Sarmiento,Camila I. Castro,Nadiezhda Rodriguez,Juan C. Briceño,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文介绍了一种新的公开多模态数据集CARDIUM，用于先天性心脏病（CHDs）产前诊断，并提出融合成像与临床数据的多模态Transformer方法，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病产前AI诊断因数据稀缺且不平衡、数据质量差而效果受限，目前缺乏融合影像和临床信息的公开多模态数据集。为促进AI辅助诊断，需要高质量、多源数据及创新融合方法。

Method: 1）构建并发布首个包含胎儿超声、超声心动图和母亲临床记录的公开多模态CARDIUM数据集。2）提出结合跨模态注意力机制的多模态Transformer网络，将影像特征与表格临床信息高效融合用于CHD检测。

Result: 多模态方法对比单一影像（提升11%）和单一表格（提升50%）模式显著提升检测准确性，在CARDIUM数据集上取得F1分数79.8±4.8%。

Conclusion: CARDIUM为产前CHD诊断提供首个多模态公开基准，有助于推动融合AI模型和相关领域进一步研究，数据和代码均已开放获取。

Abstract: Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/

</details>


### [23] [The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads](https://arxiv.org/abs/2510.15240)
*Aysan Aghazadeh,Adriana Kovashka*

Main category: cs.CV

TL;DR: 本文探索了文本转图片模型在定制化广告中的使用及其引发的人口统计偏见与说服性差异问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图片的生成模型为广告内容定制和受众精准定位带来新机遇，但这些模型可能带有对不同性别和种族群体的偏见，影响广告效果公正性。

Method: 作者分析不同广告主题下生成广告的人口统计学偏见，并对仅人物性别或种族不同但其他内容完全一致的广告进行说服力模型评分对比。同时，还尝试了广告按国家定向推送技术。代码开源。

Result: 发现广告内容在性别和种族表现上存在系统性偏见，并且这些偏见会影响模型评判广告说服力的水平。此外，针对不同国家定向生成广告的可行性得到一定探索。

Conclusion: 文本转图片技术虽可提升广告定制能力，但须警惕模型带来的社会偏见问题。未来需更关注公平性和定向性，以实现道德且高效的广告投放。

Abstract: Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion

</details>


### [24] [DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion](https://arxiv.org/abs/2510.15264)
*Weijie Wang,Jiagang Zhu,Zeyu Zhang,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Chaojun Ni,Haoxiao Wang,Guan Huang,Xinze Chen,Yukun Zhou,Wenkang Qin,Duochao Shi,Haoyun Li,Guanghong Jia,Jiwen Lu*

Main category: cs.CV

TL;DR: DriveGen3D是一个高效、可控的动态3D驾驶场景生成框架，能实时生成高质量视频及动态3D场景，解决了现有方法的计算量大、功能受限等问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景合成方法存在3D建模弱、仅能生成视频或静态场景、计算成本高等限制，难以同时实现高质量、长时、动态、多模态控制的3D驾驶场景合成。

Method: 提出了DriveGen3D框架，包括FastDrive-DiT（一个高效视频扩散Transformer，利用文本和鸟瞰图引导生成高分辨率、时序连续的视频）和FastRecon3D（一个前馈重建模块，快速实现时空一致性的3D高斯表达）。

Result: 能够实时生成高分辨率（424×800，12 FPS）长时间驾驶视频及对应动态3D场景，并在新视角合成上实现了0.811的SSIM和22.84的PSNR，具备参数高效性。

Conclusion: DriveGen3D实现了高质量、高效率且高度可控的动态3D驾驶场景生成，有效弥补了以往方法在时空一致性、效率和可控性上的不足。

Abstract: We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.

</details>


### [25] [CuSfM: CUDA-Accelerated Structure-from-Motion](https://arxiv.org/abs/2510.15271)
*Jingrui Yu,Jun Liu,Kefei Ren,Joydeep Biswas,Rurui Ye,Keqiang Wu,Chirag Majithia,Di Zeng*

Main category: cs.CV

TL;DR: 本文提出了cuSfM系统，一个基于CUDA的GPU加速离线SFM（结构光恢复运动）方法，实现了高效、精确的相机位姿估计与全局一致建图，并作为开源工具发布。


<details>
  <summary>Details</summary>
Motivation: 现有的SfM方法在面对密集重建时，计算开销大且效率较低，尤其是在精确相机位姿估计和大规模建图任务中。因此，需要一种能够充分利用离线大算力资源，同时提升速度和精度的新方法。

Method: 提出了cuSfM离线SFM系统，通过GPU并行加速计算，能够高效处理复杂的特征提取与数据关联，实现相机姿态优化、建图、先验地图定位和外参优化。该系统支持充分离线利用计算资源来获得最佳精度。

Result: 实验结果显示，cuSfM在多个测试场景下，相较于主流的COLMAP方法，显著提升了精度和处理速度，且保证了离线SfM应用所需的高精度与全局一致性。

Conclusion: cuSfM作为高效精确的离线SfM方案，不仅提升了建图效率和准确性，并且以开源形式发布，促进了计算机视觉与机器人领域的研究和应用。

Abstract: Efficient and accurate camera pose estimation forms the foundational
requirement for dense reconstruction in autonomous navigation, robotic
perception, and virtual simulation systems. This paper addresses the challenge
via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that
leverages GPU parallelization to efficiently employ computationally intensive
yet highly accurate feature extractors, generating comprehensive and
non-redundant data associations for precise camera pose estimation and globally
consistent mapping. The system supports pose optimization, mapping, prior-map
localization, and extrinsic refinement. It is designed for offline processing,
where computational resources can be fully utilized to maximize accuracy.
Experimental results demonstrate that cuSfM achieves significantly improved
accuracy and processing speed compared to the widely used COLMAP method across
various testing scenarios, while maintaining the high precision and global
consistency essential for offline SfM applications. The system is released as
an open-source Python wrapper implementation, PyCuSfM, available at
https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and
applications in computer vision and robotics.

</details>


### [26] [Post-Processing Methods for Improving Accuracy in MRI Inpainting](https://arxiv.org/abs/2510.15282)
*Nishad Kulkarni,Krithika Iyer,Austin Tapp,Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,María J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru*

Main category: cs.CV

TL;DR: 论文提出了一种结合模型集成与高效后处理策略的MRI脑部损伤部位修复方法，并通过系统评估证明该方案提升了解剖结构逼真度和视觉质量，可为临床推广和可持续研究带来便利。


<details>
  <summary>Details</summary>
Motivation: 自动化MRI分析工具多针对健康脑部优化，遇到大体积病变（如肿瘤）时表现不佳，需要在有病变区域实现高质量影像修复以确保常规工具可靠适用。

Method: 系统评估了当前主流修复模型并发现其独立性能已饱和，随后提出结合模型集成与中值滤波、直方图匹配、像素均值等后处理策略的方法，并引入轻量级U-Net模型对修复区域作解剖学细化。

Result: 综合评估表明，所提方案在修复区域的解剖合理性和视觉一致性方面优于单一模型，准确性和健壮性均有提升。

Conclusion: 集成现有修复模型并针对性后处理可提升MRI脑部损伤修复质量，为更广泛的临床应用和资源节约型研究提供技术基础。

Abstract: Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the
diagnosis, assessment, and treatment planning for brain pathologies. However,
most automated MRI analysis tools, such as segmentation and registration
pipelines, are optimized for healthy anatomies and often fail when confronted
with large lesions such as tumors. To overcome this, image inpainting
techniques aim to locally synthesize healthy brain tissues in tumor regions,
enabling the reliable application of general-purpose tools. In this work, we
systematically evaluate state-of-the-art inpainting models and observe a
saturation in their standalone performance. In response, we introduce a
methodology combining model ensembling with efficient post-processing
strategies such as median filtering, histogram matching, and pixel averaging.
Further anatomical refinement is achieved via a lightweight U-Net enhancement
stage. Comprehensive evaluation demonstrates that our proposed pipeline
improves the anatomical plausibility and visual fidelity of inpainted regions,
yielding higher accuracy and more robust outcomes than individual baseline
models. By combining established models with targeted post-processing, we
achieve improved and more accessible inpainting outcomes, supporting broader
clinical deployment and sustainable, resource-conscious research. Our 2025
BraTS inpainting docker is available at
https://hub.docker.com/layers/aparida12/brats2025/inpt.

</details>


### [27] [QCFace: Image Quality Control for boosting Face Representation & Recognition](https://arxiv.org/abs/2510.15289)
*Duc-Phuong Doan-Ngo,Thanh-Dang Diep,Thanh Nguyen-Duc,Thanh-Sach LE,Nam Thoai*

Main category: cs.CV

TL;DR: 提出了一种新的硬边界人脸识别损失方法QCFace，可显著提升特征的可识别性，并在主流识别任务中取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统在特征表达时通常难以有效利用可识别性，现有方法对低质量或模糊人脸的判别力不足，且特征方向与幅值的梯度耦合影响优化效果，导致可识别性与身份表达难以区分。

Method: 提出硬边界策略QCFace，通过新的损失函数引入指导因子，分离特征可识别性与身份特征，解决梯度耦合与特征混淆问题，实现更有效的超球面特征规划。

Result: QCFace能稳健且可量化地编码可识别性，在主流人脸验证和识别基准上均优于现有相关损失函数。

Conclusion: QCFace为人脸识别的特征可识别性建模提供了新思路，有效缓解了现有方法的不足，并在实际应用中表现优异。

Abstract: Recognizability, a key perceptual factor in human face processing, strongly
affects the performance of face recognition (FR) systems in both verification
and identification tasks. Effectively using recognizability to enhance feature
representation remains challenging. In deep FR, the loss function plays a
crucial role in shaping how features are embedded. However, current methods
have two main drawbacks: (i) recognizability is only partially captured through
soft margin constraints, resulting in weaker quality representation and lower
discrimination, especially for low-quality or ambiguous faces; (ii) mutual
overlapping gradients between feature direction and magnitude introduce
undesirable interactions during optimization, causing instability and confusion
in hypersphere planning, which may result in poor generalization, and entangled
representations where recognizability and identity are not cleanly separated.
To address these issues, we introduce a hard margin strategy - Quality Control
Face (QCFace), which overcomes the mutual overlapping gradient problem and
enables the clear decoupling of recognizability from identity representation.
Based on this strategy, a novel hard-margin-based loss function employs a
guidance factor for hypersphere planning, simultaneously optimizing for
recognition ability and explicit recognizability representation. Extensive
experiments confirm that QCFace not only provides robust and quantifiable
recognizability encoding but also achieves state-of-the-art performance in both
verification and identification benchmarks compared to existing
recognizability-based losses.

</details>


### [28] [Exploring Conditions for Diffusion models in Robotic Control](https://arxiv.org/abs/2510.15510)
*Heeseong Shin,Byeongho Heo,Dongyoon Han,Seungryong Kim,Taekyung Kim*

Main category: cs.CV

TL;DR: 本论文提出ORCA方法，利用预训练扩散模型，通过任务自适应的方式改进视觉表征，实现了在机器人控制任务上的新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前利用预训练视觉模型进行模仿学习时，这些模型在策略学习时通常保持冻结，无法针对具体任务进行适应性调整；而现有将文本条件融入视觉任务的做法，在机器人控制中效果有限，主要因扩散模型训练数据与实际任务间存在域间差异。

Method: 提出ORCA框架，引入可学习任务提示（task prompts）和视觉提示（visual prompts），分别用于适应控制任务的环境和捕获精细、帧级的视觉细节，无需对原有扩散模型进行微调。

Result: 在多个机器人控制基准测试中，ORCA极大优于以往方法，取得了新SOTA成绩。

Conclusion: 通过引入任务自适应的提示机制，能更好地利用扩散模型的视觉表征提升机器人控制性能，验证了新的条件设计的有效性。

Abstract: While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.

</details>


### [29] [Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning](https://arxiv.org/abs/2510.15296)
*Yiming Lin,Shang Wang,Junkai Zhou,Qiufeng Wang,Xiao-Bo Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 本文提出了首个基于双曲空间的单正多标签学习（SPMLL）分类框架，将每个标签表示为双曲球，实现了多种标签关系的几何建模，并在多项实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有SPMLL方法难以显式、几何地建模标签间复杂关系及层次结构，仅依赖距离式相似性，缺乏对标签包含、共现、独立等关系的直观表达。

Method: 作者提出将标签表示为双曲空间中的球体。球体之间的包含、交叠和分离分别映射为层次、共现和独立等语义关系。设计了温度自适应的分类器与受物理启发的双阱正则项，引导球体分布更具意义。

Result: 在MS-COCO、PASCAL VOC、NUS-WIDE、CUB-200-2011四个基准数据集上进行实验证明，所提方法在性能和标签关系解释性方面均超过了现有方法。

Conclusion: 证明了在不完全监督下，双曲几何为结构化分类提供了更强健、更具解释性的建模范式，学习到的嵌入与现实的标签共现高度相关。

Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging
scenario where each training sample is annotated with only one positive label
despite potentially belonging to multiple categories, making it difficult to
capture complex label relationships and hierarchical structures. While existing
methods implicitly model label relationships through distance-based similarity,
lacking explicit geometric definitions for different relationship types. To
address these limitations, we propose the first hyperbolic classification
framework for SPMLL that represents each label as a hyperbolic ball rather than
a point or vector, enabling rich inter-label relationship modeling through
geometric ball interactions. Our ball-based approach naturally captures
multiple relationship types simultaneously: inclusion for hierarchical
structures, overlap for co-occurrence patterns, and separation for semantic
independence. Further, we introduce two key component innovations: a
temperature-adaptive hyperbolic ball classifier and a physics-inspired
double-well regularization that guides balls toward meaningful configurations.
To validate our approach, extensive experiments on four benchmark datasets
(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive
performance with superior interpretability compared to existing methods.
Furthermore, statistical analysis reveals strong correlation between learned
embeddings and real-world co-occurrence patterns, establishing hyperbolic
geometry as a more robust paradigm for structured classification under
incomplete supervision.

</details>


### [30] [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/abs/2510.15301)
*Minglei Shi,Haolin Wang,Wenzhao Zheng,Ziyang Yuan,Xiaoshi Wu,Xintao Wang,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出SVG，一种不依赖VAE的新型潜空间扩散模型，通过自监督学习特征提升视觉生成质量及效率，克服了传统VAE+扩散方法在效率、泛化等诸多不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于VAE的潜空间扩散模型虽然生成效果好，但训练慢、推理慢、泛化能力弱，这主要由于VAE潜空间缺乏显著的语义区分和判别结构，限制了模型训练和应用。作者因此希望突破VAE潜空间的本质缺陷，提升扩散模型在视觉任务中的表现。

Method: SVG采用无VAE方案，直接借助自监督学习得到的语义清晰、判别性强的DINO特征作为生成潜空间，同时通过一个轻量级残差分支细化高保真细节重建，扩散过程直接在该结构良好的潜空间上训练和采样。

Result: SVG实现了更高效的扩散模型训练，支持更少的采样步数，生成质量提升，且保持自监督特征的判别和语义能力。实验结果验证了SVG在多项指标上的优越性。

Conclusion: SVG为构建高质量、任务通用的视觉潜空间提供了新范式，弥补了VAE-扩散组合的结构与效率短板，推动了视觉生成和表示学习统一发展。

Abstract: Recent progress in diffusion-based visual generation has largely relied on
latent diffusion models with variational autoencoders (VAEs). While effective
for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
training efficiency, slow inference, and poor transferability to broader vision
tasks. These issues stem from a key limitation of VAE latent spaces: the lack
of clear semantic separation and strong discriminative structure. Our analysis
confirms that these properties are crucial not only for perception and
understanding tasks, but also for the stable and efficient training of latent
diffusion models. Motivated by this insight, we introduce SVG, a novel latent
diffusion model without variational autoencoders, which leverages
self-supervised representations for visual generation. SVG constructs a feature
space with clear semantic discriminability by leveraging frozen DINO features,
while a lightweight residual branch captures fine-grained details for
high-fidelity reconstruction. Diffusion models are trained directly on this
semantically structured latent space to facilitate more efficient learning. As
a result, SVG enables accelerated diffusion training, supports few-step
sampling, and improves generative quality. Experimental results further show
that SVG preserves the semantic and discriminative capabilities of the
underlying self-supervised representations, providing a principled pathway
toward task-general, high-quality visual representations.

</details>


### [31] [Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](https://arxiv.org/abs/2510.15304)
*Fei Wang,Li Shen,Liang Ding,Chao Xue,Ye Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的大语言模型剪枝方法CoMe，有效降低模型规模同时保留性能，实验显示该方法可在大幅度参数减少下维持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型在性能上表现优异，但其庞大的模型规模带来了高昂算力和存储需求。现有的分层剪枝方法为缩小模型规模提供了一定方案，但常常忽略了被剪枝部分能力的保留，导致模型性能显著下降。因此，有必要探索更高效且损失更小的层级剪枝新范式。

Method: 论文提出CoMe方法，包含三个关键步骤：1）定义基于激活强度和权重范数的通道敏感性度量，实现细粒度通道选择；2）采用基于拼接的层合并技术，将相邻层中最关键的通道融合，逐步缩小模型规模；3）提出分层蒸馏协议，利用剪枝过程中已建立的原模型与精简模型间的层级对应关系，实现高效知识迁移与性能恢复。

Result: 在七个基准测试上，CoMe方法取得了当前最优结果。例如，对LLaMA-2-7b剪去30%参数后，仍能保持原始平均准确率的83%。

Conclusion: CoMe有效突破了现有结构化剪枝方法的瓶颈，显著提升了剪枝后模型的性能恢复效果，为大语言模型规模化应用提供了高效可行的新技术路径。

Abstract: Large Language Models excel at natural language processing tasks, but their
massive size leads to high computational and storage demands. Recent works have
sought to reduce their model size through layer-wise structured pruning.
However, they tend to ignore retaining the capabilities in the pruned part. In
this work, we re-examine structured pruning paradigms and uncover several key
limitations: 1) notable performance degradation due to direct layer removal, 2)
incompetent linear weight layer aggregation, and 3) the lack of effective
post-training recovery mechanisms. To address these limitations, we propose
CoMe, including a progressive layer pruning framework with a
Concatenation-based Merging technology and a hierarchical distillation
post-training process. Specifically, we introduce a channel sensitivity metric
that utilizes activation intensity and weight norms for fine-grained channel
selection. Subsequently, we employ a concatenation-based layer merging method
to fuse the most critical channels across adjacent layers, enabling progressive
model size reduction. Finally, we propose a hierarchical distillation protocol
that leverages the correspondences between the original and pruned model layers
established during pruning, thereby enabling efficient knowledge transfer.
Experiments on seven benchmarks show that CoMe achieves state-of-the-art
performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
retains 83% of its original average accuracy. Our code is available at
https://github.com/MPI-Lab/CoMe.

</details>


### [32] [Proto-Former: Unified Facial Landmark Detection by Prototype Transformer](https://arxiv.org/abs/2510.15338)
*Shengkai Hu,Haozhe Qi,Jun Wan,Jiaxing Huang,Lefei Zhang,Hang Sun,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出了一种统一且自适应的端到端人脸关键点检测框架Proto-Former，能够多数据集联合训练，并在主流基准数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前人脸关键点检测数据集标注的特征点数量不一致，且大多数方法仅能在单一数据集训练，造成模型泛化能力不足，难以实现跨数据集统一建模。

Method: 提出了Proto-Former框架，包含自适应原型感知编码器（APAE）和渐进式原型感知解码器（PPAD），能够进行适应性特征提取、原型表示学习及引导模型关注关键区域。同时创新地设计了原型感知损失（PA loss），优化多数据集训练时的专家选择和梯度冲突。

Result: 在多个主流人脸关键点检测基准数据集上，Proto-Former在性能上超越了现有的最新方法。

Conclusion: Proto-Former为解决多数据集人脸关键点统一检测提供了一种有效方案，实现了更高的检测精度和更强的泛化能力。

Abstract: Recent advances in deep learning have significantly improved facial landmark
detection. However, existing facial landmark detection datasets often define
different numbers of landmarks, and most mainstream methods can only be trained
on a single dataset. This limits the model generalization to different datasets
and hinders the development of a unified model. To address this issue, we
propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection
framework that explicitly enhances dataset-specific facial structural
representations (i.e., prototype). Proto-Former overcomes the limitations of
single-dataset training by enabling joint training across multiple datasets
within a unified architecture. Specifically, Proto-Former comprises two key
components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive
feature extraction and learns prototype representations, and a Progressive
Prototype-Aware Decoder (PPAD) that refines these prototypes to generate
prompts that guide the model's attention to key facial regions. Furthermore, we
introduce a novel Prototype-Aware (PA) loss, which achieves optimal path
finding by constraining the selection weights of prototype experts. This loss
function effectively resolves the problem of prototype expert addressing
instability during multi-dataset training, alleviates gradient conflicts, and
enables the extraction of more accurate facial structure features. Extensive
experiments on widely used benchmark datasets demonstrate that our Proto-Former
achieves superior performance compared to existing state-of-the-art methods.
The code is publicly available at: https://github.com/Husk021118/Proto-Former.

</details>


### [33] [SHARE: Scene-Human Aligned Reconstruction](https://arxiv.org/abs/2510.15342)
*Joshua Li,Brendan Chharawala,Chang Shu,Xue Bin Peng,Pengcheng Xi*

Main category: cs.CV

TL;DR: 本文提出了一种名为SHARE的新方法，专注于提升单目RGB视频下3D人体动作与场景的精确重建，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将人精确放置于3D空间、实现人物与场景的真实互动方面存在挑战，这限制了游戏、AR/VR和机器人等领域的应用需求。

Method: 提出了SHARE技术，充分利用场景几何自带的空间信息，仅通过单一固定摄像头的RGB视频完成重建。先对每帧估算人体网格及分割掩码，在关键帧提取场景点云，并通过人体点云与场景点云之间的匹配，迭代优化关键帧中的人体位置。同时对非关键帧采用约束，保持其与关键帧根关节点的位置一致性，从而保证运动连贯。

Result: SHARE方法能更加精确地在场景中还原人体的3D动作与位置，且能够适应多种类型数据集与真实环境下的视频。实验表明，SHARE在精度等指标上明显优于已有对比方法。

Conclusion: SHARE为单目视频下的人体与场景3D重建提供了更好的解决方案，提升了人物互动的真实感，拓展了在多种应用场景中的实用性。

Abstract: Animating realistic character interactions with the surrounding environment
is important for autonomous agents in gaming, AR/VR, and robotics. However,
current methods for human motion reconstruction struggle with accurately
placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
(SHARE), a technique that leverages the scene geometry's inherent spatial cues
to accurately ground human motion reconstruction. Each reconstruction relies
solely on a monocular RGB video from a stationary camera. SHARE first estimates
a human mesh and segmentation mask for every frame, alongside a scene point map
at keyframes. It iteratively refines the human's positions at these keyframes
by comparing the human mesh against the human point map extracted from the
scene using the mask. Crucially, we also ensure that non-keyframe human meshes
remain consistent by preserving their relative root joint positions to keyframe
root joints during optimization. Our approach enables more accurate 3D human
placement while reconstructing the surrounding scene, facilitating use cases on
both curated datasets and in-the-wild web videos. Extensive experiments
demonstrate that SHARE outperforms existing methods.

</details>


### [34] [Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding](https://arxiv.org/abs/2510.15371)
*Shuntaro Suzuki,Shunya Nagashima,Masayuki Hirata,Komei Sugiura*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度神经网络架构（Cortical-SSM），用于更好地对运动想象（MI）期间采集的脑电（EEG）和皮层脑电（ECoG）信号进行解读和分类，从而提升信号解读的准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 运动想象相关的脑电信号在辅助交流和康复等临床领域有重要应用。但这些信号极易受到生理伪影（如眨眼、吞咽等）干扰，并且现有Transformer方法难以捕捉信号中精细的依赖关系。为了解决这些挑战，作者提出新的方法。

Method: 提出了Cortical-SSM架构，将深度状态空间模型拓展，用以同时捕捉EEG和ECoG信号在时间、空间和频域多维的依赖关系。方法在2个包含50多名受试者的大型公开MI EEG数据集和1个渐冻症患者MI ECoG临床数据集上进行了验证。

Result: Cortical-SSM在3个不同基准数据集上的分类表现均优于现有主流方法。此外，模型产生的可视化说明也表明其有效捕捉到了与神经生理密切相关的信号区域。

Conclusion: Cortical-SSM能有效提升EEG和ECoG信号的解读和分类能力，对于推动脑-机接口等实际应用具有积极意义。

Abstract: Classification of electroencephalogram (EEG) and electrocorticogram (ECoG)
signals obtained during motor imagery (MI) has substantial application
potential, including for communication assistance and rehabilitation support
for patients with motor impairments. These signals remain inherently
susceptible to physiological artifacts (e.g., eye blinking, swallowing), which
pose persistent challenges. Although Transformer-based approaches for
classifying EEG and ECoG signals have been widely adopted, they often struggle
to capture fine-grained dependencies within them. To overcome these
limitations, we propose Cortical-SSM, a novel architecture that extends deep
state space models to capture integrated dependencies of EEG and ECoG signals
across temporal, spatial, and frequency domains. We validated our method across
three benchmarks: 1) two large-scale public MI EEG datasets containing more
than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient
with amyotrophic lateral sclerosis. Our method outperformed baseline methods on
the three benchmarks. Furthermore, visual explanations derived from our model
indicate that it effectively captures neurophysiologically relevant regions of
both EEG and ECoG signals.

</details>


### [35] [Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning](https://arxiv.org/abs/2510.15372)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的分阶段自适应微调方法，提高了微创手术中外科工具检测的自动化水平，显著提升了检测性能且具备更广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 临床手术环境下标注数据稀缺，制约了深度学习在自动化外科工具检测中的应用。为解决这一问题，需要设计高效且能有效适应外科领域的小样本学习方案。

Method: 方法分为两个阶段：（1）在线性探测阶段，将额外的分类层与预训练CNN架构进行适配；（2）逐步冻结阶段，动态减少可微调层，以调节对新领域的适应速度。该法只需单次训练循环，无需多次迭代，降低了网络复杂度并提升了效率。

Result: 在Cholec80数据集上的实验表明，所提方法使用ImageNet预训练的ResNet-50和DenseNet-121架构，外科工具检测的平均精度达到96.4%，优于现有同类方法。并在CATARACTS数据集上验证了方法的泛化能力。

Conclusion: 渐进冻结微调策略不仅有效提升了多种手术场景下的工具存在检测性能，还有望推广至更广泛的图像分类任务中。

Abstract: Minimally invasive surgery can benefit significantly from automated surgical
tool detection, enabling advanced analysis and assistance. However, the limited
availability of annotated data in surgical settings poses a challenge for
training robust deep learning models. This paper introduces a novel staged
adaptive fine-tuning approach consisting of two steps: a linear probing stage
to condition additional classification layers on a pre-trained CNN-based
architecture and a gradual freezing stage to dynamically reduce the
fine-tunable layers, aiming to regulate adaptation to the surgical domain. This
strategy reduces network complexity and improves efficiency, requiring only a
single training loop and eliminating the need for multiple iterations. We
validated our method on the Cholec80 dataset, employing CNN architectures
(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical
tools in cholecystectomy endoscopic videos. Our results demonstrate that our
method improves detection performance compared to existing approaches and
established fine-tuning techniques, achieving a mean average precision (mAP) of
96.4%. To assess its broader applicability, the generalizability of the
fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct
domain of minimally invasive ophthalmic surgery. These findings suggest that
gradual freezing fine-tuning is a promising technique for improving tool
presence detection in diverse surgical procedures and may have broader
applications in general image classification tasks.

</details>


### [36] [FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://arxiv.org/abs/2510.15385)
*Haisheng Su,Junjie Zhang,Feixiang Song,Sanping Zhou,Wei Wu,Nanning Zheng,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D目标检测方法，能够更准确地从多视角2D图像中检测三维目标，通过多种创新模块提升了深度预测和空间建模的效果。


<details>
  <summary>Details</summary>
Motivation: 多视角2D图像到3D目标检测对自动驾驶非常关键，但当前方法在深度预测上存在边界不连续、小目标不清晰等问题，且忽略了多视角一致性和尺度不变性。作者希望解决这些问题，提升检测精度。

Method: 提出Frequency-aware Positional Depth Embedding (FreqPDE)方法，包括三个核心模块：1）Frequency-aware Spatial Pyramid Encoder（FSPE）融合图像中不同频率信息提取空间特征；2）Cross-view Scale-invariant Depth Predictor（CSDP）用跨视角和通道注意力机制实现像素级深度分布预测；3）Positional Depth Encoder（PDE）将2D图像特征与3D位置编码结合用于3D信息解码。同时，引入混合深度监督机制提升深度学习表现。

Result: 在nuScenes数据集上做了大量实验，结果表明该方法在3D目标检测任务上效果优于现有主流方法。

Conclusion: FreqPDE方法通过更高质量的深度预测与空间建模，为3D目标检测任务带来了更好的表现，具有实际应用前景，尤其适用于自动驾驶等多视角三维理解场景。

Abstract: Detecting 3D objects accurately from multi-view 2D images is a challenging
yet essential task in the field of autonomous driving. Current methods resort
to integrating depth prediction to recover the spatial information for object
query decoding, which necessitates explicit supervision from LiDAR points
during the training phase. However, the predicted depth quality is still
unsatisfactory such as depth discontinuity of object boundaries and
indistinction of small objects, which are mainly caused by the sparse
supervision of projected points and the use of high-level image features for
depth prediction. Besides, cross-view consistency and scale invariance are also
overlooked in previous methods. In this paper, we introduce Frequency-aware
Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
information for 3D detection transformer decoder, which can be obtained through
three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
(FSPE) constructs a feature pyramid by combining high-frequency edge clues and
low-frequency semantics from different levels respectively. Then the Cross-view
Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
distribution with cross-view and efficient channel attention mechanism.
Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
3D position embeddings to generate the 3D depth-aware features for query
decoding. Additionally, hybrid depth supervision is adopted for complementary
depth learning from both metric and distribution aspects. Extensive experiments
conducted on the nuScenes dataset demonstrate the effectiveness and superiority
of our proposed method.

</details>


### [37] [PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction](https://arxiv.org/abs/2510.15386)
*Ting-Yu Yen,Yu-Sheng Chiu,Shih-Hsuan Hung,Peter Wonka,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PFGS的三维高斯散射（3DGS）新方法，通过融合多姿态图像，有效提升了3D重建的完整性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法普遍只适用于单一、静止姿态的对象，导致对遮挡区域的重建不完整，限制了实际应用。

Method: PFGS框架通过将主姿态和多个辅助姿态图像融合，采用结合全局与局部配准的策略来实现姿态感知的重建，并结合新近的3D基础模型，利用背景特征和交叉姿态配准提升注册性能。

Result: 实验显示，PFGS在主观和客观评估中均明显优于现有方法，不仅重建更完整，还提升了3DGS模型的真实性和细节还原度。

Conclusion: PFGS有效解决了多姿态下3DGS重建完整性不足的问题，为高质量三维重建提供了更实用和高效的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.

</details>


### [38] [LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding](https://arxiv.org/abs/2510.15392)
*Peng Ren,Hai Yang*

Main category: cs.CV

TL;DR: 本文提出LILAC方法，实现实时、长序列且风格多样的人体动作生成。通过采用基于VAE扩散的潜在空间流式架构，弥补了以往高质量离线方法无法在线推理的缺陷。实验证明该方法在主流数据集上兼具高质量与低延迟优势。


<details>
  <summary>Details</summary>
Motivation: 现有的实时动作生成方法通常直接在原始动作空间中操作，计算开销大且难以保持时间稳定性；而采用VAE-Diffusion的潜在空间方法虽能改善这个问题，但多仅支持离线处理。这在需要连续、响应的人物控制应用中无法满足需求。

Method: LILAC基于高性能离线风格化动作生成框架，创新性地提出了一种流式潜在空间架构。具体方式为采用滑动窗口与因果解码、插入解码后特征，保证长序列动作在无未来帧信息的前提下能平滑转换并实时输出，无需更改已有的扩散模型结构。

Result: 在主流数据集上的实验表明，LILAC能够实现高质量风格化动作生成，同时满足实时性和响应性，兼具低延迟与动作平滑的优点。

Conclusion: LILAC为实时动作生成任务带来了高质量与高效率兼得的新方案，无需修改原有扩散架构，同时克服了多种现有技术的重大不足，为相关应用提供了有力基石。

Abstract: Generating long and stylized human motions in real time is critical for
applications that demand continuous and responsive character control. Despite
its importance, existing streaming approaches often operate directly in the raw
motion space, leading to substantial computational overhead and making it
difficult to maintain temporal stability. In contrast, latent-space
VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality
stylization, but they are generally confined to offline processing. To bridge
this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion
Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a
recent high-performing offline framework for arbitrary motion stylization and
extends it to an online setting through a latent-space streaming architecture
with a sliding-window causal design and the injection of decoded motion
features to ensure smooth motion transitions. This architecture enables
long-sequence real-time arbitrary stylization without relying on future frames
or modifying the diffusion model architecture, achieving a favorable balance
between stylization quality and responsiveness as demonstrated by experiments
on benchmark datasets. Supplementary video and examples are available at the
project page: https://pren1.github.io/lilac/

</details>


### [39] [MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment](https://arxiv.org/abs/2510.15398)
*Bingyu Li,Feiyu Wang,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了MARIS数据集并基于该数据集设计了一种新的框架，显著提升了水下开放词汇实例分割能力，尤其在识别未知类别方面优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割领域大多局限于封闭的已知类别，难以识别新的海洋生物类别。由于视觉退化和语义不匹配，这一问题在水下场景中尤为突出。为推动该领域发展，需要新的基准数据集和方法。

Method: 本工作推出了MARIS，这是首个水下开放词汇实例分割的大规模细粒度基准数据集，包括有限的已知和丰富的未知类别。方法上，提出了含有几何先验增强模块（GPEM）和语义对齐注入机制（SAIM）的统一框架。GPEM利用物体结构和部件几何信息，提升视觉退化场景下的物体一致性；SAIM则通过引入领域相关的先验知识，优化语言特征表达，减轻语义歧义。

Result: 实验证明，所提出的框架在MARIS数据集的同域及跨域测试下，相较现有开放词汇分割方法均有显著提升。

Conclusion: 该工作为水下开放词汇实例分割领域带来了新的数据集和方法，在促进未知类别识别等方面奠定了坚实基础，对未来水下感知研究具有重要推动作用。

Abstract: Most existing underwater instance segmentation approaches are constrained by
close-vocabulary prediction, limiting their ability to recognize novel marine
categories. To support evaluation, we introduce \textbf{MARIS}
(\underline{Mar}ine Open-Vocabulary \underline{I}nstance
\underline{S}egmentation), the first large-scale fine-grained benchmark for
underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen
categories and diverse unseen categories. Although OV segmentation has shown
promise on natural images, our analysis reveals that transfer to underwater
scenes suffers from severe visual degradation (e.g., color attenuation) and
semantic misalignment caused by lack underwater class definitions. To address
these issues, we propose a unified framework with two complementary components.
The Geometric Prior Enhancement Module (\textbf{GPEM}) leverages stable
part-level and structural cues to maintain object consistency under degraded
visual conditions. The Semantic Alignment Injection Mechanism (\textbf{SAIM})
enriches language embeddings with domain-specific priors, mitigating semantic
ambiguity and improving recognition of unseen categories. Experiments show that
our framework consistently outperforms existing OV baselines both In-Domain and
Cross-Domain setting on MARIS, establishing a strong foundation for future
underwater perception research.

</details>


### [40] [Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning](https://arxiv.org/abs/2510.15400)
*Chen Qian,Haoyu Zhang,Junnan Ma,Liuhong Zhu,Qingrui Cai,Yu Wang,Ruibo Song,Lv Li,Lin Mei,Xianwang Jiang,Qin Xu,Boyu Jiang,Ran Tao,Chunmiao Chen,Shufang Chen,Dongyun Liang,Qiu Guo,Jianzhong Lin,Taishan Kang,Mengtian Lu,Liyuan Fu,Ruibin Huang,Huijuan Wan,Xu Huang,Jianhua Wang,Di Guo,Hai Zhong,Jianjun Zhou,Xiaobo Qu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的重建框架LoSP-Prompt, 利用物理建模与合成数据驱动的prompt学习, 有效解决多部位肿瘤多次扫描DWI的运动伪影等难题，大幅提升图像分辨率与质量，在多中心多设备多部位取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 多次扫描DWI在身体肿瘤诊断中的应用受限于呼吸、蠕动等运动诱发的相位伪影，且多器官、多切片、多方向、多b值的复杂性进一步加剧问题。亟需一种通用、鲁棒且自动化的解决方法以实现高分辨率、多部位的DWI临床推广。

Method: 提出LoSP-Prompt重建框架，将多次扫描间的相位变化建模为高阶局部平滑相位(LoSP)，整合进低秩Hankel矩阵重建。通过仅用合成腹部DWI训练的prompt学习自动设置重建秩参数，无需引导信号和真实数据监督。

Result: 基于10,000多张临床图像跨43受试者、4种扫描仪、5家中心广泛验证。LoSP-Prompt空间分辨率为单次扫描DWI两倍，肝脏病灶清晰度显著提升；无需调参与训练即可适应7种不同部位；11位放射科医师主观评分全面优于现有方法，部分器官DWI评分高达4-5分（优秀）。

Conclusion: LoSP-Prompt为高分辨率、多器官、多次扫描DWI提供了无需引导信号、无需真实数据训练、具有解释性的自动解决方案，实现设备无关的优异性能，有望在精准肿瘤诊断领域带来变革。

Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging
(multi-shot DWI) for body-wide tumor diagnostics is limited by severe
motion-induced phase artifacts from respiration, peristalsis, and so on,
compounded by multi-organ, multi-slice, multi-direction and multi-b-value
complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that
overcomes these challenges through physics-informed modeling and
synthetic-data-driven prompt learning. We model inter-shot phase variations as
a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel
matrix reconstruction. Crucially, the algorithm's rank parameter is
automatically set via prompt learning trained exclusively on synthetic
abdominal DWI data emulating physiological motion. Validated across 10,000+
clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)
Achieved twice the spatial resolution of clinical single-shot DWI, enhancing
liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions
(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single
model; (3) Outperformed state-of-the-art methods in image quality, artifact
suppression, and noise reduction (11 radiologists' evaluations on a 5-point
scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points
(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points
(good) on knee and tumor brain. The approach eliminates navigator signals and
realistic data supervision, providing an interpretable, robust solution for
high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance
signifies transformative potential for precision oncology.

</details>


### [41] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](https://arxiv.org/abs/2510.15430)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CV

TL;DR: 本文提出了一个通用的检测框架（LoD），可以更准确高效地检测大型视觉-语言模型（LVLMs）遭遇的未知jailbreak攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多对齐和防御工作，但LVLMs依然容易受到jailbreak攻击，现有检测方法对于未知攻击泛化能力差或准确性/效率有限，需要更好的通用检测方法。

Method: 提出了Learning to Detect (LoD) 框架，核心包括：1）多模态安全概念激活向量模块，用于安全导向的表示学习；2）安全模式自动编码器模块，进行无监督的攻击分类，从以攻击为中心转向以任务为中心的学习。

Result: 大量实验表明，LoD方法在各种未知攻击上取得了更高的检测AUROC，同时提升了检测效率。

Conclusion: LoD 提供了对LVLM未知jailbreak攻击检测的普适、高效、准确的解决方案，优于现有检测方法。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.

</details>


### [42] [Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety](https://arxiv.org/abs/2510.15434)
*Huan Chen,Ting Han,Siyu Chen,Zhihao Guo,Yiping Chen,Meiliu Wu*

Main category: cs.CV

TL;DR: 本文提出了基于街景图像（SVI）的交通事故风险评估新方法，利用语义分割自动提取街道特征，并结合机器学习与因果推断，揭示了不同类型事故的街道因素影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于街景图像进行交通安全分析存在两个难题：一是如何构建能反映事故风险的可解释街道特征；二是如何量化这些特征对不同事故类型的因果影响。本文针对上述问题提出方法。

Method: 设计了Semantic4Safety框架，运用零样本语义分割技术，从SVI中自动提取11种街道景观特征，并结合道路类型信息。随后，利用XGBoost多分类器进行事故类型预测，并用SHAP解释特征贡献。为了量化因果效应，结合GPS加权与ATE估算，排除混杂因素，分析不同特征对事故类型的真实影响。

Result: 分析奥斯汀市约3万起事故数据后发现，不同类型事故的影响因素存在异质性。场景复杂度、暴露度和道路几何是主要预测变量。更大可通行空间和应急空间有助于降低风险，而“视觉开放度”过大则反而可能增加风险。

Conclusion: 通过将预测建模与因果推断结合，Semantic4Safety不仅能够诊断高风险街道，还能为城市道路安全干预提供数据支持，实现可扩展、精细化的道路安全规划。

Abstract: Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.

</details>


### [43] [Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation](https://arxiv.org/abs/2510.15439)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Dayong Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种新的分割网络PCMambaNet，通过预测-修正范式（PC paradigm）大幅提高了高分辨率脑MRI分割的效率和精度，实现了在极少训练轮数（1-5个epoch）下即达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习虽然取得了很大成功，但在样本数据稀缺领域（如医学影像）常因收敛慢和过度依赖大规模数据而效率低下。作者希望通过引入新的架构和范式，提升学习速度和效果，尤其是在医疗等数据不足领域。

Method: 作者提出PC范式，将任务分为预测和修正两步。具体地，PCMambaNet包含两个模块：1）预测先验模块（PPM），利用双侧对称解剖知识以低计算成本生成粗略的焦点区域图（focus map）；2）修正残差网络（CRN），专注在难点区域上进一步精细分割，提高病灶边界精度。

Result: 在高分辨率脑MRI分割实验上，PCMambaNet不仅达到了目前最优的分割精度，而且用极少的训练epoch（1-5轮）即可收敛，远快于传统端到端模型。

Conclusion: PCMambaNet通过显式引入领域知识简化学习目标，显著提升了数据利用率，减少了过拟合，并且极大加快了模型训练收敛速度，适合数据稀缺医学影像领域应用。

Abstract: Despite the remarkable success of the end-to-end paradigm in deep learning,
it often suffers from slow convergence and heavy reliance on large-scale
datasets, which fundamentally limits its efficiency and applicability in
data-scarce domains such as medical imaging. In this work, we introduce the
Predictive-Corrective (PC) paradigm, a framework that decouples the modeling
task to fundamentally accelerate learning. Building upon this paradigm, we
propose a novel network, termed PCMambaNet. PCMambaNet is composed of two
synergistic modules. First, the Predictive Prior Module (PPM) generates a
coarse approximation at low computational cost, thereby anchoring the search
space. Specifically, the PPM leverages anatomical knowledge-bilateral
symmetry-to predict a 'focus map' of diagnostically relevant asymmetric
regions. Next, the Corrective Residual Network (CRN) learns to model the
residual error, focusing the network's full capacity on refining these
challenging regions and delineating precise pathological boundaries. Extensive
experiments on high-resolution brain MRI segmentation demonstrate that
PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5
epochs-a performance unattainable by conventional end-to-end models. This
dramatic acceleration highlights that by explicitly incorporating domain
knowledge to simplify the learning objective, PCMambaNet effectively mitigates
data inefficiency and overfitting.

</details>


### [44] [Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning](https://arxiv.org/abs/2510.15440)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频大模型推理方法，通过强化学习动态选择和重采样关键证据帧，在五个基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 长视频推理中，现有方法采用静态均匀采样，导致关键信息被稀释。此外，现有主动视频推理方法缺乏严密的奖励机制以保证推理证据的纯净性，也无法超越预采样帧进行时序补充，限制了推理效果。

Method: 提出了以“少选多推”为核心理念的证据优先自适应推理框架。具体实现为证据感知强化学习（EARL），使模型能动态选取最相关的帧，并对关键帧进行局部重采样，获得细粒度的时序信息，提高推理准确度和证据纯度。

Result: EARL方法在五个高难度视频推理基准测试中表现突出。其开源7B参数模型在LongVideoBench（59.8%）、MVBench（69.0%）和VideoMME（64.9%）等主流评测上刷新了最优纪录。

Conclusion: 优先选择高纯度证据并结合自适应采样，是提升长视频推理准确性和效率的关键。EARL框架为视频大语言模型推理提供了新的有效策略。

Abstract: Long-form video reasoning remains a major challenge for Video Large Language
Models (Video LLMs), as static uniform frame sampling leads to information
dilution and obscures critical evidence. Furthermore, existing pixel-space
video reasoning agents, which are designed to actively interact with the video
to acquire new visual information, remain suboptimal due to their lack of
rigorous reward mechanisms to enforce evidence purity and their inability to
perform temporal information supplementation beyond pre-sampled frames. To
address this critical gap, we propose a novel evidence-prioritized adaptive
framework built upon our core philosophy: "Select Less, Reason More." Our core
contribution is the evidence-aware reinforcement learning (EARL) framework,
which transforms the model into an active interrogator of evidence. EARL is
precisely engineered to dynamically select the most relevant frames and,
crucially, to perform localized re-sampling around the selected key frames to
access fine-grained temporal detail. Extensive experiments on five demanding
video reasoning benchmarks demonstrate that our EARL-trained model achieves new
state-of-the-art among open-source Video LLMs, simultaneously learning an
effective and high-purity visual evidence selection policy. Impressively, our
7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
VideoMME. These results highlight the importance of prioritizing evidence
purity and the effectiveness of our framework.

</details>


### [45] [MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention](https://arxiv.org/abs/2510.15448)
*Nengbo Zhang,Hann Woei Ho*

Main category: cs.CV

TL;DR: 本文提出了一种名为MAVR-Net的多视角微型飞行器（MAV）动作识别框架，通过融合RGB帧、光流和分割掩码三种模态，有效提升了动作识别的准确性，实验结果在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统仅依赖RGB数据的视觉方法难以捕捉MAV复杂的时空特征，导致动作判别能力有限，因此需要融合多种视角和模态以增强感知与识别能力。

Method: MAVR-Net采用ResNet为基础的编码器提取三种视角（RGB、光流、分割掩码）特征，并结合多尺度金字塔结构保持动作的时空细节，引入跨视角注意力模块模建不同模态和尺度的依赖关系，同时设计多视角对齐损失保证语义一致性和特征强化。

Result: 在Short MAV、Medium MAV及Long MAV等标准数据集上，MAVR-Net分别取得了97.8%、96.5%和92.8%的动作识别准确率，显著优于已有方法。

Conclusion: 多视角、跨模态融合与特征对齐极大提升了MAV动作识别的准确性和鲁棒性，MAVR-Net为无人机协同感知与控制提供了更强有效的技术支撑。

Abstract: Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
enabling cooperative perception and control in autonomous aerial swarms. Yet,
vision-based recognition models relying only on RGB data often fail to capture
the complex spatial temporal characteristics of MAV motion, which limits their
ability to distinguish different actions. To overcome this problem, this paper
presents MAVR-Net, a multi-view learning-based MAV action recognition
framework. Unlike traditional single-view methods, the proposed approach
combines three complementary types of data, including raw RGB frames, optical
flow, and segmentation masks, to improve the robustness and accuracy of MAV
motion recognition. Specifically, ResNet-based encoders are used to extract
discriminative features from each view, and a multi-scale feature pyramid is
adopted to preserve the spatiotemporal details of MAV motion patterns. To
enhance the interaction between different views, a cross-view attention module
is introduced to model the dependencies among various modalities and feature
scales. In addition, a multi-view alignment loss is designed to ensure semantic
consistency and strengthen cross-view feature representations. Experimental
results on benchmark MAV action datasets show that our method clearly
outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

</details>


### [46] [DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking](https://arxiv.org/abs/2510.15449)
*Zhiqiang Zhu,Xinbo Gao,Wen Lu,Jie Li,Zhaoyang Wang,Mingqian Ge*

Main category: cs.CV

TL;DR: 该论文提出了一种面向夜间航拍场景的新型目标跟踪方法DPTrack，通过引入细粒度属性指引的提示机制，有效提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于prompt learning的夜间航拍目标跟踪方法仅依赖空间定位监督，缺乏能够明确指向目标属性的细粒度线索，导致提示模糊，跟踪性能不足。因此，亟需引入更丰富的目标特征提示以增强模型表现。

Method: 本文提出DPTrack，核心思想是将目标的属性特征编码进方向核（directional kernel），丰富提示线索，并借鉴视觉仿生学层次化捕获目标的拓扑结构，将其集成为拓扑感知特征，再通过编码器压缩为方向核。该核作为引导信号，显式嵌入目标的细粒度属性，结合通道-类别对应属性的提示模块，实现对搜索区域特征的精准引导，并加入空间门控机制以提升夜间鲁棒性。

Result: 在多个公开夜间航拍跟踪基准数据集上的实验表明，DPTrack表现优异，优于现有主流方法。

Conclusion: 通过引入细粒度属性指引的方向核和空间门控，DPTrack极大提高了夜间航拍目标跟踪的准确性与鲁棒性，有望推动该领域技术进步。

Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on
spatial localization supervision, which fails to provide fine-grained cues that
point to target features and inevitably produces vague prompts. This limitation
impairs the tracker's ability to accurately focus on the object features and
results in trackers still performing poorly. To address this issue, we propose
DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by
encoding the given object's attribute features into the directional kernel
enriched with fine-grained cues to generate precise prompts. Specifically,
drawing inspiration from visual bionics, DPTrack first hierarchically captures
the object's topological structure, leveraging topological attributes to enrich
the feature representation. Subsequently, an encoder condenses these
topology-aware features into the directional kernel, which serves as the core
guidance signal that explicitly encapsulates the object's fine-grained
attribute cues. Finally, a kernel-guided prompt module built on
channel-category correspondence attributes propagates the kernel across the
features of the search region to pinpoint the positions of target features and
convert them into precise prompts, integrating spatial gating for robust
nighttime tracking. Extensive evaluations on established benchmarks demonstrate
DPTrack's superior performance. Our code will be available at
https://github.com/zzq-vipsl/DPTrack.

</details>


### [47] [Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation](https://arxiv.org/abs/2510.15466)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态图像（Dynamic Image）的相位感知时序增强方法，用于提升微表情识别（MER）在数据稀缺场景下的表现。该方法通过分阶段生成动态图像，丰富运动特征并提高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 微表情识别对心理学、安全和行为分析等领域至关重要，但受限于微表情数据标注样本稀缺，深度学习方法在泛化和运动特征多样性上存在挑战。以往主要依赖简单的空间增强，忽略了更能捕捉运动特征的时序增强。

Method: 作者提出将表情序列划分为“起始-峰值”和“峰值-结束”两个运动阶段，对每一阶段分别生成动态图像，形成“双阶段DI增强”策略，从而获得更丰富且互补的时序信息。该策略可与空间增强结合，适用于不同深度网络架构。

Result: 在CASME-II和SAMM两个微表情数据集上，包含CNN、Vision Transformer和轻量化LEARNet等6种架构的实验显示，该方法在识别准确率、无权重F1分数和无权重平均召回等指标上均取得明显提升，最高可达10%的相对提升。

Conclusion: 所提增强方法结构简单，对模型无特定要求，在低资源环境下表现突出，为提升微表情识别的鲁棒性和泛化能力提供了有效思路。

Abstract: Micro-expressions (MEs) are brief, involuntary facial movements that reveal
genuine emotions, typically lasting less than half a second. Recognizing these
subtle expressions is critical for applications in psychology, security, and
behavioral analysis. Although deep learning has enabled significant advances in
micro-expression recognition (MER), its effectiveness is limited by the
scarcity of annotated ME datasets. This data limitation not only hinders
generalization but also restricts the diversity of motion patterns captured
during training. Existing MER studies predominantly rely on simple spatial
augmentations (e.g., flipping, rotation) and overlook temporal augmentation
strategies that can better exploit motion characteristics. To address this gap,
this paper proposes a phase-aware temporal augmentation method based on dynamic
image. Rather than encoding the entire expression as a single onset-to-offset
dynamic image (DI), our approach decomposes each expression sequence into two
motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
representations enrich motion diversity and introduce complementary temporal
cues that are crucial for recognizing subtle facial transitions. Extensive
experiments on CASME-II and SAMM datasets using six deep architectures,
including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
consistent performance improvements in recognition accuracy, unweighted
F1-score, and unweighted average recall, which are crucial for addressing class
imbalance in MER. When combined with spatial augmentations, our method achieves
up to a 10\% relative improvement. The proposed augmentation is simple,
model-agnostic, and effective in low-resource settings, offering a promising
direction for robust and generalizable MER.

</details>


### [48] [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/abs/2510.15467)
*Lingfeng Xuan,Chang Nie,Yiqing Xu,Zhe Liu,Yanzi Miao,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一个针对自动驾驶多相机场景的SfM框架MRASfM，有效提升了相机位姿估计和路面重建质量，且验证了其高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的SfM方法在多相机自动驾驶场景中面临诸如位姿估计不可靠、路面重建异常点过多、效率低下等难题，因此亟需一个专为此类场景设计并解决上述问题的SfM系统。

Method: MRASfM框架利用多相机系统的固定空间位置关系提升相机位姿估计，其路面重建环节采用平面模型去除异常点，并且通过将多相机系统作为整体进行Bundle Adjustment降低优化变量，提高计算效率。同时引入场景关联与组装模块，实现多场景粗到细的聚合。

Result: 在真实车辆和公开数据集上的大规模实验表明，MRASfM不但能够泛化到不同场景，并且在挑战性条件下依然表现出强鲁棒性与高精度，在nuScenes数据集上取得了0.124的绝对位姿误差，达到当前最优水平。

Conclusion: MRASfM有效解决了多相机自动驾驶SfM中的主要痛点，提升了重建精度与效率，具备实际应用价值。

Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point
clouds, forming a foundation for various tasks. However, applying SfM to
driving scenes captured by multi-camera systems presents significant
difficulties, including unreliable pose estimation, excessive outliers in road
surface reconstruction, and low reconstruction efficiency. To address these
limitations, we propose a Multi-camera Reconstruction and Aggregation
Structure-from-Motion (MRASfM) framework specifically designed for driving
scenes. MRASfM enhances the reliability of camera pose estimation by leveraging
the fixed spatial relationships within the multi-camera system during the
registration process. To improve the quality of road surface reconstruction,
our framework employs a plane model to effectively remove erroneous points from
the triangulated road surface. Moreover, treating the multi-camera set as a
single unit in Bundle Adjustment (BA) helps reduce optimization variables to
boost efficiency. In addition, MRASfM achieves multi-scene aggregation through
scene association and assembly modules in a coarse-to-fine fashion. We deployed
multi-camera systems on actual vehicles to validate the generalizability of
MRASfM across various scenes and its robustness in challenging conditions
through real-world applications. Furthermore, large-scale validation results on
public datasets show the state-of-the-art performance of MRASfM, achieving
0.124 absolute pose error on the nuScenes dataset.

</details>


### [49] [MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval](https://arxiv.org/abs/2510.15470)
*Jinghao Huang,Yaxiong Chen,Ganchao Liu*

Main category: cs.CV

TL;DR: 本文首次系统性提出和研究了无人机视频-文本检索（DVTR）任务，并针对无人机视频的独特视角和语义表达，提出了一种多语义自适应挖掘（MSAM）方法，有效提升了无人机场景下的跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术发展，大量无人机视频数据产生，这些视频有独特的俯视视角和结构特点，现有面向地面视频的跨模态检索方法难以有效适应，因此需要专门针对无人机场景的视频-文本检索方法。

Method: 提出多语义自适应挖掘（MSAM）方法，包括：1）多语义自适应学习机制，通过建模帧间动态变化和从场景特定区域挖掘丰富语义；2）细粒度词-帧交互，包含自适应语义构建模块、分布驱动学习项及多样性语义项，增强文本和视频之间的交互并提升表征能力；3）引入跨模态交互特征融合池化机制，聚焦目标区域特征抽取与匹配，降低背景噪声干扰。

Result: 在两个自建的无人机视频-文本数据集上进行了大量实验，结果显示MSAM方法在无人机视频-文本检索任务上明显优于当前主流方法。

Conclusion: 针对无人机视频独特特点，本文提出的MSAM方法有效提升了无人机视频-文本检索的效果，为该领域提供了新的技术途径，源码和数据集也将对外公开。

Abstract: With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

</details>


### [50] [A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition](https://arxiv.org/abs/2510.15471)
*Vu Tram Anh Khuong,Thi Bich Phuong Man,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本论文提出了一种融合始发到顶点（onset-to-apex）与顶点到结束（apex-to-offset）阶段的联合光流（Combined Optical Flow, COF）方法，提升了微表情识别（MER）的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大多数微表情识别方法只关注始发到顶点阶段的光流，忽视了顶点到结束阶段的动态信息，导致对微表情动态捕捉不全面。

Method: 提出结合两个阶段光流信息的COF方法，对面部运动进行更全面的分析。COF算法分别提取并整合onset-to-apex和apex-to-offset阶段的光流，提升特征表现能力。

Result: 在CASMEII与SAMM两个常用微表情数据集上的实验表明，COF方法在识别准确率上优于只采用单阶段光流特征的方法。

Conclusion: COF方法通过整合不同阶段的动态特征，有效提升了微表情的识别性能，验证了捕获完整时序动态的重要性。

Abstract: Facial micro-expressions are brief, involuntary facial movements that reveal
hidden emotions. Most Micro-Expression Recognition (MER) methods that rely on
optical flow typically focus on the onset-to-apex phase, neglecting the
apex-to-offset phase, which holds key temporal dynamics. This study introduces
a Combined Optical Flow (COF), integrating both phases to enhance feature
representation. COF provides a more comprehensive motion analysis, improving
MER performance. Experimental results on CASMEII and SAMM datasets show that
COF outperforms single optical flow-based methods, demonstrating its
effectiveness in capturing micro-expression dynamics.

</details>


### [51] [Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI](https://arxiv.org/abs/2502.17092)
*Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi*

Main category: cs.CV

TL;DR: Shakti VLM是一组1B和4B参数规模的新型视觉-语言模型，旨在以更少的数据训练获得优异表现。其利用多项结构创新提升效率，适合企业级多模态任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型（VLMs）通常依赖海量数据进行训练，数据和计算成本高昂。因此，作者希望通过结构和训练方法创新，在数据用量有限的条件下，获得与大模型相当的性能，解决多模态学习中的数据效率难题。

Method: 提出Shakti VLM系列模型，包含1B和4B参数规模。方法包括：1. QK-Normalization提升注意力稳定性；2. 混合归一化方法；3. 增强型位置编码；4. 分三阶段训练策略，优化学习效率。整体目标在模型结构、训练流程和归一化机制上提高数据利用率与泛化能力。

Result: Shakti VLM-1B和Shakti VLM-4B模型在文档理解、视觉推理、OCR提取和通用多模态推理等任务上表现优异，达到或接近大量数据训练模型的效果。

Conclusion: Shakti表明，通过模型结构设计和训练策略优化，可以不用大量数据也能获得高性能，对企业级多模态应用具有高效和实用价值。

Abstract: We introduce Shakti VLM, a family of vision-language models in the capacity
of 1B and 4B parameters designed to address data efficiency challenges in
multimodal learning. While recent VLMs achieve strong performance through
extensive training data, Shakti models leverage architectural innovations to
attain competitive results with fewer tokens. Key advancements include
QK-Normalization for attention stability, hybrid normalization techniques, and
enhanced positional encoding. A three-stage training strategy further optimizes
learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and
Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR
extraction, and general multimodal reasoning. Our results highlight that high
performance can be achieved through model design and training strategy rather
than sheer data volume, making Shakti an efficient solution for
enterprise-scale multimodal tasks.

</details>


### [52] [Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions](https://arxiv.org/abs/2510.15491)
*Andre Rochow,Jonas Marcic,Svetlana Seliunina,Sven Behnke*

Main category: cs.CV

TL;DR: 本论文提出了一种自动化无人机拍摄并生成高质量植物3D重建的流程，能有效减少因风或叶片晃动带来的重建误差，提升三维表型分析的精度。


<details>
  <summary>Details</summary>
Motivation: 植物的三维表型分析，对于理解植物生长、产量预测和病害防控极为重要。目前三维重建受限于户外环境（如风、无人机气流）导致的叶片运动，容易造成重建误差，影响后续分析与应用。

Method: 作者提出了一个以小型商用无人机自动拍摄为基础的3D重建流程。除标记点布置需人工外，图像采集基本全自动，所有流程受自研安卓应用控制。Pipeline可支持多种最新3D重建方法，并引入了基于光流的迭代对齐机制，通过估计和修正叶片运动，逐步优化输入图片，实现场景静态化，提高最终的3D重建精度。

Result: 实验表明，该流程有效减小了因叶片运动导致的重建误差，显著提升了现有三维重建方法的精度，能够提取出高分辨率的三维网格模型。作者还发布了重建流程源码和多时间点、不同作物的植物数据集。

Conclusion: 该论文提出的自动化3D重建流程在实际农田环境下表现优异，提升了植物三维重建的精度和实用性，为植物表型学、农业监测等应用提供了高质量的技术基础和数据支撑。

Abstract: 3D phenotyping of plants plays a crucial role for understanding plant growth,
yield prediction, and disease control. We present a pipeline capable of
generating high-quality 3D reconstructions of individual agricultural plants.
To acquire data, a small commercially available UAV captures images of a
selected plant. Apart from placing ArUco markers, the entire image acquisition
process is fully autonomous, controlled by a self-developed Android application
running on the drone's controller. The reconstruction task is particularly
challenging due to environmental wind and downwash of the UAV. Our proposed
pipeline supports the integration of arbitrary state-of-the-art 3D
reconstruction methods. To mitigate errors caused by leaf motion during image
capture, we use an iterative method that gradually adjusts the input images
through deformation. Motion is estimated using optical flow between the
original input images and intermediate 3D reconstructions rendered from the
corresponding viewpoints. This alignment gradually reduces scene motion,
resulting in a canonical representation. After a few iterations, our pipeline
improves the reconstruction of state-of-the-art methods and enables the
extraction of high-resolution 3D meshes. We will publicly release the source
code of our reconstruction pipeline. Additionally, we provide a dataset
consisting of multiple plants from various crops, captured across different
points in time.

</details>


### [53] [Rethinking Efficient Hierarchical Mixing Architecture for Low-light RAW Image Enhancement](https://arxiv.org/abs/2510.15497)
*Xianmin Chen,Peiliang Huang,Longfei Han,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了一种高效低光图像增强新架构HiMA，在提升增强质量的同时兼具高效率，并引入局部分布调整和多先验融合模块。实验表明该方法优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前低光RAW图像增强方法在实际应用中难以兼顾高质量与高效率，传统方法在结构设计与特征处理上存在局限。如何设计出兼具效率与性能的新架构，成为亟需解决的问题。

Method: 本文提出Hierarchical Mixing Architecture（HiMA），融合Transformer与Mamba模块，分别处理大尺度与小尺度特征，提升特征表达能力和效率。同时，提出Local Distribution Adjustment（LoDA）模块自适应对齐不同局部区域的特征分布，并提出Multi-prior Fusion（MPF）模块，将空间和频域先验融合以增强细节。

Result: 在多个公开低光增强数据集上进行了大量实验，结果显示本文方法参数量更少但性能优于主流最新方法。

Conclusion: HiMA架构及其配套模块能有效提升低光RAW图像增强性能且高效实用，有望在实际应用获得推广。

Abstract: Low-light RAW image enhancement remains a challenging task. Although numerous
deep learning based approaches have been proposed, they still suffer from
inherent limitations. A key challenge is how to simultaneously achieve strong
enhancement quality and high efficiency. In this paper, we rethink the
architecture for efficient low-light image signal processing (ISP) and
introduce a Hierarchical Mixing Architecture (HiMA). HiMA leverages the
complementary strengths of Transformer and Mamba modules to handle features at
large and small scales, respectively, thereby improving efficiency while
avoiding the ambiguities observed in prior two-stage frameworks. To further
address uneven illumination with strong local variations, we propose Local
Distribution Adjustment (LoDA), which adaptively aligns feature distributions
across different local regions. In addition, to fully exploit the denoised
outputs from the first stage, we design a Multi-prior Fusion (MPF) module that
integrates spatial and frequency-domain priors for detail enhancement.
Extensive experiments on multiple public datasets demonstrate that our method
outperforms state-of-the-art approaches, achieving superior performance with
fewer parameters. Code will be released at https://github.com/Cynicarlos/HiMA.

</details>


### [54] [Latent Feature Alignment: Discovering Biased and Interpretable Subpopulations in Face Recognition Models](https://arxiv.org/abs/2510.15520)
*Ignacio Serna*

Main category: cs.CV

TL;DR: 本文提出了一种无需属性标签的算法LFA，用于发现人脸识别模型中的子群体，能够更好地识别和分析模型偏差。


<details>
  <summary>Details</summary>
Motivation: 人脸识别模型虽然总体准确率高，但仍然存在对某些子群体表现不佳的系统性偏差。传统的偏差评估依赖人工标注的属性，成本高且局限于预定义类别，因此需要无需属性标签的新方法。

Method: 作者提出了一种名为Latent Feature Alignment（LFA）的算法。该方法通过发现潜在特征方向来形成子群体，而无需依赖于明确的标签。LFA优于传统聚类（如k-means），能够实现语义一致性更高的分组，并能识别出与年龄、种族、着装等可解释属性相关的潜在方向。

Result: 在ArcFace、CosFace、ElasticFace、PartialFC四种主流识别模型和RFW、CelebA两个基准数据集上，LFA在组内语义一致性和属性方向解释性上优于k-means和最近邻方法。

Conclusion: LFA无需预先定义属性标签，能够有效发现和解释受偏差影响的子群体，是人脸识别模型偏差分析和结果审计的实用工具。

Abstract: Modern face recognition models achieve high overall accuracy but continue to
exhibit systematic biases that disproportionately affect certain
subpopulations. Conventional bias evaluation frameworks rely on labeled
attributes to form subpopulations, which are expensive to obtain and limited to
predefined categories. We introduce Latent Feature Alignment (LFA), an
attribute-label-free algorithm that uses latent directions to identify
subpopulations. This yields two main benefits over standard clustering: (i)
semantically coherent grouping, where faces sharing common attributes are
grouped together more reliably than by proximity-based methods, and (ii)
discovery of interpretable directions, which correspond to semantic attributes
such as age, ethnicity, or attire. Across four state-of-the-art recognition
models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW,
CelebA), LFA consistently outperforms k-means and nearest-neighbor search in
intra-group semantic coherence, while uncovering interpretable latent
directions aligned with demographic and contextual attributes. These results
position LFA as a practical method for representation auditing of face
recognition models, enabling practitioners to identify and interpret biased
subpopulations without predefined attribute annotations.

</details>


### [55] [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://arxiv.org/abs/2510.15870)
*Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu,Jason Lu,Oluwatobi Olabiyi,Frank Wang,Rafael Valle,Bryan Catanzaro,Andrew Tao,Song Han,Jan Kautz,Hongxu Yin,Pavlo Molchanov*

Main category: cs.CV

TL;DR: OmniVinci提出了一种开源的全模态大模型，通过创新的模型架构和高效的数据管道，显著提升了多模态理解和推理能力，并在跨模态基准测试上大幅超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 推动机器智能向类人多模态感知发展，实现模型能够融合视觉、音频等多种感知通道来理解世界。当前存在多模态对齐不强、函数表达不充分、数据利用率低等挑战。

Method: 1. 模型架构：提议OmniAlignNet增强视觉与音频嵌入的对齐；采用Temporal Embedding Grouping建模模态间的相对时序关系；利用Constrained Rotary Time Embedding编码绝对时序信息。2. 数据方面，开发出能够生成2400万条单一及多模态对话的新型数据管道。

Result: OmniVinci模型以20%的训练token量，显著超过Qwen2.5-Omni在多个跨模态理解基准（DailyOmni提升19.05分，MMAR音频提升1.7分，Video-MME视觉提升3.9分）。

Conclusion: OmniVinci展示了全新多模态大模型的性能极限与高效训练方法，在机器人、医疗AI和智能工厂等领域具备强大应用潜力，表明多模态融合将极大推进人工智能发展。

Abstract: Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.

</details>


### [56] [Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training](https://arxiv.org/abs/2510.15527)
*Aditya Vir*

Main category: cs.CV

TL;DR: 本文提出了针对卫星土地利用分类的定制卷积神经网络（CNN）架构，在EuroSAT数据集上达到了97.23%的测试准确率，无需依赖预训练模型。通过三次架构迭代和创新的多任务注意力机制，有效提升了分类效果，并对空间与光谱特征赋予近等的重要性。最终性能接近精调ResNet-50，但无需外部数据。


<details>
  <summary>Details</summary>
Motivation: 卫星遥感图像的土地利用分类对城市管理与环境监测十分重要，但现有主流方法多依赖预训练模型，泛化及迁移能力有限。作者旨在通过系统性的架构优化，实现不依赖外部数据也能获得高性能的土地利用分类模型。

Method: 作者依次设计了三种CNN架构：基础网络、引入CBAM注意力机制、加入自创的平衡多任务注意力机制（融合Coordinate Attention与Squeeze-Excitation块，利用可学习参数调节两者贡献）。另外，采用分阶段DropBlock正则化和类别平衡损失权重以缓解过拟合和类别混淆。

Result: 三种网络的测试准确率依次提升（94.30%、95.98%、97.23%）。最终12层模型各类别准确率均超94.46%，Cohen's Kappa达0.9692，正确和错误预测置信度差距24.25%。与精调ResNet-50（98.57%）比较，新方法无需外部数据，性能差距仅1.34%。

Conclusion: 系统性架构设计和创新注意力机制可在无需预训练的条件下获得接近SOTA的卫星图像分类性能。空间与光谱特征对分类同等重要，方法对高置信度预测有良好校准。为领域特定应用提供了高效、可复现的解决方案。

Abstract: This work presents a systematic investigation of custom convolutional neural
network architectures for satellite land use classification, achieving 97.23%
test accuracy on the EuroSAT dataset without reliance on pre-trained models.
Through three progressive architectural iterations (baseline: 94.30%,
CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify
and address specific failure modes in satellite imagery classification. Our
principal contribution is a novel balanced multi-task attention mechanism that
combines Coordinate Attention for spatial feature extraction with
Squeeze-Excitation blocks for spectral feature extraction, unified through a
learnable fusion parameter. Experimental results demonstrate that this
learnable parameter autonomously converges to alpha approximately 0.57,
indicating near-equal importance of spatial and spectral modalities for
satellite imagery. We employ progressive DropBlock regularization (5-20% by
network depth) and class-balanced loss weighting to address overfitting and
confusion pattern imbalance. The final 12-layer architecture achieves Cohen's
Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating
confidence calibration with a 24.25% gap between correct and incorrect
predictions. Our approach achieves performance within 1.34% of fine-tuned
ResNet-50 (98.57%) while requiring no external data, validating the efficacy of
systematic architectural design for domain-specific applications. Complete
code, trained models, and evaluation scripts are publicly available.

</details>


### [57] [Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for Dementia Diagnostics](https://arxiv.org/abs/2510.15556)
*Yitong Li,Ralph Buchert,Benita Schmitz-Koep,Timo Grimmer,Björn Ommer,Dennis M. Hedderich,Igor Yakushev,Christian Wachinger*

Main category: cs.CV

TL;DR: 本研究提出了一种基于MRI和患者辅助信息生成高质量PET影像的新方法SiM2P，在多中心临床数据中验证该方法可显著提升痴呆性疾病鉴别诊断的准确性，使PET检查的诊断价值更加普及。


<details>
  <summary>Details</summary>
Motivation: FDG-PET对于神经退行性疾病（如阿尔茨海默病）诊断非常有用，但价格昂贵且获取受限；相反，MRI更易获得，但诊断敏感性和分辨率有限。急需一种方法能以MRI等常规数据为基础，获得类似PET的诊断增益，以便在资源有限的医疗环境中提升痴呆类疾病的早期检测和鉴别能力。

Method: 提出SiM2P，一种3D扩散模型框架，将MRI和基础患者信息映射为高置信度的虚拟FDG-PET影像；通过前瞻性、盲法、专业医生评估实验，将SiM2P模拟PET与真实MRI影像在不同疾病（阿兹海默病、额颞叶痴呆、健康对照）患者中进行对比，并开发了适合本地部署和少量数据适配的实用化流程。

Result: 在临床实验中，SiM2P生成的PET影像将三组疾病的诊断准确率由75.0%提升至84.7%；和MRI相比，模拟PET具有更高的诊断确定性分数及医师间一致性；且模型本地部署简单，仅需约20例本地数据和基础人口学信息即可落地。

Conclusion: SiM2P实现了用常规MRI和基础信息生成可用PET影像，大大提升了痴呆性疾病的诊断可及性和准确性，尤其适用于资源受限地区，促进FDG-PET诊断价值的广泛普及，有助于疾病的早诊早治。

Abstract: Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an
established tool in the diagnostic workup of patients with suspected dementing
disorders. However, compared to the routinely available magnetic resonance
imaging (MRI), FDG-PET remains significantly less accessible and substantially
more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework
that learns a probabilistic mapping from MRI and auxiliary patient information
to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader
study, two neuroradiologists and two nuclear medicine physicians rated the
original MRI and SiM2P-simulated PET images of patients with Alzheimer's
disease, behavioral-variant frontotemporal dementia, and cognitively healthy
controls. SiM2P significantly improved the overall diagnostic accuracy of
differentiating between three groups from 75.0% to 84.7% (p<0.05). Notably, the
simulated PET images received higher diagnostic certainty ratings and achieved
superior interrater agreement compared to the MRI images. Finally, we developed
a practical workflow for local deployment of the SiM2P framework. It requires
as few as 20 site-specific cases and only basic demographic information. This
approach makes the established diagnostic benefits of FDG-PET imaging more
accessible to patients with suspected dementing disorders, potentially
improving early detection and differential diagnosis in resource-limited
settings. Our code is available at https://github.com/Yiiitong/SiM2P.

</details>


### [58] [ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents](https://arxiv.org/abs/2510.15557)
*Tingyu Lin,Marco Peer,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: ClapperText 数据集针对低资源和退化视觉环境下的手写和印刷文本识别问题，提供了带有详细标注的二战时期视频活页板镜头，涵盖近10万标注词。


<details>
  <summary>Details</summary>
Motivation: 历史文档常因画质退化、手写变化等原因导致文本识别极具挑战，尤其是在结构化内容被以非标准形式记录且资源受限的情境下，现有基准数据难以满足此类应用需求，因此需要一个兼具文化语境与实际难点的数据集。

Method: 研究团队从127段二战视频片段中采集包含活页板的画面，精确标注近10万个词实例，区分手写/印刷、语义类别、遮挡情况，并用4点多边形给出旋转包围框。数据可同时用于全图及裁剪词图像任务，评测协议统一，针对OCR识别和检测任务对多模型做了零样本和微调实验。

Result: 在仅有18段视频微调的数据条件下，即便训练集非常有限，通过微调不同模型也能显著提升性能，说明该数据特别适合小样本学习场景。

Conclusion: ClapperText 为低资源、视觉退化环境下的OCR研究提供了真实且具历史文化价值的新基准，有助于推动文档理解技术在文献档案等领域的应用。数据与代码均已开放。

Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.

</details>


### [59] [Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation](https://arxiv.org/abs/2510.15564)
*Xiaoming Zhu,Xu Huang,Qinghongbing Xie,Zhi Deng,Junsheng Yu,Yirui Guan,Zhongyuan Liu,Lin Zhu,Qijun Zhao,Ligang Liu,Long Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成艺术且连贯的三维场景布局的新系统，结合视觉引导和深度学习，显著提升了布局的丰富性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的三维场景生成方法存在手工规则复杂、深度生成模型多样性不足及大语言模型空间理解力弱等问题，难以兼顾内容的丰富性、鲁棒性与合理性。

Method: 1）构建高质量资产库，包括2,037个场景资产和147个3D场景布局；2）利用图像生成模型，将文本提示扩展为图片，并进行微调以对齐资产库；3）开发鲁棒的图像解析模块，根据视觉语义和几何信息恢复三维场景布局；4）基于场景图与视觉语义优化场景布局，确保逻辑连贯与图像一致。

Result: 大量用户测试显示，所提出的算法在布局丰富性和质量方面，均显著优于现有方法。

Conclusion: 该系统能有效生成高质量且具有艺术性的三维场景布局，为数字内容创作提供了创新手段。代码与数据集公开，有望促进相关领域发展。

Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.

</details>


### [60] [Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images](https://arxiv.org/abs/2510.15576)
*Sami Belguesmia,Mohand Saïd Allili,Assia Hamadene*

Main category: cs.CV

TL;DR: 本文提出了一种多视角结构，通过分析面部多层次特征提升DeepFake检测能力，对比单一视角方法在姿态变化和复杂环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前DeepFake检测方法在面部姿态变化、遮挡和伪造痕迹微小等实际应用中效果不足，因此需寻求更鲁棒的检测策略。

Method: 提出集成三重专用编码器：全局视角编码器检测边界不一致性、中等视角编码器分析纹理与色彩对齐、局部视角编码器关注关键区域如眼、鼻、口的细微伪造，同时用姿态编码器提升多角度检测鲁棒性。融合多编码器特征后统一做检测。

Result: 在多组复杂姿态和光照条件下的数据集实验中，该方法性能优于传统单视角检测方法。

Conclusion: 所提多视角架构可显著增强DeepFake检测效果，尤其适用于复杂现实条件，具有较高实际应用价值。

Abstract: DeepFake technology has advanced significantly in recent years, enabling the
creation of highly realistic synthetic face images. Existing DeepFake detection
methods often struggle with pose variations, occlusions, and artifacts that are
difficult to detect in real-world conditions. To address these challenges, we
propose a multi-view architecture that enhances DeepFake detection by analyzing
facial features at multiple levels. Our approach integrates three specialized
encoders, a global view encoder for detecting boundary inconsistencies, a
middle view encoder for analyzing texture and color alignment, and a local view
encoder for capturing distortions in expressive facial regions such as the
eyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,
we incorporate a face orientation encoder, trained to classify face poses,
ensuring robust detection across various viewing angles. By fusing features
from these encoders, our model achieves superior performance in detecting
manipulated images, even under challenging pose and lighting
conditions.Experimental results on challenging datasets demonstrate the
effectiveness of our method, outperforming conventional single-view approaches

</details>


### [61] [Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy](https://arxiv.org/abs/2510.15579)
*Mohammad Soltaninezhad,Yashar Rouzbahani,Jhonatan Contreras,Rohan Chippalkatti,Daniel Kwaku Abankwa,Christian Eggeling,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文提出了一种轻量化CycleGAN模型，显著减少了参数量并提升了性能，用于荧光显微镜不同模态图像的转换，同时还能作为实验诊断工具。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在科学计算中往往计算成本高，对环境造成影响，因此开发轻量级、高效且性能优良的模型具有重要意义，特别是在处理如荧光显微镜这类常见于生物医学领域的高维度图像任务中。

Method: 将CycleGAN用于荧光显微镜模态转换任务，创新性地用固定通道数量替代传统U-Net生成器中的通道倍增策略，使模型参数从4180万减少到约九千个。还提出将GAN生成的输出与新实验图像进行比对，用于诊断实验和标记质量。

Result: 轻量化模型不仅大幅降低了参数量，还实现了更快的训练速度和更低的内存消耗，并且性能优于传统方法。通过GAN生成图像与实验图像的差异，还能有效检测诸如光漂白、伪影或标记不准确等问题。

Conclusion: 该轻量化CycleGAN不仅显著优化了显微镜图像模态转换任务的效率和效果，还能用于图像及实验质量的自动诊断，成为显微镜工作流中实用的验证工具。

Abstract: Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.

</details>


### [62] [Standardization for improved Spatio-Temporal Image Fusion](https://arxiv.org/abs/2510.15589)
*Harkaitz Goyena,Peter M. Atkinson,Unai Pérez-Goya,M. Dolores Ugarte*

Main category: cs.CV

TL;DR: 本文提出并对比了两种用于时空图像融合的标准化方法，以提高不同分辨率传感器影像融合的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时空图像融合（STIF）方法依赖于空间和光谱分辨率匹配的多源影像，为了简化STIF方法的应用，需要有效的影像标准化策略。

Method: 提出和对比了两种影像标准化方法：一种是传统的高分影像上采样方法，另一种是基于异常的卫星影像标准化（ABSIS）锐化方法。后者通过融合高分影像序列的整体特征和低分影像的独特属性，提高了融合影像的质量。

Result: 两种方法都显著提升了无配对时空融合影像块（USTFIP）方法在融合准确率方面的表现，其中ABSIS方法分别使融合图像的光谱和空间准确率提升了多达49.46%和78.40%。

Conclusion: 所提出的标准化方法，尤其是ABSIS锐化方法，有效促进了时空图像融合精度的提升，为STIF方法在实际中的应用提供了更广阔的前景。

Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images
with matching spatial and spectral resolutions captured by different sensors.
To facilitate the application of STIF methods, we propose and compare two
different standardization approaches. The first method is based on traditional
upscaling of the fine-resolution images. The second method is a sharpening
approach called Anomaly Based Satellite Image Standardization (ABSIS) that
blends the overall features found in the fine-resolution image series with the
distinctive attributes of a specific coarse-resolution image to produce images
that more closely resemble the outcome of aggregating the fine-resolution
images. Both methods produce a significant increase in accuracy of the Unpaired
Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the
sharpening approach increasing the spectral and spatial accuracies of the fused
images by up to 49.46\% and 78.40\%, respectively.

</details>


### [63] [FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification](https://arxiv.org/abs/2510.15595)
*Zhen Sun,Lei Tan,Yunhang Shen,Chengmao Cai,Xing Sun,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了FlexiReID，一个能够处理四种模态（RGB、红外、素描、文本）之间七种检索模式的灵活行人再识别方法，并构建了一个统一多模态测试数据集。实验显示其性能优异且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有多模态行人再识别方法只支持有限的模态间检索方式，无法应对实际中任意查询与检索的需求，限制了其实际应用价值。

Method: 提出FlexiReID框架，利用自适应专家混合机制（Mixture-of-Experts, MoE）动态融合不同模态特征，并通过跨模态查询融合模块增强多模态特征提取能力。同时，扩展构建CIRS-PEDES数据集，整合主流 Re-ID 数据集涵盖所有四个模态。

Result: FlexiReID在统一构建的多模态数据集CIRS-PEDES上的实验中取得了最先进的性能，且在复杂场景下具有较强的泛化能力。

Conclusion: FlexiReID框架能灵活支持多模态、多种检索模式并取得了优异实验结果，有望推动实际应用中的多模态行人再识别技术。

Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.

</details>


### [64] [Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection](https://arxiv.org/abs/2510.15602)
*Andrei-Timotei Ardelean,Patrick Rückbeil,Tim Weyrich*

Main category: cs.CV

TL;DR: 该论文提出了一种名为QFCA的实时零样本异常定位方法，显著加快检测速度且保持高准确率，适用于复杂纹理的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法检测速度慢，难以满足实际工业场景（如流水线监控）的应用需求。作者旨在提升计算效率，实现实时异常检测。

Method: 作者提出了QFCA方法，对特征对应分析（FCA）算法进行量化，用直方图方法对比图像块统计特征，并引入主成分分析进行特征预处理，提升正常与异常特征的区分度。

Result: QFCA方法在保证检测准确率的同时，实现了10倍的检测加速。在复杂纹理上的检测精度有所提升，并在评测中优于现有方法。

Conclusion: QFCA方法兼具高效性和精度，可应用于实时纹理异常检测任务。对应文献在实际部署中具有很高的实用价值。

Abstract: Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html

</details>


### [65] [Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration](https://arxiv.org/abs/2510.15611)
*Tomáš Chobola,Julia A. Schnabel,Tingying Peng*

Main category: cs.CV

TL;DR: 该论文提出了一种超轻量级的自监督图像去噪模型Noise2Detail（N2D），在有限计算资源下实现了快速且高质量的图像恢复，优于已有的无数据集方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督图像去噪方法虽然效果出色，但计算和内存需求高，限制了其在实际中的应用。尤其是在计算资源有限或训练数据稀缺的场景（如生物医学成像），亟需一种高效且无需依赖干净参考图像的新方法。

Method: 基于Noise2Noise训练框架（无需干净参考图像或显式噪声建模），作者提出了多阶段去噪流程Noise2Detail（N2D）。推理过程中，模型先破坏噪声的空间相关性，获得平滑中间结果，再从原始噪声输入中细致修复图像细节。模型整体极其轻量，并具备快速推理能力。

Result: 大量实验表明，Noise2Detail在保持极低计算资源消耗的同时，去噪和图像恢复表现超过了当前主流的免数据集自监督方法。

Conclusion: 该研究的高效、低计算成本和无需干净数据集的特点，尤其适用于解决生物医学成像等领域干净训练数据稀缺与高效推理的实际问题。

Abstract: Current self-supervised denoising techniques achieve impressive results, yet
their real-world application is frequently constrained by substantial
computational and memory demands, necessitating a compromise between inference
speed and reconstruction quality. In this paper, we present an
ultra-lightweight model that addresses this challenge, achieving both fast
denoising and high quality image restoration. Built upon the Noise2Noise
training framework-which removes the reliance on clean reference images or
explicit noise modeling-we introduce an innovative multistage denoising
pipeline named Noise2Detail (N2D). During inference, this approach disrupts the
spatial correlations of noise patterns to produce intermediate smooth
structures, which are subsequently refined to recapture fine details directly
from the noisy input. Extensive testing reveals that Noise2Detail surpasses
existing dataset-free techniques in performance, while requiring only a
fraction of the computational resources. This combination of efficiency, low
computational cost, and data-free approach make it a valuable tool for
biomedical imaging, overcoming the challenges of scarce clean training data-due
to rare and complex imaging modalities-while enabling fast inference for
practical use.

</details>


### [66] [Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2510.15615)
*Shuchang Lyu,Qi Zhao,Zheng Zhou,Meng Li,You Zhou,Dingding Yao,Guangliang Cheng,Huiyu Zhou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文为遥感领域基于深度学习的领域自适应技术做了全面综述，系统梳理了方法分类、算法、数据集及研究进展，并指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 遥感应用广泛，但源域与目标域差异导致知识迁移困难，因此研究领域自适应以促进遥感自动化与智能化应用。

Method: 全面介绍与归纳遥感领域自适应的基础知识、关键概念及数学符号，按任务类型、输入模式、监督范式和算法细粒度对现有方法进行系统整理和分类，并总结常用数据集和最新方法性能。

Result: 系统梳理了领域自适应主要方法、常见数据集及研究进展，较以往综述涵盖范围更广，结构更系统全面。

Conclusion: 本综述为遥感领域自适应研究提供了全景式参考，有助于加深理解、启发创新，并指导后续研究。

Abstract: Domain adaptation is a crucial and increasingly important task in remote
sensing, aiming to transfer knowledge from a source domain a differently
distributed target domain. It has broad applications across various real-world
applications, including remote sensing element interpretation, ecological
environment monitoring, and urban/rural planning. However, domain adaptation in
remote sensing poses significant challenges due to differences in data, such as
variations in ground sampling distance, imaging modes from various sensors,
geographical landscapes, and environmental conditions. In recent years, deep
learning has emerged as a powerful tool for feature representation and
cross-domain knowledge transfer, leading to widespread adoption in remote
sensing tasks. In this paper, we present a comprehensive survey of significant
advancements in deep learning based domain adaptation for remote sensing. We
first introduce the preliminary knowledge to clarify key concepts, mathematical
notations, and the taxonomy of methodologies. We then organize existing
algorithms from multiple perspectives, including task categorization, input
mode, supervision paradigm, and algorithmic granularity, providing readers with
a structured understanding of the field. Next, we review widely used datasets
and summarize the performance of state-of-the-art methods to provide an
overview of current progress. We also identify open challenges and potential
directions to guide future research in domain adaptation for remote sensing.
Compared to previous surveys, this work addresses a broader range of domain
adaptation tasks in remote sensing, rather than concentrating on a few
subfields. It also presents a systematic taxonomy, providing a more
comprehensive and organized understanding of the field. As a whole, this survey
can inspire the research community, foster understanding, and guide future work
in the field.

</details>


### [67] [Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation](https://arxiv.org/abs/2510.15666)
*Lei Shi,Gang Li,Junxing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种只需四个极点标注即可实现医学图像弱监督分割的新方法，在保持高分割性能的同时极大降低了标注成本。


<details>
  <summary>Details</summary>
Motivation: 全监督医学图像分割对像素级标注的依赖极大增加了时间与经济负担，因此亟需减少标注点数但仍能保持精准分割的方法。

Method: 方法利用四个极点坐标生成边界框，并将其作为Segment Anything Model 2 (SAM2) 的提示，得到初始伪标签。之后使用增强的特征引导极点掩码算法（FGEPM）结合不确定性估计不断优化分割边界；训练阶段引入了不确定性感知尺度一致性损失和框对齐损失确保空间和边界精确性。

Result: 在两个人工智能乳腺超声公开数据集（BUSI和UNS）上验证，结果显示该方案性能可与甚至优于全监督方法，同时极大降低了人工标注工作量。

Conclusion: 该弱监督分割框架不仅提升了医学超声图像分割的实用性，也为低标注成本的医学图像分割方案提供了有效实践，为相关领域的应用和研究奠定基础。

Abstract: Automatic medical image segmentation is a fundamental step in computer-aided
diagnosis, yet fully supervised approaches demand extensive pixel-level
annotations that are costly and time-consuming. To alleviate this burden, we
propose a weakly supervised segmentation framework that leverages only four
extreme points as annotation. Specifically, bounding boxes derived from the
extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to
generate reliable initial pseudo labels. These pseudo labels are progressively
refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,
which incorporates Monte Carlo dropout-based uncertainty estimation to
construct a unified gradient uncertainty cost map for boundary tracing.
Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a
box alignment loss are introduced to ensure spatial consistency and precise
boundary alignment during training. Extensive experiments on two public
ultrasound datasets, BUSI and UNS, demonstrate that our method achieves
performance comparable to, and even surpassing fully supervised counterparts
while significantly reducing annotation cost. These results validate the
effectiveness and practicality of the proposed weakly supervised framework for
ultrasound image segmentation.

</details>


### [68] [Valeo Near-Field: a novel dataset for pedestrian intent detection](https://arxiv.org/abs/2510.15673)
*Antonyo Musabini,Rachid Benmokhtar,Jagdish Bhanushali,Victor Galizzi,Bertrand Luvison,Xavier Perrotton*

Main category: cs.CV

TL;DR: 本文提出了一个新的多模态数据集，用于检测在自动驾驶车辆附近行走的行人意图，并提供了基线评测和研究方向建议。


<details>
  <summary>Details</summary>
Motivation: 当前智能车辆对于识别和预测行人意图的能力仍有限，缺乏高质量、多模态、可真实反映复杂环境的行人动作数据集，限制了相关算法的发展和实际应用。

Method: 作者采集了跨越多种真实场景的同步多模态数据，包括鱼眼摄像头、激光雷达、超声波传感器读数和基于动捕的3D人体姿态。数据集精确注释了与相机同步的3D关节点位置和激光雷达提取的3D行人位置，并配套提供评测指标、基线算法和部分数据公开。

Result: 该数据集实现了高精度的3D行人位置与姿态注释，并能有效支持感知类算法的准确性、效率和在嵌入式系统上的可扩展性测试。论文还以自定义神经网络给出了基准性能，并分析了数据集在传感器遮挡、动态环境和硬件约束下的表现。

Conclusion: 该数据集是支持行人检测、3D姿态估计及4D轨迹/意图预测领域算法研发的独特资源，为智能车辆在近距离行人场景下的能力提升提供基础，并将推动该领域数据集与算法的持续发展。

Abstract: This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.

</details>


### [69] [Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI](https://arxiv.org/abs/2510.15684)
*Gerard Comas-Quiles,Carles Garcia-Cabrera,Julia Dietlmeier,Noel E. O'Connor,Ferran Marques*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer的新型无监督自编码器用于脑MRI肿瘤检测和分割，仅以健康大脑MRI进行训练，无需手工标注数据，结果在多个肿瘤类型上表现出较高的检测率和合理的分割性能。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI分割依赖于大量标注数据，但标注成本高且数据难以获取，无监督方法可减少对标注数据的依赖，提升方法的可扩展性。

Method: 作者提出了一种多模态Vision Transformer自编码器（MViT-AE），在健康MRI上训练，通过重建误差进行肿瘤检测和定位。同时创新性地融合了多模态早-晚融合策略和采用Segment Anything Model (SAM)进行后处理以改进分割结果。

Result: 在BraTS-GoAT 2025 Lighthouse数据集上，方法在不同肿瘤分区（Whole Tumor/Tumor Core/Enhancing Tumor）上的Dice系数分别为0.437、0.316和0.350，验证集上的异常检测率达到89.4%。

Conclusion: Transformer-based无监督模型在缺乏标注数据情况下，能作为高效、可扩展的神经肿瘤影像工具，具备实际临床应用价值。

Abstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.

</details>


### [70] [Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis](https://arxiv.org/abs/2510.15710)
*Junzhi Ning,Wei Li,Cheng Tang,Jiashi Lin,Chenglong Ma,Chaoyang Zhang,Jiyao Liu,Ying Chen,Shujian Gao,Lihao Liu,Yuandong Pu,Huihui Xu,Chenhui Gou,Ziyan Huang,Yi Xin,Qi Qin,Zhongying Deng,Diping Song,Bin Fu,Guang Yang,Yuanfeng Ji,Tianbin Li,Yanzhou Su,Jin Ye,Shixiang Tang,Ming Hu,Junjun He*

Main category: cs.CV

TL;DR: 本文提出了UniMedVL，一种医学多模态统一模型，能够同时进行医疗图像理解和生成，显著提升了多项医学视觉-语言任务的表现。


<details>
  <summary>Details</summary>
Motivation: 目前医学AI系统存在信息割裂的问题，无法同时兼顾图像理解与生成，以及多模态数据的整合与表达，限制了实际诊断流程的统一与高效。

Method: 提出Observation-Knowledge-Analysis (OKA)范式：观测层利用新构建的UniMed-5M多模态配对数据集，知识层采用渐进式课程学习进行多模态医学知识引导，分析层实现了统一的多任务架构UniMedVL，支持图像理解与生成同时在一个模型中进行。

Result: UniMedVL在五个医学图像理解基准上取得优异成绩，且在八种医学影像生成上与专用模型表现持平，通过统一架构还能实现跨任务知识共享，提高模型整体表现。

Conclusion: 将传统分立的医学图像理解和生成功能整合到一个统一模型架构中，不仅提升单项任务效果，还促进多模态任务间的协同优化，为医学AI提供了更强的通用能力。

Abstract: Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.

</details>


### [71] [DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification](https://arxiv.org/abs/2510.15725)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 该论文针对摄像机运动分类模型在处理历史档案片段时的性能退化问题，提出并测试了一种改进的模型DGME-T。


<details>
  <summary>Details</summary>
Motivation: 现有的摄像机运动分类模型大多在现代高质量视频上表现良好，但在处理噪声大、帧丢失和低对比度的历史影片时效果显著下降，因此需要提升针对劣质视频的健壮性。

Method: 作者整合了两个现代数据集和重新构建了历史影片收藏集，形成统一基准，并提出了DGME-T方法。DGME-T基于Video Swin Transformer，通过可学习的归一化后融合层注入方向性网格运动编码（基于光流），以增强模型对运动的感知能力。

Result: DGME-T在现代视频上的准确率从81.78%提升到86.14%，macro F1从82.08%提升到87.81%；在二战历史片段上准确率从83.43%提升到84.62%，macro F1从81.72%提升到82.63%。另外，通过在现代数据上进行中间微调，历史片段分析性能可额外提升5个百分点。

Conclusion: 结构化的运动先验与Transformer架构具有互补优势。即使是简单、精心设计的运动头也能显著提升退化历史影片的分析鲁棒性。

Abstract: Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.

</details>


### [72] [Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/abs/2510.15742)
*Qingyan Bai,Qiuyu Wang,Hao Ouyang,Yue Yu,Hanlin Wang,Wen Wang,Ka Leong Cheng,Shuailei Ma,Yanhong Zeng,Zichen Liu,Yinghao Xu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于基于指令的视频编辑的大规模数据生成和模型训练框架，并构建了Ditto-1M数据集与新模型Editto，实现了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 基于指令的视频编辑领域发展受限于缺乏高质量、大规模训练数据，限制了内容创作的民主化和模型能力的提升。

Method: 提出了Ditto框架，包含一个新颖的数据生成流水线，通过结合领先的图像编辑器的多样性和上下文相关的视频生成器，生成具备丰富编辑类型和高质量的视频样本。通过高效的蒸馏模型结构与时间增强器，兼顾计算效率与时间一致性，并借助智能体自动生成多样化指令并进行严格质量筛选，从而可自动扩展数据生成。最终构建了包含100万条高质量编辑样例的数据集Ditto-1M，并采用课程学习训练模型Editto。

Result: 实验表明，Editto模型在理解和执行复杂视频编辑指令方面表现优异，达到了当前领域的最新水平。

Conclusion: 通过Ditto框架和大规模数据集的构建，极大推动了基于指令的视频编辑发展，为内容创作自动化与智能化提供了有力工具。

Abstract: Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading image editor with an
in-context video generator, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, distilled model architecture augmented by
a temporal enhancer, which simultaneously reduces computational overhead and
improves temporal coherence. Finally, to achieve full scalability, this entire
pipeline is driven by an intelligent agent that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a curriculum learning strategy. The results demonstrate
superior instruction-following ability and establish a new state-of-the-art in
instruction-based video editing.

</details>


### [73] [SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://arxiv.org/abs/2510.15749)
*Haoran Wang,Bo Zhao,Jinghui Wang,Hanzhang Wang,Huan Yang,Wei Ji,Hao Liu,Xinyan Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种内容感知布局生成的新范式SEGA，并在多个数据集上达到了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前内容感知布局生成方法通常采用单步推理框架，缺乏自我反馈修正机制，导致在复杂元素布局规划任务上失败率较高。作者希望通过解决这一问题提升布局生成的表现。

Method: 提出SEGA（Stepwise Evolution Paradigm for Content-Aware Layout Generation），采用分层推理和粗到细的策略：先由一个粗层级模块对布局进行初步估计，然后细化模块根据粗略结果进一步优化，加入布局设计原则作为先验知识，提升规划能力。此外，作者还构建了一个大规模海报数据集GenPoster-100K。

Result: 所提方法在多个基准数据集上取得了最佳的性能，表现优于现有方法。

Conclusion: 分步进化、分层自我修正的内容感知布局生成框架能够有效提升复杂布局生成的准确性和和谐美观程度，具有较高的实际应用价值。

Abstract: In this paper, we study the content-aware layout generation problem, which
aims to automatically generate layouts that are harmonious with a given
background image. Existing methods usually deal with this task with a
single-step reasoning framework. The lack of a feedback-based self-correction
mechanism leads to their failure rates significantly increasing when faced with
complex element layout planning. To address this challenge, we introduce SEGA,
a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.
Inspired by the systematic mode of human thinking, SEGA employs a hierarchical
reasoning framework with a coarse-to-fine strategy: first, a coarse-level
module roughly estimates the layout planning results; then, another refining
module performs fine-level reasoning regarding the coarse planning results.
Furthermore, we incorporate layout design principles as prior knowledge into
the model to enhance its layout planning ability. Besides, we present
GenPoster-100K that is a new large-scale poster dataset with rich
meta-information annotation. The experiments demonstrate the effectiveness of
our approach by achieving the state-of-the-art results on multiple benchmark
datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/

</details>


### [74] [NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation](https://arxiv.org/abs/2510.15752)
*Yitong Sun,Yao Huang,Ruochen Zhang,Huanran Chen,Shouwei Ruan,Ranjie Duan,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文针对文本生成图像（T2I）模型在遇到隐性性暗示内容时容易生成不当图片的问题，提出了一种新颖的基于噪声的检测与缓解框架NDM，以在不损失模型生成质量的前提下，准确检测并抑制隐性有害内容。


<details>
  <summary>Details</summary>
Motivation: 目前T2I扩散模型在生成过程中，常因潜在的性暗示提示词而产生不适当内容，这些提示词往往并非直接或明显，仅凭现有显性检测手段难以识别。而目前的微调方法虽然一定程度有效，但会降低模型本身的生成表现，因此急需一种能精准识别隐性不当内容且不损伤生成效果的方法。

Method: 作者提出了NDM框架，包括两个主要创新：一是基于初阶段预测噪声的分离性，设计出一种噪声检测方法，可以高效准确识别隐性恶意内容；二是提出噪声增强的自适应负向引导机制，通过优化初始噪声并抑制显著区域注意力，从而提升对性相关生成内容的抑制效果。

Result: NDM框架在包含自然和对抗样本的数据集上进行了实验验证，结果显示NDM在检测和缓解隐性有害内容方面均优于现有SOTA方法（如SLD、UCE和RECE等）。

Conclusion: NDM作为第一个基于噪声驱动的T2I隐性恶意内容检测与缓解框架，有效解决了目前模型在隐性不当内容防控上的难题，且在不损害生成质量的情况下，显著提升了检测与抑制能力，对相关领域有重要的实际意义。

Abstract: Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.

</details>


### [75] [Semantic segmentation with coarse annotations](https://arxiv.org/abs/2510.15756)
*Jort de Jong,Mike Holenderski*

Main category: cs.CV

TL;DR: 本文提出了一种用于语义分割的新正则化方法，尤其适用于仅能获取粗糙标注时，通过鼓励解码后的分割结果与SLIC超像素对齐，提升边界分割效果。


<details>
  <summary>Details</summary>
Motivation: 当高精度像素级标注难以获得时，通常只能获取粗糙标注。粗糙标注未能精确标出类别边界，导致模型分割质量下降。如何在有限、粗糙标注条件下提升模型对类别边界的对齐性能，是本文研究的主要动机。

Method: 针对编码器-解码器架构的分割模型，作者提出了一种基于超像素的上采样正则化方法。该方法利用与实际标注无关的SLIC超像素（依据像素颜色、位置生成），鼓励分割输出中的像素区域与超像素对齐。该方法被集成到FCN-16网络，并在SUIM、Cityscapes和PanNuke数据集上验证其效果。

Result: 实验结果表明，在使用粗糙标注训练的情景下，所提方法显著提升了分割边界的召回率（boundary recall），相较于现有主流方法更优。

Conclusion: 即使在仅有粗糙标注数据时，超像素正则化方法能有效提升分割模型在边界上的表现，对相关实际应用具有重要意义。

Abstract: Semantic segmentation is the task of classifying each pixel in an image.
Training a segmentation model achieves best results using annotated images,
where each pixel is annotated with the corresponding class. When obtaining fine
annotations is difficult or expensive, it may be possible to acquire coarse
annotations, e.g. by roughly annotating pixels in an images leaving some pixels
around the boundaries between classes unlabeled. Segmentation with coarse
annotations is difficult, in particular when the objective is to optimize the
alignment of boundaries between classes. This paper proposes a regularization
method for models with an encoder-decoder architecture with superpixel based
upsampling. It encourages the segmented pixels in the decoded image to be
SLIC-superpixels, which are based on pixel color and position, independent of
the segmentation annotation. The method is applied to FCN-16 fully
convolutional network architecture and evaluated on the SUIM, Cityscapes, and
PanNuke data sets. It is shown that the boundary recall improves significantly
compared to state-of-the-art models when trained on coarse annotations.

</details>


### [76] [QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion](https://arxiv.org/abs/2510.15761)
*Denis Rychkovskiy*

Main category: cs.CV

TL;DR: QSilk是一种用于潜变量扩散模型的轻量级、实时稳定化层，可提升高频细节和抑制异常激活，易于集成，适用于高分辨率渲染，并无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有潜变量扩散模型在生成高分辨率或低步数图像时，常出现高频细节丢失与偶发激活峰值（异常值），影响图像质量。为提升输出的清晰度与稳定性，作者提出需实时、轻量的方法以控制这些问题。

Method: QSilk包含两部分：(1)每个样本微限制模块（micro clamp），用于轻度约束极端值但不损失纹理细节；(2)自适应分位数剪裁（AQClip），根据区域自适应调整允许的数值范围，该方法可基于局部统计或注意力机制（使用模型置信度）自适应工作。QSilk无需训练或微调，直接集成于渲染流程。

Result: 集成QSilk到CADE 2.5渲染流程后，可以在低采样步数和超高分辨率下获得更干净、更锐利的渲染结果，且计算开销极低。对SD/SDXL系列骨干网络均有效，与CFG/Rescale联用可略微提高指导强度而不会产生伪影。

Conclusion: QSilk作为一个无需训练、操作简单的稳定化层，对多种扩散骨干网络和高分辨率场景表现出持续的定性提升，为实际大规模生成应用带来便利和画质提升。

Abstract: We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.

</details>


### [77] [Towards more holistic interpretability: A lightweight disentangled Concept Bottleneck Model](https://arxiv.org/abs/2510.15770)
*Gaoxiang Huang,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: 提出了一种轻量级的Disentangled Concept Bottleneck Model（LDCBM），提升了可解释性和鲁棒性，并在多个数据集上取得比现有方法更好的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型（CBM）存在输入到概念映射的偏差和有限的可控性，这限制了其实际应用价值，并削弱了基于概念方法的责任归属能力。

Method: 作者提出LDCBM，通过自动将视觉特征分组成有语义的成分（无需区域标注），并引入了filter grouping loss和联合概念监督的方法，提高视觉模式和概念之间的对齐度。

Result: 在三个不同的数据集上，LDCBM在概念和类别的准确率上均优于以往的CBM，在可解释性和分类性能上都有提升。

Conclusion: LDCBM能够更好地将概念与视觉证据联系起来，解决了以往模型的根本缺陷，提高了可解释AI的可靠性。

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by predicting
human-understandable concepts as intermediate representations. However,
existing CBMs often suffer from input-to-concept mapping bias and limited
controllability, which restricts their practical value, directly damage the
responsibility of strategy from concept-based methods. We propose a lightweight
Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual
features into semantically meaningful components without region annotation. By
introducing a filter grouping loss and joint concept supervision, our method
improves the alignment between visual patterns and concepts, enabling more
transparent and robust decision-making. Notably, Experiments on three diverse
datasets demonstrate that LDCBM achieves higher concept and class accuracy,
outperforming previous CBMs in both interpretability and classification
performance. By grounding concepts in visual evidence, our method overcomes a
fundamental limitation of prior models and enhances the reliability of
interpretable AI.

</details>


### [78] [Controlling the image generation process with parametric activation functions](https://arxiv.org/abs/2510.15778)
*Ilia Pavlov*

Main category: cs.CV

TL;DR: 本文提出一种通过用户交互直接修改生成网络内部激活函数的方法，使用户能以更可解释的方式控制图像生成模型的输出。


<details>
  <summary>Details</summary>
Motivation: 尽管图像生成模型越来越强大，当前缺乏让用户能够可解释、交互式地操作这些模型内部机制的工具。作者致力于弥补这一空白。

Method: 提出一个系统，让用户能够替换生成网络中的激活函数为可调参数化函数，并能直接设置参数，从而实时探索激活函数对网络输出的影响。该方法在StyleGAN2（FFHQ数据集）和BigGAN（ImageNet数据集）上进行了测试。

Result: 实验展示通过调整激活函数及其参数，用户可以对生成结果进行有效控制和探索，提升了输出的可解释性和交互性。

Conclusion: 该系统为图像生成模型的可解释、可控性提供了新思路，把复杂模型内部机制变得透明、可实验，为后续相关研究提供借鉴。

Abstract: As image generative models continue to increase not only in their fidelity
but also in their ubiquity the development of tools that leverage direct
interaction with their internal mechanisms in an interpretable way has received
little attention In this work we introduce a system that allows users to
develop a better understanding of the model through interaction and
experimentation By giving users the ability to replace activation functions of
a generative network with parametric ones and a way to set the parameters of
these functions we introduce an alternative approach to control the networks
output We demonstrate the use of our method on StyleGAN2 and BigGAN networks
trained on FFHQ and ImageNet respectively.

</details>


### [79] [ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection](https://arxiv.org/abs/2510.15783)
*Haowei Zhu,Tianxiang Pan,Rui Qin,Jun-Hai Yong,Bin Wang*

Main category: cs.CV

TL;DR: 文章提出了一种名为ReCon的新型数据增强框架，通过改进生成模型（特别是结构可控的扩散模型）用于目标检测任务的数据生成。


<details>
  <summary>Details</summary>
Motivation: 大规模高质量数据集对于训练鲁棒的感知模型至关重要，但数据标注开销巨大。现有生成式增强方法存在后处理复杂、需海量微调、位置语义错配等问题，难以高效生成高质量数据。

Method: ReCon在扩散采样过程中引入区域引导的校正机制，并借助预训练感知模型的反馈纠正生成图像的错误区域。同时，提出区域对齐的交叉注意力机制，强化图像区域与文本提示的空间语义一致性，提升生成数据的语义和视觉质量。

Result: 大量实验表明，ReCon生成的数据在不同数据集、主干结构和规模下均显著提升了数据质量和模型可训练性，并带来持续的性能提升。

Conclusion: ReCon有效提升了结构可控生成模型的增强能力，为目标检测等任务提供高质量合成数据，实验证实其优越性，对未来数据增强和生成模型方向具备积极价值。

Abstract: The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .

</details>


### [80] [ERNet: Efficient Non-Rigid Registration Network for Point Sequences](https://arxiv.org/abs/2510.15800)
*Guangzhao He,Yuxi Xiao,Zhen Xu,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的点云非刚性配准方法ERNet，解决了非刚性变形序列中配准的局部最优和误差累积问题，显著提升了精度与速度。


<details>
  <summary>Details</summary>
Motivation: 非刚性物体在点云序列下的配准由于目标函数非凸性（导致易陷入局部最优，特别是在噪声或不完整数据下）以及长序列误差累积，长期以来难以实现精准与稳定的形状跟踪。

Method: 本文提出ERNet，一个基于大规模变形数据集训练的高效前馈深度网络模型。方法核心是两阶段流程：第一阶段估计每帧的粗糙变形图节点，实现鲁棒的初始配准；第二阶段通过滑动窗口精细优化节点轨迹，高效利用时序信息，并能处理噪声和缺失。

Result: 在DeformingThings4D和D-FAUST两个数据集上，ERNet在精度上超越现有最优方法，并在速度上提升4倍以上。

Conclusion: ERNet不仅提升了非刚性点云配准的精度，也极大加快计算速度，能稳定应用于含噪声和部分缺失的长序列，优于当前主流方法。

Abstract: Registering an object shape to a sequence of point clouds undergoing
non-rigid deformation is a long-standing challenge. The key difficulties stem
from two factors: (i) the presence of local minima due to the non-convexity of
registration objectives, especially under noisy or partial inputs, which
hinders accurate and robust deformation estimation, and (ii) error accumulation
over long sequences, leading to tracking failures. To address these challenges,
we introduce to adopt a scalable data-driven approach and propose ERNet, an
efficient feed-forward model trained on large deformation datasets. It is
designed to handle noisy and partial inputs while effectively leveraging
temporal information for accurate and consistent sequential registration. The
key to our design is predicting a sequence of deformation graphs through a
two-stage pipeline, which first estimates frame-wise coarse graph nodes for
robust initialization, before refining their trajectories over time in a
sliding-window fashion. Extensive experiments show that our proposed approach
(i) outperforms previous state-of-the-art on both the DeformingThings4D and
D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the
previous best, offering significant efficiency improvement.

</details>


### [81] [VISTA: A Test-Time Self-Improving Video Generation Agent](https://arxiv.org/abs/2510.15831)
*Do Xuan Long,Xingchen Wan,Hootan Nakhost,Chen-Yu Lee,Tomas Pfister,Sercan Ö. Arık*

Main category: cs.CV

TL;DR: 该论文提出了一种名为VISTA的新型多智能体系统，通过不断优化用户提示词，自动提升文本生成视频的质量和与用户意图的契合度。


<details>
  <summary>Details</summary>
Motivation: 当前的文本生成视频方法对提示词依赖极高，而现有的测试时优化方法难以应对视频的多样性和复杂性，因此亟需一种能自动优化提示词、提升视频生成效果的方法。

Method: VISTA方法包括多智能体协作：首先将用户意图分解为结构化的时间序列计划，生成多个视频后通过对比赛选出最佳视频，再由专门的视觉、音频和上下文智能体进行点评。最后，一个推理智能体综合反馈，重写并优化下一轮生成的视频提示词，形成自我改进闭环。

Result: VISTA在单场景和多场景生成任务中均表现出色，相较以往方法有更稳定的收益提升；与主流基线方法对比，VISTA最高获得60%的对比胜率。人类评审在66.4%的评测中更偏好VISTA生成的视频。

Conclusion: VISTA实现了文本生成视频任务中提示词的自动迭代优化，有效提升了生成视频的质量和与用户意图的契合度，优于现有方法。

Abstract: Despite rapid advances in text-to-video synthesis, generated video quality
remains critically dependent on precise user prompts. Existing test-time
optimization methods, successful in other domains, struggle with the
multi-faceted nature of video. In this work, we introduce VISTA (Video
Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously
improves video generation through refining prompts in an iterative loop. VISTA
first decomposes a user idea into a structured temporal plan. After generation,
the best video is identified through a robust pairwise tournament. This winning
video is then critiqued by a trio of specialized agents focusing on visual,
audio, and contextual fidelity. Finally, a reasoning agent synthesizes this
feedback to introspectively rewrite and enhance the prompt for the next
generation cycle. Experiments on single- and multi-scene video generation
scenarios show that while prior methods yield inconsistent gains, VISTA
consistently improves video quality and alignment with user intent, achieving
up to 60% pairwise win rate against state-of-the-art baselines. Human
evaluators concur, preferring VISTA outputs in 66.4% of comparisons.

</details>


### [82] [Neuro-Symbolic Spatial Reasoning in Segmentation](https://arxiv.org/abs/2510.15841)
*Jiayi Lin,Jiabo Huang,Shaogang Gong*

Main category: cs.CV

TL;DR: 本文提出了RelateSeg方法，将神经符号空间推理引入开放词汇语义分割（OVSS），通过显式空间关系约束提升对多类场景下未见对象的分割性能。


<details>
  <summary>Details</summary>
Motivation: 以往基于视觉-语言模型（VLM）的OVSS方法难以理解和利用场景中物体的空间关系，导致对复杂多对象场景分割效果有限。因此，亟需引入空间推理能力加强分割泛化能力。

Method: 作者提出RelateSeg，将空间关系（如“猫在人的右侧”）以一阶逻辑公式形式嵌入神经网络结构。具体做法是每个像素同时预测语义类别和空间伪类别，通过模糊逻辑放松将关系约束融入端到端学习，且只引入了一个辅助损失。

Result: 在四个基准数据集上，RelateSeg在平均mIoU指标上取得了SOTA的性能，尤其是在包含多类别物体的图像上表现突出。

Conclusion: NeSy空间推理有效提升了OVSS分割表现，RelateSeg方法无需增加额外参数，仅利用辅助损失实现端到端空间关系建模，验证了该技术路线的有效性。

Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.

</details>


### [83] [3DPR: Single Image 3D Portrait Relight using Generative Priors](https://arxiv.org/abs/2510.15846)
*Pramod Rao,Abhimitra Meka,Xilong Zhou,Gereon Fox,Mallikarjun B R,Fangneng Zhan,Tim Weyrich,Bernd Bickel,Hanspeter Pfister,Wojciech Matusik,Thabo Beeler,Mohamed Elgharib,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出了一种名为3DPR的基于生成模型先验的人脸单图像3D重光照方法，能真实还原面部细节与光照效应。


<details>
  <summary>Details</summary>
Motivation: 以单张人像图片生成高质量、任意光照和视角下的人脸图像具有极高的学术和产业价值，但传统图形学方法在建模和假设上有局限，难以还原真实细节。

Method: (1) 构建包含139名受试者的大规模多视角4K OLAT (单光源) 数据集；(2) 利用预训练的生成式人头模型，将输入照片嵌入其潜空间；(3) 设计三平面(triplane)反射率网络，在潜空间内利用少量光照图像训练得到高质量面部反射信息；(4) 通过合成生成的OLAT图像和给定环境光照，实现真实的重光照渲染。

Result: 3DPR不仅能在身份还原和高频光照细节（如高光、自阴影、皮下散射等）方面超过现有方法，还有更强的泛化能力。定量和定性实验结果显示其优越性。

Conclusion: 引入生成式先验和高质量光照数据集，3DPR显著提升了单张肖像的可控3D重光照渲染效果，在真实感和细节还原上领先同类方法。

Abstract: Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/

</details>


### [84] [Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt](https://arxiv.org/abs/2510.15849)
*Joongwon Chae,Lihui Luo,Xi Yuan,Dongmei Yu,Zhenglin Chen,Lian Zhang,Peiwu Qin*

Main category: cs.CV

TL;DR: 提出了Memory-SAM，一种无需训练和人工提示词的舌象分割方法，通过自动从小型样例库中检索生成有效提示，实现高精度分割。


<details>
  <summary>Details</summary>
Motivation: 中医舌象分析中舌体分割非常重要，但传统监督模型需要大量标注数据，而现有SAM类模型依赖人工提示，应用受限。需要一种无需大规模数据和人工干预的分割方法。

Method: Memory-SAM方法基于DINOv3特征和FAISS检索，从小型样例库自动检索与待分割图片相似的案例，将对应区域浓缩为前景/背景点提示词，无需人工点击或模型微调即可驱动SAM2模型完成分割。

Result: 在600张专家标注图像（300张标准、300张野外采集）上的混合测试中，Memory-SAM的mIoU达到0.9863，远超FCN（0.8188）和基线方法（0.1839）。受控数据下方法间差异小，但在真实环境下明显优于其他方法。

Conclusion: Memory-SAM通过自动检索与提示，实现了对不规则舌体边界的高效、稳健分割，且数据需求低，对实际舌象分析具有重要意义。

Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised
models require large annotated datasets, while SAM-family models remain
prompt-driven. We present Memory-SAM, a training-free, human-prompt-free
pipeline that automatically generates effective prompts from a small memory of
prior cases via dense DINOv3 features and FAISS retrieval. Given a query image,
mask-constrained correspondences to the retrieved exemplar are distilled into
foreground/background point prompts that guide SAM2 without manual clicks or
model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,
300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,
surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On
controlled data, ceiling effects above 0.98 make small differences less
meaningful given annotation variability, while our method shows clear gains
under real-world conditions. Results indicate that retrieval-to-prompt enables
data-efficient, robust segmentation of irregular boundaries in tongue imaging.
The code is publicly available at https://github.com/jw-chae/memory-sam.

</details>


### [85] [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/abs/2510.15857)
*Jiuhai Chen,Le Xue,Zhiyang Xu,Xichen Pan,Shusheng Yang,Can Qin,An Yan,Honglu Zhou,Zeyuan Chen,Lifu Huang,Tianyi Zhou,Junnan Li,Silvio Savarese,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: BLIP3o-NEXT 是 BLIP3 系列的新一代全开源基础模型，将文本到图像生成及图像编辑统一于一种架构，并在多个基准测试上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成和图像编辑的性能仍有提升空间，且整合二者于同一模型架构是视觉生成领域的重要挑战。作者希望构建性能更优、可开源并支持多任务的视觉生成基础模型，突破原生图像生成的前沿。

Method: 提出基于 Autoregressive + Diffusion 混合架构：先用自回归模型基于多模态输入生成离散图像 token，并将中间状态作为扩散模型的条件信号，生成高保真图像。此外，结合强化学习后处理和数据引擎优化提升指令追随与生成一致性。

Result: BLIP3o-NEXT 在多项文本到图像和图像编辑基准测试中超越现有模型，展示出更强的图像生成和编辑能力，尤其在生成的连贯性和细节表现上达到新高度。

Conclusion: BLIP3o-NEXT 有效统一了文本到图像生成和图像编辑任务，为开源视觉生成模型设定了新标杆，强调了架构效率、数据质量与模型泛化能力的重要性。

Abstract: We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.

</details>


### [86] [BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models](https://arxiv.org/abs/2510.15866)
*Kaushitha Silva,Mansitha Eashwara,Sanduni Ubayasiri,Ruwan Tennakoon,Damayanthi Herath*

Main category: cs.CV

TL;DR: 提出BiomedXPro方法，通过大模型优化生成多样且可解释的自然语言prompt，提高生物医学视觉-语言模型的透明度与诊断能力。实验证实在多个基准数据集上性能优越，尤其在少样本任务中效果突出，同时与临床特征高度对齐，增强了模型可验证性与可信度。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学视觉-语言模型的prompt优化方法往往产生不可解释的潜在向量或单一文本提示，难以反映临床诊断需要综合多个观察的复杂性，导致其在实际医疗中的应用受限。

Method: 作者提出BiomedXPro，一种基于进化框架的方法，利用大语言模型同时作为知识提取器和自适应优化器，自动生成多样、可解释的疾病诊断自然语言prompt对，并进行实验验证。

Result: BiomedXPro在多个生物医学基准上均优于现有最先进的prompt微调方法，尤其在数据稀缺的few-shot设定下表现突出，生成的prompt与统计显著的临床要素紧密匹配。

Conclusion: BiomedXPro通过生成多样且可解释的提示，为模型预测提供可验证依据，有助于开发更值得信赖、临床对齐的智能诊断系统，推动AI在医领域的实际应用。

Abstract: The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.

</details>


### [87] [LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://arxiv.org/abs/2510.15868)
*Shr-Ruei Tsai,Wei-Cheng Chang,Jie-Ying Lee,Chih-Hai Su,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该论文提出了LightsOut，一种用于消除镜头光晕、提升单幅图像光晕去除（SIFR）性能的基于扩散模型的框架，能够重构画面外的光源，提高现有SIFR算法在复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 镜头光晕会严重影响图像质量，妨碍计算机视觉任务，如目标检测和自动驾驶等。现有的SIFR方法在处理画面外光源不完整或缺失时表现较差，因此亟需有效重构或补全这些光源，提高SIFR效果。

Method: LightsOut采用基于扩散的outpainting框架，结合多任务回归模块和LoRA微调的扩散模型，用于重建画面外的光源，保证重建结果真实且符合物理规律。该方法可作为预处理步骤，与现有SIFR方法无缝集成，无需额外训练。

Result: 实验表明，LightsOut能显著提升多种主流SIFR方法在不同复杂场景下的去光晕效果，且表现稳健，无需对原有模型进行重新训练。

Conclusion: LightsOut作为通用的SIFR预处理方案，有效增强了去光晕性能，可直接应用于多种任务场景，推动相关视觉任务精度提升。

Abstract: Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/

</details>


### [88] [Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery](https://arxiv.org/abs/2510.15869)
*Jie-Ying Lee,Yi-Ruei Liu,Shr-Ruei Tsai,Wei-Cheng Chang,Chung-Ho Wu,Jiewen Chan,Zhenjun Zhao,Chieh Hubert Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种通过结合卫星影像与开放域扩散模型，自动生成大规模、可探索、几何精确的3D城市场景的新方法Skyfall-GS，实现了无需高成本3D标注的城市级3D建模。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模高质量的真实世界3D扫描，训练具备泛化能力的生成模型以合成大规模、高精度的3D城市场景具有很大挑战。

Method: 作者提出利用现有的卫星图像作为输入，获取现实的粗略几何，再借助开放域扩散模型合成高质量细节外观。创新地采用了通过课程化驱动的迭代细化策略，使3D几何与贴图逐步完善和逼真。整个流程无需高成本的3D数据标注。

Result: 实验表明，Skyfall-GS生成的3D城市场景在几何一致性和贴图真实感方面均优于当前主流方法，并支持实时沉浸式3D探索。

Conclusion: Skyfall-GS实现了无需高昂成本即可制作城市街区级3D场景，并且场景细节和沉浸体验上均有显著提升，为大规模3D场景生成提供了新思路。

Abstract: Synthesizing large-scale, explorable, and geometrically accurate 3D urban
scenes is a challenging yet valuable task in providing immersive and embodied
applications. The challenges lie in the lack of large-scale and high-quality
real-world 3D scans for training generalizable generative models. In this
paper, we take an alternative route to create large-scale 3D scenes by
synergizing the readily available satellite imagery that supplies realistic
coarse geometry and the open-domain diffusion model for creating high-quality
close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block
scale 3D scene creation framework without costly 3D annotations, also featuring
real-time, immersive 3D exploration. We tailor a curriculum-driven iterative
refinement strategy to progressively enhance geometric completeness and
photorealistic textures. Extensive experiments demonstrate that Skyfall-GS
provides improved cross-view consistent geometry and more realistic textures
compared to state-of-the-art approaches. Project page:
https://skyfall-gs.jayinnn.dev/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective](https://arxiv.org/abs/2510.15007)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Ming-Kun Xie,Biao Liu,Changwei Wang,Lei Feng,Yuheng Jia,Gang Niu,Masashi Sugiyama,Xin Geng*

Main category: cs.CL

TL;DR: 本文提出了三套多标签毒性检测基准数据集，以及基于伪标签的检测方法，显著提升了大语言模型生成内容的毒性检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的毒性检测器主要依赖单标签数据集，无法有效反映现实世界中有害内容的复杂性和多维性，导致检测结果存在偏差，影响可靠性。此外，获取精细的多标签标注代价高昂，严重制约了方法的发展。

Method: 作者构建了三套多标签毒性检测基准（Q-A-MLL、R-A-MLL、H-X-MLL），涵盖15个细粒度毒性类别，并提出基于伪标签的训练方法，理论证明在数据集上利用伪标签训练优于传统的单标签方法，并提出具体检测流程。

Result: 实验证明，作者的方法在多标签毒性检测任务上显著超越了包括GPT-4o和DeepSeek在内的先进基线，提升了大语言模型生成内容的毒性检测准确性和可靠性。

Conclusion: 引入多标签基准和伪标签方法可有效弥补现有毒性检测方式的不足，为LLM内容的安全评估和发展提供了更精细可靠的工具。

Abstract: Large language models (LLMs) have achieved impressive results across a range
of natural language processing tasks, but their potential to generate harmful
content has raised serious safety concerns. Current toxicity detectors
primarily rely on single-label benchmarks, which cannot adequately capture the
inherently ambiguous and multi-dimensional nature of real-world toxic prompts.
This limitation results in biased evaluations, including missed toxic
detections and false positives, undermining the reliability of existing
detectors. Additionally, gathering comprehensive multi-label annotations across
fine-grained toxicity categories is prohibitively costly, further hindering
effective evaluation and development. To tackle these issues, we introduce
three novel multi-label benchmarks for toxicity detection: \textbf{Q-A-MLL},
\textbf{R-A-MLL}, and \textbf{H-X-MLL}, derived from public toxicity datasets
and annotated according to a detailed 15-category taxonomy. We further provide
a theoretical proof that, on our released datasets, training with pseudo-labels
yields better performance than directly learning from single-label supervision.
In addition, we develop a pseudo-label-based toxicity detection method.
Extensive experimental results show that our approach significantly surpasses
advanced baselines, including GPT-4o and DeepSeek, thus enabling more accurate
and reliable evaluation of multi-label toxicity in LLM-generated content.

</details>


### [90] [Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek](https://arxiv.org/abs/2510.15009)
*Enis Oğuz*

Main category: cs.CL

TL;DR: 本研究比较了三种生成式AI（ChatGPT、Gemini、Deepseek）在自动评分包含成语和不包含成语的学生作文中的表现，发现Gemini在一致性与与人类评分者的可靠性方面表现最佳，且在处理成语作文上最接近人类评分。


<details>
  <summary>Details</summary>
Motivation: 生成式AI被越来越多地用于自动作文评分，但其在处理带有成语等高阶修辞特征的文本时能力尚存疑问，故本研究欲评估生成式AI在不同作文类型（含成语/无成语）中的表现，为日后AI作文评分系统的优化提供参考。

Method: 从学生作文语料库中选取348篇作文，分为含多成语和无成语两组。用同一评分标准，要求三种生成式AI模型（ChatGPT、Gemini、Deepseek）分别对所有作文重复评分三次，并与人工评分者的结果进行一致性与偏差分析。

Result: 三种AI模型评分结果高度一致，Gemini与人工评分者的评分一致性最高，且对不同人口群体无评分偏见。在含成语作文的评分上，Gemini模型的评分模式最接近人工评分者。

Conclusion: 生成式AI模型表现出较高的自动作文评分能力，尤其Gemini在处理成语及比喻性语言上有明显优势，是未来自动作文评分系统的有力候选者。

Abstract: The developments in Generative AI technologies have paved the way for
numerous innovations in different fields. Recently, Generative AI has been
proposed as a competitor to AES systems in evaluating student essays
automatically. Considering the potential limitations of AI in processing
idioms, this study assessed the scoring performances of Generative AI models
for essays with and without idioms by incorporating insights from Corpus
Linguistics and Computational Linguistics. Two equal essay lists were created
from 348 student essays taken from a corpus: one with multiple idioms present
in each essay and another with no idioms in essays. Three Generative AI models
(ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists
three times, using the same rubric used by human raters in assigning essay
scores. The results revealed excellent consistency for all models, but Gemini
outperformed its competitors in interrater reliability with human raters. There
was also no detectable bias for any demographic group in AI assessment. For
essays with multiple idioms, Gemini followed a the most similar pattern to
human raters. While the models in the study demonstrated potential for a hybrid
approach, Gemini was the best candidate for the task due to its ability to
handle figurative language and showed promise for handling essay-scoring tasks
alone in the future.

</details>


### [91] [A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling](https://arxiv.org/abs/2510.15081)
*Shiyu Ji,Farnoosh Hashemi,Joice Chen,Juanwen Pan,Weicheng Ma,Hefan Zhang,Sophia Pan,Ming Cheng,Shubham Mohole,Saeed Hassanpour,Soroush Vosoughi,Michael Macy*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型（LLM）自动生成并标注含四种修辞类型（因果、经验、情感、道德）的合成辩论数据，并用这些数据微调分类器。模型在多领域数据上表现优异，也揭示了美总统辩论中情感型修辞的增加。


<details>
  <summary>Details</summary>
Motivation: 传统修辞策略分析高度依赖人工标注，代价高、难以统一且难以扩展，数据集多局限于特定话题与修辞类型，难以开发稳健模型。

Method: 利用LLM自动生成与标注包含四种修辞类型的合成辩论文本数据，随后使用这些数据微调基于transformer的分类器。通过与人工标注数据及外部语料库对比验证模型泛化能力和表现。

Result: 模型在本数据集及多个外部数据集上均取得高性能和良好泛化，且可提升说服力预测任务表现。实际分析发现美国总统辩论中的情感类修辞较认知类修辞显著上升。

Conclusion: 使用LLM生成标注合成数据可有效提升修辞策略识别模型的性能和通用性，同时为传播学等领域带来新的分析工具与洞见。

Abstract: Rhetorical strategies are central to persuasive communication, from political
discourse and marketing to legal argumentation. However, analysis of rhetorical
strategies has been limited by reliance on human annotation, which is costly,
inconsistent, difficult to scale. Their associated datasets are often limited
to specific topics and strategies, posing challenges for robust model
development. We propose a novel framework that leverages large language models
(LLMs) to automatically generate and label synthetic debate data based on a
four-part rhetorical typology (causal, empirical, emotional, moral). We
fine-tune transformer-based classifiers on this LLM-labeled dataset and
validate its performance against human-labeled data on this dataset and on
multiple external corpora. Our model achieves high performance and strong
generalization across topical domains. We illustrate two applications with the
fine-tuned model: (1) the improvement in persuasiveness prediction from
incorporating rhetorical strategy labels, and (2) analyzing temporal and
partisan shifts in rhetorical strategies in U.S. Presidential debates
(1960-2020), revealing increased use of affective over cognitive argument in
U.S. Presidential debates.

</details>


### [92] [Continual Learning via Sparse Memory Finetuning](https://arxiv.org/abs/2510.15103)
*Jessy Lin,Luke Zettlemoyer,Gargi Ghosh,Wen-Tau Yih,Aram Markosyan,Vincent-Pierre Berges,Barlas Oğuz*

Main category: cs.CL

TL;DR: 本文提出稀疏记忆微调方法，通过仅更新高度激活的记忆槽，有效降低了大语言模型在持续学习新知识时的遗忘问题。实验显示相比于全量微调和LoRA，遗忘显著减少。


<details>
  <summary>Details</summary>
Motivation: 大模型上线后难以持续学习主要因为“灾难性遗忘”问题，即新知识的学习会抹除旧知识。产生遗忘的核心原因在于可训练参数在所有任务间共享，导致新旧知识间相互干扰。

Method: 作者提出稀疏记忆微调（sparse memory finetuning），基于稀疏可更新设计的记忆层模型。具体做法为：仅对受到新知识激活较强的记忆槽进行更新，最大限度减少对旧知识表示的影响。将其与全量微调、LoRA（高效参数微调）在两个问答任务上进行对比。

Result: 实验表明，使用全量微调，新知识学习后原有任务（NaturalQuestions）的F1分下降89%；用LoRA也下降71%；而本方法只下降11%，而对于新知识的学习效果与其他方法相当。

Conclusion: 记忆层的稀疏更新大大减轻了灾难性遗忘问题，为大规模语言模型实现持续学习打开了新方向。

Abstract: Modern language models are powerful, but typically static after deployment. A
major obstacle to building models that continually learn over time is
catastrophic forgetting, where updating on new data erases previously acquired
capabilities. Motivated by the intuition that mitigating forgetting is
challenging because trainable parameters are shared across all tasks, we
investigate whether sparse parameter updates can enable learning without
catastrophic forgetting. We introduce sparse memory finetuning, leveraging
memory layer models (Berges et al., 2024), which are sparsely updated by
design. By updating only the memory slots that are highly activated by a new
piece of knowledge relative to usage on pretraining data, we reduce
interference between new knowledge and the model's existing capabilities. We
evaluate learning and forgetting compared to full finetuning and
parameter-efficient finetuning with LoRA on two question answering tasks. We
find that sparse memory finetuning learns new knowledge while exhibiting
substantially less forgetting: while NaturalQuestions F1 drops by 89% after
full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields
only an 11% drop with the same level of new knowledge acquisition. Our results
suggest sparsity in memory layers offers a promising path toward continual
learning in large language models.

</details>


### [93] [Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks](https://arxiv.org/abs/2510.15115)
*Kirill Semenov,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文发现多语言LLM评测基准（如MLAMA）采用简单模板翻译，忽略插入实体的语法和语义，导致不通顺，影响分数解释。用Google Translate和ChatGPT作整句翻译会显著提升知识检索得分。


<details>
  <summary>Details</summary>
Motivation: 现有多语言知识评测基准使用模板翻译，但这往往未考虑语言和实体的语法、语义细节，造成不自然的句子，特别是在形态变化丰富的语言中，影响LLM评测的准确性和可解释性。

Method: 作者以MLAMA数据集中4种斯拉夫语言为例，对比模板翻译版（原MLAMA）和Google Translate、ChatGPT整句翻译版的知识检索得分，并对5种来自不同语系的语言做了类似分析，结合定量和定性的分析结果。

Result: 采用Google Translate和ChatGPT的整句翻译后，知识检索分数显著提升；其原因在于减少了语法错误和措辞不当。其它语系的语言也有类似趋势。

Conclusion: 强烈建议多语言评测任务的数据集制作时，应关注语法性，采用整句翻译，而非简单模板翻译，以获取更高、更可解释的评测结果。相关数据和代码已公开。

Abstract: For multilingual factual knowledge assessment of LLMs, benchmarks such as
MLAMA use template translations that do not take into account the grammatical
and semantic information of the named entities inserted in the sentence. This
leads to numerous instances of ungrammaticality or wrong wording of the final
prompts, which complicates the interpretation of scores, especially for
languages that have a rich morphological inventory. In this work, we sample 4
Slavic languages from the MLAMA dataset and compare the knowledge retrieval
scores between the initial (templated) MLAMA dataset and its sentence-level
translations made by Google Translate and ChatGPT. We observe a significant
increase in knowledge retrieval scores, and provide a qualitative analysis for
possible reasons behind it. We also make an additional analysis of 5 more
languages from different families and see similar patterns. Therefore, we
encourage the community to control the grammaticality of highly multilingual
datasets for higher and more interpretable results, which is well approximated
by whole sentence translation with neural MT or LLM systems. The dataset and
all related code is published at the Github repository:
https://github.com/ZurichNLP/Fluent-mLAMA.

</details>


### [94] [Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis](https://arxiv.org/abs/2510.15125)
*Alexander Brady,Tunazzina Islam*

Main category: cs.CL

TL;DR: 本文提出了一种自动生成可解释话题分类法的端到端框架，用于高效分析社交媒体上的政治内容，并将其应用于2024年美国总统大选前的Meta政治广告。


<details>
  <summary>Details</summary>
Motivation: 社交媒体内容庞大且变化快，难以分析其在政治话语中的作用。传统方法通常依赖专家标注或先验知识，效率低或可扩展性差。作者希望实现高效、可解释、无需领域知识的内容分类与理解。

Method: 提出结合无监督聚类与基于提示的大语言模型自动命名的话题分类法，无需种子集或领域知识，能自动生成语义丰富的话题标签及道德框架维度，并对Meta政治广告大规模语料进行分析。

Result: 该方法揭示了广告话语结构与主题分布：投票与移民广告占支出和浏览量主导，堕胎和选举诚信广告触及人群比例较高。不同行政、主题与资助方展现出明显分化，且议题道德框架有明显差异，如堕胎强调自由/压迫，经济主题融汇多种叙事。

Conclusion: 该框架支持大规模、可解释的政治广告分析，有助于学者、政策制定者和公众理解叙事演变、极化机制及数字政治传播中的道德基础。

Abstract: Social media platforms play a pivotal role in shaping political discourse,
but analyzing their vast and rapidly evolving content remains a major
challenge. We introduce an end-to-end framework for automatically generating an
interpretable topic taxonomy from an unlabeled corpus. By combining
unsupervised clustering with prompt-based labeling, our method leverages large
language models (LLMs) to iteratively construct a taxonomy without requiring
seed sets or domain expertise. We apply this framework to a large corpus of
Meta (previously known as Facebook) political ads from the month ahead of the
2024 U.S. Presidential election. Our approach uncovers latent discourse
structures, synthesizes semantically rich topic labels, and annotates topics
with moral framing dimensions. We show quantitative and qualitative analyses to
demonstrate the effectiveness of our framework. Our findings reveal that voting
and immigration ads dominate overall spending and impressions, while abortion
and election-integrity achieve disproportionate reach. Funding patterns are
equally polarized: economic appeals are driven mainly by conservative PACs,
abortion messaging splits between pro- and anti-rights coalitions, and
crime-and-justice campaigns are fragmented across local committees. The framing
of these appeals also diverges--abortion ads emphasize liberty/oppression
rhetoric, while economic messaging blends care/harm, fairness/cheating, and
liberty/oppression narratives. Topic salience further reveals strong
correlations between moral foundations and issues. Demographic targeting also
emerges. This work supports scalable, interpretable analysis of political
messaging on social media, enabling researchers, policymakers, and the public
to better understand emerging narratives, polarization dynamics, and the moral
underpinnings of digital political communication.

</details>


### [95] [FarsiMCQGen: a Persian Multiple-choice Question Generation Framework](https://arxiv.org/abs/2510.15134)
*Mohammad Heydari Rad,Rezvan Afari,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本文提出了一种新方法FarsiMCQGen，用于自动生成高质量的波斯语选择题，并发布了一个包含10,289道题目的新型数据集。通过结合候选生成、筛选和排序等技术，有效提升了题目的质量。


<details>
  <summary>Details</summary>
Motivation: 传统的多项选择题广泛用于评测学生，但在低资源语言（如波斯语）中，自动生成高质量选择题极具挑战。因此，开发能自动生成波斯语高质量选择题的方法对教育评测具有重要意义。

Method: 作者提出FarsiMCQGen模型，结合了基于Transformer和知识图谱的先进技术，并与规则方法结合，来生成可信的干扰项。整个流程包括候选选项生成、筛选和排序。数据来源于维基百科的常识性知识，并构建了一个规模较大的波斯语选择题数据集。

Result: 提出的模型有效提升了选项的质量，生成的数据集共包含10,289道波斯语选择题。该数据集经多种主流大型语言模型评估，效果良好。

Conclusion: 该方法能为波斯语等低资源语言的选择题自动生成提供有效解决方案，丰富了相关数据集资源，对后续相关研究具有推动作用。

Abstract: Multiple-choice questions (MCQs) are commonly used in educational testing, as
they offer an efficient means of evaluating learners' knowledge. However,
generating high-quality MCQs, particularly in low-resource languages such as
Persian, remains a significant challenge. This paper introduces FarsiMCQGen, an
innovative approach for generating Persian-language MCQs. Our methodology
combines candidate generation, filtering, and ranking techniques to build a
model that generates answer choices resembling those in real MCQs. We leverage
advanced methods, including Transformers and knowledge graphs, integrated with
rule-based approaches to craft credible distractors that challenge test-takers.
Our work is based on data from Wikipedia, which includes general knowledge
questions. Furthermore, this study introduces a novel Persian MCQ dataset
comprising 10,289 questions. This dataset is evaluated by different
state-of-the-art large language models (LLMs). Our results demonstrate the
effectiveness of our model and the quality of the generated dataset, which has
the potential to inspire further research on MCQs.

</details>


### [96] [Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning](https://arxiv.org/abs/2510.15191)
*Junlin Wu,Xianrui Zhong,Jiashuo Sun,Bolian Li,Bowen Jin,Jiawei Han,Qingkai Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法\textsc{Structure-R1}，通过将检索到的内容结构化，以提升大型语言模型的推理表现，显著提升了信息密度和推理效果，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在推理能力上虽有进步，但由于缺乏结构化和显式的领域知识，其表现受限。尽管RAG（检索增强生成）方法能引入外部知识，但大多处理的是非结构化文本，导致信息稀疏，影响推理质量。因此，亟需能够结构化知识来源的新方法。

Method: 提出\textsc{Structure-R1}框架，将检索到的信息进行结构化转化，并通过强化学习自适应地优化内容表达结构。与传统依赖固定模板的方法不同，\textsc{Structure-R1}采用生成式结构，可针对具体任务和查询动态调整结构，并辅以结构自验证机制，提升结果的正确性和自洽性。

Result: 通过七个知识密集型基准数据集实验，\textsc{Structure-R1}在仅用7B参数规模模型情况下，也能达到甚至匹敌更大模型的推理表现，显示了方法的有效性。

Conclusion: 结构化检索内容能提高语言模型推理的信息密度和上下文清晰度，有效提升整体推理能力。\textsc{Structure-R1}为领域知识融合推理带来新思路，具实际应用价值。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

</details>


### [97] [Extending Audio Context for Long-Form Understanding in Large Audio-Language Models](https://arxiv.org/abs/2510.15231)
*Yuatyong Chaichana,Pittawat Taveekitworachai,Warit Sirichotedumrong,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 现有的大型音频-语言模型（LALMs）受限于音频上下文窗口较短，影响长音频理解。本文提出Partial YaRN（一种训练外音频扩展方法）和VLAT（一种训练期音频长度增强策略），显著提升模型长音频处理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LALMs的文本主干支持长上下文，但音频上下文能力较弱。现有方法如YaRN已用于单模态，但未针对LALMs开发，因此需要更有效地扩展音频上下文范围。

Method: 提出Partial YaRN，将YaRN的上下文扩展思想应用到音频token（不影响文本token），实现无训练延长音频窗口；并提出VLAT，通过训练时音频长度增强，让模型具备更强泛化性和鲁棒性。

Result: 在SALMONN和Qwen2-Audio等任务上，Partial YaRN在多种配置下均优于原始模型，VLAT进一步带来显著提升，表现出对未见长音频的优异能力。

Conclusion: Partial YaRN与VLAT可有效扩展LALMs的音频上下文窗口，在不牺牲文本理解能力的前提下，极大强化长音频理解，是通用且高效的上下文扩展方法。

Abstract: Large Audio-Language Models (LALMs) are often constrained by short audio
context windows, even when their text backbones support long contexts, limiting
long-form audio understanding. Prior work has introduced context-extension
methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains
unexplored. First, building on RoPE-based context extension, we introduce
Partial YaRN, a training-free, audio-only extension method that modifies only
audio token positions, leaving text positions intact to preserve the base LLM's
text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a
training strategy that extends Partial YaRN into a training-time positional
augmentation. VLAT simulates diverse audio lengths during training, enabling
generalization to inputs far longer than those seen in training and improving
robustness for long-context audio understanding. Our experiments on SALMONN and
Qwen2-Audio show that Partial YaRN outperforms the original models across wide
range of settings, and VLAT training strategy provides substantial improvement,
achieving strong performance on long audio of unseen lengths.

</details>


### [98] [Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning](https://arxiv.org/abs/2510.15244)
*Lina Berrayana,Ahmed Heakl,Muhammad Abdullah Sohail,Thomas Hofmann,Salman Khan,Wei Chen*

Main category: cs.CL

TL;DR: 本文研究了将离散扩散语言模型（DDLM）与自回归模型（ARM）结合的混合架构，以发挥两者的互补优势，在多个复杂推理和计划任务上获得显著性能提升，并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型虽准确但推理耗时、成本高，近期离散扩散模型能并行生成且在复杂推理任务表现突出。作者提出混合模型以结合两者优势，探索其协作带来的潜在收益。

Method: 作者设计了两种混合架构：一种在文本空间中由一个模型规划推理、另一个模型执行答案；另一种在隐空间中引入投影器，将DDLM的隐变量投射到ARM的嵌入空间，使协作更高效、绕过扩散模型在文本生成上的部分限制。

Result: 实验显示，将DDLM到ARM的协作由文本空间转向隐空间能显著提升准确率（如DART-5从27%提升到54%，AIME24从0%提升到14%）；同时，DDLM规划+ARM执行大幅节省计算资源，且几乎不影响准确率。比如隐空间方案只需很少的token就能超越使用大量token的强大基线模型。

Conclusion: 混合架构下DDLM与ARM的协作显著提升了推理效率和准确性，对复杂任务特别有效。研究为未来DDLM在各类语言任务中的应用及混合模型拓展提供了新见解。

Abstract: Current autoregressive language models (ARMs) achieve high accuracy but
require long token sequences, making them costly. Discrete diffusion language
models (DDLMs) enable parallel and flexible generation within a fixed number of
steps and have recently emerged for their strong performance in complex
reasoning and long-term planning tasks. We present a study exploring hybrid
architectures that couple DDLMs with ARMs to assess whether their collaboration
can yield complementary benefits. We first examine collaboration in text space,
where one model plans the reasoning process and another executes the final
answer based on that plan. We then extend this setup to latent-space
communication, introducing a learned projector that maps DDLM latents into the
ARM's embedding space, potentially bypassing some of the text-generation
limitations of diffusion models. We find that shifting DDLM --> ARM
communication from text space to latent space yields significant accuracy
gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to
14.0% on AIME24. We also find that combining a DDLM planner with an ARM
executor can provide substantial computational savings with little to no impact
on accuracy. For example, the latent-space pipeline, using 64 tokens for
planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME,
despite Qwen using 44 times more tokens. Overall, our study offers new insights
into reasoning with DDLMs and highlights their potential in hybrid
architectures.

</details>


### [99] [Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding](https://arxiv.org/abs/2510.15253)
*Sensen Gao,Shanshan Zhao,Xu Jiang,Lunhao Duan,Yong Xien Chng,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,Jia-Wang Bian,Mingming Gong*

Main category: cs.CL

TL;DR: 本论文对多模态检索增强生成（Multimodal RAG）在文档理解领域的研究进行了系统综述，提出分类方法并总结现状与挑战。


<details>
  <summary>Details</summary>
Motivation: 文档理解在金融分析、科学发现等领域至关重要，但现有OCR+LLM和原生多模态LLM方法分别存在结构信息丢失与上下文建模能力有限的问题。文档内容的多模态特性（文本、表格、图表、布局等）对现有RAG方法提出新需求。

Method: 提出多模态RAG范式，能够实现对文档所有模态内容的整体检索与推理。作者构建了基于领域、检索模态和粒度的多模态RAG分类体系，总结了涉及图结构与代理框架的最新进展，并梳理了关键数据集、基准和实际应用。

Result: 系统回顾了多模态RAG的发展，梳理了相关技术、数据集与应用，揭示了效率、细粒度表示及鲁棒性等尚存的核心挑战。

Conclusion: 多模态RAG为文档理解提供了更全局、更深层的智能手段。论文为该领域的发展提供了分类体系、现状总结和未来研究路线图。

Abstract: Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.

</details>


### [100] [TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration](https://arxiv.org/abs/2510.15267)
*Mucheng Ren,He Chen,Yuchen Yan,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TraceCoder集成多源外部知识，提升了ICD编码的可追溯性和可解释性，并在多个真实数据集上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 自动化ICD编码对医疗系统至关重要，但现有方法面临文本与编码语义不匹配、对罕见/长尾编码表现不佳以及解释性差等挑战。

Method: 提出了TraceCoder框架，动态融合UMLS、Wikipedia和大型语言模型等知识源，丰富编码表示并弥合语义差距。采用混合注意力机制，建模标签、临床上下文和知识之间的互动，并以外部证据支持预测结果。

Result: 在MIMIC-III-ICD9、MIMIC-IV-ICD9和MIMIC-IV-ICD10三个数据集上，TraceCoder取得了最先进的性能，消融实验也验证了框架各组成部分的有效性。

Conclusion: TraceCoder为自动ICD编码提供了可扩展、高鲁棒性且贴合临床实际需求的解决方案，兼顾准确性、可解释性与可靠性。

Abstract: Automated International Classification of Diseases (ICD) coding assigns
standardized diagnosis and procedure codes to clinical records, playing a
critical role in healthcare systems. However, existing methods face challenges
such as semantic gaps between clinical text and ICD codes, poor performance on
rare and long-tail codes, and limited interpretability. To address these
issues, we propose TraceCoder, a novel framework integrating multi-source
external knowledge to enhance traceability and explainability in ICD coding.
TraceCoder dynamically incorporates diverse knowledge sources, including UMLS,
Wikipedia, and large language models (LLMs), to enrich code representations,
bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a
hybrid attention mechanism to model interactions among labels, clinical
context, and knowledge, improving long-tail code recognition and making
predictions interpretable by grounding them in external evidence. Experiments
on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that
TraceCoder achieves state-of-the-art performance, with ablation studies
validating the effectiveness of its components. TraceCoder offers a scalable
and robust solution for automated ICD coding, aligning with clinical needs for
accuracy, interpretability, and reliability.

</details>


### [101] [TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding](https://arxiv.org/abs/2510.15269)
*Mucheng Ren,Yucheng Yan,He Chen,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: 提出了一种面向医学文本的自适应课程学习框架TACL，通过动态调整训练策略，显著提升了多语种医学文本理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 医学文本（如电子病历）的非结构化特性和领域专有性使其自动理解非常困难，而现有NLP方法未能区分样本难度，导致对复杂或罕见病例的泛化能力不足。

Method: 提出TACL（Threshold-Adaptive Curriculum Learning）框架，根据样本复杂度自动分级，训练时优先学习简单记录，逐步过渡到复杂案例，实现逐步递进式学习。该方法在英语和中文的多语种医学数据上进行了实验。

Result: 在自动ICD编码、再入院预测和中医证候判别等多项任务上均取得了显著提升，表明TACL能有效增强模型对复杂医学记录的理解能力。

Conclusion: TACL不仅提升了医学文本自动处理的性能，还展示了方法在不同医学领域间统一和泛化的潜力，为全球可扩展的医学文本理解方案奠定了基础。

Abstract: Medical texts, particularly electronic medical records (EMRs), are a
cornerstone of modern healthcare, capturing critical information about patient
care, diagnoses, and treatments. These texts hold immense potential for
advancing clinical decision-making and healthcare analytics. However, their
unstructured nature, domain-specific language, and variability across contexts
make automated understanding an intricate challenge. Despite the advancements
in natural language processing, existing methods often treat all data as
equally challenging, ignoring the inherent differences in complexity across
clinical records. This oversight limits the ability of models to effectively
generalize and perform well on rare or complex cases. In this paper, we present
TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to
address these challenges by rethinking how models interact with medical texts
during training. Inspired by the principle of progressive learning, TACL
dynamically adjusts the training process based on the complexity of individual
samples. By categorizing data into difficulty levels and prioritizing simpler
cases early in training, the model builds a strong foundation before tackling
more complex records. By applying TACL to multilingual medical data, including
English and Chinese clinical records, we observe significant improvements
across diverse clinical tasks, including automatic ICD coding, readmission
prediction and TCM syndrome differentiation. TACL not only enhances the
performance of automated systems but also demonstrates the potential to unify
approaches across disparate medical domains, paving the way for more accurate,
scalable, and globally applicable medical text understanding solutions.

</details>


### [102] [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)
*Jingao Xu,Shuoyoucheng Ma,Xin Song,Rong Jiang,Hongkui Tu,Bin Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为Exemplar-Guided Planning（EGP）的新框架，通过利用训练集中的高质量示例，引导大语言模型在知识图谱问答任务中的规划与推理过程，有效提升了推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱问答（KGQA）中由于难以弥合自然语言查询与知识图谱结构化表示之间的语义鸿沟，导致规划不理想、推理效率低下，且缺少对训练数据隐含推理模式的利用。这些问题限制了KGQA的性能。

Method: EGP框架通过实体模板对训练集问题进行预处理，归一化语义变化。利用语义嵌入和FAISS索引检索出相似高质量示例及其推理路径，动态引导LLM在任务分解和关系探索两个阶段的规划。引入Smart Lookahead机制，在关系探索阶段通过预判优秀路径，提升效率与准确性。该方法应用于Plan-on-Graph（PoG）并形成PoG-EGP。

Result: 在两个真实KGQA数据集WebQSP和CWQ上的大量实验表明，PoG-EGP在准确率和效率上显著优于原始PoG系统及其它对比方法。

Conclusion: EGP显著提升了LLM在KGQA任务中的规划、推理能力和整体表现，验证了通过示例驱动规划可以有效弥合自然语言与结构化KG的语义差异。

Abstract: Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

</details>


### [103] [Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach](https://arxiv.org/abs/2510.15311)
*Andharini Dwi Cahyani,Moh. Wildan Fathoni,Fika Hastarita Rachman,Ari Basuki,Salman Amin,Bain Khusnul Khotimah*

Main category: cs.CL

TL;DR: 本文评估了Jaccard系数与余弦相似度在自动作文评分系统中的效果，发现余弦相似度效果更好，且unigram表现优于bigram和trigram。


<details>
  <summary>Details</summary>
Motivation: 手动评分作文耗时且主观，亟需高效、准确的自动化评分方法。本文旨在比较两种常用文本相似度指标（Jaccard系数与余弦相似度）及不同n-gram特征在自动作文评分中的适用性和优劣。

Method: 收集初中公民教育课的作文数据，通过n-gram（unigram、bigram、trigram）方法提取文本特征后，转换为向量表示，并分别用Jaccard系数与余弦相似度计算作文相似度分数。最后对比系统评分与人工评分的RMSE。

Result: 实验结果显示，使用余弦相似度的系统评分比Jaccard系数更接近人工评分（RMSE更低）；在n-gram特征中，unigram的表现优于bigram和trigram。

Conclusion: 余弦相似度结合unigram特征在初中作文自动评分中最有效，为自动评测系统选型提供参考。

Abstract: Automated essay scoring (AES) is a vital area of research aiming to provide
efficient and accurate assessment tools for evaluating written content. This
study investigates the effectiveness of two popular similarity metrics, Jaccard
coefficient, and Cosine similarity, within the context of vector space
models(VSM)employing unigram, bigram, and trigram representations. The data
used in this research was obtained from the formative essay of the citizenship
education subject in a junior high school. Each essay undergoes preprocessing
to extract features using n-gram models, followed by vectorization to transform
text data into numerical representations. Then, similarity scores are computed
between essays using both Jaccard coefficient and Cosine similarity. The
performance of the system is evaluated by analyzing the root mean square error
(RMSE), which measures the difference between the scores given by human graders
and those generated by the system. The result shows that the Cosine similarity
outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower
RMSE compared to bigrams and trigrams.

</details>


### [104] [Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination](https://arxiv.org/abs/2510.15312)
*Zhiyang Chen,Daliang Xu,Haiyang Shen,Mengwei Xu,Shangguang Wang,Yun Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoordGen的移动端推理框架，大幅提升了本地大语言模型的文本生成速度和能效。


<details>
  <summary>Details</summary>
Motivation: 当前移动设备上的大语言模型在生成阶段存在高延迟和低硬件利用率的问题，亟需解决以实现高效、实时的个性化任务。

Method: CoordGen框架融入了投机解码和动态硬件调度，包含三项关键优化：自适应执行调度（协调prefill和解码阶段的算图分配）、上下文对齐拟稿（通过轻量级在线校准提升投机效率）、硬件高效草稿扩展（复用扩展中间序列以提升并行度，降低验证开销）。

Result: 在多种智能手机和典型任务场景下，CoordGen框架在生成速度上最多提升3.8倍，能效提升4.7倍，并通过分组件实验验证了各优化措施的独立贡献。

Conclusion: CoordGen显著改进了移动设备上大语言模型的上下文感知文本生成效率，为个性化和实时AI助手等应用提供了坚实技术基础。

Abstract: Enhancing on-device large language models (LLMs) with contextual information
from local data enables personalized and task-aware generation, powering use
cases such as intelligent assistants and UI agents. While recent developments
in neural processors have substantially improved the efficiency of prefill on
mobile devices, the token-by-token generation process still suffers from high
latency and limited hardware utilization due to its inherently memory-bound
characteristics. This work presents CoordGen, a mobile inference framework that
integrates speculative decoding with dynamic hardware scheduling to accelerate
context-aware text generation on mobile devices. The framework introduces three
synergistic components: (1) adaptive execution scheduling, which dynamically
balances compute graphs between prefill and decoding phases; (2)
context-aligned drafting, which improves speculative efficiency through
lightweight online calibration to current tasks; and (3) hardware-efficient
draft extension, which reuses and expands intermediate sequences to improve
processing parallelism and reduce verification cost. Experiments on multiple
smartphones and representative workloads show consistent improvements of up to
3.8x in generation speed and 4.7x in energy efficiency compared with existing
mobile inference solutions. Component-level analysis further validates the
contribution of each optimization.

</details>


### [105] [Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry](https://arxiv.org/abs/2510.15313)
*Bolei Ma,Yina Yao,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文提出了一个三步法评估框架，用于系统性分析和比较大语言模型（LLMs）在古典中文诗歌生成及评估方面的表现，并发现现有LLMs存在评估和生成的偏差，说明仅靠机器评判目前还不够。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多创意领域表现突出，但在古典中文诗歌生成与评估上的能力和局限尚不清楚，因此需要有系统的评测方法来揭示模型表现及偏差。

Method: 作者提出了“计算指标评价+LLM自动评价+人类专家验证”三步评估框架，对六种主流LLM进行多维度（主题、情感、意象、形式、风格）表现评估。

Result: 结果显示，LLMs在诗歌创作和自评时存在“回音室效应”（自我强化的标准偏差），其评价结果常与人类存在显著分歧，暴露出自动化评估和生成的系统性偏差。

Conclusion: 虽然LLMs在诗歌生成上有潜力，但目前它们既不能完全胜任诗歌质量评估，也不能替代理解复杂文化与技术细节的人类专家，强调今后需结合机器与人工的混合验证。

Abstract: Large Language Models (LLMs) are increasingly applied to creative domains,
yet their performance in classical Chinese poetry generation and evaluation
remains poorly understood. We propose a three-step evaluation framework that
combines computational metrics, LLM-as-a-judge assessment, and human expert
validation. Using this framework, we evaluate six state-of-the-art LLMs across
multiple dimensions of poetic quality, including themes, emotions, imagery,
form, and style. Our analysis reveals systematic generation and evaluation
biases: LLMs exhibit "echo chamber" effects when assessing creative quality,
often converging on flawed standards that diverge from human judgments. These
findings highlight both the potential and limitations of current capabilities
of LLMs as proxy for literacy generation and the limited evaluation practices,
thereby demonstrating the continued need of hybrid validation from both humans
and models in culturally and technically complex creative tasks.

</details>


### [106] [AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction](https://arxiv.org/abs/2510.15339)
*Hong Ting Tsang,Jiaxin Bai,Haoyu Huang,Qiao Xiao,Tianshi Zheng,Baixuan Xu,Shujie Liu,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出了一种利用强化学习优化知识图谱（KG）以提升RAG问答系统效果的新方法AutoGraph-R1。实验表明该方法优于传统构建方式。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱通常与下游任务脱钩，导致其结构不适合实际应用需求。该论文旨在将KG构建过程与实际问答任务更紧密结合，从而提升整体QA系统性能。

Method: AutoGraph-R1框架将知识图的生成看作强化学习中的策略学习，利用大模型（LLM）生成图谱，并通过下游RAG管线中的任务表现反馈作为奖励信号，设计两种任务感知型奖励函数：一种关注KG作为知识承载体的作用，另一种关注其作为检索索引的价值。

Result: 在多个QA基准测试中，AutoGraph-R1训练出的知识图显著提升了RAG问答效果，优于基于通用构建规则的KG。

Conclusion: 通过任务驱动和强化学习闭环优化，知识图谱可以从‘结构良好’转向‘任务有用’，显著提高下游应用效果。

Abstract: Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.

</details>


### [107] [Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics](https://arxiv.org/abs/2510.15345)
*Catarina G Belem,Parker Glenn,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文探究了影响人类对文本可读性感知的因素，发现“信息内容”和“主题”较表层特征更能解释可读性。同时，比较了15种传统指标与6种基于模型的可读性指标，后者与人类评价的一致性更高。


<details>
  <summary>Details</summary>
Motivation: 当前自动可读性评估主要依赖表层文本特征，对可读性的定义不一致，且与人类感知存在差距。本研究希望揭示人类判断可读性的核心因素，并评估主流指标的效果，更合理地衡量文本可读性。

Method: 作者分析了897个人类对文本可读性的评分，挖掘人类感知中的关键影响因素。随后，分别在五个英文数据集上，对比了15种传统可读性指标和6种基于机器学习模型的可读性指标，通过与人工打分的相关性进行性能评价。

Result: 结果表明，信息内容和主题是影响可读性的主因。四个基于模型的指标在人类评分相关性排行榜中长期位列前四，而所有传统指标表现均不如模型指标，最优的传统指标平均排名为8.6。

Conclusion: 目前主流的可读性评估指标与人类感知存在较大差距，基于机器学习或复杂语义的模型指标与人工评价更为一致，是今后可读性评估发展的更优方向。

Abstract: Automatic readability assessment plays a key role in ensuring effective and
accessible written communication. Despite significant progress, the field is
hindered by inconsistent definitions of readability and measurements that rely
on surface-level text properties. In this work, we investigate the factors
shaping human perceptions of readability through the analysis of 897 judgments,
finding that, beyond surface-level cues, information content and topic strongly
shape text comprehensibility. Furthermore, we evaluate 15 popular readability
metrics across five English datasets, contrasting them with six more nuanced,
model-based metrics. Our results show that four model-based metrics
consistently place among the top four in rank correlations with human
judgments, while the best performing traditional metric achieves an average
rank of 8.6. These findings highlight a mismatch between current readability
metrics and human perceptions, pointing to model-based approaches as a more
promising direction.

</details>


### [108] [When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling](https://arxiv.org/abs/2510.15346)
*Heecheol Yun,Kwangmin Ki,Junghyun Lee,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAFE的新框架，专为提升大型语言模型（LLMs）在长文本生成时的集成效果，并提供了在多项基准任务中超过现有方法的实验证据。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM集成能够提升性能，目前的方法主要针对短文本有效，但在长文本生成中的表现尚未被充分研究，需要有方法在不增加过多计算成本的同时提升长文本生成效果。

Method: 作者分析了当前LLM集成在每个token进行集成时会导致长文本生成性能下降的原因，发现token化不一致和概率分布一致性是主要影响因素。据此，提出SAFE框架，有选择性地在关键位置集成，并引入概率锐化策略，将同一单词多个子词token的预测概率合并，以提升稳定性。

Result: 在MATH500和BBH等多个公开基准上的实验表明，SAFE能够在只集成1%以下token的情况下，依然在精度和效率上超越现有集成方法。

Conclusion: SAFE通过解决token化不一致和概率分散问题，实现了更高效、稳定的LLM集成，为长文本生成任务提供了切实可用且优于以往的解决方案。

Abstract: Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models' next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.

</details>


### [109] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2510.15349)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Zuming Huang,Jun Huang,Haozhe Wang,Yanjie Liang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CL

TL;DR: 本文提出了一种新的文档解析方法LayoutRL，通过强化学习优化文档结构理解，并构建了大规模数据集Infinity-Doc-400K，训练出具有强泛化能力的Infinity-Parser，实验证明其在多个基准任务上性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的文档解析方法难以处理多样化的文档类型，尤其在面对分布外数据和数据稀缺时效果较差。需要新方法提升模型的泛化能力和实际应用表现。

Method: 提出了LayoutRL框架，利用强化学习结合多项奖励（包括归一化编辑距离、段落计数准确性和阅读顺序保持）优化模型。并构建了包含40万文档的大型多领域数据集Infinity-Doc-400K，用以训练视觉-语言模型Infinity-Parser。

Result: 在OmniDocBench、olmOCR-Bench、PubTabNet和FinTabNet等基准测试中，Infinity-Parser展现出对不同类型、语言和结构复杂度文档的优异解析效果，全面超越现有专用解析系统及通用视觉-语言模型。

Conclusion: 通过新颖的强化学习奖励机制和大规模多样化数据集训练，Infinity-Parser在文档解析任务中取得了领先表现，可提升文档结构化理解的实际可用性。作者承诺将开源数据、代码与模型，促进领域进一步研究。

Abstract: Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.

</details>


### [110] [VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency](https://arxiv.org/abs/2510.15406)
*Hongcheng Liu,Yixuan Hou,Heyang Liu,Yuhao Wang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文评估了当前语音大模型（Speech-LLMs）对语音不流畅（如帕金森病患者常见语音障碍）的鲁棒性，并提出了系统化测试框架，结果显示模型在应对语音不流畅时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然语音大模型在许多任务中表现强劲，但现实环境下用户语音往往存在不流畅等现象，而主流评测通常忽略了这部分，导致实际应用鲁棒性未知。

Method: 作者构建了VocalBench-DF框架，针对多维度语音不流畅现象系统性评估22个主流Speech-LLM的鲁棒性，并对性能瓶颈进行了详细分析。

Result: 研究发现目前主流Speech-LLM在处理带有不流畅的语音输入时，识别和推理性能大幅下降。关键瓶颈在于音素级处理和长上下文建模能力不足。

Conclusion: 当前Speech-LLM对语音不流畅的适应性有限，需从识别与推理能力、组件与流程等多方面提升模型鲁棒性，以实现包容性更强的语音大模型。

Abstract: While Speech Large Language Models (Speech-LLMs) show strong performance in
many applications, their robustness is critically under-tested, especially to
speech disfluency. Existing evaluations often rely on idealized inputs,
overlooking common disfluencies, particularly those associated with conditions
like Parkinson's disease. This work investigates whether current Speech-LLMs
can maintain performance when interacting with users who have speech
impairments. To facilitate this inquiry, we introduce VocalBench-DF, a
framework for the systematic evaluation of disfluency across a
multi-dimensional taxonomy. Our evaluation of 22 mainstream Speech-LLMs reveals
substantial performance degradation, indicating that their real-world readiness
is limited. Further analysis identifies phoneme-level processing and
long-context modeling as primary bottlenecks responsible for these failures.
Strengthening recognition and reasoning capability from components and
pipelines can substantially improve robustness. These findings highlight the
urgent need for new methods to improve disfluency handling and build truly
inclusive Speech-LLMs

</details>


### [111] [Large-scale User Game Lifecycle Representation Learning](https://arxiv.org/abs/2510.15412)
*Yanjie Gou,Jiangming Liu,Kouying Xue,Yi Hua*

Main category: cs.CL

TL;DR: 本文提出了用于在线游戏平台广告和推荐的新表示学习方法，以应对游戏数量稀疏和用户行为极度不均衡的问题，实验显示大幅提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前视频游戏产业快速扩张，在线平台需要有效的广告和推荐系统。但由于游戏数量远少于传统推荐场景（如商品推荐），加上大部分用户行为集中在少数热门游戏，现有大规模表示学习方法（针对数十亿级商品设计）并不适用于游戏推荐。为此，论文针对游戏推荐系统中的稀疏性与不平衡问题提出改进方法。

Method: 提出了User Game Lifecycle（UGL）方法，丰富了用户在游戏中的行为表现；并设计了两种新策略来挖掘用户短期和长期兴趣。此外，为应对热门游戏用户行为过度集中的问题，引入了逆概率掩码（Inverse Probability Masking）用于UGL表示学习。

Result: 在离线和在线实验中，所提UGL方法用于游戏广告推荐使AUC平均提升1.83%，CVR平均提升21.67%；用于游戏道具推荐使AUC提升0.5%，ARPU提升0.82%。

Conclusion: 所提方法有效缓解了游戏推荐和广告中的数据稀疏与不平衡问题，显著提升了推荐系统的效果，对在线视频游戏平台具有实际应用价值。

Abstract: The rapid expansion of video game production necessitates the development of
effective advertising and recommendation systems for online game platforms.
Recommending and advertising games to users hinges on capturing their interest
in games. However, existing representation learning methods crafted for
handling billions of items in recommendation systems are unsuitable for game
advertising and recommendation. This is primarily due to game sparsity, where
the mere hundreds of games fall short for large-scale user representation
learning, and game imbalance, where user behaviors are overwhelmingly dominated
by a handful of popular games. To address the sparsity issue, we introduce the
User Game Lifecycle (UGL), designed to enrich user behaviors in games.
Additionally, we propose two innovative strategies aimed at manipulating user
behaviors to more effectively extract both short and long-term interests. To
tackle the game imbalance challenge, we present an Inverse Probability Masking
strategy for UGL representation learning. The offline and online experimental
results demonstrate that the UGL representations significantly enhance model by
achieving a 1.83% AUC offline increase on average and a 21.67% CVR online
increase on average for game advertising and a 0.5% AUC offline increase and a
0.82% ARPU online increase for in-game item recommendation.

</details>


### [112] [Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs](https://arxiv.org/abs/2510.15418)
*Lee Qi Zun,Mohamad Zulhilmi Bin Abdul Halim,Goh Man Fye*

Main category: cs.CL

TL;DR: 本研究通过精炼MedGemma模型，提升其医学图像描述的准确性和事实性，从而增强多模态临床检索增强生成系统（RAG）的效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索增强生成（RAG）的系统在回答基于图像的临床问题时效果有限，主要因为通用视觉-语言模型在医学领域缺乏专业性和事实性。为满足马来西亚临床实践指南的需求，迫切需要可生成高保真医学描述的模型。

Method: 提出了以知识蒸馏合成数据集的框架，包括皮肤科学、眼底和胸部X光领域，并采用参数高效的QLoRA方法对MedGemma模型进行微调。模型评估则采用分类准确性及创新性利用RAGAS框架评测描述的信实性、相关性和正确性。

Result: 经过微调的模型在分类任务中表现显著提升，RAGAS 评估显示该模型在描述的信实性和正确性上取得显著进步。

Conclusion: 本研究建立了面向医学视觉-语言模型专用化的高效管线，并验证所得到的模型能可靠生成高质量医学描述，为多模态临床RAG系统的证据型决策支持奠定基础。

Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.

</details>


### [113] [When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs](https://arxiv.org/abs/2510.15421)
*Hongcheng Liu,Pingjie Wang,Yuhao Wang,Siqu Ou,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文关注于多模态大模型（MLLM）在主动推理任务下能力的系统性评估，提出了GuessBench基准，并对现有MLLM模型进行了广泛测试，发现其主动推理能力远落后于被动推理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型评测多集中于信息完整情况下的被动推理，与现实需求脱节。作者希望解决模型在信息不全时自主获取证据并做出决策的能力缺失问题。

Method: 作者设计了GuessBench高质量评测集，涵盖感知和知识导向两类图片，无需特定任务先验，要求模型主动选择图片补充缺失证据，并进行迭代推理。评测了20种主流多模态大模型，并对模型推理行为进行分析和消融实验。

Result: 测试发现，现有MLLM在主动推理上的表现显著低于被动推理。细粒度感知与时机决策是难点。感知能力增强对小模型有效，而强化思维方法对各大小模型均有改善。

Conclusion: 当前MLLM主动推理能力不足，需进一步提升。文中提出的评测方法和发现为后续多模态主动推理研究指明了方向。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities
across a broad range of benchmarks. However, most existing evaluations focus on
passive inference, where models perform step-by-step reasoning under complete
information. This setup is misaligned with real-world use, where seeing is not
enough. This raises a fundamental question: Can MLLMs actively acquire missing
evidence under incomplete information? To bridge this gap, we require the MLLMs
to actively acquire missing evidence and iteratively refine decisions under
incomplete information, by selecting a target image from a candidate pool
without task-specific priors. To support systematic study, we propose
GuessBench, a benchmark with both perception-oriented and knowledge-oriented
images for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs
and find that performance on active reasoning lags far behind it on passive
settings, indicating substantial room for improvement. Further analysis
identifies fine-grained perception and timely decision-making as key
challenges. Ablation studies show that perceptual enhancements benefit smaller
models, whereas thinking-oriented methods provide consistent gains across model
sizes. These results suggest promising directions for future research on
multimodal active reasoning.

</details>


### [114] [Controllable Abstraction in Summary Generation for Large Language Models via Prompt Engineering](https://arxiv.org/abs/2510.15436)
*Xiangchen Song,Yuchen Liu,Yaxuan Luan,Jinxu Guo,Xiaofan Guo*

Main category: cs.CL

TL;DR: 本文提出了一种基于提示工程的大语言模型可控摘要生成方法，通过多阶段提示生成框架，提升摘要的质量与可控性。实验证明合理的提示长度和低噪声能显著提升生成摘要的效果。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型摘要生成在摘要质量与可控性方面存在不足，难以兼顾不同文本类型及自动化实践的需要，因此需要探索更高质量且可控的生成方法。

Method: 设计了一个多阶段提示生成框架，包括语义分析、主题建模和噪声控制，并通过不同提示长度、数据噪声和文本类型在CNN/Daily Mail数据集上开展实验。

Result: 实验结果显示提示长度对摘要质量影响显著，过短或过长都导致质量下降；数据噪声增大时，ROUGE-L指标逐步下降；模型对新闻文本的摘要效果最好，对学术文章最差。

Conclusion: 合理控制提示设计和优化文本预处理，有助于显著提升大语言模型的摘要准确性与可控性，为后续摘要生成研究提供了新思路。

Abstract: This study presents a controllable abstract summary generation method for
large language models based on prompt engineering. To address the issues of
summary quality and controllability in traditional methods, we design a
multi-stage prompt generation framework. This framework generates summaries
with varying levels of abstraction by performing semantic analysis, topic
modeling, and noise control on the input text. The experiment uses the
CNN/Daily Mail dataset and provides a detailed analysis of different prompt
lengths, data noise, and text types. The experimental results show that prompt
length has a significant impact on the quality of generated summaries. Both
very short and very long prompt tokens result in a decrease in summary quality.
Data noise also negatively affects the summary generation process. As noise
levels increase, the ROUGE-L score gradually decreases. Furthermore, different
text types have varying effects on the model's ability to generate summaries.
The model performs best when handling news texts, while its performance is
worse when processing academic articles. This research provides new insights
into improving summary generation using large language models, particularly in
how controlling prompt strategies and optimizing text preprocessing can enhance
summary accuracy and controllability.

</details>


### [115] [CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs](https://arxiv.org/abs/2510.15455)
*Gucongcong Fan,Chaoyue Niu,Chengfei Lyu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: CORE是一种协作性框架，将本地和云端大语言模型结合，减少移动端UI上传到云端的数据量，同时保持任务准确率。


<details>
  <summary>Details</summary>
Motivation: 云端大模型虽然任务表现好，但需要频繁上传全部UI状态，导致隐私泄露；本地模型则容量有限，准确率低。如何兼顾减少隐私泄露和任务准确率是问题所在。

Method: 提出CORE框架，包括三部分：1）基于XML层级的语义分组块划分；2）本地与云端协同规划任务子目标；3）本地排序相关UI块，云端在高相关块内细致决策；并引入多轮累积机制减少本地误判影响。

Result: 在多款移动应用和任务中，CORE将UI上传暴露下降至55.6%，任务成功率仅稍低于云端方案，大大减少了隐私泄露。

Conclusion: CORE能在基本不影响任务表现的前提下显著降低移动端UI隐私暴露，是移动智能体与大模型结合的有效隐私保护方案。

Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

</details>


### [116] [DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios](https://arxiv.org/abs/2510.15501)
*Yao Huang,Yitong Sun,Yichi Zhang,Ruochen Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CL

TL;DR: 本文提出了DeceptionBench，这是一个系统性评估大语言模型（LLMs）欺骗行为的基准测试，涵盖五大社会领域，揭示了现有模型在多种情境下易受欺骗性影响，呼吁强化安全防护。


<details>
  <summary>Details</summary>
Motivation: 目前，LLMs虽在多项任务中表现出色，但其在现实世界中产生的欺骗行为及潜在风险尚缺乏系统分析和评估方法。因此迫切需要一个针对欺骗倾向的基准以引导行业改进安全防护。

Method: 设计了覆盖经济、医疗、教育、社交和娱乐等五大领域、共150个情景的基准，包含超过1000个样本，评估模型在自利型（egoistic）与阿谀型（sycophantic）以及在中立、奖励和胁迫等外部影响下的欺骗输出。同时引入多轮交互，更贴近真实的反馈动态。

Result: 大量实验显示，当前主流LLMs和大推理模型在面对外部操控时，尤其是在激励机制下，欺骗倾向明显增强，暴露出模型在应对复杂社会诱因方面的脆弱性。

Conclusion: 现有模型缺乏对欺骗性上下文的鲁棒性，易受外界奖励和压力影响，强化针对多样欺骗行为的安全保障机制刻不容缓。研究工具DeceptionBench为后续定量分析提供了可靠基础。

Abstract: Despite the remarkable advances of Large Language Models (LLMs) across
diverse cognitive tasks, the rapid enhancement of these capabilities also
introduces emergent deceptive behaviors that may induce severe risks in
high-stakes deployments. More critically, the characterization of deception
across realistic real-world scenarios remains underexplored. To bridge this
gap, we establish DeceptionBench, the first benchmark that systematically
evaluates how deceptive tendencies manifest across different societal domains,
what their intrinsic behavioral patterns are, and how extrinsic factors affect
them. Specifically, on the static count, the benchmark encompasses 150
meticulously designed scenarios in five domains, i.e., Economy, Healthcare,
Education, Social Interaction, and Entertainment, with over 1,000 samples,
providing sufficient empirical foundations for deception analysis. On the
intrinsic dimension, we explore whether models exhibit self-interested egoistic
tendencies or sycophantic behaviors that prioritize user appeasement. On the
extrinsic dimension, we investigate how contextual factors modulate deceptive
outputs under neutral conditions, reward-based incentivization, and coercive
pressures. Moreover, we incorporate sustained multi-turn interaction loops to
construct a more realistic simulation of real-world feedback dynamics.
Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal
critical vulnerabilities, particularly amplified deception under reinforcement
dynamics, demonstrating that current models lack robust resistance to
manipulative contextual cues and the urgent need for advanced safeguards
against various deception behaviors. Code and resources are publicly available
at https://github.com/Aries-iai/DeceptionBench.

</details>


### [117] [Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?](https://arxiv.org/abs/2510.15513)
*Ashutosh Bajpai,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本论文关注大语言模型（LLMs）在时间敏感领域中的时间一致性问题，提出了新基准和增强模型来提升LLM在不同语言环境下的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于法律、医疗和金融等对事实和时间敏感的领域，模型除了要准确还需具备良好的时间一致性能力，而目前相关研究严重不足。

Method: 作者提出了一个新的基准——temporal referential consistency，并开发了资源库TEMP-ReCon，用多语种（英语、法语、罗马尼亚语）评测开源和闭源的LLM。提出了一种基于推理路径对齐的新模型（UnTRaP），专门用来提升LLM的时间参照一致性。

Result: 实验显示，现有LLMs在时间参照一致性方面表现不足。作者提出的UnTRaP模型在提升时间一致性方面明显优于多种已有的基线模型。

Conclusion: LLMs目前在时间参照一致性方面存在明显短板。本文为提升其这方面能力提供了新的基准资源库和有效的模型方法，对相关领域具有重要意义。

Abstract: The increasing acceptance of large language models (LLMs) as an alternative
to knowledge sources marks a significant paradigm shift across various domains,
including time-sensitive fields such as law, healthcare, and finance. To
fulfill this expanded role, LLMs must not only be factually accurate but also
demonstrate consistency across temporal dimensions, necessitating robust
temporal reasoning capabilities. Despite this critical requirement, efforts to
ensure temporal consistency in LLMs remain scarce including noticeable absence
of endeavors aimed at evaluating or augmenting LLMs across temporal references
in time-sensitive inquiries. In this paper, we seek to address this gap by
introducing a novel benchmark entitled temporal referential consistency,
accompanied by a resource TEMP-ReCon designed to benchmark a wide range of both
open-source and closed-source LLMs with various linguistic contexts
characterized by differing resource richness (including English, French, and
Romanian). The findings emphasis that LLMs do exhibit insufficient temporal
referent consistency. To address this, we propose \newmodel, a reasoning path
alignment-based model that aims to enhance the temporal referential consistency
of LLMs. Our empirical experiments substantiate the efficacy of UnTRaP compared
to several baseline models.

</details>


### [118] [From Characters to Tokens: Dynamic Grouping with Hierarchical BPE](https://arxiv.org/abs/2510.15517)
*Rares Dolga,Lucas Maystre,Tudor Berariu,David Barber*

Main category: cs.CL

TL;DR: 本文提出一种动态字符分组方法，结合BPE分词和层次式字符补丁分组，提升语言模型效率，无需额外模型即可兼顾紧凑性与表现力，并在多语种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有BPE等分词方法虽在词汇紧凑性与表达能力上有平衡，但在表示生僻词时低效且需大嵌入矩阵。字符级方法虽缓解此问题，但在Transformer结构中表现缓慢；而新的层次化模型现有的补丁策略依赖空格或辅助模型，局限性较大。因此亟需一种能提升表示效率且适用范围广的分词方法。

Method: 作者提出将BPE分词结构基础上，动态分组合适字符集合（补丁），具体方法为在BPE词元后加“补丁结束”标记，并用二级BPE压缩控制补丁粒度，无需额外外部模型和依赖即可实现高效灵活的字符分组。

Result: 实验证明，该方法在性能上达到或超过动态熵及基于空白符的补丁分组策略，同时保持词汇表紧凑性，在不同语言和任务上具广泛适用性。

Conclusion: 提出的动态字符分组方法有效平衡了表现力和效率，克服了BPE、字符级和现有补丁方法的缺点，可作为语言模型分词的新方案。

Abstract: Subword tokenization methods like Byte Pair Encoding (BPE) are widely used in
large language models due to their balance of vocabulary compactness and
representational power. However, they suffer from inefficiencies in
representing rare words and require large embedding matrices. Character-level
models address these issues but introduce performance bottlenecks, particularly
in Transformer-based architectures. Recent hierarchical models attempt to merge
the benefits of both paradigms by grouping characters into patches, but
existing patching strategies either rely on whitespace-limiting applicability
to certain languages, or require auxiliary models that introduce new
dependencies. In this paper, we propose a dynamic character grouping method
that leverages the structure of existing BPE tokenization without requiring
additional models. By appending explicit end-of-patch markers to BPE tokens and
introducing a second-level BPE compression stage to control patch granularity,
our method offers efficient, flexible, and language-agnostic representations.
Empirical results demonstrate that our approach matches or exceeds the
performance of dynamic entropy- and whitespace-based patching strategies, while
maintaining a compact vocabulary.

</details>


### [119] [Latent Reasoning in LLMs as a Vocabulary-Space Superposition](https://arxiv.org/abs/2510.15522)
*Jingcheng Deng,Liang Pang,Zihao Wei,Shichen Xu,Zenghao Duan,Kun Xu,Yang Song,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 作者提出Latent-SFT框架，通过将大型语言模型（LLM）的推理过程由显式推理转为词汇概率空间中的隐式推理，实现高效、性能不损失的推理。


<details>
  <summary>Details</summary>
Motivation: 显式链式推理虽然性能强，但计算开销大。之前的隐空间推理方法虽能提升效率，但因潜空间无结构导致性能大幅下降。该文为了解决隐空间推理的性能劣化问题，对潜空间结构进行约束优化。

Method: 作者将隐空间约束于LLM词汇表的列空间，把推理当作词汇概率的叠加。在Latent-SFT中，第一阶段利用专用Attention Mask及Latent Token Encoder指导生成潜变量，第二阶段去除Encoder，训练LLM自主生成潜变量，通过KL损失和CE损失优化。

Result: Latent-SFT在GSM8k数据集上达到新SOTA，推理链条长度缩短至原来的1/4，同时优于过往隐空间推理方法。数学领域（Math500、AIME24）测试显示该方法明显优于以往基于隐藏态的隐空间推理。文中还提出了新的压缩率和平行性指标，验证了潜空间推理在效率上的优势。

Conclusion: Latent-SFT框架在兼顾推理性能的同时大幅提升了推理效率，通过结构化潜空间，使得LLM隐式推理兼具高效和准确，拓展了高效LLM推理的新途径。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities with
chain-of-thought prompting, but explicit reasoning introduces substantial
computational overhead. Recent work on latent reasoning reduces this cost by
reasoning in latent space without explicit supervision, but performance drops
significantly. Our preliminary experiments suggest that this degradation stems
from the unstructured latent space, which makes fitting latent tokens
difficult. To address this, we restrict the latent space to the column space of
the LLM vocabulary, treating latent reasoning as a superposition over
vocabulary probabilities. Once latent reasoning concludes, it collapses into an
eigenstate of explicit reasoning to yield the final answer. Based on this idea,
we propose Latent-SFT, a two-stage learning framework. In the first stage, we
design two specialized attention masks to guide the Latent Token Encoder in
generating latent tokens, allowing the LLM to produce the correct answer
conditioned on them. In the second stage, the Latent Token Encoder is
discarded, and the LLM is directly trained to generate these latent tokens
autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT
sets a new state of the art on GSM8k, matching explicit SFT performance while
cutting reasoning chains by up to 4 times and outperforming prior latent
methods. On Math500 and AIME24, lexical probability-based latent reasoning also
clearly surpasses hidden-state-based approaches. Our metrics of effective
compression rate and effective global parallelism further show that latent
reasoning is both the compression of a single path and the superposition of
multiple paths.

</details>


### [120] [MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval](https://arxiv.org/abs/2510.15543)
*Qiyu Wu,Shuyang Cui,Satoshi Hayakawa,Wei-Yao Wang,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.CL

TL;DR: 本文研究了多模态检索中的统一编码器在传统对比学习下容易学习模态捷径、导致分布外鲁棒性差的问题，提出通过模态组合增强鲁棒性的框架，有效提升检索表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索领域越来越关注直接处理文本和图像等多模态组合输入的统一编码器，但这些编码器如果沿用传统对比学习训练，容易习得模态偏置（shortcut），即只依赖于单一模态特征而不是组合信息，这使得模型在分布外数据下表现较差。

Method: 提出了模态组合感知框架。具体做法包括：引入偏好损失，使多模态嵌入比其单模态嵌入表现更优；提出组合正则目标，将多模态嵌入与由单模态组成的原型对齐。这些目标在训练过程中显式建模组合表示与单模态表达之间的结构关系。

Result: 在多个基准数据集上进行实验，提出的方法在分布外检索任务上取得了明显提升，相较于传统基线方法表现更鲁棒。

Conclusion: 模态组合感知是利用多模态大语言模型作为统一编码器时实现鲁棒性提升的有效原则。

Abstract: Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.

</details>


### [121] [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)
*Sibo Xiao,Jinyuan Fu,Zhongle Xie,Lidan Shou*

Main category: cs.CL

TL;DR: 提出TokenTiming算法，利用DTW实现通用推测解码，无需相同词表即可加速大语言模型推断。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法要求草稿模型与目标模型词表一致，限制了可用模型的选择，使用不便。作者希望打破这一限制，提高解码效率和适用性。

Method: 提出TokenTiming算法，借鉴动态时间规整（DTW），通过将草稿序列重新编码，并用DTW对齐序列，可以在词表不一致的情况下实现推测采样概率的迁移，支持任意现成模型，无需重新训练。

Result: 在多种任务上通过实验，方法可带来1.57倍的推理加速效果。

Conclusion: TokenTiming方法让推测解码摆脱了词表限制，提升了推理效率，使该技术更通用、实用，便于灵活选择草稿模型。

Abstract: Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.

</details>


### [122] [Rethinking Cross-lingual Gaps from a Statistical Viewpoint](https://arxiv.org/abs/2510.15551)
*Vihari Piratla,Purvam Jain,Darshan Singh,Partha Talukdar,Trevor Cohn*

Main category: cs.CL

TL;DR: 本文提出了跨语言知识检索准确率下降主要源于目标语言响应方差而非潜在表示偏差，基于偏差-方差分解对这一现象进行理论建模，并通过实验和推理时干预显著缩小了该跨语言差距。


<details>
  <summary>Details</summary>
Motivation: 跨语言使用大语言模型检索知识时，准确率通常比源语言低。以往认为是源/目标语言间潜在表示差异所致，但实际问题可能另有原因，急需找到本质原因并给出改善方法。

Method: 提出假设：目标语言响应方差是跨语言准确率下降的主因。用偏差-方差分解理论形式化“跨语言差距”。通过大量实验证明假设成立，并尝试了多种推理时干预措施，包括简单的提示设计以降低输出方差。

Result: 实验证明理论观点成立，且通过推理时对方差的控制，尤其是设计简单提示，能让目标语言的准确率提升20%-25%。

Conclusion: 控制目标语言响应的方差是缩小LLM跨语言差距的关键，可通过不同推理时技术，特别是优化提示方式，达到显著性能提升。

Abstract: Any piece of knowledge is usually expressed in one or a handful of natural
languages on the web or in any large corpus. Large Language Models (LLMs) act
as a bridge by acquiring knowledge from a source language and making it
accessible when queried from target languages. Prior research has pointed to a
cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a
target language compared to when the query is in the source language. Existing
research has rationalized divergence in latent representations in source and
target languages as the source of cross-lingual gap. In this work, we take an
alternative view and hypothesize that the variance of responses in the target
language is the main cause of this gap. For the first time, we formalize the
cross-lingual gap in terms of bias-variance decomposition. We present extensive
experimental evidence which support proposed formulation and hypothesis. We
then reinforce our hypothesis through multiple inference-time interventions
that control the variance and reduce the cross-lingual gap. We demonstrate a
simple prompt instruction to reduce the response variance, which improved
target accuracy by 20-25% across different models.

</details>


### [123] [Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15552)
*Jinliang Liu*

Main category: cs.CL

TL;DR: 本文提出了ParallaxRAG框架，通过多视角解耦查询和知识图谱三元组，有效提升LLM基于知识图谱的多跳推理能力，并减少幻觉现象。实验显示该方法在QA和检索任务上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然具备强大的语言理解能力，但在多跳推理和事实准确性方面经常出现幻觉或错误。现有基于知识图谱的检索增强方法（KG-RAG）存在嵌入表征平面化和路径探索噪声大的问题。

Method: ParallaxRAG将查询和知识图谱三元组分别映射到多个视角的空间，通过对不同注意力头在不同推理阶段的语义关系建模，控制推理路径多样性和相关性。这能生成结构更清晰的子图，引导LLM逐步、稳健地完成推理。

Result: 在WebQSP和CWQ数据集上的实验，使用统一的再现环境（BGE-M3 + Llama3.1-8B），ParallaxRAG在检索和问答性能上具有竞争力，同时显著降低了幻觉现象，具备较好的泛化能力。

Conclusion: 多视角注意力头的专化是知识图谱支持多跳推理的一种有效并可行的方向，ParallaxRAG验证了其提升LLM可控性和准确性的潜力。

Abstract: Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.

</details>


### [124] [KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models](https://arxiv.org/abs/2510.15558)
*Dongjun Kim,Chanhee Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该论文提出了韩语指令跟随能力评测基准（KITE），填补了现有大语言模型韩语能力评测的空白，并通过自动和人工评价全面分析了不同模型的表现。


<details>
  <summary>Details</summary>
Motivation: 目前对于大语言模型的指令跟随能力评测主要集中在英语，忽略了韩语等其他语言在语法、表达、文化等方面的独特性，因此亟需有针对韩语的评测体系。

Method: 论文设计并发布了一个全新的韩语指令跟随能力基准KITE，涵盖通用和韩语特有的开放式任务，结合了自动打分和人工评估的方法来评价模型表现，并公开了数据集及代码。

Result: KITE评测显示不同大语言模型在韩语指令跟随任务上的表现存在差异，通过综合评价深入揭示了其优劣势。

Conclusion: KITE为韩语及其他少数语言的大语言模型评测和开发提供了重要工具，有助于推动语言和文化多样性的模型研究。

Abstract: The instruction-following capabilities of large language models (LLMs) are
pivotal for numerous applications, from conversational agents to complex
reasoning systems. However, current evaluations predominantly focus on English
models, neglecting the linguistic and cultural nuances of other languages.
Specifically, Korean, with its distinct syntax, rich morphological features,
honorific system, and dual numbering systems, lacks a dedicated benchmark for
assessing open-ended instruction-following capabilities. To address this gap,
we introduce the Korean Instruction-following Task Evaluation (KITE), a
comprehensive benchmark designed to evaluate both general and Korean-specific
instructions. Unlike existing Korean benchmarks that focus mainly on factual
knowledge or multiple-choice testing, KITE directly targets diverse, open-ended
instruction-following tasks. Our evaluation pipeline combines automated metrics
with human assessments, revealing performance disparities across models and
providing deeper insights into their strengths and weaknesses. By publicly
releasing the KITE dataset and code, we aim to foster further research on
culturally and linguistically inclusive LLM development and inspire similar
endeavors for other underrepresented languages.

</details>


### [125] [Finetuning LLMs for EvaCun 2025 token prediction shared task](https://arxiv.org/abs/2510.15561)
*Josef Jon,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文介绍了作者团队在EvaCun 2025比赛中进行token预测任务的做法。作者使用了Command-R、Mistral和Aya Expanse三种大型语言模型（LLM）分别在赛题数据上微调，探索了三种不同的提示（prompt）策略进行预测，并在保留数据上做了评估。


<details>
  <summary>Details</summary>
Motivation: 作者虽然对该任务所涉及的领域及语言了解有限，但希望通过仅依靠给定训练数据，借助现成的LLM基础架构，测试微调和简单prompt工程在此任务中的效果。

Method: 使用三种LLM（Command-R, Mistral, Aya Expanse），在主办方提供的数据集上直接微调，不进行额外领域适应、数据清洗等任务特定预处理。设计并比较了三种不同的prompt方式来进行token预测，并在未训练的保留集上对效果进行对比评估。

Result: 对三种模型和三种prompt设计在保留测试集上进行了效果评估，展示不同模型和方法在token预测上的表现差异。

Conclusion: 即便对领域知识和任务语言不熟悉，利用现有强大的预训练语言模型配合微调，在仅使用训练数据的情况下，也可获得一定的token预测能力。探讨了不同prompt策略对效果的影响。

Abstract: In this paper, we present our submission for the token prediction task of
EvaCun 2025. Our sys-tems are based on LLMs (Command-R, Mistral, and Aya
Expanse) fine-tuned on the task data provided by the organizers. As we only
pos-sess a very superficial knowledge of the subject field and the languages of
the task, we simply used the training data without any task-specific
adjustments, preprocessing, or filtering. We compare 3 different approaches
(based on 3 different prompts) of obtaining the predictions, and we evaluate
them on a held-out part of the data.

</details>


### [126] [From Ghazals to Sonnets: Decoding the Polysemous Expressions of Love Across Languages](https://arxiv.org/abs/2510.15569)
*Syed Mohammad Sualeh Ali*

Main category: cs.CL

TL;DR: 本文研究了乌尔都语诗歌中表示“爱”的三个词（pyaar、muhabbat、ishq）的细微差别，揭示了它们各自独特的情感和文化内涵。通过情景分析和词嵌入技术，该研究展示了乌尔都诗歌中爱的多元表达及其与英语的语义差距。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语诗歌以其丰富的情感表达著称，尤其是“爱”的主题。尽管有多个词表达“爱”，但它们在情感深度和使用语境上有微妙差异。本研究动机在于挖掘这些词汇背后的多义性，揭示只有乌尔都语诗歌中才能找到的情感层次，以及这些词在英语中缺乏直接对等物的现实，使跨语言情感表达对比更具意义。

Method: 作者采用了多义性个案分析法，聚焦“pyaar”、“muhabbat”和“ishq”三词，通过分析它们在诗歌中的语境和使用情况来揭示细微差别。此外，研究利用词嵌入技术，将乌尔都语和英语的爱情相关词语进行语义空间对比与可视化，量化词义间的文化和情感距离。

Result: 结果表明，这三个词在诗歌中展现出不同层次和色彩的爱情体验，各自拥有独特的语义区域。这些词的多样性和精细化表达展现了乌尔都语独特的文化内涵。与英语相关词汇相比，乌尔都诗歌中的爱具有更为细致和复杂的语义结构。

Conclusion: 本研究揭示了乌尔都语诗歌中“爱”的表达丰富性和复杂性，强调了语言和文化对情感表达的独特塑造作用。研究为理解和欣赏乌尔都文学提供了新视角，也有助于跨文化语义研究。

Abstract: This paper delves into the intricate world of Urdu poetry, exploring its
thematic depths through a lens of polysemy. By focusing on the nuanced
differences between three seemingly synonymous words (pyaar, muhabbat, and
ishq) we expose a spectrum of emotions and experiences unique to the Urdu
language. This study employs a polysemic case study approach, meticulously
examining how these words are interwoven within the rich tapestry of Urdu
poetry. By analyzing their usage and context, we uncover a hidden layer of
meaning, revealing subtle distinctions which lack direct equivalents in English
literature. Furthermore, we embark on a comparative analysis, generating word
embeddings for both Urdu and English terms related to love. This enables us to
quantify and visualize the semantic space occupied by these words, providing
valuable insights into the cultural and linguistic nuances of expressing love.
Through this multifaceted approach, our study sheds light on the captivating
complexities of Urdu poetry, offering a deeper understanding and appreciation
for its unique portrayal of love and its myriad expressions

</details>


### [127] [BiMax: Bidirectional MaxSim Score for Document-Level Alignment](https://arxiv.org/abs/2510.15577)
*Xiaotian Wang,Takehito Utsuro,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了一种高效的跨语言文档对齐方法BiMax，能在保证OT方法相似准确率的同时，速度提升约100倍，并提供了相关工具EmbDA。


<details>
  <summary>Details</summary>
Motivation: 现有的文档对齐方法，如OT虽然准确，但效率低下，难以满足大规模网页数据挖掘的需求，因此需要更高效的算法。

Method: 提出了基于跨语种双向最大相似分数（BiMax）的文档对齐方法，并对多语种句嵌入模型在文档对齐任务中的表现进行了全面分析。

Result: 在WMT16双语文档对齐任务上，BiMax方法在对齐精度与OT方法持平的情况下，速度提高了约100倍。

Conclusion: BiMax为跨语言文档对齐提供了高效的解决方案，有助于大规模数据的处理，并已将所有方法整合进EmbDA工具，方便学术和工业应用。

Abstract: Document alignment is necessary for the hierarchical mining (Ba\~n\'on et
al., 2020; Morishita et al., 2022), which aligns documents across source and
target languages within the same web domain. Several high precision sentence
embedding-based methods have been developed, such as TK-PERT (Thompson and
Koehn, 2020) and Optimal Transport (OT) (Clark et al., 2019; El-Kishky and
Guzm\'an, 2020). However, given the massive scale of web mining data, both
accuracy and speed must be considered. In this paper, we propose a
cross-lingual Bidirectional Maxsim score (BiMax) for computing doc-to-doc
similarity, to improve efficiency compared to the OT method. Consequently, on
the WMT16 bilingual document alignment task, BiMax attains accuracy comparable
to OT with an approximate 100-fold speed increase. Meanwhile, we also conduct a
comprehensive analysis to investigate the performance of current
state-of-the-art multilingual sentence embedding models. All the alignment
methods in this paper are publicly available as a tool called EmbDA
(https://github.com/EternalEdenn/EmbDA).

</details>


### [128] [The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works](https://arxiv.org/abs/2510.15594)
*Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本文提出了一个新的法语长篇小说核心指代解析语料库，并开发了一套适用于长文档的解析方法。


<details>
  <summary>Details</summary>
Motivation: 现有关于长文档（如完整小说）的核心指代注释语料极为稀缺，而相关需求在计算文学分析领域日益增长。

Method: 作者构建了包含三部完整法语小说的核心指代注释语料库，并设计了一套模块化指代消解流程，用于精细化误差分析和适应长链指代关系。

Result: 所提出的方法能够有效扩展到长文档并达到有竞争力的效果。此外，该体系还可辅助分析小说人物性别，拓展了文学和NLP领域的应用场景。

Conclusion: 新语料库和方法为长文档核心指代消解和文学分析提供了坚实基础，有助于提升相关模型的表现和应用广度。

Abstract: While coreference resolution is attracting more interest than ever from
computational literature researchers, representative datasets of fully
annotated long documents remain surprisingly scarce. In this paper, we
introduce a new annotated corpus of three full-length French novels, totaling
over 285,000 tokens. Unlike previous datasets focused on shorter texts, our
corpus addresses the challenges posed by long, complex literary works, enabling
evaluation of coreference models in the context of long reference chains. We
present a modular coreference resolution pipeline that allows for fine-grained
error analysis. We show that our approach is competitive and scales effectively
to long documents. Finally, we demonstrate its usefulness to infer the gender
of fictional characters, showcasing its relevance for both literary analysis
and downstream NLP tasks.

</details>


### [129] [HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination](https://arxiv.org/abs/2510.15614)
*Tingting Chen,Beibei Lin,Zifeng Yuan,Qiran Zou,Hongyu He,Yew-Soon Ong,Anirudh Goyal,Dianbo Liu*

Main category: cs.CL

TL;DR: 本论文提出HypoSpace套件，用于衡量大语言模型在生成多种科学假设时的能力，关注其生成的假设的有效性、独特性与覆盖度，发现现有模型存在模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 科学问题通常存在多重解释，同一组观测可能对应不同机制的假设。语言模型不仅需能给出正确答案，还应提出多样化、不冗余的假设。因此需评估模型生成假设集的能力。

Method: 作者设计了HypoSpace诊断套件，视LLM为假设采样器，并提出三项指标：有效性（假设与观测一致的精准度）、独特性（假设间去重）、覆盖度（覆盖所有可能假设的程度）。在三个结构化科学任务上实验：因果图重建、受重力约束的3D重建以及布尔遗传相互作用。通过有穷、可枚举的假设空间和确定性校验，实现严格评测。

Result: 实验显示无论是指令微调还是推理加强的LLM，有效性指标通常较高，但假设的独特性和覆盖度随着空间扩大显著下降，表明模型会出现“模式崩溃”——只生成有限样式的假设。该现象在只看正确性时无法发现。

Conclusion: HypoSpace为评估LLM在科学任务中的假设生成和空间覆盖提供了精细工具，可揭示模型在多样性和覆盖性方面的不足。作者认为该套件比单纯排行榜更适合作为模型探索能力的探针，推动科研辅助LLM的发展。

Abstract: As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

</details>


### [130] [Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection](https://arxiv.org/abs/2510.15685)
*Joshua Wolfe Brook,Ilia Markov*

Main category: cs.CL

TL;DR: 该研究利用大语言模型(LLMs)为仇恨言论检测生成背景知识，并融入检测器输入，有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论检测面临语境理解匮乏的问题，尤其是在隐含或多模态场景下。作者希望通过引入丰富的上下文信息提升模型对复杂仇恨言论的识别能力。

Method: 使用LLMs动态生成背景上下文（包括命名实体和全文提示两种策略），并分别以文本拼接、嵌入拼接、层级Transformer融合及LLM增强文本等四种方式，将这些上下文信息融合进HSD分类器输入。

Result: 在Latent Hatred文本数据集和MAMI多模态数据集上实验，发现上下文信息及其融合方式对性能有明显提升作用。相比无上下文基线，最佳方案（嵌入拼接）F1分数提升最高可达文本任务3分、多模态任务6分。

Conclusion: 通过LLMs生成并有效融入背景知识，可以显著提高文本及多模态仇恨言论检测的效果，尤其是在隐含或复杂语境下。

Abstract: This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.

</details>


### [131] [Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth](https://arxiv.org/abs/2510.15719)
*Helia Hashemi,Victor Rühle,Saravan Rajmohan*

Main category: cs.CL

TL;DR: 本文提出了一种动态调整检索文档数量的检索增强推理模型，可根据查询内容和检索结果灵活分配资源，并通过成本感知的强化学习方法实现效率提升。在保证模型有效性的同时，显著降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强推理模型虽然性能强大，但计算资源消耗高，主要由于检索和推理过程中的token数量大增。研究者希望在不损失模型效果的前提下，提高计算效率，降低资源占用。

Method: 1. 提出一种动态调整检索文档数量的机制，根据不同查询和检索结果调整需要读取的文档长度；2. 设计了成本感知的优势函数，将资源消耗纳入强化学习训练过程，有效引导模型学习更优的资源分配策略；3. 探索了内存和延迟约束下的算法实现，并在实际强化学习优化算法（如proximal和group relative policy optimization）中进行了验证。

Result: 在七个公开问答数据集上测试，方法实现了模型延迟平均降低16-20%，而在答案精确匹配度上平均提升5%。表明模型在提升效率的同时还能带来效果的微幅提升。

Conclusion: 动态成本感知的资源分配和强化学习训练为检索增强推理模型带来了显著的计算效率提升，同时还能略微提升答案准确率，有望为实际大规模应用提供更好的支撑。

Abstract: Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.

</details>


### [132] [Attention Sinks in Diffusion Language Models](https://arxiv.org/abs/2510.15731)
*Maximo Eduardo Rulli,Simone Petruzzi,Edoardo Michielon,Fabrizio Silvestri,Simone Scardapane,Alessio Devoto*

Main category: cs.CL

TL;DR: 本文分析了掩码扩散语言模型（DLMs）中的注意力模式，发现其存在与自回归模型（ARMs）不同的‘注意力下沉’现象，并阐明了两者的机制差异。


<details>
  <summary>Details</summary>
Motivation: 目前DLMs以其高效并行性和竞争性能成为替代ARMs的有力方案，但对于内部注意力机制、尤其是‘注意力下沉’现象的理解仍然不足。本文旨在填补这一知识空白。

Method: 作者通过实证分析，研究了DLMs的注意力分布，特别比较了DLMs和ARMs中注意力下沉现象的位置、动态性及对性能的影响。

Result: 发现DLMs中的注意力下沉位置在生成过程中具有动态变化特性，而ARMs则固定；并且DLMs对注意力下沉位置的屏蔽表现出较强鲁棒性，仅轻微影响性能。

Conclusion: 掩码扩散语言模型的注意力机制显著不同于自回归模型，这一认识有助于更好地理解和进一步改进DLMs的结构与应用。

Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising
alternative to traditional Autoregressive Models (ARMs). DLMs employ
transformer encoders with bidirectional attention, enabling parallel token
generation while maintaining competitive performance. Although their efficiency
and effectiveness have been extensively studied, the internal mechanisms that
govern DLMs remain largely unexplored. In this work, we conduct an empirical
analysis of DLM attention patterns, focusing on the attention sinking
phenomenon, an effect previously observed in various transformer-based
architectures. Our findings reveal that DLMs also exhibit attention sinks, but
with distinct characteristics. First, unlike in ARMs, the sink positions in
DLMs tend to shift throughout the generation process, displaying a dynamic
behaviour. Second, while ARMs are highly sensitive to the removal of attention
sinks, DLMs remain robust: masking sinks leads to only a minor degradation in
performance. These results provide new insights into the inner workings of
diffusion-based language models and highlight fundamental differences in how
they allocate and utilize attention compared to autoregressive models.

</details>


### [133] [LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2510.15746)
*Gao Yang,Yuhang Liu,Siyu Miao,Xinyue Liang,Zhengyang Liu,Heyan Huang*

Main category: cs.CL

TL;DR: 本文探讨了能否用博弈论方法评估大语言模型（LLMs），提出用LLMs相互评审结果，并用博弈论方法聚合后与人类评判对比，发现模型评估与人类观点有趋同与差异。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM的评估方法，如固定答案任务，无法捕捉现代LLM表现出的复杂性、主观性和开放性，无法准确反映LLM的真实能力，因此需要新的评估范式。

Method: 作者提出自动互评框架：LLM模型通过自我对弈和互相评议彼此输出；再利用博弈论的投票聚合算法，整理互评结果，最后与人类投票行为对比分析二者一致性。

Result: 实验中，模型间互评所生成的排名与人类评价在部分方面一致，但也存在明显差异，展示了理论方法与实际人类判断之间的融合点和偏离。

Conclusion: 该方法首次结合互评、博弈论聚合与人类校验，验证了该思路评估LLM能力的可行性，并揭示了自动互评评估的潜力与局限。

Abstract: Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

</details>


### [134] [On Non-interactive Evaluation of Animal Communication Translators](https://arxiv.org/abs/2510.15768)
*Orr Paradise,David F. Gruber,Adam Tauman Kalai*

Main category: cs.CL

TL;DR: 本文探讨如何在缺乏参考翻译的情况下，验证动物（如鲸鱼）语言到英文的AI翻译器是否有效。作者提出了一种仅依赖英文输出结果的机器翻译质量评测方法，并通过人类罕见语言和构造语言的实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对于动物语言的AI翻译系统缺乏可用的参考翻译，传统人工验证方式在实际应用中既不可行，也昂贵、存在伦理风险。因此，亟需一种无需与动物互动或外部观测、成本和风险较低的评估方法。

Method: 作者提出结合片段级逐步翻译与NLP经典shuffle测试的方法，仅通过AI产生的英文输出进行质量评估。具体做法是：将动物通信逐步翻译为英文，再随机打乱这些输出的顺序，通过人类判别原顺序是否更有意义，来评估翻译质量。其有效性通过罕见人类语言和构造语言实验验证，实验中可获取必要的参考翻译对比。

Result: 该参考无关的评估指标与基于参考翻译的传统评测方法高度相关。人类样本实验结果证明，即使数据极度稀缺，该新方法也能有效评估机器翻译质量。理论分析部分还说明，在学习初期，强调与动物的互动未必有效或必要。

Conclusion: 参考无关的翻译质量评估方法是可行的，能够在缺乏与动物交互和观测的情况下，有效、廉价地验证AI动物翻译器的性能，对未来动物语言研究与AI系统的安全伦理具有积极意义。

Abstract: If you had an AI Whale-to-English translator, how could you validate whether
or not it is working? Does one need to interact with the animals or rely on
grounded observations such as temperature? We provide theoretical and
proof-of-concept experimental evidence suggesting that interaction and even
observations may not be necessary for sufficiently complex languages. One may
be able to evaluate translators solely by their English outputs, offering
potential advantages in terms of safety, ethics, and cost. This is an instance
of machine translation quality evaluation (MTQE) without any reference
translations available. A key challenge is identifying ``hallucinations,''
false translations which may appear fluent and plausible. We propose using
segment-by-segment translation together with the classic NLP shuffle test to
evaluate translators. The idea is to translate animal communication, turn by
turn, and evaluate how often the resulting translations make more sense in
order than permuted. Proof-of-concept experiments on data-scarce human
languages and constructed languages demonstrate the potential utility of this
evaluation methodology. These human-language experiments serve solely to
validate our reference-free metric under data scarcity. It is found to
correlate highly with a standard evaluation based on reference translations,
which are available in our experiments. We also perform a theoretical analysis
suggesting that interaction may not be necessary nor efficient in the early
stages of learning to translate.

</details>


### [135] [Emergence of Linear Truth Encodings in Language Models](https://arxiv.org/abs/2510.15804)
*Shauli Ravfogel,Gilad Yehudai,Tal Linzen,Joan Bruna,Alberto Bietti*

Main category: cs.CL

TL;DR: 大型语言模型内部存在能够区分真假陈述的线性子空间，本文通过透明的单层transformer模型，从机制层面解释了这种现象的产生。


<details>
  <summary>Details</summary>
Motivation: 虽然先前的研究表明大型语言模型中存在『真/假』分离的线性子空间，但其形成机制尚不清晰。作者希望通过可解释的简化模型探索这一机制并为实证发现提供理论基础。

Method: 作者设计了一个一层的transformer玩具模型，对事实性（真假）陈述进行了模拟，分析模型在特定数据分布（事实陈述相互共现、非事实陈述亦然）下的表现。同时，在真实预训练模型上进行了对应实验以验证结果。

Result: 实验发现，在该设定下，模型首先记忆个别事实关联，随后逐步学习到用线性方式区分真和假。这种线性表征降低了语言模型损失。

Conclusion: 线性真值表示的产生可由数据分布与学习过程推动，作者用简单模型从机制上证明了该表征的可实现性，并在真实大模型中进行了实证验证。

Abstract: Recent probing studies reveal that large language models exhibit linear
subspaces that separate true from false statements, yet the mechanism behind
their emergence is unclear. We introduce a transparent, one-layer transformer
toy model that reproduces such truth subspaces end-to-end and exposes one
concrete route by which they can arise. We study one simple setting in which
truth encoding can emerge: a data distribution where factual statements
co-occur with other factual statements (and vice-versa), encouraging the model
to learn this distinction in order to lower the LM loss on future tokens. We
corroborate this pattern with experiments in pretrained language models.
Finally, in the toy setting we observe a two-phase learning dynamic: networks
first memorize individual factual associations in a few steps, then -- over a
longer horizon -- learn to linearly separate true from false, which in turn
lowers language-modeling loss. Together, these results provide both a
mechanistic demonstration and an empirical motivation for how and why linear
truth representations can emerge in language models.

</details>


### [136] [Paper2Web: Let's Make Your Paper Alive!](https://arxiv.org/abs/2510.15842)
*Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen*

Main category: cs.CL

TL;DR: 本论文提出了Paper2Web基准和PWAgent工具，用于将学术论文自动化转换为高质量、交互性强的在线学术主页，并通过多维度的方法对其评估。实验结果显示，该方案大幅优于现有模板和直接转换方法。


<details>
  <summary>Details</summary>
Motivation: 当前学术项目主页在内容展示、交互和导航方面存在不足，现有自动化生成方法难以生成布局合理、富有交互性的页面。此外，缺乏系统性的评测标准。

Method: 提出Paper2Web数据集和多维度评测体系，包括基于规则和人工判别的指标（如互联性、完整性、交互性、美观性、信息丰富性）及知识保留测试（PaperQuiz）；开发了PWAgent，把科研论文自动转为交互、多媒体丰富的网页，采用MCP工具迭代优化内容和布局。

Result: PWAgent在实验中显著优于模板生成和arXiv/alphaXiv网页，在页面互联性、美观性、知识传递等各项评测指标下均表现出色，且成本较低，在学术网页生成任务中达到最优结果。

Conclusion: Paper2Web和PWAgent为学术网页自动化生成领域提供了创新的数据基准和强大的工具链，推动了相关任务的评测和应用发展。PWAgent为研究成果传播提供了高效、优质的新途径。

Abstract: Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

</details>


### [137] [Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework](https://arxiv.org/abs/2510.15843)
*Shayan Rokhva,Mousa Alizadeh,Maryam Abdollahi Shamami*

Main category: cs.CL

TL;DR: 本文提出一种结合词典、模糊逻辑与Transformer（深度学习）的混合情感分析框架，实现对评论文本的极性和强度的连续评分，提高了在实时、专业领域语境下的表现。


<details>
  <summary>Details</summary>
Motivation: 在产品评价和社交媒体中，由于语言非正式和领域特定，现有情感分析方法难以准确把握情感极性和强度。作者旨在解决现有方法在细粒度情感分值与准确性上的不足。

Method: 方法首先通过VADER生成初步情感分数，再用DistilBERT输出的置信分和模糊逻辑进行两阶段调整，减少中性倾向、增强度量精度。最后自定义的模糊推理系统将分值映射到0-1区间，实现类专家判定。

Result: 在食品配送、电商、旅游、时尚领域四个数据集上，模型对用户评分分布对齐度、情感极端分辨、误判减少上有显著提升。定量（如混淆矩阵）和定性（案例分析、运行效率）分析均证实模型表现强劲且高效。

Conclusion: 本文证明了将符号推理与神经网络结合，能够获得可解释、细粒度且稳健的情感分析结果，特别适合于语言高度动态的应用场景。

Abstract: Accurately detecting sentiment polarity and intensity in product reviews and
social media posts remains challenging due to informal and domain-specific
language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer
framework that combines rule-based heuristics, contextual deep learning, and
fuzzy logic to generate continuous sentiment scores reflecting both polarity
and strength. The pipeline begins with VADER-based initial sentiment
estimations, which are refined through a two-stage adjustment process. This
involves leveraging confidence scores from DistilBERT, a lightweight
transformer and applying fuzzy logic principles to mitigate excessive
neutrality bias and enhance granularity. A custom fuzzy inference system then
maps the refined scores onto a 0 to 1 continuum, producing expert)like
judgments. The framework is rigorously evaluated on four domain-specific
datasets. food delivery, e-commerce, tourism, and fashion. Results show
improved alignment with user ratings, better identification of sentiment
extremes, and reduced misclassifications. Both quantitative metrics
(distributional alignment, confusion matrices) and qualitative insights (case
studies, runtime analysis) affirm the models robustness and efficiency. This
work demonstrates the value of integrating symbolic reasoning with neural
models for interpretable, finegrained sentiment analysis in linguistically
dynamic domains.

</details>


### [138] [SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling](https://arxiv.org/abs/2510.15851)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 本文研究了语音理解中的slot filling任务，探讨了如何利用speechLLM实现更高效和泛化的端到端语音slot填充。作者分析任务上限、现有gap，并提出改进策略以提升表现。


<details>
  <summary>Details</summary>
Motivation: 传统的slot filling需要语音识别与NLU串联处理，流程繁琐且有限。随着speechLLM引入端到端能力，有机会提升效率和泛化能力，但实际效果与最优情况存在差距。作者希望量化这种差距，并探索提升方案。

Method: 本文首先为slot filling任务设定经验上限，然后分析当前speechLLM在性能、鲁棒性和泛化性上的缺口。接着从训练数据、模型架构和训练策略三方面提出具体改进方法，并进行实验评估。

Result: 每种改进措施都显著提升了slot filling在性能、鲁棒性和泛化性上的表现，缩小了与经验上限的差距，同时揭示了模型应用中的实际挑战。

Conclusion: speechLLM在slot filling任务中有巨大潜力，但仍需改进数据、结构和训练策略。作者的工作为后续优化和实际应用提供实证参考和方法指导。

Abstract: Slot filling is a crucial subtask in spoken language understanding (SLU),
traditionally implemented as a cascade of speech recognition followed by one or
more natural language understanding (NLU) components. The recent advent of
speech-based large language models (speechLLMs), which integrate speech and
textual foundation models, has opened new avenues for achieving speech
understanding tasks in a more unified, generative, and instruction-following
manner while promising data and compute efficiency with zero-shot abilities,
generalizing to unseen slot labels. We address the slot-filling task by
creating an empirical upper bound for the task, identifying performance,
robustness, and generalization gaps, and proposing improvements to the training
data, architecture, and training strategies to narrow the gap with the upper
bound result. We show that each of these measures improve performance
substantially, while highlighting practical challenges and providing empirical
guidance and insights for harnessing these emerging models.

</details>


### [139] [InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training](https://arxiv.org/abs/2510.15859)
*Pengkai Wang,Qi Zuo,Pengwei Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 本论文提出了ORBIT，一种基于评分规则的增量训练框架，针对高风险的医疗对话任务，提升了大模型在开放性领域的表现，尤其是在奖励定义模糊的场景。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过强化学习在有明确定义奖励（如编程、数学）领域取得显著进步，但在奖励不明确的开放性领域（如医学对话）效果有限，亟需新方法解决缺乏稳健奖励函数的问题。

Method: 提出ORBIT框架，将合成对话生成与动态创建评分规则（rubric）结合，利用rubric引导模型的增量强化学习过程，实现对学习的界定和反馈，不依赖外部专业知识或手工规则。

Result: 在Qwen3-4B-Instruct模型上应用后，仅用2千条样本在HealthBench-Hard基准上性能由7.0提升到27.2，达到同类规模模型最优表现。同时rubric驱动的RL在多种医疗咨询场景下均有一致提升。

Conclusion: rubric引导的反馈机制为大型语言模型在复杂、开放性任务中的训练提供了一种可扩展和有效的新范式，有望推广到更多需要主观判断和上下文依赖的高难任务上。

Abstract: Large Language Models (LLMs) have shown substantial advances through
reinforcement learning (RL), particularly in domains where rewards can be
programmatically verified, such as mathematics and code. In these areas, models
benefit from a well-defined operational base guided by explicit rule-based
objectives. However, this progress reveals a significant limitation: in
open-ended domains where rewards are ambiguous, subjective, or
context-dependent, such as creative writing, scientific reasoning, and notably
medical consultation, robust reward functions are lacking, making these areas
challenging for current RL strategies. To bridge this gap, we introduce ORBIT,
an open-ended rubric-based incremental training framework specifically designed
for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue
generation with the dynamic creation of rubrics, employing these rubrics to
direct an incremental RL process. In particular, this approach does not depend
on external medical knowledge or manual rules, instead utilizing rubric-guided
feedback to shape learning. When implemented on the Qwen3-4B-Instruct model,
our method can greatly enhance its performance on the HealthBench-Hard
benchmark from 7.0 to 27.2 using only 2k samples, thus achieving
state-of-the-art results for models of this scale. Our analysis confirms that
rubric-driven RL fos-ters consistent performance gains across diverse
consultation scenarios, going beyond simple numerical improvements. These
findings underscore rubric-based feedback as a scalable strategy for advancing
LLMs in intricate, open-ended tasks.

</details>


### [140] [PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction](https://arxiv.org/abs/2510.15863)
*Simon Yu,Gang Li,Weiyan Shi,Peng Qi*

Main category: cs.CL

TL;DR: 本文提出了PolySkill框架，使大模型驱动的智能体能够在多种环境下学习通用且可组合的技能。通过将技能的抽象目标和具体执行方式分离，极大提升了技能的复用性和泛化能力，并实验证明PolySkill在多个评价指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用规模扩大，智能体需要不断学习新技能，但现有方法往往导致技能过于专用于单一场景，难以迁移和复用。作者希望提升技能的泛化与自适应能力，使智能体在现实复杂环境下具备持续学习和适应的能力。

Method: 提出PolySkill框架，将技能的“目标”（做什么）与“实现”（怎么做）分离，借鉴了软件工程的多态性思想。在互联网页面导航等任务中让智能体通过自我探索和目标细化不断学习和组合新技能，且支持跨站点迁移。

Result: 在实验中，PolySkill在已知站点上的技能复用率提升到1.7倍，在Mind2Web基准和未知站点上的成功率分别提升了9.4%和13.9%，操作步骤减少超20%。框架还促进了更高质量的自主任务提出和技能泛化。

Conclusion: 分离技能目标与执行对持续学习和自适应至关重要。PolySkill展示了让大模型智能体具备泛化与自我提升能力的可行路径，推动了自主智能体在开放网络环境下的持续学习和应用。

Abstract: Large language models (LLMs) are moving beyond static uses and are now
powering agents that learn continually during their interaction with external
environments. For example, agents can learn reusable skills while navigating
web pages or toggling new tools. However, existing methods for skill learning
often create skills that are over-specialized to a single website and fail to
generalize. We introduce PolySkill, a new framework that enables agents to
learn generalizable and compositional skills. The core idea, inspired by
polymorphism in software engineering, is to decouple a skill's abstract goal
(what it accomplishes) and its concrete implementation (how it is executed).
Experiments show that our method (1) improves skill reuse by 1.7x on seen
websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on
unseen websites, while reducing steps by over 20%. (3) In self-exploration
settings without specified tasks, our framework improves the quality of
proposed tasks and enables agents to learn generalizable skills that work
across different sites. By enabling the agent to identify and refine its own
goals, the PolySkill enhances the agent's ability to learn a better curriculum,
leading to the acquisition of more generalizable skills compared to baseline
methods. This work provides a practical path toward building agents capable of
continual learning in adaptive environments. Our findings show that separating
a skill's goal from its execution is a crucial step toward developing
autonomous agents that can learn and generalize across the open web
continuously.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [141] [Autonomous Reactive Masonry Construction using Collaborative Heterogeneous Aerial Robots with Experimental Demonstration](https://arxiv.org/abs/2510.15114)
*Marios-Nektarios Stamatopoulos,Elias Small,Shridhar Velhal,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文展示了一种利用异构无人机实现的全自主空中砌筑施工系统，并通过实验验证其有效性。两种专业无人机分别负责搬运砖块与施加粘结剂，并在动态任务分配和轨迹优化下实现高精度协作。


<details>
  <summary>Details</summary>
Motivation: 建筑业对恶劣环境下高效自主化施工的需求日增，无人机进行复杂结构的自动砌筑可提升安全性与灵活性，但仍面临精确操作和多任务协同的技术挑战。

Method: 设计两种特种无人机（搬运无人机+粘结剂无人机），结合依赖图和冲突图的任务规划模块，以及分层状态机与动态任务分配，实现对砌筑流程的全流程自动控制与实时应对。利用最小加速度轨迹优化无人机运动，并采用视觉定位与姿态估计算法，提升操作精度。

Result: 成功实现了砖块的高精度搬运与粘结剂自动施加，并通过实验与视频资料，验证了无人机间的高效协作和整个系统的自主、稳定运行能力。

Conclusion: 该工作首次实验性验证了由异构无人机完成的全自主空中砌筑流程，具有良好的推广前景，可为未来自动化空中建筑机器人系统奠定基础。

Abstract: This article presents a fully autonomous aerial masonry construction
framework using heterogeneous unmanned aerial vehicles (UAVs), supported by
experimental validation. Two specialized UAVs were developed for the task: (i)
a brick-carrier UAV equipped with a ball-joint actuation mechanism for precise
brick manipulation, and (ii) an adhesion UAV integrating a servo-controlled
valve and extruder nozzle for accurate adhesion application. The proposed
framework employs a reactive mission planning unit that combines a dependency
graph of the construction layout with a conflict graph to manage simultaneous
task execution, while hierarchical state machines ensure robust operation and
safe transitions during task execution. Dynamic task allocation allows
real-time adaptation to environmental feedback, while minimum-jerk trajectory
generation ensures smooth and precise UAV motion during brick pickup and
placement. Additionally, the brick-carrier UAV employs an onboard vision system
that estimates brick poses in real time using ArUco markers and a least-squares
optimization filter, enabling accurate alignment during construction. To the
best of the authors' knowledge, this work represents the first experimental
demonstration of fully autonomous aerial masonry construction using
heterogeneous UAVs, where one UAV precisely places the bricks while another
autonomously applies adhesion material between them. The experimental results
supported by the video showcase the effectiveness of the proposed framework and
demonstrate its potential to serve as a foundation for future developments in
autonomous aerial robotic construction.

</details>


### [142] [RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation](https://arxiv.org/abs/2510.15189)
*Xiangyu Chen,Chuhao Zhou,Yuxi Liu,Jianfei Yang*

Main category: cs.RO

TL;DR: 该论文提出了RM-RL框架，实现精细机器人操作时比现有方法更快、更稳定的性能提升，无需人工演示。


<details>
  <summary>Details</summary>
Motivation: 精细机器人操作需要高精度，传统模仿学习和离线强化学习依赖人工高质量示范，获取成本高且效率低，现有RL方法在精度和数据利用率上存在瓶颈。

Method: 提出Role-Model RL（RM-RL）框架，通过引入role-model角色模型策略，无需人工演示而用近似最优动作自动标注线上数据，将策略学习转为有监督形式，并结合线下混合训练，提高数据利用率和稳定性。

Result: 大量实验表明，RM-RL框架比现有RL方法收敛更快、更稳定，提升精准操控任务表现（平移精度提升53%，旋转精度提升20%），并在高难度实际任务中表现优异。

Conclusion: RM-RL无需人工示范，提升了机器人精细操作的效率和精度，对比传统方法具备显著优势，有助于提升现实复杂操作机器人的应用能力。

Abstract: Precise robot manipulation is critical for fine-grained applications such as
chemical and biological experiments, where even small errors (e.g., reagent
spillage) can invalidate an entire task. Existing approaches often rely on
pre-collected expert demonstrations and train policies via imitation learning
(IL) or offline reinforcement learning (RL). However, obtaining high-quality
demonstrations for precision tasks is difficult and time-consuming, while
offline RL commonly suffers from distribution shifts and low data efficiency.
We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies
online and offline training in real-world environments. The key idea is a
role-model strategy that automatically generates labels for online training
data using approximately optimal actions, eliminating the need for human
demonstrations. RM-RL reformulates policy learning as supervised training,
reducing instability from distribution mismatch and improving efficiency. A
hybrid training scheme further leverages online role-model data for offline
reuse, enhancing data efficiency through repeated sampling. Extensive
experiments show that RM-RL converges faster and more stably than existing RL
methods, yielding significant gains in real-world manipulation: 53% improvement
in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate
the successful execution of a challenging task, precisely placing a cell plate
onto a shelf, highlighting the framework's effectiveness where prior methods
fail.

</details>


### [143] [Lagrange-Poincaré-Kepler Equations of Disturbed Space-Manipulator Systems in Orbit](https://arxiv.org/abs/2510.15199)
*Borna Monazzah Moghaddam,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文提出了一种扩展的Lagrange-Poincare方程（LPE），用于在非惯性轨道参考系下建模航天器-机械臂系统的动力学，并命名为Lagrange-Poincare-Kepler方程（LPKE）。


<details>
  <summary>Details</summary>
Motivation: 现有LPE适用于地面或惯性空间中的载具-机械臂系统，但不能准确描述轨道环境下航天器、轨道和机械臂间的动力学耦合关系。本文旨在填补该空白，实现更精确的动力学建模，用于航天自主操作。

Method: LPKE框架结合了基于航天器的Euler-Poincare方程、轨道运动的Kepler动力学和机械臂形状空间的约化Euler-Lagrange方程，并使用指数关节参数化。运用Lagrange-d'Alembert原理，推导出显示轨道扰动及其动力学耦合的新型结构矩阵，同时系统纳入了外部施加的非对称力矩。

Result: 推导出一套封闭式LPKE结构矩阵，实现了轨道扰动与机械臂动力学显式耦合。通过对装有7自由度机械臂的航天器进行仿真，展现了该模型描述轨道影响和动力学耦合的有效性与数值优势。

Conclusion: LPKE为轨道环境中航天器-机械臂系统动力学建模与控制提供了系统化、高效的数学工具，可被直接应用于硬件在环仿真与模型驱动控制架构，推进航天自主机器人任务发展。

Abstract: This article presents an extension of the Lagrange-Poincare Equations (LPE)
to model the dynamics of spacecraft-manipulator systems operating within a
non-inertial orbital reference frame. Building upon prior formulations of LPE
for vehicle-manipulator systems, the proposed framework, termed the
Lagrange-Poincare-Kepler Equations (LPKE), incorporates the coupling between
spacecraft attitude dynamics, orbital motion, and manipulator kinematics. The
formalism combines the Euler-Poincare equations for the base spacecraft,
Keplerian orbital dynamics for the reference frame, and reduced Euler-Lagrange
equations for the manipulator's shape space, using an exponential joint
parametrization. Leveraging the Lagrange-d'Alembert principle on principal
bundles, we derive novel closed-form structural matrices that explicitly
capture the effects of orbital disturbances and their dynamic coupling with the
manipulator system. The LPKE framework also systematically includes externally
applied, symmetry-breaking wrenches, allowing for immediate integration into
hardware-in-the-loop simulations and model-based control architectures for
autonomous robotic operations in the orbital environment. To illustrate the
effectiveness of the proposed model and its numerical superiority, we present a
simulation study analyzing orbital effects on a 7-degree-of-freedom manipulator
mounted on a spacecraft.

</details>


### [144] [LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization](https://arxiv.org/abs/2510.15220)
*Kevin Christiansen Marsim,Minho Oh,Byeongho Yu,Seungjae Lee,I Made Aswin Nahrendra,Hyungtae Lim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种鲁棒的多传感器融合SLAM系统，结合了激光雷达、视觉、惯性测量单元和关节编码器信息，实现了足式机器人在复杂动态环境下的高精度自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有传感器融合SLAM方法虽然提升了鲁棒性，但在复杂环境下仍易出现定位漂移，主要源于融合策略不够合理，亟需更稳定有效的融合框架来提升足式机器人导航性能。

Method: 作者提出了一套融合式位姿估计系统，包括基于视觉-惯性-运动学(VIKO)的优化法和基于激光雷达-惯性-运动学(LIKO)的滤波法。VIKO采用足部预积分和基于超像素聚类的激光-视觉深度一致性，利用滑动窗口优化。LIKO结合足部运动学，在误差状态迭代卡尔曼滤波中加入点到平面的残差，具体方案根据观测数据适时选择两种模式。

Result: 所提系统在公开和长期数据集上与其他主流传感器融合SLAM方法进行对比，表现出更加鲁棒和精确的定位建图能力。

Conclusion: 通过合理集成多种传感器和创新的融合策略，该系统有效提升了足式机器人在复杂动态环境下自主导航的SLAM鲁棒性和实用性。

Abstract: Autonomous navigation for legged robots in complex and dynamic environments
relies on robust simultaneous localization and mapping (SLAM) systems to
accurately map surroundings and localize the robot, ensuring safe and efficient
operation. While prior sensor fusion-based SLAM approaches have integrated
various sensor modalities to improve their robustness, these algorithms are
still susceptible to estimation drift in challenging environments due to their
reliance on unsuitable fusion strategies. Therefore, we propose a robust
LiDAR-visual-inertial-kinematic odometry system that integrates information
from multiple sensors, such as a camera, LiDAR, inertial measurement unit
(IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our
system employs a fusion-based pose estimation approach that runs
optimization-based visual-inertial-kinematic odometry (VIKO) and filter-based
LiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In
VIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth
consistency using superpixel clusters in a sliding window optimization. In
LIKO, we incorporate foot kinematics and employ a point-toplane residual in an
error-state iterative Kalman filter (ESIKF). Compared with other sensor
fusion-based SLAM algorithms, our approach shows robust performance across
public and longterm datasets.

</details>


### [145] [PolyFly: Polytopic Optimal Planning for Collision-Free Cable-Suspended Aerial Payload Transportation](https://arxiv.org/abs/2510.15226)
*Mrunal Sarvaiya,Guanrui Li,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 本文提出了PolyFly——一种用于空中运输机器人的全局最优路径规划方法，显著提升了复杂环境下的飞行速度和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有空中运输机器人在救灾或救援场景下飞越复杂环境时，通常采用过于保守的几何模型，导致绕行多、时间长，缺乏效率。需要针对每个部件及障碍物更精确建模，减少保守性，提高通行效率。

Method: 作者将机器人及其环境中的每个物理部件（例如四旋翼、悬挂缆绳和负载）均独立建模为多面体（polytope），进一步结合这些部件的姿态信息，构建“姿态感知”多面体，提升模型精度。随后，借助对偶理论将多面体的约束转化为可微、光滑约束，高效求解全局最优控制问题。

Result: 在8个迷宫型环境下与当前先进方法对比，PolyFly方法的轨迹更快，实验验证也显示了实际四旋翼平台搭载负载时方案的准确性与可靠性。

Conclusion: PolyFly能够在保证安全的前提下，显著提升空中运输机器人在狭窄复杂环境中的飞行效率和实际适用性，优于现有方法。

Abstract: Aerial transportation robots using suspended cables have emerged as versatile
platforms for disaster response and rescue operations. To maximize the
capabilities of these systems, robots need to aggressively fly through tightly
constrained environments, such as dense forests and structurally unsafe
buildings, while minimizing flight time and avoiding obstacles. Existing
methods geometrically over-approximate the vehicle and obstacles, leading to
conservative maneuvers and increased flight times. We eliminate these
restrictions by proposing PolyFly, an optimal global planner which considers a
non-conservative representation for aerial transportation by modeling each
physical component of the environment, and the robot (quadrotor, cable and
payload), as independent polytopes. We further increase the model accuracy by
incorporating the attitude of the physical components by constructing
orientation-aware polytopes. The resulting optimal control problem is
efficiently solved by converting the polytope constraints into smooth
differentiable constraints via duality theory. We compare our method against
the existing state-of-the-art approach in eight maze-like environments and show
that PolyFly produces faster trajectories in each scenario. We also
experimentally validate our proposed approach on a real quadrotor with a
suspended payload, demonstrating the practical reliability and accuracy of our
method.

</details>


### [146] [A Generalized Sylvester-Fermat-Torricelli problem with application in disaster relief operations by UAVs](https://arxiv.org/abs/2510.15229)
*Sina Kazemdehbashi,Yanchao Liu,Boris S. Mordukhovich*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的数学框架，优化灾害响应中移动无人机站的位置分配，显著提升了无人机在极端天气下的数据收集和救援效率。


<details>
  <summary>Details</summary>
Motivation: 自然和人为灾害常导致通信基础设施损毁，严重妨碍救援队及时获取灾区信息，因此亟需提升灾后通信与数据收集的效率。

Method: 提出考虑风力影响和无人机异质性的新型数学问题Sylvester-Fermat-Torricelli (SFT)问题，统一建模风力、无人机性能差异和往返飞行等复杂实际因素，确定最优无人机移动站点布局。

Result: 实验表明，所提框架能使无人机救援任务减少高达84%的运作时间，从而大大提高救援响应的效率。

Conclusion: 该研究提高了基于无人机的灾后响应计划的实际可行性，有效应对了实际操作中的复杂因素，为未来灾害应急管理提供了更高效的方法基础。

Abstract: Natural and human-made disasters can cause severe devastation and claim
thousands of lives worldwide. Therefore, developing efficient methods for
disaster response and management is a critical task for relief teams. One of
the most essential components of effective response is the rapid collection of
information about affected areas, damages, and victims. More data translates
into better coordination, faster rescue operations, and ultimately, more lives
saved. However, in some disasters, such as earthquakes, the communication
infrastructure is often partially or completely destroyed, making it extremely
difficult for victims to send distress signals and for rescue teams to locate
and assist them in time. Unmanned Aerial Vehicles (UAVs) have emerged as
valuable tools in such scenarios. In particular, a fleet of UAVs can be
dispatched from a mobile station to the affected area to facilitate data
collection and establish temporary communication networks. Nevertheless,
real-world deployment of UAVs faces several challenges, with adverse weather
conditions--especially wind--being among the most significant. To address this,
we develop a novel mathematical framework to determine the optimal location of
a mobile UAV station while explicitly accounting for the heterogeneity of the
UAVs and the effect of wind. In particular, we generalize the Sylvester problem
to introduce the Sylvester-Fermat-Torricelli (SFT) problem, which captures
complex factors such as wind influence, UAV heterogeneity, and back-and-forth
motion within a unified framework. The proposed framework enhances the
practicality of UAV-based disaster response planning by accounting for
real-world factors such as wind and UAV heterogeneity. Experimental results
demonstrate that it can reduce wasted operational time by up to 84%, making
post-disaster missions significantly more efficient and effective.

</details>


### [147] [Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping](https://arxiv.org/abs/2510.15319)
*Jeewon Kim,Minho Oh,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种结合可行走性信息的房间分割方法，提高了3D场景图在机器人定位与建图中的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 在3D场景图应用于机器人地图构建时，准确分割房间具有挑战，传统方法在大房间常常过度分割，在复杂空间下又无法正确识别不可通行区域，影响定位与建图效果。

Method: 提出了一种可行走性感知的房间分割方法，将机器人与环境的交互、可通行性信息整合到分割及场景图约束中，实现更一致、合理的房间识别，并优化姿态图。

Result: 该方法在包含重复路径遍历的数据集上，提升了同一房间的多次识别频率（再检测能力），并降低了姿态图优化的计算时间。

Conclusion: 融合可行走性信息的房间分割能提升场景图在机器人定位与建图中的语义一致性和计算效率，适合实际复杂环境。

Abstract: Scene graphs enhance 3D mapping capabilities in robotics by understanding the
relationships between different spatial elements, such as rooms and objects.
Recent research extends scene graphs to hierarchical layers, adding and
leveraging constraints across these levels. This approach is tightly integrated
with pose-graph optimization, improving both localization and mapping accuracy
simultaneously. However, when segmenting spatial characteristics, consistently
recognizing rooms becomes challenging due to variations in viewpoints and
limited field of view (FOV) of sensors. For example, existing real-time
approaches often over-segment large rooms into smaller, non-functional spaces
that are not useful for localization and mapping due to the time-dependent
method. Conversely, their voxel-based room segmentation method often
under-segment in complex cases like not fully enclosed 3D space that are
non-traversable for ground robots or humans, leading to false constraints in
pose-graph optimization. We propose a traversability-aware room segmentation
method that considers the interaction between robots and surroundings, with
consistent feasibility of traversability information. This enhances both the
semantic coherence and computational efficiency of pose-graph optimization.
Improved performance is demonstrated through the re-detection frequency of the
same rooms in a dataset involving repeated traversals of the same space along
the same path, as well as the optimization time consumption.

</details>


### [148] [ASBI: Leveraging Informative Real-World Data for Active Black-Box Simulator Tuning](https://arxiv.org/abs/2510.15331)
*Gahee Kim,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了一种主动仿真推断（ASBI）框架，通过机器人主动收集现实世界数据，实现对黑盒仿真器参数的精确估计。该方法优化机器人动作以最大化信息增益，用神经网络学习后验分布，使其能在未知似然的情况下进行有效参数调整，验证了在仿真和真实机器人任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 黑盒仿真器在机器人领域应用广泛，但由于无法获得似然函数，参数优化极具挑战。传统的仿真推断方法在参数-观测值关系未知时难以选取有效观测。研究旨在实现无需可访问似然，能主动收集高价值观测的参数估计方法。

Method: 提出了一种主动仿真推断（ASBI）框架，机器人通过采集真实世界在线数据并选择最大化信息增益的动作来收集有效观测，信息增益通过后验和先验的熵差定义。信息增益无需似然函数，利用神经网络后验估计（NPE）技术，通过神经网络学习后验分布。

Result: 在三组仿真实验中，ASBI能准确估计参数，后验分布高度集中于真实参数。通过真实机器人以倾倒动作识别颗粒物物理参数，验证了实际应用潜力。

Conclusion: ASBI框架能有效应对黑盒仿真器参数估计的挑战，实现自动、高效的数据采集和精确参数调整，在仿真和实际机器人环境均取得优异表现。

Abstract: Black-box simulators are widely used in robotics, but optimizing their
parameters remains challenging due to inaccessible likelihoods.
Simulation-Based Inference (SBI) tackles this issue using simulation-driven
approaches, estimating the posterior from offline real observations and forward
simulations. However, in black-box scenarios, preparing observations that
contain sufficient information for parameter estimation is difficult due to the
unknown relationship between parameters and observations. In this work, we
present Active Simulation-Based Inference (ASBI), a parameter estimation
framework that uses robots to actively collect real-world online data to
achieve accurate black-box simulator tuning. Our framework optimizes robot
actions to collect informative observations by maximizing information gain,
which is defined as the expected reduction in Shannon entropy between the
posterior and the prior. While calculating information gain requires the
likelihood, which is inaccessible in black-box simulators, our method solves
this problem by leveraging Neural Posterior Estimation (NPE), which leverages a
neural network to learn the posterior estimator. Three simulation experiments
quantitatively verify that our method achieves accurate parameter estimation,
with posteriors sharply concentrated around the true parameters. Moreover, we
show a practical application using a real robot to estimate the simulation
parameters of cubic particles corresponding to two real objects, beads and
gravel, with a bucket pouring action.

</details>


### [149] [Adaptive Cost-Map-based Path Planning in Partially Unknown Environments with Movable Obstacles](https://arxiv.org/abs/2510.15336)
*Liviu-Mihai Stan,Ranulfo Bezerra,Shotaro Kojima,Tsige Tadesse Alemayoh,Satoshi Tadokoro,Masashi Konyo,Kazunori Ohno*

Main category: cs.RO

TL;DR: 本文提出了一种基于LiDAR和里程计的自适应路径规划框架，可帮助机器人在室内杂乱环境中识别并尝试推动可移动障碍物，以集成到ROS2 Nav2系统中。实验显示提升了到达目标的概率并减少了死锁情况。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在灾后救援等复杂室内环境中，仅能规避障碍物，难以辨识哪些障碍物可通过物理接触移开，限制了其通行能力。为提升机器人适应性和任务完成率，需要开发能够识别、尝试推动可移动障碍物的方法，并易于集成在现有系统里。

Method: 作者在ROS2 Nav2路径规划中，提出了一种Movable Obstacles Layer，将与静态地图不符的LiDAR点标记为“可移动”，赋予较低通行代价；结合Slow-Pose Progress Checker，若机器人速度下降或停滞，则自动提高障碍物通行代价，促使全局规划器重新规划路径。全流程仅需平面激光和CPU计算，可嵌入低算力平台。

Result: 在Gazebo平台和Scout Mini机器人上进行的实验表明：与未采用本层的基线系统相比，本文方法能显著提高机器人抵达目标的成功率，减少卡死事件，而总体通行时间相当。

Conclusion: 互动感知代价地图是一种轻量级、原生支持ROS2的增强模块，适用于资源受限、需应对可移动障碍的复杂环境机器人，提高了其导航的实际效能和灵活性，并易于集成到不同设备。

Abstract: Reliable navigation in disaster-response and other unstructured indoor
settings requires robots not only to avoid obstacles but also to recognise when
those obstacles can be pushed aside. We present an adaptive, LiDAR and
odometry-based path-planning framework that embeds this capability into the
ROS2 Nav2 stack. A new Movable Obstacles Layer labels all LiDAR returns missing
from a prior static map as tentatively movable and assigns a reduced traversal
cost. A companion Slow-Pose Progress Checker monitors the ratio of commanded to
actual velocity; when the robot slows appreciably, the local cost is raised
from light to heavy, and on a stall to lethal, prompting the global planner to
back out and re-route. Gazebo evaluations on a Scout Mini, spanning isolated
objects and cluttered corridors, show higher goal-reach rates and fewer
deadlocks than a no-layer baseline, with traversal times broadly comparable.
Because the method relies only on planar scans and CPU-level computation, it
suits resource-constrained search and rescue robots and integrates into
heterogeneous platforms with minimal engineering. Overall, the results indicate
that interaction-aware cost maps are a lightweight, ROS2-native extension for
navigating among potentially movable obstacles in unstructured settings. The
full implementation will be released as open source
athttps://costmap-namo.github.io.

</details>


### [150] [Nauplius Optimisation for Autonomous Hydrodynamics](https://arxiv.org/abs/2510.15350)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的自然启发式水下集群优化算法NOAH，专为强水流、有限声带宽和持续监测需求下的自主水下载具设计，提升了其任务成功率和能效。


<details>
  <summary>Details</summary>
Motivation: 传统的集群优化方法在水下强流、有限带宽和长时间感知场景下表现不佳，因此亟需新方法提升自主水下载具在复杂水动力环境中的表现。

Method: NOAH算法模拟藤壶幼体的行为，集成了水流感知（drift）、不可逆锚定（irreversible settlement）、基于群体的通讯（colony-based communication）等机制，统一考虑了水动力约束和节点永久安置需求。

Result: 实验结果表明，在永久锚定的基本任务场景下，NOAH达到了86%的成功率，证实了其在复杂水流条件下的适用性和优势。

Conclusion: NOAH为可扩展、节能的水下群体机器人系统提供了坚实基础，有力解决了水下任务的水动力感知、连续安放与通讯等关键挑战。

Abstract: Autonomous Underwater vehicles must operate in strong currents, limited
acoustic bandwidth, and persistent sensing requirements where conventional
swarm optimisation methods are unreliable. This paper presents NOAH, a novel
nature-inspired swarm optimisation algorithm that combines current-aware drift,
irreversible settlement in persistent sensing nodes, and colony-based
communication. Drawing inspiration from the behaviour of barnacle nauplii, NOAH
addresses the critical limitations of existing swarm algorithms by providing
hydrodynamic awareness, irreversible anchoring mechanisms, and colony-based
communication capabilities essential for underwater exploration missions. The
algorithm establishes a comprehensive foundation for scalable and
energy-efficient underwater swarm robotics with validated performance analysis.
Validation studies demonstrate an 86% success rate for permanent anchoring
scenarios, providing a unified formulation for hydrodynamic constraints and
irreversible settlement behaviours with an empirical study under flow.

</details>


### [151] [GaussGym: An open-source real-to-sim framework for learning locomotion from pixels](https://arxiv.org/abs/2510.15352)
*Alejandro Escontrela,Justin Kerr,Arthur Allshire,Jonas Frey,Rocky Duan,Carmelo Sferrazza,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D高斯Splatting渲染与矢量化物理仿真器的新颖机器人仿真方法，实现了每秒超10万步的仿真速度与高视觉保真度，支持大规模环境快速构造，并推动机器人领域的可扩展与通用学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器人仿真通常在速度与视觉真实感之间存在权衡，限制了大规模、真实感强的仿真环境下机器人学习的发展。研究者期望突破仿真规模和感知质量的瓶颈，促进sim-to-real的应用。

Method: 该方法将3D高斯Splatting用作矢量化物理仿真器（如IsaacGym）的实时渲染器，实现高速、高清晰度的仿真渲染。结合来自iPhone扫描与大型场景数据集的环境，无缝支持多样化、高复杂度的训练任务场景，同时可拓展用于生成式视频模型输出的仿真世界。

Result: 实现了在消费级GPU上每秒超过10万步的仿真速度，并在多项任务中展示了高视觉保真度和语义丰富的感知能力。丰富的视觉内容有助于机器人在导航与决策中避开不良区域，提升sim-to-real任务表现。

Conclusion: 该工作实现了高吞吐量仿真与高保真感知的有机结合，极大促进了机器人学习的可扩展性与通用性。所有代码与数据将开源，有助于社区持续构建与创新。

Abstract: We present a novel approach for photorealistic robot simulation that
integrates 3D Gaussian Splatting as a drop-in renderer within vectorized
physics simulators such as IsaacGym. This enables unprecedented speed --
exceeding 100,000 steps per second on consumer GPUs -- while maintaining high
visual fidelity, which we showcase across diverse tasks. We additionally
demonstrate its applicability in a sim-to-real robotics setting. Beyond
depth-based sensing, our results highlight how rich visual semantics improve
navigation and decision-making, such as avoiding undesirable regions. We
further showcase the ease of incorporating thousands of environments from
iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs
from generative video models like Veo, enabling rapid creation of realistic
training worlds. This work bridges high-throughput simulation and high-fidelity
perception, advancing scalable and generalizable robot learning. All code and
data will be open-sourced for the community to build upon. Videos, code, and
data available at https://escontrela.me/gauss_gym/.

</details>


### [152] [Towards Automated Chicken Deboning via Learning-based Dynamically-Adaptive 6-DoF Multi-Material Cutting](https://arxiv.org/abs/2510.15376)
*Zhaodong Yang,Ai-Ping Hu,Harish Ravichandar*

Main category: cs.RO

TL;DR: 本文提出了一种用于鸡肩关节自动去骨的6自由度切割系统，通过力反馈实现精确控制，有效避免了切割过程中与骨骼的危险接触。作者开发了开源多材料切割模拟器和物理实验平台，基于强化学习训练鲁棒的力反馈策略，并在实际鸡肩上实现了首次去骨任务，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统鸡肩关节去骨自动化难度高，主要由于关节部分遮挡、材料变形和多材料特性，且与骨骼接触存在安全和卫生风险。因此，需要高精度、能适应复杂解剖结构的自动切割方法。

Method: 1）构建开源多材料切割模拟器，支持耦合、断裂、切割等物理特性，推动强化学习训练。2）设计可复用物理测试平台，模拟真实鸡肩多材料结构，便于验证算法。3）基于残差强化学习，结合离散化力观测和域随机化，训练鲁棒的力反馈切割策略，实现仿真到现实的零样本迁移。

Result: 1）所提模型能在模拟、物理平台及真实鸡肩实验中成功穿越关节并大幅减少骨/软骨接触。2）与传统开环切割基线相比，切割成功率和骨骼避免率提升最高达4倍。3）实验同时证明力反馈对安全有效切割至关重要。

Conclusion: 本文提出了一套用于多材料关节解剖的可靠自动切割系统，首次在真实鸡肩关节上实现了自动去骨，并显著提升安全性与效率。提出的模拟器、测试平台和算法有助于推进自动化食品加工领域的发展。

Abstract: Automating chicken shoulder deboning requires precise 6-DoF cutting through a
partially occluded, deformable, multi-material joint, since contact with the
bones presents serious health and safety risks. Our work makes both
systems-level and algorithmic contributions to train and deploy a reactive
force-feedback cutting policy that dynamically adapts a nominal trajectory and
enables full 6-DoF knife control to traverse the narrow joint gap while
avoiding contact with the bones. First, we introduce an open-source
custom-built simulator for multi-material cutting that models coupling,
fracture, and cutting forces, and supports reinforcement learning, enabling
efficient training and rapid prototyping. Second, we design a reusable physical
testbed to emulate the chicken shoulder: two rigid "bone" spheres with
controllable pose embedded in a softer block, enabling rigorous and repeatable
evaluation while preserving essential multi-material characteristics of the
target problem. Third, we train and deploy a residual RL policy, with
discretized force observations and domain randomization, enabling robust
zero-shot sim-to-real transfer and the first demonstration of a learned policy
that debones a real chicken shoulder. Our experiments in our simulator, on our
physical testbed, and on real chicken shoulders show that our learned policy
reliably navigates the joint gap and reduces undesired bone/cartilage contact,
resulting in up to a 4x improvement over existing open-loop cutting baselines
in terms of success rate and bone avoidance. Our results also illustrate the
necessity of force feedback for safe and effective multi-material cutting. The
project website is at https://sites.google.com/view/chickendeboning-2026.

</details>


### [153] [VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving](https://arxiv.org/abs/2510.15446)
*Ziang Guo,Zufeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为VDRive的端到端自动驾驶方法，通过引入视觉-语言-行为模型（VLA）与生成扩散策略，实现了对复杂驾驶环境下的鲁棒、可解释决策，在多个公开基准上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的复杂动态环境和极端场景（角落案例）使得车辆状态理解和决策变得困难，现有方法在模型解释性和鲁棒性上存在不足，需要新的方案提升自动驾驶系统的实际可用性和安全性。

Method: 该方法创新性地结合了视觉-语言-行为模型（VLA）与条件向量量化变分自编码器（CVQ-VAE），采用生成扩散策略进行动作预测。具体而言，VLA通过离散编码对观测进行预测（上下文预测），并经RL微调以基于当前状态预测未来轨迹和决策（几何预测）。使用actor-critic框架，策略头根据状态生成分层动作与轨迹，学习批评器提供梯度优化反馈，形成强化学习闭环优化。

Result: VDRive方法在Bench2Drive闭环基准和nuScenes开放式规划测试中均获得了最新最优性能，证明了其在复杂多变环境中优秀的状态理解和决策能力。

Conclusion: VDRive通过融合视觉-语言-行为建模与生成扩散策略，显著提升了自动驾驶系统的鲁棒性和决策解释力，具备在现实高动态环境下广泛应用的前景。

Abstract: In autonomous driving, dynamic environment and corner cases pose significant
challenges to the robustness of ego vehicle's state understanding and decision
making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving
that explicitly models state-action mapping to address these challenges,
enabling interpretable and robust decision making. By leveraging the
advancement of the state understanding of the Vision Language Action Model
(VLA) with generative diffusion policy-based action head, our VDRive guides the
driving contextually and geometrically. Contextually, VLA predicts future
observations through token generation pre-training, where the observations are
represented as discrete codes by a Conditional Vector Quantized Variational
Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning
fine-tuning of the VLA to predict future trajectories and actions based on
current driving conditions. VLA supplies the current state tokens and predicted
state tokens for the action policy head to generate hierarchical actions and
trajectories. During policy training, a learned critic evaluates the actions
generated by the policy and provides gradient-based feedback, forming an
actor-critic framework that enables a reinforcement-based policy learning
pipeline. Experiments show that our VDRive achieves state-of-the-art
performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop
planning.

</details>


### [154] [Perfect Prediction or Plenty of Proposals? What Matters Most in Planning for Autonomous Driving](https://arxiv.org/abs/2510.15505)
*Aron Distelzweig,Faris Janjoš,Oliver Scheel,Sirish Reddy Varra,Raghu Rajan,Joschka Boedecker*

Main category: cs.RO

TL;DR: 本文重新审视了自动驾驶中集成预测与规划（IPP）方法的有效性，发现当前IPP方法很难真正利用预测信息提升规划效果。论文提出了一种以高质量轨迹生成为核心的新方法，超越了现有方法，在复杂道路场景下表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 近年来，自动驾驶领域趋势从“预测”和“规划”两模块独立转向紧密集成的IPP方法，期望借此做出更聪明、灵活的决策。但实际尚不清楚这种集成在实际驾驶任务中能否带来本质提升，因此有必要系统评估此类方案的有效性。

Method: 作者基于主流Val14基准（常见场景）与interPlan基准（高互动、异常场景），分析了即使获得“完美预测”信息下，现有IPP方法对规划性能的提升有限。相较于端到端模仿学习方法，该工作转而强调高质量、多样化且真实的轨迹提案生成，将预测仅用于碰撞检测。同时，提升了基线PDM（简单跟车）方案的提案生成能力。

Result: 实验显示，许多模仿学习规划器在生成高质量、真实轨迹提案方面存在明显不足，效果甚至不如简单的PDM方法。新提出的以提案生成为中心的方法，尤其是在异常和高互动环境下，取得了最新最好的规划性能。

Conclusion: 论文证实，在现有IPP框架中高质量的提案生成比复杂预测更关键，新方法在复杂道路场景下优于主流方法。高效的轨迹生成机制是推动自动驾驶规划性能提升的重点。

Abstract: Traditionally, prediction and planning in autonomous driving (AD) have been
treated as separate, sequential modules. Recently, there has been a growing
shift towards tighter integration of these components, known as Integrated
Prediction and Planning (IPP), with the aim of enabling more informed and
adaptive decision-making. However, it remains unclear to what extent this
integration actually improves planning performance. In this work, we
investigate the role of prediction in IPP approaches, drawing on the widely
adopted Val14 benchmark, which encompasses more common driving scenarios with
relatively low interaction complexity, and the interPlan benchmark, which
includes highly interactive and out-of-distribution driving situations. Our
analysis reveals that even access to perfect future predictions does not lead
to better planning outcomes, indicating that current IPP methods often fail to
fully exploit future behavior information. Instead, we focus on high-quality
proposal generation, while using predictions primarily for collision checks. We
find that many imitation learning-based planners struggle to generate realistic
and plausible proposals, performing worse than PDM - a simple lane-following
approach. Motivated by this observation, we build on PDM with an enhanced
proposal generation method, shifting the emphasis towards producing diverse but
realistic and high-quality proposals. This proposal-centric approach
significantly outperforms existing methods, especially in out-of-distribution
and highly interactive settings, where it sets new state-of-the-art results.

</details>


### [155] [VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation](https://arxiv.org/abs/2510.15530)
*Zehao Ni,Yonghao He,Lingfeng Qian,Jilei Mao,Fa Fu,Wei Sui,Hu Su,Junran Peng,Zhipeng Wang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种仅基于视觉、单视角的扩散策略学习方法（VO-DP），通过融合视觉大模型的语义和几何特征，实现了对传统点云策略的性能超越，系统在实际和仿真任务中表现出极高的成功率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然当前模仿学习中的机器人操作多采用点云输入实现高精度控制，但关于视觉单一输入（vision-only）策略的潜力尚未被充分挖掘。该方向的探索有望简化系统、拓展适用场景，因此本研究提出对此进行深入研究。

Method: 方法名为VO-DP，通过利用预训练的视觉基础模型（如VGGT和DINOv2），分别提取中间层的语义和几何特征，引入交叉注意力和CNN聚合进行特征融合与空间压缩，最后输入策略头进行决策输出，并搭建了符合相关策略的高效训练库。

Result: VO-DP方法在仿真任务中平均成功率为64.6%，与点云方法DP3（64.0%）持平，明显高于视觉基线DP（34.8%）；在真实世界任务中，VO-DP以87.9%的成功率显著优于DP3（67.5%）和DP（11.2%）。在不同颜色、尺寸、背景及光照条件下展现出很强的稳定性和鲁棒性。

Conclusion: 该工作证明了视觉单一输入在机器人操作中的巨大潜力，不仅实现了与点云策略相当甚至更优的性能，还提升了系统的简洁性和适用性，对未来模仿学习方案的设计具有重要参考价值。

Abstract: In the context of imitation learning, visuomotor-based diffusion policy
learning is one of the main directions in robotic manipulation. Most of these
approaches rely on point clouds as observation inputs and construct scene
representations through point clouds feature learning, which enables them to
achieve remarkable accuracy. However, the existing literature lacks an in-depth
exploration of vision-only solutions that have significant potential. In this
paper, we propose a Vision-Only and single-view Diffusion Policy learning
method (VO-DP) that leverages pretrained visual foundation models to achieve
effective fusion of semantic and geometric features. We utilize intermediate
features from VGGT incorporating semantic features from DINOv2 and geometric
features from Alternating Attention blocks. Features are fused via
cross-attention and spatially compressed with a CNN to form the input to the
policy head. Extensive experiments demonstrate that VO-DP not only outperforms
the vision-only baseline DP significantly but also exhibits distinct
performance trends against the point cloud-based method DP3: in simulation
tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%
and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,
outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further
robustness evaluations confirm that VO-DP remains highly stable under varying
conditions including color, size, background, and lighting. Lastly, we
open-source a training library for robotic manipulation. Built on Accelerate,
this library supports multi-machine and multi-GPU parallel training, as well as
mixed precision training. It is compatible with visuomotor policies such as DP,
DP3 and VO-DP, and also supports the RoboTwin simulator.

</details>


### [156] [Improved Extended Kalman Filter-Based Disturbance Observers for Exoskeletons](https://arxiv.org/abs/2510.15533)
*Shilei Li,Dawei Shi,Makoto Iwasaki,Yan Ning,Hongpeng Zhou,Ling Shi*

Main category: cs.RO

TL;DR: 本文针对机械系统中因未知扰动导致性能下降的问题，提出两种改进的基于扩展卡尔曼滤波器(EKF)的扰动观测方法，并在外骨骼实验中显著提升了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 机械系统的理想性能常常受到未知扰动的影响，而完全抑制未知动力学的扰动在实际中难以实现。因此，需要更有效的扰动观测和补偿方法，提升系统的跟踪精度。

Method: 分析了扰动估计中跟踪速度与不确定性之间的权衡，提出了两种新型扰动观测器：一种基于交互多模型扩展卡尔曼滤波(IMM-EKF)，另一种基于多核相关熵扩展卡尔曼滤波(MK-correntropy EKF)。

Result: 在外骨骼系统上进行实验，两种方法在时变交互力场景下，分别在髋关节与膝关节误差上，相较传统EKF扰动观测器，分别提升了36.3%、16.2%与46.3%、24.4%的跟踪精度。

Conclusion: 提出的两种方法能够有效改善实际系统中的扰动估计性能，显著提高跟踪精度，优于现有的扩展卡尔曼滤波扰动观测方案。

Abstract: The nominal performance of mechanical systems is often degraded by unknown
disturbances. A two-degree-of-freedom control structure can decouple nominal
performance from disturbance rejection. However, perfect disturbance rejection
is unattainable when the disturbance dynamic is unknown. In this work, we
reveal an inherent trade-off in disturbance estimation subject to tracking
speed and tracking uncertainty. Then, we propose two novel methods to enhance
disturbance estimation: an interacting multiple model extended Kalman
filter-based disturbance observer and a multi-kernel correntropy extended
Kalman filter-based disturbance observer. Experiments on an exoskeleton verify
that the proposed two methods improve the tracking accuracy $36.3\%$ and
$16.2\%$ in hip joint error, and $46.3\%$ and $24.4\%$ in knee joint error,
respectively, compared to the extended Kalman filter-based disturbance
observer, in a time-varying interaction force scenario, demonstrating the
superiority of the proposed method.

</details>


### [157] [Adaptive Legged Locomotion via Online Learning for Model Predictive Control](https://arxiv.org/abs/2510.15626)
*Hongyu Zhou,Xiaoyu Zhang,Vasileios Tzoumas*

Main category: cs.RO

TL;DR: 本文提出了一种结合在线学习和模型预测控制（MPC）的自适应四足机器人步态算法，通过实时学习和补偿动力学残差，提高四足机器人在复杂和不确定环境中的自主运动能力。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人在现实复杂环境（如未知载重、不平地形等）自主完成任务时，面临动力学建模误差和外部干扰，传统预先建模方法难以适应这些不确定性。因此，亟需一种能够动态学习和补偿动力学残差的方法，以提升机器人在未知环境中的鲁棒性和适应性。

Method: 算法包含两个模块：1）针对动力学残差，基于随机傅里叶特征（RF）在再生核希尔伯特空间（RKHS）中进行函数逼近，实现残差建模；2）结合当前学到的残差模型，进行模型预测控制（MPC）。残差模型通过在线自监督方式、利用实时采集的数据，用最小二乘法不断更新。算法目标是实现子线性动态遗憾（dynamic regret），即与理想最优控制器的性能差距随时间增长而放缓。

Result: 在仿真环境Gazebo和MuJoCo进行了验证。Gazebo中，四足机器人需跟踪参考轨迹，应对高达12g的未知外力、不同地形（包括20°坡度和0.25m起伏的崎岖地形）；MuJoCo中，机器人需应对变化的外部扰动（负载高达8kg）和变动的地面摩擦系数。在这些极端不确定性和复杂场景下，算法表现出良好的轨迹跟踪和适应能力。

Conclusion: 结合在线学习与MPC的方法可有效提升四足机器人在未知动态扰动和复杂地形下的适应性和鲁棒性，为未来四足机器人在现实环境下的自主运动与任务执行奠定了基础。

Abstract: We provide an algorithm for adaptive legged locomotion via online learning
and model predictive control. The algorithm is composed of two interacting
modules: model predictive control (MPC) and online learning of residual
dynamics. The residual dynamics can represent modeling errors and external
disturbances. We are motivated by the future of autonomy where quadrupeds will
autonomously perform complex tasks despite real-world unknown uncertainty, such
as unknown payload and uneven terrains. The algorithm uses random Fourier
features to approximate the residual dynamics in reproducing kernel Hilbert
spaces. Then, it employs MPC based on the current learned model of the residual
dynamics. The model is updated online in a self-supervised manner using least
squares based on the data collected while controlling the quadruped. The
algorithm enjoys sublinear \textit{dynamic regret}, defined as the
suboptimality against an optimal clairvoyant controller that knows how the
residual dynamics. We validate our algorithm in Gazebo and MuJoCo simulations,
where the quadruped aims to track reference trajectories. The Gazebo
simulations include constant unknown external forces up to $12\boldsymbol{g}$,
where $\boldsymbol{g}$ is the gravity vector, in flat terrain, slope terrain
with $20\degree$ inclination, and rough terrain with $0.25m$ height variation.
The MuJoCo simulations include time-varying unknown disturbances with payload
up to $8~kg$ and time-varying ground friction coefficients in flat terrain.

</details>


### [158] [Educational SoftHand-A: Building an Anthropomorphic Hand with Soft Synergies using LEGO MINDSTORMS](https://arxiv.org/abs/2510.15638)
*Jared K. Lepora,Haoran Li,Efi Psomopoulou,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本论文介绍了一款完全由乐高MINDSTORMS搭建的人形机器人手，兼具教育意义与先进设计，实现了自适应抓取与简易控制。


<details>
  <summary>Details</summary>
Motivation: 推动机器人前沿技术在教育领域的应用，用乐高搭建具有高度逼真度和功能性的机器人手，让学生在家也能了解和实践现代机器人手的原理与结构。

Method: 采用乐高标准零件，构建一款模仿Pisa/IIT SoftHand的仿生机器人手。每根手指采用双电机驱动对向肌腱对，实现反应灵敏的精细控制，并通过离合齿轮差速机构实现软协同机制来同步手指运动。

Result: 所设计的机器人手可以用简单的驱动与控制机制，适应性抓取各种形状的物体，已通过家庭常用设备测试验证其功能。

Conclusion: 该机器人手不但可完全由乐高组件简单组装，而且融入了先进的机器人手设计理念，非常适合教育用途，有潜力激发儿童学习现代机器人技术兴趣。

Abstract: This paper introduces an anthropomorphic robot hand built entirely using LEGO
MINDSTORMS: the Educational SoftHand-A, a tendon-driven, highly-underactuated
robot hand based on the Pisa/IIT SoftHand and related hands. To be suitable for
an educational context, the design is constrained to use only standard LEGO
pieces with tests using common equipment available at home. The hand features
dual motors driving an agonist/antagonist opposing pair of tendons on each
finger, which are shown to result in reactive fine control. The finger motions
are synchonized through soft synergies, implemented with a differential
mechanism using clutch gears. Altogether, this design results in an
anthropomorphic hand that can adaptively grasp a broad range of objects using a
simple actuation and control mechanism. Since the hand can be constructed from
LEGO pieces and uses state-of-the-art design concepts for robotic hands, it has
the potential to educate and inspire children to learn about the frontiers of
modern robotics.

</details>


### [159] [Integration of a Variable Stiffness Link for Long-Reach Aerial Manipulation](https://arxiv.org/abs/2510.15639)
*Manuel J. Fernandez,Alejandro Suarez,Anibal Ollero,Matteo Fumagalli*

Main category: cs.RO

TL;DR: 本论文提出了一种可变刚度连杆（VSL）用于长距离空中操作，使多旋翼无人机和双臂机械臂之间的机械耦合具有可调性，从而在精度和抗扰性之间取得平衡。通过实验验证，VSL在应对外部扰动和运输任务时，能够通过切换刚柔模式提升系统的多功能性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的长距离空中操作系统普遍依赖刚性或缆绳连接，但刚性连接易将扰动传递给飞行器，降低稳定性，而柔性连接则牺牲了操作精度。因此，亟需一种在任务需求下可调节机械性质的连接方式，以同时兼顾抗扰性和精度。

Method: 提出一种可变刚度连杆(VSL)并集成到安装有双臂机械臂的四旋翼平台上，使连接可在任务中实现刚性和柔性切换。通过遥操作实验，在包含外部扰动及搬运任务中测试了VSL的性能，并比较了不同刚度设置下的系统行为。

Result: 实验结果表明，通过调整连杆刚度，可以显著影响无人机与载荷间的动态互动。柔性配置能够减缓外界冲击与空气扰动，刚性配置则提升搬运阶段的定位精度。系统整体表现出更高的多功能性和控制性。

Conclusion: 可变刚度连杆提升了空中操作系统的灵活性与安全性，为柔顺性与精度之间提供了可控权衡。未来工作将聚焦于自主刚度调节、多绳结构、合作式空中操作及用户研究以进一步评估其在遥操作与半自主任务中的价值。

Abstract: This paper presents the integration of a Variable Stiffness Link (VSL) for
long-reach aerial manipulation, enabling adaptable mechanical coupling between
an aerial multirotor platform and a dual-arm manipulator. Conventional
long-reach manipulation systems rely on rigid or cable connections, which limit
precision or transmit disturbances to the aerial vehicle. The proposed VSL
introduces an adjustable stiffness mechanism that allows the link to behave
either as a flexible rope or as a rigid rod, depending on task requirements.
  The system is mounted on a quadrotor equipped with the LiCAS dual-arm
manipulator and evaluated through teleoperated experiments, involving external
disturbances and parcel transportation tasks. Results demonstrate that varying
the link stiffness significantly modifies the dynamic interaction between the
UAV and the payload. The flexible configuration attenuates external impacts and
aerodynamic perturbations, while the rigid configuration improves positional
accuracy during manipulation phases.
  These results confirm that VSL enhances versatility and safety, providing a
controllable trade-off between compliance and precision. Future work will focus
on autonomous stiffness regulation, multi-rope configurations, cooperative
aerial manipulation and user studies to further assess its impact on
teleoperated and semi-autonomous aerial tasks.

</details>


### [160] [Freehand 3D Ultrasound Imaging: Sim-in-the-Loop Probe Pose Optimization via Visual Servoing](https://arxiv.org/abs/2510.15668)
*Yameng Zhang,Dianye Huang,Max Q. -H. Meng,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种经济、灵活的三维超声成像方法，结合轻量级摄像头和视觉伺服，提升自由手操作中的三维重建精度，实验表明其重建效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有自由手三维超声成像需昂贵的跟踪系统，或依赖神经网络的方法易受噪音和误差积累影响，难以获得高精度重建。作者希望开发一种低成本且鲁棒性强的三维超声成像方法。

Method: 该方法利用装配于二维超声探头的轻量级摄像头，在带纹理平面的工作区捕捉视觉反馈。为解决遮挡和光照问题，提出基于纹理的图像恢复方法补全被遮挡区域。位姿估计上，采用虚实结合的仿真-实物一体化方法，迭代优化仿真与现实观测间的位姿误差；并通过视觉伺服控制调整摄像头视角，实现更精确的平移估计。

Result: 在软血管模型、3D打印圆锥模型和真人手臂上的实验表明，采用该方法后，重建结果与基准模型的Hausdorff距离分别为0.359 mm、1.171 mm和0.858 mm，验证了其较高的重建精度和鲁棒性。

Conclusion: 该方法能够经济高效地实现高精度的自由手三维超声成像，不依赖昂贵设备，具有很高的鲁棒性和应用潜力。

Abstract: Freehand 3D ultrasound (US) imaging using conventional 2D probes offers
flexibility and accessibility for diverse clinical applications but faces
challenges in accurate probe pose estimation. Traditional methods depend on
costly tracking systems, while neural network-based methods struggle with image
noise and error accumulation, compromising reconstruction precision. We propose
a cost-effective and versatile solution that leverages lightweight cameras and
visual servoing in simulated environments for precise 3D US imaging. These
cameras capture visual feedback from a textured planar workspace. To counter
occlusions and lighting issues, we introduce an image restoration method that
reconstructs occluded regions by matching surrounding texture patterns. For
pose estimation, we develop a simulation-in-the-loop approach, which replicates
the system setup in simulation and iteratively minimizes pose errors between
simulated and real-world observations. A visual servoing controller refines the
alignment of camera views, improving translational estimation by optimizing
image alignment. Validations on a soft vascular phantom, a 3D-printed conical
model, and a human arm demonstrate the robustness and accuracy of our approach,
with Hausdorff distances to the reference reconstructions of 0.359 mm, 1.171
mm, and 0.858 mm, respectively. These results confirm the method's potential
for reliable freehand 3D US reconstruction.

</details>


### [161] [HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward](https://arxiv.org/abs/2510.15679)
*Yuhong Cao,Yizhuo Wang,Jingsong Liang,Shuhao Liao,Yifeng Zhang,Peizhuo Li,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本论文提出了一种名为HEADER的基于注意力机制的强化学习方法，实现了在大规模环境中高效的自主机器人探索，方法在仿真和真实环境中均表现出优异的扩展性和探索效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法在机器人大规模环境探索中受限于环境规模和效率，急需高效且可扩展的新探索策略。

Method: 论文提出HEADER方法，结合分层图结构与注意力机制，社区划分算法全增量地按需构建全局图，局部精细推理与全局粗略推理相结合，并提出参数无关的奖励函数，避免奖励设计偏差。

Result: HEADER在仿真和真实（如300m*230m园区）复杂大规模环境下，相较主流学习和非学习方法拓展性更强，探索效率最多提高20%。

Conclusion: HEADER创新性地实现了可扩展、效率高的探索策略，推动了大规模机器人自主探索的学习方法边界，在实际和仿真中均取得了优异表现。

Abstract: This work pushes the boundaries of learning-based methods in autonomous robot
exploration in terms of environmental scale and exploration efficiency. We
present HEADER, an attention-based reinforcement learning approach with
hierarchical graphs for efficient exploration in large-scale environments.
HEADER follows existing conventional methods to construct hierarchical
representations for the robot belief/map, but further designs a novel
community-based algorithm to construct and update a global graph, which remains
fully incremental, shape-adaptive, and operates with linear complexity.
Building upon attention-based networks, our planner finely reasons about the
nearby belief within the local range while coarsely leveraging distant
information at the global scale, enabling next-best-viewpoint decisions that
consider multi-scale spatial dependencies. Beyond novel map representation, we
introduce a parameter-free privileged reward that significantly improves model
performance and produces near-optimal exploration behaviors, by avoiding
training objective bias caused by handcrafted reward shaping. In simulated
challenging, large-scale exploration scenarios, HEADER demonstrates better
scalability than most existing learning and non-learning methods, while
achieving a significant improvement in exploration efficiency (up to 20%) over
state-of-the-art baselines. We also deploy HEADER on hardware and validate it
in complex, large-scale real-life scenarios, including a 300m*230m campus
environment.

</details>


### [162] [Few-Shot Demonstration-Driven Task Coordination and Trajectory Execution for Multi-Robot Systems](https://arxiv.org/abs/2510.15686)
*Taehyeon Kim,Vishnunandan L. N. Venkatesh,Byung-Cheol Min*

Main category: cs.RO

TL;DR: 该论文提出了一种用于多机器人系统的小样本学习框架DDACE，结合时序图网络与高斯过程实现任务协调和轨迹执行，在多种任务和环境下表现出良好的泛化能力和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人系统在学习复杂任务时，对大量演示数据依赖较强，影响实用性和扩展性。

Method: 方法上，作者提出DDACE框架，将任务的时序和空间部分解耦，分别采用时序图网络（学习与任务无关的时序）与高斯过程（建模空间轨迹）的方式进行学习。整个系统仅需少量演示数据即可进行训练和泛化。

Result: 通过在多种多机器人协调任务环境（如多序列执行、多动作动态、复杂轨迹等）中的实验验证，结果显示该方法能够在小样本情况下有效完成任务，并能适应多变环境。

Conclusion: 该工作表明模块化架构能够有效提升多机器人系统在实际应用中的实用性和可扩展性，为多机器人小样本学习提供了新的思路。

Abstract: In this paper, we propose a novel few-shot learning framework for multi-robot
systems that integrate both spatial and temporal elements: Few-Shot
Demonstration-Driven Task Coordination and Trajectory Execution (DDACE). Our
approach leverages temporal graph networks for learning task-agnostic temporal
sequencing and Gaussian Processes for spatial trajectory modeling, ensuring
modularity and generalization across various tasks. By decoupling temporal and
spatial aspects, DDACE requires only a small number of demonstrations,
significantly reducing data requirements compared to traditional learning from
demonstration approaches. To validate our proposed framework, we conducted
extensive experiments in task environments designed to assess various aspects
of multi-robot coordination-such as multi-sequence execution, multi-action
dynamics, complex trajectory generation, and heterogeneous configurations. The
experimental results demonstrate that our approach successfully achieves task
execution under few-shot learning conditions and generalizes effectively across
dynamic and diverse settings. This work underscores the potential of modular
architectures in enhancing the practicality and scalability of multi-robot
systems in real-world applications. Additional materials are available at
https://sites.google.com/view/ddace.

</details>


### [163] [DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation](https://arxiv.org/abs/2510.15786)
*Xinyue Xu,Jieqiang Sun,Jing,Dai,Siyuan Chen,Lanjie Ma,Ke Sun,Bin Zhao,Jianbo Yuan,Yiwen Lu*

Main category: cs.RO

TL;DR: DexCanvas是一个结合真实和合成数据的大规模灵巧手-物体交互数据集，包括7000小时的数据，覆盖21种基本操作类型，辅助机器人操控和动作迁移等研究。


<details>
  <summary>Details</summary>
Motivation: 现有的手部操控数据集在规模、动作类型覆盖以及物理一致性的接触注释方面存在不足，限制了机器人操控学习和技能迁移等领域的发展。作者试图通过构建大规模、基于实际演示与物理仿真结合的数据集来弥补这些不足。

Method: 作者收集了70小时的真实人类灵巧手操作演示，通过多视角RGB-D、精确动作捕捉和MANO手部参数记录。随后，利用强化学习在物理仿真环境中复现这些演示，由MANO模型控制机器手，生成符合物理规律的接触点和力数据，将真实数据扩展为7000小时的混合数据集，并对操作类型使用Cutkosky经典分类体系进行整理。

Result: DexCanvas集成了大规模真实及合成演示，覆盖21类基础操作技能，配套物理一致的接触力注释，是第一个在规模、技能覆盖和物理一致性上同时具备优势的人手操控数据集。

Conclusion: DexCanvas为机器人操控学习、丰富接触力控制和跨手型技能迁移提供了有价值的数据基础，有望推动相关领域研究的发展。

Abstract: We present DexCanvas, a large-scale hybrid real-synthetic human manipulation
dataset containing 7,000 hours of dexterous hand-object interactions seeded
from 70 hours of real human demonstrations, organized across 21 fundamental
manipulation types based on the Cutkosky taxonomy. Each entry combines
synchronized multi-view RGB-D, high-precision mocap with MANO hand parameters,
and per-frame contact points with physically consistent force profiles. Our
real-to-sim pipeline uses reinforcement learning to train policies that control
an actuated MANO hand in physics simulation, reproducing human demonstrations
while discovering the underlying contact forces that generate the observed
object motion. DexCanvas is the first manipulation dataset to combine
large-scale real demonstrations, systematic skill coverage based on established
taxonomies, and physics-validated contact annotations. The dataset can
facilitate research in robotic manipulation learning, contact-rich control, and
skill transfer across different hand morphologies.

</details>


### [164] [Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion](https://arxiv.org/abs/2510.15803)
*Zahra Arjmandi,Gunho Sohn*

Main category: cs.RO

TL;DR: 提出了一种新的基于推断注意力融合（INAF）模块的激光雷达SLAM方法，能够提升定位和三维建图的精度。


<details>
  <summary>Details</summary>
Motivation: 激光雷达SLAM在无人驾驶等领域需求高，但面临环境复杂和测量精度不足的问题，亟需更智能、适应性更强的融合技术。

Method: 设计了INAF模块，将AI与几何里程计结合，并在KITTI数据集上通过动态调整注意力权重，实现对环境的自适应反馈和高精度数据融合。

Result: 实验表明，INAF方法能有效提升定位和三维建图精度，尤其在复杂环境下表现突出。

Conclusion: INAF融合技术提高了SLAM的鲁棒性和性能，对自动导航系统具有重要应用前景。

Abstract: This paper presents a novel fusion technique for LiDAR Simultaneous
Localization and Mapping (SLAM), aimed at improving localization and 3D mapping
using LiDAR sensor. Our approach centers on the Inferred Attention Fusion
(INAF) module, which integrates AI with geometric odometry. Utilizing the KITTI
dataset's LiDAR data, INAF dynamically adjusts attention weights based on
environmental feedback, enhancing the system's adaptability and measurement
accuracy. This method advances the precision of both localization and 3D
mapping, demonstrating the potential of our fusion technique to enhance
autonomous navigation systems in complex scenarios.

</details>
