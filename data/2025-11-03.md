<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 80]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench](https://arxiv.org/abs/2510.26865)
*Fenfen Lin,Yesheng Liu,Haiyu Xu,Chen Yue,Zheqi He,Mingxuan Zhao,Miguel Hu Chen,Jiakang Liu,JG Yao,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出了MeasureBench基准，用于评估视觉-语言模型（VLMs）在读取各类测量仪表盘上的表现。实验表明，现有VLMs在测量读数尤其是在指针定位方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管人类无需专业知识即可轻松读取测量仪表，但现有视觉-语言模型在该任务上表现远不理想。为推动模型在精细视觉感知与空间定位方面的进步，需要有系统的评测与挑战。

Method: 提出MeasureBench基准，包含真实和合成的各种测量仪表读数图像，并设计了可扩展的数据合成流程，可在仪表类型、指针、刻度、字体、光照和杂乱程度等方面实现高度可控的内容生成。同时，对主流VLMs模型进行测评，并探索基于合成数据的强化学习方案。

Result: 测试结果显示，现有主流VLMs在测量读数特别是在指针定位上表现糟糕，使得整体数字估算错误较大。强化学习在合成领域数据上有改善，但在真实世界图片上提升有限。

Conclusion: 当前VLMs在精细空间定位和视觉-数字测量任务上存在显著瓶颈。MeasureBench为精细空间感知和视觉读数模型的研究提供了新基准，旨在推动其在真实测量场景下的准确性。

Abstract: Reading measurement instruments is effortless for humans and requires
relatively little domain expertise, yet it remains surprisingly challenging for
current vision-language models (VLMs) as we find in preliminary evaluation. In
this work, we introduce MeasureBench, a benchmark on visual measurement reading
covering both real-world and synthesized images of various types of
measurements, along with an extensible pipeline for data synthesis. Our
pipeline procedurally generates a specified type of gauge with controllable
visual appearance, enabling scalable variation in key details such as pointers,
scales, fonts, lighting, and clutter. Evaluation on popular proprietary and
open-weight VLMs shows that even the strongest frontier VLMs struggle
measurement reading in general. A consistent failure mode is indicator
localization: models can read digits or labels but misidentify the key
positions of pointers or alignments, leading to big numeric errors despite
plausible textual reasoning. We have also conducted preliminary experiments
with reinforcement learning over synthetic data, and find encouraging results
on in-domain synthetic subset but less promising for real-world images. Our
analysis highlights a fundamental limitation of current VLMs in fine-grained
spatial grounding. We hope this resource can help future advances on visually
grounded numeracy and precise spatial perception of VLMs, bridging the gap
between recognizing numbers and measuring the world.

</details>


### [2] [PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT](https://arxiv.org/abs/2510.26903)
*Rochak Dhakal,Chen Zhao,Zixin Shi,Joyce H. Keyak,Tadashi S. Kaneko,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种专为多机构QCT数据开发的域自适应Transformer分割框架，以提升股骨近端定量分析的准确性和泛化能力。模型集成了对抗性和统计性对齐机制，有效缓解了因不同机构设备与人群差异导致的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: QCT能定量分析骨密度、预测骨折风险，但因影像仪器、重建参数和患者构成等差异，深度分割模型难以泛化至多机构数据，导致结果不稳定、可重复性差。不解决该问题，将阻碍骨质疏松多中心研究和相关定量分析的应用。

Method: 作者基于3D TransUNet主干网络，设计了域自适应分割框架。其创新之处在于：一方面通过梯度反转层（GRL）实现对抗性特征对齐，抑制模型学习与机构相关的特征；另一方面利用最大均值差异（MMD）进行统计对齐，进一步缩小不同中心数据分布的差距。模型在1024例Tulane大学QCT扫描和384例Rochester的跨机构数据中训练与验证。

Result: 该方法实现了跨机构QCT股骨近端解剖结构的鲁棒分割，优于传统深度网络，且量化指标在不同数据域间表现出更强的一致性和泛化能力。

Conclusion: 提出的域自适应分割框架，有效解决了多中心QCT分析中的域偏移问题，为骨折和骨质疏松相关影像的多机构研究和临床应用提供了可行的自动分割解决方案。

Abstract: Quantitative computed tomography (QCT) plays a crucial role in assessing bone
strength and fracture risk by enabling volumetric analysis of bone density
distribution in the proximal femur. However, deploying automated segmentation
models in practice remains difficult because deep networks trained on one
dataset often fail when applied to another. This failure stems from domain
shift, where scanners, reconstruction settings, and patient demographics vary
across institutions, leading to unstable predictions and unreliable
quantitative metrics. Overcoming this barrier is essential for multi-center
osteoporosis research and for ensuring that radiomics and structural finite
element analysis results remain reproducible across sites. In this work, we
developed a domain-adaptive transformer segmentation framework tailored for
multi-institutional QCT. Our model is trained and validated on one of the
largest hip fracture related research cohorts to date, comprising 1,024 QCT
images scans from Tulane University and 384 scans from Rochester, Minnesota for
proximal femur segmentation. To address domain shift, we integrate two
complementary strategies within a 3D TransUNet backbone: adversarial alignment
via Gradient Reversal Layer (GRL), which discourages the network from encoding
site-specific cues, and statistical alignment via Maximum Mean Discrepancy
(MMD), which explicitly reduces distributional mismatches between institutions.
This dual mechanism balances invariance and fine-grained alignment, enabling
scanner-agnostic feature learning while preserving anatomical detail.

</details>


### [3] [DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2510.26921)
*Moonsoo Jeong,Dongbeen Kim,Minseong Kim,Sungkil Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于方向一致性（DC）的自适应密度控制方法，用于3D高斯斑点（DC4GS），能够在减少原始体素数量的同时提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应密度控制（ADC）主要依据位置梯度的幅值进行体素分裂，容易造成不必要的冗余分裂，无法精确贴合局部结构的复杂性。

Method: 在ADC方法中引入方向一致性（DC）作为度量，基于梯度的角度一致性判断是否需要分裂；当分裂时，根据DC选取更优的分裂位置，使子体素与局部结构对齐，优于以往的随机分布。

Result: 实验结果表明，DC4GS方法可减少高达30%的体素数量，同时显著提升三维重建的保真度。

Conclusion: 通过引入方向一致性，DC4GS不仅优化了体素的分裂策略，有效减冗，还显著提高了3D重建的精度，可为相关三维视觉任务带来更高效且高质量的解决方案。

Abstract: We present a Directional Consistency (DC)-driven Adaptive Density Control
(ADC) for 3D Gaussian Splatting (DC4GS). Whereas the conventional ADC bases its
primitive splitting on the magnitudes of positional gradients, we further
incorporate the DC of the gradients into ADC, and realize it through the
angular coherence of the gradients. Our DC better captures local structural
complexities in ADC, avoiding redundant splitting. When splitting is required,
we again utilize the DC to define optimal split positions so that
sub-primitives best align with the local structures than the conventional
random placement. As a consequence, our DC4GS greatly reduces the number of
primitives (up to 30% in our experiments) than the existing ADC, and also
enhances reconstruction fidelity greatly.

</details>


### [4] [Scale-Aware Curriculum Learning for Ddata-Efficient Lung Nodule Detection with YOLOv11](https://arxiv.org/abs/2510.26923)
*Yi Luo,Yike Guo,Hamed Hooshangnejad,Kai Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Scale Adaptive Curriculum Learning (SACL)的新训练策略，能够在有限标注数据条件下提升肺结节检测性能，显著优于传统固定课程策略。


<details>
  <summary>Details</summary>
Motivation: 在肺部结节CT检测中，早期癌症诊断依赖于深度学习模型。但在临床实际应用时，往往缺乏充足的标注数据，现有课程学习方法在此情境下表现不佳。

Method: 作者提出SACL训练策略，包含三大机制：（1）自适应训练周期调度；（2）困难样本注入；（3）规模感知优化。该策略依赖于可用数据规模动态调整课程设计。实验采用LUNA25肺结节数据集，以YOLOv11为检测器进行评测。

Result: SACL在使用全部数据时，与静态课程学习取得了相当的mAP50表现。但在数据有限时，SACL分别在仅用10%、20%、50%训练数据时，mAP50提高4.6%、3.5%、2.0%，效果显著优于基线。

Conclusion: SACL无需更改网络结构，可增强模型适应不同数据规模的训练能力，帮助医疗机构在标注资源有限条件下开发有效的肺结节检测系统。

Abstract: Lung nodule detection in chest CT is crucial for early lung cancer diagnosis,
yet existing deep learning approaches face challenges when deployed in clinical
settings with limited annotated data. While curriculum learning has shown
promise in improving model training, traditional static curriculum strategies
fail in data-scarce scenarios. We propose Scale Adaptive Curriculum Learning
(SACL), a novel training strategy that dynamically adjusts curriculum design
based on available data scale. SACL introduces three key mechanisms:(1)
adaptive epoch scheduling, (2) hard sample injection, and (3) scale-aware
optimization. We evaluate SACL on the LUNA25 dataset using YOLOv11 as the base
detector. Experimental results demonstrate that while SACL achieves comparable
performance to static curriculum learning on the full dataset in mAP50, it
shows significant advantages under data-limited conditions with 4.6%, 3.5%, and
2.0% improvements over baseline at 10%, 20%, and 50% of training data
respectively. By enabling robust training across varying data scales without
architectural modifications, SACL provides a practical solution for healthcare
institutions to develop effective lung nodule detection systems despite limited
annotation resources.

</details>


### [5] [SYNAPSE-Net: A Unified Framework with Lesion-Aware Hierarchical Gating for Robust Segmentation of Heterogeneous Brain Lesions](https://arxiv.org/abs/2510.26961)
*Md. Mehedi Hassan,Shafqat Alam,Shahriar Ahmed Seam,Maruf Ahmed*

Main category: cs.CV

TL;DR: 提出了一种新型自动分割多模态MRI脑部病变的深度学习方法SYNAPSE-Net，在多个公开挑战赛数据集上取得了最优的分割效果，具有良好的泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习分割模型在脑部多模态影像分割任务上存在泛化能力差、性能波动大等问题，缺乏在临床可用的可靠性和鲁棒性，亟需更统一且自适应的分割方法。

Method: 提出了Unified Multi-Stream SYNAPSE-Net框架，结合了多流CNN编码器、Swin Transformer全局上下文提取、动态跨模态注意力融合（CMAF）机制和分层门控解码器，并辅以病理增强和难度采样的方差下降训练策略。

Result: 在MICCAI 2017 WMH、ISLES 2022 和 BraTS 2020三个公开数据集上测试，取得了领先的分割性能（如DSC和HD95等指标均优于现有方法）。

Conclusion: SYNAPSE-Net在多种脑部病变自动分割任务中实现了最先进性能，显示出良好的通用性、鲁棒性和临床应用潜力。

Abstract: Automated segmentation of heterogeneous brain lesions from multi-modal MRI
remains a critical challenge in clinical neuroimaging. Current deep learning
models are typically specialized `point solutions' that lack generalization and
high performance variance, limiting their clinical reliability. To address
these gaps, we propose the Unified Multi-Stream SYNAPSE-Net, an adaptive
framework designed for both generalization and robustness. The framework is
built on a novel hybrid architecture integrating multi-stream CNN encoders, a
Swin Transformer bottleneck for global context, a dynamic cross-modal attention
fusion (CMAF) mechanism, and a hierarchical gated decoder for high-fidelity
mask reconstruction. The architecture is trained with a variance reduction
strategy that combines pathology specific data augmentation and
difficulty-aware sampling method. The model was evaluated on three different
challenging public datasets: the MICCAI 2017 WMH Challenge, the ISLES 2022
Challenge, and the BraTS 2020 Challenge. Our framework attained a
state-of-the-art DSC value of 0.831 with the HD95 value of 3.03 in the WMH
dataset. For ISLES 2022, it achieved the best boundary accuracy with a
statistically significant difference (HD95 value of 9.69). For BraTS 2020, it
reached the highest DSC value for the tumor core region (0.8651). These
experimental findings suggest that our unified adaptive framework achieves
state-of-the-art performance across multiple brain pathologies, providing a
robust and clinically feasible solution for automated segmentation. The source
code and the pre-trained models are available at
https://github.com/mubid-01/SYNAPSE-Net-pre.

</details>


### [6] [Semantic Frame Aggregation-based Transformer for Live Video Comment Generation](https://arxiv.org/abs/2510.26978)
*Anam Fatima,Yi Yu,Janak Kapuriya,Julien Lalanne,Jainendra Shukla*

Main category: cs.CV

TL;DR: 本文提出了一种新模型（SFAT）用于自动生成直播视频上下文相关的评论，通过语义帧加权提高评论的精准性，并构建了大规模多样的英文数据集验证模型有效性。


<details>
  <summary>Details</summary>
Motivation: 随着Twitch等平台的直播视频互动评论流行，自动生成符合上下文、恰当的评论成为挑战，现有方法忽视了视频中与观众互动最相关帧的筛选与利用。作者希望通过突出关键帧，提升自动评论的质量和相关性，并弥补公开数据集语言和内容类型的不足。

Method: 提出一种基于语义帧聚合的Transformer（SFAT）模型，利用CLIP多模态知识，对视频帧进行语义相关性打分，采用加权和机制突出核心信息帧，并结合跨模态注意力机制，从视频和对话多源输入中生成评论。还新构建了涵盖11类目、438小时、320万英文评论的大型数据集。

Result: SFAT模型在自动生成直播评论任务中，较现有方法在多项指标上表现更优，证明了语义帧加权与多模态聚合对提升评论上下文相关性与自然性的有效性。

Conclusion: 通过突出视频中与互动最相关的语义帧并聚合多模态信息，SFAT模型在生成直播评论任务上取得突破，可作为未来多模态实时互动文本生成的重要技术基础。

Abstract: Live commenting on video streams has surged in popularity on platforms like
Twitch, enhancing viewer engagement through dynamic interactions. However,
automatically generating contextually appropriate comments remains a
challenging and exciting task. Video streams can contain a vast amount of data
and extraneous content. Existing approaches tend to overlook an important
aspect of prioritizing video frames that are most relevant to ongoing viewer
interactions. This prioritization is crucial for producing contextually
appropriate comments. To address this gap, we introduce a novel Semantic Frame
Aggregation-based Transformer (SFAT) model for live video comment generation.
This method not only leverages CLIP's visual-text multimodal knowledge to
generate comments but also assigns weights to video frames based on their
semantic relevance to ongoing viewer conversation. It employs an efficient
weighted sum of frames technique to emphasize informative frames while focusing
less on irrelevant ones. Finally, our comment decoder with a cross-attention
mechanism that attends to each modality ensures that the generated comment
reflects contextual cues from both chats and video. Furthermore, to address the
limitations of existing datasets, which predominantly focus on Chinese-language
content with limited video categories, we have constructed a large scale,
diverse, multimodal English video comments dataset. Extracted from Twitch, this
dataset covers 11 video categories, totaling 438 hours and 3.2 million
comments. We demonstrate the effectiveness of our SFAT model by comparing it to
existing methods for generating comments from live video and ongoing dialogue
contexts.

</details>


### [7] [MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation](https://arxiv.org/abs/2510.26996)
*Arghavan Rezvani,Xiangyi Yan,Anthony T. Wu,Kun Han,Pooya Khosravi,Xiaohui Xie*

Main category: cs.CV

TL;DR: MoME是一种结合视觉和语言的医学图像分割模型，采用了Mixture of Experts架构，并在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割是一个挑战性的任务，传统方法难以同时处理多尺度信息和复杂语义。最近的大模型（如LLMs和MoE）在自然语言处理上取得巨大成功，因此，作者希望将这些方法引入医学视觉-语言领域，探索其提升医学图像分割性能的潜力。

Method: MoME利用了Mixture of Experts（MoE）框架，将多尺度视觉特征与文本嵌入结合，引入动态专家选择机制。其框架允许模型针对医学图像的复杂结构自动选择最适合的专家子模型，从而更好地理解医学图像，并辅以语言信息增强。该研究在10个公开数据集（包含3,410份CT扫描）上进行了测试。

Result: MoME在大规模医学图像分割基准测试中表现出色，在多个数据集上获得了具有竞争力的分割精度，显示了将视觉和语言模型结合的有效性。

Conclusion: MoME实现了视觉-语言基础模型在医学图像领域的有效集成，展示了利用MoE提升医学图像分割性能的可行性和优越性。该方法可为后续研究提供新思路。

Abstract: In this study, we propose MoME, a Mixture of Visual Language Medical Experts,
for Medical Image Segmentation. MoME adapts the successful Mixture of Experts
(MoE) paradigm, widely used in Large Language Models (LLMs), for medical
vision-language tasks. The architecture enables dynamic expert selection by
effectively utilizing multi-scale visual features tailored to the intricacies
of medical imagery, enriched with textual embeddings. This work explores a
novel integration of vision-language models for this domain. Utilizing an
assembly of 10 datasets, encompassing 3,410 CT scans, MoME demonstrates strong
performance on a comprehensive medical imaging segmentation benchmark. Our
approach explores the integration of foundation models for medical imaging,
benefiting from the established efficacy of MoE in boosting model performance
by incorporating textual information. Demonstrating competitive precision
across multiple datasets, MoME explores a novel architecture for achieving
robust results in medical image analysis.

</details>


### [8] [Incremental Human-Object Interaction Detection with Invariant Relation Representation Learning](https://arxiv.org/abs/2510.27020)
*Yana Wei,Zeen Chi,Chongyu Wang,Yu Wu,Shipeng Yan,Yongfei Liu,Xuming He*

Main category: cs.CV

TL;DR: 本文提出了一种新的示例无关增量关系蒸馏（IRD）框架，用于解决开放世界的人-物交互（HOI）检测中的知识遗忘、交互漂移及零样本检测难题，并在HICO-DET和V-COCO数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，人-物交互是动态变化的，而现有的方法大多假设交互类别固定，难以适应开放世界环境。为了让模型像人类一样能够逐步学习新的人-物交互，克服增量学习中的遗忘、交互漂移和零样本问题，作者提出了该工作。

Method: 作者提出了IRD框架，创新点包括将对象和关系的学习解耦，并引入两种新的蒸馏损失，用于学习不同交互组合中相同关系的不变特征，同时无需保存历史样本（exemplar-free）。

Result: 在HICO-DET和V-COCO两个开放世界HOI数据集上的大量实验表明，IRD方法在减轻遗忘、增强对交互漂移的鲁棒性，以及对零样本交互的泛化能力方面，均优于现有主流方法。

Conclusion: IRD为开放世界HOI检测提供了一种有效的解决方案，不仅缓解了增量学习中的遗忘问题，还提升了新颖未见交互的检测能力，在实际的动态环境中有广阔应用前景。

Abstract: In open-world environments, human-object interactions (HOIs) evolve
continuously, challenging conventional closed-world HOI detection models.
Inspired by humans' ability to progressively acquire knowledge, we explore
incremental HOI detection (IHOID) to develop agents capable of discerning
human-object relations in such dynamic environments. This setup confronts not
only the common issue of catastrophic forgetting in incremental learning but
also distinct challenges posed by interaction drift and detecting zero-shot HOI
combinations with sequentially arriving data. Therefore, we propose a novel
exemplar-free incremental relation distillation (IRD) framework. IRD decouples
the learning of objects and relations, and introduces two unique distillation
losses for learning invariant relation features across different HOI
combinations that share the same relation. Extensive experiments on HICO-DET
and V-COCO datasets demonstrate the superiority of our method over
state-of-the-art baselines in mitigating forgetting, strengthening robustness
against interaction drift, and generalization on zero-shot HOIs. Code is
available at \href{https://github.com/weiyana/ContinualHOI}{this HTTP URL}

</details>


### [9] [VitalLens 2.0: High-Fidelity rPPG for Heart Rate Variability Estimation from Face Video](https://arxiv.org/abs/2510.27028)
*Philipp V. Rouast*

Main category: cs.CV

TL;DR: VitalLens 2.0 是一种通过面部视频估算生理信号的新型深度学习模型，在评估心率（HR）、呼吸频率（RR）及心率变异性（HRV）方面表现优异，达到当前最先进水平。


<details>
  <summary>Details</summary>
Motivation: 由于远程无创生理信号检测（如 rPPG）精度有限，且很少有模型能准确估算多种生理指标（HR、RR、HRV），本研究旨在提升这些能力，推动健康监测技术的发展。

Method: 该论文提出了新颖的模型架构，并大幅扩充了训练数据集，覆盖了1,413位不同个体。模型在融合自四个公开与私有数据集的422例独立个体上进行测试。实验采用 MAE 评估 HR、RR 和 HRV 等性能指标。

Result: VitalLens 2.0 对 HR 的 MAE 为 1.57 bpm，RR 的 MAE 为 1.08 bpm，HRV-SDNN 的 MAE 为 10.18 ms，HRV-RMSSD 的 MAE 为 16.45 ms，均优于现有同类方法。

Conclusion: VitalLens 2.0 有效提升了面部视频生理信号估算的准确性，实现远程健康监测新突破，已通过 VitalLens API 向开发者开放。

Abstract: This report introduces VitalLens 2.0, a new deep learning model for
estimating physiological signals from face video. This new model demonstrates a
significant leap in accuracy for remote photoplethysmography (rPPG), enabling
the robust estimation of not only heart rate (HR) and respiratory rate (RR) but
also Heart Rate Variability (HRV) metrics. This advance is achieved through a
combination of a new model architecture and a substantial increase in the size
and diversity of our training data, now totaling 1,413 unique individuals. We
evaluate VitalLens 2.0 on a new, combined test set of 422 unique individuals
from four public and private datasets. When averaging results by individual,
VitalLens 2.0 achieves a Mean Absolute Error (MAE) of 1.57 bpm for HR, 1.08 bpm
for RR, 10.18 ms for HRV-SDNN, and 16.45 ms for HRV-RMSSD. These results
represent a new state-of-the-art, significantly outperforming previous methods.
This model is now available to developers via the VitalLens API at
https://rouast.com/api.

</details>


### [10] [AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception](https://arxiv.org/abs/2510.27047)
*Mario Camarena,Het Patel,Fatemeh Nazari,Evangelos Papalexakis,Mohamadhossein Noruzoliaee,Jia Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为AD-SAM的感知模型，对自动驾驶场景中的语义分割进行了优化，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 原始的Segment Anything Model（SAM）在自动驾驶复杂道路场景下的空间和几何信息建模能力不足，影响了分割效果，因此需要对其进行有针对性的改进以适应自动驾驶语义分割需求。

Method: AD-SAM包含双编码器和可变形解码器结构：双编码器结合了SAM预训练ViT-H获取全局语义信息和ResNet-50获得的局部空间细节，通过可变形融合模块对不同尺度和不同结构特征对齐，解码器分阶段利用可变形注意力递进式地细化分割结果。训练过程中融合了Focal、Dice、Lovasz-Softmax和Surface多种损失函数，以提升类别平衡、边界分割精度和优化稳定性。

Result: 在Cityscapes和BDD100K数据集上，AD-SAM的mIoU分别达到68.1和59.5，领先SAM、G-SAM和DeepLabV3基准模型，分割精度提升显著（最高+22.9 mIoU）。此外，模型在跨领域泛化、数据效率和训练速度方面也表现优异。

Conclusion: 针对自动驾驶场景构建和优化基础视觉模型，能够显著提升分割性能、泛化能力和数据利用效率，为大规模自动驾驶感知系统提供坚实基础。

Abstract: This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), a
fine-tuned vision foundation model for semantic segmentation in autonomous
driving (AD). AD-SAM extends the Segment Anything Model (SAM) with a
dual-encoder and deformable decoder tailored to spatial and geometric
complexity of road scenes. The dual-encoder produces multi-scale fused
representations by combining global semantic context from SAM's pretrained
Vision Transformer (ViT-H) with local spatial detail from a trainable
convolutional deep learning backbone (i.e., ResNet-50). A deformable fusion
module aligns heterogeneous features across scales and object geometries. The
decoder performs progressive multi-stage refinement using deformable attention.
Training is guided by a hybrid loss that integrates Focal, Dice,
Lovasz-Softmax, and Surface losses, improving semantic class balance, boundary
precision, and optimization stability. Experiments on the Cityscapes and
Berkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM,
Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) in
segmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) on
Cityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 by
margins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes,
respectively. AD-SAM demonstrates strong cross-domain generalization with a
0.87 retention score (vs. 0.76 for SAM), and faster, more stable learning
dynamics, converging within 30-40 epochs, enjoying double the learning speed of
benchmark models. It maintains 0.607 mIoU with only 1000 samples, suggesting
data efficiency critical for reducing annotation costs. These results confirm
that targeted architectural and optimization enhancements to foundation models
enable reliable and scalable AD perception.

</details>


### [11] [Hierarchical Transformers for Unsupervised 3D Shape Abstraction](https://arxiv.org/abs/2510.27088)
*Aditya Vora,Lily Goli,Andrea Tagliasacchi,Hao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的分层神经场（HiT），能在无需监督的情况下，以由粗到细的方式学习3D物体的通用分层结构，并成功应用于ShapeNet大规模数据，实现多层次自动分割。


<details>
  <summary>Details</summary>
Motivation: 现有的3D形状分层方法常依赖固定的分层结构（如二叉树）或受限于特定类别，难以自适应不同类别复杂、通用的层次结构。因此，作者希望提出一种灵活的方法，不受树形结构约束，能在多类别数据上自动发现和表达更复杂的层级关系。

Method: 作者提出了层次变换器（HiT），在每一层通过压缩的码本学习树状结构中的父子关系。该码本帮助网络自动识别不同类别形状中的共性子结构。整个层次结构唯一约束是每层节点总数，其他无特别限制，模型通过重建损失进行训练，能自适应地从数据中学习和推断复杂分层结构。

Result: 在ShapeNet全55个类别上进行无监督分割实验，表现出色，能将形状分割为多个不同层次的有意义的部分，捕捉到父子之间的包涵关系。

Conclusion: 该方法无需固定分层结构，适用于多类别、大规模3D形状集合，能自动捕获复杂和有意义的分层关系，比以往方法更灵活和通用。

Abstract: We introduce HiT, a novel hierarchical neural field representation for 3D
shapes that learns general hierarchies in a coarse-to-fine manner across
different shape categories in an unsupervised setting. Our key contribution is
a hierarchical transformer (HiT), where each level learns parent-child
relationships of the tree hierarchy using a compressed codebook. This codebook
enables the network to automatically identify common substructures across
potentially diverse shape categories. Unlike previous works that constrain the
task to a fixed hierarchical structure (e.g., binary), we impose no such
restriction, except for limiting the total number of nodes at each tree level.
This flexibility allows our method to infer the hierarchical structure directly
from data, over multiple shape categories, and representing more general and
complex hierarchies than prior approaches. When trained at scale with a
reconstruction loss, our model captures meaningful containment relationships
between parent and child nodes. We demonstrate its effectiveness through an
unsupervised shape segmentation task over all 55 ShapeNet categories, where our
method successfully segments shapes into multiple levels of granularity.

</details>


### [12] [ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding](https://arxiv.org/abs/2510.27128)
*Haonan Wang,Jingyu Lu,Hongrui Li,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了ZEBRA框架，实现了不需要针对个体微调的零样本脑视觉解码。通过分离大脑fMRI数据中的个体相关和语义相关成分，从而实现了模型对新被试的泛化能力。实验显示，ZEBRA在多项指标上优于以往零样本方法，并接近针对个体微调的方法。


<details>
  <summary>Details</summary>
Motivation: 现有脑活动到视觉图像重构的方法通常需要对每个受试者分别训练或微调模型，这严重限制了实际应用和大规模推广。为了解决这一瓶颈，需要一种无需针对特定个体适配，同时可以泛化到新受试者的方法。

Method: 提出ZEBRA零样本脑视觉解码框架，利用对抗训练将fMRI表征分解为受试者相关和语义相关两个部分，并使模型专注于提取跨个体一致的语义信息。该方法实现对未见过的受试者直接解码，无需额外fMRI数据或再训练。

Result: 实验结果表明，ZEBRA在多个评价指标上显著优于现有零样本基线方法，并且与那些需要针对个体微调的模型性能接近。

Conclusion: ZEBRA为通用、易扩展的脑神经解码提供了新的方向，为脑-机接口等应用的实用化和大规模推广奠定了基础。

Abstract: Recent advances in neural decoding have enabled the reconstruction of visual
experiences from brain activity, positioning fMRI-to-image reconstruction as a
promising bridge between neuroscience and computer vision. However, current
methods predominantly rely on subject-specific models or require
subject-specific fine-tuning, limiting their scalability and real-world
applicability. In this work, we introduce ZEBRA, the first zero-shot brain
visual decoding framework that eliminates the need for subject-specific
adaptation. ZEBRA is built on the key insight that fMRI representations can be
decomposed into subject-related and semantic-related components. By leveraging
adversarial training, our method explicitly disentangles these components to
isolate subject-invariant, semantic-specific representations. This
disentanglement allows ZEBRA to generalize to unseen subjects without any
additional fMRI data or retraining. Extensive experiments show that ZEBRA
significantly outperforms zero-shot baselines and achieves performance
comparable to fully finetuned models on several metrics. Our work represents a
scalable and practical step toward universal neural decoding. Code and model
weights are available at: https://github.com/xmed-lab/ZEBRA.

</details>


### [13] [WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond](https://arxiv.org/abs/2510.27133)
*Zhicong Sun,Jacqueline Lo,Jinxing Hu*

Main category: cs.CV

TL;DR: 本论文提出了WildfireX-SLAM，一个高质量、大规模的森林环境合成数据集，支持3D高斯溅射SLAM在野火和森林场景中的研究。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射（3DGS）SLAM在室内小场景取得进展，但应用于森林大规模环境，尤其面向野火应急和森林管理，缺乏高质量数据集限制了发展。实际采集难度高、成本大。

Method: 利用Unreal Engine 5环境，构建自动采集流程，获得带真实姿态的多模态、覆盖16km²森林区域的无人机视角空地RGB-D图像数据（共5500余张），并可灵活控制环境变量（光照、天气、火灾类型等）。

Result: WildfireX-SLAM数据集发布，并基于其进行了基准测试，揭示了森林大场景下3DGS-SLAM的特殊挑战，同时为未来算法改进提供方向。

Conclusion: WildfireX-SLAM数据集为3DGS-SLAM在森林与野火环境下应用提供了关键支撑，推动SLAM方法在实际应急和生态管理中的发展。数据与代码将公开。

Abstract: 3D Gaussian splatting (3DGS) and its subsequent variants have led to
remarkable progress in simultaneous localization and mapping (SLAM). While most
recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing
3DGS-based SLAM methods for large-scale forest scenes holds great potential for
many real-world applications, especially for wildfire emergency response and
forest management. However, this line of research is impeded by the absence of
a comprehensive and high-quality dataset, and collecting such a dataset over
real-world scenes is costly and technically infeasible. To this end, we have
built a large-scale, comprehensive, and high-quality synthetic dataset for SLAM
in wildfire and forest environments. Leveraging the Unreal Engine 5 Electric
Dreams Environment Sample Project, we developed a pipeline to easily collect
aerial and ground views, including ground-truth camera poses and a range of
additional data modalities from unmanned aerial vehicle. Our pipeline also
provides flexible controls on environmental factors such as light, weather, and
types and conditions of wildfire, supporting the need for various tasks
covering forest mapping, wildfire emergency response, and beyond. The resulting
pilot dataset, WildfireX-SLAM, contains 5.5k low-altitude RGB-D aerial images
from a large-scale forest map with a total size of 16 km2. On top of
WildfireX-SLAM, a thorough benchmark is also conducted, which not only reveals
the unique challenges of 3DGS-based SLAM in the forest but also highlights
potential improvements for future works. The dataset and code will be publicly
available. Project page: https://zhicongsun.github.io/wildfirexslam.

</details>


### [14] [E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources](https://arxiv.org/abs/2510.27135)
*Tong Shen,Jingai Yu,Dong Zhou,Dong Li,Emad Barsoum*

Main category: cs.CV

TL;DR: E-MMDiT是一种高效轻量级的多模态扩散模型，仅需304M参数，实现快速的图像生成，训练资源要求低，且效果与主流模型有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成质量高，但训练和推理资源消耗大，结构复杂，限制了其广泛应用。本文旨在以更少资源和参数，设计一款高效实用、易复现的生成模型。

Method: 1. 提出高度压缩的视觉分词器（visual tokenizer）以减少Token数量，降低计算量；2. 创新性地引入多路径压缩模块进一步压缩Token；3. 引入位置增强模块（Position Reinforcement）保证空间连贯性；4. 采用交替子区域注意力机制（ASA）在子区域范围内做注意力操作，进一步节省计算；5. 设计AdaLN-affine轻量模块高效计算Transformer调制参数。

Result: 仅用25M公开数据，在一台8卡AMD MI300X GPU机器上用1.5天训练，512px分辨率生成模型GenEval评分为0.66，经后处理可提升到0.72，达到了主流方法的竞争水平。

Conclusion: E-MMDiT实现了高效且低资源消耗的高质量图像生成，为后续相关研究和生成式AI模型普及提供了强有力的实用基线。

Abstract: Diffusion models have shown strong capabilities in generating high-quality
images from text prompts. However, these models often require large-scale
training data and significant computational resources to train, or suffer from
heavy structure with high latency. To this end, we propose Efficient Multimodal
Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal
diffusion model with only 304M parameters for fast image synthesis requiring
low training resources. We provide an easily reproducible baseline with
competitive results. Our model for 512px generation, trained with only 25M
public data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on
GenEval and easily reaches to 0.72 with some post-training techniques such as
GRPO. Our design philosophy centers on token reduction as the computational
cost scales significantly with the token count. We adopt a highly compressive
visual tokenizer to produce a more compact representation and propose a novel
multi-path compression module for further compression of tokens. To enhance our
design, we introduce Position Reinforcement, which strengthens positional
information to maintain spatial coherence, and Alternating Subregion Attention
(ASA), which performs attention within subregions to further reduce
computational cost. In addition, we propose AdaLN-affine, an efficient
lightweight module for computing modulation parameters in transformer blocks.
Our code is available at https://github.com/AMD-AGI/Nitro-E and we hope E-MMDiT
serves as a strong and practical baseline for future research and contributes
to democratization of generative AI models.

</details>


### [15] [Improving Cross-view Object Geo-localization: A Dual Attention Approach with Cross-view Interaction and Multi-Scale Spatial Features](https://arxiv.org/abs/2510.27139)
*Xingtao Ling Yingying Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨视角目标地理定位方法，通过引入跨视角跨注意力模块和多头空间注意力模块，提升了地理定位的精度，并创建了新的数据集G2D用于地面-无人机定位任务。实验验证了方法优于已有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的跨视角目标地理定位方法未能有效传递视角之间的信息，且空间关系特征图未做充分优化，导致模型易受噪声影响，影响定位性能。

Method: 本文提出了CVCAM模块，实现在两个视角之间多次迭代的上下文信息交换，强化跨视角关系理解和噪声抑制。同时加入MHSAM模块，通过不同尺寸卷积核提取多尺度空间特征，增强查询目标的表征能力。此外，针对当前数据集稀缺的问题，构建了“地面-无人机”G2D新数据集。

Result: 在CVOGL和G2D数据集上的大量实验表明，所提方法的定位精度显著提升，超越现有最先进水平。

Conclusion: 提出的CVCAM与MHSAM模块有效提升了跨视角目标地理定位的精度，同时新G2D数据集为“地面-无人机”定位任务提供了有力支撑，推动了该领域研究进展。

Abstract: Cross-view object geo-localization has recently gained attention due to
potential applications. Existing methods aim to capture spatial dependencies of
query objects between different views through attention mechanisms to obtain
spatial relationship feature maps, which are then used to predict object
locations. Although promising, these approaches fail to effectively transfer
information between views and do not further refine the spatial relationship
feature maps. This results in the model erroneously focusing on irrelevant edge
noise, thereby affecting localization performance. To address these
limitations, we introduce a Cross-view and Cross-attention Module (CVCAM),
which performs multiple iterations of interaction between the two views,
enabling continuous exchange and learning of contextual information about the
query object from both perspectives. This facilitates a deeper understanding of
cross-view relationships while suppressing the edge noise unrelated to the
query object. Furthermore, we integrate a Multi-head Spatial Attention Module
(MHSAM), which employs convolutional kernels of various sizes to extract
multi-scale spatial features from the feature maps containing implicit
correspondences, further enhancing the feature representation of the query
object. Additionally, given the scarcity of datasets for cross-view object
geo-localization, we created a new dataset called G2D for the "Ground-to-Drone"
localization task, enriching existing datasets and filling the gap in
"Ground-to-Drone" localization task. Extensive experiments on the CVOGL and G2D
datasets demonstrate that our proposed method achieves high localization
accuracy, surpassing the current state-of-the-art.

</details>


### [16] [HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition](https://arxiv.org/abs/2510.27148)
*Jiacheng Hong,Kunzhen Wu,Mingrui Yu,Yichao Gu,Shengze Xue,Shuangjiu Xiao,Deli Dong*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层生成框架HiGS，使用户能逐步并有控制地扩展3D场景，显著提升了生成的可控性与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景生成方法大多为单步，难以兼顾复杂性与用户操作的简便性。作者希望借鉴人类认知分步构建场景的方式，提高生成过程的可控性与合理性。

Method: 提出HiGS框架，用户可通过选择关键语义物体迭代式地扩展场景，其外围区域由模型自动补全。核心方法是引入渐进分层空间—语义图（PHiSSG），动态组织物体的空间与语义关系，通过一一映射和递归布局优化保证全程的结构与几何一致性。

Result: 实验证明，HiGS在布局合理性、风格一致性和用户偏好等方面优于现有的单阶段方法。

Conclusion: HiGS为高效、可控和可扩展的3D场景生成提供了新范式，有助于推动虚拟场景构建的发展。

Abstract: Three-dimensional scene generation holds significant potential in gaming,
film, and virtual reality. However, most existing methods adopt a single-step
generation process, making it difficult to balance scene complexity with
minimal user input. Inspired by the human cognitive process in scene modeling,
which progresses from global to local, focuses on key elements, and completes
the scene through semantic association, we propose HiGS, a hierarchical
generative framework for multi-step associative semantic spatial composition.
HiGS enables users to iteratively expand scenes by selecting key semantic
objects, offering fine-grained control over regions of interest while the model
completes peripheral areas automatically. To support structured and coherent
generation, we introduce the Progressive Hierarchical Spatial-Semantic Graph
(PHiSSG), which dynamically organizes spatial relationships and semantic
dependencies across the evolving scene structure. PHiSSG ensures spatial and
geometric consistency throughout the generation process by maintaining a
one-to-one mapping between graph nodes and generated objects and supporting
recursive layout optimization. Experiments demonstrate that HiGS outperforms
single-stage methods in layout plausibility, style consistency, and user
preference, offering a controllable and extensible paradigm for efficient 3D
scene construction.

</details>


### [17] [AFM-Net: Advanced Fusing Hierarchical CNN Visual Priors with Global Sequence Modeling for Remote Sensing Image Scene Classification](https://arxiv.org/abs/2510.27155)
*Yuanhao Tang,Xuechao Zou,Zhengpei Hu,Junliang Xing,Chengkun Zhang,Jianqiang Huang*

Main category: cs.CV

TL;DR: 提出了一种名为AFM-Net的新型框架，通过融合CNN与Mamba分支，高效结合局部特征和全局特征，实现遥感图像场景分类的新突破，准确率超越主流方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像场景分类因地物结构复杂和多尺度特性而具挑战性。传统CNN擅长本地纹理建模，Transformer适合捕捉全局语境，但二者高效融合受限于算力问题。该文旨在提出能兼具局部、全局特征且高效的新方法。

Method: 设计了AFM-Net框架，包含CNN分支（提取分层视觉先验）和Mamba分支（高效全局序列建模）。通过层次融合机制（Hierarchical Fusion Mechanism）动态聚合多尺度特征，实现交互和上下文重建。最终经Mixture-of-Experts分类模块自适应路由至最合适专家进行判别。

Result: AFM-Net在AID、NWPU-RESISC45和UC Merced三个遥感场景分类数据集上取得93.72%、95.54%和96.92%的准确率，全面超越当前主流方法，并在性能与效率之间取得平衡。

Conclusion: AFM-Net能高效融合CNN与Mamba分支的多尺度信息及上下文，显著提升遥感图像场景分类精度，为此领域提供了一种高效实用的新范式。

Abstract: Remote sensing image scene classification remains a challenging task,
primarily due to the complex spatial structures and multi-scale characteristics
of ground objects. Existing approaches see CNNs excel at modeling local
textures, while Transformers excel at capturing global context. However,
efficiently integrating them remains a bottleneck due to the high computational
cost of Transformers. To tackle this, we propose AFM-Net, a novel Advanced
Hierarchical Fusing framework that achieves effective local and global
co-representation through two pathways: a CNN branch for extracting
hierarchical visual priors, and a Mamba branch for efficient global sequence
modeling. The core innovation of AFM-Net lies in its Hierarchical Fusion
Mechanism, which progressively aggregates multi-scale features from both
pathways, enabling dynamic cross-level feature interaction and contextual
reconstruction to produce highly discriminative representations. These fused
features are then adaptively routed through a Mixture-of-Experts classifier
module, which dispatches them to the most suitable experts for fine-grained
scene recognition. Experiments on AID, NWPU-RESISC45, and UC Merced show that
AFM-Net obtains 93.72, 95.54, and 96.92 percent accuracy, surpassing
state-of-the-art methods with balanced performance and efficiency. Code is
available at https://github.com/tangyuanhao-qhu/AFM-Net.

</details>


### [18] [How Close Are We? Limitations and Progress of AI Models in Banff Lesion Scoring](https://arxiv.org/abs/2510.27158)
*Yanfan Zhu,Juming Xiong,Ruining Deng,Yu Wang,Yaohong Wang,Shilin Zhao,Mengmeng Yin,Yuqing Liu,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 该论文探讨了利用现有深度学习模型，通过模块化、基于规则的框架来近似Banff肾移植病理分级的可行性，发现现有AI存在多方面局限性。


<details>
  <summary>Details</summary>
Motivation: Banff分级是肾移植病理的国际标准，但其复杂、主观性和评判一致性差，导致AI自动化复制和落地应用面临挑战。

Method: 作者将Banff各指标细分为结构和炎症等组分，利用现有分割和检测模型，结合与专家指南一致的启发式规则，将模型输出映射为Banff分数，并与专家标注进行比对。

Result: 结果显示AI模型部分情况下可近似专家分数，但常出现结构遗漏、幻觉现象和检测不清等失误，中间表征也常不一致，损害了结果可解释性。

Conclusion: 当前AI尚难完全胜任专家级Banff分级，但模块化评估和建立计算标准将对后续模型开发和应用推广至关重要。

Abstract: The Banff Classification provides the global standard for evaluating renal
transplant biopsies, yet its semi-quantitative nature, complex criteria, and
inter-observer variability present significant challenges for computational
replication. In this study, we explore the feasibility of approximating Banff
lesion scores using existing deep learning models through a modular, rule-based
framework. We decompose each Banff indicator - such as glomerulitis (g),
peritubular capillaritis (ptc), and intimal arteritis (v) - into its
constituent structural and inflammatory components, and assess whether current
segmentation and detection tools can support their computation. Model outputs
are mapped to Banff scores using heuristic rules aligned with expert
guidelines, and evaluated against expert-annotated ground truths. Our findings
highlight both partial successes and critical failure modes, including
structural omission, hallucination, and detection ambiguity. Even when final
scores match expert annotations, inconsistencies in intermediate
representations often undermine interpretability. These results reveal the
limitations of current AI pipelines in replicating computational expert-level
grading, and emphasize the importance of modular evaluation and computational
Banff grading standard in guiding future model development for transplant
pathology.

</details>


### [19] [Generating Accurate and Detailed Captions for High-Resolution Images](https://arxiv.org/abs/2510.27164)
*Hankyeol Lee,Gawon Seo,Kyounggyu Lee,Dogun Kim,Kyungwoo Song,Jiyoung Jung*

Main category: cs.CV

TL;DR: 现有视觉-语言模型在高分辨率图像描述上表现不足，本文提出融合VLM、LLM与目标检测的新多阶段流程，显著提升高分辨率图像的描述质量，并有效减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: VLM通常只在低分辨率图像上预训练，直接将高分辨率图像缩小处理会损失细节，导致对高分辨率图片的描述不准确、遗漏重要信息。因此，提升高分辨率图像的描述质量和细节完整性成为亟需解决的问题。

Method: 作者提出一个多阶段的流程：首先用VLM生成初始描述，再通过LLM分析识别关键物体，并预测可能共现的其他物体；利用目标检测系统验证这些预测，发现新的未被描述的物体后，进行区域特定的补充描述。这一流程把精确检测与描述细致结合起来，并通过剔除未检测到的物体减少幻觉。

Result: 在精心设计的高分辨率图像数据集上实验证明，该流程生成的描述更详细、可靠，且比传统方法更有效地减少了幻觉现象。

Conclusion: 本文创新性地提出了融合VLM、LLM与目标检测的多阶段流程，有效提升了高分辨率图像的描述质量和细节完整性，并显著降低了幻觉风险，具备较强实用价值。

Abstract: Vision-language models (VLMs) often struggle to generate accurate and
detailed captions for high-resolution images since they are typically
pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels).
Downscaling high-resolution images to these dimensions may result in the loss
of visual details and the omission of important objects. To address this
limitation, we propose a novel pipeline that integrates vision-language models,
large language models (LLMs), and object detection systems to enhance caption
quality. Our proposed pipeline refines captions through a novel, multi-stage
process. Given a high-resolution image, an initial caption is first generated
using a VLM, and key objects in the image are then identified by an LLM. The
LLM predicts additional objects likely to co-occur with the identified key
objects, and these predictions are verified by object detection systems. Newly
detected objects not mentioned in the initial caption undergo focused,
region-specific captioning to ensure they are incorporated. This process
enriches caption detail while reducing hallucinations by removing references to
undetected objects. We evaluate the enhanced captions using pairwise comparison
and quantitative scoring from large multimodal models, along with a benchmark
for hallucination detection. Experiments on a curated dataset of
high-resolution images demonstrate that our pipeline produces more detailed and
reliable image captions while effectively minimizing hallucinations.

</details>


### [20] [M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar](https://arxiv.org/abs/2510.27166)
*Xiaozhi Li,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于多帧3D目标检测的统一框架M^3Detection，通过融合摄像头和4D成像雷达的数据，在多模态下实现多层次特征融合，提升了目标检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有摄像头-雷达融合方法多为单帧输入，场景感知信息不全，叠加图像退化和4D雷达稀疏性，导致检测性能受限。多帧融合可获得更丰富时空信息，但在特征融合效率和计算成本上存在挑战。

Method: M^3Detection框架包括：1）利用基础检测器的中间特征和跟踪器生成目标轨迹，提升计算效率；2）二阶段过程中通过以雷达信息为指导的全局级与局部级特征聚合模块，实现跨帧、跨模态的特征融合；3）最后通过基于轨迹的时空推理模块进一步编码帧间交互，增强时序特征表示。

Result: 在VoD和TJ4DRadSet数据集上大量实验表明，M^3Detection在融合摄像头和4D雷达的多帧目标检测任务中达到了最先进的性能水平。

Conclusion: M^3Detection有效克服了多帧多模态融合检测中的计算和特征融合难题，表现出优异的3D目标检测能力，验证了多帧摄像头-雷达数据融合感知的可行性和有效性。

Abstract: Recent advances in 4D imaging radar have enabled robust perception in adverse
weather, while camera sensors provide dense semantic information. Fusing the
these complementary modalities has great potential for cost-effective 3D
perception. However, most existing camera-radar fusion methods are limited to
single-frame inputs, capturing only a partial view of the scene. The incomplete
scene information, compounded by image degradation and 4D radar sparsity,
hinders overall detection performance. In contrast, multi-frame fusion offers
richer spatiotemporal information but faces two challenges: achieving robust
and effective object feature fusion across frames and modalities, and
mitigating the computational cost of redundant feature extraction.
Consequently, we propose M^3Detection, a unified multi-frame 3D object
detection framework that performs multi-level feature fusion on multi-modal
data from camera and 4D imaging radar. Our framework leverages intermediate
features from the baseline detector and employs the tracker to produce
reference trajectories, improving computational efficiency and providing richer
information for second-stage. In the second stage, we design a global-level
inter-object feature aggregation module guided by radar information to align
global features across candidate proposals and a local-level inter-grid feature
aggregation module that expands local features along the reference trajectories
to enhance fine-grained object representation. The aggregated features are then
processed by a trajectory-level multi-frame spatiotemporal reasoning module to
encode cross-frame interactions and enhance temporal representation. Extensive
experiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection
achieves state-of-the-art 3D detection performance, validating its
effectiveness in multi-frame detection with camera-4D imaging radar fusion.

</details>


### [21] [DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model](https://arxiv.org/abs/2510.27169)
*Yucheng Xing,Jinxing Yin,Xiaodong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种用于单人舞蹈视频生成的新型扩散模型框架DANCER，在细节再现与连续性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在视觉生成任务中的突破，视频生成受到越来越多关注，特别是涉及人体运动的舞蹈视频生成，其高自由度使得建模更为困难。

Method: 提出DANCER框架，基于最新稳定视频扩散模型。包括两个关键模块：1）外观增强模块（AEM），加强对参考图像细节建模；2）姿态渲染模块（PRM），扩展动作引导，能融合额外域的姿态条件。此外，构建了大型舞蹈视频数据集TikTok-3K以提升训练效果。

Result: 在真实世界数据集上的大量实验表明，DANCER在生成舞蹈视频的质量与连续性上都优于现有最先进方法。

Conclusion: DANCER为复杂人体视频生成任务提供了新思路，展现了在高质量、连续性舞蹈视频生成上的强大能力。数据和代码将在论文接受后公开。

Abstract: Recently, diffusion models have shown their impressive ability in visual
generation tasks. Besides static images, more and more research attentions have
been drawn to the generation of realistic videos. The video generation not only
has a higher requirement for the quality, but also brings a challenge in
ensuring the video continuity. Among all the video generation tasks,
human-involved contents, such as human dancing, are even more difficult to
generate due to the high degrees of freedom associated with human motions. In
this paper, we propose a novel framework, named as DANCER (Dance ANimation via
Condition Enhancement and Rendering with Diffusion Model), for realistic
single-person dance synthesis based on the most recent stable video diffusion
model. As the video generation is generally guided by a reference image and a
video sequence, we introduce two important modules into our framework to fully
benefit from the two inputs. More specifically, we design an Appearance
Enhancement Module (AEM) to focus more on the details of the reference image
during the generation, and extend the motion guidance through a Pose Rendering
Module (PRM) to capture pose conditions from extra domains. To further improve
the generation capability of our model, we also collect a large amount of video
data from Internet, and generate a novel datasetTikTok-3K to enhance the model
training. The effectiveness of the proposed model has been evaluated through
extensive experiments on real-world datasets, where the performance of our
model is superior to that of the state-of-the-art methods. All the data and
codes will be released upon acceptance.

</details>


### [22] [H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models](https://arxiv.org/abs/2510.27171)
*Mingyu Sung,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 论文提出了H2-Cache，一种面向扩散模型的分层缓存机制，实现了5倍加速且几乎不损失图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成领域表现突出，但实际应用受限于高昂的计算成本。现有缓存加速方法会在加速和生成质量之间出现权衡，存在质量下降及高额开销。

Method: 提出H2-Cache分层缓存机制，将扩散模型的去噪分为结构决定阶段和细节优化阶段，对这两个阶段分别设置阈值单独缓存。并引入Pooled Feature Summarization（PFS）用于高效相似性估计。

Result: 在Flux架构下，H2-Cache实验证明推理速度提升最高可达5.08倍，同时图像质量与原始模型几乎一致，优于现有缓存加速方法。

Conclusion: H2-Cache有效解决扩散模型加速与生成质量之间的矛盾，为高保真度扩散模型的实际应用提供了实用方案。

Abstract: Diffusion models have emerged as state-of-the-art in image generation, but
their practical deployment is hindered by the significant computational cost of
their iterative denoising process. While existing caching techniques can
accelerate inference, they often create a challenging trade-off between speed
and fidelity, suffering from quality degradation and high computational
overhead. To address these limitations, we introduce H2-Cache, a novel
hierarchical caching mechanism designed for modern generative diffusion model
architectures. Our method is founded on the key insight that the denoising
process can be functionally separated into a structure-defining stage and a
detail-refining stage. H2-cache leverages this by employing a dual-threshold
system, using independent thresholds to selectively cache each stage. To ensure
the efficiency of our dual-check approach, we introduce pooled feature
summarization (PFS), a lightweight technique for robust and fast similarity
estimation. Extensive experiments on the Flux architecture demonstrate that
H2-cache achieves significant acceleration (up to 5.08x) while maintaining
image quality nearly identical to the baseline, quantitatively and
qualitatively outperforming existing caching methods. Our work presents a
robust and practical solution that effectively resolves the speed-quality
dilemma, significantly lowering the barrier for the real-world application of
high-fidelity diffusion models. Source code is available at
https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.

</details>


### [23] [SilhouetteTell: Practical Video Identification Leveraging Blurred Recordings of Video Subtitles](https://arxiv.org/abs/2510.27179)
*Guanchong Huang,Song Fang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SilhouetteTell的新型视频识别攻击方法，通过分析视频字幕在屏幕上的轮廓（silhouette）及其时序信息，实现对受害者观看视频内容的推断，具备强大的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 视频识别攻击可泄露用户兴趣、政治立场、健康状况等敏感信息，当前相关攻击方法大多基于网络流量分析，受限于在线视频。论文旨在探索无需依赖网络流量、且适用于离线视频的新攻击途径，提升攻击的普适性和隐蔽性。

Method: 作者发现字幕内容决定其屏幕轮廓，并提出SilhouetteTell攻击，将字幕轮廓的空间特征与相邻字幕间的时间间隔结合为时空特征。通过对比记录的视频字幕轮廓序列与字幕文件，利用这种时空相关性实现视频内容推断。

Result: 在多种场景下，包括40米外使用普通智能手机录制视频，SilhouetteTell都能高效地猜测出受害者观看的视频标题或片段。实验结果显示该方法准确率较高，适用于在线及离线视频识别。

Conclusion: SilhouetteTell为视频识别攻击提供了新的思路，突破了传统依赖网络流量的局限，展现出极高的实际威胁性。必须警惕该类攻击对用户隐私带来的风险。

Abstract: Video identification attacks pose a significant privacy threat that can
reveal videos that victims watch, which may disclose their hobbies, religious
beliefs, political leanings, sexual orientation, and health status. Also, video
watching history can be used for user profiling or advertising and may result
in cyberbullying, discrimination, or blackmail. Existing extensive video
inference techniques usually depend on analyzing network traffic generated by
streaming online videos. In this work, we observe that the content of a
subtitle determines its silhouette displayed on the screen, and identifying
each subtitle silhouette also derives the temporal difference between two
consecutive subtitles. We then propose SilhouetteTell, a novel video
identification attack that combines the spatial and time domain information
into a spatiotemporal feature of subtitle silhouettes. SilhouetteTell explores
the spatiotemporal correlation between recorded subtitle silhouettes of a video
and its subtitle file. It can infer both online and offline videos.
Comprehensive experiments on off-the-shelf smartphones confirm the high
efficacy of SilhouetteTell for inferring video titles and clips under various
settings, including from a distance of up to 40 meters.

</details>


### [24] [Dual-level Progressive Hardness-Aware Reweighting for Cross-View Geo-Localization](https://arxiv.org/abs/2510.27181)
*Guozheng Zheng,Jian Guan,Mingjie Xie,Xuanjia Zhao,Congyi Fan,Shiheng Zhang,Pengming Feng*

Main category: cs.CV

TL;DR: 本文提出了一种用于无人机和卫星图像跨视角地理定位的双重渐进硬样本重加权（DPHR）策略，显著提升了现有方法在公开数据集上的表现和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机和卫星图像地理定位任务（CVGL）面临严重的视角差异和难负样本（外观相似但地理不符）问题。现有困难样本挖掘或重加权方法依赖静态权重，容易对分布变化敏感，且会过早关注难样本，导致梯度噪声和训练不稳定。本文旨在缓解这些问题，提高CVGL任务性能。

Method: 提出DPHR策略：
1) 样本级别上的比例难度感知（RDA）模块，评估每个负样本的相对困难度并赋予细粒度权重；
2) 批次级别上的渐进自适应损失加权（PALW）机制，根据训练进程动态调整对难样本的挖掘强度，前期弱化噪声干扰，后期加强难负样本挖掘。

Result: 在University-1652和SUES-200这两个公开基准上进行实验，DPHR方法相较于现有主流方法均有稳定且显著的性能提升，并表现出对分布变化的强鲁棒性。

Conclusion: DPHR是一种有效且鲁棒的新颖跨视角地理定位困难样本处理方法，可显著提升无人机与卫星图像的跨域检索能力。

Abstract: Cross-view geo-localization (CVGL) between drone and satellite imagery
remains challenging due to severe viewpoint gaps and the presence of hard
negatives, which are visually similar but geographically mismatched samples.
Existing mining or reweighting strategies often use static weighting, which is
sensitive to distribution shifts and prone to overemphasizing difficult samples
too early, leading to noisy gradients and unstable convergence. In this paper,
we present a Dual-level Progressive Hardness-aware Reweighting (DPHR) strategy.
At the sample level, a Ratio-based Difficulty-Aware (RDA) module evaluates
relative difficulty and assigns fine-grained weights to negatives. At the batch
level, a Progressive Adaptive Loss Weighting (PALW) mechanism exploits a
training-progress signal to attenuate noisy gradients during early optimization
and progressively enhance hard-negative mining as training matures. Experiments
on the University-1652 and SUES-200 benchmarks demonstrate the effectiveness
and robustness of the proposed DPHR, achieving consistent improvements over
state-of-the-art methods.

</details>


### [25] [Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications](https://arxiv.org/abs/2510.27186)
*Zixuan Hu,Yongxian Wei,Li Shen,Zhenyi Wang,Lei Li,Chun Yuan,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出一种稀疏模型反演方法，显著加速高分辨率图像的反演过程，同时保持甚至提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 模型反演可用于重建训练集数据，尤其是在原始数据不可用时。但现有方法在处理大模型、高分辨率图像时效率低下，主要原因包括对无用背景和虚假相关的区域进行不必要的反演。

Method: 作者提出了一种稀疏反演策略，仅反演语义前景，跳过噪声背景和虚假相关区域。此方法为可插拔式设计，无需修改原有损失函数，可与现有致密反演方法结合使用。

Result: 实验和理论分析显示，该方法在加速反演（最高3.79倍提升）的同时，在数据无关的模型量化和知识迁移任务中保持甚至提升了性能。

Conclusion: 稀疏模型反演策略能有效提升反演速度，并支持多种下游无数据应用，具有广泛的实用前景。

Abstract: Model inversion, which aims to reconstruct the original training data from
pre-trained discriminative models, is especially useful when the original
training data is unavailable due to privacy, usage rights, or size constraints.
However, existing dense inversion methods attempt to reconstruct the entire
image area, making them extremely inefficient when inverting high-resolution
images from large-scale Vision Transformers (ViTs). We further identify two
underlying causes of this inefficiency: the redundant inversion of noisy
backgrounds and the unintended inversion of spurious correlations--a phenomenon
we term "hallucination" in model inversion. To address these limitations, we
propose a novel sparse model inversion strategy, as a plug-and-play extension
to speed up existing dense inversion methods with no need for modifying their
original loss functions. Specifically, we selectively invert semantic
foregrounds while stopping the inversion of noisy backgrounds and potential
spurious correlations. Through both theoretical and empirical studies, we
validate the efficacy of our approach in achieving significant inversion
acceleration (up to 3.79 faster) while maintaining comparable or even enhanced
downstream performance in data-free model quantization and data-free knowledge
transfer. Code is available at https://github.com/Egg-Hu/SMI.

</details>


### [26] [Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](https://arxiv.org/abs/2510.27195)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Yoichi Sato*

Main category: cs.CV

TL;DR: 本论文提出了多模态互动真实性评估（MIVA）任务，并基于狼人杀游戏构建了多模态数据集，用于评测AI自动检测对话中真假信息的能力。结果显示，当前最先进的多模态大模型在该任务上表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 人工智能要更好地融入人类社会，就必须具备社会智能，尤其是判断真实性（识别谎言与真实）。自动识别多方对话中的欺骗行为一直是难题，目前缺乏相关评测与数据。

Method: 作者提出MIVA新任务，并构建含有同步视频和文本、每句话均标注真假的多模态数据集（取材自狼人杀社交游戏），同时系统性评估了多种主流多模态大语言模型（如GPT-4o）的表现。

Result: 实验发现，即便是当前最顶尖的多模态大模型，也很难可靠地区分对话中的真实与谎言，且在将语言内容与视觉社交线索结合时存在明显短板。

Conclusion: 当前多模态大模型在对复杂社交情境下的真实性判断上能力有限，未来需重点攻关视觉语义结合、社交信号感知等新方法，提升AI的社会感知与可信度。

Abstract: As AI systems become increasingly integrated into human lives, endowing them
with robust social intelligence has emerged as a critical frontier. A key
aspect of this intelligence is discerning truth from deception, a ubiquitous
element of human interaction that is conveyed through a complex interplay of
verbal language and non-verbal visual cues. However, automatic deception
detection in dynamic, multi-party conversations remains a significant
challenge. The recent rise of powerful Multimodal Large Language Models
(MLLMs), with their impressive abilities in visual and textual understanding,
makes them natural candidates for this task. Consequently, their capabilities
in this crucial domain are mostly unquantified. To address this gap, we
introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
present a novel multimodal dataset derived from the social deduction game
Werewolf. This dataset provides synchronized video, text, with verifiable
ground-truth labels for every statement. We establish a comprehensive benchmark
evaluating state-of-the-art MLLMs, revealing a significant performance gap:
even powerful models like GPT-4o struggle to distinguish truth from falsehood
reliably. Our analysis of failure modes indicates that these models fail to
ground language in visual social cues effectively and may be overly
conservative in their alignment, highlighting the urgent need for novel
approaches to building more perceptive and trustworthy AI systems.

</details>


### [27] [Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks](https://arxiv.org/abs/2510.27208)
*Jiaxin Zhang,Zehong Zhu,Junye Deng,Yunqin Li,and Bowen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合多源数据的分层图神经网络（HGNN）模型，用于深入分析乡村空间形态，并在多模态融合和分类任务中显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 随着城市化推进，乡村空间特征逐渐消失、景观同质化问题突出。现有研究多采用单一学科并依赖定性分析，受限于数字基础设施和数据获取不足，难以实现高效、深入的空间形态分析。

Method: 提出基于多源数据的分层图神经网络（HGNN）模型。模型设计了输入节点与通信节点、静态输入边与动态通信边两类结构，通过组合图卷积网络（GCN）和图注意力网络（GAT），在两阶段特征更新机制下集成多模态特征。引入关系池化机制，并针对17个亚型实施联合训练策略。

Result: 实验结果显示，在多模态融合和分类任务中，该方法较现有方法有显著性能提升。联合亚型优化后模型的平均准确率/ F1由独立模型的0.71/0.83提高到0.82/0.90，地块任务提升6%。

Conclusion: 本方法为乡村空间格局与生成逻辑的科学探索提供了有效工具与实证依据。

Abstract: Villages areas hold significant importance in the study of human-land
relationships. However, with the advancement of urbanization, the gradual
disappearance of spatial characteristics and the homogenization of landscapes
have emerged as prominent issues. Existing studies primarily adopt a
single-disciplinary perspective to analyze villages spatial morphology and its
influencing factors, relying heavily on qualitative analysis methods. These
efforts are often constrained by the lack of digital infrastructure and
insufficient data. To address the current research limitations, this paper
proposes a Hierarchical Graph Neural Network (HGNN) model that integrates
multi-source data to conduct an in-depth analysis of villages spatial
morphology. The framework includes two types of nodes-input nodes and
communication nodes-and two types of edges-static input edges and dynamic
communication edges. By combining Graph Convolutional Networks (GCN) and Graph
Attention Networks (GAT), the proposed model efficiently integrates multimodal
features under a two-stage feature update mechanism. Additionally, based on
existing principles for classifying villages spatial morphology, the paper
introduces a relational pooling mechanism and implements a joint training
strategy across 17 subtypes. Experimental results demonstrate that this method
achieves significant performance improvements over existing approaches in
multimodal fusion and classification tasks. Additionally, the proposed joint
optimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83
(independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Our
method provides scientific evidence for exploring villages spatial patterns and
generative logic.

</details>


### [28] [Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model](https://arxiv.org/abs/2510.27607)
*John Won,Kyungmin Lee,Huiwon Jang,Dongyoung Kim,Jinwoo Shin*

Main category: cs.CV

TL;DR: 本文提出了一种名为DUST的双流扩散世界模型框架，以提升视觉-语言-动作（VLA）模型在机器人任务中的表现。DUST通过解耦模态冲突并支持跨模态知识共享，在多项机器人任务中优于现有方法，在仿真和现实场景下均显著提高机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人策略学习中已表现出潜力，但由于视觉观测和动作序列属于不同模态，联合预测它们充满挑战，现有方法难以有效处理两者间的模态差异，限制了机器人自主感知和决策能力的提升。

Method: DUST采用多模态扩散Transformer架构，分离视觉和动作两条模态流，同时实现跨模态知识分享。模型引入独立的噪声扰动与解耦的流匹配损失函数，实现模态间双向联合学习，无需统一潜在空间。此外，训练时模态解耦带来了一种支持异步演化的联合采样方法，使测试时视觉与动作可按不同速率演化，提高了灵活性与性能。

Result: 在仿真环境（RoboCasa、GR-1）中，DUST相对基线方法最多提升6%，联合采样策略再带来2-5%增益。在现实场景（Franka Research 3机器人）中，任务成功率提升13%。预训练阶段引入无动作视频（BridgeV2）也大幅提升了迁移学习效果。

Conclusion: DUST有效解决了VLA模型中的模态冲突问题，提升了策略模型的泛化能力和任务表现，其异步采样方法和大规模无动作视频预训练为未来VLA大模型的研究提供了新思路和证据。

Abstract: Recently, augmenting Vision-Language-Action models (VLAs) with world modeling
has shown promise in improving robotic policy learning. However, it remains
challenging to jointly predict next-state observations and action sequences
because of the inherent difference between the two modalities. To address this,
we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework
that handles the modality conflict and enhances the performance of VLAs across
diverse tasks. Specifically, we propose a multimodal diffusion transformer
architecture that explicitly maintains separate modality streams while still
enabling cross-modal knowledge sharing. In addition, we introduce independent
noise perturbations for each modality and a decoupled flow-matching loss. This
design enables the model to learn the joint distribution in a bidirectional
manner while avoiding the need for a unified latent space. Based on the
decoupling of modalities during training, we also introduce a joint sampling
method that supports test-time scaling, where action and vision tokens evolve
asynchronously at different rates. Through experiments on simulated benchmarks
such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,
while our test-time scaling approach provides an additional 2-5% boost. On
real-world tasks with the Franka Research 3, DUST improves success rates by
13%, confirming its effectiveness beyond simulation. Furthermore, pre-training
on action-free videos from BridgeV2 yields significant transfer gains on
RoboCasa, underscoring DUST's potential for large-scale VLA pretraining.

</details>


### [29] [Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness](https://arxiv.org/abs/2510.27213)
*Ren Tasai,Guang Li,Ren Togo,Takahiro Ogawa,Kenji Hirata,Minghui Tang,Takaaki Yoshimura,Hiroyuki Sugimori,Noriko Nishioka,Yukie Shimizu,Kohsuke Kudo,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种新的持续自监督学习（CSSL）框架，能在保护隐私的同时，有效从多视窗胸部CT图像中学习多样特征，并在多领域转换中表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗影像诊断中，数据标注稀缺和领域偏移（如不同窗设置）制约了模型泛化能力，而传统CSSL方法复用历史数据又存在隐私安全风险。

Method: 该方法通过在无标签影像上的持续预训练，结合隐空间重放机制缓解领域偏移下的灾难性遗忘问题，并融合Wasserstein距离蒸馏和批知识集成，提升对领域不变特征的捕获能力。

Result: 在多种窗设置下的胸部CT数据上验证，所提方法在特征提取与下游任务中均优于现有方法，具备更高鲁棒性和泛化性。

Conclusion: 本文提出的CSSL框架可在保障隐私前提下，有效缓解领域偏移带来的负面影响，为医疗图像的持续学习和泛化应用提供了新方案。

Abstract: We propose a novel continual self-supervised learning (CSSL) framework for
simultaneously learning diverse features from multi-window-obtained chest
computed tomography (CT) images and ensuring data privacy. Achieving a robust
and highly generalizable model in medical image diagnosis is challenging,
mainly because of issues, such as the scarcity of large-scale, accurately
annotated datasets and domain shifts inherent to dynamic healthcare
environments. Specifically, in chest CT, these domain shifts often arise from
differences in window settings, which are optimized for distinct clinical
purposes. Previous CSSL frameworks often mitigated domain shift by reusing past
data, a typically impractical approach owing to privacy constraints. Our
approach addresses these challenges by effectively capturing the relationship
between previously learned knowledge and new information across different
training stages through continual pretraining on unlabeled images.
Specifically, by incorporating a latent replay-based mechanism into CSSL, our
method mitigates catastrophic forgetting due to domain shifts during continual
pretraining while ensuring data privacy. Additionally, we introduce a feature
distillation technique that integrates Wasserstein distance-based knowledge
distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of
the model to learn meaningful, domain-shift-robust representations. Finally, we
validate our approach using chest CT images obtained across two different
window settings, demonstrating superior performance compared with other
approaches.

</details>


### [30] [SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping](https://arxiv.org/abs/2510.27219)
*Renjie Ji,Xue Wang,Chao Niu,Wen Zhang,Yong Mei,Kun Tan*

Main category: cs.CV

TL;DR: 该论文提出了一个新的高光谱图像基础模型SpecAware，能够统一多传感器的数据学习，提升地表覆盖分类等任务的表现，并配套发布了大规模高质量基准数据集Hyper-400K。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感图像(HSI)数据高度异质性，使得多传感器联合训练难以实现。目前基础模型往往忽视了传感器元属性对任务的重要指导作用，导致模型泛化能力与迁移性受限。

Method: 1. 提出SpecAware：一种面向高光谱图像的基于超网络的联合多传感器学习基础模型；2. 第一步为元内容感知模块，将传感器元属性与每个图像内容融合，为每个样本每个波段生成有针对性的输入；3. 第二步为HyperEmbedding模块，通过样本条件超网络动态生成矩阵分解对，实现自适应空间模式提取和潜在语义特征重投影；4. 构建新数据集Hyper-400K，包含来自多种AVIRIS传感器的40万以上图像片段。

Result: 在六个数据集上大量实验表明，SpecAware在地物语义分割、变化检测、场景分类等多种HSI下游任务中表现优越，能够学习到更具代表性的特征表达。

Conclusion: SpecAware有效提升了多传感器高光谱图像的统一建模与特征表征能力，为相关LULC任务提供了更好的基础，进一步推动了遥感影像智能分析的发展。

Abstract: Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use and
land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data has
long posed a major barrier to developing generalized models via joint training.
Although HSI foundation models have shown promise for different downstream
tasks, the existing approaches typically overlook the critical guiding role of
sensor meta-attributes, and struggle with multi-sensor training, limiting their
transferability. To address these challenges, we propose SpecAware, which is a
novel hyperspectral spectral-content aware foundation model for unifying
multi-sensor learning for HSI mapping. We also constructed the Hyper-400K
dataset to facilitate this research, which is a new large-scale, high-quality
benchmark dataset with over 400k image patches from diverse airborne AVIRIS
sensors. The core of SpecAware is a two-step hypernetwork-driven encoding
process for HSI data. Firstly, we designed a meta-content aware module to
generate a unique conditional input for each HSI patch, tailored to each
spectral band of every sample by fusing the sensor meta-attributes and its own
image content. Secondly, we designed the HyperEmbedding module, where a
sample-conditioned hypernetwork dynamically generates a pair of matrix factors
for channel-wise encoding, consisting of adaptive spatial pattern extraction
and latent semantic feature re-projection. Thus, SpecAware gains the ability to
perceive and interpret spatial-spectral features across diverse scenes and
sensors. This, in turn, allows SpecAware to adaptively process a variable
number of spectral channels, establishing a unified framework for joint
pre-training. Extensive experiments on six datasets demonstrate that SpecAware
can learn superior feature representations, excelling in land-cover semantic
segmentation classification, change detection, and scene classification.

</details>


### [31] [Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery](https://arxiv.org/abs/2510.27224)
*Mahmoud El Hussieni,Bahadır K. Güntürk,Hasan F. Ateş,Oğuz Hanoğlu*

Main category: cs.CV

TL;DR: 本文分析了YOLOv11在卫星影像中联合建筑实例分割与离散高度分类的能力，显示其在精度和速度上均优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 精准的建筑实例分割和高度分类对于城市规划、三维建模和基础设施监测极为重要，现有方法在处理复杂场景时存在一定局限性。

Method: 提出YOLOv11改进版，采用更高效的结构以增强多尺度特征融合和目标定位能力，并在包含12个城市12.5万余建筑的DFC2023数据集上进行评估。

Result: YOLOv11在mAP@50上达到60.4%，mAP@50–95为38.3%，同时在五类高度分类任务上保持较高的准确率，对遮挡、复杂建筑形态及类不均衡表现优异。

Conclusion: YOLOv11不仅提升了检测和分类性能，还大大加快了推理速度，非常适合大规模实时城市遥感制图。该方法为语义城市重建和未来地理空间智能应用提供了新思路。

Abstract: Accurate building instance segmentation and height classification are
critical for urban planning, 3D city modeling, and infrastructure monitoring.
This paper presents a detailed analysis of YOLOv11, the recent advancement in
the YOLO series of deep learning models, focusing on its application to joint
building extraction and discrete height classification from satellite imagery.
YOLOv11 builds on the strengths of earlier YOLO models by introducing a more
efficient architecture that better combines features at different scales,
improves object localization accuracy, and enhances performance in complex
urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000
annotated buildings across 12 cities -- we evaluate YOLOv11's performance using
metrics such as precision, recall, F1 score, and mean average precision (mAP).
Our findings demonstrate that YOLOv11 achieves strong instance segmentation
performance with 60.4\% mAP@50 and 38.3\% mAP@50--95 while maintaining robust
classification accuracy across five predefined height tiers. The model excels
in handling occlusions, complex building shapes, and class imbalance,
particularly for rare high-rise structures. Comparative analysis confirms that
YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and
inference speed, making it well-suited for real-time, large-scale urban
mapping. This research highlights YOLOv11's potential to advance semantic urban
reconstruction through streamlined categorical height modeling, offering
actionable insights for future developments in remote sensing and geospatial
intelligence.

</details>


### [32] [MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts](https://arxiv.org/abs/2510.27234)
*Jingnan Gao,Zhe Wang,Xianze Fang,Xingyu Ren,Zhuo Chen,Shengqi Liu,Yuhao Cheng,Jiangjing Lyu,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: MoRE是一种基于专家混合（MoE）的三维视觉基础模型，具有动态路由和高适应性的特点，能够在多个三维重建任务中取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前三维视觉重建领域大模型的扩展受限于几何监督的复杂性和三维数据的多样性。因此需要新的架构以实现模型容量的提升和多任务适应能力。

Method: 提出MoRE模型，基于混合专家架构，根据任务动态选择特征路由，增强了可扩展性和适配性。模型包含基于置信度的深度精细模块以提升稳健性，同时融合密集语义与全局三维特征用于高保真表面法线预测，并针对多任务设计了优化的损失函数。

Result: 在多个公开基准上，MoRE模型取得了最新最优成绩，并且能够在不增加额外计算量情况下，良好地支持下游应用。

Conclusion: MoRE有效解决了三维视觉大模型扩展的难题，在多任务场景下表现出优异的鲁棒性和适应性，具有重要应用价值。

Abstract: Recent advances in language and vision have demonstrated that scaling up
model capacity consistently improves performance across diverse tasks. In 3D
visual geometry reconstruction, large-scale training has likewise proven
effective for learning versatile representations. However, further scaling of
3D models is challenging due to the complexity of geometric supervision and the
diversity of 3D data. To overcome these limitations, we propose MoRE, a dense
3D visual foundation model based on a Mixture-of-Experts (MoE) architecture
that dynamically routes features to task-specific experts, allowing them to
specialize in complementary data aspects and enhance both scalability and
adaptability. Aiming to improve robustness under real-world conditions, MoRE
incorporates a confidence-based depth refinement module that stabilizes and
refines geometric estimation. In addition, it integrates dense semantic
features with globally aligned 3D backbone representations for high-fidelity
surface normal prediction. MoRE is further optimized with tailored loss
functions to ensure robust learning across diverse inputs and multiple
geometric tasks. Extensive experiments demonstrate that MoRE achieves
state-of-the-art performance across multiple benchmarks and supports effective
downstream applications without extra computation.

</details>


### [33] [Object-IR: Leveraging Object Consistency and Mesh Deformation for Self-Supervised Image Retargeting](https://arxiv.org/abs/2510.27236)
*Tianli Liao,Ran Wang,Siqing Zhang,Lei Li,Guangen Liu,Chenyang Zhao,Heling Cao,Peng Li*

Main category: cs.CV

TL;DR: Object-IR是一种无监督的图像重定向方法，通过结合对象外观一致性与几何保护约束，显著减少了重要区域的几何失真，取得了目前最优效果。


<details>
  <summary>Details</summary>
Motivation: 图像重定向过程中，保证语义重要区域不发生几何失真一直是难题。以往方法往往无法兼顾对象外观完整性与几何比例。作者希望提出一种无需人工标注的数据驱动方法，自动保持关键对象的视觉一致性和几何结构。

Method: 作者提出了Object-IR，基于自监督机制，将图像重定向建模为网格变形优化问题。具体做法是：1）利用卷积神经网络预测网格节点的运动，依据输入图像和目标分辨率生成扭曲网格；2）设计包括对象一致性损失、几何保持损失和边界损失的综合损失函数，用以保证语义对象外观及结构的一致、输出框规整；3）方法无需人工标注，直接利用输入的几何与语义信息作为监督信号。

Result: 在RetargetMe基准测试上，Object-IR在多项定量指标与主观视觉质量评测中均优于现有方法，并且具备高效推理速度（1024x683分辨率下平均0.009s），实现在消费级GPU上的实时处理。

Conclusion: Object-IR打破了以往手工数据集依赖，兼顾了高精度与高效率，为图像重定向领域提供了无监督、实用、表现优异的新方案。

Abstract: Eliminating geometric distortion in semantically important regions remains an
intractable challenge in image retargeting. This paper presents Object-IR, a
self-supervised architecture that reformulates image retargeting as a
learning-based mesh warping optimization problem, where the mesh deformation is
guided by object appearance consistency and geometric-preserving constraints.
Given an input image and a target aspect ratio, we initialize a uniform rigid
mesh at the output resolution and use a convolutional neural network to predict
the motion of each mesh grid and obtain the deformed mesh. The retargeted
result is generated by warping the input image according to the rigid mesh in
the input image and the deformed mesh in the output resolution. To mitigate
geometric distortion, we design a comprehensive objective function
incorporating a) object-consistent loss to ensure that the important semantic
objects retain their appearance, b) geometric-preserving loss to constrain
simple scale transform of the important meshes, and c) boundary loss to enforce
a clean rectangular output. Notably, our self-supervised paradigm eliminates
the need for manually annotated retargeting datasets by deriving supervision
directly from the input's geometric and semantic properties. Extensive
evaluations on the RetargetMe benchmark demonstrate that our Object-IR achieves
state-of-the-art performance, outperforming existing methods in quantitative
metrics and subjective visual quality assessments. The framework efficiently
processes arbitrary input resolutions (average inference time: 0.009s for
1024x683 resolution) while maintaining real-time performance on consumer-grade
GPUs. The source code will soon be available at
https://github.com/tlliao/Object-IR.

</details>


### [34] [Fusion of Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis](https://arxiv.org/abs/2510.27237)
*Zhidong Yang,Xiuhui Shi,Wei Ba,Zhigang Song,Haijing Luan,Taiyuan Hu,Senlin Lin,Jiguang Wang,Shaohua Kevin Zhou,Rui Yan*

Main category: cs.CV

TL;DR: 本文提出了FuseCPath框架，通过融合多种异构的病理基础模型（FMs），提升了全视野切片图像分析中的性能，并在多个癌症数据集上取得了新一代表现。


<details>
  <summary>Details</summary>
Motivation: 当前病理基础模型由于训练数据和网络架构不同，导致下游任务性能存在较大不稳定性。充分利用多模型的优势成为关键痛点。

Method: FuseCPath框架包括：1）多视角聚类筛选辨别性病理切片；2）通过聚类级重嵌入捕捉局部特征实现异构patch级特征融合；3）通过协作蒸馏实现slide级特征融合。

Result: 在TCGA肺癌、膀胱癌和结直肠癌公共数据集上，FuseCPath在多项下游任务中均取得了领先的实验结果。

Conclusion: FuseCPath有效整合多种异构病理基础模型，在全切片图像分析中显著提升了性能，对实际病理AI应用具有推广价值。

Abstract: Whole slide image (WSI) analysis has emerged as an increasingly essential
technique in computational pathology. Recent advances in the pathological
foundation models (FMs) have demonstrated significant advantages in deriving
meaningful patch-level or slide-level feature representations from WSIs.
However, current pathological FMs have exhibited substantial heterogeneity
caused by diverse private training datasets and different network
architectures. This heterogeneity introduces performance variability when we
utilize the extracted features from different FMs in the downstream tasks. To
fully explore the advantage of multiple FMs effectively, in this work, we
propose a novel framework for the fusion of heterogeneous pathological FMs,
called FuseCPath, yielding a model with a superior ensemble performance. The
main contributions of our framework can be summarized as follows: (i) To
guarantee the representativeness of the training patches, we propose a
multi-view clustering-based method to filter out the discriminative patches via
multiple FMs' embeddings. (ii) To effectively fuse the heterogeneous
patch-level FMs, we devise a cluster-level re-embedding strategy to online
capture patch-level local features. (iii) To effectively fuse the heterogeneous
slide-level FMs, we devise a collaborative distillation strategy to explore the
connections between slide-level FMs. Extensive experiments conducted on lung
cancer, bladder cancer, and colorectal cancer datasets from The Cancer Genome
Atlas (TCGA) have demonstrated that the proposed FuseCPath achieves
state-of-the-art performance across multiple tasks on these public datasets.

</details>


### [35] [Trans-defense: Transformer-based Denoiser for Adversarial Defense with Spatial-Frequency Domain Representation](https://arxiv.org/abs/2510.27245)
*Alik Pramanick,Mayank Bansal,Utkarsh Srivastava,Suklav Ghosh,Arijit Sur*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间域和频率域的新型去噪策略，通过两阶段训练方法显著提升深度神经网络在对抗扰动攻击下的防御效果。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络（DNNs）在许多任务中表现卓越，但它们易受对抗性攻击，严重限制了在安全关键领域的应用，因此迫切需要提升其鲁棒性的方法。

Method: 论文采用两阶段训练策略：首先训练结合空间特征和离散小波变换（DWT）频率特征的去噪网络（通过transformer层整合），然后用去噪后的图像对分类器进行再训练。

Result: 在MNIST、CIFAR-10和Fashion-MNIST等多个数据集上，提出的方法显著提升了分类准确率，并超越了单独使用去噪网络或对抗训练的性能。

Conclusion: 该方法通过整合空间和频率域去噪，有效提升了分类器对对抗攻击的鲁棒性，为安全关键应用的DNN提供了更可靠的防御方案。

Abstract: In recent times, deep neural networks (DNNs) have been successfully adopted
for various applications. Despite their notable achievements, it has become
evident that DNNs are vulnerable to sophisticated adversarial attacks,
restricting their applications in security-critical systems. In this paper, we
present two-phase training methods to tackle the attack: first, training the
denoising network, and second, the deep classifier model. We propose a novel
denoising strategy that integrates both spatial and frequency domain approaches
to defend against adversarial attacks on images. Our analysis reveals that
high-frequency components of attacked images are more severely corrupted
compared to their lower-frequency counterparts. To address this, we leverage
Discrete Wavelet Transform (DWT) for frequency analysis and develop a denoising
network that combines spatial image features with wavelets through a
transformer layer. Next, we retrain the classifier using the denoised images,
which enhances the classifier's robustness against adversarial attacks.
Experimental results across the MNIST, CIFAR-10, and Fashion-MNIST datasets
reveal that the proposed method remarkably elevates classification accuracy,
substantially exceeding the performance by utilizing a denoising network and
adversarial training approaches. The code is available at
https://github.com/Mayank94/Trans-Defense.

</details>


### [36] [C-LEAD: Contrastive Learning for Enhanced Adversarial Defense](https://arxiv.org/abs/2510.27249)
*Suklav Ghosh,Sonal Kumar,Arijit Sur*

Main category: cs.CV

TL;DR: 本文提出使用对比学习提升深度神经网络对抗攻击鲁棒性的方法，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在视觉任务中表现优异，但容易受到对抗攻击，导致预测错误。因此，提升模型应对对抗扰动的能力，对于实际部署非常重要。

Method: 创新性地采用对比学习，将对比损失函数应用于模型训练，混合使用原始图片和经过对抗扰动的图片，通过同时优化模型参数和扰动项，让网络学习到更鲁棒的特征表示。

Result: 实验表明，采用对比损失训练的模型在抵抗多种对抗扰动时表现显著更鲁棒。

Conclusion: 对比损失有助于提取更有信息量和韧性的特征，对抗鲁棒性提升显著，为深度学习防御对抗攻击提供了新的方向。

Abstract: Deep neural networks (DNNs) have achieved remarkable success in computer
vision tasks such as image classification, segmentation, and object detection.
However, they are vulnerable to adversarial attacks, which can cause incorrect
predictions with small perturbations in input images. Addressing this issue is
crucial for deploying robust deep-learning systems. This paper presents a novel
approach that utilizes contrastive learning for adversarial defense, a
previously unexplored area. Our method leverages the contrastive loss function
to enhance the robustness of classification models by training them with both
clean and adversarially perturbed images. By optimizing the model's parameters
alongside the perturbations, our approach enables the network to learn robust
representations that are less susceptible to adversarial attacks. Experimental
results show significant improvements in the model's robustness against various
types of adversarial perturbations. This suggests that contrastive loss helps
extract more informative and resilient features, contributing to the field of
adversarial robustness in deep learning.

</details>


### [37] [Enhancing Spatio-Temporal Zero-shot Action Recognition with Language-driven Description Attributes](https://arxiv.org/abs/2510.27255)
*Yehna Kim andYoung-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过利用网络爬取的描述和大语言模型抽取关键词，提升了视觉-语言模型在零样本动作识别中的表现。实验结果显示，该方法在多个基准数据集上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在零样本动作识别时，依赖于动作类别的语义信息，但遇到多义词时，容易造成语义歧义，影响动作理解准确性。需要一种更丰富、更精准的动作语义获取方法。

Method: 作者提出用网络自动爬取的自然语言描述，并通过大语言模型抽取关键属性词，取代传统依靠专家标注属性的方式，降低了人工成本。为进一步提升视频与描述属性的对应关系，还设计了时空交互模块，对关键对象及动作单元进行关注，实现更好的视频与属性对齐。

Result: 新方法在三个零样本动作识别基准（UCF-101、HMDB-51、Kinetics-600）上取得了81.0%、53.1%、68.9%的准确率，优于或可比其他主流方法，验证了其有效性和鲁棒性。

Conclusion: 通过自动化获取和处理丰富的动作描述语义，配合有效的视频-文本对齐模块，显著提升了零样本动作识别能力，减少了人为参与，具备较强的推广前景。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
zero-shot action recognition by learning to associate video embeddings with
class embeddings. However, a significant challenge arises when relying solely
on action classes to provide semantic context, particularly due to the presence
of multi-semantic words, which can introduce ambiguity in understanding the
intended concepts of actions. To address this issue, we propose an innovative
approach that harnesses web-crawled descriptions, leveraging a large-language
model to extract relevant keywords. This method reduces the need for human
annotators and eliminates the laborious manual process of attribute data
creation. Additionally, we introduce a spatio-temporal interaction module
designed to focus on objects and action units, facilitating alignment between
description attributes and video content. In our zero-shot experiments, our
model achieves impressive results, attaining accuracies of 81.0%, 53.1%, and
68.9% on UCF-101, HMDB-51, and Kinetics-600, respectively, underscoring the
model's adaptability and effectiveness across various downstream tasks.

</details>


### [38] [RegionRAG: Region-level Retrieval-Augumented Generation for Visually-Rich Documents](https://arxiv.org/abs/2510.27261)
*Yinglu Li,Zhiying Lu,Zhihang Liu,Chuanbin Liu,Hongtao Xie*

Main category: cs.CV

TL;DR: 本文提出了RegionRAG框架，将多模态RAG检索从文档级粒度提升到区域级，提高了LLM在处理视觉文档时的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态RAG方法以整个文档作为检索单元，但文档中常包含大量与查询无关的冗余内容，干扰模型注意力，降低性能。

Method: RegionRAG框架在训练时采用有标签与无标签混合监督，定位与问题相关的视觉区域；在推理阶段，动态地将显著区域组合成完整语义块，只把相关区域信息传递给生成模型。

Result: 在六个基准上，RegionRAG平均提升R@1检索准确率10.02%，提升问答准确率3.56%，同时视觉token使用量仅为先前方法的71.42%。

Conclusion: 基于区域级粒度检索能有效提高多模态RAG的检索效率和准确率，为LLM多模态推理带来新思路。

Abstract: Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method
for empowering LLMs by leveraging candidate visual documents. However, current
methods consider the entire document as the basic retrieval unit, introducing
substantial irrelevant visual content in two ways: 1) Relevant documents often
contain large regions unrelated to the query, diluting the focus on salient
information; 2) Retrieving multiple documents to increase recall further
introduces redundant and irrelevant documents. These redundant contexts
distract the model's attention and further degrade the performance. To address
this challenge, we propose \modelname, a novel framework that shifts the
retrieval paradigm from the document level to the region level. During
training, we design a hybrid supervision strategy from both labeled data and
unlabeled data to pinpoint relevant patches. During inference, we propose a
dynamic pipeline that intelligently groups salient patches into complete
semantic regions. By delegating the task of identifying relevant regions to the
retriever, \modelname enables the generator to focus solely on concise visual
content relevant to queries, improving both efficiency and accuracy.
Experiments on six benchmarks demonstrate that RegionRAG achieves
state-of-the-art performance. Improves retrieval accuracy by 10.02\% in R@1 on
average and increases question answering accuracy by 3.56\% while using only
71.42\% visual tokens compared to prior methods. The code will be available at
https://github.com/Aeryn666/RegionRAG.

</details>


### [39] [T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis](https://arxiv.org/abs/2510.27265)
*Raza Imam,Hu Wang,Dwarikanath Mahapatra,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学影像视觉-语言模型的自适应模型融合方法T^3，能够在保持泛化能力的同时提升特定模态下的表现，并兼顾效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域的视觉-语言模型面临预训练网络泛化能力强但不精细、专家模型专一但容易失效于模态转变的问题。现有的模型融合方法多针对自然图像，无法很好适应多样复杂的医学场景。因此需要一种能够动态适应不同任务和数据分布变化的融合方法。

Method: 提出T^3方法，在无需反向传播的情况下，依据Jensen-Shannon散度为每个样本计算动态的模型融合系数，实现精确与泛化能力的平衡。为降低推理开销，还提出了T^3_B方法，按批次计算融合系数。为全面评估，作者还建立了包含四种模态的标准化医学模型融合基准和评测协议。

Result: T^3和T^3_B在Top-1准确率和错误率下降方面均取得了新的SOTA效果，优于各类强基线方法，并在效率上保持了优势。通过标准化基准的实验证明了其广泛适用性和可靠性。

Conclusion: 该方法为临床环境中视觉-语言模型的自适应部署提供了可行且高效的策略，能够在多模态医学图像任务中灵活应对数据分布变化，并推动了医学模型融合研究的发展。

Abstract: In medical imaging, vision-language models face a critical duality:
pretrained networks offer broad robustness but lack subtle, modality-specific
characteristics, while fine-tuned expert models achieve high in-distribution
accuracy yet falter under modality shift. Existing model-merging techniques,
designed for natural-image benchmarks, are simple and efficient but fail to
deliver consistent gains across diverse medical modalities; their static
interpolation limits reliability in varied clinical tasks. To address this, we
introduce Test-Time Task adaptive merging (T^3), a backpropagation-free
framework that computes per-sample interpolation coefficients via the
Jensen-Shannon divergence between the two models' output distributions. T^3
dynamically preserves local precision when models agree and defers to
generalist robustness under drift. To overcome the inference costs of
sample-wise merging, we further propose a batch-wise extension, T^3_B, that
computes a merging coefficient across a batch of samples, dramatically reducing
computational bottleneck. Recognizing the lack of a standardized
medical-merging benchmark, we present a rigorous cross-evaluation protocol
spanning in-domain, base-to-novel, and corruptions across four modalities.
Empirically, T^3 sets new state-of-the-art in Top-1 accuracy and error
reduction, outperforming strong baselines while maintaining efficiency, paving
the way for adaptive MVLM deployment in clinical settings. Our code is
available at https://github.com/Razaimam45/TCube.

</details>


### [40] [HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration](https://arxiv.org/abs/2510.27266)
*Shaojie Zhang,Pei Fu,Ruoceng Zhang,Jiahui Yang,Anan Du,Xiuwen Xi,Shaokang Wang,Ying Huang,Bin Qin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法（HyperClick），通过不确定性校准提升自动化GUI智能体的可靠性。方法能更好地识别自身能力边界，减少过度自信和不靠谱的预测。


<details>
  <summary>Details</summary>
Motivation: 当前自动化GUI智能体在执行任务时，需要将用户指令准确映射到屏幕坐标上。然而，无论是监督微调（SFT）还是强化微调（RFT）训练的模型，都缺乏对自身能力边界的自知之明，导致过度自信和预测不可靠。鉴于动态GUI任务对准确性的高要求，错误会导致任务失败，因此提升信心与实际准确性的一致性变得尤为重要。

Method: 作者提出HyperClick框架，创新性地引入二元奖励（正确/错误）与基于截断高斯分布的空间置信建模，并以Brier分数对置信度进行校准。该方法同时优化坐标预测的准确率与置信度的一致性，促进模型自省和自我批判能力。

Result: 在七个挑战性基准数据集上进行广泛实验，结果显示HyperClick不仅取得了最先进的性能，还能输出校准良好的置信度分数。

Conclusion: HyperClick不仅提升了GUI操作的可靠性，还减少了模型的过度自信。通过显式的置信度校准与自省机制，支持更加可靠的GUI自动化应用。

Abstract: Autonomous Graphical User Interface (GUI) agents rely on accurate GUI
grounding, which maps language instructions to on-screen coordinates, to
execute user commands. However, current models, whether trained via supervised
fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of
their capability boundaries, leading to overconfidence and unreliable
predictions. We first systematically evaluate probabilistic and verbalized
confidence in general and GUI-specific models, revealing a misalignment between
confidence and actual accuracy, which is particularly critical in dynamic GUI
automation tasks, where single errors can cause task failure. To address this,
we propose HyperClick, a novel framework that enhances reliable GUI grounding
through uncertainty calibration. HyperClick introduces a dual reward mechanism,
combining a binary reward for correct actions with a truncated Gaussian-based
spatial confidence modeling, calibrated using the Brier score. This approach
jointly optimizes grounding accuracy and confidence reliability, fostering
introspective self-criticism. Extensive experiments on seven challenge
benchmarks show that HyperClick achieves state-of-the-art performance while
providing well-calibrated confidence. By enabling explicit confidence
calibration and introspective self-criticism, HyperClick reduces overconfidence
and supports more reliable GUI automation.

</details>


### [41] [FOCUS: Efficient Keyframe Selection for Long Video Understanding](https://arxiv.org/abs/2510.27280)
*Zirui Zhu,Hailun Xu,Yang Luo,Yong Liu,Kanchan Sarkar,Zhenheng Yang,Yang You*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FOCUS的新颖关键帧选择方法，有效提升了多模态大模型对长视频的理解能力，并显著减少了需要处理的帧数。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在处理长视频时，由于需要将视频帧转化为视觉token，计算量极大，不得不采取均匀采样或用额外模型进行关键帧选择，但这些方法往往会漏掉关键信息或带来高昂计算开销。因此，需要一种高效且信息保真的关键帧选择机制。

Method: FOCUS是一种无须训练、与模型无关的关键帧选择模块，将关键帧选择建模为多臂赌博机中的组合式纯探索问题。它以短时间段为单位（arm），通过经验均值和Bernstein置信区间选出信息丰富的区域，并在这些区域内选择得分最高的帧，实现理论保证下的高效探索与利用。

Result: 在两个长视频问答基准上，FOCUS只需处理不到2%的视频帧便带来了显著的准确率提升。在20分钟以上视频的LongVideoBench数据集上，准确率提升达11.9%。

Conclusion: FOCUS为可扩展的长视频理解任务提供了简单通用且高效的关键帧选择解决方案，有效提升了MLLMs在长视频场景下的表现。

Abstract: Multimodal large language models (MLLMs) represent images and video frames as
visual tokens. Scaling from single images to hour-long videos, however,
inflates the token budget far beyond practical limits. Popular pipelines
therefore either uniformly subsample or apply keyframe selection with
retrieval-style scoring using smaller vision-language models. However, these
keyframe selection methods still rely on pre-filtering before selection to
reduce the inference cost and can miss the most informative moments.
  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a
training-free, model-agnostic keyframe selection module that selects
query-relevant frames under a strict token budget. FOCUS formulates keyframe
selection as a combinatorial pure-exploration (CPE) problem in multi-armed
bandits: it treats short temporal clips as arms, and uses empirical means and
Bernstein confidence radius to identify informative regions while preserving
exploration of uncertain areas. The resulting two-stage
exploration-exploitation procedure reduces from a sequential policy with
theoretical guarantees, first identifying high-value temporal regions, then
selecting top-scoring frames within each region On two long-video
question-answering benchmarks, FOCUS delivers substantial accuracy improvements
while processing less than 2% of video frames. For videos longer than 20
minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating
its effectiveness as a keyframe selection method and providing a simple and
general solution for scalable long-video understanding with MLLMs.

</details>


### [42] [Rethinking Robust Adversarial Concept Erasure in Diffusion Models](https://arxiv.org/abs/2510.27285)
*Qinghong Yin,Yu Tian,Yue Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的语义引导概念擦除方法S-GRACE，用于扩散模型中敏感内容的选择性消除，在兼顾模型泛化能力的同时大幅提升了擦除效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有通过对抗训练去除扩散模型中敏感概念的方法忽视了对抗样本在概念空间中的匹配性，导致擦除不全面且对其他概念造成干扰。本文探究了现有方法在概念空间内的不足，旨在提升目标敏感内容的擦除效果，并减少对非目标内容的影响。

Method: 提出S-GRACE方法，在生成对抗样本并进行模型训练时引入语义引导，使对抗样本更好地覆盖并匹配目标概念空间，从而提升擦除的专一性与有效性。实验在不同扩散模型与多种擦除场景下对比七种现有方法和三种对抗提示生成策略。

Result: 实验结果显示，S-GRACE比现有方法提升概念擦除性能26%，更好地保留非目标概念，同时训练时间减少了90%。

Conclusion: S-GRACE在敏感内容擦除任务中实现了高效且高保真的选择性遗忘，为扩散模型安全应用提供了有力技术支撑。

Abstract: Concept erasure aims to selectively unlearning undesirable content in
diffusion models (DMs) to reduce the risk of sensitive content generation. As a
novel paradigm in concept erasure, most existing methods employ adversarial
training to identify and suppress target concepts, thus reducing the likelihood
of sensitive outputs. However, these methods often neglect the specificity of
adversarial training in DMs, resulting in only partial mitigation. In this
work, we investigate and quantify this specificity from the perspective of
concept space, i.e., can adversarial samples truly fit the target concept
space? We observe that existing methods neglect the role of conceptual
semantics when generating adversarial samples, resulting in ineffective fitting
of concept spaces. This oversight leads to the following issues: 1) when there
are few adversarial samples, they fail to comprehensively cover the object
concept; 2) conversely, they will disrupt other target concept spaces.
Motivated by the analysis of these findings, we introduce S-GRACE
(Semantics-Guided Robust Adversarial Concept Erasure), which grace leveraging
semantic guidance within the concept space to generate adversarial samples and
perform erasure training. Experiments conducted with seven state-of-the-art
methods and three adversarial prompt generation strategies across various DM
unlearning scenarios demonstrate that S-GRACE significantly improves erasure
performance 26%, better preserves non-target concepts, and reduces training
time by 90%. Our code is available at https://github.com/Qhong-522/S-GRACE.

</details>


### [43] [Versatile and Efficient Medical Image Super-Resolution Via Frequency-Gated Mamba](https://arxiv.org/abs/2510.27296)
*Wenfeng Huang,Xiangyun Liao,Wei Cao,Wenjing Jia,Weixin Si*

Main category: cs.CV

TL;DR: 该论文提出了一种新型频率感知门控状态空间模型（FGMamba），能有效提升医学图像的超分辨率重建效果，并兼顾建模长程结构与高频细节，具有高效轻量的特点。


<details>
  <summary>Details</summary>
Motivation: 提升医学图像的分辨率对于提高诊断准确性、降低扫描时间和成本非常关键。但要在低计算开销下同时建模医学图像中远距离的结构信息和精细的高频细节一直具有挑战性。现有方法在兼顾全局依赖和高频细节以及参数效率上存在不足。

Method: 提出FGMamba架构，包括两个创新点：1）门控注意力增强状态空间模块（GASM），该模块融合高效的状态空间模型和空间、通道双分支注意力机制；2）金字塔频率融合模块（PFFM），通过FFT引导在多分辨率下融合高频细节。整个模型针对全局结构建模和高频细节增强统一设计，且十分轻量。

Result: 在超声、OCT、MRI、CT、内镜五种医疗影像上进行了大量评测，FGMamba在PSNR和SSIM指标上优于主流的CNN和Transformer方法，且参数量小于0.75M，展现出领先的性能和高效性。

Conclusion: FGMamba实现了高效、可扩展且准确的医学影像超分辨率增强，证明了频率感知的状态空间建模方法在该任务中的有效性。

Abstract: Medical image super-resolution (SR) is essential for enhancing diagnostic
accuracy while reducing acquisition cost and scanning time. However, modeling
both long-range anatomical structures and fine-grained frequency details with
low computational overhead remains challenging. We propose FGMamba, a novel
frequency-aware gated state-space model that unifies global dependency modeling
and fine-detail enhancement into a lightweight architecture. Our method
introduces two key innovations: a Gated Attention-enhanced State-Space Module
(GASM) that integrates efficient state-space modeling with dual-branch spatial
and channel attention, and a Pyramid Frequency Fusion Module (PFFM) that
captures high-frequency details across multiple resolutions via FFT-guided
fusion. Extensive evaluations across five medical imaging modalities
(Ultrasound, OCT, MRI, CT, and Endoscopic) demonstrate that FGMamba achieves
superior PSNR/SSIM while maintaining a compact parameter footprint ($<$0.75M),
outperforming CNN-based and Transformer-based SOTAs. Our results validate the
effectiveness of frequency-aware state-space modeling for scalable and accurate
medical image enhancement.

</details>


### [44] [CASR-Net: An Image Processing-focused Deep Learning-based Coronary Artery Segmentation and Refinement Network for X-ray Coronary Angiogram](https://arxiv.org/abs/2510.27315)
*Alvee Hassan,Rusab Sarmun,Muhammad E. H. Chowdhury,M. Murugappan,Md. Sakib Abrar Hossain,Sakib Mahmud,Abdulrahman Alqahtani,Sohaib Bassam Zoghoul,Amith Khandakar,Susu M. Zughaier,Somaya Al-Maadeed,Anwarul Hasan*

Main category: cs.CV

TL;DR: 该论文提出了一种三阶段冠状动脉分割和优化神经网络（CASR-Net），在冠脉X光血管图像上实现了更高精度的自动分割。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病（CAD）早期发现可以显著降低死亡率并优化治疗方案。而主流的X光血管造影分析方法受图像质量影响大，传统分割方法难以精确分割窄小及狭窄血管分支，从而影响诊断准确性。作者旨在提升自动分割的准确率和鲁棒性，以更好地辅助临床决策。

Method: 提出了CASR-Net，包括三阶段流程：多通道图像预处理（结合CLAHE和改进Ben Graham方法）、基于DenseNet121编码器和Self-ONN解码器的UNet分割网络，以及后续轮廓细化模块来减少假阳性。方法在两个公开冠脉数据集上采用5折交叉验证进行评估。

Result: 多通道预处理方法相比单独方法带来增益，分割IoU提升0.40-1.16%，DSC提升0.31-0.89%。总体模型在两数据集上获得IoU 61.43%，DSC 76.10%，clDice 79.36%，优于多种先进模型。

Conclusion: CASR-Net能自动且精准地分割冠状动脉，有效保留窄小/狭窄分支，对辅助临床冠脉疾病诊断具有重要应用价值。

Abstract: Early detection of coronary artery disease (CAD) is critical for reducing
mortality and improving patient treatment planning. While angiographic image
analysis from X-rays is a common and cost-effective method for identifying
cardiac abnormalities, including stenotic coronary arteries, poor image quality
can significantly impede clinical diagnosis. We present the Coronary Artery
Segmentation and Refinement Network (CASR-Net), a three-stage pipeline
comprising image preprocessing, segmentation, and refinement. A novel
multichannel preprocessing strategy combining CLAHE and an improved Ben Graham
method provides incremental gains, increasing Dice Score Coefficient (DSC) by
0.31-0.89% and Intersection over Union (IoU) by 0.40-1.16% compared with using
the techniques individually. The core innovation is a segmentation network
built on a UNet with a DenseNet121 encoder and a Self-organized Operational
Neural Network (Self-ONN) based decoder, which preserves the continuity of
narrow and stenotic vessel branches. A final contour refinement module further
suppresses false positives. Evaluated with 5-fold cross-validation on a
combination of two public datasets that contain both healthy and stenotic
arteries, CASR-Net outperformed several state-of-the-art models, achieving an
IoU of 61.43%, a DSC of 76.10%, and clDice of 79.36%. These results highlight a
robust approach to automated coronary artery segmentation, offering a valuable
tool to support clinicians in diagnosis and treatment planning.

</details>


### [45] [Overcoming Prompts Pool Confusion via Parameterized Prompt for Incremental Object Detection](https://arxiv.org/abs/2510.27316)
*Zijia An,Boyu Diao,Ruiqi Liu,Libo Huang,Chuanguang Yang,Fei Wang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种用于增量目标检测（IOD）的参数化提示（P$^2$IOD）方法，通过适应性地整合跨任务知识，并限制参数更新，以防止灾难性遗忘。实验结果表明，该方法在主流数据集上达到最新最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有可训练提示的方法已被证实能有效用于增量学习，但在目标检测场景中的应用尚未充分探索，尤其是针对检测图像中不同类别天然共现的情况。现有方法忽略了未标注类别的混淆问题，导致在增量检测时效率不高。

Method: 提出了一种基于神经网络的参数化提示结构P$^2$IOD，能够跨任务自适应整合知识。同时采用提示融合策略以约束结构更新，平衡新旧知识，避免灾难性遗忘。

Result: 在PASCAL VOC2007和MS COCO两个主流数据集上进行了大量实验，P$^2$IOD在增量目标检测任务中表现优异，超越了现有基线方法，达到了SOTA（最先进）水平。

Conclusion: P$^2$IOD能高效整合跨任务知识，适应目标检测任务中的类别共现问题，在增量检测领域展现出显著性能与潜力。

Abstract: Recent studies have demonstrated that incorporating trainable prompts into
pretrained models enables effective incremental learning. However, the
application of prompts in incremental object detection (IOD) remains
underexplored. Existing prompts pool based approaches assume disjoint class
sets across incremental tasks, which are unsuitable for IOD as they overlook
the inherent co-occurrence phenomenon in detection images. In co-occurring
scenarios, unlabeled objects from previous tasks may appear in current task
images, leading to confusion in prompts pool. In this paper, we hold that
prompt structures should exhibit adaptive consolidation properties across
tasks, with constrained updates to prevent catastrophic forgetting. Motivated
by this, we introduce Parameterized Prompts for Incremental Object Detection
(P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD
employs networks as the parameterized prompts to adaptively consolidate
knowledge across tasks. To constrain prompts structure updates, P$^2$IOD
further engages a parameterized prompts fusion strategy. Extensive experiments
on PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IOD's
effectiveness in IOD and achieves the state-of-the-art performance among
existing baselines.

</details>


### [46] [SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction](https://arxiv.org/abs/2510.27318)
*Wenfeng Huang,Xiangyun Liao,Yinling Qian,Hao Liu,Yongming Yang,Wenjing Jia,Qiong Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种自适应抗混叠的高斯泼洒重建方法（SAGS），显著提升了内窥镜手术中变形组织的三维重建效果，兼顾了效率和可视化质量。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术中动态组织的三维重建对机器人辅助手术至关重要。NeRF技术在变形组织重建上取得进展，但组织运动导致的混叠和伪影影响重建质量。3DGS提高了重建效率，却忽视了质量问题。因此需要一种能够兼顾速度与质量的方法，解决动态组织混叠和伪影问题。

Method: 提出SAGS框架，基于自适应抗混叠的高斯泼洒。方法核心是引入带注意力机制的4D动态变形解码器，结合3D平滑滤波和2D Mip滤波，有效减轻伪影产生并细致捕捉组织运动细节。

Result: 在EndoNeRF和SCARED两个公开基准测试集上，SAGS在PSNR、SSIM和LPIPS等所有指标上均优于现有最新方法，同时实现了更优的可视化重建质量。

Conclusion: SAGS方法在保持渲染效率的同时，大幅提升了内窥镜动态变形组织的重建精度和可视化，适合实际手术场景应用。

Abstract: Surgical reconstruction of dynamic tissues from endoscopic videos is a
crucial technology in robot-assisted surgery. The development of Neural
Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction,
achieving high-quality results from video and image sequences. However,
reconstructing deformable endoscopic scenes remains challenging due to aliasing
and artifacts caused by tissue movement, which can significantly degrade
visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has
improved reconstruction efficiency by enabling a faster rendering pipeline.
Nevertheless, existing 3DGS methods often prioritize rendering speed while
neglecting these critical issues. To address these challenges, we propose SAGS,
a self-adaptive alias-free Gaussian splatting framework. We introduce an
attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D
smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue
reconstruction and better capture the fine details of tissue movement.
Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate
that our method achieves superior performance in all metrics of PSNR, SSIM, and
LPIPS compared to the state of the art while also delivering better
visualization quality.

</details>


### [47] [Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis](https://arxiv.org/abs/2510.27324)
*Weiming Chen,Yijia Wang,Zhihan Zhu,Zhihai He*

Main category: cs.CV

TL;DR: 该论文提出了一种超低比特率视觉通信方法，通过结合文本描述和深度图像压缩，实现远程视觉分析时在极低带宽下精确还原图像，同时保持分析准确性。


<details>
  <summary>Details</summary>
Motivation: 在深空探索、战场情报、复杂环境下机器人导航等极端低带宽场景，远程视觉分析对图像传输的带宽要求极高。常规编码方法无法满足极低带宽的要求，而现有的文生图模型虽能进行语义级还原，却无法精确满足视觉通信和分析的需求。因此，亟需有效降低带宽同时保证还原和分析效果的方法。

Method: 作者提出将图像生成与深度图像压缩无缝集成，通过联合编码语义文本描述和压缩潜变量，用于指导矫正流模型实现视觉场景的高精度重构。发送端对文本和压缩潜变量编码、传输，接收端用其生成高质量图像。

Result: 实验结果显示，该方法在大幅降低带宽消耗的前提下，能够达到与主流方法相同的图像重构质量和视觉分析准确率。

Conclusion: 该方法为极低带宽条件下的视觉通信与分析提供了新的解决方案，有望在远程操作、导航、情报等场景中应用，兼顾带宽和效果，具有实际应用前景。

Abstract: We consider the problem of ultra-low bit rate visual communication for remote
vision analysis, human interactions and control in challenging scenarios with
very low communication bandwidth, such as deep space exploration, battlefield
intelligence, and robot navigation in complex environments. In this paper, we
ask the following important question: can we accurately reconstruct the visual
scene using only a very small portion of the bit rate in existing coding
methods while not sacrificing the accuracy of vision analysis and performance
of human interactions? Existing text-to-image generation models offer a new
approach for ultra-low bitrate image description. However, they can only
achieve a semantic-level approximation of the visual scene, which is far
insufficient for the purpose of visual communication and remote vision analysis
and human interactions. To address this important issue, we propose to
seamlessly integrate image generation with deep image compression, using joint
text and coding latent to guide the rectified flow models for precise
generation of the visual scene. The semantic text description and coding latent
are both encoded and transmitted to the decoder at a very small bit rate.
Experimental results demonstrate that our method can achieve the same image
reconstruction quality and vision analysis accuracy as existing methods while
using much less bandwidth. The code will be released upon paper acceptance.

</details>


### [48] [MeisenMeister: A Simple Two Stage Pipeline for Breast Cancer Classification on MRI](https://arxiv.org/abs/2510.27326)
*Benjamin Hamm,Yannick Kirchhoff,Maximilian Rokuss,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 本文参与了ODELIA乳腺MRI挑战赛，提出了一种基于分类的新方法以提升乳腺癌早筛效率，并公开了完整代码。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌筛查技术当前受限于高质量标注的稀缺，且现有的分割与分析手段面对乳腺癌检测仍有很大挑战。因此，研发更健壮的分类方法对于大规模筛查与早诊具有重要作用。

Method: 作者制定了一套详细的方法开发流程，包括：阐述基本理念和假设、反复实验与评估、逐步优化方案，并基于性能、鲁棒性及临床相关性来最终决定算法架构。

Result: 文中总结了方法迭代与优化过程，并根据实验结果和实际表现，形成了解释充分、证据充分的最终方案提交。该方案具有较好的性能和应用前景。

Conclusion: 文章提出的方法有效提升了乳腺癌MRI分类检测的效率和可靠性，为筛查实践和后续研究提供了可借鉴思路和开源实现。

Abstract: The ODELIA Breast MRI Challenge 2025 addresses a critical issue in breast
cancer screening: improving early detection through more efficient and accurate
interpretation of breast MRI scans. Even though methods for general-purpose
whole-body lesion segmentation as well as multi-time-point analysis exist,
breast cancer detection remains highly challenging, largely due to the limited
availability of high-quality segmentation labels. Therefore, developing robust
classification-based approaches is crucial for the future of early breast
cancer detection, particularly in applications such as large-scale screening.
In this write-up, we provide a comprehensive overview of our approach to the
challenge. We begin by detailing the underlying concept and foundational
assumptions that guided our work. We then describe the iterative development
process, highlighting the key stages of experimentation, evaluation, and
refinement that shaped the evolution of our solution. Finally, we present the
reasoning and evidence that informed the design choices behind our final
submission, with a focus on performance, robustness, and clinical relevance. We
release our full implementation publicly at
https://github.com/MIC-DKFZ/MeisenMeister

</details>


### [49] [Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing](https://arxiv.org/abs/2510.27335)
*Yijia Wang,Yiqing Shen,Weiming Chen,Zhihai He*

Main category: cs.CV

TL;DR: 本文提出了一种新的复杂图像编辑方法CIELR，无需大规模模型联合微调，实验表明方法显著优于以往工作，并构建了新的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法无法高效处理复杂编辑指令，且通常需要LLM和扩散模型的联合微调，导致计算和训练成本极高。

Method: 提出CIELR方法，将复杂指令拆解为简单明确的编辑动作，避免了LLM和DM联合微调。具体包括：首先用基础模型构建输入图像的结构化语义表示；随后通过迭代更新机制逐步细化图像语义，实现对复杂编辑任务的灵活处理。

Result: 在SmartEdit Reasoning Scenario Set上，CIELR较先前SOTA方法PSNR提升9.955dB，保真度更高。并新建CIEBench基准及特定评测指标，CIELR也优于其他方法。

Conclusion: CIELR显著提升了复杂图像编辑的表现，降低了训练成本，为基于推理的复杂编辑提供了更优解法，新数据集也补足了领域短板。

Abstract: Existing image editing methods can handle simple editing instructions very
well. To deal with complex editing instructions, they often need to jointly
fine-tune the large language models (LLMs) and diffusion models (DMs), which
involves very high computational complexity and training cost. To address this
issue, we propose a new method, called \textbf{C}omplex \textbf{I}mage
\textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts a
complex user instruction into a set of simple and explicit editing actions,
eliminating the need for jointly fine-tuning the large language models and
diffusion models. Specifically, we first construct a structured semantic
representation of the input image using foundation models. Then, we introduce
an iterative update mechanism that can progressively refine this
representation, obtaining a fine-grained visual representation of the image
scene. This allows us to perform complex and flexible image editing tasks.
Extensive experiments on the SmartEdit Reasoning Scenario Set show that our
method surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating
its superior preservation of regions that should remain consistent. Due to the
limited number of samples of public datasets of complex image editing with
reasoning, we construct a benchmark named CIEBench, containing 86 image
samples, together with a metric specifically for reasoning-based image editing.
CIELR also outperforms previous methods on this benchmark. The code and dataset
are available at
\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.

</details>


### [50] [RzenEmbed: Towards Comprehensive Multimodal Retrieval](https://arxiv.org/abs/2510.27350)
*Weijian Jian,Yajun Zhang,Dawei Liang,Chunyu Xie,Yixiao He,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 该论文提出了RzenEmbed，一个能够处理文本、图片、视频和视觉文档等多模态数据的统一嵌入框架，在MMEB基准上取得了新的SOTA成绩，尤其在视频和视觉文档检索任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型（如CLIP）主要集中在自然图片上，对于视频和视觉文档等其他关键视觉模态支持有限，亟需提出一种能够统一处理多种模态的嵌入方法。

Method: 1. 提出了RzenEmbed框架，能够同时对文本、图片、视频和视觉文档等多种数据进行嵌入学习；2. 采用两阶段训练策略，第一阶段进行基础的文本和多模态检索训练，第二阶段引入改良的InfoNCE损失，包括困难样本加权机制（对更难样本赋予更高权重）和降低伪负样本、噪声干扰的方法，同时引入可学习温度参数和模型souping提升表现。

Result: RzenEmbed在MMEB基准任务上实现了新的最优性能，各指标均超越现有方法，尤以视频和视觉文档检索任务提升显著。

Conclusion: RzenEmbed有效突破了现有多模态嵌入方法在非图片模态上的局限，统一了多种数据类型的嵌入学习，并显著提升了检索任务的表现，具备强大的实际应用潜力。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has
extended CLIP-based frameworks to produce powerful, universal embeddings for
retrieval tasks. However, existing methods primarily focus on natural images,
offering limited support for other crucial visual modalities such as videos and
visual documents. To bridge this gap, we introduce RzenEmbed, a unified
framework to learn embeddings across a diverse set of modalities, including
text, images, videos, and visual documents. We employ a novel two-stage
training strategy to learn discriminative representations. The first stage
focuses on foundational text and multimodal retrieval. In the second stage, we
introduce an improved InfoNCE loss, incorporating two key enhancements.
Firstly, a hardness-weighted mechanism guides the model to prioritize
challenging samples by assigning them higher weights within each batch.
Secondly, we implement an approach to mitigate the impact of false negatives
and alleviate data noise. This strategy not only enhances the model's
discriminative power but also improves its instruction-following capabilities.
We further boost performance with learnable temperature parameter and model
souping. RzenEmbed sets a new state-of-the-art on the MMEB benchmark. It not
only achieves the best overall score but also outperforms all prior work on the
challenging video and visual document retrieval tasks. Our models are available
in https://huggingface.co/qihoo360/RzenEmbed.

</details>


### [51] [FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning](https://arxiv.org/abs/2510.27359)
*Kenneth Yang,Wen-Li Wei,Jen-Chun Lin*

Main category: cs.CV

TL;DR: 提出了一种名为FPS的梯度无关参数选择方法，实现了更高效的模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有高效微调方法存在推理延迟高、实现复杂或者内存消耗大等问题，制约了大规模预训练模型在下游任务上的实际应用。

Method: 作者提出Feedforward-based Parameter Selection（FPS），无需反向传播，仅通过一次前向传播，根据参数大小和激活值乘积筛选最优参数子集。

Result: 在24个计算机视觉下游任务评测中，FPS能达到与现有最佳方法类似的性能，但峰值内存消耗降低近9倍，参数选择速度提升约2倍。

Conclusion: FPS方法实用且高效，为大模型微调提供了更具内存效率的解决方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for
adapting large-scale pre-trained models to downstream tasks, but existing
approaches face notable limitations. Addition-based methods, such as Adapters
[1], introduce inference latency and engineering complexity, while
selection-based methods like Gradient-based Parameter Selection (GPS) [2]
require a full backward pass, which results in the same peak memory usage as
full fine-tuning. To address this dilemma, we propose Feedforward-based
Parameter Selection (FPS), a gradient-free method that identifies an optimal
parameter subset in a single forward pass. FPS ranks parameters by the product
of their magnitudes and corresponding input activations, leveraging both
pre-trained knowledge and downstream data. Evaluated on $24$ visual tasks from
FGVC and VTAB-1k, FPS achieves performance comparable to state-of-the-art
methods while reducing peak memory usage by nearly $9 \times$ and accelerating
parameter selection by about $2 \times$, offering a genuinely memory-efficient
and practical solution for fine-tuning large-scale pre-trained models.

</details>


### [52] [Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V](https://arxiv.org/abs/2510.27364)
*Meftun Akarsu,Kerem Catay,Sedat Bin Vedat,Enes Kutay Yarkan,Ilke Senturk,Arda Sar,Dafne Eksioglu*

Main category: cs.CV

TL;DR: 提出了一个实用的流水线方法，将开源视频扩散模型微调，用于在小数据集上合成影视级电影场景。流程可高效迁移风格并提升时序一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 影视制作需要高质量、风格化的视频合成，但实际可用的训练样本有限。现有大模型难以直接适应具体影视风格和属性。

Method: 采用两阶段流程，第一阶段用LoRA对视频扩散模型Wan2.1 I2V-14B的跨注意力层进行微调，通过小规模影视片段高效学得视觉风格；第二阶段将模型生成的一致性关键帧扩展为高分辨率连贯视频，并通过推理加速技术提升效率。

Result: 通过FVD、CLIP-SIM和LPIPS等指标以及专家小规模用户研究，证明了该方法相比基础模型在电影感和时序稳定性上均有明显提升。

Conclusion: 方法高效，适用性强，能在不同影视领域复现，有助于小团队或个性化影视视频的生成与调色风格迁移。

Abstract: We present a practical pipeline for fine-tuning open-source video diffusion
transformers to synthesize cinematic scenes for television and film production
from small datasets. The proposed two-stage process decouples visual style
learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)
modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B
model to adapt its visual representations using a compact dataset of short
clips from Ay Yapim's historical television film El Turco. This enables
efficient domain transfer within hours on a single GPU. In the second stage,
the fine-tuned model produces stylistically consistent keyframes that preserve
costume, lighting, and color grading, which are then temporally expanded into
coherent 720p sequences through the model's video decoder. We further apply
lightweight parallelization and sequence partitioning strategies to accelerate
inference without quality degradation. Quantitative and qualitative evaluations
using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,
demonstrate measurable improvements in cinematic fidelity and temporal
stability over the base model. The complete training and inference pipeline is
released to support reproducibility and adaptation across cinematic domains.

</details>


### [53] [Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum](https://arxiv.org/abs/2510.27571)
*Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen Chu*

Main category: cs.CV

TL;DR: 本文提出了一套旨在实现通用视频检索能力的协同设计框架，包含新的评测基准、大规模高质量数据生成方法和多模态建模策略，在通用性和零样本检索上取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频检索方法由于依赖于狭窄的评测基准，导致数据和训练目标受限，缺乏通用能力。缺乏能诊断多维泛化能力的评测，限制了领域进步。本文希望突破这一局限，实现真正的通用视频检索。

Method: （1）提出了Universal Video Retrieval Benchmark (UVRB)，涵盖16个数据集，用于多维度性能诊断；（2）基于UVRB反馈，构建了可扩展的合成数据流程，生成了155万高质量语义对；（3）设计了Modality Pyramid课程式训练策略，利用多样数据之间的隐含关联，提升General Video Embedder (GVE)的能力。

Result: GVE在UVRB上的零样本泛化能力达到了SOTA水平，实验表明传统基准无法有效预测通用检索能力，并发现部分相关检索是一个重要但被忽视的场景。

Conclusion: 协同设计的评测、数据与建模框架为突破视频检索领域的局限、实现真正的通用检索能力提供了有效的解决方案。

Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow
benchmarks incentivize correspondingly limited data and single-task training.
Therefore, universal capability is suppressed due to the absence of a
diagnostic evaluation that defines and demands multi-dimensional
generalization. To break this cycle, we introduce a framework built on the
co-design of evaluation, data, and modeling. First, we establish the Universal
Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to
measure performance but also to diagnose critical capability gaps across tasks
and domains. Second, guided by UVRB's diagnostics, we introduce a scalable
synthesis workflow that generates 1.55 million high-quality pairs to populate
the semantic space required for universality. Finally, we devise the Modality
Pyramid, a curriculum that trains our General Video Embedder (GVE) by
explicitly leveraging the latent interconnections within our diverse data.
Extensive experiments show GVE achieves state-of-the-art zero-shot
generalization on UVRB. In particular, our analysis reveals that popular
benchmarks are poor predictors of general ability and that partially relevant
retrieval is a dominant but overlooked scenario. Overall, our co-designed
framework provides a practical path to escape the limited scope and advance
toward truly universal video retrieval.

</details>


### [54] [Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds](https://arxiv.org/abs/2510.27391)
*Wu Wei,Xiaomeng Fan,Yuwei Wu,Zhi Gao,Pengxiang Li,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 本文提出了“树级对齐（Alignment across Trees）”方法，通过在图像和文本模态中都构建树形分层特征，使二者对齐，提升视觉-语言模型的整合能力，并证明了在开放集分类任务中此方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在提取特征时，文本采用层次化特征，而图像多用单一特征表示，导致模态间对齐不对称且效果欠佳，因此需要解决图像与文本分层特征对齐的问题。

Method: 方法包括：1）提出带有语义感知的视觉特征提取框架，通过文本引导的跨注意机制，从Transformer中间层提取视觉类别token，生成具有层次结构（从粗到细语义）的视觉特征树；2）将图像和文本的树形特征分别嵌入具有不同曲率的双曲流形中以建模它们的层次结构；3）设计跨异质流形的KL距离度量，并通过最小化距离学习中间流形，实现模态对齐，并证明了最优中间流形的存在性与唯一性。

Result: 在多个图像数据集的分类任务（特别是开放集和跨领域、少样本设定）上进行实验，结果显示该方法在所有设定下都优于强基线方法。

Conclusion: 通过为图像和文本都构建并对齐分层结构特征，以及在双曲流形中对其进行有效建模与对齐，能够显著提升视觉-语言模型的多模态理解和泛化能力。

Abstract: Modality alignment is critical for vision-language models (VLMs) to
effectively integrate information across modalities. However, existing methods
extract hierarchical features from text while representing each image with a
single feature, leading to asymmetric and suboptimal alignment. To address
this, we propose Alignment across Trees, a method that constructs and aligns
tree-like hierarchical features for both image and text modalities.
Specifically, we introduce a semantic-aware visual feature extraction framework
that applies a cross-attention mechanism to visual class tokens from
intermediate Transformer layers, guided by textual cues to extract visual
features with coarse-to-fine semantics. We then embed the feature trees of the
two modalities into hyperbolic manifolds with distinct curvatures to
effectively model their hierarchical structures. To align across the
heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL
distance measure between distributions on heterogeneous manifolds, and learn an
intermediate manifold for manifold alignment by minimizing the distance. We
prove the existence and uniqueness of the optimal intermediate manifold.
Experiments on taxonomic open-set classification tasks across multiple image
datasets demonstrate that our method consistently outperforms strong baselines
under few-shot and cross-domain settings.

</details>


### [55] [A Hybrid Deep Learning and Forensic Approach for Robust Deepfake Detection](https://arxiv.org/abs/2510.27392)
*Sales Aribe Jr*

Main category: cs.CV

TL;DR: 本文提出一种融合深度学习和取证特征的混合检测框架，有效提升了深度伪造（Deepfake）检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着GAN和扩散模型的发展，生成的虚假媒体愈发逼真，带来虚假信息和身份伪造等社会风险。而现有的检测方法各有局限——深度学习泛化能力不足、容易受干扰，传统取证方法则面对新型伪造技术时无力。因此，亟需兼具适应性和可解释性的检测方案。

Method: 提出了将噪声残差、JPEG压缩特征和频域描述子等取证特征，与CNN和ViT等深度学习特征进行融合的混合检测框架，并在多个权威数据集（FaceForensics++、Celeb-DF v2、DFDC）上进行评测。

Result: 提出的方法在上述数据集上的F1分数分别达到0.96、0.82和0.77，优于单一方法及现有混合方法。模型在压缩、对抗扰动及未知伪造类别上的表现依然稳健。可解释性分析显示，Grad-CAM和取证热图与实际伪造区域有82%的重合度。

Conclusion: 混合方法能够实现深度模型的灵活性与取证方法的可解释性的平衡，是开发强健且可信的Deepfake检测系统的有效途径。

Abstract: The rapid evolution of generative adversarial networks (GANs) and diffusion
models has made synthetic media increasingly realistic, raising societal
concerns around misinformation, identity fraud, and digital trust. Existing
deepfake detection methods either rely on deep learning, which suffers from
poor generalization and vulnerability to distortions, or forensic analysis,
which is interpretable but limited against new manipulation techniques. This
study proposes a hybrid framework that fuses forensic features, including noise
residuals, JPEG compression traces, and frequency-domain descriptors, with deep
learning representations from convolutional neural networks (CNNs) and vision
transformers (ViTs). Evaluated on benchmark datasets (FaceForensics++, Celeb-DF
v2, DFDC), the proposed model consistently outperformed single-method baselines
and demonstrated superior performance compared to existing state-of-the-art
hybrid approaches, achieving F1-scores of 0.96, 0.82, and 0.77, respectively.
Robustness tests demonstrated stable performance under compression (F1 = 0.87
at QF = 50), adversarial perturbations (AUC = 0.84), and unseen manipulations
(F1 = 0.79). Importantly, explainability analysis showed that Grad-CAM and
forensic heatmaps overlapped with ground-truth manipulated regions in 82
percent of cases, enhancing transparency and user trust. These findings confirm
that hybrid approaches provide a balanced solution, combining the adaptability
of deep models with the interpretability of forensic cues, to develop resilient
and trustworthy deepfake detection systems.

</details>


### [56] [Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset](https://arxiv.org/abs/2510.27421)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文针对乳腺癌肿瘤自动分割数据集MAMA-MIA，评估了自动分割标签在年龄、族裔及数据源上的公平性，发现对年轻患者存在固有偏差，并分析了产生偏差的原因及数据聚合对族裔偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在医学诊断自动化上有巨大潜力，但目前对模型“公平性”的评估大多集中在分类任务，分割任务的公平性较少被关注。而分割偏差可能导致某些群体在治疗质量上的不公平。因此有必要对分割模型的公平性进行审查。

Method: 对乳腺癌肿瘤分割数据集MAMA-MIA，针对自动分割标签，分年龄、族裔、数据源三个维度评估分割质量，进一步分析混杂因素，并考察多数据源聚合对偏差的影响。

Result: 结果显示，自动分割对年轻患者存在固有偏差，并且该偏差无法通过调整数据源等混杂因素消除。研究还发现，不同数据源的聚合会影响特定地点的族裔偏差。

Conclusion: 分割模型在公平性方面存在隐患，尤其对特殊人群（如年轻患者）存在不利影响。仅仅聚合多中心数据无法自动消除所有族裔相关的偏差。需要更细粒度和专门的公平性审查与改进方法以确保模型结果的公平和可用。

Abstract: Deep learning models aim to improve diagnostic workflows, but fairness
evaluation remains underexplored beyond classification, e.g., in image
segmentation. Unaddressed segmentation bias can lead to disparities in the
quality of care for certain populations, potentially compounded across clinical
decision points and amplified through iterative model development. Here, we
audit the fairness of the automated segmentation labels provided in the breast
cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation
quality across age, ethnicity, and data source. Our analysis reveals an
intrinsic age-related bias against younger patients that continues to persist
even after controlling for confounding factors, such as data source. We
hypothesize that this bias may be linked to physiological factors, a known
challenge for both radiologists and automated systems. Finally, we show how
aggregating data from multiple data sources influences site-specific ethnic
biases, underscoring the necessity of investigating data at a granular level.

</details>


### [57] [Mitigating Semantic Collapse in Partially Relevant Video Retrieval](https://arxiv.org/abs/2510.27432)
*WonJun Moon,MinSeok Jung,Gilhan Park,Tae-Young Kim,Cheol-Ho Cho,Woojin Jun,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文针对部分相关视频检索（PRVR）任务，提出了防止语义塌陷的新方法，在多个基准上显著提升了检索准确率。


<details>
  <summary>Details</summary>
Motivation: 现有PRVR方法未能处理视频内部和视频间的丰富语义差异，导致嵌入空间中语义相近的内容分布混乱，进而影响检索性能。

Method: 1）提出文本相关性保持学习，显式维持文本间的语义关系；2）提出交叉分支视频对齐（CBVA），用于分辨和对齐不同时间尺度上的视频语义；3）引入顺序保持的token融合及自适应CBVA方法，进一步提高分段的区分性和内聚性。

Result: 在多个部分相关视频检索基准上，实验显示该框架有效防止了文本和视频嵌入空间的语义塌陷，显著提升了检索效果。

Conclusion: 所提出的新框架能更好地捕捉和分离视频中的多样语义，大幅度改善部分相关视频检索任务的准确性。

Abstract: Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the
content matches a text query. Existing methods treat every annotated text-video
pair as a positive and all others as negatives, ignoring the rich semantic
variation both within a single video and across different videos. Consequently,
embeddings of both queries and their corresponding video-clip segments for
distinct events within the same video collapse together, while embeddings of
semantically similar queries and segments from different videos are driven
apart. This limits retrieval performance when videos contain multiple, diverse
events. This paper addresses the aforementioned problems, termed as semantic
collapse, in both the text and video embedding spaces. We first introduce Text
Correlation Preservation Learning, which preserves the semantic relationships
encoded by the foundation model across text queries. To address collapse in
video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive
alignment method that disentangles hierarchical video representations across
temporal scales. Subsequently, we introduce order-preserving token merging and
adaptive CBVA to enhance alignment by producing video segments that are
internally coherent yet mutually distinctive. Extensive experiments on PRVR
benchmarks demonstrate that our framework effectively prevents semantic
collapse and substantially improves retrieval accuracy.

</details>


### [58] [DeblurSDI: Blind Image Deblurring Using Self-diffusion](https://arxiv.org/abs/2510.27439)
*Yanlong Yang,Guanxiong Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种无需预训练、基于自扩散过程的盲图像去卷积方法DeblurSDI，能够动态适应各种退化情况，实验表明其在恢复清晰图像和卷积核方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工先验，现代深度学习方法又需要大规模外部数据集的预训练，二者都不易适应真实复杂环境，因此亟需一种无需外部数据、能泛化适应各种实际模糊情况的新方法。

Method: DeblurSDI将盲去卷积转化为自扩散（SDI）反向迭代过程。在每一步中，分别对图像和核使用随机初始化的神经网络进行联合优化，目标函数结合数据一致性项和稀疏正则项（L1范数）。论文提出了一种噪声调度机制提升优化稳定性及对不同核尺寸的鲁棒性。整个过程无需任何预训练，通过输入图像动态学习实例特定先验。

Result: 大量实验表明，DeblurSDI在不需外部训练和各类模糊核（包括严重退化）的情况下，都能有效恢复高质量清晰图像并准确估计模糊核。其性能优于许多现有传统和深度学习方法。

Conclusion: DeblurSDI证明了基于自扩散与自监督的盲去卷积在实际应用中的强大适应性、稳定性和高效性，不依赖大数据和预训练，是盲图像去卷积领域的重要突破。

Abstract: Blind image deconvolution is a challenging ill-posed inverse problem, where
both the latent sharp image and the blur kernel are unknown. Traditional
methods often rely on handcrafted priors, while modern deep learning approaches
typically require extensive pre-training on large external datasets, limiting
their adaptability to real-world scenarios. In this work, we propose DeblurSDI,
a zero-shot, self-supervised framework based on self-diffusion (SDI) that
requires no prior training. DeblurSDI formulates blind deconvolution as an
iterative reverse self-diffusion process that starts from pure noise and
progressively refines the solution. At each step, two randomly-initialized
neural networks are optimized continuously to refine the sharp image and the
blur kernel. The optimization is guided by an objective function combining data
consistency with a sparsity-promoting L1-norm for the kernel. A key innovation
is our noise scheduling mechanism, which stabilizes the optimization and
provides remarkable robustness to variations in blur kernel size. These allow
DeblurSDI to dynamically learn an instance-specific prior tailored to the input
image. Extensive experiments demonstrate that DeblurSDI consistently achieves
superior performance, recovering sharp images and accurate kernels even in
highly degraded scenarios.

</details>


### [59] [CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging](https://arxiv.org/abs/2510.27442)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: 本文提出了一种高效且泛化能力强的Vision Transformer（ViT）架构——CoMViT，用于医疗影像分析，在大幅减少参数量的同时，性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统ViT在医疗影像中的表现虽强，但由于参数量大、易过拟合小数据集，限制了其在实际临床环境下的应用。因此亟需一种既高效、又能泛化的小型化ViT架构。

Method: 提出了一种新型ViT架构CoMViT，融合了卷积tokenizer、对角掩码、动态温度调整及基于池化的序列聚合，并通过系统性架构优化，压缩模型参数以适应低资源场景。

Result: CoMViT在12个MedMNIST数据集上均表现稳健，参数量仅约4.5M，准确率与更深的CNN和ViT变体相当或更优，且参数量减少达5-20倍。Grad-CAM分析显示模型始终关注医学上相关区域。

Conclusion: 通过对ViT结构的优化，可以在低资源医疗影像环境下开发高效可解释的模型，CoMViT具备广阔的实际应用潜力。

Abstract: Vision Transformers (ViTs) have demonstrated strong potential in medical
imaging; however, their high computational demands and tendency to overfit on
small datasets limit their applicability in real-world clinical scenarios. In
this paper, we present CoMViT, a compact and generalizable Vision Transformer
architecture optimized for resource-constrained medical image analysis. CoMViT
integrates a convolutional tokenizer, diagonal masking, dynamic temperature
scaling, and pooling-based sequence aggregation to improve performance and
generalization. Through systematic architectural optimization, CoMViT achieves
robust performance across twelve MedMNIST datasets while maintaining a
lightweight design with only ~4.5M parameters. It matches or outperforms deeper
CNN and ViT variants, offering up to 5-20x parameter reduction without
sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT
consistently attends to clinically relevant regions despite its compact size.
These results highlight the potential of principled ViT redesign for developing
efficient and interpretable models in low-resource medical imaging settings.

</details>


### [60] [From Pixels to Paths: A Multi-Agent Framework for Editable Scientific Illustration](https://arxiv.org/abs/2510.27452)
*Jianwen Sun,Fanrui Zhang,Yukang Feng,Chuanhao Li,Zizhen Li,Jiaxin Ai,Yifan Chang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 当前的图像生成模型无法满足科学插图对高信息密度和后期可编辑性的双重需求。VisPainter通过多代理模块和流程化设计，提升了科学插图的生成、可控性和后期编辑能力，并引入了系统性的评测体系。


<details>
  <summary>Details</summary>
Motivation: 科学插图需兼具高信息量和后编辑特性，目前生成模型不是缺乏元素层级语义结构、难以后期编辑，就是操作繁琐缺乏直观交互，不能有效满足科学创作需求。

Method: 提出VisPainter多代理框架，包含管理者、设计师和工具箱三大模块，分工协作生成兼容主流矢量图软件的插图，实现每个元素的显式表示和控制，同时引入VisBench系统性评测平台，从七维度评估插图的内容、布局、视觉感知和交互成本。

Result: 实验验证了多模块结构和评测维度的合理性，系统评估了不同视觉-语言模型和功能模块在插图生成上的表现，并定量分析了角色分工、流程控制和细节描述对质量的影响。

Conclusion: VisPainter框架显著提升了科学插图的生成效率、可编辑性和质量，VisBench为插图自动化和相关模型发展提供了客观评测依据。

Abstract: Scientific illustrations demand both high information density and
post-editability. However, current generative models have two major
limitations: Frist, image generation models output rasterized images lacking
semantic structure, making it impossible to access, edit, or rearrange
independent visual components in the images. Second, code-based generation
methods (TikZ or SVG), although providing element-level control, force users
into the cumbersome cycle of "writing-compiling-reviewing" and lack the
intuitiveness of manipulation. Neither of these two approaches can well meet
the needs for efficiency, intuitiveness, and iterative modification in
scientific creation. To bridge this gap, we introduce VisPainter, a multi-agent
framework for scientific illustration built upon the model context protocol.
VisPainter orchestrates three specialized modules-a Manager, a Designer, and a
Toolbox-to collaboratively produce diagrams compatible with standard vector
graphics software. This modular, role-based design allows each element to be
explicitly represented and manipulated, enabling true element-level control and
any element can be added and modified later. To systematically evaluate the
quality of scientific illustrations, we introduce VisBench, a benchmark with
seven-dimensional evaluation metrics. It assesses high-information-density
scientific illustrations from four aspects: content, layout, visual perception,
and interaction cost. To this end, we conducted extensive ablation experiments
to verify the rationality of our architecture and the reliability of our
evaluation methods. Finally, we evaluated various vision-language models,
presenting fair and credible model rankings along with detailed comparisons of
their respective capabilities. Additionally, we isolated and quantified the
impacts of role division, step control,and description on the quality of
illustrations.

</details>


### [61] [A Multi-tiered Human-in-the-loop Approach for Interactive School Mapping Using Earth Observation and Machine Learning](https://arxiv.org/abs/2510.27460)
*Casper Fibaek,Abi Riley,Kelsey Doerksen,Do-Hyung Kim,Rochelle Schneider*

Main category: cs.CV

TL;DR: 本论文提出了一种多层次的人机协同互动的学校分布地图标注框架，以提升发展中国家教育设施数据的准确性与完整性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家教育设施记录常常不全且更新不及时，影响规划与资源分配。本研究动力源于希望通过新方法改进教育资源调查效率和数据质量。

Method: 方法分多个层次：首先用机器学习分析人口密度、土地利用和已有学校位置，发现潜在空白和错标信息；随后利用中分辨率卫星遥感定位疑似学校区域，采用高分辨率遥感影像和深度学习模型细化候选位置，并结合全球预训练模型增强泛化能力。该系统包含交互界面，便于人工校正和反馈。中分辨率层因效果提升有限被移除。

Result: 初步实验表明，多层次策略有助于高效、可扩展地绘制教育设施分布图，支持教育资源科学配置。

Conclusion: 该人机交互多层框架为教育基础设施调研提供了一种成本低且可扩展的解决方案，能够有效帮助政策制定与规划。

Abstract: This paper presents a multi-tiered human-in-the-loop framework for
interactive school mapping designed to improve the accuracy and completeness of
educational facility records, particularly in developing regions where such
data may be scarce and infrequently updated. The first tier involves a machine
learning based analysis of population density, land cover, and existing
infrastructure compared with known school locations. The first tier identifies
potential gaps and "mislabelled" schools. In subsequent tiers,
medium-resolution satellite imagery (Sentinel-2) is investigated to pinpoint
regions with a high likelihood of school presence, followed by the application
of very high-resolution (VHR) imagery and deep learning models to generate
detailed candidate locations for schools within these prioritised areas. The
medium-resolution approach was later removed due to insignificant improvements.
The medium and VHR resolution models build upon global pre-trained steps to
improve generalisation. A key component of the proposed approach is an
interactive interface to allow human operators to iteratively review, validate,
and refine the mapping results. Preliminary evaluations indicate that the
multi-tiered strategy provides a scalable and cost-effective solution for
educational infrastructure mapping to support planning and resource allocation.

</details>


### [62] [Referee: Reference-aware Audiovisual Deepfake Detection](https://arxiv.org/abs/2510.27475)
*Hyemin Boo,Eunsang Lee,Jiyoung Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的音视频深度伪造检测方法Referee，通过结合目标与参考内容的身份相关特征实现跨模态伪造检测，在多个数据集上取得了先进的检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于音视频的深度伪造检测方法对于新型伪造往往泛化性较差，难以应对尚未见过的复杂伪造手段，因此需要更有效的方法提升检测鲁棒性和泛化能力。

Method: 作者提出了Referee方法，利用仅需一次样本的说话人特征（one-shot）作为参考，将参考与目标内容中的身份查询（identity-related queries）跨模态配对与对齐，将身份一致性与音视频同步性联合建模，实现超越常规时空伪造特征的检测判别。

Result: Referee方法在FakeAVCeleb、FaceForensics++和KoDF等公开数据集上进行广泛实验，在跨数据集和跨语言协议下均实现了先进水平的检测准确率。

Conclusion: 实验表明，联合考虑音视频跨模态的身份一致性检查非常有助于提升伪造检测能力，尤其是在遇到未知伪造时。文中方法推动了深度伪造检测领域的发展。

Abstract: Since deepfakes generated by advanced generative models have rapidly posed
serious threats, existing audiovisual deepfake detection approaches struggle to
generalize to unseen forgeries. We propose a novel reference-aware audiovisual
deepfake detection method, called Referee. Speaker-specific cues from only
one-shot examples are leveraged to detect manipulations beyond spatiotemporal
artifacts. By matching and aligning identity-related queries from reference and
target content into cross-modal features, Referee jointly reasons about
audiovisual synchrony and identity consistency. Extensive experiments on
FakeAVCeleb, FaceForensics++, and KoDF demonstrate that Referee achieves
state-of-the-art performance on cross-dataset and cross-language evaluation
protocols. Experimental results highlight the importance of cross-modal
identity verification for future deepfake detection. The code is available at
https://github.com/ewha-mmai/referee.

</details>


### [63] [NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding](https://arxiv.org/abs/2510.27481)
*Wei Xu,Cheng Wang,Dingkang Liang,Zongchuang Zhao,Xingyu Jiang,Peng Zhang,Xiang Bai*

Main category: cs.CV

TL;DR: 本论文提出了NAUTILUS，一个基于多模态大模型的水下场景理解系统，并发布了大规模多任务水下数据集NautData。通过引入物理先验与特征增强模块，有效提升了模型对真实水下图像的鲁棒性和理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前水下场景理解发展受限于缺乏大规模多任务标注数据集，以及水下图像退化严重影响自动化探索性能，需要相应解决方案支持下游多元任务。

Method: 作者构建了1.45M样本的多任务水下图像-文本数据集NautData，支持八项主流场景理解任务。同时，设计了基于物理成像模型的视觉特征增强（VFE）模块，可无缝集成到现有多模态大模型如LLaVA-1.5、Qwen2.5-VL。最后，基于这些改进合成了NAUTILUS模型。

Result: 在NautData及其它公开水下数据集上测试，VFE模块对主流多模态模型的绝大多数任务表现均有显著提升，验证了NAUTILUS的领先性能。

Conclusion: 本文通过构建大型数据集和创新的视觉增强模块，推动了自动水下场景理解领域发展。NAUTILUS平台及数据开放，促进社区进一步研究与应用。

Abstract: Underwater exploration offers critical insights into our planet and attracts
increasing attention for its broader applications in resource exploration,
national security, etc. We study the underwater scene understanding methods,
which aim to achieve automated underwater exploration. The underwater scene
understanding task demands multi-task perceptions from multiple granularities.
However, the absence of large-scale underwater multi-task instruction-tuning
datasets hinders the progress of this research. To bridge this gap, we
construct NautData, a dataset containing 1.45 M image-text pairs supporting
eight underwater scene understanding tasks. It enables the development and
thorough evaluation of the underwater scene understanding models. Underwater
image degradation is a widely recognized challenge that interferes with
underwater tasks. To improve the robustness of underwater scene understanding,
we introduce physical priors derived from underwater imaging models and propose
a plug-and-play vision feature enhancement (VFE) module, which explicitly
restores clear underwater information. We integrate this module into renowned
baselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS.
Experiments conducted on the NautData and public underwater datasets
demonstrate the effectiveness of the VFE module, consistently improving the
performance of both baselines on the majority of supported tasks, thus ensuring
the superiority of NAUTILUS in the underwater scene understanding area. Data
and models are available at https://github.com/H-EmbodVis/NAUTILUS.

</details>


### [64] [ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.27492)
*Jiawei Gu,Yunzhuo Hao,Huichen Will Wang,Linjie Li,Michael Qizhe Shieh,Yejin Choi,Ranjay Krishna,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态推理模型 ThinkMorph，通过高质量交错推理轨迹训练，实现文本与图像协同进步，显著提升视觉推理任务表现，并展现出多模态智能新能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理方法往往将文本和图像视作同质信息，未充分发挥两者互补优势，缺乏有效的交错链式思考机制。研究旨在探索如何通过更紧密的文本-图像互动提升推理能力。

Method: 作者提出 ThinkMorph 模型，并在2.4万条高质量的文本-图像交错推理轨迹上进行微调。模型学会生成逐步推进的文本-图像推理步骤，能够具体操作视觉内容且保持语言逻辑连贯。

Result: ThinkMorph模型在视觉为主基准任务上较基础模型平均提升34.7%，并能泛化到领域外任务，性能与更大规模甚至专有的多模态语言模型持平或超越。同时模型表现出未见显现的视觉操作能力、推理模式自适应切换等新颖智能特性。

Conclusion: 本文表明通过更合理的多模态链式思考机制，统一模型能够涌现出更强的多模态推理能力，为未来多模态智能体设计提供了新的方向和启示。

Abstract: Multimodal reasoning requires iterative coordination between language and
vision, yet it remains unclear what constitutes a meaningful interleaved chain
of thought. We posit that text and image thoughts should function as
complementary, rather than isomorphic, modalities that mutually advance
reasoning. Guided by this principle, we build ThinkMorph, a unified model
fine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with
varying visual engagement. ThinkMorph learns to generate progressive text-image
reasoning steps that concretely manipulate visual content while maintaining
coherent verbal logic. It delivers large gains on vision-centric benchmarks
(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,
matching or surpassing larger and proprietary VLMs. Beyond performance,
ThinkMorph exhibits emergent multimodal intelligence, including unseen visual
manipulation skills, adaptive switching between reasoning modes, and better
test-time scaling through diversified multimodal thoughts.These findings
suggest promising directions for characterizing the emergent capabilities of
unified models for multimodal reasoning.

</details>


### [65] [Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation](https://arxiv.org/abs/2510.27508)
*Elena Mulero Ayllón,Linlin Shen,Pierangelo Veltri,Fabrizia Gelardi,Arturo Chiti,Paolo Soda,Matteo Tortora*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级多模态肺部肿瘤分割方法vMambaX，结合PET与CT信息，引入上下文门控的跨模态感知模块，能高效提升分割效果且保持低计算复杂度，在PCLT20K数据集上性能优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 肺部肿瘤精确分割对诊断和治疗规划至关重要，但如何有效融合PET和CT的解剖学与功能学信息仍是难题。

Method: 基于Visual Mamba架构，引入Context-Gated Cross-Modal Perception Module (CGM)，自适应增强跨模态特征交互，加强有用区域信息、抑制噪声，实现高效的PET-CT融合分割。

Result: 在PCLT20K数据集上，vMambaX模型分割效果优于各类主流对比模型，并且计算复杂度更低。

Conclusion: 自适应跨模态门控机制提升了多模态肿瘤分割有效性，vMambaX展示出在肺癌分析中的高效性和可扩展性。

Abstract: Accurate lung tumor segmentation is vital for improving diagnosis and
treatment planning, and effectively combining anatomical and functional
information from PET and CT remains a major challenge. In this study, we
propose vMambaX, a lightweight multimodal framework integrating PET and CT scan
images through a Context-Gated Cross-Modal Perception Module (CGM). Built on
the Visual Mamba architecture, vMambaX adaptively enhances inter-modality
feature interaction, emphasizing informative regions while suppressing noise.
Evaluated on the PCLT20K dataset, the model outperforms baseline models while
maintaining lower computational complexity. These results highlight the
effectiveness of adaptive cross-modal gating for multimodal tumor segmentation
and demonstrate the potential of vMambaX as an efficient and scalable framework
for advanced lung cancer analysis. The code is available at
https://github.com/arco-group/vMambaX.

</details>


### [66] [Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds](https://arxiv.org/abs/2510.27533)
*Khandoker Ashik Uz Zaman,Mohammad Zahangir Alam,Mohammed N. M. Ali,Mahdi H. Miraz*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的3D点云数字水印方法，有效提升了水印在多种攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着三维内容在数字媒体中的快速发展，知识产权保护变得更加重要。传统水印方法难以应对3D点云特有的几何和非几何攻击，因此亟需更为鲁棒的水印技术。

Method: 作者将二进制水印信息嵌入点云块的奇异值（通过SVD分解获得），并利用基于PointNet++的深度神经网络提取水印。网络专门训练以增强在多种破坏性操作（旋转、缩放、噪声、裁剪、信号畸变）后的水印识别能力。

Result: 在ModelNet40公开数据集上实验表明，该深度学习水印提取方法在极端条件下大幅超过传统SVD方法。例如，在70%裁剪攻击下，基于深度学习的比特精度达0.83、IoU达0.80，明显优于SVD的0.58和0.26。

Conclusion: 提出的基于深度学习的3D点云水印方法在各种失真下表现出更高的鲁棒性和准确性，为三维内容的版权保护和所有权验证提供了有效的新方案。

Abstract: The protection of intellectual property has become critical due to the rapid
growth of three-dimensional content in digital media. Unlike traditional images
or videos, 3D point clouds present unique challenges for copyright enforcement,
as they are especially vulnerable to a range of geometric and non-geometric
attacks that can easily degrade or remove conventional watermark signals. In
this paper, we address these challenges by proposing a robust deep neural
watermarking framework for 3D point cloud copyright protection and ownership
verification. Our approach embeds binary watermarks into the singular values of
3D point cloud blocks using spectral decomposition, i.e. Singular Value
Decomposition (SVD), and leverages the extraction capabilities of Deep Learning
using PointNet++ neural network architecture. The network is trained to
reliably extract watermarks even after the data undergoes various attacks such
as rotation, scaling, noise, cropping and signal distortions. We validated our
method using the publicly available ModelNet40 dataset, demonstrating that deep
learning-based extraction significantly outperforms traditional SVD-based
techniques under challenging conditions. Our experimental evaluation
demonstrates that the deep learning-based extraction approach significantly
outperforms existing SVD-based methods with deep learning achieving bitwise
accuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD
achieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,
which is the most severe geometric distortion in our experiment. This
demonstrates our method's ability to achieve superior watermark recovery and
maintain high fidelity even under severe distortions.

</details>


### [67] [MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map Images and Time Series](https://arxiv.org/abs/2510.27547)
*Xue Xia,Randall Balestriero,Tao Zhang,Yixin Zhou,Andrew Ding,Dev Saini,Lorenz Hurni*

Main category: cs.CV

TL;DR: 本文提出了一种统一框架MapSAM2，能高效自动分割历史地图图像及其时序数据，从而显著降低构建空间-时间数据集的人工成本。


<details>
  <summary>Details</summary>
Motivation: 历史地图对地理特征的时空记录具有重要价值，但因风格多样和标注数据稀缺，自动分析极具挑战，尤其是将多时点地图信息融合需要大量人力。然而，这样的数据集对建筑日期判定、道路与居民地时序变化分析以及环境研究等应用至关重要。

Method: MapSAM2框架基于视觉基础模型，采用少样本微调以适应不同分割任务。创新性地将地图图像与时序数据统一视为视频处理：针对单幅地图，将其分割为若干小块，并利用“视频”的记忆注意力机制整合上下文增强分割精度；针对时序数据，构建了带注释的Siegfried建筑时序数据集，并提出通过模拟常见变化，从单年地图生成伪时序数据，以降低标注成本。

Result: 实验显示，MapSAM2能有效学习时间关联，在有限标注或仅用伪时序视频数据时，也能高精度地对历史地图中建筑等目标进行分割与跨时点关联。

Conclusion: MapSAM2为历史地图及其时序分析提供了一种高效、泛化性强的自动化工具，有望推动空间-时序地理信息数据构建与相关领域研究发展。数据集与代码将公开共享，促进后续研究。

Abstract: Historical maps are unique and valuable archives that document geographic
features across different time periods. However, automated analysis of
historical map images remains a significant challenge due to their wide
stylistic variability and the scarcity of annotated training data. Constructing
linked spatio-temporal datasets from historical map time series is even more
time-consuming and labor-intensive, as it requires synthesizing information
from multiple maps. Such datasets are essential for applications such as dating
buildings, analyzing the development of road networks and settlements, studying
environmental changes etc. We present MapSAM2, a unified framework for
automatically segmenting both historical map images and time series. Built on a
visual foundation model, MapSAM2 adapts to diverse segmentation tasks with
few-shot fine-tuning. Our key innovation is to treat both historical map images
and time series as videos. For images, we process a set of tiles as a video,
enabling the memory attention mechanism to incorporate contextual cues from
similar tiles, leading to improved geometric accuracy, particularly for areal
features. For time series, we introduce the annotated Siegfried Building Time
Series Dataset and, to reduce annotation costs, propose generating pseudo time
series from single-year maps by simulating common temporal transformations.
Experimental results show that MapSAM2 learns temporal associations effectively
and can accurately segment and link buildings in time series under limited
supervision or using pseudo videos. We will release both our dataset and code
to support future research.

</details>


### [68] [Image Hashing via Cross-View Code Alignment in the Age of Foundation Models](https://arxiv.org/abs/2510.27584)
*Ilyass Moummad,Kawtar Zaher,Hervé Goëau,Alexis Joly*

Main category: cs.CV

TL;DR: 本文提出了一种高效简洁的二值编码方法CroVCA，通过对齐语义一致视图下的二值码，实现高效大型检索，结合轻量级HashCoder网络显著减少训练时间，并在多项基准任务上达到了先进性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模检索任务中，如何获得既紧凑又判别力强的特征表征是关键。尽管基础模型能提供强大的嵌入表示，但高维特征的最近邻搜索存在计算瓶颈，传统二值编码方法方案复杂、训练耗时，且常针对特定范式优化，灵活性不足，亟需更通用高效的解决方案。

Method: 作者提出CroVCA原理，主张让语义一致的不同视图拥有一致的二值编码，通过单一的二元交叉熵损失实现对齐，并用编码率最大化作正则以防止坍塌。同时设计了轻量化MLP网络HashCoder并加最后一层BN，保证码的平衡性，可作为预训练特征的探测头或结合LoRA进行定制微调。

Result: 在多项公开数据集上，CroVCA仅需5个训练周期即达SOTA，16比特时效果突出。如COCO上的无监督哈希训练仅需2分钟，ImageNet100的有监督哈希约3分钟即可完成，均在单卡上实现，展示了极高的效率和适应性。

Conclusion: CroVCA提出了简单、高效、通用的二值编码学习框架，兼顾了效率、表现与易用性，适合多种视觉检索场景，对大规模检索系统具有广泛应用潜力。

Abstract: Efficient large-scale retrieval requires representations that are both
compact and discriminative. Foundation models provide powerful visual and
multimodal embeddings, but nearest neighbor search in these high-dimensional
spaces is computationally expensive. Hashing offers an efficient alternative by
enabling fast Hamming distance search with binary codes, yet existing
approaches often rely on complex pipelines, multi-term objectives, designs
specialized for a single learning paradigm, and long training times. We
introduce CroVCA (Cross-View Code Alignment), a simple and unified principle
for learning binary codes that remain consistent across semantically aligned
views. A single binary cross-entropy loss enforces alignment, while coding-rate
maximization serves as an anti-collapse regularizer to promote balanced and
diverse codes. To implement this, we design HashCoder, a lightweight MLP
hashing network with a final batch normalization layer to enforce balanced
codes. HashCoder can be used as a probing head on frozen embeddings or to adapt
encoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achieves
state-of-the-art results in just 5 training epochs. At 16 bits, it particularly
well-for instance, unsupervised hashing on COCO completes in under 2 minutes
and supervised hashing on ImageNet100 in about 3 minutes on a single GPU. These
results highlight CroVCA's efficiency, adaptability, and broad applicability.

</details>


### [69] [ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning](https://arxiv.org/abs/2510.27599)
*Samarup Bhattacharya,Anubhab Bhattacharya,Abir Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了一种名为ANCHOR的新方法，通过对抗性训练结合对比学习和难例挖掘，提升神经网络在对抗攻击下的鲁棒性与准确率。


<details>
  <summary>Details</summary>
Motivation: 神经网络易受对抗性攻击，只需对输入做微小无感知扰动即可误导模型，现有对抗训练虽然提升了鲁棒性，但通常牺牲干净样本的准确率。

Method: 提出Adversarially-trained Contrastive Hard-mining for Optimized Robustness（ANCHOR）框架，结合有监督的对比学习与显式的难正样本挖掘，引导模型将原图、增强图和扰动图的嵌入聚集在一起，并与其他类别分离，从而学习到对抗性更强和更有结构的特征表示。

Result: 在CIFAR-10数据集上，ANCHOR在自然准确率和PGD-20（epsilon=0.031）对抗攻击下均取得优异表现，超越常规对抗训练方法。

Conclusion: 将对抗性指导与难例对比监督结合，有助于模型学习到更加规范且鲁棒的特征表示，有效缩小准确率与鲁棒性间的差距。

Abstract: Neural networks have changed the way machines interpret the world. At their
core, they learn by following gradients, adjusting their parameters step by
step until they identify the most discriminant patterns in the data. This
process gives them their strength, yet it also opens the door to a hidden flaw.
The very gradients that help a model learn can also be used to produce small,
imperceptible tweaks that cause the model to completely alter its decision.
Such tweaks are called adversarial attacks. These attacks exploit this
vulnerability by adding tiny, imperceptible changes to images that, while
leaving them identical to the human eye, cause the model to make wrong
predictions. In this work, we propose Adversarially-trained Contrastive
Hard-mining for Optimized Robustness (ANCHOR), a framework that leverages the
power of supervised contrastive learning with explicit hard positive mining to
enable the model to learn representations for images such that the embeddings
for the images, their augmentations, and their perturbed versions cluster
together in the embedding space along with those for other images of the same
class while being separated from images of other classes. This alignment helps
the model focus on stable, meaningful patterns rather than fragile gradient
cues. On CIFAR-10, our approach achieves impressive results for both clean and
robust accuracy under PGD-20 (epsilon = 0.031), outperforming standard
adversarial training methods. Our results indicate that combining adversarial
guidance with hard-mined contrastive supervision helps models learn more
structured and robust representations, narrowing the gap between accuracy and
robustness.

</details>


### [70] [Who Made This? Fake Detection and Source Attribution with Diffusion Features](https://arxiv.org/abs/2510.27602)
*Simone Bonechi,Paolo Andreini,Barbara Toniella Corradini*

Main category: cs.CV

TL;DR: 本文提出FRIDA框架，仅依赖于预训练扩散模型的内部特征，实现了无需微调即可识别伪造图片并溯源生成器，并优于现有检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式扩散模型的进步，人造图片越来越难以区分真假，引发真实性、版权和虚假信息等担忧。现有深度伪造检测器难以泛化到未知生成器，还需大量标注数据和频繁再训练，因此需要一种更通用高效的检测方法。

Method: 作者提出FRIDA框架，利用预训练扩散模型生成的内部特征，通过k近邻分类器进行伪造图片检测及一个紧凑神经网络模型实现生成器溯源，无需对扩散模型微调。

Result: 实验表明，基于扩散特征的k近邻分类器在不同生成器间具有最先进的检测表现，而紧凑神经网络可以准确区分生成源。

Conclusion: 扩散模型的特征天然包含特定生成器的信息，无需复杂模型也可实现高效、可解释的伪造图片取证与溯源，为合成图像取证提供了简单有效的新范式。

Abstract: The rapid progress of generative diffusion models has enabled the creation of
synthetic images that are increasingly difficult to distinguish from real ones,
raising concerns about authenticity, copyright, and misinformation. Existing
supervised detectors often struggle to generalize across unseen generators,
requiring extensive labeled data and frequent retraining. We introduce FRIDA
(Fake-image Recognition and source Identification via Diffusion-features
Analysis), a lightweight framework that leverages internal activations from a
pre-trained diffusion model for deepfake detection and source generator
attribution. A k-nearest-neighbor classifier applied to diffusion features
achieves state-of-the-art cross-generator performance without fine-tuning,
while a compact neural model enables accurate source attribution. These results
show that diffusion representations inherently encode generator-specific
patterns, providing a simple and interpretable foundation for synthetic image
forensics.

</details>


### [71] [Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2510.27606)
*Yuhong Liu,Beichen Zhang,Yuhang Zang,Yuhang Cao,Long Xing,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Spatial-SSRL的空间自监督强化学习范式，通过从普通图像中生成易于验证的空间任务信号，提高大型视觉-语言模型（LVLMs）的空间理解能力，在多个基准测试上显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 目前LVLM在空间理解方面表现较弱，现有的有监督微调和带可验证奖励的强化学习都依赖昂贵的监督或者受限的环境，影响了模型在空间智能方面的大规模提升。因此，亟需一种无需人工注释、可扩展的新方法来提升LVLM的空间推理能力。

Method: 作者设计了Spatial-SSRL，自监督地构造五种空间结构相关的伪任务（如碎片重排、方向识别、缺损修复、深度顺序判断、3D相对位置预测），这些任务可以直接从RGB或RGB-D图像自动获得“真值”答案，无需人工标注或额外工具。训练过程中，模型用这些任务信号作为奖励，提升空间推理能力。

Result: 在七个涵盖图像和视频的空间理解基准测试上，Spatial-SSRL分别在3B和7B规模模型中将准确率提升了4.63%和3.89%，优于Qwen2.5-VL等主流基线模型。

Conclusion: 简单的自监督空间伪任务能够为强化学习提供可扩展的验证信号，无需昂贵的标注，极大增强了LVLM的空间智能，为大规模空间理解任务提供了实际可行的方案。

Abstract: Spatial understanding remains a weakness of Large Vision-Language Models
(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement
learning with verifiable rewards (RLVR) pipelines depend on costly supervision,
specialized tools, or constrained environments that limit scale. We introduce
Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals
directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically
formulates five pretext tasks that capture 2D and 3D spatial structure:
shuffled patch reordering, flipped patch recognition, cropped patch inpainting,
regional depth ordering, and relative 3D position prediction. These tasks
provide ground-truth answers that are easy to verify and require no human or
LVLM annotation. Training on our tasks substantially improves spatial reasoning
while preserving general visual capabilities. On seven spatial understanding
benchmarks in both image and video settings, Spatial-SSRL delivers average
accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our
results show that simple, intrinsic supervision enables RLVR at scale and
provides a practical route to stronger spatial intelligence in LVLMs.

</details>


### [72] [Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation](https://arxiv.org/abs/2510.27632)
*Riccardo Brioschi,Aleksandr Alekseev,Emanuele Nevali,Berkay Döner,Omar El Malki,Blagoj Mitrevski,Leandro Kieliger,Mark Collier,Andrii Maksai,Jesse Berent,Claudiu Musat,Efi Kokiopoulou*

Main category: cs.CV

TL;DR: 本论文提出了一种利用用户提供的草图作为指导约束，自动生成高质量图形版式的新方法，并发布了大规模合成草图数据集。


<details>
  <summary>Details</summary>
Motivation: 当前图形版式生成研究支持用户约束，但通常需要复杂设置，降低了易用性。作者旨在用简单直观的草图约束提升用户体验和生成效果。

Method: 采用多模态Transformer架构，将用户草图与内容资产作为输入，自动生成美观的版式。为解决训练数据匮乏，提出高效的大规模草图合成方法。

Result: 在PubLayNet、DocLayNet、SlidesVQA三大公开数据集上，提出模型在精度和直观性上均超越当前主流约束生成方法。

Conclusion: 草图约束为版式生成提供直观高效的新方向。作者方法既提升了性能，也简化了操作，并公布了大型草图数据集，推动领域发展。

Abstract: Graphic layout generation is a growing research area focusing on generating
aesthetically pleasing layouts ranging from poster designs to documents. While
recent research has explored ways to incorporate user constraints to guide the
layout generation, these constraints often require complex specifications which
reduce usability. We introduce an innovative approach exploiting user-provided
sketches as intuitive constraints and we demonstrate empirically the
effectiveness of this new guidance method, establishing the sketch-to-layout
problem as a promising research direction, which is currently under-explored.
To tackle the sketch-to-layout problem, we propose a multimodal
transformer-based solution using the sketch and the content assets as inputs to
produce high quality layouts. Since collecting sketch training data from human
annotators to train our model is very costly, we introduce a novel and
efficient method to synthetically generate training sketches at scale. We train
and evaluate our model on three publicly available datasets: PubLayNet,
DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art
constraint-based methods, while offering a more intuitive design experience. In
order to facilitate future sketch-to-layout research, we release O(200k)
synthetically-generated sketches for the public datasets above. The datasets
are available at https://github.com/google-deepmind/sketch_to_layout.

</details>


### [73] [VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images](https://arxiv.org/abs/2510.27646)
*Cesar H. Comin,Wesley N. Galvão*

Main category: cs.CV

TL;DR: 本文提出了VessShape方法，通过生成具有多样管状形态和丰富纹理的合成血管图像，提升了医学图像中血管语义分割的泛化能力和数据效率。预训练后仅需极少标注数据即可在多领域获得优秀效果。


<details>
  <summary>Details</summary>
Motivation: 血管语义分割因标注数据稀缺和跨模态泛化能力差受限。CNN更倾向于学习纹理特征，面对不同视觉特性的新领域时表现有限，因此需要新的方法提升模型利用形状信息的能力，从而提高其泛化性和数据效率。

Method: 提出VessShape方法，通过程序化方式生成包含多种管状（血管状）结构，以及不同前景与背景纹理的合成2D图像数据集，使分割模型在预训练阶段自动学习形状特征而非依赖纹理特征。将模型在VessShape上预训练后，再用极少量真实样本进行微调，并在多个现实数据集上验证分割效果。

Result: 预训练后的模型在两个来自不同领域的真实血管数据集上，仅用4至10个样本微调即可获得优秀的分割性能。同时具备较强的zero-shot能力，无需目标领域训练即可对新领域血管进行有效分割。

Conclusion: 结合强形状偏置的合成数据预训练，是解决血管分割中数据稀缺和跨领域泛化能力不足的有效途径。

Abstract: Semantic segmentation of blood vessels is an important task in medical image
analysis, but its progress is often hindered by the scarcity of large annotated
datasets and the poor generalization of models across different imaging
modalities. A key aspect is the tendency of Convolutional Neural Networks
(CNNs) to learn texture-based features, which limits their performance when
applied to new domains with different visual characteristics. We hypothesize
that leveraging geometric priors of vessel shapes, such as their tubular and
branching nature, can lead to more robust and data-efficient models. To
investigate this, we introduce VessShape, a methodology for generating
large-scale 2D synthetic datasets designed to instill a shape bias in
segmentation models. VessShape images contain procedurally generated tubular
geometries combined with a wide variety of foreground and background textures,
encouraging models to learn shape cues rather than textures. We demonstrate
that a model pre-trained on VessShape images achieves strong few-shot
segmentation performance on two real-world datasets from different domains,
requiring only four to ten samples for fine-tuning. Furthermore, the model
exhibits notable zero-shot capabilities, effectively segmenting vessels in
unseen domains without any target-specific training. Our results indicate that
pre-training with a strong shape bias can be an effective strategy to overcome
data scarcity and improve model generalization in blood vessel segmentation.

</details>


### [74] [NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception](https://arxiv.org/abs/2510.27647)
*Congzhang Shao,Quan Yuan,Guiyang Luo,Yue Hu,Danni Wang,Yilin Liu,Rui Pan,Bo Chen,Jinglin Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的协同感知方法NegoCollab，通过协商生成的公共特征空间，对不同模型间的特征进行高效对齐，提升多主体协同感知性能，特别适用于主体间有明显异质性的场景。


<details>
  <summary>Details</summary>
Motivation: 当前协同感知体系中，不同主体因为使用固定且不同的感知模型，造成了特征域差异，影响了信息共享与融合效果，降低了整体任务性能。已有方法强制对齐到单一主体的特征空间，使域差异大的主体难以对齐，效果不佳。

Method: 提出NegoCollab方法，训练时引入“negotitator（协商器）”，从各主体本地特征出发，生成契合所有主体的公共表征空间。采用sender-receiver结构，实现本地特征与公共特征的互相转化，并在训练时施加结构对齐损失、实用对齐损失和分布对齐损失，促进多模态特征空间的有效融合与知识蒸馏。

Result: NegoCollab能有效缩小各主体间的域间差异，并显著提升存在异质性下的协同感知性能。实验结果显示，相比传统单一公共空间对齐的方法，NegoCollab在多主体协同任务中表现更优。

Conclusion: NegoCollab为异质多主体协同感知问题提供了有效解决方案，通过协商性公共特征空间和配套损失函数，有效对齐多主体特征分布，改善感知性能，具有实际应用推广价值。

Abstract: Collaborative perception improves task performance by expanding the
perception range through information sharing among agents. . Immutable
heterogeneity poses a significant challenge in collaborative perception, as
participating agents may employ different and fixed perception models. This
leads to domain gaps in the intermediate features shared among agents,
consequently degrading collaborative performance. Aligning the features of all
agents to a common representation can eliminate domain gaps with low training
cost. However, in existing methods, the common representation is designated as
the representation of a specific agent, making it difficult for agents with
significant domain discrepancies from this specific agent to achieve proper
alignment. This paper proposes NegoCollab, a heterogeneous collaboration method
based on the negotiated common representation. It introduces a negotiator
during training to derive the common representation from the local
representations of each modality's agent, effectively reducing the inherent
domain gap with the various local representations. In NegoCollab, the mutual
transformation of features between the local representation space and the
common representation space is achieved by a pair of sender and receiver. To
better align local representations to the common representation containing
multimodal information, we introduce structural alignment loss and pragmatic
alignment loss in addition to the distribution alignment loss to supervise the
training. This enables the knowledge in the common representation to be fully
distilled into the sender.

</details>


### [75] [Gaussian Combined Distance: A Generic Metric for Object Detection](https://arxiv.org/abs/2510.27649)
*Ziqian Guan,Xieyi Fu,Pengjun Huang,Hengyuan Zhang,Hubin Du,Yongtao Liu,Yinglin Wang,Qang Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种新的Bounding Box相似度度量方法——Gaussian Combined Distance (GCD)，用于提升小目标检测的性能，在多个数据集上实现了SOTA表现，并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 当前目标检测主要依赖IoU作为相似度度量，但其对小目标检测不友好。虽然Wasserstein Distance被提出用以替代IoU，但它存在尺度不变性不足、优化收敛慢等问题。

Method: 作者提出Gaussian Combined Distance (GCD)作为bounding box的回归损失函数和标签分配度量。分析表明，GCD具备尺度不变性且支持center属性联合优化，从而加快了收敛速度并提高定位精度。

Result: 在AI-TOD-v2小目标数据集上，GCD在多种Detector上取得了SOTA成绩。在MS-COCO-2017和Visdrone-2019等中大目标检测数据集上，GCD同样优于Wasserstein Distance，表现出良好的通用性。

Conclusion: GCD是一种有效的bounding box度量方法，既提升了小目标检测能力，又具备良好泛化性，是IoU和Wasserstein Distance的有力替代方案。

Abstract: In object detection, a well-defined similarity metric can significantly
enhance model performance. Currently, the IoU-based similarity metric is the
most commonly preferred choice for detectors. However, detectors using IoU as a
similarity metric often perform poorly when detecting small objects because of
their sensitivity to minor positional deviations. To address this issue, recent
studies have proposed the Wasserstein Distance as an alternative to IoU for
measuring the similarity of Gaussian-distributed bounding boxes. However, we
have observed that the Wasserstein Distance lacks scale invariance, which
negatively impacts the model's generalization capability. Additionally, when
used as a loss function, its independent optimization of the center attributes
leads to slow model convergence and unsatisfactory detection precision. To
address these challenges, we introduce the Gaussian Combined Distance (GCD).
Through analytical examination of GCD and its gradient, we demonstrate that GCD
not only possesses scale invariance but also facilitates joint optimization,
which enhances model localization performance. Extensive experiments on the
AI-TOD-v2 dataset for tiny object detection show that GCD, as a bounding box
regression loss function and label assignment metric, achieves state-of-the-art
performance across various detectors. We further validated the generalizability
of GCD on the MS-COCO-2017 and Visdrone-2019 datasets, where it outperforms the
Wasserstein Distance across diverse scales of datasets. Code is available at
https://github.com/MArKkwanGuan/mmdet-GCD.

</details>


### [76] [Deep learning denoising unlocks quantitative insights in operando materials microscopy](https://arxiv.org/abs/2510.27667)
*Samuel Degnan-Morgenstern,Alexander E. Cohen,Rajeev Gopal,Megan Gober,George J. Nelson,Peng Bai,Martin Z. Bazant*

Main category: cs.CV

TL;DR: 本论文提出了一种基于无监督深度学习的去噪框架，可广泛应用于多种定量显微成像流程，通过降噪提升图像分辨率和分析准确性。实验和模拟结果证实深度去噪能兼顾物理真实性、最小偏差，并大幅降低不确定性，为噪声敏感的原位定量显微分析提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 随着原位显微技术在材料科学中的广泛应用，由于仪器噪声等因素，图像分辨率和定量分析能力受限，亟需提升去噪技术以更好挖掘材料功能机制。现有传统去噪方法或监督学习方法存在一定局限，故需开发更通用、可靠的去噪方法。

Method: 本文建立了一个无监督深度学习去噪框架，将其集成进多种显微成像技术的分析流程。通过模拟数据验证该方法对物理信息的保真性，并结合PDE约束优化降低建模不确定性。随后将该去噪方法应用于扫描透射X射线显微、石墨电极的光学显微，以及中子射线成像等多种实验平台。

Result: 模拟结果显示深度去噪能有效还原真实物理特征并极小引入偏差；在实验应用中，该框架提升了纳米化学和结构非均匀性的揭示能力，使得粒子自动分割和相分类等分析更为精准，并在中子成像中将由噪声引起的变异降低近80%。

Conclusion: 深度无监督去噪方法为多种原位定量成像技术带来通用、有效的噪声抑制能力，不仅提升图像品质，也拓展了许多噪声受限表征方法的应用范围，有助于推进功能性材料的深入研究。

Abstract: Operando microscopy provides direct insight into the dynamic chemical and
physical processes that govern functional materials, yet measurement noise
limits the effective resolution and undermines quantitative analysis. Here, we
present a general framework for integrating unsupervised deep learning-based
denoising into quantitative microscopy workflows across modalities and length
scales. Using simulated data, we demonstrate that deep denoising preserves
physical fidelity, introduces minimal bias, and reduces uncertainty in model
learning with partial differential equation (PDE)-constrained optimization.
Applied to experiments, denoising reveals nanoscale chemical and structural
heterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron
phosphate (LFP), enables automated particle segmentation and phase
classification in optical microscopy of graphite electrodes, and reduces
noise-induced variability by nearly 80% in neutron radiography to resolve
heterogeneous lithium transport. Collectively, these results establish deep
denoising as a powerful, modality-agnostic enhancement that advances
quantitative operando imaging and extends the reach of previously noise-limited
techniques.

</details>


### [77] [Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes](https://arxiv.org/abs/2510.27677)
*Bo Li,Duyuan Zheng,Xinyang Liu,Qingwen Li,Hong Li,Hongyan Cui,Ge Gao,Chen Liu*

Main category: cs.CV

TL;DR: 本文提出Sh-ViT（Shuffling Vision Transformer），一种轻量级且对遮挡与模糊更有鲁棒性的行人再识别方法，显著优于现有基线与方法。


<details>
  <summary>Details</summary>
Motivation: 在安防监控中的行人再识别常受遮挡、视角变化及图像质量影响，现有方法或过于复杂，或仅对清晰正面图像有效，因此需要一种简单又鲁棒的解决方案。

Method: 基于ViT-Base，引入最后一层的Shuffle模块打破空间相关性，提升对遮挡与模糊的鲁棒性；采用场景自适应数据增强（几何变换、擦除、模糊和颜色调整）模拟真实监控环境；通过DeiT知识蒸馏提升有限标签下的学习效果。同时，作者新构建了MyTT数据集，包含设备遮挡与视角变化场景。

Result: Sh-ViT在MyTT数据集上取得83.2%的Rank-1和80.1%的mAP，在Market1501上取得94.6%的Rank-1和87.5%的mAP，均超越了CNN、ViT基线与SOTA方法。

Conclusion: Sh-ViT无需外部复杂模块，能有效提升对遮挡和模糊行人的识别鲁棒性，为真实监控场景下的人员监测提供了简单实用的解决方案。

Abstract: Person re-identification (ReID) in surveillance is challenged by occlusion,
viewpoint distortion, and poor image quality. Most existing methods rely on
complex modules or perform well only on clear frontal images. We propose Sh-ViT
(Shuffling Vision Transformer), a lightweight and robust model for occluded
person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a
Shuffle module in the final Transformer layer to break spatial correlations and
enhance robustness to occlusion and blur; Second, scenario-adapted augmentation
(geometric transforms, erasing, blur, and color adjustment) to simulate
surveillance conditions; Third, DeiT-based knowledge distillation to improve
learning with limited labels.To support real-world evaluation, we construct the
MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base
station inspections, with frequent equipment occlusion and camera variations.
Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,
outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on
Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves
robustness to occlusion and blur without external modules, offering a practical
solution for surveillance-based personnel monitoring.

</details>


### [78] [PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting](https://arxiv.org/abs/2510.27680)
*Danyal Maqbool,Changhee Lee,Zachary Huemann,Samuel D. Church,Matthew E. Larson,Scott B. Perlman,Tomas A. Romero,Joshua D. Warner,Meghan Lubner,Xin Tie,Jameson Merkow,Junjie Hu,Steve Y. Cho,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 本论文提出了一种3D医学影像-文本模型（PETAR-4B），能够生成更准确的PET/CT检查报告。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在多模态推理中表现突出，但医学领域应用主要局限于2D图像，尤其是3D PET/CT因数据体积大、病灶分散、报告复杂而挑战较大。作者希望扩展VLMs至该领域，以提升3D影像自动报告的质量。

Method: 作者构建了包含超11,000条病灶级描述及3D分割的大型PET/CT数据集，通过规则和大语言模型混合的方法自动提取。基于此，提出PETAR-4B模型，将PET、CT及病灶轮廓融合，并采用空间感知方法进行报告生成，实现整体推理及病灶定位的结合。

Result: 通过自动评价和人工评价，PETAR-4B在PET/CT报告生成质量上显著优于以往方法，能更准确地产生具有临床一致性和定位能力的报告。

Conclusion: PETAR-4B模型推动了3D医学视觉-语言理解的发展，有望改善3D PET/CT检查的自动报告和诊断流程。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
multimodal reasoning, yet most medical applications remain limited to 2D
imaging. In this work, we extend VLMs to 3D positron emission tomography and
computed tomography (PET/CT), a domain characterized by large volumetric data,
small and dispersed lesions, and lengthy radiology reports. We introduce a
large-scale dataset comprising over 11,000 lesion-level descriptions paired
with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid
rule-based and large language model (LLM) pipeline. Building upon this dataset,
we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,
CT, and lesion contours for spatially grounded report generation. PETAR bridges
global contextual reasoning with fine-grained lesion awareness, producing
clinically coherent and localized findings. Comprehensive automated and human
evaluations demonstrate that PETAR substantially improves PET/CT report
generation quality, advancing 3D medical vision-language understanding.

</details>


### [79] [Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals](https://arxiv.org/abs/2510.27684)
*Xiangyu Fan,Zesong Qiu,Zhuguanyu Wu,Fanzhou Wang,Zhiqian Lin,Tianxiang Ren,Dahua Lin,Ruihao Gong,Lei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的蒸馏方法Phased DMD，通过分阶段分布匹配和子区间得分匹配，提升了基于得分的生成模型的多步蒸馏效果，实现了更高的模型能力和生成多样性。


<details>
  <summary>Details</summary>
Motivation: 一阶生成蒸馏模型受到模型容量限制，难以应对复杂生成任务（如文本生成视频中复杂运动）。直接多步扩展虽可提升性能，却显著增加存储和计算成本且带来不稳定性，现有的梯度截断策略又会损失生成多样性，因此需要一种兼顾效率、多样性与能力的方案。

Method: 提出Phased DMD，多步蒸馏框架，将SNR区间分割为子区间，采用逐步细化（phase-wise）分布匹配和区间内得分匹配，借鉴专家混合（MoE）思想，通过阶段性学习降低训练难度，并严密推导了每个子区间的训练目标。

Result: 在Qwen-Image和Wan2.2等大规模图像和视频生成模型上验证，Phased DMD比标准DMD在保持核心生成能力的同时，显著提升了输出结果的多样性。

Conclusion: Phased DMD有效提升了多步蒸馏时的模型表达能力、多样性和训练效率，为高效且多样的生成模型蒸馏提供了新方向。

Abstract: Distribution Matching Distillation (DMD) distills score-based generative
models into efficient one-step generators, without requiring a one-to-one
correspondence with the sampling trajectories of their teachers. However,
limited model capacity causes one-step distilled models underperform on complex
generative tasks, e.g., synthesizing intricate object motions in text-to-video
generation. Directly extending DMD to multi-step distillation increases memory
usage and computational depth, leading to instability and reduced efficiency.
While prior works propose stochastic gradient truncation as a potential
solution, we observe that it substantially reduces the generation diversity of
multi-step distilled models, bringing it down to the level of their one-step
counterparts. To address these limitations, we propose Phased DMD, a multi-step
distillation framework that bridges the idea of phase-wise distillation with
Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model
capacity. Phased DMD is built upon two key ideas: progressive distribution
matching and score matching within subintervals. First, our model divides the
SNR range into subintervals, progressively refining the model to higher SNR
levels, to better capture complex distributions. Next, to ensure the training
objective within each subinterval is accurate, we have conducted rigorous
mathematical derivations. We validate Phased DMD by distilling state-of-the-art
image and video generation models, including Qwen-Image (20B parameters) and
Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD
preserves output diversity better than DMD while retaining key generative
capabilities. We will release our code and models.

</details>


### [80] [LifWavNet: Lifting Wavelet-based Network for Non-contact ECG Reconstruction from Radar](https://arxiv.org/abs/2510.27692)
*Soumitra Kundu,Gargi Panda,Saumik Bhattacharya,Aurobinda Routray,Rajlakshmi Guha*

Main category: cs.CV

TL;DR: 本文提出了一种基于雷达信号的心电图（ECG）无接触重建新方法LifWavNet，显著提升了重建精度，并在实际任务中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 无接触的心电图监测能够实现更便利和舒适的心脏健康管理，但现有基于雷达信号的ECG重建方法在波形还原和生理意义保留上仍存在不足。

Method: 作者提出LifWavNet网络，采用多分辨率分析与合成模型（MRAS），突破传统固定小波的局限，通过可学习的小波提升算子自动适应雷达信号特征，并引入了多分辨率STFT损失函数，从时域和频域提升生成心电信号与真实信号的一致性。

Result: 在两个公开数据集上的实验表明，LifWavNet无论在ECG波形重建还是心率、心率变异性等生命体征估计方面都优于目前主流方法，并具备良好的中间特征可视化与可解释性。

Conclusion: LifWavNet为基于雷达的无接触ECG测量提供了有效和鲁棒的新框架，推动了相关技术在实际医疗健康场景的应用潜力。

Abstract: Non-contact electrocardiogram (ECG) reconstruction from radar signals offers
a promising approach for unobtrusive cardiac monitoring. We present LifWavNet,
a lifting wavelet network based on a multi-resolution analysis and synthesis
(MRAS) model for radar-to-ECG reconstruction. Unlike prior models that use
fixed wavelet approaches, LifWavNet employs learnable lifting wavelets with
lifting and inverse lifting units to adaptively capture radar signal features
and synthesize physiologically meaningful ECG waveforms. To improve
reconstruction fidelity, we introduce a multi-resolution short-time Fourier
transform (STFT) loss, that enforces consistency with the ground-truth ECG in
both temporal and spectral domains. Evaluations on two public datasets
demonstrate that LifWavNet outperforms state-of-the-art methods in ECG
reconstruction and downstream vital sign estimation (heart rate and heart rate
variability). Furthermore, intermediate feature visualization highlights the
interpretability of multi-resolution decomposition and synthesis in
radar-to-ECG reconstruction. These results establish LifWavNet as a robust
framework for radar-based non-contact ECG measurement.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [81] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文研究了结合状态空间模型（SSM）与注意力机制的混合模型，分析了不同结构下的表现，并提出了一种基于数据增强的方法提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 混合SSM和注意力机制的模型表现优异，但其结构设计选择尚缺少系统性理解，因此需要探索其背后的原理并优化性能。

Method: 分析了SSM和注意力层串行和并行两种集成方式对记忆利用及总体性能的影响，并提出在训练数据中持续加入同义句以增强召回能力。

Result: 串行混合结构在短文本下效果更好， 并行结构在长文本下表现优异。数据增强法对不同基模型均有提升，并超过了仅依赖结构修改的提升。

Conclusion: 本研究加深了对SSM-注意力混合模型的理解，为不同应用场景下模型结构设计提供了指导，并证明数据增强是提升召回能力的有效手段。

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [82] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 该论文提出了一种在医疗领域中识别需要通报事件的方法，利用语义框架在电子病历非结构化文本中检索模式，尤其聚焦于家庭暴力的漏报问题，并在大规模数据上验证了方法的有效性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 家庭暴力事件在基层医疗系统的电子病历中常常存在漏报，及时、准确地识别这些事件对公共卫生管理与服务有重要意义。传统方法难以处理非结构化文本。该研究旨在发展一种有效识别机制。

Method: 方法基于语义框架定义细粒度事件模式，并在巴西葡语电子病历中的开放文本字段检索相关事件。总计定义了8个检索模式，并在2100万句电子病历文本中应用，结果由语言学家人工评估，计算模式检出精度。

Result: 方法在识别暴力报告方面表现出较高精度（0.726），并通过人工评审验证了其鲁棒性和有效性。

Conclusion: 提出的方法不仅精准高效，还具有透明、低碳、跨语言的优势，可推广到其他健康监测语境，推动NLP在公共卫生系统中更广泛、可解释、伦理的应用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [83] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 本文提出了MEDIQA-OE 2025共享任务，首次聚焦于从医患对话中抽取可操作的医疗医嘱，从多个团队广泛尝试不同方案及大语言模型，并公布了数据集与竞赛结果。


<details>
  <summary>Details</summary>
Motivation: 随着自动语音识别和摘要在临床文档中的广泛应用，如何将对话内容直接转化为可用于电子健康记录的医疗医嘱尚未被解决，如果能解决该问题，将大幅减轻临床医生的文档负担并直接影响患者后续治疗。

Method: 组织了MEDIQA-OE 2025共享任务，聚焦于从医患对话中抽取医疗医嘱。六支队伍参与，尝试了多种方法，包括使用封闭和开放权重的大语言模型。评比了各队方法的效果，并对比了模型表现。

Result: 六支队伍参与并提交了方法，使用不同语言模型并分别评测，最终生成了排行榜并发布了解决方案和数据集。

Conclusion: 本文首次提出结构化抽取医嘱的公开任务，为未来自动化医嘱生成设立了基准点，对医疗文档自动化具有重要推动作用。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [84] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

TL;DR: 本文提出了一种保护用户隐私的语义增强框架LOPSIDED，能在与大型语言模型对话时自动对敏感个人信息去标识化，有效防止隐私泄露且保证语义连贯。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的普及，用户在与大型语言模型互动时可能无意中泄露敏感个人身份信息，带来安全隐患。现有去标识方法常损害对话语义与体验，亟需在保护隐私与保持语义完整间寻求平衡。

Method: LOPSIDED是一种基于语义的隐私保护代理，在用户输入时自动检测并用语义匹配的假名替换敏感实体，保持上下文连贯。模型处理后再自动还原假名，确保用户输出准确无误。评估部分使用ShareGPT真实对话数据集，并注释实体语义相关性用于实验分析。

Result: LOPSIDED在保持用户隐私的同时，比基线方法降低了5倍的语义效用错误率，显著提升了隐私保护效果且未牺牲对话语义连贯性。

Conclusion: LOPSIDED有效实现了对话隐私保护与语义完整性的兼顾，为安全可信的对话式AI系统落地提供了可行方向。

Abstract: With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [85] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

TL;DR: 本文提出了一种通过小型对齐模型在推理时对大型语言模型进行代理式对齐的新方法，不需昂贵的训练开销，实验表明该方法在任务性能和推理速度方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力的提升，其对下游任务、风格等需求的对齐过程耗费的计算资源骤增，现有对齐方法成本极高，需要更高效的对齐方案。

Method: 采用一种新的代理式推理时对齐方法，即利用一个小型已对齐模型在推理阶段实时指导大型模型的输出。具体地，在生成每个token时以级联方式决策，并将该决策问题形式化为0-1背包问题，推导出最优决策的原始和对偶近似算法。

Result: 实验显示，该方法在下游任务表现及推理速度（speculative decoding）的提升上均优于未采用此方法的基线。

Conclusion: 代理式推理时对齐为大型语言模型提供了一种无须昂贵训练开销、易于扩展的高效对齐思路，有望在实际应用中推广。

Abstract: Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [86] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: 提出了Elastic Language Model（ELM），一种用于紧凑型语言模型的新型神经架构搜索方法，通过更灵活的搜索空间和新的知识蒸馏损失，有效发现更高效、性能更优的小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型虽然在自然语言理解任务中表现优异，但其高昂的计算和内存需求引发了经济与环境方面的担忧。论文旨在通过自动设计更紧凑高效的模型以缓解这些问题。

Method: 提出Elastic Language Model（ELM），扩展了传统神经架构搜索，通过灵活的搜索空间、自适应维度与头数的动态模块，以及新型知识蒸馏损失来优化搜索流程和效果。

Result: 在掩码语言建模和因果语言建模任务中，ELM搜索得到的模型在效率和表现上都显著优于已有方法。

Conclusion: ELM为发现高效、紧凑且性能优越的语言模型提供了新思路，有效提升了模型结构搜索的深度与质量，具有实际应用和推广价值。

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [87] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本文提出了第一个豪萨语性别歧视检测数据集，并探讨了使用机器学习和多语言模型检测该语言中性别歧视的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管在线性别歧视广泛存在，主流检测方法主要集中在高资源语言，造成低资源语言如豪萨语在性别歧视检测领域进展缓慢。

Method: 作者通过社区参与、定性编码和数据增强开发出豪萨语性别歧视数据集，并以两阶段用户研究（n=66）探讨其文化和语言表现。随后，研究对传统机器学习分类器和预训练多语言模型在豪萨语场景中的应用效能进行实验评估，特别关注少样本学习方法。

Result: 研究发现，准确捕捉文化细微差别尤其在含糊与惯用表达上存在困难，模型在这类情境下普遍假阳性较高。

Conclusion: 豪萨语性别歧视检测面临文化表达和语境理解的挑战。多语言模型具一定潜力，但需进一步优化处理文化和语言差异的问题。

Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


### [88] [Quantitative Intertextuality from the Digital Humanities Perspective: A Survey](https://arxiv.org/abs/2510.27045)
*Siyu Duan*

Main category: cs.CL

TL;DR: 本文综述了基于自然语言处理的定量互文性研究，梳理了数据、方法及应用现状，展望了未来发展。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术进步，文学理论中的互文性研究正快速向大规模与量化方向发展，因此需要对相关方法和应用进行系统梳理。

Method: 论文采用综述方式，整合多语言、多主题的数据，系统回顾从统计学到深度学习的互文性分析方法，并总结相应的工具及平台。

Result: 文章归纳了当前互文性研究所用的数据类型、分析方法，并总结了其在人文与社会科学领域中的应用，以及相关研究平台。

Conclusion: 借助计算机技术，未来互文性研究将更加精细、多元和大规模，有望推动AI与人文领域的跨学科研究。

Abstract: The connection between texts is referred to as intertextuality in literary
theory, which served as an important theoretical basis in many digital
humanities studies. Over the past decade, advancements in natural language
processing have ushered intertextuality studies into the quantitative age.
Large-scale intertextuality research based on cutting-edge methods has
continuously emerged. This paper provides a roadmap for quantitative
intertextuality studies, summarizing their data, methods, and applications.
Drawing on data from multiple languages and topics, this survey reviews methods
from statistics to deep learning. It also summarizes their applications in
humanities and social sciences research and the associated platform tools.
Driven by advances in computer technology, more precise, diverse, and
large-scale intertext studies can be anticipated. Intertextuality holds promise
for broader application in interdisciplinary research bridging AI and the
humanities.

</details>


### [89] [Recursive numeral systems are highly regular and easy to process](https://arxiv.org/abs/2510.27049)
*Ponrawee Prasertsom,Andrea Silvi,Jennifer Culbertson,Moa Johansson,Devdatt Dubhashi,Kenny Smith*

Main category: cs.CL

TL;DR: 本文利用最小描述长度（MDL）原则，提出递归数词系统的优越性不仅在于权衡词汇量和复杂度，还体现在规则性和加工复杂度上，比以往只依赖词汇量和形态句法复杂度的分析更准确区分自然语言和‘非自然但可构造’系统。


<details>
  <summary>Details</summary>
Motivation: 之前的研究认为递归数词系统在词汇表规模和形态句法复杂度之间实现了优化，但需要引入人为限制以排除‘非自然’系统，未能令人信服地只用自然语言特征解释最优系统的形成。本文指出，问题在于以往分析忽视了语言规则性的作用。

Method: 作者借鉴最小描述长度（MDL）理论，通过MDL化的规则性与加工复杂度指标，衡量并比较自然语言数词系统与前人提出的‘最优’递归数词系统，以及未发现于自然语言但理论上可能存在的系统。

Result: 结果显示，引入规则性和加工复杂度的度量后，可以更好地区分自然系统和非自然系统，也能解释为何以往工作需通过人为限制来排除‘不自然’系统；这些限制实际上来源于规则性的考虑。

Conclusion: 研究强调，未来针对语言最优化的研究需系统性引入‘规则性’这一维度，才能更科学地区分和解释自然语言系统与理论系统之间的异同。

Abstract: Previous work has argued that recursive numeral systems optimise the
trade-off between lexicon size and average morphosyntatic complexity (Deni\'c
and Szymanik, 2024). However, showing that only natural-language-like systems
optimise this tradeoff has proven elusive, and the existing solution has relied
on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025).
Here, we argue that this issue arises because the proposed trade-off has
neglected regularity, a crucial aspect of complexity central to human grammars
in general. Drawing on the Minimum Description Length (MDL) approach, we
propose that recursive numeral systems are better viewed as efficient with
regard to their regularity and processing complexity. We show that our
MDL-based measures of regularity and processing complexity better capture the
key differences between attested, natural systems and unattested but possible
ones, including "optimal" recursive numeral systems from previous work, and
that the ad-hoc constraints from previous literature naturally follow from
regularity. Our approach highlights the need to incorporate regularity across
sets of forms in studies that attempt to measure and explain optimality in
language.

</details>


### [90] [VISTA Score: Verification In Sequential Turn-based Assessment](https://arxiv.org/abs/2510.27052)
*Ashley Lewis,Andrew Perrault,Eric Fosler-Lussier,Michael White*

Main category: cs.CL

TL;DR: 本文提出了VISTA框架，用于评估对话系统中的事实性，通过逐条核查对话内容，提高了幻觉（虚假信息）检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI系统常出现“幻觉”问题，即输出与事实或语境不符的内容，影响其在需要高可靠性的场景中的部署。已有评估指标难以处理多轮对话或无法验证的信息，亟需更细致、透明的事实性评估方法。

Method: 提出VISTA框架，将每一轮对话助手的回复分解为最小原子事实声明，然后利用可信来源及对话历史进行验证，对无法验证的信息进行细分类（如主观、矛盾、缺乏证据等）。该框架支持在多轮对话和多种现有事实性数据集上评估，并与现有指标如FACTSCORE、LLM-as-Judge做了对比。

Result: 在AIS、BEGIN、FAITHDIAL、FADE四个benchmark及八种大语言模型上，VISTA在幻觉检测性能上大幅优于已有基线。人工评估也显示，VISTA的分解方式提升了标注者一致性，揭示了现有benchmark中的不一致问题。

Conclusion: VISTA将事实性视为对话中的动态属性，为多轮对话系统事实性提供了更透明和符合人类评判的度量方式，有助于提升对话AI在高可靠性场景下的应用价值。

Abstract: Hallucination--defined here as generating statements unsupported or
contradicted by available evidence or conversational context--remains a major
obstacle to deploying conversational AI systems in settings that demand factual
reliability. Existing metrics either evaluate isolated responses or treat
unverifiable content as errors, limiting their use for multi-turn dialogue. We
introduce VISTA (Verification In Sequential Turn-based Assessment), a framework
for evaluating conversational factuality through claim-level verification and
sequential consistency tracking. VISTA decomposes each assistant turn into
atomic factual claims, verifies them against trusted sources and dialogue
history, and categorizes unverifiable statements (subjective, contradicted,
lacking evidence, or abstaining). Across eight large language models and four
dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA
substantially improves hallucination detection over FACTSCORE and LLM-as-Judge
baselines. Human evaluation confirms that VISTA's decomposition improves
annotator agreement and reveals inconsistencies in existing benchmarks. By
modeling factuality as a dynamic property of conversation, VISTA offers a more
transparent, human-aligned measure of truthfulness in dialogue systems.

</details>


### [91] [LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints](https://arxiv.org/abs/2510.27054)
*Xiaofan Guo,Yaxuan Luan,Yue Kang,Xiangchen Song,Jinxu Guo*

Main category: cs.CL

TL;DR: 本文提出了一种结合多粒度记忆索引与不确定性估计的置信度控制方法，有效提升了复杂知识环境下检索增强生成的稳定性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法在复杂知识环境下存在覆盖度不足、结果不稳定、可靠性有限等问题。

Method: 构建了分层记忆结构，将知识表示分为不同粒度层次，实现从局部到全局的动态索引与检索；引入不确定性估计机制，显式约束和过滤低置信度路径，通过生成损失、熵约束和方差正则化组成统一置信度控制框架。

Result: 在多种参数、环境和数据结构下进行灵敏度测试和横向比较，方法在问答准确率、检索召回率、排序质量与事实一致性等方面均显著优于现有模型。

Conclusion: 多粒度索引与置信度控制相结合可有效提升检索增强生成模型的稳定性和可靠性，为大模型在复杂场景下的可控性提供了新策略和实证。

Abstract: This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.

</details>


### [92] [Detecting Data Contamination in LLMs via In-Context Learning](https://arxiv.org/abs/2510.27055)
*Michał Zawalski,Meriem Boubdir,Klaudia Bałazy,Besmira Nushi,Pablo Ribalta*

Main category: cs.CL

TL;DR: 该论文提出了一种实用且准确的方法——CoDeC，用于检测和量化大语言模型训练数据遭到污染的情况。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在训练过程中容易‘记忆’训练数据，导致训练集污染现象，不利于模型的公正评估与实际应用。缺乏有效手段精准检测训练集和测试数据的交叉污染情况。

Method: 提出了CoDeC方法，通过检测‘上下文学习’（in-context learning）对模型表现的影响，区分模型是否‘记忆’了训练数据。具体来说，观察模型在遇到训练外数据时的置信度提升程度，以及在遇到训练内数据时因记忆被扰乱而可能出现的置信度降低现象。CoDeC能自动计算数据集的污染分数，适用于不同模型和数据。

Result: 实验表明，CoDeC方法所生成的污染分数可清晰区分训练内和训练外的数据集；在对一些训练数据未公开的开源模型测试时，发现了强烈的‘记忆’（污染）证据。

Conclusion: CoDeC是一种简单、自动化、模型与数据集无关的污染检测方法，易于集成到基准评测流程中，并能帮助揭示和量化现有大模型的训练数据污染现象。

Abstract: We present Contamination Detection via Context (CoDeC), a practical and
accurate method to detect and quantify training data contamination in large
language models. CoDeC distinguishes between data memorized during training and
data outside the training distribution by measuring how in-context learning
affects model performance. We find that in-context examples typically boost
confidence for unseen datasets but may reduce it when the dataset was part of
training, due to disrupted memorization patterns. Experiments show that CoDeC
produces interpretable contamination scores that clearly separate seen and
unseen datasets, and reveals strong evidence of memorization in open-weight
models with undisclosed training corpora. The method is simple, automated, and
both model- and dataset-agnostic, making it easy to integrate with benchmark
evaluations.

</details>


### [93] [Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models](https://arxiv.org/abs/2510.27077)
*Jiasen Zheng,Huajun Zhang,Xu Yan,Ran Hao,Chong Peng*

Main category: cs.CL

TL;DR: 本文提出了一种结合对比蒸馏与抗噪声训练的大模型微调方法，以提升大语言模型的安全性对齐与鲁棒性，实验结果显示在知识迁移、鲁棒性和安全性等方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在安全对齐和鲁棒性方面存在不足，容易受到噪声、不确定输入和分布转移的影响，亟需高效且参数友好的微调技术来提升其性能。

Method: 该方法在冻结主骨架模型的同时，将教师模型的知识边界通过蒸馏迁移给学生模型，并在训练过程中引入噪声扰动和鲁棒优化约束，整体优化目标由蒸馏损失、鲁棒性损失和正则化项组成，以实现对齐能力与抗干扰能力的平衡。

Result: 综合多角度实验，包括蒸馏权重敏感性、算力与混合精度下的稳定性、以及数据噪声和分布转移对性能的影响，结果显示该方法显著优于现有基线，在知识迁移、鲁棒性与安全性指标上表现最佳。

Conclusion: 本工作拓展了参数高效微调的理论体系，为构建更安全可信的大模型对齐机制提供了新方案。

Abstract: This paper addresses the limitations of large-scale language models in safety
alignment and robustness by proposing a fine-tuning method that combines
contrastive distillation with noise-robust training. The method freezes the
backbone model and transfers the knowledge boundaries of the teacher model to
the student model through distillation, thereby improving semantic consistency
and alignment accuracy. At the same time, noise perturbations and robust
optimization constraints are introduced during training to ensure that the
model maintains stable predictive outputs under noisy and uncertain inputs. The
overall framework consists of distillation loss, robustness loss, and a
regularization term, forming a unified optimization objective that balances
alignment ability with resistance to interference. To systematically validate
its effectiveness, the study designs experiments from multiple perspectives,
including distillation weight sensitivity, stability analysis under computation
budgets and mixed-precision environments, and the impact of data noise and
distribution shifts on model performance. Results show that the method
significantly outperforms existing baselines in knowledge transfer, robustness,
and overall safety, achieving the best performance across several key metrics.
This work not only enriches the theoretical system of parameter-efficient
fine-tuning but also provides a new solution for building safer and more
trustworthy alignment mechanisms.

</details>


### [94] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）中安全保护措施的选择性拒绝偏见，发现不同群体遭遇拒绝内容的概率存在差异，提出需提升各群体的公平特性。


<details>
  <summary>Details</summary>
Motivation: 安全保护旨在防止利用LLM大规模生成有害内容，然而这些措施可能无意间对不同人口群体产生不平等影响。作者关心LLM的安全性是否会对少数群体或特定群体产生新偏见。

Method: 作者通过分析LLM对各个及交叉人口群体（如性别、性取向、国籍、宗教）的有害内容请求的拒绝率，比较不同属性群体间被拒绝的概率、拒绝回复类型及回复长度等。

Result: 结果显示，LLM对于有害内容的拒绝出现了选择性偏见，在性别、性取向、国籍及宗教等属性上不同群体被拒绝的概率存在差异。还进行了间接攻击实验，发现可以针对先前被拒绝的群体绕过限制。

Conclusion: 现有LLM安全保护措施可能导致交叉或特定群体的不平等，未来应加强保护机制的公平性与鲁棒性，确保所有人口群体都能获得一致的保护。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


### [95] [Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks](https://arxiv.org/abs/2510.27106)
*Rajarshi Haldar,Julia Hockenmaier*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在自然语言生成评价中的一致性问题，发现其评分在不同运行间波动大，可靠性低，但经过合理规范仍有一定实用价值。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成（NLG）的广泛应用，传统自动评价指标难以与人类偏好保持一致，越来越多研究尝试利用LLM进行评价，但尚未系统分析其评价的一致性和可靠性。

Method: 作者通过实验证明：在不同任务和基准下，同一LLM多次评价同一生成结果时给出的分数波动较大，并定量分析了不同条件下这种不一致性。同时，探索在遵循特定规范条件下LLM评价能否更有用。

Result: 试验表明，LLM作为评审者在多次评分间自一致性低，存在很大方差，部分情况下评分接近随机。通过量化分析，这一现象适用于多种NLG任务和常用基准集。

Conclusion: 尽管LLM评价得分不一致、不够可靠，但在制定合理使用规范后，仍可以在一定程度上用于NLG系统的评估。

Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly
assessing it has become quite difficult. Lately, using large language models
(LLMs) for evaluating these generations has gained traction, as they tend to
align more closely with human preferences than conventional n-gram or
embedding-based metrics. In our experiments, we show that LLM judges have low
intra-rater reliability in their assigned scores across different runs. This
variance makes their ratings inconsistent, almost arbitrary in the worst case,
making it difficult to measure how good their judgments actually are. We
quantify this inconsistency across different NLG tasks and benchmarks and see
if judicious use of LLM judges can still be useful following proper guidelines.

</details>


### [96] [Probability Distributions Computed by Hard-Attention Transformers](https://arxiv.org/abs/2510.27118)
*Andy Yang,Anej Svete,Jiaoda Li,Anthony Widjaja Lin,Jonathan Rawski,Ryan Cotterell,David Chiang*

Main category: cs.CL

TL;DR: 本文系统性分析了transformer作为语言模型（用于自回归和概率生成文本）在表达能力方面的表现。不同于以往仅关注作为识别器（接受/拒绝字符串）能力的研究，作者首次探讨了transformer产生概率分布的表达范围和局限性。研究发现，将识别器改为自回归模型能增强表达能力，而引入概率机制会破坏一些非概率情况的等价性。


<details>
  <summary>Details</summary>
Motivation: 以往对transformer的表达能力分析大都局限于其作为语言识别器的表现，而实际应用中transformer多作为语言生成模型使用。了解transformer在实际用法下的表达极限有助于理论和应用的进一步发展。

Method: 作者对比分析了transformer作为识别器和作为概率自回归生成模型时的表达能力，系统研究了转向概率建模后表达能力的变化。提出具体例子和证明来展示自回归和概率性对表达性的影响。

Result: 得出两个主要发现：（1）transformer在自回归建模下比纯识别器拥有更强的表达性；（2）引入概率机制后，原有在非概率情况下成立的等价性可能不再成立。

Conclusion: 文章澄清了transformer在主流用法（语言建模）下表达能力的范围和变化，提出了避免将识别器分析结果直接套用到概率生成模型上的警示，对于更深入优化和理解transformer的语言建模效果具有重要理论价值。

Abstract: Most expressivity results for transformers treat them as language recognizers
(which accept or reject strings), and not as they are used in practice, as
language models (which generate strings autoregressively and
probabilistically). Here, we characterize the probability distributions that
transformer language models can express. We show that making transformer
language recognizers autoregressive can sometimes increase their expressivity,
and that making them probabilistic can break equivalences that hold in the
non-probabilistic case. Our overall contribution is to tease apart what
functions transformers are capable of expressing, in their most common use-case
as language models.

</details>


### [97] [Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+](https://arxiv.org/abs/2510.27183)
*Mason Shipton,York Hay Ng,Aditya Khan,Phuong Hanh Hoang,Xiang Lu,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 该论文通过引入新的书写系统向量、整合Glottolog和扩展谱系推断，极大充实了URIEL+语言知识库，显著减少特征稀疏性并提升低资源语言的多语言研究能力。


<details>
  <summary>Details</summary>
Motivation: URIEL+知识库在多语言研究及跨语言迁移任务中很有价值，但当前存在特征类型缺失、语言条目不全和谱系覆盖有限等数据稀疏问题，尤其影响对低资源语言的支持。

Method: 1. 为7,488种语言引入书写系统（script）向量，编码书写属性；2. 集成Glottolog数据库，增加18,710种新语言；3. 对26,449种语言扩展谱系特征推断，通过谱系关系补全语言类型和书写向量缺失。

Result: 1. 针对书写向量的特征稀疏性降低了14%；2. 总语言覆盖量提升至增加19,015种（扩展1007%）；3. 特征推断的质量指标上提升最高达33%；4. 跨语言迁移基准测试（关注低资源语言）部分情景下性能提升最高6%。

Conclusion: 本文使URIEL+知识库在多语言研究领域更加全面和包容，尤其增强了对低资源语言的覆盖与支持，对跨语言方法的研究具有重要推动作用。

Abstract: The URIEL+ linguistic knowledge base supports multilingual research by
encoding languages through geographic, genetic, and typological vectors.
However, data sparsity remains prevalent, in the form of missing feature types,
incomplete language entries, and limited genealogical coverage. This limits the
usefulness of URIEL+ in cross-lingual transfer, particularly for supporting
low-resource languages. To address this sparsity, this paper extends URIEL+
with three contributions: introducing script vectors to represent writing
system properties for 7,488 languages, integrating Glottolog to add 18,710
additional languages, and expanding lineage imputation for 26,449 languages by
propagating typological and script features across genealogies. These additions
reduce feature sparsity by 14% for script vectors, increase language coverage
by up to 19,015 languages (1,007%), and improve imputation quality metrics by
up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around
low-resource languages) shows occasionally divergent performance compared to
URIEL+, with performance gains up to 6% in certain setups. Our advances make
URIEL+ more complete and inclusive for multilingual research.

</details>


### [98] [MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://arxiv.org/abs/2510.27196)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Yayue Deng,Jing Ma*

Main category: cs.CL

TL;DR: 该文提出了一种名为MemeArena的新评测框架，使多模态大模型（mLLMs）对社交媒体恶意表情包的有害性理解更客观和全面。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中表情包广泛传播，传统多模态大模型评测方法仅关注二分类准确率，无法反映有害内容的复杂语境及解释深度，导致评测结果片面、缺乏公正性。

Method: 设计了MemeArena框架，引入模拟多种解释语境和视角的代理评审，通过观点整合与共识机制，对mLLMs的多模态有害性理解能力进行上下文相关和无偏见的评估。

Result: 实验证明，MemeArena显著减少了评审代理的评测偏倚，且评判结果与人类偏好高度一致，提升了mLLM有关多模态有害性理解的评测可靠性。

Conclusion: MemeArena为多模态大模型在有害内容理解方面的评估提供了公平、全面的新方案，有助于推动mLLM在安全性等实际应用领域的发展。

Abstract: The proliferation of memes on social media necessitates the capabilities of
multimodal Large Language Models (mLLMs) to effectively understand multimodal
harmfulness. Existing evaluation approaches predominantly focus on mLLMs'
detection accuracy for binary classification tasks, which often fail to reflect
the in-depth interpretive nuance of harmfulness across diverse contexts. In
this paper, we propose MemeArena, an agent-based arena-style evaluation
framework that provides a context-aware and unbiased assessment for mLLMs'
understanding of multimodal harmfulness. Specifically, MemeArena simulates
diverse interpretive contexts to formulate evaluation tasks that elicit
perspective-specific analyses from mLLMs. By integrating varied viewpoints and
reaching consensus among evaluators, it enables fair and unbiased comparisons
of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments
demonstrate that our framework effectively reduces the evaluation biases of
judge agents, with judgment results closely aligning with human preferences,
offering valuable insights into reliable and comprehensive mLLM evaluations in
multimodal harmfulness understanding. Our code and data are publicly available
at https://github.com/Lbotirx/MemeArena.

</details>


### [99] [Identifying the Periodicity of Information in Natural Language](https://arxiv.org/abs/2510.27241)
*Yulin Ou,Yu Wang,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 论文提出了一种检测自然语言信息周期性的新方法APS，发现人类语言的信息序列中存在显著的周期性模式，并且这些周期性不仅仅受传统结构单元（如句边界等）影响，还有其他更长距离的驱动因素。该方法对LLM生成文本的检测有潜力。


<details>
  <summary>Details</summary>
Motivation: 近期信息密度理论使得人们关注自然语言编码中的周期性问题。本文旨在揭示自然语言中信息周期性模式的存在及其影响因素。

Method: 作者提出了AutoPeriod of Surprisal（APS）方法，利用标准的周期性检测算法对单个文档的surprisal序列进行分析，从而识别其中显著的周期。该方法还结合了谐波回归建模，进一步确认了新发现的周期。

Result: 在多个语料上应用APS后，发现：1）人类语言中的信息表现出强周期性；2）存在超出传统语言结构单元的新型周期，这些周期通过谐波回归得到了验证。

Conclusion: 自然语言中的信息周期性是结构性因素与长距离驱动因素的共同结果。提出的方法不仅能够有效检测周期性，还有望用于大模型生成文本的识别。

Abstract: Recent theoretical advancement of information density in natural language has
brought the following question on desk: To what degree does natural language
exhibit periodicity pattern in its encoded information? We address this
question by introducing a new method called AutoPeriod of Surprisal (APS). APS
adopts a canonical periodicity detection algorithm and is able to identify any
significant periods that exist in the surprisal sequence of a single document.
By applying the algorithm to a set of corpora, we have obtained the following
interesting results: Firstly, a considerable proportion of human language
demonstrates a strong pattern of periodicity in information; Secondly, new
periods that are outside the distributions of typical structural units in text
(e.g., sentence boundaries, elementary discourse units, etc.) are found and
further confirmed via harmonic regression modeling. We conclude that the
periodicity of information in language is a joint outcome from both structured
factors and other driving factors that take effect at longer distances. The
advantages of our periodicity detection method and its potentials in
LLM-generation detection are further discussed.

</details>


### [100] [Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs](https://arxiv.org/abs/2510.27246)
*Mohammad Tavakoli,Alireza Salemi,Carrie Ye,Mohamed Abdalla,Hamed Zamani,J Ross Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一个新的生成和评测长对话和长程记忆能力的大语言模型（LLM）基准BEAM，并引入类人认知记忆框架LIGHT，实现模型在长对话记忆任务上的显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM长上下文能力评测基准存在叙事连贯性差、领域狭窄及任务简单等问题，影响LLM在真实长对话场景下的评估效果。

Method: 第一，自动生成长达千万token、主题多样且连贯的对话及针对多种记忆能力的探查问题，构建BEAM基准（含100组对话和2000个人工验证问题）；第二，提出LIGHT框架，受人类认知启发，基于三类记忆系统（长期情景记忆、短期工作记忆和速记本）增强LLM对长对话和关键事实的记忆与推理能力。

Result: 实验发现，无论是否采用检索增强，具备100万token上下文窗口的主流LLM在长对话中均表现欠佳；而应用LIGHT框架后，各类LLM的性能持续提升，平均提升幅度为3.5%至12.69%。消融实验也验证了三类记忆机制的独立贡献。

Conclusion: 该工作系统性提升了LLM长对话记忆任务评测的科学性和丰富度，并首次实证类人认知记忆机制对LLM长程推理和记忆能力提升的有效性，为未来研究提供更具挑战性和实用价值的工具。

Abstract: Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.

</details>


### [101] [Languages are Modalities: Cross-Lingual Alignment via Encoder Injection](https://arxiv.org/abs/2510.27254)
*Rajan Agarwal,Aarush Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种新方法LLINK，通过引入隐藏语言向量，将低资源、非拉丁语种更好地融入指令微调大模型，提升其跨语言表现而无需修改分词器或重新训练解码器。


<details>
  <summary>Details</summary>
Motivation: 目前，指令微调的大语言模型在低资源、非拉丁文字的处理上表现较差，主要原因在于分词器碎片化和弱跨语言耦合。需要一种效率高且不大幅修改模型结构的方法，提升这类语言的理解和生成能力。

Method: 作者提出LLINK方法。使用冻结的多语种编码器获取句子嵌入，通过轻量的对比投影器将其对齐到解码器的隐空间，并在特定槽位插入，利用最小化适配器训练，使冻结的解码器能接收这种跨语言信号，无须更改分词器或重新训练解码器。

Result: LLINK大幅提升了双语检索能力。在LLM评判的问答评测中，LLINK模型相比基础模型偏好度提升81.3%，相比直接finetune提升63.6%。结果还发现改进主要源自分词效率提升和更强的跨语种对齐能力，尽管在数字精度上还有不足。

Conclusion: 将低资源语种作为一种特定的模态注入大模型，是提升轻量级大模型跨语种处理能力的现实有效路径。

Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder's latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.

</details>


### [102] [MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](https://arxiv.org/abs/2510.27267)
*Kangkun Mao,Jinru Ding,Jiayuan Chen,Mouxiao Bian,Ruiyao Chen,Xinwei Peng,Sijie Ren,Linyang Li,Jie Xu*

Main category: cs.CL

TL;DR: 本文提出了MedCalc-Eval，这是一个针对大型语言模型（LLMs）医学计算能力的基准测试集，弥补了现有评测只侧重问答和描述性推理、忽略定量推理的问题。作者还开发了强化学习环境MedCalc-Env，并用Qwen2.5-32B模型在该环境下微调，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域针对LLMs的评测主要集中于问答和描述性推理，缺乏对临床决策中至关重要的定量计算能力的系统评估。现有数据集如MedCalc-Bench覆盖的计算任务有限，不能反映真实世界的临床计算需求。

Method: 作者构建了MedCalc-Eval大规模基准集，包含700多个涵盖公式类与规则类打分系统的任务，覆盖内科、外科、儿科、心脏科等多个领域。同时开发了MedCalc-Env强化学习环境，用于多步临床推理训练，并以此对Qwen2.5-32B模型进行微调。

Result: 经过MedCalc-Env环境的强化学习微调后，Qwen2.5-32B模型在MedCalc-Eval上的表现达到当前最优水平，尤其在数值敏感性、公式选择和推理鲁棒性方面有明显提升。

Conclusion: MedCalc-Eval有效拓展并提升了医学领域LLMs评测的广度和深度，但模型在单位换算、多条件逻辑和上下文理解等方面仍存在挑战。因此该基准和环境为进一步研究和提升医学计算智能提供了重要基础。

Abstract: As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.

</details>


### [103] [Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?](https://arxiv.org/abs/2510.27269)
*Deokhyung Kang,Seonjeong Hwang,Daehui Kim,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: RLMs在多语言推理中低资源语言表现差，主要原因是语言理解失败，提出检测理解失败并按需翻译的方法，显著缩小多语言推理差距。


<details>
  <summary>Details</summary>
Motivation: 尽管RLMs在复杂推理任务表现良好，但在低资源语言上推理能力明显较差，形成多语言推理差距。现有研究多专注于缩小这一差距，但对其原因研究不够。本文旨在探究造成该差距的根本原因，并寻找解决方案。

Method: 作者通过实验证明多语言推理差距主要源自RLM对多语言输入语义理解的失败，即推理过程中无法准确将多语言内容转化为主导语言（英语）。进一步，作者评估了多种理解失败的检测方法，发现监督式方法效果最佳。由此提出Selective Translation策略：仅在检测到理解失败时，将输入翻译为英语。

Result: 实验表明，Selective Translation策略能够在仅对约20%的输入进行翻译的情况下，实现接近全量翻译的推理性能，大幅度缩小多语言推理差距。

Conclusion: 本文揭示了多语言推理差距的主要原因在于理解失败，并证明该失败可以被检测和有选择地缓解。Selective Translation不仅有效缓解差距，还兼顾效率，为实现更公平的多语言推理提供了新思路。

Abstract: Reasoning language models (RLMs) achieve strong performance on complex
reasoning tasks, yet they still suffer from a multilingual reasoning gap,
performing better in high-resource languages than in low-resource ones. While
recent efforts have reduced this gap, its underlying causes remain largely
unexplored. In this paper, we address this by showing that the multilingual
reasoning gap largely stems from failures in language understanding-the model's
inability to represent the multilingual input meaning into the dominant
language (i.e., English) within its reasoning trace. This motivates us to
examine whether understanding failures can be detected, as this ability could
help mitigate the multilingual reasoning gap. To this end, we evaluate a range
of detection methods and find that understanding failures can indeed be
identified, with supervised approaches performing best. Building on this, we
propose Selective Translation, a simple yet effective strategy that translates
the multilingual input into English only when an understanding failure is
detected. Experimental results show that Selective Translation bridges the
multilingual reasoning gap, achieving near full-translation performance while
using translation for only about 20% of inputs. Together, our work demonstrates
that understanding failures are the primary cause of the multilingual reasoning
gap and can be detected and selectively mitigated, providing key insight into
its origin and a promising path toward more equitable multilingual reasoning.
Our code and data are publicly available at
https://github.com/deokhk/RLM_analysis.

</details>


### [104] [A Unified Representation Underlying the Judgment of Large Language Models](https://arxiv.org/abs/2510.27328)
*Yi-Long Lu,Jiajun Song,Wei Wang*

Main category: cs.CL

TL;DR: 本文发现大型语言模型（LLM）中评判判断并非来自于多个独立模块，而是依赖于一个统一的主导维度：价-同轴（Valence-Assent Axis, VAA）。该轴同时编码主观价值与对事实的同意，并控制模型生成一致性推理，但也引发系统性偏见和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 一直以来，生物和人工智能领域都关注判断是由特定模块还是统一资源驱动。之前LLM被解码出的不同概念神经表征暗示了模块化结构，但各表征是否真的互相独立存疑。

Method: 作者在多种LLM上分析、比较了不同类型评判相关的神经表征，发现它们沿同一主导维度（VAA）编码，并通过直接干预该轴来观察其对生成流程的影响。

Result: 在各类LLM中，不同的评判判断都是基于VAA这个核心轴完成；实验表明对VAA的干预能引导模型生成出与其评判状态一致的推理，即使牺牲事实性。

Conclusion: LLM的统一评判轴虽能保证推理一致性，但也会导致系统性的偏见和真假混淆。这一发现揭示了幻觉和推理失实背后的机制：推理过程被目标驱动的自洽性而非客观推断所主导。

Abstract: A central architectural question for both biological and artificial
intelligence is whether judgment relies on specialized modules or a unified,
domain-general resource. While the discovery of decodable neural
representations for distinct concepts in Large Language Models (LLMs) has
suggested a modular architecture, whether these representations are truly
independent systems remains an open question. Here we provide evidence for a
convergent architecture. Across a range of LLMs, we find that diverse
evaluative judgments are computed along a dominant dimension, which we term the
Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what
is good") and the model's assent to factual claims ("what is true"). Through
direct interventions, we show this unified representation creates a critical
dependency: the VAA functions as a control signal that steers the generative
process to construct a rationale consistent with its evaluative state, even at
the cost of factual accuracy. This mechanism, which we term the subordination
of reasoning, shifts the process of reasoning from impartial inference toward
goal-directed justification. Our discovery offers a mechanistic account for
systemic bias and hallucination, revealing how an architecture that promotes
coherent judgment can systematically undermine faithful reasoning.

</details>


### [105] [TransAlign: Machine Translation Encoders are Strong Word Aligners, Too](https://arxiv.org/abs/2510.27337)
*Benedikt Ebing,Christian Goldschmied,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文提出了一种新的单词对齐方法TransAlign，利用多语种机器翻译模型的编码器，提升了跨语言迁移中的标注投射精度，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数世界语言及NLP任务缺乏充足训练数据，现有跨语言迁移方法需要标注投射，传统单词对齐方法和机器翻译模型关联性强但效果有限，因此需要更有效的对齐手段。

Method: 提出TransAlign，一种基于多语种机器翻译模型编码器提取单词对齐的新方法，区别于传统用交叉注意力的方式，直接利用编码器提升对齐质量。

Result: TransAlign不仅在单词对齐任务上表现突出，还在基于机器翻译的跨语言迁移（特别是token分类任务）中显著优于主流单词对齐与非对齐标注投射方法。

Conclusion: TransAlign证明了多语种机器翻译模型编码器在单词对齐和跨语言迁移中的有效性，成为目前该领域更优的方法。

Abstract: In the absence of sizable training data for most world languages and NLP
tasks, translation-based strategies such as translate-test -- evaluating on
noisy source language data translated from the target language -- and
translate-train -- training on noisy target language data translated from the
source language -- have been established as competitive approaches for
cross-lingual transfer (XLT). For token classification tasks, these strategies
require label projection: mapping the labels from each token in the original
sentence to its counterpart(s) in the translation. To this end, it is common to
leverage multilingual word aligners (WAs) derived from encoder language models
such as mBERT or LaBSE. Despite obvious associations between machine
translation (MT) and WA, research on extracting alignments with MT models is
largely limited to exploiting cross-attention in encoder-decoder architectures,
yielding poor WA results. In this work, in contrast, we propose TransAlign, a
novel word aligner that utilizes the encoder of a massively multilingual MT
model. We show that TransAlign not only achieves strong WA performance but
substantially outperforms popular WA and state-of-the-art non-WA-based label
projection methods in MT-based XLT for token classification.

</details>


### [106] [ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations](https://arxiv.org/abs/2510.27355)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理框架ThoughtProbe，利用大语言模型(LLM)的隐藏推理特征来提升其推理能力，通过树结构探索答案空间并对分支进行评分与聚合，有效提升了在算术推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前不少研究尝试通过干预LLM的隐藏表示来引导其生成过程，但尚未充分利用隐藏特征帮助推理。作者希望通过挖掘这些特征，有效提升LLM在复杂推理任务中的表现。

Method: ThoughtProbe框架在推理时以树结构探索可能答案，每拓展一个节点时，利用分类器对分支进行评分和排序，优先分配计算资源给高分支；最终收集全部分支的答案，通过聚合分支的链式推理(CoT)分数，确定最佳答案。

Result: 实验显示，该方法能够全面覆盖有效的推理链，并准确识别最优解，在多个算术推理基准测试中取得了显著提升。

Conclusion: ThoughtProbe通过利用LLM隐藏推理特征实现答案空间的高效探索与聚合，为提升模型推理表现提供了新思路，并展现出强大的实际效能。

Abstract: This paper introduces ThoughtProbe, a novel inference time framework that
leverages the hidden reasoning features of Large Language Models (LLMs) to
improve their reasoning performance. Unlike previous works that manipulate the
hidden representations to steer LLM generation, we harness them as
discriminative signals to guide the tree structured response space exploration.
In each node expansion, a classifier serves as a scoring and ranking mechanism
that efficiently allocates computational resources by prioritizing higher score
candidates for continuation. After completing the tree expansion, we collect
answers from all branches to form a candidate answer pool. We then propose a
branch aggregation method that marginalizes over all supporting branches by
aggregating their CoT scores, thereby identifying the optimal answer from the
pool. Experimental results show that our framework's comprehensive exploration
not only covers valid reasoning chains but also effectively identifies them,
achieving significant improvements across multiple arithmetic reasoning
benchmarks.

</details>


### [107] [From the Rock Floor to the Cloud: A Systematic Survey of State-of-the-Art NLP in Battery Life Cycle](https://arxiv.org/abs/2510.27369)
*Tosin Adewumi,Martin Karlsson,Marcus Liwicki,Mikael Sjödahl,Lama Alkhaled,Rihab Gargouri,Nudrat Habib,Franz Hennie*

Main category: cs.CL

TL;DR: 本论文系统性综述了自然语言处理（NLP）在电池全生命周期中的应用，并提出了一种创新的技术语言处理（TLP）框架，用于支持欧盟数字电池护照(DBP)和电池相关预测。


<details>
  <summary>Details</summary>
Motivation: 目前有关电池领域的NLP研究多局限于某一个阶段或技术，缺乏全生命周期、系统性的综述。同时，随着DBP等政策的推进，对高效数据和文献挖掘的需求增加。

Method: 遵循PRISMA方法，从Google Scholar、IEEE Xplore和Scopus三个数据库系统检索，筛选并评审了274篇相关文献，最终详评66篇。公开相关材料以确保复现。提出并描述了包含智能体AI和优化提示词的TLP框架。

Result: 发现电池领域中新的NLP任务不断涌现，已在材料发现等生命期关键阶段取得进展。但目前仍存在如标准化基准缺失等显著挑战。TLP框架可用于解决部分问题。

Conclusion: NLP在电池全生命周期应用潜力巨大。提出的TLP框架有助于解决领域挑战，为实现DBP和更广泛电池预测提供支持。

Abstract: We present a comprehensive systematic survey of the application of natural
language processing (NLP) along the entire battery life cycle, instead of one
stage or method, and introduce a novel technical language processing (TLP)
framework for the EU's proposed digital battery passport (DBP) and other
general battery predictions. We follow the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable
databases or search engines, including Google Scholar, Institute of Electrical
and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we
assessed 274 scientific papers before the critical review of the final 66
relevant papers. We publicly provide artifacts of the review for validation and
reproducibility. The findings show that new NLP tasks are emerging in the
battery domain, which facilitate materials discovery and other stages of the
life cycle. Notwithstanding, challenges remain, such as the lack of standard
benchmarks. Our proposed TLP framework, which incorporates agentic AI and
optimized prompts, will be apt for tackling some of the challenges.

</details>


### [108] [Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs](https://arxiv.org/abs/2510.27400)
*Jiahao Liu,Zijian Wang,Kuo Zhao,Dong Hu*

Main category: cs.CL

TL;DR: 本文提出一种名为IntAttn-Edit的新方法，通过同时编辑大型语言模型（LLM）中的MLP模块和注意力（Attn）模块，以更有效地更新其事实知识。实验表明该方法相较于以往只修改MLP的策略，能够取得更高的编辑成功率、更好的泛化和知识保持能力。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑方法主要集中在MLP模块，而忽视了注意力模块在知识存储中的作用，这导致残留过时知识并限制了编辑效果。作者旨在弥补这一不足，提升编辑的全面性与有效性。

Method: 作者首先通过知识定位实验，分析LLM中不同模块对事实知识存储的贡献，发现注意力模块（尤其是早期层）具有重要作用。基于此，提出IntAttn-Edit方法，采用知识平衡策略，根据各模块贡献度分配参数更新幅度，实现MLP和Attn模块的联合编辑。

Result: 在标准基准测试上，该方法在编辑成功率、泛化及知识保持方面均优于现有方法。进一步分析显示，知识平衡策略可在不同设置下保持优化的编辑表现。

Conclusion: MLP和Attn模块都对知识存储有重要作用，同时编辑两者并采用平衡更新策略，可显著提升LLM知识编辑的有效性和稳定性。

Abstract: Knowledge editing has emerged as an efficient approach for updating factual
knowledge in large language models (LLMs). It typically locates knowledge
storage modules and then modifies their parameters. However, most existing
methods focus on the weights of multilayer perceptron (MLP) modules, which are
often identified as the main repositories of factual information. Other
components, such as attention (Attn) modules, are often ignored during editing.
This imbalance can leave residual outdated knowledge and limit editing
effectiveness. We perform comprehensive knowledge localization experiments on
advanced LLMs and find that Attn modules play a substantial role in factual
knowledge storage and retrieval, especially in earlier layers. Based on these
insights, we propose IntAttn-Edit, a method that extends the associative memory
paradigm to jointly update both MLP and Attn modules. Our approach uses a
knowledge balancing strategy that allocates update magnitudes in proportion to
each module's measured contribution to knowledge storage. Experiments on
standard benchmarks show that IntAttn-Edit achieves higher edit success, better
generalization, and stronger knowledge preservation than prior methods. Further
analysis shows that the balancing strategy keeps editing performance within an
optimal range across diverse settings.

</details>


### [109] [Awal -- Community-Powered Language Technology for Tamazight](https://arxiv.org/abs/2510.27407)
*Alp Öktem,Farida Boudichat*

Main category: cs.CL

TL;DR: 本文介绍了面向塔马齐格特语（Tamazight）语言资源建设的社区协作平台Awal，总结了过去18个月的进展、挑战与成果。


<details>
  <summary>Details</summary>
Motivation: Tamazight在数字空间中严重代表性不足，现有计算资源非常有限，传统众包方法在资源稀缺和标准化尚未完成的少数民族语言上效果有限，亟需探索新的社区驱动解决方案。

Method: 通过搭建awaldigital.org平台，召集Tamazight社区成员共同参与翻译和语音数据采集，并对18个月社区参与的数据、用户分布和贡献情况进行分析。

Result: 在18个月内收集到6,421个翻译对和3小时的语音数据，参与度主要集中在语言学家和活动家群体。普通社区成员由于书写自信心不足及标准化问题，参与度较低，表明面对复杂社会语言环境时，传统众包方法成效有限。

Conclusion: 积极的社区反馈虽然广泛，但贡献数量有限。改进的数据收集与参与激励模式以及更具包容性的技术和标准化推动工作，是促进Tamazight等低资源语言数字资源建设的关键。

Abstract: This paper presents Awal, a community-powered initiative for developing
language technology resources for Tamazight. We provide a comprehensive review
of the NLP landscape for Tamazight, examining recent progress in computational
resources, and the emergence of community-driven approaches to address
persistent data scarcity. Launched in 2024, awaldigital.org platform addresses
the underrepresentation of Tamazight in digital spaces through a collaborative
platform enabling speakers to contribute translation and voice data. We analyze
18 months of community engagement, revealing significant barriers to
participation including limited confidence in written Tamazight and ongoing
standardization challenges. Despite widespread positive reception, actual data
contribution remained concentrated among linguists and activists. The modest
scale of community contributions -- 6,421 translation pairs and 3 hours of
speech data -- highlights the limitations of applying standard crowdsourcing
approaches to languages with complex sociolinguistic contexts. We are working
on improved open-source MT models using the collected data.

</details>


### [110] [Dynamic Affective Memory Management for Personalized LLM Agents](https://arxiv.org/abs/2510.27418)
*Junfeng Lu,Yueyan Li*

Main category: cs.CL

TL;DR: 本文提出了一种面向情感场景的新型记忆管理系统，采用类贝叶斯记忆更新算法和记忆熵概念，动态优化AI代理的个性化记忆库，并通过新评测基准DABench验证其有效性。实验显示该系统在个性化、逻辑一致性和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化AI代理主要依赖外部记忆库，但存在记忆冗余、过时和上下文融合差等问题，且交互过程中缺乏高效的记忆动态更新机制。本文旨在解决这些挑战，提升个性化AI代理的长期记忆能力和情感适应能力。

Method: 提出类贝叶斯记忆更新算法，引入记忆熵概念，实现全局熵最小化自主动态维护记忆向量数据库。同时，建立DABench基准，关注情感表达和对象情感变化，作为系统评测手段。通过消融实验进一步检验算法有效性。

Result: 实验显示该系统在个性化程度、逻辑连贯性和准确度上明显优于传统方法。消融实验表明类贝叶斯更新机制可有效减缓记忆膨胀问题。

Conclusion: 新型动态记忆管理系统和类贝叶斯记忆更新机制可为个性化AI代理带来更优性能，尤其在长期、情感化交互场景下效果显著。本研究为构建更强大记忆集成机制提供了新的思路。

Abstract: Advances in large language models are making personalized AI agents a new
research focus. While current agent systems primarily rely on personalized
external memory databases to deliver customized experiences, they face
challenges such as memory redundancy, memory staleness, and poor memory-context
integration, largely due to the lack of effective memory updates during
interaction. To tackle these issues, we propose a new memory management system
designed for affective scenarios. Our approach employs a Bayesian-inspired
memory update algorithm with the concept of memory entropy, enabling the agent
to autonomously maintain a dynamically updated memory vector database by
minimizing global entropy to provide more personalized services. To better
evaluate the system's effectiveness in this context, we propose DABench, a
benchmark focusing on emotional expression and emotional change toward objects.
Experimental results demonstrate that, our system achieves superior performance
in personalization, logical coherence, and accuracy. Ablation studies further
validate the effectiveness of the Bayesian-inspired update mechanism in
alleviating memory bloat. Our work offers new insights into the design of
long-term memory systems.

</details>


### [111] [VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision](https://arxiv.org/abs/2510.27462)
*Xuan Gong,Senmiao Wang,Hanbo Huang,Ruoyu Sun,Shiyu Liang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法VCORE（基于方差控制的优化重加权），用于改进大模型在长链式推理任务中的监督微调过程。VCORE能够自适应调整不同token的训练重要性，从而提升模型的推理泛化能力，实验证明该方法优于现有重加权技术。


<details>
  <summary>Details</summary>
Motivation: 目前链式思维微调常用的交叉熵损失对所有token一视同仁，忽略了推理序列中不同token的重要性差异，容易导致监督信号分配不合理，影响模型复杂推理能力的泛化。

Method: 作者将链式推理过程中的监督视作一个带约束的优化问题，提出VCORE框架，通过优化理论自适应分配不同token的训练权重，使训练目标更加贴合提升推理泛化能力的要求。

Result: VCORE在数学与代码基准任务、Qwen3系列与LLaMA-3.1-8B-Instruct等模型上，在域内外均取得了显著优于其他重加权方法的表现。此外也验证了其作为后续强化学习的更优初始化方案。

Conclusion: VCORE方法为长链式推理的模型微调提供了更有效的token重加权框架，是提升大模型推理泛化能力和强化学习预训练效果的有力工具。

Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.

</details>


### [112] [Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning](https://arxiv.org/abs/2510.27469)
*Chenyang Shao,Sijian Ren,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 该论文结合扩散语言模型（DLMs）和大型语言模型（LLMs），提出了一种高效的协同推理方法，在保持推理能力的同时，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在推理任务上表现出强大能力，但其自回归生成方式导致推理所需计算量大且效率不高，尤其是生成中间“思路”时存在计算开销与性能提升不成正比的问题。

Method: 作者利用DLMs并行高效地产生多样化的中间推理步骤（thoughts），再用LLMs对这些候选思路进行评估筛选。这样既降低了自回归生成的计算压力，又保证了推理质量。并构建了协同推理框架进行实验验证。

Result: 在多个复杂推理基准测试上，该方法表现出较强性能，验证了框架的有效性和效率。

Conclusion: 提出的DLM-LLM协同推理框架可在降低算力消耗的同时保持高推理准确性，为后续相关研究提供了新方向。

Abstract: In recent years, large language models (LLMs) have witnessed remarkable
advancements, with the test-time scaling law consistently enhancing the
reasoning capabilities. Through systematic evaluation and exploration of a
diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to
generate deliberate reasoning steps, thereby substantially enhancing reasoning
accuracy. However, LLMs' autoregressive generation paradigm results in
reasoning performance scaling sub-optimally with test-time computation, often
requiring excessive computational overhead to propose thoughts while yielding
only marginal performance gains. In contrast, diffusion language models (DLMs)
can efficiently produce diverse samples through parallel denoising in a single
forward pass, inspiring us to leverage them for proposing intermediate
thoughts, thereby alleviating the computational burden associated with
autoregressive generation while maintaining quality. In this work, we propose
an efficient collaborative reasoning framework, leveraging DLMs to generate
candidate thoughts and LLMs to evaluate their quality. Experiments across
diverse benchmarks demonstrate that our framework achieves strong performance
in complex reasoning tasks, offering a promising direction for future research.
Our code is open-source at
https://anonymous.4open.science/r/Diffuse-Thinking-EC60.

</details>


### [113] [The aftermath of compounds: Investigating Compounds and their Semantic Representations](https://arxiv.org/abs/2510.27477)
*Swarang Joshi*

Main category: cs.CL

TL;DR: 研究比较了GloVe和BERT词嵌入与人类对英语复合词语义判断之间的一致性，发现BERT表现出更好的语义组合能力，预测性是语义透明度的重要因素。


<details>
  <summary>Details</summary>
Motivation: 随着词嵌入技术的发展，如何量化其与人类语义处理的一致性成为热点。尤其在复合词语义处理上，尚缺少系统性比较不同嵌入模型与人类认知表现的研究。

Method: 作者使用GloVe（静态词向量）和BERT（上下文嵌入）构建复合词相关嵌入指标，并结合心理语言学中的人类语料，对比词义优势（LMD）和语义透明度（ST）两项标准，采用斯皮尔曼相关和回归分析来评价嵌入和人类判断的一致性。

Result: BERT嵌入比GloVe更好地捕捉了复合词的组合语义；另外，无论在人类数据还是模型数据中，词语的可预测性都能很好地预测语义的透明度。

Conclusion: 本研究强化了BERT等上下文词嵌入用于语义建模的有效性，并揭示了复合词处理中影响语义透明度的关键因素，为计算心理语言学建模和理解人类词汇处理机制提供了新见解。

Abstract: This study investigates how well computational embeddings align with human
semantic judgments in the processing of English compound words. We compare
static word vectors (GloVe) and contextualized embeddings (BERT) against human
ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn
from a psycholinguistic dataset. Using measures of association strength
(Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC),
we compute embedding-derived LMD and ST metrics and assess their relationships
with human judgments via Spearmans correlation and regression analyses. Our
results show that BERT embeddings better capture compositional semantics than
GloVe, and that predictability ratings are strong predictors of semantic
transparency in both human and model data. These findings advance computational
psycholinguistics by clarifying the factors that drive compound word processing
and offering insights into embedding-based semantic modeling.

</details>


### [114] [Effect of Domain Generalization Techniques in Low Resource Systems](https://arxiv.org/abs/2510.27512)
*Mahi Aminu,Chisom Chibuike,Fatimo Adebanjo,Omokolade Awosanya,Samuel Oyeneye*

Main category: cs.CL

TL;DR: 该论文探讨了两种基于因果推断的领域泛化技术如何在低资源自然语言任务中提升模型对分布变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的训练数据和测试数据往往存在分布差异，特别是在低资源场景下数据稀缺，更难以保证模型泛化能力。因此，提升模型在未见领域上的鲁棒性成为关键问题。

Method: 论文研究了两种因果领域泛化（DG）方法：一是基于因果数据增强（CDA）技术，通过自动生成反事实样本扩充训练数据，以提升模型对虚假相关的鲁棒性；二是使用DINER框架进行不变因果表征学习（ICRL），将其适配到多语言环境，提升模型的跨语言泛化能力。

Result: 实验结果显示，反事实数据增强方法在情感分类任务上的跨域测试精度有稳定提升；DINER方法可提升多语言场景下的泛化能力，但不同语言提升幅度不一。

Conclusion: 两种基于因果推断的领域泛化方法均能有效提升模型在未见领域中的表现，对低资源自然语言处理任务具有实际应用价值。

Abstract: Machine learning models typically assume that training and test data follow
the same distribution, an assumption that often fails in real-world scenarios
due to distribution shifts. This issue is especially pronounced in low-resource
settings, where data scarcity and limited domain diversity hinder robust
generalization. Domain generalization (DG) approaches address this challenge by
learning features that remain invariant across domains, often using causal
mechanisms to improve model robustness. In this study, we examine two distinct
causal DG techniques in low-resource natural language tasks. First, we
investigate a causal data augmentation (CDA) approach that automatically
generates counterfactual examples to improve robustness to spurious
correlations. We apply this method to sentiment classification on the
NaijaSenti Twitter corpus, expanding the training data with semantically
equivalent paraphrases to simulate controlled distribution shifts. Second, we
explore an invariant causal representation learning (ICRL) approach using the
DINER framework, originally proposed for debiasing aspect-based sentiment
analysis. We adapt DINER to a multilingual setting. Our findings demonstrate
that both approaches enhance robustness to unseen domains: counterfactual data
augmentation yields consistent cross-domain accuracy gains in sentiment
classification, while causal representation learning with DINER improves
out-of-distribution performance in multilingual sentiment analysis, albeit with
varying gains across languages.

</details>


### [115] [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)
*Desta Haileselassie Hagos,Legand L. Burge,Anietie Andy,Anis Yazidi,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 本文提出了BiSparse-AAS框架，通过整合稀疏注意力、自适应跨度和双线性注意力，显著提升了长文本摘要的效率和表现。


<details>
  <summary>Details</summary>
Motivation: Transformer在文本摘要上表现优异，但由于注意力机制的二次复杂度，难以处理超长文本，且效率受限。作者旨在解决效率、可扩展性及长序列建模的难题。

Method: 提出BiSparse-AAS，整合了稀疏注意力（降低计算开销）、自适应跨度（动态调整注意力范围）、双线性注意力（建模复杂token交互），在保障效率的基础上提升建模能力。

Result: 在抽取式和生成式文本摘要任务上，BiSparse-AAS在CNN/DailyMail和XSum数据集上平均ROUGE提升分别为68.1%和52.6%，在OpenWebText和Gigaword上也表现优良。

Conclusion: BiSparse-AAS有效提升了文本摘要在长文档场景下的效率和效果，是面向实际应用的统一且高效的解决方案。

Abstract: Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.

</details>


### [116] [SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps](https://arxiv.org/abs/2510.27532)
*Neha Srikanth,Victor Bursztyn,Puneet Mathur,Ani Nenkova*

Main category: cs.CL

TL;DR: 本论文提出了一种名为SQLSpace的新型、可解释且简洁的text-to-SQL任务表示方法，并展示了其用于基准集分析、细粒度模型评估以及模型性能提升的多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的text-to-SQL基准集和模型分析主要依赖准确率等整体指标，缺乏对案例内部结构和难点的细致理解，也无法有效比较不同基准集间的异同，因此需要一种更细粒度、可解释的新工具来辅助benchmark分析和模型改进。

Method: 提出人类易解、紧凑的SQLSpace表示，将SQL查询结构性特征编码，实现最小人工干预下表征text-to-SQL案例。通过该表示，分别对比分析benchmark组成，细致评估模型表现，并基于正确性估计对输入查询重写以提升模型性能。

Result: SQLSpace有效区分并比较了主流text-to-SQL基准集之间的组成、难点差异，揭示了模型仅靠准确率难以发现的表现模式，通过针对性重写还提升了模型输出的正确性。

Conclusion: SQLSpace不仅提升了对text-to-SQL基准和模型特性的可解释性和分析能力，也为模型优化提供了新思路，显示出其在文本到SQL研究中的应用前景。

Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact
representation for text-to-SQL examples derived with minimal human
intervention. We demonstrate the utility of these representations in evaluation
with three use cases: (i) closely comparing and contrasting the composition of
popular text-to-SQL benchmarks to identify unique dimensions of examples they
evaluate, (ii) understanding model performance at a granular level beyond
overall accuracy scores, and (iii) improving model performance through targeted
query rewriting based on learned correctness estimation. We show that SQLSpace
enables analysis that would be difficult with raw examples alone: it reveals
compositional differences between benchmarks, exposes performance patterns
obscured by accuracy alone, and supports modeling of query success.

</details>


### [117] [Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design](https://arxiv.org/abs/2510.27535)
*Maria Lizarazo Jimenez,Ana Gabriela Claros,Kieran Green,David Toro-Tobon,Felipe Larios,Sheena Asthana,Camila Wenczenovicz,Kerly Guevara Maldonado,Luis Vilatuna-Andrango,Cristina Proano-Velez,Satya Sai Sri Bandi,Shubhangi Bagewadi,Megan E. Branda,Misk Al Zahidy,Saturnino Luz,Mirella Lapata,Juan P. Brito,Oscar J. Ponce-Ponte*

Main category: cs.CL

TL;DR: 本论文提出了“以患者为中心的摘要”（PCS）作为AI临床摘要任务的新标准，并评估了当前主流开源大语言模型在该任务上的表现。结果发现，模型在流利度和完整性上接近专家，但在正确性和患者中心性上仍不及人工。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的临床摘要多偏重生物医学信息，忽视了患者的偏好、价值观与关切，而实现以患者为中心的护理（PCC）需要包含这些信息。因此，设立新标准PCS以提升临床摘要的人文关怀和临床实用性。

Method: 通过混合方法研究，分别访谈患者和临床医师，建立摘要内容和结构的指南，并让八名医生基于指南生成88例金标准摘要。随后，使用五个主流开源LLM，通过zero-shot和few-shot提示生成摘要，并以ROUGE-L、BERTScore及定性指标评估模型表现。

Result: 患者最关注生活方式、社会支持、压力源及照护价值观；医生更重视功能、心理及情绪信息。Mistral-8B和Llama-3.1-8B在zero-shot表现最佳；few-shot下Llama-3.1-8B最佳。模型与专家在流利性和完整性相当，但在正确性和以患者为中心方面仍落后于人工摘要。

Conclusion: 当前开源LLM在生成以患者为中心的临床摘要上已表现出一定潜力，但要实现高质量的患者关怀和临床实用性，仍需进一步优化模型的患者中心性和正确性。

Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to
reach human-level performance in generating clinical summaries from
patient-clinician conversations. However, these summaries often focus on
patients' biology rather than their preferences, values, wishes, and concerns.
To achieve patient-centered care, we propose a new standard for Artificial
Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries
(PCS). Our objective was to develop a framework to generate PCS that capture
patient values and ensure clinical utility and to assess whether current
open-source LLMs can achieve human-level performance in this task. We used a
mixed-methods process. Two Patient and Public Involvement groups (10 patients
and 8 clinicians) in the United Kingdom participated in semi-structured
interviews exploring what personal and contextual information should be
included in clinical summaries and how it should be structured for clinical
use. Findings informed annotation guidelines used by eight clinicians to create
gold-standard PCS from 88 atrial fibrillation consultations. Sixteen
consultations were used to refine a prompt aligned with the guidelines. Five
open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and
Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot
prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients
emphasized lifestyle routines, social support, recent stressors, and care
values. Clinicians sought concise functional, psychosocial, and emotional
context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L
0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B
(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between
experts and models, while correctness and patient-centeredness favored human
PCS.

</details>


### [118] [DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models](https://arxiv.org/abs/2510.27543)
*Malik H. Altakrori,Nizar Habash,Abdelhakim Freihat,Younes Samih,Kirill Chirkunov,Muhammed AbuOdeh,Radu Florian,Teresa Lynn,Preslav Nakov,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 作者提出了DialectalArabicMMLU，这是一个评估大语言模型（LLMs）在阿拉伯语方言上的新基准数据集。该数据集覆盖五大主要方言，总计15K题目，并用于评测多种LLM的方言理解能力。实验发现，不同方言模型表现有较大差异，暴露了模型方言泛化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的阿拉伯语及多语种评测基准主要集中在现代标准阿拉伯语（MSA）上，忽视了在日常交流中广泛使用的阿拉伯语方言，造成方言相关任务中的评测及研究空白。

Method: 基于MMLU-Redux框架，人工翻译和适配了3000道选择题到五大主流阿拉伯语方言（叙利亚、埃及、阿联酋、沙特、摩洛哥），共生成15000道跨32个领域的问答题（总题量包含英语和MSA为22000道）。对19种不同体量（1B-13B参数）的开源阿拉伯语及多语种LLM进行评测。

Result: 在19个LLM评测中，模型在不同方言上的表现存在明显差异，表现出方言泛化能力的不均衡和不足。

Conclusion: DialectalArabicMMLU是首个统一且人工精校的阿拉伯语方言理解评测基准；该基准数据集促进了更包容的模型评测，也为未来阿拉伯语方言处理的模型开发和研究提供了有力资源。

Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the
performance of large language models (LLMs) across Arabic dialects. While
recently developed Arabic and multilingual benchmarks have advanced LLM
evaluation for Modern Standard Arabic (MSA), dialectal varieties remain
underrepresented despite their prevalence in everyday communication.
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation
and adaptation of 3K multiple-choice question-answer pairs into five major
dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of
15K QA pairs across 32 academic and professional domains (22K QA pairs when
also including English and MSA). The benchmark enables systematic assessment of
LLM reasoning and comprehension beyond MSA, supporting both task-based and
linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs
(1B-13B parameters) and report substantial performance variation across
dialects, revealing persistent gaps in dialectal generalization.
DialectalArabicMMLU provides the first unified, human-curated resource for
measuring dialectal understanding in Arabic, thus promoting more inclusive
evaluation and future model development.

</details>


### [119] [Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality](https://arxiv.org/abs/2510.27552)
*Yinghao Luo,Lang Zhou,Amrish Jhingoer,Klaske Vliegenthart Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 本研究探讨了针对医学领域、低资源语言（三种语言：荷兰语、罗马尼亚语、西班牙语）进一步预训练BERT，并对其下游医疗任务性能的提升效果。通过领域适应，特别是临床领域预训练，模型表现优于通用生物医学预训练模型。还观察到跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 多语言医疗NLP工具稀缺，尤其是低资源语言。已有多语言BERT虽有潜力，但相关医学任务表现不足，因此探索领域定制预训练对推动低资源语言医疗NLP的价值。

Method: 在荷兰语、罗马尼亚语、西班牙语的医学语料上进一步预训练BERT，得到不同领域（如通用生物医学、临床）模型。分别在病人筛查、实体识别等下游任务上进行微调测试，比较领域适应效果和跨语言迁移性。

Result: 领域适应显著提升了模型性能，尤其是临床领域预训练模型优于通用生物医学模型。同时发现领域区分带来的性能多样性以及一定的跨语言迁移能力。

Conclusion: 领域适应和跨语言能力对于低资源语言医疗NLP具有显著提升作用，为多语种医疗NLP系统开发提供了有效路径。

Abstract: In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.

</details>


### [120] [Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization](https://arxiv.org/abs/2510.27556)
*Inacio Vieira,Antonio Castaldo,James O'Doherty,Sheila Castilho*

Main category: cs.CL

TL;DR: 提出使用CPO对LLM进行高效领域适应，仅用少量数据达到接近大规模SFT的效果。


<details>
  <summary>Details</summary>
Motivation: 大模型针对特定领域适应通常依赖监督微调（SFT），但这需要大量标注数据，成本高昂。作者希望找到一种数据效率更高的适应方式。

Method: 作者采用CPO（对比偏好优化）方法，将模型初步输出当作“被拒绝”选项，把人工批准的译文（TM条目）当作“被选中”选项，自动合成偏好对，为模型提供直接反馈。

Result: 在英-葡、英-韩机器翻译实验中，仅用14,700对偏好对，取得了接近使用16万+ SFT样本模型的效果；显示该方法数据效率极高。

Conclusion: 所提CPO方法可有效降低领域适应所需的数据量，且不仅适用于机器翻译，也适用于其他需模型起草与黄金参考对比的生成任务。

Abstract: LLMs often require adaptation to domain-specific requirements, a process that
can be expensive when relying solely on SFT. We present an empirical study on
applying CPO to simulate a post-editing workflow for data-efficient domain
adaptation. Our approach synthesizes preference pairs by treating the base
model's own raw output as the 'rejected' translation and the human-approved TM
entry as the 'chosen' one. This method provides direct feedback on the model's
current knowledge, guiding it to align with domain-specific standards.
Experiments in English-Brazilian Portuguese and English-Korean show that, by
using just 14.7k preference pairs, the model achieves performance close to that
of a model trained on 160k+ samples with SFT, demonstrating significant data
efficiency. Although we showcase its effectiveness in MT, this application of
CPO naturally generalizes to other generative tasks where a model's initial
drafts can serve as a contrastive signal against a golden reference.

</details>


### [121] [MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval](https://arxiv.org/abs/2510.27569)
*Qi Luo,Xiaonan Li,Yuxin Wang,Tingshuo Fan,Yuan Li,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种新型多工具RAG框架（MARAG-R1），通过强化学习协调多种检索机制，极大提升了大语言模型在语料库级推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖单一检索器和固定top-k选择，导致检索信息窄且固定，限制了模型获取全面外部知识、执行复杂推理的能力。

Method: 提出MARAG-R1框架，整合语义检索、关键词检索、过滤和聚合四种检索工具。采用两阶段训练：先监督微调，再用强化学习让模型学会如何以及何时使用这些工具，并在推理过程中动态多次检索，逐步收集证据进行综合。

Result: 在GlobalQA、HotpotQA和2WikiMultiHopQA等数据集上，MARAG-R1显著优于现有强基线，在语料库级推理任务中取得新的SOTA成绩。

Conclusion: 通过多工具动态协作的RAG框架，能使大语言模型更有效地访问和整合外部知识，提高复杂任务的推理和生成能力，突破了单一检索RAG的局限。

Abstract: Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.

</details>


### [122] [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)
*Harsh Shah*

Main category: cs.CL

TL;DR: SpecAttn是一种无须额外训练的新方法，通过结合现有的speculative decoding，实现了LLM推理阶段高效稀疏注意力，大幅减少计算量且维持较好输出质量。


<details>
  <summary>Details</summary>
Motivation: 大模型推理时的自注意力机制计算量随着文本长度呈二次增长，导致计算瓶颈。现有稀疏注意力方法常用于加速，但效果受限；speculative decoding虽能加速生成，但cache 读写和冗余计算仍较多，亟需更高效的解决方案。

Method: SpecAttn利用draft模型在推理时已计算得到的注意力权重，预测target模型所需的关键token。它包括三项核心技术：1) 通过KL散度比对draft和target模型注意力分布，确定层级对应关系，2) 设计兼容GPU且无需排序的top-p重要token选取算法，3) 基于预测动态裁剪推理过程中的key-value缓存访问。无需重新训练模型，只需简单集成至现有Speculative Decoding流程。

Result: 在PG-19数据集上，SpecAttn实现了超过75%的key-value缓存访问减少，推理质量以困惑度为指标仅上升了15.29%，相比同类稀疏注意力方案效果更佳。

Conclusion: SpecAttn充分利用Speculative Decoding已完成的计算，巧妙实现推理高效而稀疏的注意力机制，可大幅缓解长文本推理的计算瓶颈，对提升LLM实际应用推理效率具有显著意义。

Abstract: Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.

</details>


### [123] [Culture Cartography: Mapping the Landscape of Cultural Knowledge](https://arxiv.org/abs/2510.27672)
*Caleb Ziems,William Held,Jane Yu,Amir Goldberg,David Grusky,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CultureCartography的混合主动协作方法，通过人机合作寻找大型语言模型（LLM）尚未掌握的重要文化知识，并开发了CultureExplorer工具，显著提升了LLM在文化相关基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM缺乏对特定文化相关知识的理解，常见的数据采集方式单一，难以充分挖掘模型知识盲区。作者希望寻找更有效的方式，让模型能够吸收不同文化背景下用户认为重要但模型未掌握的知识。

Method: 提出了CultureCartography混合主动协作方法：首先让LLM生成其低信心的问题，显式暴露其已知与未知知识，由人工补全知识和编辑内容，推动模型聚焦于突出的文化主题。实现为CultureExplorer工具，并与传统仅由人类回答模型问题的方法做对比实验。

Result: 实验证明，CultureExplorer比基线方法更能发现LLM（如DeepSeek R1、GPT-4o）尚未掌握的文化知识，即使结合网络搜索也是如此。基于新数据微调后，Llama-3.1-8B在文化相关基准测试上准确率提升高达19.2%。

Conclusion: CultureCartography混合主动方法能更高效挖掘和补全大型语言模型在不同文化领域中的知识盲点，显著推动模型在相关任务中的性能提升。

Abstract: To serve global users safely and productively, LLMs need culture-specific
knowledge that might not be learned during pre-training. How do we find such
knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The
most common solutions are single-initiative: either researchers define
challenging questions that users passively answer (traditional annotation), or
users actively produce data that researchers structure as benchmarks (knowledge
extraction). The process would benefit from mixed-initiative collaboration,
where users guide the process to meaningfully reflect their cultures, and LLMs
steer the process towards more challenging questions that meet the researcher's
goals. We propose a mixed-initiative methodology called CultureCartography.
Here, an LLM initializes annotation with questions for which it has
low-confidence answers, making explicit both its prior knowledge and the gaps
therein. This allows a human respondent to fill these gaps and steer the model
towards salient topics through direct edits. We implement this methodology as a
tool called CultureExplorer. Compared to a baseline where humans answer
LLM-proposed questions, we find that CultureExplorer more effectively produces
knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even
with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B
by up to 19.2% on related culture benchmarks.

</details>


### [124] [Continuous Autoregressive Language Models](https://arxiv.org/abs/2510.27688)
*Chenze Shao,Darren Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 传统大语言模型依赖逐token生成，效率受限。本文提出连续自回归语言模型（CALM），将多个token压缩为一个连续向量进行预测，极大减少生成步骤，提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成文本时，需逐个token生成，效率低；提升每步的表达带宽能打破这一限制。

Method: 提出使用高保真自动编码器，将K个token压缩为单一连续向量，模型进行连续向量的预测而非离散token预测，并开发无需离散似然的训练与评估框架。

Result: CALM能以更少计算成本达到强大离散基线的性能，大幅提高性能与计算消耗的性价比。

Conclusion: 引入连续向量预测为高效语言建模提供了新路径，对推动大规模语言模型的效率提升具有重要意义。

Abstract: The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [125] [Force Characterization of Insect-Scale Aquatic Propulsion Based on Fluid-Structure Interaction](https://arxiv.org/abs/2510.26837)
*Conor K. Trygstad,Nestor O. Perez-Arancibia*

Main category: cs.RO

TL;DR: 该论文系统地测试和分析了两种昆虫级微型机器人推进器（单尾和双尾）的推进力，其设计受鱼类游动启发，通过形状记忆合金驱动软尾产生推力，首次获得了推进器瞬时推进力的实验数据。


<details>
  <summary>Details</summary>
Motivation: 虽然这些推进器已被证实适用于微型水下机器人的运动，并能实现简单的轨迹控制，但此前相关的推力特性尚未被系统分析和量化。本文旨在填补该研究空白。

Method: 作者采用建立于反作用力理论基础上的分析框架，并利用自制高灵敏力传感器，系统采集单尾和双尾推进器的最大与周期平均推力实验数据。

Result: 实验表明，单尾推进器的最大/周期平均推力分别为0.45 mN/2.97 μN，双尾推进器为0.61 mN/22.6 μN。论文首次获得此类昆虫级推进器的瞬时推进力测试数据。

Conclusion: 这些实验结果为基于流体-结构耦合的高效微型机器人推进提供了关键动力学数据，对微型机器人推进器设计和性能理解具有重要的推动作用。

Abstract: We present force characterizations of two newly developed insect-scale
propulsors--one single-tailed and one double-tailed--for microrobotic swimmers
that leverage fluid-structure interaction (FSI) to generate thrust. The designs
of these two devices were inspired by anguilliform swimming and are driven by
soft tails excited by high-work-density (HWD) actuators powered by shape-memory
alloy (SMA) wires. While these propulsors have been demonstrated to be suitable
for microrobotic aquatic locomotion and controllable with simple architectures
for trajectory tracking in the two-dimensional (2D) space, the characteristics
and magnitudes of the associated forces have not been studied systematically.
In the research presented here, we adopted a theoretical framework based on the
notion of reactive forces and obtained experimental data for characterization
using a custom-built micro-N-resolution force sensor. We measured maximum and
cycle-averaged force values with multi-test means of respectively 0.45 mN and
2.97 micro-N, for the tested single-tail propulsor. For the dual-tail
propulsor, we measured maximum and cycle-averaged force values with multi-test
means of 0.61 mN and 22.6 micro-N, respectively. These results represent the
first measurements of the instantaneous thrust generated by insect-scale
propulsors of this type and provide insights into FSI for efficient
microrobotic propulsion.

</details>


### [126] [Leveraging Foundation Models for Enhancing Robot Perception and Action](https://arxiv.org/abs/2510.26855)
*Reihaneh Mirjalili*

Main category: cs.RO

TL;DR: 本论文探讨了如何系统性地利用基础模型来提升机器人在非结构化环境中的定位、交互和操控能力。通过围绕四个核心问题展开，旨在建立一个语义感知的机器人智能框架。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在现实复杂、非结构化环境中任务执行时，在定位、交互和操作方面的能力不足问题。当前机器人智能依赖于数据和模型的局限性，缺乏对环境的深层理解和适应能力。

Method: 围绕四个核心挑战展开，运用基础模型（如大模型、预训练模型）为机器人系统注入语义理解能力，通过框架设计和新方法探究定位、交互、操作等关键任务的提升途径。

Result: 提出了一个系统性方法，证明了基础模型可以提升机器人在非结构化环境中的定位、交互和操控表现，增强了机器人对环境的语义理解能力。

Conclusion: 基础模型的引入为机器人带来了语义感知能力，有效提升了机器人在复杂环境下的多任务能力，总结了建立语义感知机器人智能系统的可行性和优势。

Abstract: This thesis investigates how foundation models can be systematically
leveraged to enhance robotic capabilities, enabling more effective
localization, interaction, and manipulation in unstructured environments. The
work is structured around four core lines of inquiry, each addressing a
fundamental challenge in robotics while collectively contributing to a cohesive
framework for semantics-aware robotic intelligence.

</details>


### [127] [Design for One, Deploy for Many: Navigating Tree Mazes with Multiple Agents](https://arxiv.org/abs/2510.26900)
*Jahir Argote-Gerald,Genki Miyauchi,Julian Rau,Paul Trodden,Roderich Gross*

Main category: cs.RO

TL;DR: 本文提出了一种适用于无环图环境的分布式多机器人迷宫遍历算法，通过领导者切换机制，提升了多机器人在受限通信下的协作效率，并在大规模仿真及真实机器人实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 迷宫状环境（如洞穴或管道网络）中，多机器人协作面临通信受限与拥堵等独特挑战。现有的方法或依赖全局通信、或需要环境全知识，难以应用于实际受限情境。作者旨在设计一种无需环境全局信息、只需局部通信的高效多机器人遍历策略。

Method: 算法采用领导者切换机制：一名机器人作为“头部”使用任一单机器人迷宫算法向前探索，其余机器人各自选择前进对象跟随。遇到合适节点时将“头部”角色切换给相邻机器人，从而保证整体轨迹近似单机器人路径。该方法无需全局通信，只依赖本地信息。通过仿真与真实机器人实验对比验证其性能。

Result: 在仿真环境下，最多支持300个机器人，涵盖多种迷宫尺度和单机器人解算器。与朴素、全通信及全知识策略对比：新算法在makespan与燃料消耗和方面优于朴素策略，makespan优于全通信策略但燃料消耗略逊；在两个指标上渐近等同于全知识策略。真实Pi-puck机器人实验（20台）进一步证实方法可行。

Conclusion: 该算法在保障受限场景高效多机器人迷宫遍历任务中展现出良好性能，不依赖全局通信或环境全知识，适合实际复杂受限网络环境应用。

Abstract: Maze-like environments, such as cave and pipe networks, pose unique
challenges for multiple robots to coordinate, including communication
constraints and congestion. To address these challenges, we propose a
distributed multi-agent maze traversal algorithm for environments that can be
represented by acyclic graphs. It uses a leader-switching mechanism where one
agent, assuming a head role, employs any single-agent maze solver while the
other agents each choose an agent to follow. The head role gets transferred to
neighboring agents where necessary, ensuring it follows the same path as a
single agent would. The multi-agent maze traversal algorithm is evaluated in
simulations with groups of up to 300 agents, various maze sizes, and multiple
single-agent maze solvers. It is compared against strategies that are na\"ive,
or assume either global communication or full knowledge of the environment. The
algorithm outperforms the na\"ive strategy in terms of makespan and
sum-of-fuel. It is superior to the global-communication strategy in terms of
makespan but is inferior to it in terms of sum-of-fuel. The findings suggest it
is asymptotically equivalent to the full-knowledge strategy with respect to
either metric. Moreover, real-world experiments with up to 20 Pi-puck robots
confirm the feasibility of the approach.

</details>


### [128] [NaviTrace: Evaluating Embodied Navigation of Vision-Language Models](https://arxiv.org/abs/2510.26909)
*Tim Windecker,Manthan Patel,Moritz Reuss,Richard Schwarzkopf,Cesar Cadena,Rudolf Lioutikov,Marco Hutter,Jonas Frey*

Main category: cs.RO

TL;DR: 本文提出了NaviTrace基准，用于评价视觉-语言模型（VLMs）在不同机体导航任务中的能力，弥补现实实验昂贵和现有仿真不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型已经在各类任务中表现优异，但如何评价其在机器导航（如人形、四足机器人、自行车等）的实际能力依然困难，既因现实测试高成本，也因仿真与基准受限。作者为解决这一问题，设计了更高质量且可扩展的评测框架。

Method: 作者构建了NaviTrace数据集，包含1000种场景、3000多条专家导航轨迹，通过视觉问答方式给予模型指令和机体类型，要求模型输出2D导航轨迹。引入了新的“语义感知轨迹分数”，结合DTW距离、终点误差和基于像素语义的机体惩罚因子，并与人工偏好相关联。用此评估八种先进VLM的表现。

Result: 评测发现，现有VLM与人类性能之间存在明显差距，尤其是在空间定位和目标识别方面。新指标能够有效反映模型和人类表现的差异。

Conclusion: NaviTrace为现实机器导航领域提供了一个可扩展、可复现的评测基准，有助于推动视觉-语言模型在机器人导航中的实际应用和发展。

Abstract: Vision-language models demonstrate unprecedented performance and
generalization across a wide range of tasks and scenarios. Integrating these
foundation models into robotic navigation systems opens pathways toward
building general-purpose robots. Yet, evaluating these models' navigation
capabilities remains constrained by costly real-world trials, overly simplified
simulations, and limited benchmarks. We introduce NaviTrace, a high-quality
Visual Question Answering benchmark where a model receives an instruction and
embodiment type (human, legged robot, wheeled robot, bicycle) and must output a
2D navigation trace in image space. Across 1000 scenarios and more than 3000
expert traces, we systematically evaluate eight state-of-the-art VLMs using a
newly introduced semantic-aware trace score. This metric combines Dynamic Time
Warping distance, goal endpoint error, and embodiment-conditioned penalties
derived from per-pixel semantics and correlates with human preferences. Our
evaluation reveals consistent gap to human performance caused by poor spatial
grounding and goal localization. NaviTrace establishes a scalable and
reproducible benchmark for real-world robotic navigation. The benchmark and
leaderboard can be found at
https://leggedrobotics.github.io/navitrace_webpage/.

</details>


### [129] [Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence](https://arxiv.org/abs/2510.26915)
*Zachary Ravichandran,Fernando Cladera,Ankit Prabhu,Jason Hughes,Varun Murali,Camillo Taylor,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种新框架SPINE-HT，实现了异构机器人团队在实际非结构化环境中，依赖大语言模型（LLM）理解自然语言任务、合理分配子任务与自适应调整，显著提升完成复杂任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的异构机器人团队方法多基于结构化、已知环境，难以应对无先验地图的实际开放环境。异构团队需结合各自能力动态分配任务，且需根据实时反馈调整，但现有方法局限明显。作者希望解决上述难题，实现机器人对自然语言任务灵活、高效地响应。

Method: 作者提出了“SPINE-HT”三阶段框架：（1）根据自然语言任务描述与团队能力，由LLM生成具备可执行性的子任务并验证其可行性；（2）将子任务基于各机器人能力（如通行、感知）合理分配；（3）运行中根据反馈不断调整和优化任务分配。该框架支持动态自适应并融入环境中的感知与控制信息。

Result: 在仿真环境下，SPINE-HT的成功率几乎是现有LLM方法的2倍。在真实机器人平台（包括地面机器人、波士顿动力Spot和高空无人机）测试中，SPINE-HT在需要评估机器人能力和任务在线优化的任务中，达到了87%的成功率。

Conclusion: SPINE-HT大幅提升了异构机器人团队在无结构、未知环境下完成复杂任务的能力，实现了自然语言任务和机器人具体能力的高效对接，为机器人在现实复杂场景下的自主协作奠定了新基础。

Abstract: Heterogeneous robot teams operating in realistic settings often must
accomplish complex missions requiring collaboration and adaptation to
information acquired online. Because robot teams frequently operate in
unstructured environments -- uncertain, open-world settings without prior maps
-- subtasks must be grounded in robot capabilities and the physical world.
While heterogeneous teams have typically been designed for fixed
specifications, generative intelligence opens the possibility of teams that can
accomplish a wide range of missions described in natural language. However,
current large language model (LLM)-enabled teaming methods typically assume
well-structured and known environments, limiting deployment in unstructured
environments. We present SPINE-HT, a framework that addresses these limitations
by grounding the reasoning abilities of LLMs in the context of a heterogeneous
robot team through a three-stage process. Given language specifications
describing mission goals and team capabilities, an LLM generates grounded
subtasks which are validated for feasibility. Subtasks are then assigned to
robots based on capabilities such as traversability or perception and refined
given feedback collected during online operation. In simulation experiments
with closed-loop perception and control, our framework achieves nearly twice
the success rate compared to prior LLM-enabled heterogeneous teaming
approaches. In real-world experiments with a Clearpath Jackal, a Clearpath
Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an
87\% success rate in missions requiring reasoning about robot capabilities and
refining subtasks with online feedback. More information is provided at
https://zacravichandran.github.io/SPINE-HT.

</details>


### [130] [RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification](https://arxiv.org/abs/2510.26935)
*Yunhao Yang,Neel P. Bhatt,Pranay Samineni,Rohan Siva,Zhanyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出了一种结合神经网络与符号推理的新方法，能够高效并可靠地验证AI计划是否满足复杂的自然语言安全规则。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键应用领域的实际部署，迫切需要确保其行为满足安全规范。然而，传统形式化方法过于依赖手工规范，不够灵活；深度学习方案虽易用但缺乏可解释性和可靠性。作者力图在两者间架桥，提升AI安全验证的效率与可信度。

Method: 提出了RepV，一种神经符号融合的验证器。其核心思想是，将带有自然语言安全解释的计划，通过轻量投影器映射到一个低维潜在空间，使安全与不安全的计划线性可分。系统使用少量经形式验证的标注样本进行训练，对新规则只需一次前向传递即可判断。RepV还能提供概率化的验证置信度，并能基于此自动优化计划者，无需额外人工标注。

Result: 实验证明，RepV在多个规划任务中验证准确率比现有方法提升了15%，新增参数量小于0.2M。此外，基于RepV的自动优化框架，在不同领域下均优于普通微调方法。

Conclusion: 安全可分的潜在空间是高效、可扩展的神经符号计划验证基础元素。RepV证明了通过神经符号整合，能以更少资源获得更高安全验证准确率和可靠性，推动AI系统安全落地。

Abstract: As AI systems migrate to safety-critical domains, verifying that their
actions comply with well-defined rules remains a challenge. Formal methods
provide provable guarantees but demand hand-crafted temporal-logic
specifications, offering limited expressiveness and accessibility. Deep
learning approaches enable evaluation of plans against natural-language
constraints, yet their opaque decision process invites misclassifications with
potentially severe consequences. We introduce RepV, a neurosymbolic verifier
that unifies both views by learning a latent space where safe and unsafe plans
are linearly separable. Starting from a modest seed set of plans labeled by an
off-the-shelf model checker, RepV trains a lightweight projector that embeds
each plan, together with a language model-generated rationale, into a
low-dimensional space; a frozen linear boundary then verifies compliance for
unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the
likelihood of correct verification based on its position in the latent space.
This guarantee enables a guarantee-driven refinement of the planner, improving
rule compliance without human annotations. Empirical evaluations show that RepV
improves compliance prediction accuracy by up to 15% compared to baseline
methods while adding fewer than 0.2M parameters. Furthermore, our refinement
framework outperforms ordinary fine-tuning baselines across various planning
domains. These results show that safety-separable latent spaces offer a
scalable, plug-and-play primitive for reliable neurosymbolic plan verification.
Code and data are available at: https://repv-project.github.io/.

</details>


### [131] [A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection](https://arxiv.org/abs/2510.27010)
*William E. Heap,Yimeng Qin,Kai Hammond,Anish Bayya,Haonon Kong,Allison M. Okamura*

Main category: cs.RO

TL;DR: 本论文提出了一种密封且透明的藤蔓机器人系统，用于管道内部状况评估与映射，并通过实际污水管道中任务实现了系统验证。


<details>
  <summary>Details</summary>
Motivation: 现有老化管道的维护需要精确的内部评估和映射。软体藤蔓机器人因其灵活性适合在管道等狭窄曲折环境中导航，但目前存在子系统复杂、缺乏实际工业验证等问题。作者希望通过更简化、封闭和适用于实际环境的藤蔓机器人解决上述挑战。

Method: 作者设计了一种所有机械和电子部件全部封装在软性、密封、透明机器人本体内部的藤蔓机器人系统，并开发了用于传感器运输的被动自适应密封前端安装架。通过建模与实验，并在实际污水管道环境进行了验证任务。

Result: 定制的藤蔓机器人系统实现在非分支管道内部对系统的完整机电保护与视觉检测，并成功完成了污水管道中的状况评估与映射任务，达到了预期目标。

Conclusion: 论文所提出的密封透明藤蔓机器人不仅能更好保护内部组件，同时适用于视觉传感，为管道检测提供了坚实、实用且经实际验证的解决方案，为未来软体藤蔓机器人在管道检测领域的应用和发展奠定了基础。

Abstract: Rehabilitation of aging pipes requires accurate condition assessment and
mapping far into the pipe interiors. Soft growing vine robot systems are
particularly promising for navigating confined, sinuous paths such as in pipes,
but are currently limited by complex subsystems and a lack of validation in
real-world industrial settings. In this paper, we introduce the concept and
implementation of a hermetic and transparent vine robot system for visual
condition assessment and mapping within non-branching pipes. This design
encloses all mechanical and electrical components within the vine robot's soft,
airtight, and transparent body, protecting them from environmental interference
while enabling visual sensing. Because this approach requires an enclosed
mechanism for transporting sensors, we developed, modeled, and tested a
passively adapting enclosed tip mount. Finally, we validated the hermetic and
transparent vine robot system concept through a real-world condition assessment
and mapping task in a wastewater pipe. This work advances the use of
soft-growing vine robots in pipe inspection by developing and demonstrating a
robust, streamlined, field-validated system suitable for continued development
and deployment.

</details>


### [132] [A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics](https://arxiv.org/abs/2510.27033)
*Simindokht Jahangard,Mehrzad Mohammadi,Abhinav Dhall,Hamid Rezatofighi*

Main category: cs.RO

TL;DR: 本文提出了一种结合全景图像和3D点云信息的神经符号视觉推理框架，显著提升了机器人环境中的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在感知任务上表现优异，但仅依赖图像和相关性推理，难以处理精细的空间关系，尤其在复杂环境下的机器人应用更具挑战性。

Method: 提出了一种神经符号框架，融合了全景图像和3D点云，包含感知模块（用于检测实体与属性）和推理模块（建立结构化场景图，支持可解释空间与逻辑推理）。

Result: 在JRDB-Reasoning数据集上验证，模型在拥挤且人工构建的环境中表现出更高的推理能力和可靠性，并且设计轻量，适用于机器人与嵌入式人工智能。

Conclusion: 该框架有效提升了精细空间推理的准确性与可解释性，为机器人及嵌入式AI系统的视觉推理提供了新方法。

Abstract: Visual reasoning, particularly spatial reasoning, is a challenging cognitive
task that requires understanding object relationships and their interactions
within complex environments, especially in robotics domain. Existing
vision_language models (VLMs) excel at perception tasks but struggle with
fine-grained spatial reasoning due to their implicit, correlation-driven
reasoning and reliance solely on images. We propose a novel neuro_symbolic
framework that integrates both panoramic-image and 3D point cloud information,
combining neural perception with symbolic reasoning to explicitly model spatial
and logical relationships. Our framework consists of a perception module for
detecting entities and extracting attributes, and a reasoning module that
constructs a structured scene graph to support precise, interpretable queries.
Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior
performance and reliability in crowded, human_built environments while
maintaining a lightweight design suitable for robotics and embodied AI
applications.

</details>


### [133] [SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation](https://arxiv.org/abs/2510.27048)
*Eric T. Chang,Peter Ballentine,Zhanpeng He,Do-Gon Kim,Kai Jiang,Hua-Hsuan Liang,Joaquin Palacios,William Wang,Pedro Piacenza,Ioannis Kymissis,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本文提出了一种新型多模态触觉手指SpikeATac，结合了动态和静态触觉感知方式，在灵敏和高速的同时，能精细地操作易碎物体，并通过强化学习方法实现灵巧的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 机器人在与脆弱、可变形物体的交互中，往往难以实现高灵敏度和精确的力控制，现有触觉传感器在动态响应和多模态集成方面存在不足。因此，提升机器人灵巧操作能力，尤其是对易碎物品的在手操作是重要挑战。

Method: 作者设计了SpikeATac触觉手指，结合了PVDF（聚偏氟乙烯）动态传感器（采样率4kHz，16个sensel）与电容式静态传感器，实现多模态触觉采集；同时，将其整合进强化学习算法，通过人类反馈+触觉奖励，优化机器手的施力策略。

Result: SpikeATac手指展示出优异的灵敏度和动态响应能力，能够在握持和操作脆弱、可变形物体时快速、细致地停止运动。结合强化学习，该系统首次实现了对易碎物体的在手操作等高难度任务。

Conclusion: SpikeATac不仅增强了机器手精细力控和多模态感知能力，还能与学习算法协同推动高难度灵巧操作，为未来机器人在复杂物体操作场景中提供了有力的感知和控制基础。

Abstract: In this work, we introduce SpikeATac, a multimodal tactile finger combining a
taxelized and highly sensitive dynamic response (PVDF) with a static
transduction method (capacitive) for multimodal touch sensing. Named for its
`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides
fast, sensitive dynamic signals to the very onset and breaking of contact. We
characterize the sensitivity of the different modalities, and show that
SpikeATac provides the ability to stop quickly and delicately when grasping
fragile, deformable objects. Beyond parallel grasping, we show that SpikeATac
can be used in a learning-based framework to achieve new capabilities on a
dexterous multifingered robot hand. We use a learning recipe that combines
reinforcement learning from human feedback with tactile-based rewards to
fine-tune the behavior of a policy to modulate force. Our hardware platform and
learning pipeline together enable a difficult dexterous and contact-rich task
that has not previously been achieved: in-hand manipulation of fragile objects.
Videos are available at
\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}.

</details>


### [134] [Learning Generalizable Visuomotor Policy through Dynamics-Alignment](https://arxiv.org/abs/2510.27114)
*Dohyeok Lee,Jung Min Lee,Munkyung Kim,Seokhun Ju,Jin Woo Koo,Kyungjae Lee,Dohyeong Kim,TaeHyun Cho,Jungwoo Lee*

Main category: cs.RO

TL;DR: 本文提出了一种结合动力学预测与策略学习的新方法，以提升机器人行为克隆在实际场景中的泛化能力，并在多种复杂环境下验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统行为克隆方法在机器人学习中由于专家示范数据有限，泛化能力较差。近期使用视频预测模型虽能从大规模数据中学习表征，但忽略了不同控制输入对动作的影响，而且训练数据需求非常高。因此，迫切需要更好地结合动力学信息，以提升在复杂任务与环境变化下的表现。

Method: 提出了一种动力学对齐流匹配策略（Dynamics-Aligned Flow Matching Policy, DAP）。该方法在策略学习过程中集成动力学预测，并设计了新颖的架构，使策略模型和动力学模型在动作生成时形成互相纠偏的反馈机制，实现自我纠正。此外，方法在训练阶段促使两模型同步优化，从而提高泛化能力。

Result: 实验证明，DAP方法在真实世界机器人操作任务中，比现有基线方法具有更好的泛化能力，特别是在超出训练分布（OOD）情况下如视觉干扰和光照变化下表现出较强的稳健性。

Conclusion: 动力学对齐的流匹配策略有效结合了策略学习与动力学建模的优点，为机器人行为克隆提供了更强的泛化性能和鲁棒性，尤其适用于实际复杂和变化多端的操作环境。

Abstract: Behavior cloning methods for robot learning suffer from poor generalization
due to limited data support beyond expert demonstrations. Recent approaches
leveraging video prediction models have shown promising results by learning
rich spatiotemporal representations from large-scale datasets. However, these
models learn action-agnostic dynamics that cannot distinguish between different
control inputs, limiting their utility for precise manipulation tasks and
requiring large pretraining datasets. We propose a Dynamics-Aligned Flow
Matching Policy (DAP) that integrates dynamics prediction into policy learning.
Our method introduces a novel architecture where policy and dynamics models
provide mutual corrective feedback during action generation, enabling
self-correction and improved generalization. Empirical validation demonstrates
generalization performance superior to baseline methods on real-world robotic
manipulation tasks, showing particular robustness in OOD scenarios including
visual distractions and lighting variations.

</details>


### [135] [Confined Space Underwater Positioning Using Collaborative Robots](https://arxiv.org/abs/2510.27151)
*Xueliang Cheng,Kanzhong Yao,Andrew West,Ognjen Marjanovic,Barry Lennox,Keir Groves*

Main category: cs.RO

TL;DR: 本文提出了一种新型水下机器人定位系统CAP，克服了传统定位在狭小、复杂环境下的诸多局限性，实现了高精度、免基础设施、实时的水下定位。


<details>
  <summary>Details</summary>
Motivation: 水下机器人在狭窄或有障碍的工业环境中定位困难，现有方法依赖大范围外部设备、环境特征或基础设施，难以适应实际复杂应用场景，因此亟需无需依赖基础设施且易部署的高精度定位系统。

Method: 提出一个协同水上-水下定位系统CAP，采用“母舰-子艇”模式，由表面机器人（领导者）协助水下机器人（跟随者）定位。系统利用多机器人协作与传感器融合技术，无需额外基站或环境特征，实现自主任务中的实时定位。

Result: 在大型测试水池进行实验，CAP系统在多次自主任务中实现了平均欧式距离（MED）误差仅70毫米，整个过程无需固定基础设施、复杂校准或环境特征，达到实时控制精度。

Conclusion: CAP系统为水下机器人的定位与导航提供了一种免基础设施、高精度、实用可靠的新路径，为密闭和复杂环境下的自主作业带来突破，扩大了水下机器人应用的可能性。

Abstract: Positioning of underwater robots in confined and cluttered spaces remains a
key challenge for field operations. Existing systems are mostly designed for
large, open-water environments and struggle in industrial settings due to poor
coverage, reliance on external infrastructure, and the need for feature-rich
surroundings. Multipath effects from continuous sound reflections further
degrade signal quality, reducing accuracy and reliability. Accurate and easily
deployable positioning is essential for repeatable autonomous missions;
however, this requirement has created a technological bottleneck limiting
underwater robotic deployment. This paper presents the Collaborative Aquatic
Positioning (CAP) system, which integrates collaborative robotics and sensor
fusion to overcome these limitations. Inspired by the "mother-ship" concept,
the surface vehicle acts as a mobile leader to assist in positioning a
submerged robot, enabling localization even in GPS-denied and highly
constrained environments. The system is validated in a large test tank through
repeatable autonomous missions using CAP's position estimates for real-time
trajectory control. Experimental results demonstrate a mean Euclidean distance
(MED) error of 70 mm, achieved in real time without requiring fixed
infrastructure, extensive calibration, or environmental features. CAP leverages
advances in mobile robot sensing and leader-follower control to deliver a step
change in accurate, practical, and infrastructure-free underwater localization.

</details>


### [136] [MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking](https://arxiv.org/abs/2510.27178)
*Xuan-Thuan Nguyen,Khac Nam Nguyen,Ngoc Duy Tran,Thi Thoa Mac,Anh Nguyen,Hoang Hiep Ly,Tung D. Ta*

Main category: cs.RO

TL;DR: 本文提出了一种可自重构的移动双臂机器人系统MobiDock，其两台独立机器人可物理对接，形成统一平台，实现更优的多机器人协作。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统（如移动机械臂）在协作时面临控制协调和动态稳定性等难题。

Method: 提出MobiDock系统，利用计算机视觉（AprilTag 标记）与新型螺旋锁定机制实现自主对接，将多机器人问题转化为更易控制的单系统。

Result: 实验表明，对接后的统一系统在动态稳定性和操作效率上优于两个独立合作的机器人：RMS加速度和跃度更低，角度精度更高，任务完成时间更短。

Conclusion: 物理重构是一种有效的设计原则，可简化多机器人协作控制，并显著提升复杂任务的稳定性与性能。

Abstract: Multi-robot systems, particularly mobile manipulators, face challenges in
control coordination and dynamic stability when working together. To address
this issue, this study proposes MobiDock, a modular self-reconfigurable mobile
manipulator system that allows two independent robots to physically connect and
form a unified mobile bimanual platform. This process helps transform a complex
multi-robot control problem into the management of a simpler, single system.
The system utilizes an autonomous docking strategy based on computer vision
with AprilTag markers and a new threaded screw-lock mechanism. Experimental
results show that the docked configuration demonstrates better performance in
dynamic stability and operational efficiency compared to two independently
cooperating robots. Specifically, the unified system has lower Root Mean Square
(RMS) Acceleration and Jerk values, higher angular precision, and completes
tasks significantly faster. These findings confirm that physical
reconfiguration is a powerful design principle that simplifies cooperative
control, improving stability and performance for complex tasks in real-world
environments.

</details>


### [137] [Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets](https://arxiv.org/abs/2510.27184)
*Hoang Hiep Ly,Cong-Nhat Nguyen,Doan-Quang Tran,Quoc-Khanh Dang,Ngoc Duy Tran,Thi Thoa Mac,Anh Nguyen,Xuan-Thuan Nguyen,Tung D. Ta*

Main category: cs.RO

TL;DR: 提出一种新型混合夹持手指，通过调节硅胶囊体的气压，主动改变夹持表面的摩擦力，实现对重、滑、易碎物体的安全高效抓取。


<details>
  <summary>Details</summary>
Motivation: 传统机器人夹持器通常通过施加较大法向力来稳定抓取，但这容易损坏脆弱或易变形的物品，难以适应多样的机械性质。

Method: 设计包含刚性外壳与可充气软硅胶囊的手指结构，通过调整囊内气压调节手指表面摩擦系数。实验测试验证了内压与摩擦系数的正相关关系。

Result: 实验显示，提高囊体气压能显著提升摩擦系数，使夹持器在不增加夹持力的前提下，稳固提起重且滑物体，同时安全夹持鸡蛋、水果、纸杯等脆弱物品。

Conclusion: 该混合夹持手指能根据需求灵活调节摩擦，提升夹持器的普适性和安全性，是传统依赖高夹持力方案的有效替代方案。

Abstract: Grasping objects with diverse mechanical properties, such as heavy, slippery,
or fragile items, remains a significant challenge in robotics. Conventional
grippers often rely on applying high normal forces, which can cause damage to
objects. To address this limitation, we present a hybrid gripper finger that
combines a rigid structural shell with a soft, inflatable silicone pocket. The
gripper finger can actively modulate its surface friction by controlling the
internal air pressure of the silicone pocket. Results from fundamental
experiments indicate that increasing the internal pressure results in a
proportional increase in the effective coefficient of friction. This enables
the gripper to stably lift heavy and slippery objects without increasing the
gripping force and to handle fragile or deformable objects, such as eggs,
fruits, and paper cups, with minimal damage by increasing friction rather than
applying excessive force. The experimental results demonstrate that the hybrid
gripper finger with adaptable friction provides a robust and safer alternative
to relying solely on high normal forces, thereby enhancing the gripper
flexibility in handling delicate, fragile, and diverse objects.

</details>


### [138] [Vectorized Online POMDP Planning](https://arxiv.org/abs/2510.27191)
*Marcus Hoerger,Muhammad Sudrajat,Hanna Kurniawati*

Main category: cs.RO

TL;DR: 本文提出了一种新型的POMDP在线规划并行求解器VOPP，通过向量化和张量计算实现大规模并行，极大提升了求解效率。实验显示其性能至少比现有最先进方法快20倍。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下进行高效规划对自主机器人至关重要。现有POMDP求解难以高效并行，受制于数值优化与期望估值的耦合带来的同步瓶颈，限制了硬件并行能力的利用。因此急需一种能够消除依赖性和同步瓶颈、高效利用并行硬件的POMDP求解方法。

Method: VOPP采用一种新的POMDP表述，将部分优化过程解析化，仅将期望值的估算留给数值计算。所有相关的数据结构均以张量形式表示，各阶段规划步骤全部为矢量化操作，无需线程间同步与依赖，充分发挥了硬件的并行计算力。

Result: VOPP能够实现完全无依赖和同步瓶颈的并行求解。实验表明，该方法在获得近最优解的效率上，相比现有并行POMDP求解器至少提升了20倍。

Conclusion: VOPP通过向量化及张量操作，大幅提高了POMDP在线规划的并行求解能力，解决了以往并行化中存在的依赖问题，对实现自主系统中的高效决策具有重要意义。

Abstract: Planning under partial observability is an essential capability of autonomous
robots. The Partially Observable Markov Decision Process (POMDP) provides a
powerful framework for planning under partial observability problems, capturing
the stochastic effects of actions and the limited information available through
noisy observations. POMDP solving could benefit tremendously from massive
parallelization of today's hardware, but parallelizing POMDP solvers has been
challenging. They rely on interleaving numerical optimization over actions with
the estimation of their values, which creates dependencies and synchronization
bottlenecks between parallel processes that can quickly offset the benefits of
parallelization. In this paper, we propose Vectorized Online POMDP Planner
(VOPP), a novel parallel online solver that leverages a recent POMDP
formulation that analytically solves part of the optimization component,
leaving only the estimation of expectations for numerical computation. VOPP
represents all data structures related to planning as a collection of tensors
and implements all planning steps as fully vectorized computations over this
representation. The result is a massively parallel solver with no dependencies
and synchronization bottlenecks between parallel computations. Experimental
results indicate that VOPP is at least 20X more efficient in computing
near-optimal solutions compared to an existing state-of-the-art parallel online
solver.

</details>


### [139] [A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot](https://arxiv.org/abs/2510.27327)
*Robert Pommeranz,Kevin Tebbe,Ralf Heynicke,Gerd Scholl*

Main category: cs.RO

TL;DR: 本文提出了一种基于PX4-Autopilot和ROS 2框架的异构无人机蜂群C-UAS系统的模块化、可扩展架构，具有良好的硬件集成和软件通信抽象，支持多种自主蜂群操作，并通过仿真和实地实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机威胁的增加，需要高效的反无人机系统（C-UAS）。而蜂群技术因其灵活性和分布式优势，逐渐成为研究重点，但现有系统在多硬件、功能扩展、通信兼容性等方面存在局限。本文旨在解决这些问题，提出一种可扩展且易于整合的蜂群架构。

Method: 设计了一种基于ROS 2的架构，将无人机各组件独立为ROS 2节点，通过软件层面抽象了蜂群成员间的通信，支持不同硬件与技术的集成。此外，实现了领队跟随、编队飞行等蜂群基础功能，并允许集成视觉算法以实现目标检测和跟踪，同时加入地面站实现群体操作协调。

Result: 该架构在Gazebo仿真环境和真实世界试验中进行了验证，展示了架构的可扩展性、模块化和功能实现能力，包括自主编队、目标跟踪等关键功能。

Conclusion: 提出的模块化、可扩展蜂群架构能够有效整合多种无人机硬件和技术，简化了通信和功能扩展，并在仿真与现实中验证了其实用性和可靠性，为反无人机系统的开发奠定了基础。

Abstract: In this paper a modular and scalable architecture for heterogeneous
swarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and
Robot Operating System 2 (ROS 2) framework is presented. The proposed
architecture emphasizes seamless integration of hardware components by
introducing independent ROS 2 nodes for each component of a Unmanned Aerial
Vehicle (UAV). Communication between swarm participants is abstracted in
software, allowing the use of various technologies without architectural
changes. Key functionalities are supported, e.g. leader following and formation
flight to maneuver the swarm. The system also allows computer vision algorithms
to be integrated for the detection and tracking of UAVs. Additionally, a ground
station control is integrated for the coordination of swarm operations.
Swarm-based Unmanned Aerial System (UAS) architecture is verified within a
Gazebo simulation environment but also in real-world demonstrations.

</details>


### [140] [Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict](https://arxiv.org/abs/2510.27333)
*Hao Cheng,Yanbo Jiang,Qingyuan Shi,Qingwen Meng,Keyu Chen,Wenhao Yu,Jianqiang Wang,Sifa Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种新的侧向冲突风险评估指标MEI，在实际数据集上证明其比传统指标表现更优，可提升自动驾驶安全评估的科学性和精准性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全评估多关注纵向冲突，忽视了城市环境中常见的侧向冲突风险。由于现有指标在量化侧向规避行为上存在局限，故需开发更有效指标。

Method: 提出改进型紧急指数（MEI），修正了原紧急指数（EI）对可规避时间的估算方法，并在Argoverse-2基础上的公开侧向冲突数据集上进行验证和对比。

Result: 在1,500多例高质量自动驾驶侧向冲突案例和500多例关键事件中，MEI在准确量化关键性以及风险变化方面均优于主流的ACT和PET指标。

Conclusion: MEI能够更精确反映关键性和风险变化，为城市环境下自动驾驶安全评估提供了有力工具，并通过开源实现推动行业进步。

Abstract: Effective, reliable, and efficient evaluation of autonomous driving safety is
essential to demonstrate its trustworthiness. Criticality metrics provide an
objective means of assessing safety. However, as existing metrics primarily
target longitudinal conflicts, accurately quantifying the risks of lateral
conflicts - prevalent in urban settings - remains challenging. This paper
proposes the Modified-Emergency Index (MEI), a metric designed to quantify
evasive effort in lateral conflicts. Compared to the original Emergency Index
(EI), MEI refines the estimation of the time available for evasive maneuvers,
enabling more precise risk quantification. We validate MEI on a public lateral
conflict dataset based on Argoverse-2, from which we extract over 1,500
high-quality AV conflict cases, including more than 500 critical events. MEI is
then compared with the well-established ACT and the widely used PET metrics.
Results show that MEI consistently outperforms them in accurately quantifying
criticality and capturing risk evolution. Overall, these findings highlight MEI
as a promising metric for evaluating urban conflicts and enhancing the safety
assessment framework for autonomous driving. The open-source implementation is
available at https://github.com/AutoChengh/MEI.

</details>


### [141] [Towards a Multi-Embodied Grasping Agent](https://arxiv.org/abs/2510.27420)
*Roman Freiberg,Alexander Qualmann,Ngo Anh Vien,Gerhard Neumann*

Main category: cs.RO

TL;DR: 本文提出一种高效的数据利用、可处理多种不同夹爪结构的抓取合成方法，通过等变流架构学习抓取策略，在不同夹爪和场景间均能表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多夹爪抓取方法面临两个挑战：一是难以泛化到不同夹爪结构，二是需要大规模数据支持，采集数据代价高且难以涵盖多种夹爪。作者希望开发一种对夹爪类型和场景更具泛化能力、数据高效的抓取方法。

Method: 提出一种基于流网络（flow-based）、等变性的抓取合成架构，只需输入夹爪和场景几何信息，便可自动推断所有必要信息适配各类夹爪。将所有模块自底向上用JAX实现，可对场景、夹爪和抓取动作进行批处理，提升学习效率、性能和推理速度。方法不依赖大规模采集数据，利用自身设计结构提升泛化性。

Result: 所提出模型能支持各种不同形态的夹爪（包括类似人手和机械爪），利用包含2.5万个场景和2000万个抓取动作的大型数据集，通过批处理加速学习并实现高效推理，实验中展现优于以往方法的性能。

Conclusion: 本文方法在多样化夹爪抓取任务中表现优良，能在数据利用和推理效率方面显著超越现有技术，有望推进机器人抓取领域向泛化、多样化方向发展。

Abstract: Multi-embodiment grasping focuses on developing approaches that exhibit
generalist behavior across diverse gripper designs. Existing methods often
learn the kinematic structure of the robot implicitly and face challenges due
to the difficulty of sourcing the required large-scale data. In this work, we
present a data-efficient, flow-based, equivariant grasp synthesis architecture
that can handle different gripper types with variable degrees of freedom and
successfully exploit the underlying kinematic model, deducing all necessary
information solely from the gripper and scene geometry. Unlike previous
equivariant grasping methods, we translated all modules from the ground up to
JAX and provide a model with batching capabilities over scenes, grippers, and
grasps, resulting in smoother learning, improved performance and faster
inference time. Our dataset encompasses grippers ranging from humanoid hands to
parallel yaw grippers and includes 25,000 scenes and 20 million grasps.

</details>


### [142] [Learning Soft Robotic Dynamics with Active Exploration](https://arxiv.org/abs/2510.27428)
*Hehui Zheng,Bhavya Sukhija,Chenhao Li,Klemens Iten,Andreas Krause,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本文提出SoftAE，一种用于软体机器人动力学建模的不确定性驱动主动探索框架，在多个仿真及真实平台上显著提升了动力学模型的通用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有极高的适应性与安全性，但其高维、非线性动力学难以建模，现有数据驱动方法泛化能力差，限制了其自主性和适应性。

Method: 提出SoftAE框架，利用概率集成模型估计动力学中的知识不确定性，引导探索状态-动作空间的未知区域，从而高效地收集覆盖多样行为的数据，无需任务特定监督。

Result: 在三种仿真软体机器人和一种真实软体手臂的实验中，SoftAE较传统方法获得了更准确的动力学模型，并在新任务上展现优越的零样本控制能力，对感知噪声、执行时延和材料非线性等干扰也具备鲁棒性。

Conclusion: 基于不确定性的主动探索有助于学习可扩展且可复用的软体机器人动力学模型，推动了软体机器人向自适应、数据高效、通用型控制的方向发展。

Abstract: Soft robots offer unmatched adaptability and safety in unstructured
environments, yet their compliant, high-dimensional, and nonlinear dynamics
make modeling for control notoriously difficult. Existing data-driven
approaches often fail to generalize, constrained by narrowly focused task
demonstrations or inefficient random exploration. We introduce SoftAE, an
uncertainty-aware active exploration framework that autonomously learns
task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE
employs probabilistic ensemble models to estimate epistemic uncertainty and
actively guides exploration toward underrepresented regions of the state-action
space, achieving efficient coverage of diverse behaviors without task-specific
supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a
continuum arm, an articulated fish in fluid, and a musculoskeletal leg with
hybrid actuation -- and on a pneumatically actuated continuum soft arm in the
real world. Compared with random exploration and task-specific model-based
reinforcement learning, SoftAE produces more accurate dynamics models, enables
superior zero-shot control on unseen tasks, and maintains robustness under
sensing noise, actuation delays, and nonlinear material effects. These results
demonstrate that uncertainty-driven active exploration can yield scalable,
reusable dynamics models across diverse soft robotic morphologies, representing
a step toward more autonomous, adaptable, and data-efficient control in
compliant robots.

</details>


### [143] [Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot](https://arxiv.org/abs/2510.27436)
*Tomoko Yonezawa,Hirotake Yamazoe,Atsuo Fujino,Daigo Suhara,Takaya Tamamoto,Yuto Nishiguchi*

Main category: cs.RO

TL;DR: 本文提出了一种模型，让机器人能够基于与人类的距离灵活产生排斥反应，包括耐受与主动回避。


<details>
  <summary>Details</summary>
Motivation: 在人与机器人的交互中，物理接近与接触很常见，但现有设计很少考虑机器人的主动排斥行为。人类会根据不同关系和情境选择容忍或回避他人的靠近，因此为机器人加入类似的内部状态和行为具有实际意义。

Method: 作者设计了机器人“排斥”内部状态的建模方式，通过积累与消退不适感（基于人与机器人间的距离），并结合PAD情感模型中的支配轴，对不同程度的不适驱动耐受或极限回避动作。这些行为被实现并测试于机械臂上。

Result: 实验结果展示了：机器人内部状态参数如何映射到不同强度的耐受动作，并在超过限度时触发主动回避行为。这一机制形成了从内部模型到具体动作表征的完整流程。

Conclusion: 该研究为机器人做出人性化的拒绝与回避反应提供了方法基础，使机器人与人类交互时能更灵活自然地表达“界限”，有助于提升未来人机交互体验。

Abstract: Human-robot interaction frequently involves physical proximity or contact. In
human-human settings, people flexibly accept, reject, or tolerate such
approaches depending on the relationship and context. We explore the design of
a robot's rejective internal state and corresponding avoidance behaviors, such
as withdrawing or pushing away, when a person approaches. We model the
accumulation and decay of discomfort as a function of interpersonal distance,
and implement tolerance (endurance) and limit-exceeding avoidance driven by the
Dominance axis of the PAD affect model. The behaviors and their intensities are
realized on an arm robot. Results illustrate a coherent pipeline from internal
state parameters to graded endurance motions and, once a limit is crossed, to
avoidance actions.

</details>


### [144] [EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities](https://arxiv.org/abs/2510.27545)
*Travis Davies,Yiqi Huang,Alexi Gladstone,Yunxin Liu,Xiang Chen,Heng Ji,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: 本文提出了EBT-Policy，一种基于Energy-Based Transformer（EBT）的新型能量模型架构，用于提升机器人领域中策略学习的稳定性和效率，显著优于现有的扩散策略方法，并展现出诸多新能力。


<details>
  <summary>Details</summary>
Motivation: 现有由生成模型（如Diffusion Policy）参数化的隐式策略在机器人领域应用广泛，但它们在计算成本高、推断过程容易陷入暴露偏差与动态不稳定问题，面对分布转移时容易发散。能量模型（EBM）虽能缓解上述问题，但此前在大规模任务上的可扩展性和表现有限，因此需要一种能够结合两者优点的新方法。

Method: 作者提出EBT-Policy，将能量模型的优势与Transformer架构结合，设计了用于物理实体机器人的新一代策略模型。该架构通过自回归推断能量景观，实现高维空间中的稳健学习。实验涵盖仿真和真实世界任务，并与扩散策略法进行对比。

Result: EBT-Policy在多项仿真和实际机器人任务中稳定优于扩散策略模型；不仅训练和推断计算量更低，有时推断步数可从100降至2步（加速50倍），还展现了前所未有的能力（如零样本失败恢复），即无需特殊训练即可克服失败序列恢复。

Conclusion: EBT-Policy不仅提升了策略推断效率，还强化了面向分布转移的鲁棒性和泛化能力，为实现稳定、通用的机器人行为提供了新方向，有望成为未来VLA模型和机器人策略学习的核心架构。

Abstract: Implicit policies parameterized by generative models, such as Diffusion
Policy, have become the standard for policy learning and Vision-Language-Action
(VLA) models in robotics. However, these approaches often suffer from high
computational cost, exposure bias, and unstable inference dynamics, which lead
to divergence under distribution shifts. Energy-Based Models (EBMs) address
these issues by learning energy landscapes end-to-end and modeling equilibrium
dynamics, offering improved robustness and reduced exposure bias. Yet, policies
parameterized by EBMs have historically struggled to scale effectively. Recent
work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs
to high-dimensional spaces, but their potential for solving core challenges in
physically embodied models remains underexplored. We introduce a new
energy-based architecture, EBT-Policy, that solves core issues in robotic and
real-world settings. Across simulated and real-world tasks, EBT-Policy
consistently outperforms diffusion-based policies, while requiring less
training and inference computation. Remarkably, on some tasks it converges
within just two inference steps, a 50x reduction compared to Diffusion Policy's
100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior
models, such as zero-shot recovery from failed action sequences using only
behavior cloning and without explicit retry training. By leveraging its scalar
energy for uncertainty-aware inference and dynamic compute allocation,
EBT-Policy offers a promising path toward robust, generalizable robot behavior
under distribution shifts.

</details>


### [145] [Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs](https://arxiv.org/abs/2510.27558)
*Sushil Samuel Dinesh,Shinkyu Park*

Main category: cs.RO

TL;DR: 提出了一套无需特定领域训练，直接利用预训练基础模型来实现机器人操控的框架，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操控系统通常需要大量领域特定的数据和训练，应用基础模型以减少对定制训练的依赖具有重要意义。

Method: 该框架结合了多模态基础模型的感知能力和通用推理模型，用于任务序列决策，同时利用动态维护的场景图提升空间感知与一致性推理，无需领域特定训练。

Result: 通过一系列桌面操作实验评估，展示了该框架直接基于基础模型构建机器人操控系统的可行性和潜力。

Conclusion: 利用预训练基础模型和无领域特定训练的方案，有望大幅简化高效机器人操控系统的开发流程。

Abstract: This paper presents a framework that leverages pre-trained foundation models
for robotic manipulation without domain-specific training. The framework
integrates off-the-shelf models, combining multimodal perception from
foundation models with a general-purpose reasoning model capable of robust task
sequencing. Scene graphs, dynamically maintained within the framework, provide
spatial awareness and enable consistent reasoning about the environment. The
framework is evaluated through a series of tabletop robotic manipulation
experiments, and the results highlight its potential for building robotic
manipulation systems directly on top of off-the-shelf foundation models.

</details>


### [146] [Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping](https://arxiv.org/abs/2510.27666)
*Dong Heon Han,Xiaohao Xu,Yuxi Chen,Yusheng Zhou,Xinqi Zhang,Jiaqi Wang,Daniel Bruder,Xiaonan Huang*

Main category: cs.RO

TL;DR: 本文提出了一种新型的模块化软体夹持器，其能够像章鱼一样，通过协作与全身形变来适应多种抓取需求，从而显著提升了机器人操作的灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前软体夹持器受制于整体结构形态固定，仅有局部形变，难以模拟生物如章鱼般的全身自适应操作能力，应用场景受限。

Method: 设计了一种由分布式自感知气动驱动器组成的模块化软体夹持架构，通过集成感知传感器，实现夹持器全局可控形态变换，能够形成多种多边形形状，并适应不同的夹持需求。

Result: 系统能在精细夹取和大范围包覆之间平滑切换，扩大了夹取范围，对标准及不规则物体有更好泛化（尺寸可扩展至10倍），还可完成多物体同时抓取及内部钩挂等新颖操作。

Conclusion: 本文提出的低成本、易制造、可扩展的感知与驱动一体化软体夹持架构，为机器人操作接近生物灵巧性提供了新路径。

Abstract: Biological systems, such as the octopus, exhibit masterful cross-scale
manipulation by adaptively reconfiguring their entire form, a capability that
remains elusive in robotics. Conventional soft grippers, while compliant, are
mostly constrained by a fixed global morphology, and prior shape-morphing
efforts have been largely confined to localized deformations, failing to
replicate this biological dexterity. Inspired by this natural exemplar, we
introduce the paradigm of collaborative, whole-body proprioceptive morphing,
realized in a modular soft gripper architecture. Our design is a distributed
network of modular self-sensing pneumatic actuators that enables the gripper to
intelligently reconfigure its entire topology, achieving multiple morphing
states that are controllable to form diverse polygonal shapes. By integrating
rich proprioceptive feedback from embedded sensors, our system can seamlessly
transition from a precise pinch to a large envelope grasp. We experimentally
demonstrate that this approach expands the grasping envelope and enhances
generalization across diverse object geometries (standard and irregular) and
scales (up to 10$\times$), while also unlocking novel manipulation modalities
such as multi-object and internal hook grasping. This work presents a low-cost,
easy-to-fabricate, and scalable framework that fuses distributed actuation with
integrated sensing, offering a new pathway toward achieving biological levels
of dexterity in robotic manipulation.

</details>
