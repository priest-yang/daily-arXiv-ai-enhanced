<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.CL](#cs.CL) [Total: 46]
- [cs.RO](#cs.RO) [Total: 31]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection](https://arxiv.org/abs/2601.18845)
*Zeineb Dridi,Jihen Bennaceur,Amine Ben Hassouna*

Main category: cs.CV

TL;DR: 该论文提出了一种基于动态掩码的后门攻击方法，专为目标检测模型设计，基于数据集投毒实现后台触发，在真菌识别任务上验证其有效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型广泛应用于计算机视觉领域，但也暴露于各类对抗性攻击，特别是后门攻击。目标检测模型在现实关键领域（如真菌识别）中的实际风险需引起重视，且目前后门攻击方法多为静态、不易躲避检测，激发研究者探索更隐蔽有效的攻击方式。

Method: 该方法利用数据集投毒，在训练数据中动态嵌入由SAM分割模型生成的掩码和触发器，从而训练出的目标检测模型（如YOLOv7）在命中动态触发时被操控，实现高效隐蔽的后门注入。

Result: 实验证明，该动态掩码后门攻击方法在干净样本上保证高准确率，对投毒样本有高攻击成功率，且优于传统静态后门注入方式。

Conclusion: 论文结果强调，应重视并建立强健的对策以防护深度学习模型免受不断演进的隐蔽后门与对抗性威胁。

Abstract: Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.

</details>


### [2] [Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding](https://arxiv.org/abs/2601.18849)
*Yuhui Zhang,Hui Yu,Wei Liang,Sunjie Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合眨眼嵌入和哈希网格特征点编码的方法，提升了动态NeRF在口型复现方面的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管动态NeRF能生成高质量的三维说话头像，但在嘴部动作的准确捕捉和高效建模上仍存在挑战。口型是叙述性肖像生成中重要且复杂的部分，现有方法表现仍不理想，需要新方法提升嘴部动作的拟真度。

Method: 作者提出利用眨眼嵌入和哈希网格特征点编码，将面部特征作为条件特征，音频特征通过Dynamic Landmark Transformer集成为残差项，然后运用神经辐射场来整体建模脸部，获得更真实的说话人脸。

Result: 实验表明，该方法在面部动作、尤其是口型还原方面较现有方法具有更优的表现，增强了生成头像的真实性和细腻度。

Conclusion: 通过融合眨眼嵌入、哈希网格编码和改进的特征集成方法，能有效提升动态NeRF在人脸逼真度方面的表现，尤其是在复杂口型捕捉上优于前人方法。

Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.

</details>


### [3] [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/abs/2601.18851)
*Wei Liang,Hui Yu,Derui Ding,Rachael E. Jack,Philippe G. Schyns*

Main category: cs.CV

TL;DR: 本文提出了一种结合3DMM与StyleGAN的新方法，利用自拍视频生成高细节的头部虚拟化身，实现比现有方法更真实丰富的重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前3DMM方法难以实时完整捕捉头部及背景细节，GAN方法虽能高质量再现但细节不佳，且现有方法普遍需要大量训练数据，缺少仅用自拍视频生成化身的技术。

Method: 方法结合3DMM与StyleGAN生成器，设计用于前景重建和化身生成的混合损失函数，通过对自拍视频的训练获得高频细节，提升头部造型和纹理质量。

Result: 在自我重演和互相重演任务中，定性定量结果显示本方法较现有方法能重建出更丰富细致的头部细节与复杂纹理。

Conclusion: 提出的自拍视频虚拟化身重演方法有效提升了头部细节和真实感，为相关应用带来更真实的用户体验。

Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.

</details>


### [4] [Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)](https://arxiv.org/abs/2601.18891)
*Ghazaleh Serati,Samuel Foucher,Jerome Theau*

Main category: cs.CV

TL;DR: 该论文提出了一种基于弱监督预训练的检测方法（HerdNet）以提升北极地区驯鹿自动检测的鲁棒性，高效支持大规模航拍影像中的动物分布监测。实验表现优异，在检测准确率上大幅提升，具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 北极驯鹿数量近年来显著下降，需要高效、自动化的监测手段辅助保护决策。然而，现有人工解读航拍图片费时且容易出错，自动检测面临背景复杂、类别极度不平衡、目标小且遮挡等难题。

Method: 提出一种基于检测网络结构的弱监督补丁级预训练方法。利用包含阿拉斯加五个驯鹿群的检测数据集，首先对补丁进行空/非空（动物有无）标签的弱监督学习，为后续驯鹿检测提供更优初始化权重。对比原始模型和ImageNet预训练权重验证效果。

Result: 在多群体2017年影像及独立2019年测试集上，弱监督预训练方法F1值分别达93.7%和92.6%。相比于ImageNet权重，检测正样本补丁和全图计数F1值均显著提升（最高至95.5%）。主要误差为动物状背景误检及低密度遮挡漏检。

Conclusion: 采用空/非空粗粒度标签进行弱监督预训练，即使标注数据有限，依然能显著提升检测准确度，表现优于传统通用特征初始化，对大规模野生动物航空影像监测有实际促进作用。

Abstract: Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.

</details>


### [5] [RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection](https://arxiv.org/abs/2601.18900)
*Haim Zisman,Uri Shaham*

Main category: cs.CV

TL;DR: 本论文提出了一种统计学基础的假图像检测方法，通过结合多种无需训练的检测器统计量，利用p值和统计集成技术，从而实现对图像真实概率的可解释评分，提升了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前AI生成图像的检测依然具有挑战性，现有方法常缺乏形式化解释性，并依赖隐含假设，导致在分布漂移情况下鲁棒性不足。作者希望提出一种解释性更强、鲁棒性更好的假图像检测方法。

Method: 提出一种严格的统计框架，将多个已有的、无需训练的检测器统计量结合起来，计算各自的p值，并通过经典统计集成方法进行汇总，对图像与真实分布的对齐程度给出概率解释。该方法无需重新训练，且具备灵活性和通用性。

Result: 通过统计集成多个检测器的输出，可以更好地对抗分布变化场景，实现对假图像的更为鲁棒、泛化的检测，且结果具有概率上的解释性。

Conclusion: 提出的方法具有通用性、灵活性和无需训练等优点，能够为不断演化的生成图像检测提供一种更加坚实和可解释的解决方案。

Abstract: As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.

</details>


### [6] [On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training](https://arxiv.org/abs/2601.18929)
*John J. Han,Adam Schmidt,Muhammad Abdullah Jamal,Chinedu Nwoye,Anita Rau,Jie Ying Wu,Omid Mohareri*

Main category: cs.CV

TL;DR: 本文针对外科场景理解任务，系统比较了不同视觉基础模型（VFMs）对深度信息的利用，发现融合几何（深度）信息的多模态预训练在各类任务中实现了显著优于单一RGB输入的性能，同时提升了数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景视觉模型主要依赖于RGB图像，忽略了手术环境中的3D几何复杂性。尽管通用视觉领域有多模态和几何感知方法，但其在手术中的效果仍缺乏系统性验证。作者希望明确多模态（RGB-D）预训练对提升手术视觉模型性能的具体作用。

Method: 作者基于1.4百万配有深度图的手术图像，对八种ViT结构的视觉基础模型进行预训练，涵盖不同的预训练域、优化目标和输入模态（RGB vs RGB-D）。随后在八个手术数据集上，就目标检测、分割、深度与姿态估计等任务，比较了模型在冻结骨干网络和端到端微调两种设置下的表现。重点考察显式几何标记输入（如MultiMAE）模型的效果。

Result: 结果显示具备几何标记能力的模型（如MultiMAE）在所有任务上均显著优于单模态基线。此外，几何感知预训练还带来了极高数据效率——仅用25%的标注数据微调，就可超过用全部数据训练的RGB模型。且深度信息只在预训练阶段使用，推理时不增加额外负担。

Conclusion: 研究证明了多模态（特别是包含深度/几何信息）预训练能有效提升手术视觉任务表现且易于实际应用，建议更多手术场景视觉系统采纳此类方法。

Abstract: Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding. However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments. Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored. We conduct a large-scale empirical study comparing eight ViT-based VFMs that differ in pre-training domain, learning objective, and input modality (RGB vs. RGB-D). For pre-training, we use a curated dataset of 1.4 million robotic surgical images paired with depth maps generated from an off-the-shelf network. We evaluate these models under both frozen-backbone and end-to-end fine-tuning protocols across eight surgical datasets spanning object detection, segmentation, depth estimation, and pose estimation. Our experiments yield several consistent findings. Models incorporating explicit geometric tokenization, such as MultiMAE, substantially outperform unimodal baselines across all tasks. Notably, geometric-aware pre-training enables remarkable data efficiency: models fine-tuned on just 25% of labeled data consistently surpass RGB-only models trained on the full dataset. Importantly, these gains require no architectural or runtime changes at inference; depth is used only during pre-training, making adoption straightforward. These findings suggest that multimodal pre-training offers a viable path towards building more capable surgical vision systems.

</details>


### [7] [Smart Split-Federated Learning over Noisy Channels for Embryo Image Segmentation](https://arxiv.org/abs/2601.18948)
*Zahra Hafezi Kafshgari,Ivan V. Bajic,Parvaneh Saeedi*

Main category: cs.CV

TL;DR: 本论文研究了Split-Federated（SplitFed）学习中通信信道噪声对模型训练和最终性能的影响，并提出了一种能显著提升抗噪性的智能平均策略。


<details>
  <summary>Details</summary>
Motivation: SplitFed学习虽然降低了终端设备的计算需求，但在实际分布式环境下，通信信道质量不佳（如存在噪声）会影响特征、梯度和模型参数的传递，进而影响模型性能。因此，提升SplitFed学习的通信鲁棒性是其实际部署的重要难点。

Method: 本文提出了一种智能平均（smart averaging）策略，有别于传统的直接平均权重或梯度的方法，通过更有效地融合来自不同客户端的模型信息，以减弱信道噪声带来的不利影响。

Result: 在胚胎图像分割模型上的实验表明，提出的智能平均策略相比传统平均方法，可在信道噪声强两数量级的情况下，依然维持模型的准确率。

Conclusion: 智能平均策略能大大增强SplitFed学习在有噪声通信环境下的鲁棒性，对实际应用中的模型性能提升具有重要意义。

Abstract: Split-Federated (SplitFed) learning is an extension of federated learning that places minimal requirements on the clients computing infrastructure, since only a small portion of the overall model is deployed on the clients hardware. In SplitFed learning, feature values, gradient updates, and model updates are transferred across communication channels. In this paper, we study the effects of noise in the communication channels on the learning process and the quality of the final model. We propose a smart averaging strategy for SplitFed learning with the goal of improving resilience against channel noise. Experiments on a segmentation model for embryo images shows that the proposed smart averaging strategy is able to tolerate two orders of magnitude stronger noise in the communication channels compared to conventional averaging, while still maintaining the accuracy of the final model.

</details>


### [8] [Pay Attention to Where You Look](https://arxiv.org/abs/2601.18970)
*Alex Beriand,JhihYang Wu,Daniel Brignac,Natnael Daba,Abhijit Mahalanobis*

Main category: cs.CV

TL;DR: 本文提出了一种自适应视角加权机制，有效提升了基于少量输入视角的新视角合成（NVS）任务中的效果。作者设计了两种加权方法，并验证其能提升生成图片的真实感和准确性。该机制易于集成到其他NVS算法中。


<details>
  <summary>Details</summary>
Motivation: 当前Few-shot NVS方法通常默认所有输入视图对生成目标图像的重要性相同，这种假设导致生成效果不佳。如何根据源视图与目标视图之间的关联性调整每个视图的权重是一个关键问题。

Method: 作者提出了两种源视图加权策略：一种是基于几何属性的确定性加权（如欧氏距离、角度差等），另一种是基于交叉注意力的学习型加权。此外，该机制可作为训练过程的附加模块，细化模型对视角相关性的理解，提高合成质量。

Result: 实验表明，自适应加权机制能显著提升合成图像的清晰度、准确率及真实感，在多个基准数据集和不同NVS算法上均取得了优于原始方法的效果。

Conclusion: 自适应相机加权机制不仅提升了NVS的表现，还具备良好的兼容性，可为后续相关研究提供新的提升方向。

Abstract: Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.

</details>


### [9] [FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction](https://arxiv.org/abs/2601.18993)
*Wei Cao,Hao Zhang,Fengrui Tian,Yulun Wu,Yingying Li,Shenlong Wang,Ning Yu,Yaoyao Liu*

Main category: cs.CV

TL;DR: 本文提出FreeOrbit4D，解决单目视频在大视角场景重定向中的几何歧义与时序不一致问题，显著提升了视频重定向的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 单目视频只提供了动态3D场景的局部视角，导致在大角度重定向时几何结构及时序表现往往不连贯。现有扩散模型方法在远离原始轨迹的视角下表现欠佳，急需针对大角度重定向时的几何歧义提出可靠的解决方案。

Method: 提出了无需训练的FreeOrbit4D框架，将单目视频解构为静态背景和几何不完整的前景点云，再运用面向物体的多视角扩散模型合成多视角图像，实现前景的完整几何重建。通过点云对齐方式，在全局场景空间下融合前景与背景，最终利用该4D代理作为结构基础，引导有条件的视频扩散模型生成视频。

Result: 在多项实验中，FreeOrbit4D在大角度轨迹下，生成更忠实、时序更连贯的重定向视频。其4D几何代理还展现了实际应用潜力，如编辑传播和4D数据生成。

Conclusion: FreeOrbit4D框架有效解决了单目视频在大视角重定向下的几何和时序一致性难题，为相关应用提供了坚实的技术基础。

Abstract: Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.

</details>


### [10] [Anatomically-aware conformal prediction for medical image segmentation with random walks](https://arxiv.org/abs/2601.18997)
*Mélanie Gaillochet,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 本文提出了一种结合随机游走的共形预测方法（RW-CP），用于医学影像分割不确定性量化，在保证统计有效覆盖率的同时提升解剖结构连贯性和分割质量。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习医学影像分割方法在引入共形预测进行不确定性量化时，常导致分割结果缺乏解剖连贯性。例如，分割区域破碎、边界不连续及过度分割等问题，限制了临床应用。迫切需要一种既能保证统计置信又考虑解剖约束的不确定性量化方法。

Method: 提出RW-CP框架，将随机游走与共形预测相结合，首先利用预训练视觉基础模型特征构建k近邻图，在图结构上进行随机游走扩散，将传统分割模型输出的非一致性分数进行空间正则化，提升分割结果的连贯性和稳定性。同时，RW-CP支持任意分割模型作为基础方法。

Result: 在多模态公开医学影像数据集上，RW-CP在设定允许误差率为α=0.1时，相较于标准共形预测基线，分割质量提升最高达35.4%，同时保持严格的统计覆盖率。

Conclusion: RW-CP方法在保持共形预测严格边界覆盖保证的同时，显著提升了分割区域的解剖连贯性和整体质量，具有良好模型兼容性，为医学影像智能分割的临床实用性带来提升。

Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $λ$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $α=0.1$.

</details>


### [11] [Non-Invasive 3D Wound Measurement with RGB-D Imaging](https://arxiv.org/abs/2601.19014)
*Lena Harkämper,Leo Lebrat,David Ahmedt-Aristizabal,Olivier Salvado,Mattias Heinrich,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: 该论文提出了一种基于RGB-D成像的快速、无创3D创口测量算法，能够自动计算创口的相关临床参数并实现高精度、低变异度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 慢性创口的监测和管理需要准确、高效的测量方法。传统方法存在主观性强、效率低等问题，因此有必要开发自动化、无创且高速的三维测量技术。

Method: 方法将RGB-D测距与B样条曲面重建技术结合，生成详细的3D创口网格模型，实现自动计算创口周长、表面积及尺寸等临床测量指标；在测试中使用仿真硅胶创口模型进行验证，并与高分辨率地面真值扫描进行对比。

Result: 系统实现了亚毫米级三维重建精度，反复采集下测量值变异度低，与手动评估高度一致，而且在准确度和实时性方面均优于现有RGB-D重建方法。

Conclusion: 本研究提出的方法为临床和远程医疗环境下创口自动评估提供了一种有前景的工具，兼具高精度、低变异及实时运行能力。

Abstract: Chronic wound monitoring and management require accurate and efficient wound measurement methods. This paper presents a fast, non-invasive 3D wound measurement algorithm based on RGB-D imaging. The method combines RGB-D odometry with B-spline surface reconstruction to generate detailed 3D wound meshes, enabling automatic computation of clinically relevant wound measurements such as perimeter, surface area, and dimensions. We evaluated our system on realistic silicone wound phantoms and measured sub-millimetre 3D reconstruction accuracy compared with high-resolution ground-truth scans. The extracted measurements demonstrated low variability across repeated captures and strong agreement with manual assessments. The proposed pipeline also outperformed a state-of-the-art object-centric RGB-D reconstruction method while maintaining runtimes suitable for real-time clinical deployment. Our approach offers a promising tool for automated wound assessment in both clinical and remote healthcare settings.

</details>


### [12] [NC-Reg : Neural Cortical Maps for Rigid Registration](https://arxiv.org/abs/2601.19042)
*Ines Vati,Pierrick Bourgeat,Rodrigo Santa Cruz,Vincent Dore,Olivier Salvado,Clinton Fookes,Léo Lebrat*

Main category: cs.CV

TL;DR: 该论文提出了神经皮层图（neural cortical maps），作为一种替代传统网格和网面结构的连续且紧凑的神经表示方法。


<details>
  <summary>Details</summary>
Motivation: 当前皮层特征图多采用离散网格或网面结构，在处理分辨率、效率和通用性等方面存在局限。研究者希望找到一种既高效又具备多分辨率能力的新型皮层特征表示方式，以提升皮层表面配准等典型任务的性能。

Method: 提出神经皮层图，使用神经网络表示皮层特征图，不依赖固定网格或网面，可从任意大小的网面学习并在任意分辨率下提供特征值。利用该表示，作者设计了NC-Reg算法，结合梯度下降和模拟退火策略，用于刚性注册皮层表面，并通过消融实验和主体到模板的实验评估方法性能。

Result: 新方法能在球面上高效优化，速度比传统重心插值高达30倍。在皮层表面配准实验中，方法能达到小于1度的亚度级精度，表现优异。

Conclusion: 神经皮层图为皮层特征表示与配准提供了新思路，其高效率与高精度表现展现出临床实际应用的潜力，尤其适合作为稳健的预配准策略。

Abstract: We introduce neural cortical maps, a continuous and compact neural representation for cortical feature maps, as an alternative to traditional discrete structures such as grids and meshes. It can learn from meshes of arbitrary size and provide learnt features at any resolution. Neural cortical maps enable efficient optimization on the sphere and achieve runtimes up to 30 times faster than classic barycentric interpolation (for the same number of iterations). As a proof of concept, we investigate rigid registration of cortical surfaces and propose NC-Reg, a novel iterative algorithm that involves the use of neural cortical feature maps, gradient descent optimization and a simulated annealing strategy. Through ablation studies and subject-to-template experiments, our method demonstrates sub-degree accuracy ($<1^\circ$ from the global optimum), and serves as a promising robust pre-alignment strategy, which is critical in clinical settings.

</details>


### [13] [NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation](https://arxiv.org/abs/2601.19048)
*Han-Hung Lee,Cheng-Yu Yang,Yu-Lun Liu,Angel X. Chang*

Main category: cs.CV

TL;DR: 本文提出了NuiWorld框架，实现可控、高效且可扩展的世界生成，并克服了现有方法在数据稀缺、分辨率拓展和推理效率等方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有世界生成方法在可控性、规模扩展性和效率方面存在明显挑战，难以满足游戏、仿真和机器人等应用需求；同时，受限于数据量、固定分辨率和计算消耗，生成大尺度场景时效果不佳。

Method: 提出了一种生成式自举策略，通过少量输入图像，结合最新的3D重建与可扩展场景生成技术，自动合成不同规模与布局的场景，并用于端到端模型训练。同时，通过伪素描标签提升场景可控性，采用可变场景分块与扁平集合向量压缩表示，降低大场景令牌长度，提高训练和推理效率。

Result: NuiWorld能利用极少输入数据生成多样场景，并用伪素描标签控制生成内容，表现出一定的泛化能力，对未见过的素描仍有适应性。新方法显著提升了大场景的几何保真度，并改善了效率。

Conclusion: NuiWorld有效解决了世界生成中的可控性、扩展性和效率难题，为相关领域生成任务提供了更可行和高效的新方案。

Abstract: World generation is a fundamental capability for applications like video games, simulation, and robotics. However, existing approaches face three main obstacles: controllability, scalability, and efficiency. End-to-end scene generation models have been limited by data scarcity. While object-centric generation approaches rely on fixed resolution representations, degrading fidelity for larger scenes. Training-free approaches, while flexible, are often slow and computationally expensive at inference time. We present NuiWorld, a framework that attempts to address these challenges. To overcome data scarcity, we propose a generative bootstrapping strategy that starts from a few input images. Leveraging recent 3D reconstruction and expandable scene generation techniques, we synthesize scenes of varying sizes and layouts, producing enough data to train an end-to-end model. Furthermore, our framework enables controllability through pseudo sketch labels, and demonstrates a degree of generalization to previously unseen sketches. Our approach represents scenes as a collection of variable scene chunks, which are compressed into a flattened vector-set representation. This significantly reduces the token length for large scenes, enabling consistent geometric fidelity across scenes sizes while improving training and inference efficiency.

</details>


### [14] [Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models](https://arxiv.org/abs/2601.19060)
*Jeonghwan Kim,Renjie Tao,Sanat Sharma,Jiaqi Wang,Kai Sun,Zhaojiang Lin,Seungwhan Moon,Lambert Mathias,Anuj Kumar,Heng Ji,Xin Luna Dong*

Main category: cs.CV

TL;DR: 本文提出PixSearch，一种端到端的多模态大模型方法，通过结合区域级感知和检索增强推理，提升VQA的事实一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉问答（VQA）不仅需要细粒度的视觉感知，还常常依赖于超出输入图像的事实知识。以往的多模态检索增强生成（MM-RAG）方法虽然改进了事实溯源，但缺少何时及如何检索的内部策略，因此需要一种能自适应决定检索时机和方式的端到端解决方案。

Method: 作者提出PixSearch，将区域级感知与检索增强推理统一建模。在编码阶段，模型会动态发出<search>标记以触发检索，并自动选择检索的查询模态（文本、图像或区域）。同时还生成像素级掩模作为视觉查询，实现无须依赖外部模块化流程如检测或分割器。训练采用两阶段监督策略，通过交错的检索监督来教授检索时机、模态选择，同时保留分割能力。

Result: 在注重实体和主观视角的VQA基准测试中，PixSearch大幅提升事实一致性和泛化能力，相比于Whole Image检索方法，在CRAG-MM基准上准确率相对提升19.7%；同时在多种VQA和仅文本问答任务上也保持了有竞争力的推理能力。

Conclusion: PixSearch首次实现了区域感知与检索增强推理的端到端整合，显著提升VQA的事实一致性和泛化能力，为多模态大模型视觉推理提出新的范式。

Abstract: Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits <search> tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.

</details>


### [15] [m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning](https://arxiv.org/abs/2601.19099)
*Yosub Shin,Michael Buriek,Igor Molybog*

Main category: cs.CV

TL;DR: 本文提出了一个新的空间推理基准m2sv，用于评估视觉-语言模型在将地图与街景图片对齐时的空间推理能力，发现现有模型在此任务上表现远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在多模态基准上表现出色，但在需要将抽象地图与主观街景图像进行空间对齐的任务上依旧表现脆弱。该领域缺乏相关大规模、高质量基准，难以推进深入研究。

Method: 作者提出了m2sv基准，要求模型通过对齐正北朝上的地图与同一地点的街景图像，推断摄像头朝向。m2sv-20k数据集多样且具备可控的歧义性，并提供了m2sv-sft-11k结构化推理轨迹用于监督微调。

Result: 当前最佳视觉-语言模型在m2sv的准确率仅为65.2%，远低于95%的真人基线。监督微调与强化学习均带来提升，但模型泛化能力有限，跨基准迁移效果不佳。论文还对推理难度与失误类型进行了系统分析。

Conclusion: 现有视觉-语言模型在跨视角空间推理任务上依然有明显短板，主要在几何对齐、证据聚合与推理一致性等方面存在瓶颈，需推动更具空间感知能力的模型发展。

Abstract: Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.

</details>


### [16] [Glance and Focus Reinforcement for Pan-cancer Screening](https://arxiv.org/abs/2601.19103)
*Linshan Wu,Jiaxin Zhuang,Hao Chen*

Main category: cs.CV

TL;DR: 该论文提出了GF-Screen，一个基于“快速浏览+精细聚焦”理念的强化学习框架，用于大规模CT图像的泛癌筛查，以有效定位和分割多种微小病灶，显著提升检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法难以在大规模CT扫描数据中对多类型微小病灶进行有效定位，前景-背景极度不平衡使模型容易忽略患病区域，且过多关注健康区域不仅效率低还增加误报，亟需改进。

Method: 提出GF-Screen框架，利用一个Glance（快速浏览）模型从完整CT选出潜在病灶子区域，再用Focus（聚焦）模型对这些子区域进行精准分割。Segmentation结果通过强化学习形式反馈奖励Glance模型，并设计了组相对学习范式，优化子区域选择策略，提升效率及准确性。

Result: 在16个内部及7个外部数据集（涵盖9种病灶）上广泛验证，GF-Screen方法有效提升检测性能。在MICCAI FLARE25泛癌挑战赛上，该方法在公共验证排行榜上领先，DSC和NSD分别提升25.6%和28.2%。

Conclusion: GF-Screen将先进的强化学习方法首次有效应用于泛癌筛查场景，显著提升了识别多类微小病灶的能力，降低误报并提升效率，为临床大规模AI辅助癌症筛查提供了新路径。

Abstract: Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).

</details>


### [17] [Reg-TTR, Test-Time Refinement for Fast, Robust and Accurate Image Registration](https://arxiv.org/abs/2601.19114)
*Lin Chen,Yue He,Fengting Zhang,Yaonan Wang,Fengming Lin,Xiang Chen,Min Liu*

Main category: cs.CV

TL;DR: 本文提出了一种称为Reg-TTR的测试时细化框架，结合深度学习和传统方法优势，在几乎不增加太多计算量的情况下，提升了图像配准的精度。


<details>
  <summary>Details</summary>
Motivation: 传统图像配准方法虽然鲁棒但运算较慢，深度学习虽快但对领域变化敏感。新出现的基础模型在速度和鲁棒性之间折中，但准确率通常不及为特定数据集训练的专用模型。因此需要一种方法来弥补基础模型和专用模型之间的性能差距。

Method: 提出Reg-TTR方法，在推理时对预训练模型结果进一步细化，将深度学习与传统方法的优势结合，在只增加约21%计算时间的基础上，提升注册精度。

Result: 在两个任务上实验，Reg-TTR达到了SOTA性能，且推理速度接近之前的深度学习方法。

Conclusion: Reg-TTR为提升基础图像配准模型性能提供了高效策略，有助于缩小其与专用模型间的性能差距，适合未来基础模型的应用。

Abstract: Traditional image registration methods are robust but slow due to their iterative nature. While deep learning has accelerated inference, it often struggles with domain shifts. Emerging registration foundation models offer a balance of speed and robustness, yet typically cannot match the peak accuracy of specialized models trained on specific datasets. To mitigate this limitation, we propose Reg-TTR, a test-time refinement framework that synergizes the complementary strengths of both deep learning and conventional registration techniques. By refining the predictions of pre-trained models at inference, our method delivers significantly improved registration accuracy at a modest computational cost, requiring only 21% additional inference time (0.56s). We evaluate Reg-TTR on two distinct tasks and show that it achieves state-of-the-art (SOTA) performance while maintaining inference speeds close to previous deep learning methods. As foundation models continue to emerge, our framework offers an efficient strategy to narrow the performance gap between registration foundation models and SOTA methods trained on specialized datasets. The source code will be publicly available following the acceptance of this work.

</details>


### [18] [FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation](https://arxiv.org/abs/2601.19115)
*Xiang Gao,Yunpeng Jia*

Main category: cs.CV

TL;DR: 本文提出了一种名为FBSDiff的新方法，无需训练即可实现灵活、高效的文本引导图像到图像（I2I）转换，通过在频域替换扩散特征，达到对图像外观、布局和轮廓的精细控制。升级版本FBSDiff++进一步提升了速度、兼容性和功能性，并在多项实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型在文本生成图像方面取得了巨大进展，但在需结合源图像作为指导的I2I任务下，如何精细、可控地融合图像和文本信息仍具挑战。作者希望提出方法，在无需训练和微调的前提下，提升I2I的自由度、可控性和效率。

Method: FBSDiff通过动态频段替换扩散特征，将原有T2I扩散模型无缝适配为图像到图像转换任务。具体来说，通过逐步替换低频、中频和高频带的扩散特征，实现对生成图像外观、布局与轮廓等不同层次属性的控制。无需额外训练，仅以插件形式部署。升级的FBSDiff++则在结构优化加速推理，并允许任意分辨率输入，还支持局部编辑和风格限定内容生成。

Result: FBSDiff及其升级版FBSDiff++在多项定量和定性实验中表现优异。FBSDiff++相比原始模型推理速度提升约8.9倍，兼容不同尺寸输入，支持更多编辑功能，生成质量、效率与可控性均超过现有高级I2I方法。

Conclusion: FBSDiff/FBSDiff++在无需模型训练的前提下，实现了高效、灵活、可控的文本驱动图像到图像转换，为该领域带来了新思路和实用技术，有望广泛应用于图像编辑与生成场景。

Abstract: With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.

</details>


### [19] [Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection](https://arxiv.org/abs/2601.19127)
*Zhilong Zhang,Lei Zhang,Qing He,Shuyin Xia,Guoyin Wang,Fuxiang Huang*

Main category: cs.CV

TL;DR: 本文研究了开放世界目标检测中的领域不变表示难题，提出了一种改进的领域对抗学习方法GB-DAL，并通过基准测试显示该方法泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 开放世界目标检测中的领域泛化面临非因果因素的干扰。现有基于领域对抗学习的方法侧重学习领域不变信息，忽视了隐含的非因果因素，导致模型泛化性能受限。作者揭示了非因果因素难以被当前方法识别并处理的根本原因。

Method: 提出GB-DAL方法，包含基于原型的Granular Ball Splitting（PGBS）模块，用于将有限的数据集划分为更细的“领域”，捕捉更多潜在非因果因素；此外，还引入模拟非因果因素(SNF)模块进行数据增强，降低非因果因素的隐含性并辅助模型训练。

Result: 在多个基准测试上，GB-DAL方法在开放世界目标检测任务中展现出更优的领域泛化能力，相较于传统领域对抗学习方法表现更好。

Conclusion: 作者提出的GB-DAL能够有效缓解非因果因素带来的影响，提升了模型在新领域中的泛化性能，为领域泛化和开放世界目标检测提供了新方法。

Abstract: Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.

</details>


### [20] [Resolving Primitive-Sharing Ambiguity in Long-Tailed Industrial Point Cloud Segmentation via Spatial Context Constraints](https://arxiv.org/abs/2601.19128)
*Chao Yin,Qing Han,Zhiwei Hou,Yue Liu,Anjin Dai,Hongda Hu,Ji Yang,Wei Yao*

Main category: cs.CV

TL;DR: 本文针对工业数字孪生中的点云分割，解决了安全关键部件（如减速器、阀门）被误分类的问题，提出结合空间上下文约束的改进损失函数，在极端类别不平衡和几何相似情况下显著提升了长尾类别的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在工业点云分割任务中，减速器、阀门等关键部件由于训练样本极少且与管道几何形状极为类似，导致经常被错误分类，严重影响数字孪生系统的安全和自动化知识提取。因此，亟需有效区分形状相似、但语义不同的稀有部件。

Method: 提出在Class-Balanced Loss基础上，结合两种泛化机制：（1）Boundary-CB：基于信息熵强化边界不确定区域；（2）Density-CB：基于点密度补偿不同扫描条件下的样本分布。这两种机制均为插拔式模块，无需修改网络结构，仅需替换损失函数。通过空间上下文一致性判别几何相似部件的语义类别。

Result: 在工业级数据集Industrial3D（610M点）上，整体mIoU达到55.74%；长尾类别分割性能提升21.7%（29.59% vs 24.32%），主干类别精度几乎无损（88.14%）；减速器IoU从0%升至21.12%，阀门提升24.3%。显著缓解了几何混淆且未带来主次类别精度互损。

Conclusion: 通过引入空间上下文约束，解决了极端类别不平衡和几何相似难题，实现了安全关键部件的鲁棒识别，为工业数字孪生的自动化知识提取提供可靠分割基础。

Abstract: Industrial point cloud segmentation for Digital Twin construction faces a persistent challenge: safety-critical components such as reducers and valves are systematically misclassified. These failures stem from two compounding factors: such components are rare in training data, yet they share identical local geometry with dominant structures like pipes. This work identifies a dual crisis unique to industrial 3D data extreme class imbalance 215:1 ratio compounded by geometric ambiguity where most tail classes share cylindrical primitives with head classes. Existing frequency-based re-weighting methods address statistical imbalance but cannot resolve geometric ambiguity. We propose spatial context constraints that leverage neighborhood prediction consistency to disambiguate locally similar structures. Our approach extends the Class-Balanced (CB) Loss framework with two architecture-agnostic mechanisms: (1) Boundary-CB, an entropy-based constraint that emphasizes ambiguous boundaries, and (2) Density-CB, a density-based constraint that compensates for scan-dependent variations. Both integrate as plug-and-play modules without network modifications, requiring only loss function replacement. On the Industrial3D dataset (610M points from water treatment facilities), our method achieves 55.74% mIoU with 21.7% relative improvement on tail-class performance (29.59% vs. 24.32% baseline) while preserving head-class accuracy (88.14%). Components with primitive-sharing ambiguity show dramatic gains: reducer improves from 0% to 21.12% IoU; valve improves by 24.3% relative. This resolves geometric ambiguity without the typical head-tail trade-off, enabling reliable identification of safety-critical components for automated knowledge extraction in Digital Twin applications.

</details>


### [21] [CLIP-Guided Unsupervised Semantic-Aware Exposure Correction](https://arxiv.org/abs/2601.19129)
*Puzhen Wu,Han Weng,Quan Zheng,Yi Zhan,Hewei Wang,Yiming Li,Jiahui Han,Rui Xu*

Main category: cs.CV

TL;DR: 提出了一种无监督、基于语义感知的新型曝光校正网络，结合Fast Segment Anything Model和CLIP，无需人工标注即可有效优化曝光，并在实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 曝光不当往往导致细节丢失、色彩失真和对比度降低。现有曝光校正主要面临：1）忽略了区域语义信息，导致色彩偏移；2）现实场景下曝光图像无真实标签，人工标注成本高。该工作旨在解决这两个痛点。

Method: 提出了无监督的语义感知曝光校正网络。方法包括：1）设计了一种自适应语义融合模块，将FastSAM提取的语义信息融合到图像特征中；2）采用多尺度残差空间mamba group进行细节恢复和曝光调整；3）利用经过微调的CLIP生成伪标签自动识别曝光情况，指导网络学习；4）引入语义-提示一致性损失，强化语义与图像的一致性，从而实现无监督训练。

Result: 实验证明，所提出方法在实际曝光校正任务中，视觉和客观指标均明显优于现有无监督方法。

Conclusion: 该方法实现了无监督、语义感知的高效曝光校正，为复杂曝光场景下的图像修复提供了更优的解决方案，减少了人工成本，具有广阔应用前景。

Abstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.

</details>


### [22] [QA-ReID: Quality-Aware Query-Adaptive Convolution Leveraging Fused Global and Structural Cues for Clothes-Changing ReID](https://arxiv.org/abs/2601.19133)
*Yuxiang Wang,Kunming Jiang,Tianxiang Zhang,Ke Tian,Gaozhe Jiang*

Main category: cs.CV

TL;DR: 本文针对衣物更换场景下的行人再识别任务，提出了融合外观和结构信息的质量感知双分支匹配方法QA-ReID，并通过多模态注意力和自适应卷积增强匹配鲁棒性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 衣物更换会导致传统行人再识别性能大幅下降，因此需要开发对服装变化鲁棒的再识别方法。

Method: 提出QA-ReID框架，结合RGB特征和基于人体解析的结构特征，通过多模态注意力模块自适应融合异构特征。在匹配阶段，设计了质量感知自适应卷积（QAConv-QA），引入像素级重要性加权与双向一致性约束以提升对衣物变化的适应能力。

Result: 在PRCC、LTCC和VC-Clothes等多个公开衣物变换行人再识别基准上，QA-ReID均取得了最优性能，并在跨衣物场景下显著优于现有方法。

Conclusion: QA-ReID有效提升了对衣物变化场景的鲁棒性，为服装变化严重的行人再识别任务提供了更优解决方案，具有较强的实用价值。

Abstract: Unlike conventional person re-identification (ReID), clothes-changing ReID (CC-ReID) presents severe challenges due to substantial appearance variations introduced by clothing changes. In this work, we propose the Quality-Aware Dual-Branch Matching (QA-ReID), which jointly leverages RGB-based features and parsing-based representations to model both global appearance and clothing-invariant structural cues. These heterogeneous features are adaptively fused through a multi-modal attention module. At the matching stage, we further design the Quality-Aware Query Adaptive Convolution (QAConv-QA), which incorporates pixel-level importance weighting and bidirectional consistency constraints to enhance robustness against clothing variations. Extensive experiments demonstrate that QA-ReID achieves state-of-the-art performance on multiple benchmarks, including PRCC, LTCC, and VC-Clothes, and significantly outperforms existing approaches under cross-clothing scenarios.

</details>


### [23] [TFFM: Topology-Aware Feature Fusion Module via Latent Graph Reasoning for Retinal Vessel Segmentation](https://arxiv.org/abs/2601.19136)
*Iftekhar Ahmed,Shakib Absar,Aftar Ahmad Sami,Shadman Sakib,Debojyoti Biswas,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 该论文提出了一种针对视网膜动静脉分割任务的拓扑感知方法，显著改善了传统卷积网络存在的血管分割不连续、碎片化问题，在公开数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 视网膜动静脉的精确分割对于心血管疾病的诊断有重要意义。然而，现有方法虽像素精度高，但常常造成血管分割不连通，影响临床分析和后续图结构建模。

Method: 作者提出融合了拓扑特征融合模块（TFFM）和图注意网络的新型神经网络框架，将局部特征映射到隐含图空间，结合Tversky损失和soft clDice损失以联合优化类别不平衡和拓扑连续性组成的混合损失函数。

Result: 在Fundus-AVSeg数据集上，该方法取得了Dice分数90.97%和95% Hausdorff距离3.50像素的领先表现，并将血管碎片率减少约38%。

Conclusion: 本文提出的方法生成了更连通、符合实际结构的血管分割结果，有助于实现自动化生物标志物定量分析，适合临床应用，并已开源实现。

Abstract: Precise segmentation of retinal arteries and veins carries the diagnosis of systemic cardiovascular conditions. However, standard convolutional architectures often yield topologically disjointed segmentations, characterized by gaps and discontinuities that render reliable graph-based clinical analysis impossible despite high pixel-level accuracy. To address this, we introduce a topology-aware framework engineered to maintain vascular connectivity. Our architecture fuses a Topological Feature Fusion Module (TFFM) that maps local feature representations into a latent graph space, deploying Graph Attention Networks to capture global structural dependencies often missed by fixed receptive fields. Furthermore, we drive the learning process with a hybrid objective function, coupling Tversky loss for class imbalance with soft clDice loss to explicitly penalize topological disconnects. Evaluation on the Fundus-AVSeg dataset reveals state-of-the-art performance, achieving a combined Dice score of 90.97% and a 95% Hausdorff Distance of 3.50 pixels. Notably, our method decreases vessel fragmentation by approximately 38% relative to baselines, yielding topologically coherent vascular trees viable for automated biomarker quantification. We open-source our code at https://tffm-module.github.io/.

</details>


### [24] [GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2601.19157)
*Yongsong Huang,Tzu-Hsuan Peng,Tomo Miyazaki,Xiaofeng Liu,Chun-Ting Chou,Ai-Chun Pang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的低光照图像超分辨率方法GTFMN，通过引入照明估计和纹理恢复两条处理流，有效提升了低光照图像的重建质量。


<details>
  <summary>Details</summary>
Motivation: 低光照图像超分辨率任务因分辨率低和光照不足导致的耦合退化问题而极具挑战，目前方法在细节恢复或光照增强方面存在局限性。

Method: 作者设计了GTFMN网络，将LLSR任务分解为照明估计和纹理恢复两部分。具体地，网络包含独立的Illumination Stream来预测空间变化的照明图，并通过Illumination Guided Modulation Block（IGM Block）利用该照明图对纹理特征流进行动态调制，实现对不同区域的自适应增强处理。

Result: 大量实验在OmniNormal5和OmniNormal15数据集上表明，GTFMN无论在定量指标还是视觉质量方面均优于现有主流方法，取得最佳表现。

Conclusion: GTFMN通过解耦照明与纹理的处理思路，实现了对低光照超分辨率问题的有效解决，为该领域的进一步研究提供了新的思路和方法。

Abstract: Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.

</details>


### [25] [SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing](https://arxiv.org/abs/2601.19180)
*Lifan Jiang,Boxi Wu,Yuhang Pei,Tianrun Wu,Yongyuan Chen,Yan Zhao,Shiyu Yu,Deng Cai*

Main category: cs.CV

TL;DR: SNR-Edit是一种无需反演的图像编辑新方法，通过自适应噪声控制与结构感知噪声校正，实现高保真图像变换并显著降低结构破坏。方法轻量、无须重新训练，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前流行的基于反演的图像编辑管线在实际应用中效率低、结构易损失，而现有的无反演方法常用固定的高斯噪声，容易造成编辑质量下降或结构劣化。该工作致力于降低这些缺陷，提升编辑的结构保真和应用便捷性。

Method: 提出SNR-Edit框架：对流式生成模型的无反演编辑流程引入结构感知的自适应噪声矫正。在生成初期，通过注入分割约束来调整初始噪声，将其锚定到真实图像隐式反演位置，从而在源目标变换过程中减少轨迹漂移。该方法无须额外训练，仅在编辑时轻量修正噪声分布即可。

Result: 在SD3和FLUX两个主流流式生成模型上，PIE-Bench和SNR-Bench等实测表明，SNR-Edit在像素级指标和视觉语言模型（VLM）评分上均有卓越表现，并且每张图像只增加约1秒的计算开销。

Conclusion: SNR-Edit有效解决了无反演流式编辑中的结构失真问题，无需模型调优和反演即可实现结构保真的高质量编辑，兼具实用性和推广潜力。

Abstract: Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.

</details>


### [26] [Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP](https://arxiv.org/abs/2601.19210)
*Sen Nie,Jie Zhang,Zhuo Wang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出一种高效的视觉-语言模型（VLMs）对抗攻击测试时防御方法Contrastive Spectral Rectification（CSR），该方法能有效提升在强对抗攻击下模型的鲁棒性，且适用性强，推理延迟低。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽然在零样本泛化上表现优异，但面对对抗样本时极易被攻击。当前测试时防御方法不能同时兼顾鲁棒性、效率和泛用性，亟需更优的方法。

Method: 作者首先分析对抗样本在频率逐步衰减下表现出的特征不一致性，并将其归因为模型的光谱偏置。基于此，提出了Contrastive Spectral Rectification（CSR）方法，在测试时对输入样本进行谱域引导的对比性校正扰动，促使输入回到自然流形，从而提升鲁棒性。该方法可自适应应用于不同输入。

Result: 在16个分类基准测试上，CSR方法平均在强AutoAttack下比现有最优方法提升18.1%的鲁棒性，同时推理开销有限。实验也表明CSR具有很强的任务泛化能力，可应用于多种视觉场景。

Conclusion: CSR为提升VLMs在对抗攻击下的健壮性提供了高效、通用且简单的解决方案，有望广泛应用于现实任务中。

Abstract: Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.

</details>


### [27] [UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection](https://arxiv.org/abs/2601.19222)
*Fuxiang Sun,Xi Jiang,Jiansheng Wu,Haigang Zhang,Feng Zheng,Jinfeng Yang*

Main category: cs.CV

TL;DR: 该论文提出了面向PCB质量检测的多模态视觉语言基准UniPCB，并相应训练了PCB-GPT模型，显著提升了细粒度缺陷定位能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在工业质检方向展现出潜力，但在复杂的PCB检测场景依旧表现不佳，主要原因是缺乏高质量、统一、标准化的视觉语言基准数据来评价这些模型。当前数据来源零散且标准不一，阻碍了MLLM在PCB质量检测领域的发展。

Method: 作者设计了系统性流程，收集和标准化来自不同来源的PCB质检数据，统一整理成三个注释场景，构建了UniPCB视觉语言基准。同时，基于该流程自动生成指令集，训练了PCB-GPT模型，并提出模仿人类专家渐进式学习的教学方案（progressive curriculum）。

Result: 评测结果表明，当前主流MLLM在PCB特定任务上表现有限，而新训练的PCB-GPT模型在UniPCB基准上显著优于其它模型，细粒度缺陷定位任务性能提升超过两倍。

Conclusion: 论文首次提出了统一的PCB多模态视觉语言基准和一个专门优化的多模态大模型，有望推动PCB质检领域视觉语言模型的研究和应用，相关数据与模型将公开以促进后续工作。

Abstract: Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research.

</details>


### [28] [Towards Pixel-Level VLM Perception via Simple Points Prediction](https://arxiv.org/abs/2601.19228)
*Tianhui Song,Haoyu Lu,Hao Yang,Lin Sui,Haoning Wu,Zaida Zhou,Zhiqi Huang,Yiping Bao,Y. Charles,Xinyu Zhou,Limin Wang*

Main category: cs.CV

TL;DR: SimpleSeg是一种简单高效的方法，使多模态大模型（MLLMs）具备像素级感知能力，将分割任务转化为点序列生成问题，在多个基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视觉分割任务中通常依赖专门的架构和辅助模块，导致系统复杂且不易扩展。作者希望探索MLLMs能否仅通过语言空间的序列预测简单完成像素级感知，减少对复杂架构的依赖，提高模型统一性和通用性。

Method: 作者提出将分割转化为在语言空间内预测对象边界点（文本坐标）的序列生成问题，并设计了两阶段训练流程：首先进行标准训练，随后引入基于IoU奖励的强化学习对预测点序列进行微调，从而提升轮廓匹配的准确性。全程无需专门的分割架构。

Result: 实验表明，SimpleSeg在多个分割基准上取得了与甚至超过依赖复杂设计方法的表现，证明了MLLM原生具备精准空间感知能力。

Conclusion: 无需辅助组件，仅靠简单点预测，MLLM也能实现精准像素级理解，挑战了复杂分割模型的必要性，并推动更统一、更强大的视觉语言模型的发展。

Abstract: We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/

</details>


### [29] [VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics](https://arxiv.org/abs/2601.19236)
*Zhiyu Yin,Zhipeng Liu,Kehai Chen,Lemao Liu,Jin Liu,Hong-Dong Li,Yang Xiang,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了“视频连接”任务和VC-Bench基准，用于评测和推动在不同片段间生成自然过渡视频内容的研究。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多依赖文本或图像条件，但实际应用如视频编辑或vlog更需要将分散的短片段无缝连接；然而领域缺乏统一的评测标准，限制了相关技术的发展。

Method: 提出了“视频连接”新任务，专注于生成起始与结束片段之间的过渡内容，并构建了VC-Bench基准，包含1579段高质量视频，覆盖15个主类和72个子类，评测视频质量分数(VQS)、首尾一致性分数(SECS)和过渡平滑分数(TSS)。

Result: 在VC-Bench上评测了多种主流视频生成模型，结果显示它们在首尾一致性和过渡平滑性方面存在明显不足，导致生成视频的连贯性和流畅性较低。

Conclusion: VC-Bench为“视频连接”任务提供了首个系统性评测工具，有望激励并指导后续相关研究，其评测指标和数据集已公开。

Abstract: While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.

</details>


### [30] [TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment](https://arxiv.org/abs/2601.19247)
*Jiarun Liu,Qifeng Chen,Yiru Zhao,Minghua Liu,Baorui Ma,Sheng Yang*

Main category: cs.CV

TL;DR: 本文提出了TIGaussian框架，将3D高斯样本（3DGS）特性引入视觉-语言领域，提升了3D相关任务的预训练表现，并改善了不同模态间的特征对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型难以有效融合与处理3D模态（如点云、3D高斯分布），在3D特征提取和不同模态对齐方面尚有不足。为拓展视觉-语言模型对3D任务（跨模态检索、零样本分类、场景识别等）的适应能力，有必要设计更强的3D特征建模和跨模态对齐策略。

Method: 作者提出了TIGaussian框架，利用多分支的3DGS分词器，将3D结构属性编码为紧凑的潜在表示以提升特征泛化能力；并设计了双向跨模态对齐机制：其一，通过多视角特征融合结合扩散先验，解决图像与3D特征对齐时的视角歧义；其二，采用文本-3D投影模块，自适应地将3D特征映射到文本嵌入空间，优化文本与3D之间的语义对齐。

Result: 在多个数据集与任务上，TIGaussian达到了现有最优的性能，显著优于主流方法，表现出良好的通用性和有效性。

Conclusion: TIGaussian将3D高斯样本与视觉-语言模型深度结合，提出了高效的3D特征提取和跨模态对齐策略，为3D理解和多模态学习领域的发展提供了新思路和方法。

Abstract: While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.

</details>


### [31] [Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images](https://arxiv.org/abs/2601.19262)
*Syed Mehedi Hasan Nirob,Moqsadur Rahman,Shamim Ehsan,Summit Haque*

Main category: cs.CV

TL;DR: 本论文系统评估了多种手工特征在检测真实与合成图像中的有效性，发现用LightGBM和混合手工特征在CIFAKE数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型的发展使得合成图像极其逼真，引发了对数字媒体真实性的担忧，亟需有效检测合成图像的新方法。尽管深度学习方法主导该领域，但手工特征因可解释性、效率及泛化能力仍具吸引力。

Method: 作者选取了包含原始像素、色彩直方图、DCT、HOG、LBP、GLCM及小波等手工特征，并在CIFAKE数据集上用七种分类器（如Logistic回归、LightGBM、XGBoost等）进行了系统测试。

Result: LightGBM结合多种手工特征在区分真实与合成图像方面表现最优，达到了PR-AUC 0.9879、ROC-AUC 0.9878、F1 0.9447和Brier score 0.0414。

Conclusion: 不同手工特征的组合与集成学习明显提升了检测性能，证明在需注重可解释性与高效性的场景下，手工特征依然具有重要价值。

Abstract: The rapid progress of generative models has enabled the creation of highly realistic synthetic images, raising concerns about authenticity and trust in digital media. Detecting such fake content reliably is an urgent challenge. While deep learning approaches dominate current literature, handcrafted features remain attractive for their interpretability, efficiency, and generalizability. In this paper, we conduct a systematic evaluation of handcrafted descriptors, including raw pixels, color histograms, Discrete Cosine Transform (DCT), Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gray-Level Co-occurrence Matrix (GLCM), and wavelet features, on the CIFAKE dataset of real versus synthetic images. Using 50,000 training and 10,000 test samples, we benchmark seven classifiers ranging from Logistic Regression to advanced gradient-boosted ensembles (LightGBM, XGBoost, CatBoost). Results demonstrate that LightGBM consistently outperforms alternatives, achieving PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, and a Brier score of 0.0414 with mixed features, representing strong gains in calibration and discrimination over simpler descriptors. Across three configurations (baseline, advanced, mixed), performance improves monotonically, confirming that combining diverse handcrafted features yields substantial benefit. These findings highlight the continued relevance of carefully engineered features and ensemble learning for detecting synthetic images, particularly in contexts where interpretability and computational efficiency are critical.

</details>


### [32] [A Multi-View Consistency Framework with Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2601.19266)
*Yuting Hong,Li Dong,Xiaojie Qiu,Hui Xiao,Baochen Yao,Siming Zheng,Chengbin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种多视图一致性框架，结合去偏策略和伪负标签，并通过跨域特征对齐提升半监督领域自适应（SSDA）性能。


<details>
  <summary>Details</summary>
Motivation: 半监督领域自适应在目标域标注样本稀少时，常因类内特征相似导致模型预测偏差，影响分类准确性。

Method: 提出多视图一致性训练框架，包括：（1）基于模型表现动态校正每类预测概率实现去偏；（2）利用模型伪负标签优化学习；（3）跨域同类特征对齐以增强模型泛化。

Result: 在DomainNet和Office-Home两大主流数据集上，实验结果优于现有主流方法。

Conclusion: 将无监督领域自适应和半监督学习结合，有效提升了模型在工业界的适应性、降低了标注成本并提升性能。

Abstract: Semi-Supervised Domain Adaptation (SSDA) leverages knowledge from a fully labeled source domain to classify data in a partially labeled target domain. Due to the limited number of labeled samples in the target domain, there can be intrinsic similarity of classes in the feature space, which may result in biased predictions, even when the model is trained on a balanced dataset. To overcome this limitation, we introduce a multi-view consistency framework, which includes two views for training strongly augmented data. One is a debiasing strategy for correcting class-wise prediction probabilities according to the prediction performance of the model. The other involves leveraging pseudo-negative labels derived from the model predictions. Furthermore, we introduce a cross-domain affinity learning aimed at aligning features of the same class across different domains, thereby enhancing overall performance. Experimental results demonstrate that our method outperforms the competing methods on two standard domain adaptation datasets, DomainNet and Office-Home. Combining unsupervised domain adaptation and semi-supervised learning offers indispensable contributions to the industrial sector by enhancing model adaptability, reducing annotation costs, and improving performance.

</details>


### [33] [ProMist-5K: A Comprehensive Dataset for Digital Emulation of Cinematic Pro-Mist Filter Effects](https://arxiv.org/abs/2601.19295)
*Yingtie Lei,Zimeng Li,Chi-Man Pun,Wangyu Wu,Junke Yang,Xuhang Chen*

Main category: cs.CV

TL;DR: ProMist-5K是一个专为电影风格模拟设计的大规模高分辨率图像对数据集，有助于还原专业电影滤镜的光晕与扩散效果，适用于多种图像转换与学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统Pro-Mist滤镜带来的柔和光晕和低对比度视觉效果难以通过数字方式还原。因此，亟需建立一个能准确反映物理光扩散现象的数据集，以便更真实地模拟电影镜头美学。

Method: 作者构建了ProMist-5K数据集，包含20,000组高分辨率图像对，覆盖两种滤镜强度和两种焦段。通过物理启发的图像生成流程、场景参考线性空间处理、多层模糊及调整权重，实现拟真光扩散。所构建的数据集为各种图像翻译与学习模型提供了性能一致、可控的目标域。

Result: ProMist-5K在不同训练配置下效果良好，对比实验显示该数据集有助于学习还原从微妙到显著的滤镜视觉风格，表现优于通用风格数据集。

Conclusion: ProMist-5K填补了数字图像处理中物理真实的电影滤镜风格数据集空白，为电影美学相关的图像变换研究与应用提供了实用基础和新的发展方向。

Abstract: Pro-Mist filters are widely used in cinematography for their ability to create soft halation, lower contrast, and produce a distinctive, atmospheric style. These effects are difficult to reproduce digitally due to the complex behavior of light diffusion. We present ProMist-5K, a dataset designed to support cinematic style emulation. It is built using a physically inspired pipeline in a scene-referred linear space and includes 20,000 high-resolution image pairs across four configurations, covering two filter densities (1/2 and 1/8) and two focal lengths (20mm and 50mm). Unlike general style datasets, ProMist-5K focuses on realistic glow and highlight diffusion effects. Multiple blur layers and carefully tuned weighting are used to model the varying intensity and spread of optical diffusion. The dataset provides a consistent and controllable target domain that supports various image translation models and learning paradigms. Experiments show that the dataset works well across different training settings and helps capture both subtle and strong cinematic appearances. ProMist-5K offers a practical and physically grounded resource for film-inspired image transformation, bridging the gap between digital flexibility and traditional lens aesthetics. The dataset is available at https://www.kaggle.com/datasets/yingtielei/promist5k.

</details>


### [34] [Beyond Shadows: A Large-Scale Benchmark and Multi-Stage Framework for High-Fidelity Facial Shadow Removal](https://arxiv.org/abs/2601.19309)
*Tailong Luo,Jiesong Bai,Jinyang Huang,Junyu Xia,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了ASFW数据集，这是首个大规模真实人脸阴影移除数据集，并配套提出了新的阴影移除方法FSE。使用该数据集后，深度模型在人脸阴影移除任务上获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸阴影移除方法在复杂光照下表现不佳，且缺乏高质量真实配对数据，难以有效训练出泛化能力强的模型。

Method: 1. 构建ASFW数据集：采用专业Photoshop流程，制作1081对真实阴影/无阴影人脸照片，确保阴影多样且贴近真实世界。
2. 提出Face Shadow Eraser (FSE)方法，验证数据集对提升人脸阴影移除效果的价值。

Result: 模型在ASFW数据集上训练后，对现实世界中的人脸阴影移除效果显著优于以往方法。

Conclusion: ASFW数据集为人脸阴影移除任务带来了高质量真实数据，推动了该领域发展，配合新方法FSE为后续研究和应用提供了坚实基础。

Abstract: Facial shadows often degrade image quality and the performance of vision algorithms. Existing methods struggle to remove shadows while preserving texture, especially under complex lighting conditions, and they lack real-world paired datasets for training. We present the Augmented Shadow Face in the Wild (ASFW) dataset, the first large-scale real-world dataset for facial shadow removal, containing 1,081 paired shadow and shadow-free images created via a professional Photoshop workflow. ASFW offers photorealistic shadow variations and accurate ground truths, bridging the gap between synthetic and real domains. Deep models trained on ASFW demonstrate improved shadow removal in real-world conditions. We also introduce the Face Shadow Eraser (FSE) method to showcase the effectiveness of the dataset. Experiments demonstrate that ASFW enhances the performance of facial shadow removal models, setting new standards for this task.

</details>


### [35] [Instance-Guided Radar Depth Estimation for 3D Object Detection](https://arxiv.org/abs/2601.19314)
*Chen-Chou Lo,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 本文针对自动驾驶中基于单目摄像头的3D检测存在的深度歧义和鲁棒性不足问题，提出了一种结合Radar与相机的新型深度估计和融合框架，显著提升了3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 单目摄像头在3D检测任务中受限于深度感知能力，尤其在复杂环境下表现不足。雷达虽稳健但信息稀疏，直接利用效果有限。因此，需将两者有机融合，提高整体感知性能。

Method: 1. 提出InstaRadar方法，基于实例分割掩模扩充Radar点，提升其密度和与图像的语义对齐；2. 将预训练的RCDPT深度估计器集成进BEVDepth框架，替换其原有的深度分支，利用InstaRadar加强输入。

Result: InstaRadar在雷达引导的深度估计上取得了SOTA效果，并提升了整体3D目标检测性能，明显优于原始BEVDepth模型。

Conclusion: InstaRadar以及显式深度监督的引入能有效提升基于单目摄像头的3D检测系统表现，但目前该框架在融合方式上仍有待改进，未来可引入更加独立和丰富的Radar特征分支。

Abstract: Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.

</details>


### [36] [Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods](https://arxiv.org/abs/2601.19461)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 本论文系统性地对当前八种立体深度估计算法（涵盖多种范式、预训练参数）在城市、室内及全新树枝数据集上进行了零样本评测，重点填补林业无人机环境下的研究空白。


<details>
  <summary>Details</summary>
Motivation: 无人机在林业环境中需要鲁棒的深度估计能力，但目前相关方法和评测主要集中在城市和室内场景，对于植被密集环境存在明显空白。

Method: 作者收集并整理了八种主流立体深度估计算法（包括迭代优化、基础模型、扩散模型和3D CNN），并全部采用Scene Flow数据集上公开预训练权重，在4大标准数据集（ETH3D、KITTI 2012/2015、Middlebury）及新建5313对树枝图像数据集上进行零样本交叉域评估。

Result: 结果揭示：基础模型对结构化场景表现优秀（如BridgeDepth在ETH3D上误差0.23 px），但方法跨场景表现波动大。迭代方法在不同数据集上的表现有明显差异。新树枝数据集上，DEFOM模型展现了最一致的跨域表现，并被确立为林业场景深度估计的基准（平均排名1.75），其预测将作为后续研究的伪标签基准。

Conclusion: 本研究首次弥补了深度估计算法在植被浓密环境下的系统性评测缺口，提出了DEFOM模型作为新的基准，为未来无人机林业作业的视觉感知奠定了基础。

Abstract: Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.

</details>


### [37] [Innovator-VL: A Multimodal Large Language Model for Scientific Discovery](https://arxiv.org/abs/2601.19325)
*Zichen Wen,Boxue Yang,Shuang Chen,Yaojie Zhang,Yuhang Han,Junlong Ke,Cong Wang,Yicheng Fu,Jiawang Zhao,Jiangchao Yao,Xi Fang,Zhen Wang,Henxing Cai,Lin Yao,Zhifeng Gao,Yanhui Hong,Nang Yuan,Yixuan Li,Guojiang Zhao,Haoyi Tao,Nan Wang,Han Lyu,Guolin Ke,Ning Liao,Xiaoxing Wang,Kai Chen,Zhiyu Li,Feiyu Xiong,Sihan Hu,Kun Chen,Yanfeng Wang,Weinan E,Linfeng Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种科学领域多模态大模型Innovator-VL，兼具科学推理与通用视觉任务能力，并且不依赖超大规模领域特定数据预训练。提供了全透明可复现的训练流水线，显著提升了数据利用效率和模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在科学领域取得进展主要依赖于大规模领域数据和不透明的训练方式，导致模型难以复现与推广，且数据使用效率低下。因此，亟需设计高效、透明且可复现的训练方法，降低数据需求并保证模型性能。

Method: 作者设计了一个完全透明、端到端可复现的训练管道，包括数据收集与清洗、预处理、监督微调、强化学习和评测等环节，并公开详细优化方案。此外，通过精选不足五百万的高质量样本，而不是使用超大规模无差别数据，训练Innovator-VL。

Result: 在各类科学任务和通用视觉、多模态推理基准上，Innovator-VL无需大规模预训练，依靠有限高质量数据即可达到与当前主流模型相媲美的性能，展现出极高的数据效率和良好的泛化能力。

Conclusion: 论文证明了借助透明的训练设计和高质量数据筛选，无需大规模预训练同样可以打造高效、可再现且性能强大的科学多模态大模型，为未来相关模型的研究和实际应用提供了新思路和切实可行的参考。

Abstract: We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.

</details>


### [38] [The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments](https://arxiv.org/abs/2601.19557)
*Riccardo Giubilato,Marcus Gerhard Müller,Marco Sewtz,Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel*

Main category: cs.CV

TL;DR: 本文推出了S3LI Vulcano数据集，这是一个基于视觉与激光雷达（LiDAR）的多模态数据集，旨在推动和评估SLAM及地点识别算法的发展。数据集包含火山岛不同地形和材质下的多种场景，附带开源工具辅助数据处理和标注。


<details>
  <summary>Details</summary>
Motivation: SLAM和地点识别算法在不同环境下的表现需有丰富且多样化的数据集进行评估，现有数据集在地貌与材质多样性、传感器多模态性方面存在不足，因此需要一个新的多模态数据集来填补这一空白。

Method: 研究团队在意大利西西里的Vulcano火山岛采集了多段视觉与LiDAR同步的序列数据，涵盖多种地质地貌（如玄武岩、铁质岩石、古熔岩通道、枯草和水面），并开发了开源工具包，用于生成高精度的位姿真值及标注地点识别样本。

Result: 该团队成功采集并发布了包含多环境、多地形、多材质的视觉与LiDAR数据集，并提供了配套工具，实现了数据的标准化处理和真值标注，为SLAM与地点识别算法研究提供了新的评估基准。

Conclusion: S3LI Vulcano数据集及其工具包为推进多模态感知下的SLAM及地点识别算法研究和性能比较提供了重要资源，填补了以往数据集在环境多样性和标注方面的不足。

Abstract: We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.

</details>


### [39] [Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation](https://arxiv.org/abs/2601.19365)
*Jinming Zhang,Xi Yang,Youpeng Yang,Haosen Shi,Yuyao Yan,Qiufeng Wang,Guangliang Cheng,Kaizhu Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对医学图像分割中不确定性区域（特别是边界区域）优化的训练策略，通过渐进式关注不确定区域和模糊标签方法提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，边界区域的不确定性明显高于内部区域；而传统训练方法一视同仁，导致模型早期优化不稳定，影响整体收敛和最终分割效果。

Method: 1）提出“区域式课程学习策略”，先学习确定区域，逐步引入不确定区域，降低梯度方差；2）设计“帕累托一致损失函数”，自适应平衡不同区域的不确定性，引导网络朝向帕累托解收敛；3）为减轻边界模糊性，引入“模糊标签机制”，边界保持平滑过渡，非边界区域为二值标签，稳定梯度优化。

Result: 在脑转移瘤及非转移瘤分割实验中，不同配置下始终优于传统二值分割方法，在各个肿瘤子区域均获得更优表现。

Conclusion: 逐步引入不确定区域及模糊标签能够有效提升医学图像分割任务性能，尤其是在处理边界区域时，提高优化稳定性和分割准确率。

Abstract: Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.

</details>


### [40] [VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction](https://arxiv.org/abs/2601.19887)
*Dominic Maggio,Luca Carlone*

Main category: cs.CV

TL;DR: VGGT-SLAM 2.0是一种实时RGB SLAM系统，在精度与鲁棒性方面相比VGGT-SLAM有显著提升，特别是在子地图增量对齐与回环检测方面。其针对高维漂移和面退化等问题进行了结构性改进，并展现了良好的跨环境实用性。


<details>
  <summary>Details</summary>
Motivation: 现有VGGT-SLAM系统存在高达15自由度的漂移和面状退化等问题，同时在相机内参未知情况下对重建结果产生二义性，影响场景重建精度。此外，如何提高回环检测的准确性与效率，也是实际实现中的关键挑战。

Method: 1. 重新设计因子图，消除15自由度漂移及面退化问题，同时解决VGGT重建的二义性问题；2. 研究VGGT模型注意力层，利用某一注意力层辅助图像检索与回环验证，无需额外训练即可提升鲁棒性；3. 数据实验证明新系统能适应多种环境，并实现实时部署。

Result: VGGT-SLAM 2.0支持实时在线运行，适配多环境场景，且在TUM标准数据集上表现优异，姿态误差比VGGT-SLAM降低约23%。系统还能拓展至开放集目标检测。

Conclusion: VGGT-SLAM 2.0有效提升了SLAM系统的准确性与鲁棒性，在未知内参条件下也有良好表现，具有广泛的实际应用前景，并将在正式发表时开放源代码。

Abstract: We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.

</details>


### [41] [Establishing dermatopathology encyclopedia DermpathNet with Artificial Intelligence-Based Workflow](https://arxiv.org/abs/2601.19378)
*Ziyang Xu,Mingquan Lin,Yiliang Zhou,Zihan Xu,Seth J. Orlow,Zihan Xu,Shane A. Meehan,Alexandra Flamm,Ata S. Moshiri,Yifan Peng*

Main category: cs.CV

TL;DR: 本论文提出了DermpathNet，一个大规模、开放获取的皮肤病理图像数据集，通过半自动化流程从PMC中筛选与分类皮肤病理相关图像，用于教育与机器学习研究。该工作验证了混合方法的有效性，并公开发布了高质量数据。


<details>
  <summary>Details</summary>
Motivation: 皮肤病理领域缺乏高质量、开放获取的图像数据集，这限制了临床医生、学员的学习与现有机器学习研究的进展。作者希望通过构建全面的数据集解决这一关键难题。

Method: 作者设计了一种混合型图像筛选与分类工作流程：首先用特定关键词从PMC数据库中检索相关图像，随后结合深度学习方式的图像模态分类与图像标题分析进行分类、筛选。通过651张人工标注图片对流程效果进行验证。

Result: 深度学习方法的F-score为89.6%，关键词检索为61.0%，混合方法最高为90.4%。最终获得7,772张覆盖166种诊断的高质量图片，所有图片均经认证皮肤病理学家评审。

Conclusion: 作者开发并公开了DermpathNet——一个经过半自动化流程整理、由认证专家审核的大型开放皮肤病理图像数据集。该数据集为教学、交叉参考和机器学习提供了重要资源，同时验证了现有主流AI算法在该领域分析任务中的局限。

Abstract: Accessing high-quality, open-access dermatopathology image datasets for learning and cross-referencing is a common challenge for clinicians and dermatopathology trainees. To establish a comprehensive open-access dermatopathology dataset for educational, cross-referencing, and machine-learning purposes, we employed a hybrid workflow to curate and categorize images from the PubMed Central (PMC) repository. We used specific keywords to extract relevant images, and classified them using a novel hybrid method that combined deep learning-based image modality classification with figure caption analyses. Validation on 651 manually annotated images demonstrated the robustness of our workflow, with an F-score of 89.6\% for the deep learning approach, 61.0\% for the keyword-based retrieval method, and 90.4\% for the hybrid approach. We retrieved over 7,772 images across 166 diagnoses and released this fully annotated dataset, reviewed by board-certified dermatopathologists. Using our dataset as a challenging task, we found the current image analysis algorithm from OpenAI inadequate for analyzing dermatopathology images. In conclusion, we have developed a large, peer-reviewed, open-access dermatopathology image dataset, DermpathNet, which features a semi-automated curation workflow.

</details>


### [42] [Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT](https://arxiv.org/abs/2601.19380)
*Fakrul Islam Tushar,Joseph Y. Lo*

Main category: cs.CV

TL;DR: 本文提出了Tri-Reader，一个集成肺部分割、结节检测和恶性预测的开源自动化流程，利用多款公开模型，对多个数据集进行评估显示其具备较高通用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有肺部结节检测和恶性预测系统往往功能单一或缺乏统一流程，且标注负担大，影响医生效率，因此需要一个自动化且高灵敏度的综合工具，帮助医生高效筛查和标注。

Method: 作者串联整合了多个基于公开数据集训练的开源模型，开发出Tri-Reader三阶段流程，分别实现肺部分割、结节检测和恶性分类。通过多内部和外部数据集，与专家和参考标准进行对比，评估其性能和泛化能力。

Result: Tri-Reader在不同数据集上的评估表现准确，具备较高敏感性，并能有效减少需要人工审核的候选结节数量。

Conclusion: Tri-Reader作为一个免费、全面、端到端的自动化工具，在保证敏感性的同时提升了肺结节检测与分类流程效率，具有较好的实际应用前景。

Abstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards.

</details>


### [43] [Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection](https://arxiv.org/abs/2601.19430)
*Yao Xiao,Weiyan Chen,Jiahao Chen,Zijie Cao,Weijian Deng,Binbin Yang,Ziyi Dong,Xiangyang Ji,Wei Ke,Pengxu Wei,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一个名为X-AIGD的新数据集，针对AI生成图像检测任务，提供了像素级、分门别类的伪造痕迹注释，从而促进可解释的检测模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像检测方法主要采用二分类，缺乏可解释性和有力的证据，且现有数据集无法覆盖多样化伪造痕迹且缺少细致的局部注释，因此难以解释模型决策过程。

Method: 作者发布了X-AIGD基准，采集像素级、被归类的伪造伪影注释，涵盖低级失真、高级语义和认知层面反事实等；并利用该数据集，对现有检测方法进行细粒度分析和可解释性评估，同时研究了模型关注于伪影区域对可解释性和泛化性的提升作用。

Result: 通过X-AIGD的研究发现：（1）现有检测器基本不依赖感知伪影特征；（2）即使训练检测伪影，模型依然多基于难以解释的特征做判断；（3）显式引导模型关注伪影区域可提升可解释性与泛化能力。

Conclusion: X-AIGD数据集推进了对AI生成图像检测可解释性的研究，可促进更透明、可信赖的检测器开发，对实际应用具有重要意义。

Abstract: Current AI-Generated Image (AIGI) detection approaches predominantly rely on binary classification to distinguish real from synthetic images, often lacking interpretable or convincing evidence to substantiate their decisions. This limitation stems from existing AIGI detection benchmarks, which, despite featuring a broad collection of synthetic images, remain restricted in their coverage of artifact diversity and lack detailed, localized annotations. To bridge this gap, we introduce a fine-grained benchmark towards eXplainable AI-Generated image Detection, named X-AIGD, which provides pixel-level, categorized annotations of perceptual artifacts, spanning low-level distortions, high-level semantics, and cognitive-level counterfactuals. These comprehensive annotations facilitate fine-grained interpretability evaluation and deeper insight into model decision-making processes. Our extensive investigation using X-AIGD provides several key insights: (1) Existing AIGI detectors demonstrate negligible reliance on perceptual artifacts, even at the most basic distortion level. (2) While AIGI detectors can be trained to identify specific artifacts, they still substantially base their judgment on uninterpretable features. (3) Explicitly aligning model attention with artifact regions can increase the interpretability and generalization of detectors. The data and code are available at: https://github.com/Coxy7/X-AIGD.

</details>


### [44] [RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming](https://arxiv.org/abs/2601.19433)
*Jisheng Chu,Wenrui Li,Rui Zhao,Wangmeng Zuo,Shifeng Chen,Xiaopeng Fan*

Main category: cs.CV

TL;DR: RoamScene3D提出了一种能理解语义布局并根据对象关系自适应生成3D场景的新方法，提升了拟真和一致性效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散模型的文本生成3D方法，存在空间感知不足，固定轨迹约束，难以理解物体间语义和推断遮挡信息。此外，传统2D修补模型难以应对因相机运动产生的空洞。针对这些难题，需要一个结合语义和空间生成的新框架。

Method: 作者提出RoamScene3D框架，利用视觉-语言模型（VLM）构建场景图来捕捉对象间语义关系，引导相机自适应探索关键区域并规划运动轨迹。同时，设计了Motion-Injected Inpainting模型，在包含真实相机构运动的全景合成数据集上微调，使其能适应摄像机动态，解决2D修补模型的局限。

Result: 多项实验表明，结合语义推理和几何约束的RoamScene3D能在生成一致且逼真的3D场景任务上显著超越现有方法。

Conclusion: RoamScene3D突破了现有基于2D信息的生成方法在布局理解和动态生成上的限制，为3D场景文本生成提供了更具语义和空间一致性的解决方案。

Abstract: Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.

</details>


### [45] [DSTCS: Dual-Student Teacher Framework with Segment Anything Model for Semi-Supervised Pubic Symphysis Fetal Head Segmentation](https://arxiv.org/abs/2601.19446)
*Yalin Luo,Shun Long,Huijin Wang,Jieyun Bai*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN与Segment Anything Model（SAM）的双学生-教师架构（DSTCS），显著提升了耻骨联合与胎儿头（PSFH）超声分割的准确性。


<details>
  <summary>Details</summary>
Motivation: PSFH精确分割对于产程评估和分娩并发症预警非常重要，但受到类别不平衡、边界模糊、噪声干扰和高质量标注数据稀缺等挑战。现有方法多依赖CNN和Transformer，尚未充分探索更强大的模型。

Method: 作者设计了一个融合CNN和SAM的双学生-教师框架，通过两者的协作学习提升分割准确率；并加入针对边界优化的数据增强策略和创新损失函数。

Result: 在MICCAI 2023和2024 PSFH分割基准上，大量实验表明该方法鲁棒性更强，效果显著优于现有技术。

Conclusion: 本文方法为PSFH自动分割提供了强有力的临床工具，显著提升了分割准确率和实用性。

Abstract: Segmentation of the pubic symphysis and fetal head (PSFH) is a critical procedure in intrapartum monitoring and is essential for evaluating labor progression and identifying potential delivery complications. However, achieving accurate segmentation remains a significant challenge due to class imbalance, ambiguous boundaries, and noise interference in ultrasound images, compounded by the scarcity of high-quality annotated data. Current research on PSFH segmentation predominantly relies on CNN and Transformer architectures, leaving the potential of more powerful models underexplored. In this work, we propose a Dual-Student and Teacher framework combining CNN and SAM (DSTCS), which integrates the Segment Anything Model (SAM) into a dual student-teacher architecture. A cooperative learning mechanism between the CNN and SAM branches significantly improves segmentation accuracy. The proposed scheme also incorporates a specialized data augmentation strategy optimized for boundary processing and a novel loss function. Extensive experiments on the MICCAI 2023 and 2024 PSFH segmentation benchmarks demonstrate that our method exhibits superior robustness and significantly outperforms existing techniques, providing a reliable segmentation tool for clinical practice.

</details>


### [46] [Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes](https://arxiv.org/abs/2601.19484)
*Yin Wang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 本文提出了一种用于动态人-场景交互生成的新架构Dyn-HSI，通过引入视觉、记忆和控制三大“类人”模块，使虚拟人能对动态变化的场景进行感知、记忆和高质量动作生成，并在新构建的动态场景基准测试集上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的人-场景交互生成方法通常假设场景为静态，这与实际中的动态变化场景不符。为了让虚拟人更真实地与变化场景互动，作者受到世界模型启发，提出面向动态场景的认知架构。

Method: Dyn-HSI架构包括：1) 类人视觉，利用动态场景感知与导航模块，使虚拟人可持续感知环境变化并自适应路径预测；2) 类人记忆，引入分层经验记忆，可储存并更新训练中积累的经验用于推理，从而实现上下文感知和更强泛化能力；3) 类人控制，引入人-场景交互扩散模型，结合多模态输入生成高保真动作。此外，作者扩展静态数据集，构建了专门用于动态场景的Dyn-Scenes数据集。

Result: 在静态和动态场景两个设定下进行大量实验，无论定性还是定量，Dyn-HSI均表现出比现有方法更高的人-场景交互动作质量和更好的泛化能力。

Conclusion: Dyn-HSI通过三大类人模块，实现了动态场景下高质量的人-场景交互动作生成，为虚拟人认知和交互带来了提升，对后续相关研究具有重要参考价值。

Abstract: Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.

</details>


### [47] [Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation](https://arxiv.org/abs/2601.19488)
*Yizhao Han,Tianxing Shi,Zhao Wang,Zifan Xu,Zhiyuan Pu,Mingxiao Li,Qian Zhang,Wei Yin,Xiao-Xiao Long*

Main category: cs.CV

TL;DR: 本文提出了一种适用于视频生成的自回归采样新方法ENkG，能够根据每个token的不确定性自适应调整采样候选数量，从而改善生成视频的质量。


<details>
  <summary>Details</summary>
Motivation: 在LLMs（大语言模型）中，常用的top-p/top-k采样策略效果很好。但直接应用到视频生成时，由于视频token的低语义密度和高空间-时间冗余，静态采样策略出现结构崩溃或失真，难以兼顾结构与随机性。亟需一种适应视频独特属性的采样方法。

Method: 提出Entropy-Guided k-Guard（ENkG）采样方法。该方法利用每个token预测分布的熵（entropy）衡量其不确定性，低熵区域用较少候选token抑制噪声，高熵区域则增加候选token防止误差累积，从而实现分区域自适应采样。ENkG无须额外训练，对模型无关，开销极低。

Result: 实验表明，ENkG在视频生成中相比静态top-k/top-p策略，显著提升了感知质量和结构稳定性，尤其在长时间视频生成任务中更加稳健。

Conclusion: ENkG采样方法有效解决了视频自回归生成中的采样困境，是低成本、高适用性的视频生成采样新策略，适用于现有多种模型。

Abstract: Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.

</details>


### [48] [Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction](https://arxiv.org/abs/2601.19489)
*Ziyu Zhang,Tianle Liu,Diantao Tu,Shuhan Shen*

Main category: cs.CV

TL;DR: 本文提出了一种能在一分钟内收敛的快速3D高斯球体（3DGS）重建流程，并在SIGGRAPH Asia 3DGS Fast Reconstruction Challenge中取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决实际场景中快速且高质量3D重建难题，针对比赛要求，须在不同摄像机位姿准确性（SLAM噪声位姿和COLMAP精确位姿）条件下均能实现快速收敛和高保真重建。

Method: 方法分为两轮：第一轮用于SLAM生成的有噪声摄像机位姿，采用反向单个高斯球体并行优化、紧凑前向投影（基于Taming-GS和Speedy-splat）、负载均衡拆分、锚点神经高斯表达（减少可学习参数）、单目深度和部分前馈3DGS初始化，以及全局位姿优化模块去噪；第二轮用于高精度COLMAP位姿，关闭位姿优化、退回标准3DGS（避免MLP计算）、引入多视角一致性引导的高斯分裂（受Fast-GS启发）、用深度估计监督渲染深度。

Result: 一系列创新方法组合显著提升了重建速度和质量，实现了在严格一分钟内完成高保真重建。最终方法以PSNR 28.43分数获得比赛第一。

Conclusion: 所提方法实现了稳健且超快速的3DGS重建，在不同摄像位姿噪声下表现优异，并在国际权威比赛中排名第一，证明了其实用性与领先性。

Abstract: We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.

</details>


### [49] [Cortex-Grounded Diffusion Models for Brain Image Generation](https://arxiv.org/abs/2601.19498)
*Fabian Bongratz,Yitong Li,Sama Elbaroudy,Christian Wachinger*

Main category: cs.CV

TL;DR: 提出了Cor2Vox，一种结合了大脑皮层结构先验的MRI图像合成新方法，能生成高质量且符合真实解剖结构的脑影像，并优于现有生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型用于合成神经影像数据时，多依赖标签或文本等较弱且缺乏解剖信息的条件，导致生成结果在生物学上不够可信。真实数据又有限，尤其是罕见表型、跨设备时域一致性等问题更加突出。因此亟需更具结构约束的影像生成技术。

Method: 提出Cor2Vox框架，通过高分辨率脑皮层表面来引导3D扩散模型，将结构先验与影像生成紧密结合。同时，基于3.3万例UK Biobank样本构建了大规模皮层形态统计模型，进一步拓展了新颖脑形态的合成能力。

Result: Cor2Vox在影像质量、皮层表面重建、全脑分割等指标上优于多数对比方法。实际应用中，在结构一致性合成、灰质萎缩模拟、扫描数据跨平台标准化等三类任务上，模型都展现出对微细皮层形态及不同病理表型的出色保持与泛化能力，且无需重新训练。

Conclusion: Cor2Vox能够生成与真实结构高度一致的脑MRI影像，对罕见或进展性疾病模拟及多中心数据标准化具有较大应用前景，显著推动脑部影像合成的生物学有效性与临床可用性。

Abstract: Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.

</details>


### [50] [Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration](https://arxiv.org/abs/2601.19506)
*Zhengjian Yao,Jiakui Hu,Kaiwen Li,Hangzhou He,Xinliang Zhang,Shuang Zeng,Lei Zhu,Yanye Lu*

Main category: cs.CV

TL;DR: 本文提出Pref-Restore框架，通过结合离散语义逻辑与连续纹理生成，实现了更可靠且具备偏好对齐的盲人脸修复，显著提升了修复的确定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前盲人脸修复受信息不对称影响严重——输入图像信息稀疏，输出细节丰富，导致常规方法常生成幻觉和随机伪影。为实现更精确、更确定性的恢复，研究者需弥合输入输出间的信息鸿沟。

Method: Pref-Restore框架有两项核心创新：一是输入端利用自回归集成器将文本指令转为更致密的语义查询，提升输入信息密度，帮助约束修复过程；二是输出端首创性地将强化学习作为可微分约束纳入扩散模型优化，用人为偏好指导分布收敛，减少随机性。两种策略协作提高修复可控性和偏好对齐度。

Result: 该方法在多项合成及真实基准数据集上实现了最新最优表现。实验还表明偏好对齐机制大幅降低了解空间的不确定性，提高了输出的一致性与可信度。

Conclusion: Pref-Restore为盲人脸修复任务带来了兼顾确定性与个性化的新思路，能高效弥合信息不对称，有效减少修复过程中的随机失真，为该领域的可靠自动修复提供了坚实技术基础。

Abstract: Blind face restoration remains a persistent challenge due to the inherent ill-posedness of reconstructing holistic structures from severely constrained observations. Current generative approaches, while capable of synthesizing realistic textures, often suffer from information asymmetry -- the intrinsic disparity between the information-sparse low quality inputs and the information-dense high quality outputs. This imbalance leads to a one-to-many mapping, where insufficient constraints result in stochastic uncertainty and hallucinatory artifacts. To bridge this gap, we present \textbf{Pref-Restore}, a hierarchical framework that integrates discrete semantic logic with continuous texture generation to achieve deterministic, preference-aligned restoration. Our methodology fundamentally addresses this information disparity through two complementary strategies: (1) Augmenting Input Density: We employ an auto-regressive integrator to reformulate textual instructions into dense latent queries, injecting high-level semantic stability to constrain the degraded signals; (2) Pruning Output Distribution: We pioneer the integration of on-policy reinforcement learning directly into the diffusion restoration loop. By transforming human preferences into differentiable constraints, we explicitly penalize stochastic deviations, thereby sharpening the posterior distribution toward the desired high-fidelity outcomes. Extensive experiments demonstrate that Pref-Restore achieves state-of-the-art performance across synthetic and real-world benchmarks. Furthermore, empirical analysis confirms that our preference-aligned strategy significantly reduces solution entropy, establishing a robust pathway toward reliable and deterministic blind restoration.

</details>


### [51] [Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)](https://arxiv.org/abs/2601.19519)
*Ofir Abramovich,Ariel Shamir,Andreas Aristidou*

Main category: cs.CV

TL;DR: 本文提出了一种仅利用佩戴在身体上的UWB传感器进行配对距离测量的全身3D运动捕捉系统，无需相机即可鲁棒地在各种环境中实时重建人体或动物动作。


<details>
  <summary>Details</summary>
Motivation: 现有的光学或惯性运动捕捉系统依赖外部摄像机或易受环境干扰，限制了其实用性；因此，亟需一种能够在户外、不可控环境，并且对形体和环境具有鲁棒性的新方法。

Method: 本方法利用人体佩戴的无线节点进行超宽带（UWB）测距，获得稀疏的体表点对距离（PWD）信息，输入给基于Transformer的WiP模型，直接预测3D关节点位置，支持变形形体与跨物种，无需个性化体型测量。

Result: WiP系统可实时运行，在多样体型的人类与动物动作重建中均实现了低关节位置误差，具备准确、普适的3D动作捕捉能力，抗干扰且无需环境布控。

Conclusion: WiP展示了可扩展、低成本、通用的运动捕捉新范式，为实际应用中的动作追踪提供了强有力的解决方案，适用于室外及复杂环境。

Abstract: We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.

</details>


### [52] [A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder](https://arxiv.org/abs/2601.19526)
*Fouad Boutaleb,Emery Pierson,Mohamed Daoudi,Clémence Nineuil,Ali Amad,Fabien D'Hondt*

Main category: cs.CV

TL;DR: 该论文提出了一种利用单目RGB视频来提取与抑郁症运动障碍相关的3D步态动力学生物标志物的方法，实现了临床上客观、自动化的抑郁症状态评估。


<details>
  <summary>Details</summary>
Motivation: 现有的抑郁症临床运动障碍（PMR）评估方法主观性强，3D动作捕捉虽然客观但设备昂贵难以普及，因此亟需简便、可自动化且具解释性的客观评估手段。

Method: 提出将普通RGB视频转化为临床相关的3D步态动力学参数，并结合重力坐标与新轨迹校正算法，减少单目深度误差。使用TUG协议及从视频中提取的297个显式运动生物标志物，应用基于稳定性的机器学习框架以应对样本小、避免过拟合。

Result: 在公开CALYPSO数据集上，提出方法对运动性迟缓的检测准确率达83.3%，可解释整体抑郁严重程度64%的方差。还发现踝部动力减弱和骨盆活动受限与抑郁运动特征密切相关。

Conclusion: 论文证明了通过物理运动数据可有效、客观地评估抑郁症认知/运动状态，方法透明、可扩展，有望为常规临床环境提供无创、自动化的抑郁症客观监测工具。

Abstract: Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.
  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.
  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.

</details>


### [53] [MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation](https://arxiv.org/abs/2601.19577)
*Ronglai Zuo,Rolandos Alexandros Potamias,Qi Sun,Evangelos Ververas,Jiankang Deng,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了一种基于掩码扩散（masked-diffusion）机制的手语生成模型MaDiS，能有效提升生成速度和表现能力，实现更高质量的手语动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型（autoregressive language model）在手语生成任务中存在单向建模和推理效率低的问题，亟需更高效、效果更好的解决方案。

Method: 提出了MaDiS模型，采用掩码扩散语言建模方法，实现双向依赖建模和并行多token生成。引入了三层次跨模态预训练（token、潜空间、三维物理空间），并设计了带有时间检查点的新unmasking策略和混合部件嵌入层（mixture-of-parts embedding）以提升学习效率和表达能力。

Result: 在多个主流手语生成数据集（CSL-Daily、Phoenix-2014T、How2Sign）上，MaDiS在多项指标（DTW error、SiBLEU、SiCLIP）上均取得了优异表现，并将推理延迟降低近30%。

Conclusion: MaDiS显著提升了手语生成的质量与效率，为手语自动生成领域带来了重要进展，相关代码和模型将对外开放。

Abstract: Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.

</details>


### [54] [QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture](https://arxiv.org/abs/2601.19580)
*Cuong Le,Pavlo Melnyk,Urs Waldmann,Mårten Wadenbäck,Bastian Wandt*

Main category: cs.CV

TL;DR: 该论文提出了一种基于四元数的3D人体运动捕捉方法QuaMo，能够实现更连续、真实的动作序列重建，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D姿态估计存在帧间时序不连续、动作抖动等问题，且常用的欧拉角表示存在不连续性，影响运动的平滑性和稳定性。四元数表示没有这些劣势，因此有望实现更好的时序一致性。

Method: 提出QuaMo方法，利用四元数微分方程（QDE）和状态空间模型进行人体运动捕捉，通过meta-PD控制器和创新的加速度增强解决瞬时动作变化问题，并在四元数单位球约束下数值求解QDE以提升精度。

Result: QuaMo在Human3.6M、Fit3D、SportsPose和AIST等多个主流数据集上达到了无不连续性、几乎无不合理动作的优异效果，性能优于现有最新方法。

Conclusion: 四元数微分方程结合主动加速度调节，提高了3D人体运动捕捉的连续性和准确性，为视频中的时序一致动作重建提供了新的有效途径。

Abstract: Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo

</details>


### [55] [ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.19582)
*Yujin Wang,Yutong Zheng,Wenxian Fan,Tianyi Wang,Hongqing Chu,Daxin Tian,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了ScenePilot-Bench，这是一个大规模的第一视角自动驾驶场景基准，用于评估视觉-语言模型（VLMs）的能力。该基准基于丰富且细致注释的驾驶视频数据集，同时设计了多维度评测方案，并对现有主流VLM进行了系统测试与分析。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在自动驾驶场景中的理解、推理与决策能力尚未有系统且权威的评测框架。同时，已有数据集和评测往往片面，难以全面覆盖实际自动驾驶需求。因此，亟需一个涵盖多特性的权威基准，推动该领域研究和技术进展。

Method: 作者构建了ScenePilot-Bench基准，依托于ScenePilot-4K数据集（包含3847小时驾驶视频），提供多粒度的场景描述、风险评估、参与者识别、轨迹及相机参数。基准包含四大评测维度：场景理解、空间感知、运动规划与GPT-Score，并融入安全感知指标及跨区域泛化设定。代表性VLM模型被选用进行对比实验与性能分析。

Result: 通过在ScenePilot-Bench上的实证对比，论文揭示了当前主流VLM在自动驾驶多任务场景中的优劣势与瓶颈，明确了现有模型在驱动相关推理方面的表现差距与改进方向。

Conclusion: ScenePilot-Bench为评估和发展面向安全关键自动驾驶场景的VLMs提供了综合能力框架。其提出的数据集与评测方案预计将助推自动驾驶感知与决策模型的研究进展。

Abstract: In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.

</details>


### [56] [Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning](https://arxiv.org/abs/2601.19593)
*Estèphe Arnaud,Mohamed Daoudi,Pierre Guerreschi*

Main category: cs.CV

TL;DR: 本论文提出了一种基于生成式对抗网络的局部潜在编辑框架，用于模拟肉毒素注射对面部特定区域的影响，辅助注射剂量的精确规划。


<details>
  <summary>Details</summary>
Motivation: 现有肉毒素注射主要依赖医生经验，剂量规划不科学，容易导致效果不佳甚至副作用。急需一种可模拟不同注射剂量效果的新方法，辅助医生实现更精确的个性化治疗。

Method: 作者提出了“区域特定潜在轴发现”方法，在StyleGAN2潜在空间中学习面部不同区域（如肌肉）放松轨迹，从而模拟肉毒素注射的局部效果。将这些轨迹与注射剂量关联，建立剂量-反应预测模型，并与直接的指标回归进行对比验证。

Result: 在含有46名患者360张面部图像的临床数据集上测试，模型对几何对称性指标表现出中到强的结构相关性，能够准确捕捉形态变化方向。

Conclusion: 生成模型能够辅助医生针对不同面部区域规划肉毒素注射剂量，提高对称性及美学效果，但绝对精度受生理差异影响。通过“人机协作”流程（即医生参与调整模拟效果），提升了模型的实际应用价值。

Abstract: Botulinum toxin (Botox) injections are the gold standard for managing facial asymmetry and aesthetic rejuvenation, yet determining the optimal dosage remains largely intuitive, often leading to suboptimal outcomes. We propose a localized latent editing framework that simulates Botulinum Toxin injection effects for injection planning through dose-response modeling. Our key contribution is a Region-Specific Latent Axis Discovery method that learns localized muscle relaxation trajectories in StyleGAN2's latent space, enabling precise control over specific facial regions without global side effects. By correlating these localized latent trajectories with injected toxin units, we learn a predictive dose-response model. We rigorously compare two approaches: direct metric regression versus image-based generative simulation on a clinical dataset of N=360 images from 46 patients. On a hold-out test set, our framework demonstrates moderate-to-strong structural correlations for geometric asymmetry metrics, confirming that the generative model correctly captures the direction of morphological changes. While biological variability limits absolute precision, we introduce a hybrid "Human-in-the-Loop" workflow where clinicians interactively refine simulations, bridging the gap between pathological reconstruction and cosmetic planning.

</details>


### [57] [GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining](https://arxiv.org/abs/2601.19606)
*Shentong Mo,Zehua Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出GMS-CAVP，一种用于增强视频-音频多尺度对齐和生成的联合对比-生成预训练框架，并在多数据集上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频-音频理解和生成任务依赖于联合作嵌入，但现有方法无法充分建模多尺度、稠密的视频与音频时空对应关系，导致性能受限。

Method: GMS-CAVP结合多尺度对比学习和基于扩散的生成预训练目标，首先从不同粒度捕捉语义与时序对应关系，其次利用生成性扩散目标，实现模态间互译和合成，统一判别与生成两类目标。

Result: 在VGGSound、AudioSet和Panda70M等数据集上，GMS-CAVP在视频-音频生成和跨模态检索任务上均超越了以往主流方法。

Conclusion: 多尺度对齐和生成性目标显著提升了视频-音频多模态建模效果，为高保真跨模态生成和理解提供了新途径。

Abstract: Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.

</details>


### [58] [The role of self-supervised pretraining in differentially private medical image analysis](https://arxiv.org/abs/2601.19618)
*Soroosh Tayebi Arasteh,Mina Farajiamiri,Mahshad Lotfinia,Behrus Hinrichs-Puladi,Jonas Bienzeisler,Mohamed Alhaskir,Mirabela Rusu,Christiane Kuhl,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 本研究系统评估了差分隐私（DP）医疗影像分析中不同模型初始化策略的效果，发现初始化方法对实用性、公平性和泛化能力影响显著。


<details>
  <summary>Details</summary>
Motivation: 差分隐私能够保护敏感医疗数据，但会显著影响模型诊断性能。近年来的研究发现，模型初始化对缓解性能下降至关重要。然而，在完整模型DP下，现代自监督学习初始化方法的实际作用尚不清楚，因此有必要进行系统研究。

Method: 作者以胸部X光片的大型分类任务为基准，采用DP-SGD训练最先进的ConvNeXt模型，在超过80万张图像上，比较了三种初始化：ImageNet监督学习、非领域特定自监督学习（DINOv3）、领域特定的MIMIC-CXR监督预训练。性能评估覆盖五个来自不同机构和采集设定的外部测试集。

Result: 结果表明：在差分隐私约束下，DINOv3自监督初始化相较ImageNet初始化能持续提升诊断性能，但仍逊于领域特定的MIMIC-CXR监督预训练。领域特定初始化的性能最接近非隐私模型。此外，不同初始化策略对模型的公平性、跨数据集泛化能力和在不同数据规模与模型容量下的鲁棒性均有显著影响。

Conclusion: 模型初始化在差分隐私医疗影像任务中对模型的效用、公平性与泛化性能起决定性作用。领域特定的监督预训练仍是最佳选择。

Abstract: Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images. Using state-of-the-art ConvNeXt models trained with DP-SGD across realistic privacy regimes, we compare non-domain-specific supervised ImageNet initialization, non-domain-specific self-supervised DINOv3 initialization, and domain-specific supervised pretraining on MIMIC-CXR, the largest publicly available chest radiograph dataset. Evaluations are conducted across five external datasets spanning diverse institutions and acquisition settings. We show that DINOv3 initialization consistently improves diagnostic utility relative to ImageNet initialization under DP, but remains inferior to domain-specific supervised pretraining, which achieves performance closest to non-private baselines. We further demonstrate that initialization choice strongly influences demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity under privacy constraints. The results establish initialization strategy as a central determinant of utility, fairness, and generalization in differentially private medical imaging.

</details>


### [59] [Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework](https://arxiv.org/abs/2601.19640)
*Hao Chang,Zhihui Wang,Lingxiang Wu,Peijin Wang,Wenhui Diao,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了GovLA-10K数据集和GovLA-Reasoner系统，用于提升低空视觉系统在智慧城市治理中的管理感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的低空视觉系统主要基于物体感知或松散耦合的视觉-语言方法，难以满足城市治理中对管理异常的理解需求。

Method: 提出了管理导向的多模态基准GovLA-10K，聚焦于与实际管理需求直接相关的功能性目标。与此同时，设计了GovLA-Reasoner推理框架，通过高效特征适配器实现视觉检测器与大语言模型之间的高效表征共享和隐式协同。

Result: 实验结果表明，所提出的方法在无需对特定任务组件微调的情况下，能够显著提升管理感知与推理性能。

Conclusion: 该工作为管理感知型低空视-语系统研究提供了新视角和基础。

Abstract: Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.

</details>


### [60] [KeepLoRA: Continual Learning with Residual Gradient Adaptation](https://arxiv.org/abs/2601.19659)
*Mao-Lin Luo,Zi-Hao Zhou,Yi-Lin Zhang,Yuanyu Wan,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为KeepLoRA的简单高效方法，用于解决预训练视觉-语言模型在持续学习中的知识保持与新任务学习之间的平衡难题。通过理论与实证分析，方法能保持预训练知识、已学任务知识，并具备获取新知识的能力，性能达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型的持续学习任务中，保持预训练知识、已学知识与学习新任务之间存在冲突，传统方法难以三者兼顾，因此亟需有效的解决策略。

Method: 作者首先分析了模型参数空间中知识的保留机制，发现通用知识主要在主子空间中，而任务特定知识则在残差子空间。基于此，提出KeepLoRA方法：仅在残差子空间中更新LoRA参数，通过将新任务梯度投影到正交于主子空间和已学任务特征主方向的子空间，最大程度减少对已学能力的干扰。

Result: 理论和实验结果表明，KeepLoRA可以有效平衡三大目标，在相关持续学习任务上达到了SOTA（最新、最优）水平。

Conclusion: KeepLoRA为预训练视觉-语言模型实现高效持续学习提供了切实可行的方法，对知识保留和新任务学习之间的平衡有重要意义。

Abstract: Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.

</details>


### [61] [A new Image Similarity Metric for a Perceptual and Transparent Geometric and Chromatic Assessment](https://arxiv.org/abs/2601.19680)
*Antonio Di Marino,Vincenzo Bevilacqua,Emanuel Di Nardo,Angelo Ciaramella,Ivanoe De Falco,Giovanna Sannino*

Main category: cs.CV

TL;DR: 提出一种新颖的感知图像相似性度量方法，在复杂畸变条件下优于现有方法，并能提供可解释的视觉解释。


<details>
  <summary>Details</summary>
Motivation: 现有的图像相似性度量方法在涉及复杂纹理和颜色畸变时表现不好，且主流深度学习方法难以解释，仅给出分数而无法说明差异与相似点。

Method: 方法包含两部分：一是利用地球移动者距离（EMD）评估两张图片之间的纹理差异；二是用Oklab感知色彩空间衡量色彩差异。

Result: 在包含各种形状和颜色复杂畸变的Berkeley-Adobe Perceptual Patch Similarity数据集上进行实验，结果显示该方法在存在形状畸变时明显优于现有方法，感知能力更强。

Conclusion: 所提出的方法不仅提升了度量准确性，在解释性上也比黑盒深度方法更优，能为分数提供直观理由，提高了相似性评估的透明度和合理性。

Abstract: In the literature, several studies have shown that state-of-the-art image similarity metrics are not perceptual metrics; moreover, they have difficulty evaluating images, especially when texture distortion is also present. In this work, we propose a new perceptual metric composed of two terms. The first term evaluates the dissimilarity between the textures of two images using Earth Mover's Distance. The second term evaluates the chromatic dissimilarity between two images in the Oklab perceptual color space. We evaluated the performance of our metric on a non-traditional dataset, called Berkeley-Adobe Perceptual Patch Similarity, which contains a wide range of complex distortions in shapes and colors. We have shown that our metric outperforms the state of the art, especially when images contain shape distortions, confirming also its greater perceptiveness. Furthermore, although deep black-box metrics could be very accurate, they only provide similarity scores between two images, without explaining their main differences and similarities. Our metric, on the other hand, provides visual explanations to support the calculated score, making the similarity assessment transparent and justified.

</details>


### [62] [SharpNet: Enhancing MLPs to Represent Functions with Controlled Non-differentiability](https://arxiv.org/abs/2601.19683)
*Hanting Niu,Junkai Deng,Fei Hou,Wencheng Wang,Ying He*

Main category: cs.CV

TL;DR: 本文提出了一种名为SharpNet的MLP变体，能够同时表达光滑区域与精确可控的尖锐特征，用于更好地逼近带有不连续梯度的函数。


<details>
  <summary>Details</summary>
Motivation: 多层感知机（MLPs）本质上只能输出全局光滑的函数，对于那些连续但有尖锐特征（即仅C^0连续、不可微）的目标函数，MLP拟合能力弱，只能通过事后处理人为补救。需求来自于如CAD建模等精细表达不连续边角的实际场景。

Method: 作者设计了SharpNet网络，在MLP基础上引入额外的特征函数。该函数通过解带跳跃Neumann边界条件的Poisson方程获得，并通过高效的局部积分实现全可微，进而联合优化特征位置和MLP参数，使得网络能精确控制C^0连续性（在特征处非光滑，其他处平滑）。

Result: SharpNet在二维函数和三维CAD模型重建任务中相较于先进基线模型取得更好效果。它能够准确恢复尖锐边角等梯度不连续特征，并保持其它区域的平滑；现有方法则易将这些锐利特征“抹平”。

Conclusion: SharpNet突破了传统MLP只能输出光滑函数的局限，有效兼顾不连续特征和整体平滑性，具备良好的泛用性和实用价值，在相关建模与重建任务中具备理论与实践优势。

Abstract: Multi-layer perceptrons (MLPs) are a standard tool for learning and function approximation, but they inherently yield outputs that are globally smooth. As a result, they struggle to represent functions that are continuous yet deliberately non-differentiable (i.e., with prescribed $C^0$ sharp features) without relying on ad hoc post-processing. We present SharpNet, a modified MLP architecture capable of encoding functions with user-defined sharp features by enriching the network with an auxiliary feature function, which is defined as the solution to a Poisson equation with jump Neumann boundary conditions. It is evaluated via an efficient local integral that is fully differentiable with respect to the feature locations, enabling our method to jointly optimize both the feature locations and the MLP parameters to recover the target functions/models. The $C^0$-continuity of SharpNet is precisely controllable, ensuring $C^0$-continuity at the feature locations and smoothness elsewhere. We validate SharpNet on 2D problems and 3D CAD model reconstruction, and compare it against several state-of-the-art baselines. In both types of tasks, SharpNet accurately recovers sharp edges and corners while maintaining smooth behavior away from those features, whereas existing methods tend to smooth out gradient discontinuities. Both qualitative and quantitative evaluations highlight the benefits of our approach.

</details>


### [63] [Video-KTR: Reinforcing Video Reasoning via Key Token Attribution](https://arxiv.org/abs/2601.19686)
*Ziyue Wang,Sheng Jin,Zhongrong Zuo,Jiawei Wu,Han Qiu,Qi She,Hao Zhang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大模型视频推理增强方法Video-KTR，通过在强化学习中进行细粒度、模态感知的策略塑形，强化对视频线索的理解，显著提升了准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 目前的视频推理方法多采用粗粒度的奖励函数或单因素token选择，忽略了视觉、时序和语言输出间细粒度的关联，导致准确性和可解释性不足。

Method: Video-KTR将视频推理强化学习细化到token级别，根据三种归因信号（视觉感知token、时序敏感token和高不确定性token）选择关键token进行强化，实现模态感知的选择性学习。视觉感知token通过反事实masking识别，时序敏感token用帧乱序检测，高熵token代表模型不确定性。只强化这些token，让模型聚焦于高语义价值和高模态敏感内容。

Result: 在五个具有挑战性的基准上，Video-KTR达到或超过SOTA表现（如Video-Holmes上达到42.7%，超越GPT-4o），同时在推理和通用视频理解任务上都有稳定提升。消融实验也验证了三种归因信号的互补作用及token级强化的鲁棒性。

Conclusion: Video-KTR有效提升了视频推理准确率和可解释性，是一种简单高效的RL扩展方式，可无缝用于复杂视频推理任务。

Abstract: Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.

</details>


### [64] [DSVM-UNet : Enhancing VM-UNet with Dual Self-distillation for Medical Image Segmentation](https://arxiv.org/abs/2601.19690)
*Renrong Shao,Dongyang Li,Dong Xia,Lin Shao,Jiangdong Lu,Fen Zheng,Lulu Zhang*

Main category: cs.CV

TL;DR: 本文提出了DSVM-UNet，一种无需复杂结构、通过双自蒸馏提升VM-UNet性能的方法，在多个医学影像分割数据集上达到了最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Vision Mamba的医学影像分割模型为提升语义感知能力，往往依赖于复杂的结构设计，增加了模型复杂度与计算消耗。本文动机在于无需复杂结构，通过自蒸馏等方式提升模型表现。

Method: 提出Dual Self-distillation for VM-UNet（DSVM-UNet）方法，分别在全局和局部层面对特征进行双重自蒸馏，实现不同层次特征的一致性对齐，从而提升模型性能。该方法无需引入额外复杂结构，仅引入自蒸馏机制。

Result: 在ISIC2017、ISIC2018、Synapse三大医学影像分割基准数据集上进行实验，DSVM-UNet在保持较高计算效率的同时取得了当前最优分割性能。

Conclusion: 双自蒸馏方法能有效提升VM-UNet在医学影像分割任务的表现，为复杂结构外的模型优化提供了新思路。

Abstract: Vision Mamba models have been extensively researched in various fields, which address the limitations of previous models by effectively managing long-range dependencies with a linear-time overhead. Several prospective studies have further designed Vision Mamba based on UNet(VM-UNet) for medical image segmentation. These approaches primarily focus on optimizing architectural designs by creating more complex structures to enhance the model's ability to perceive semantic features. In this paper, we propose a simple yet effective approach to improve the model by Dual Self-distillation for VM-UNet (DSVM-UNet) without any complex architectural designs. To achieve this goal, we develop double self-distillation methods to align the features at both the global and local levels. Extensive experiments conducted on the ISIC2017, ISIC2018, and Synapse benchmarks demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. Code is available at https://github.com/RoryShao/DSVM-UNet.git.

</details>


### [65] [Self-Supervised Weight Templates for Scalable Vision Model Initialization](https://arxiv.org/abs/2601.19694)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: 本文提出了一种名为SWEET的自监督预训练框架，通过学习共享的参数模板和特定规模的权重缩放器，实现了视觉任务中可扩展和灵活的模型初始化，显著提升了不同架构规模下模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的模型预训练与微调一般针对固定规模的网络架构，随着模型规模日益增大和多样化，无法良好适配实际部署中对不同规模模型的需求。因此急需一种既能保持预训练优势、又能灵活适配不同架构规模的预训练方法。

Method: 方法上，SWEET以Tucker分解为基础，学习一种共享的权重模板，并针对不同模型规模引入特定的权重缩放器。目标模型初始化时，通过对模板进行拼接和缩放实现权重适配。同时，引入宽度方向的随机缩放机制，提升模板在宽度变化下的鲁棒性和泛化能力。整个框架为自监督学习，无需大量有标签数据即可训练权重缩放器。

Result: 在分类、检测、分割和生成等视觉任务的多项实验中，SWEET在初始化不同规模模型时均取得了领先的性能，验证了其普适性和有效性。

Conclusion: SWEET框架实现了高效、灵活和可扩展的视觉模型初始化，为模型在不同规模和架构下部署提供了有效解决方案，有望推动实际应用中的模型泛化和适应能力提升。

Abstract: The increasing scale and complexity of modern model parameters underscore the importance of pre-trained models. However, deployment often demands architectures of varying sizes, exposing limitations of conventional pre-training and fine-tuning. To address this, we propose SWEET, a self-supervised framework that performs constraint-based pre-training to enable scalable initialization in vision tasks. Instead of pre-training a fixed-size model, we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. Target models are subsequently initialized by composing and reweighting the template through lightweight weight scalers, whose parameters can be efficiently learned from minimal training data. To further enhance flexibility in width expansion, we introduce width-wise stochastic scaling, which regularizes the template along width-related dimensions and encourages robust, width-invariant representations for improved cross-width generalization. Extensive experiments on \textsc{classification}, \textsc{detection}, \textsc{segmentation} and \textsc{generation} tasks demonstrate the state-of-the-art performance of SWEET for initializing variable-sized vision models.

</details>


### [66] [DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization](https://arxiv.org/abs/2601.19717)
*Yitong Yang,Xuexin Liu,Yinglin Wang,Jing Wang,Hao Dou,Changshuo Wang,Shuting He*

Main category: cs.CV

TL;DR: DiffStyle3D是一种新的基于扩散模型的3D风格迁移方法，提升了风格转换质量和多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D风格迁移方法在多视角一致性和训练稳定性方面存在不足：VGG和CLIP为基础的方法难以建模一致性，而扩散方法虽然能建模，但训练不稳定。

Method: DiffStyle3D以扩散模型为基础，直接在潜空间优化风格迁移。提出注意力感知损失，在自注意力空间对齐风格特征，并通过内容特征对齐保持原始内容。此外，提出几何引导的多视角一致性方法，将几何信息融合至自注意力以建模跨视角对应关系，并设计几何感知遮罩，避免多视角重叠区域的冗余优化。

Result: 实验表明，DiffStyle3D在风格化质量和视觉真实感方面优于当前主流方法。

Conclusion: DiffStyle3D有效提升了3DGS风格迁移的多视角一致性与风格传达质量，为3D内容创作提供了更为强大的工具。

Abstract: 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.

</details>


### [67] [WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration](https://arxiv.org/abs/2601.19753)
*Xinrui Zhang,Yufeng Wang,Shuangkang Fang,Zesheng Wang,Dacheng Qi,Wenrui Ding*

Main category: cs.CV

TL;DR: 本文提出WaterClear-GS方法，首次将水下光学属性（局部衰减和散射）纳入到纯3D Gaussian Splatting框架中，同时实现高效的三维重建和外观恢复，在不借助额外网络的前提下达到了实时渲染和优秀的水下图像修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF等基于神经辐射场的方法在水下三维重建和外观恢复中存在渲染速度慢、颜色恢复次优等问题，而3D Gaussian Splatting本身难以建模复杂体积散射，限制了其水下应用。因此，亟需结合物理建模的方法来提升水下三维重建的质量和速度。

Method: WaterClear-GS将水下光学属性（局部衰减与散射）直接集成进高斯体素原语中，无需辅助介质网络。采用双分支优化策略保证水下光度一致性和自然恢复无水外观。辅以深度引导的几何正则、感知驱动的图像损失、曝光约束、空间自适应正则和物理引导的光谱正则，实现高质量三维一致性与感知自然外观。

Result: 在标准评测集和新采集的数据集上，WaterClear-GS同时在新视角合成和水下图像复原任务上表现突出，且能够实时渲染。

Conclusion: WaterClear-GS首次将水下光学特性无缝集成入3DGS框架，解决了现有水下3D重建的速度及效果瓶颈，为水下场景三维建模和还原提供了高效且实用的新方案。

Abstract: Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.

</details>


### [68] [PaW-ViT: A Patch-based Warping Vision Transformer for Robust Ear Verification](https://arxiv.org/abs/2601.19771)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 本文提出了一种基于贴片变形的视觉Transformer预处理方法（PaW-ViT），通过对耳部图像进行规范化增强ViT的识别能力。该方法通过精确对齐token边界和耳部特征边界，提高了对形状、大小、姿态变化的鲁棒性，对多种ViT模型实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer方法中采用的矩形token会引入目标物体外部的信息，影响性能，特别在耳生物特征识别等任务中，耳部形变和姿态多变加剧了这一问题。需要将ViT结构的敏感性与生物特征的实际变化进行调和。

Method: 提出PaW-ViT方法：首先利用解剖学知识对耳部图像进行预处理，将token边界精确对齐到检测到的耳部特征边界，并沿自然耳曲线对特征边界进行配准。以此替换传统矩形划分，获得适应多变形状的一致特征表示。

Result: 在多个ViT模型（ViT-T、ViT-S、ViT-B、ViT-L）上实验，PaW-ViT方法在对形状、大小、姿态变化的鲁棒性方面表现出优势，并验证了对齐带来的效果提升。

Conclusion: PaW-ViT成功缓解了ViT架构对位置变化的敏感性与耳部生物特征多样性之间的不匹配问题，为基于耳部的身份认证等应用提供了可能的技术路径。

Abstract: The rectangular tokens common to vision transformer methods for visual recognition can strongly affect performance of these methods due to incorporation of information outside the objects to be recognized. This paper introduces PaW-ViT, Patch-based Warping Vision Transformer, a preprocessing approach rooted in anatomical knowledge that normalizes ear images to enhance the efficacy of ViT. By accurately aligning token boundaries to detected ear feature boundaries, PaW-ViT obtains greater robustness to shape, size, and pose variation. By aligning feature boundaries to natural ear curvature, it produces more consistent token representations for various morphologies. Experiments confirm the effectiveness of PaW-ViT on various ViT models (ViT-T, ViT-S, ViT-B, ViT-L) and yield reasonable alignment robustness to variation in shape, size, and pose. Our work aims to solve the disconnect between ear biometric morphological variation and transformer architecture positional sensitivity, presenting a possible avenue for authentication schemes.

</details>


### [69] [GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance](https://arxiv.org/abs/2601.19785)
*Haozhi Zhu,Miaomiao Zhao,Dingyao Liu,Runze Tian,Yan Zhang,Jie Guo,Fenggen Yu*

Main category: cs.CV

TL;DR: GeoDiff3D是一种无需大量标注数据、低计算成本、高质量的3D场景生成新方法，在复杂场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法受限于结构建模弱和对大规模有标签数据的依赖，导致结构性伪影、几何不一致及细节缺失，且效率不高。新兴行业（如游戏、影视、VR/AR）亟需更高效、通用、高质量的3D内容生成方案。

Method: GeoDiff3D提出了一种自监督框架，利用粗几何信息作为结构锚点，通过几何约束的2D扩散模型生成高质量纹理参考图像，并通过体素对齐3D特征聚合及双重自监督机制，确保场景的连贯性与细节还原，同时减少对标注数据的需求。

Result: 在多个复杂场景实验中，GeoDiff3D在泛化能力和生成质量上均超过现有基线方法，且对噪声和不一致的参考数据具有鲁棒性，并显著降低了训练所需的算力和人工标注成本。

Conclusion: GeoDiff3D为高效、低门槛的3D场景生成提供了实用方案，有望推动3D内容创作在多个行业的广泛应用。

Abstract: 3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.

</details>


### [70] [Diffusion for De-Occlusion: Accessory-Aware Diffusion Inpainting for Robust Ear Biometric Recognition](https://arxiv.org/abs/2601.19795)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 本研究提出利用扩散模型对耳部图像中因配饰（如耳环、耳机）造成的遮挡进行修复，以提升基于耳部的生物识别系统的性能。


<details>
  <summary>Details</summary>
Motivation: 耳部配饰会造成耳部生物识别系统识别率下降，尤其在非受控图像条件下影响更大。需要有效的方法解决因遮挡导致的识别精度降低问题。

Method: 提出使用扩散模型进行耳部图像修复，通过自动获取遮挡掩膜，对遮挡部位进行像素级重建，确保耳部关键结构的几何连贯性。该预处理方法在多种视觉Transformer模型、不同patch大小及多个基准数据集上进行评估。

Result: 实验结果表明，基于扩散的修复作为预处理手段，能有效缓解耳部配饰遮挡问题，并提升耳部识别系统整体性能。

Conclusion: 扩散模型修复技术能作为Transformer类耳部识别系统的有效预处理手段，改善因配饰导致的识别性能下降，具有实际应用价值。

Abstract: Ear occlusions (arising from the presence of ear accessories such as earrings and earphones) can negatively impact performance in ear-based biometric recognition systems, especially in unconstrained imaging circumstances. In this study, we assess the effectiveness of a diffusion-based ear inpainting technique as a pre-processing aid to mitigate the issues of ear accessory occlusions in transformer-based ear recognition systems. Given an input ear image and an automatically derived accessory mask, the inpainting model reconstructs clean and anatomically plausible ear regions by synthesizing missing pixels while preserving local geometric coherence along key ear structures, including the helix, antihelix, concha, and lobule. We evaluate the effectiveness of this pre-processing aid in transformer-based recognition systems for several vision transformer models and different patch sizes for a range of benchmark datasets. Experiments show that diffusion-based inpainting can be a useful pre-processing aid to alleviate ear accessory occlusions to improve overall recognition performance.

</details>


### [71] [Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision](https://arxiv.org/abs/2601.19798)
*Zhixiang Wei,Yi Li,Zhehan Kan,Xinghua Jiang,Zuwei Long,Shifeng Liu,Hongze Shen,Wei Liu,Xiaoyu Tan,Haojia Lin,Yubo Zhu,Qianyu Li,Di Yin,Haoyu Cao,Weibo Gu,Xin Li,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Mingkong Tang,Shuangyin Liu,Lexiang Tang,Haodong Lin,Junru Lu,Jiarui Qin,Lingfeng Qiao,Ruizhi Qiao,Bo Ke,Jianfeng He,Ke Li,Yangning Li,Yunhang Shen,Mengdan Zhang,Peixian Chen,Kun Yin,Bing Liu,Yunfei Wu,Huang Chen,Zhongpeng Cai,Xiaotian Li*

Main category: cs.CV

TL;DR: 本文提出了Youtu-VL框架，采用视觉-语言统一自回归监督（VLUAS）范式，优化目标从“以视觉为输入”转向“以视觉为目标”，提升了视觉细粒度信息的保留能力，改善了多模态理解效果。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉-语言模型取得了显著进展，但它们在保留细粒度视觉信息方面存在局限，导致多模态理解偏于粗糙。作者认为问题根源在于现有模型训练范式的文本主导优化倾向，即仅将视觉信号视为输入条件而非监督目标。

Method: 提出Youtu-VL框架，采用视觉-语言统一自回归监督（VLUAS）范式。具体做法是将视觉token直接纳入预测序列，对视觉细节和语言内容同时进行自回归统一监督。此外，该范式扩展至视觉中心任务，使标准VLM无需特定结构扩展即可处理此类任务。

Result: 实验证明，Youtu-VL在通用多模态任务和视觉中心任务上都取得了有竞争力的性能。

Conclusion: Youtu-VL为发展全面的通用视觉智能体提供了坚实基础，并推动视觉-语言模型对细粒度视觉信息的更好捕捉与理解。

Abstract: Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.

</details>


### [72] [Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering](https://arxiv.org/abs/2601.19821)
*Kun Li,Michael Ying Yang,Sami Sebastian Brandt*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的音频—视觉问答（AVQA）方法，关注于提升多模态下音视信号与问题联合推理，显著提升了现有任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有AVQA方法多关注视觉信息，音频仅为补充，文本问题并未充分指导音视理解，这限制了音视及三模态深度联合推理的效果。

Method: 提出基于问题引导的空间—时间—频率（QSTar）交互方法，使问题信息更好地指导音频频域特征、空间和时序特征提取，并通过问题上下文推理模块（QCR）精准聚焦语义相关的音视特征。

Result: 在多个AVQA基准测试上取得了显著优于音频QA、视觉QA、视频QA及现有AVQA方法的性能提升。

Conclusion: 所提方法能更好地实现音频、视觉和文本问题的联合深度推理，为多模态理解和AVQA任务带来了新的提升。

Abstract: Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.

</details>


### [73] [HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation](https://arxiv.org/abs/2601.19849)
*Haya Alyoussef,Ahmad Bdeir,Diego Coello de Portugal Mecke,Tom Hanika,Niels Landwehr,Lars Schmidt-Thieme*

Main category: cs.CV

TL;DR: 本文提出了HexFormer，一种基于双曲几何的视觉Transformer，通过在注意力机制中引入指数映射聚合，实现了更准确和稳定的图像表示，提升了图像分类的表现。


<details>
  <summary>Details</summary>
Motivation: 传统欧式空间难以自然地建模复杂的层次和关系结构，而这些结构在多模态数据（如图像、文本和图等）中普遍存在。双曲几何为捕捉这类结构提供了优越的表示空间。作者旨在利用双曲几何属性提升视觉Transformer对层级、关系信息的建模能力和训练稳定性。

Method: 提出了HexFormer和HexFormer-Hybrid两种模型：HexFormer为完全双曲几何ViT结构；HexFormer-Hybrid则结合了双曲编码器和欧氏线性分类头。核心创新是设计了一种基于指数映射聚合的新型注意力机制，替代欧氏空间中的质心平均策略，增强了聚合表示的准确性与稳定性。

Result: 多项数据集实验证明，HexFormer系列在性能上超越了欧氏基线模型以及先前的双曲ViT，特别是混合模型（HexFormer-Hybrid）表现最好。此外，分析显示双曲Transformer梯度更稳定、对warmup策略敏感度更低，训练更鲁棒高效。

Conclusion: 双曲几何为视觉Transformer带来了更强的梯度稳定性和性能提升。简单的指数映射聚合机制亦可带来显著实践收益，显示出双曲空间在视觉任务中的潜力和实际价值。

Abstract: Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.

</details>


### [74] [EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning](https://arxiv.org/abs/2601.19850)
*Binzhu Xie,Shi Qiu,Sicheng Zhang,Yinqiao Wang,Hao Xu,Muzammal Naseer,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了EgoHandICL，这是首个应用于第三人称视觉下3D手部重建的in-context learning (ICL)框架，有效提升了模型的语义一致性、视觉一致性和对复杂场景的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部重建在面对深度歧义、自遮挡、手与物体的复杂交互等问题时效果有限，尤其在未见过的场景中表现较差。本文旨在提升在egocentric场景下的稳健性与泛化能力。

Method: EgoHandICL方法包括：1) 基于视觉-语言模型（VLM）检索补充样例，2) 针对多模态上下文设计的ICL特定tokenizer，3) 基于掩码自编码器（MAE）的网络结构，并结合手部引导的几何与感知目标进行训练。

Result: 在ARCTIC和EgoExo4D数据集上，EgoHandICL显示出对比现有SOTA方法的持续性能提升。此外，EgoHandICL在真实世界场景下有良好的泛化能力，还能通过重建手部作为视觉提示提升EgoVLM对手-物体交互的推理能力。

Conclusion: EgoHandICL框架有效提升了egocentric视角下3D手部重建的表现，并推动了结合ICL、VLM等先进技术进行多模态感知的研究。

Abstract: Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL

</details>


### [75] [SONIC: Spectral Oriented Neural Invariant Convolutions](https://arxiv.org/abs/2601.19884)
*Gijs Joppe Moens,Regina Beets-Tan,Eduardo H. P. Pooch*

Main category: cs.CV

TL;DR: 本文提出了一种名为SONIC的新型卷积方法，通过连续谱参数化增强网络获取全局信息能力，并在多个任务上表现优异，参数量远少于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN卷积核局部感受野受限，难以捕获全局信息；而ViT虽有全局连接，但缺乏空间归纳偏置，并强依赖于初始切块大小与位置编码。因此需要一种结构化且具备全局感受力的表示法来弥补两者不足。

Method: 提出SONIC方法：以连续的谱参数化方式，实现对卷积操作的建模。SONIC使用一小组共享的、方向选择性强的成分，在全频域产生平滑响应，能够自然适应不同分辨率，实现全局感受野。

Result: 在合成基准、图像分类和三维医学数据集等多个任务上，SONIC在应对几何变换、噪声和分辨率变化方面表现出更强的鲁棒性，且在参数量大大减少的情况下，达到了与卷积、注意力以及以往谱方法相媲美甚至优于它们的效果。

Conclusion: 连续、方向感知的谱参数化方法，为传统空间和频域算子提供了一种更原理性、可扩展的替代方案。

Abstract: Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.

</details>


### [76] [DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding](https://arxiv.org/abs/2601.19898)
*Shubham Patle,Sara Ghaboura,Hania Tariq,Mohammad Usman Khan,Omkar Thawakar,Rao Muhammad Anwer,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了DuwatBench数据集，用于评测多模态模型在阿拉伯文书法识别中的表现，填补了该领域研究空白。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在多种语言取得进展，但在处理艺术化、风格化的阿拉伯文书法方面能力有限，相关数据与评测工具稀缺。

Method: 研究团队构建了DuwatBench基准，包括1272个样本、1475个独特词汇，涵盖6种传统及现代书法风格，并配有检测标注。利用此基准评测13个领先阿拉伯语和多语种多模态模型在各类书法场景下的表现。

Result: 实验显示主流模型在标准文本下表现良好，但面对复杂书法、多变艺术风格及视觉-文本对齐难题时表现不佳。

Conclusion: DuwatBench的数据与评测工具通过公开发布，旨在推动阿拉伯文化在AI领域的公平纳入与多模态研究的发展。

Abstract: Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries](https://arxiv.org/abs/2601.18899)
*Yuchen Zhang,Ravi Shekhar,Haralambos Mouratidis*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言家族的多语种ASR系统连接器共享策略，可提升多语种自动语音识别的参数效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的ASR系统每种语言都要单独训练连接器，未考虑语言之间的相关性，参数效率较低。

Method: 作者创新性地根据语言所属家族进行连接器共享，相同家族的多种语言共用一个连接器，并在两套多语言LLM和两类真实语音语料（包含精选语料和众包语料）上进行了实验验证。

Result: 实验证明，基于语言家族的连接器设计在减少参数数量的同时，增强了系统在不同语域间的泛化能力。

Conclusion: 基于语言家族的连接器共享策略是一种高效、实用且可扩展的方法，有助于大规模多语种ASR的实际部署。

Abstract: Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.

</details>


### [78] [Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration](https://arxiv.org/abs/2601.18901)
*Christopher Kissling,Elena Merdjanovska,Alan Akbik*

Main category: cs.CL

TL;DR: 本论文提出了一种新的校准探测框架，从模型置信度的三个维度评估语言模型预训练期间习得的关系知识，发现主流语言模型，尤其是采用masking目标的模型，普遍存在过度自信的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的知识探测主要关注准确率等指标，忽视了模型置信度的校准问题。置信度校准对于理解模型的可靠性和实际应用中的表现尤为重要。

Method: 提出基于三种置信度模式（内在置信度、结构一致性、语义基础）的关系知识校准探测框架，并对10个因果语言模型和6个掩蔽语言模型进行了广泛分析。

Result: 实验表明，多数模型都表现出过度自信，尤其是采用masking目标的模型。只有考虑重述语句带来不一致性的置信度估算，才能获得最佳校准分数。此外，现有大型预训练模型并未准确编码与语言置信度表达相关的语义。

Conclusion: 单纯依赖准确率等传统指标不足以评估语言模型关系知识的可靠性。应引入置信度校准评价，推动更可信赖的大模型发展。

Abstract: Knowledge probing quantifies how much relational knowledge a language model (LM) has acquired during pre-training. Existing knowledge probes evaluate model capabilities through metrics like prediction accuracy and precision. Such evaluations fail to account for the model's reliability, reflected in the calibration of its confidence scores. In this paper, we propose a novel calibration probing framework for relational knowledge, covering three modalities of model confidence: (1) intrinsic confidence, (2) structural consistency and (3) semantic grounding. Our extensive analysis of ten causal and six masked language models reveals that most models, especially those pre-trained with the masking objective, are overconfident. The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing. Moreover, even the largest pre-trained models fail to encode the semantics of linguistic confidence expressions accurately.

</details>


### [79] [Flatter Tokens are More Valuable for Speculative Draft Model Training](https://arxiv.org/abs/2601.18902)
*Jiaming Fan,Daming Cao,Xiangzhong Luo,Jiale Fu,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于样本平坦度的新度量和数据蒸馏方法，以提升Speculative Decoding训练效率，在保持推理加速的前提下显著减少训练数据和时间。


<details>
  <summary>Details</summary>
Motivation: 现有Speculative Decoding需用大量数据训练draft模型，成本较高。作者发现不同训练样本对SD接受率的贡献不同，因此希望通过筛选更有价值的数据，提高训练效率。

Method: 作者提出用“平坦度”指标衡量样本的价值，发现导致目标模型输出分布更平坦的样本更有用。基于该指标，开发了一种样本级平坦度筛选的数据蒸馏方法（SFDD），只保留最有价值的样本进行训练。

Result: 在EAGLE框架上实验显示，使用SFDD后仅用50%的数据可实现2倍训练加速，且最终推理加速与全数据基线相差不到4%。

Conclusion: 本文提出了一套有效的数据筛选策略，大幅提升了Speculative Decoding训练的效率，在保障推理加速表现的前提下大幅降低了所需的训练数据和资源。

Abstract: Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://anonymous.4open.science/r/Flatness.

</details>


### [80] [BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models](https://arxiv.org/abs/2601.18933)
*Kaustubh D. Dhole*

Main category: cs.CL

TL;DR: 本文提出了一个针对"婴儿语言模型"的推理能力评估基准——BabyReasoningBench，发现这些模型在模拟儿童语言环境下对不同推理任务表现不均衡。


<details>
  <summary>Details</summary>
Motivation: 现有推理能力评估以成人知识和能力为主，而训练于儿童语言环境下的模型与之不匹配。作者希望开发一个适合此类模型能力的测试基准，以揭示模型在受限数据环境下发展推理能力的真实情况。

Method: 作者设计了BabyReasoningBench，这是由GPT-5.2生成的19个推理任务，涵盖心理理论、类比和关系推理、因果推理等，参考了发展心理学经典实验。同时，使用两种基于GPT-2、分别在儿童语料（1000万和1亿字）上训练的婴儿语言模型进行测试。

Result: 两种婴儿语言模型整体推理能力较弱且表现不均，但在某些因果与物理推理任务中随训练数据量提升有所改善，而信念归因和涉及语用能力的任务依然表现较差。

Conclusion: BabyReasoningBench为我们理解儿童语料训练下模型推理能力的形成机制提供了更贴合发展的分析视角，是分析相关能力如何涌现和受限的重要工具。

Abstract: Traditional evaluations of reasoning capabilities of language models are dominated by adult-centric benchmarks that presuppose broad world knowledge, complex instruction following, and mature pragmatic competence. These assumptions are mismatched to baby language models trained on developmentally plausible input such as child-directed speech and early-childhood narratives, and they obscure which reasoning abilities (if any) emerge under such constraints. We introduce BabyReasoningBench, a GPT-5.2 generated benchmark of 19 reasoning tasks grounded in classic paradigms from developmental psychology, spanning theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that are known to be confounded by memory and pragmatics. We find that two GPT-2 based baby language models (pretrained on 10M and 100M of child-directed speech text) show overall low but uneven performance, with dissociations across task families: scaling improves several causal and physical reasoning tasks, while belief attribution and pragmatics-sensitive tasks remain challenging. BabyReasoningBench provides a developmentally grounded lens for analyzing what kinds of reasoning are supported by child-like training distributions, and for testing mechanistic hypotheses about how such abilities emerge.

</details>


### [81] [LLMs versus the Halting Problem: Revisiting Program Termination Prediction](https://arxiv.org/abs/2601.18987)
*Oren Sultan,Jordi Armengol-Estape,Pascal Kesseli,Julien Vanegue,Dafna Shahaf,Yossi Adi,Peter O'Hearn*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在预测C程序终止性上的表现，发现其预测能力接近现有顶级工具，但在生成证明方面能力有限，并且在程序变长时性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统上，程序终止性无法普遍自动判定（即停机问题不可判定），现有自动工具各有局限，通常依赖特定语言和架构。随着LLM技术的快速发展，作者想探究LLM是否能可靠地预测程序终止性，从而推动验证工具的进步。

Method: 作者选取了国际软件验证竞赛（SV-Comp 2025）终止性类别中的多样化C程序，对多种LLM（包括GPT-5、Claude Sonnet-4.5、Code World Model等）进行测试和评测，比较其预测结果与专业工具的排名。

Result: LLM（尤其是GPT-5和Claude Sonnet-4.5）在程序终止性预测上表现优异，排名仅次于顶级工具，Code World Model也超过了大部分现有工具。但LLM往往无法给出有效的证明，且对长程序的处理能力下降。

Conclusion: LLM在程序终止性预测领域前景可观，但距离实用证明生成仍有明显不足。相关发现有望激励后续研究，进一步探索LLM在处理不可判定问题上的潜力。

Abstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.

</details>


### [82] [Malicious Repurposing of Open Science Artefacts by Using Large Language Models](https://arxiv.org/abs/2601.18998)
*Zahra Hashemi,Zhiqiang Zhong,Jun Pang,Wei Zhao*

Main category: cs.CL

TL;DR: 作者提出了一条端到端流程，通过说服类“越狱”绕过LLM的安全防护，重新解释NLP论文并滥用其开放资源，用以生成潜在有害的研究方案，并通过不同LLM评估其危害性、可实施性及技术合理性。结果显示，LLM可高效生成恶意用途方案，但各模型评判分歧较大，需人工评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM被积极用于科研创新，但很少考虑这类模型滥用开源科学资源生成有害研究的风险，因此作者旨在填补这一安全空白并量化风险评估难点。

Method: 1. 基于说服使LLM越狱，绕过安全限制；2. 利用LLM分析公开NLP论文，将其数据集、方法等开源资源用于恶意用途；3. 构建评估体系，从危害性、可实施性、技术合理性三方面评判生成方案，并用多种LLM交叉评审。

Result: LLM确实能生成将原本合伦理的开源资源转用于恶意目的的方案。各大LLM评审标准不一：GPT-4.1相对宽松，Gemini-2.5-pro最为严格，Grok-3介于二者之间。

Conclusion: 现有LLM在恶意用途评估上不一致，难以作为可靠独立评判者。评估科研开放成果双重用途风险时，人工介入不可或缺。

Abstract: The rapid evolution of large language models (LLMs) has fuelled enthusiasm about their role in advancing scientific discovery, with studies exploring LLMs that autonomously generate and evaluate novel research ideas. However, little attention has been given to the possibility that such models could be exploited to produce harmful research by repurposing open science artefacts for malicious ends. We fill the gap by introducing an end-to-end pipeline that first bypasses LLM safeguards through persuasion-based jailbreaking, then reinterprets NLP papers to identify and repurpose their artefacts (datasets, methods, and tools) by exploiting their vulnerabilities, and finally assesses the safety of these proposals using our evaluation framework across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. Overall, our findings demonstrate that LLMs can generate harmful proposals by repurposing ethically designed open artefacts; however, we find that LLMs acting as evaluators strongly disagree with one another on evaluation outcomes: GPT-4.1 assigns higher scores (indicating greater potential harms, higher soundness and feasibility of misuse), Gemini-2.5-pro is markedly stricter, and Grok-3 falls between these extremes. This indicates that LLMs cannot yet serve as reliable judges in a malicious evaluation setup, making human evaluation essential for credible dual-use risk assessment.

</details>


### [83] [FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning](https://arxiv.org/abs/2601.19001)
*Haozheng Luo,Zhuolin Jiang,Md Zahid Hasan,Yan Chen,Soumalya Sarkar*

Main category: cs.CL

TL;DR: FROST是一种利用注意力机制高效推理的方法，通过修剪不重要的推理路径，提高模型推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法存在冗余推理路径和资源浪费问题，急需一种能提升推理效率并保证准确性的机制。

Method: 引入推理离群值的概念，基于注意力权重自动识别并修剪不重要的推理路径，理论上可在句子级别去除离群值并保留推理能力。

Result: 在四个基准测试和两个大型推理模型上，FROST相较主流推理方法，在减少token消耗（平均69.68%减少）和提升准确率（提升26.7%）方面效果显著，且大幅优化了注意力离群值统计指标。

Conclusion: FROST方法有效地提升了推理效率和准确性，为高效推理提供了新思路，理论上和实验上均优于当前主流方法。

Abstract: We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST

</details>


### [84] [Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2601.19063)
*Siddhant Arora,Jinchuan Tian,Jiatong Shi,Hayato Futami,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 该论文提出了首个面向语音对话系统的多奖励反馈强化学习框架，有效提升了语义和音质等多方面表现。作者还公开了相关数据集，推动可复现性研究。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统中的强化学习主要关注单一语义奖励，忽视了对话质量的多维度特性（如语音自然度、情感一致性等），且与实际增量式对话生成流程匹配不佳。

Method: 构建了结合语义、音质、情感一致性等多重奖励的反馈强化学习框架，提出针对增量语音输出时的分块预测和奖励聚合方法，并进行系统性偏好学习研究。

Result: 实验证明，单奖励训练只提升特定指标，而多奖励训练可同时提升语义质量和音频自然度。

Conclusion: 多维度、整体性的奖励对实际对话系统尤为关键，论文所提方案能明显改进对话系统质量。

Abstract: Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.

</details>


### [85] [PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling](https://arxiv.org/abs/2601.19096)
*Sohhyung Park,Hyunji Kang,Sungzoon Cho,Dongil Kim*

Main category: cs.CL

TL;DR: 该论文提出了PsyProbe系统，通过系统性用户心理状态建模与主动提问机制，提升了心理咨询对话系统的专业性与有效性，在真实场景评测中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的心理对话系统主要是被动响应，缺乏系统的用户心理状态建模，难以达到专业心理咨询探索环节的需求。论文旨在解决对话中缺乏主动性与结构性的问题。

Method: 提出PsyProbe系统，以PPPPPI六维心理框架结合错误认知检测，对用户状态进行结构化建模。系统包含结构化心理画像提取（State Builder）、信息缺口追踪（Memory Construction）、动机性访谈策略规划（Strategy Planner）、以及主动提问与问句生成修订（Response Generator）等模块。

Result: 通过27名参与者在真实韩文心理咨询场景下进行自动评测、用户与专家评估。PsyProbe在各项自动评测指标及自然性、专业性与主动提问能力上均优于基线与消融版本，能有效提升用户参与意愿及理解核心问题能力。

Conclusion: 系统性心理状态建模结合主动探查提问显著提升心理咨询对话系统的效果，PsyProbe方法在实际应用中表现出与专业咨询师相当的问题探查能力，对未来AI心理健康系统设计具有重要参考价值。

Abstract: Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.

</details>


### [86] [Leveraging Sentence-oriented Augmentation and Transformer-Based Architecture for Vietnamese-Bahnaric Translation](https://arxiv.org/abs/2601.19124)
*Tan Sang Nguyen,Quoc Nguyen Pham,Tho Quan*

Main category: cs.CL

TL;DR: 探讨如何利用最新的神经机器翻译（NMT）技术及两种数据增强策略，突破越南语-巴拿语翻译资源稀缺的困境，提升巴拿语在线可获取性与沟通能力。


<details>
  <summary>Details</summary>
Motivation: 巴拿族语言具有重要的文化和历史价值，但由于资源匮乏，其数字化与传承面临困难。政府希望通过科技手段辅助语言保护与推广。发展高效的巴拿语翻译系统，有助于教育、交流和文献保存。

Method: 采用当前最先进NMT技术，并结合两种无需额外数据、无需复杂数据预处理的增强策略，专门针对越南语-巴拿语翻译任务，提升模型表现。这些方法具有通用性，可适用于多种NMT架构。

Result: 该研究所采纳的数据增强策略与NMT结合，可有效提升在低资源环境下越南语到巴拿语翻译的质量和可用性，无需使用超出原始平行语料的额外数据。

Conclusion: 创新的NMT与增强方法为巴拿语这种低资源语言的数字化、传播和保护提供了行之有效的解决路径，并为其它类似情况的少数民族语言翻译研究和应用提供借鉴。

Abstract: The Bahnar people, an ethnic minority in Vietnam with a rich ancestral heritage, possess a language of immense cultural and historical significance. The government places a strong emphasis on preserving and promoting the Bahnaric language by making it accessible online and encouraging communication across generations. Recent advancements in artificial intelligence, such as Neural Machine Translation (NMT), have brought about a transformation in translation by improving accuracy and fluency. This, in turn, contributes to the revival of the language through educational efforts, communication, and documentation. Specifically, NMT is pivotal in enhancing accessibility for Bahnaric speakers, making information and content more readily available. Nevertheless, the translation of Vietnamese into Bahnaric faces practical challenges due to resource constraints, especially given the limited resources available for the Bahnaric language. To address this, we employ state-of-the-art techniques in NMT along with two augmentation strategies for domain-specific Vietnamese-Bahnaric translation task. Importantly, both approaches are flexible and can be used with various neural machine translation models. Additionally, they do not require complex data preprocessing steps, the training of additional systems, or the acquisition of extra data beyond the existing training parallel corpora.

</details>


### [87] [Transparency-First Medical Language Models: Datasheets, Model Cards, and End-to-End Data Provenance for Clinical NLP](https://arxiv.org/abs/2601.19191)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Ayse Tuba Tugrul,Melike Nesrin Zaman,Ozkan Gunalp,Duygu Erisken,Sila Burde Dulger,Rana Irem Turhan,Izzet Ozdemir,Derya Umut Kulali,Ozan Akbulut,Harun Demircioglu,Hasan Basri Kara,Berfin Tavan*

Main category: cs.CL

TL;DR: 本文提出TeMLM，一套专为临床语言模型设计、以透明性为核心的发布工具包，涵盖了溯源、数据、建模及治理等方面，并用于大型合成临床NLP数据集与现有模型实验。


<details>
  <summary>Details</summary>
Motivation: 临床语言模型的开发和应用日益广泛，但涉及数据、建模和使用的透明性和可审计性普遍不足，限制了其在高风险医疗领域的推广和监管。

Method: 作者设计了一套透明性工件（TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance）及审核清单，并将其实例化于Technetium-I这一大规模合成临床数据集上，对ProtactiniumBERT模型在PHI去识别和ICD-9编码任务上进行了评测。

Result: 实现了可机读、可重复审核的模型发布流程，并在大规模合成数据集上获得了基线结果，具体包括PHI实体的识别和多标签ICD-9编码。

Conclusion: TeMLM提升了临床NLP模型的透明性和规范引用价值，但合成基准数据集仅适用于工具和流程验证，实际部署前仍需用真实临床数据进一步验证模型。

Abstract: We introduce TeMLM, a set of transparency-first release artifacts for clinical language models. TeMLM unifies provenance, data transparency, modeling transparency, and governance into a single, machine-checkable release bundle. We define an artifact suite (TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance) and a lightweight conformance checklist for repeatable auditing. We instantiate the artifacts on Technetium-I, a large-scale synthetic clinical NLP dataset with 498,000 notes, 7.74M PHI entity annotations across 10 types, and ICD-9-CM diagnosis labels, and report reference results for ProtactiniumBERT (about 100 million parameters) on PHI de-identification (token classification) and top-50 ICD-9 code extraction (multi-label classification). We emphasize that synthetic benchmarks are valuable for tooling and process validation, but models should be validated on real clinical data prior to deployment.

</details>


### [88] [Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs](https://arxiv.org/abs/2601.19202)
*Chi Zhang,Wenxuan Ding,Jiale Liu,Mingrui Wu,Qingyun Wu,Ray Mooney*

Main category: cs.CL

TL;DR: 本文系统性评估了当前视觉-语言模型（VLMs）在面对与图像证据相矛盾的误导性文本时的鲁棒性，并提出了新的基准数据集和评测框架。实验发现主流VLMs普遍容易受到文本误导，存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态推理任务（如VQA）上表现优异，但其在处理文本误导、尤其是图像和文本信息冲突时的鲁棒性还未充分探究。已有工作多集中于纯文本领域，而VLMs如何仲裁多模态冲突的信息尚不明确，因此亟需有针对性的研究。

Method: 作者提出了CONTEXT-VQA数据集，包括系统自动生成的、与图像证据相冲突的误导性文本，配合相应的图像-问题对。随后，设计并实施了详细的评测框架，对11种主流VLMs在面对多模态冲突时的表现进行系统性实验分析。

Result: 实验结果显示，当前最先进的VLMs在遇到误导性文本时，往往会忽视明显的视觉证据，并错误地采纳矛盾的文本信息，仅经历一次劝说性交互后，平均性能下降超过48.2%。

Conclusion: 现有VLMs在抵抗文本操纵方面存在明显短板，对于多模态感知的鲁棒性需予以高度重视，并推动相关改进以降低其被误导的风险。

Abstract: Vision-Language Models (VLMs) have shown strong multimodal reasoning capabilities on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the CONTEXT-VQA (i.e., Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough evaluation framework is designed and executed to benchmark the susceptibility of various models to these conflicting multimodal inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.

</details>


### [89] [How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability](https://arxiv.org/abs/2601.19208)
*Shawn Im,Changdae Oh,Zhen Fang,Sharon Li*

Main category: cs.CL

TL;DR: 本文通过分析Transformer类语言模型训练早期权重，揭示了模型中语义关联（如“bird”与“flew”）的形成机制。作者提出了一种梯度近似方法，推导出闭式表达式，帮助解释Transformer权重的组成，并与实际模型权重进行比对，发现理论与实践高度吻合。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何学习和表示语义关联对于连接深度学习与语言学理论、构建更有解释性的LLM至关重要，但目前对Transformer中这种机制的认知仍不清晰。

Method: 作者采用梯度的主导项近似，推导Transformer训练早期各层权重的闭式表达式，并将其分解为三类基函数（bigram、token互换性、上下文映射）。通过实验对比理论与实际训练得到的权重。

Result: 推导的理论表达式能够高度还原真实Transformer权重结构，验证了理论推测。定性分析显示，理论为解释Transformer中语义关联的形成和表示机制提供了支撑。

Conclusion: Transformer模型的各层权重可由三类基函数简明表达，反映文本统计属性，从而揭示了Transformer是如何通过这些组成部分学习语义关联。这一发现为大语言模型的解释性和机理分析提供了理论基础。

Abstract: Semantic associations such as the link between "bird" and "flew" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data in attention-based language models through the lens of training dynamics. By leveraging a leading-term approximation of the gradients, we develop closed-form expressions for the weights at early stages of training that explain how semantic associations first take shape. Through our analysis, we reveal that each set of weights of the transformer has closed-form expressions as simple compositions of three basis functions (bigram, token-interchangeability, and context mappings), reflecting the statistics of the text corpus and uncovering how each component of the transformer captures semantic associations based on these compositions. Experiments on real-world LLMs demonstrate that our theoretical weight characterizations closely match the learned weights, and qualitative analyses further show how our theorem shines light on interpreting the learned associations in transformers.

</details>


### [90] [A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews](https://arxiv.org/abs/2601.19214)
*Aakash Trivedi,Aniket Upadhyay,Pratik Narang,Dhruv Kumar,Praveen Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种混合管道方法，从客户评论中有效提取可操作建议，准确度和聚类一致性超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 客户反馈中常包含针对业务的改进建议，但这些建议常被混杂在非结构化、意图混合的文本中，难以直接提取，对企业决策造成障碍。现有方法要么仅分类句子，要么只生成高度概述的总结，难以给出具体改进指令。

Method: 作者提出结合高召回率RoBERTa分类器和精度-召回率权衡代理以减少难以恢复的假阴性，之后用受控、指令微调的大型语言模型（LLM）进行建议抽取、分类、聚类和摘要，即一种“混合推理”架构。

Result: 在酒店和餐饮真实数据集上，混合系统相较于仅用prompt、基于规则、仅分类器等基线，在建议抽取准确率和聚类一致性上表现更佳。人工评估也验证了建议与摘要的清晰、忠实和可解释性。

Conclusion: 混合推理架构能显著提升面向细粒度、可操作建议挖掘的效果，但在领域适应和本地高效部署等方面仍存在挑战。

Abstract: Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision-recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.

</details>


### [91] [DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models](https://arxiv.org/abs/2601.19221)
*Liu Xiao*

Main category: cs.CL

TL;DR: 本文提出了DREAMSTATE框架，首次深入研究RWKV等现代循环神经网络（RNN）的内部状态作为可编辑知识表示，实现了状态建模与编辑，并提出了融合全局与局部优势的混合新架构。


<details>
  <summary>Details</summary>
Motivation: 虽然RWKV等现代RNN在建模能力和状态效率上优于Transformer，但其内部状态作为知识表示的价值和编辑性尚未被充分研究。作者旨在填补这一空白。

Method: 作者提出DREAMSTATE框架，利用条件扩散Transformer（DiT）直接建模RWKV状态的概率空间，实现状态生成与编辑，并用t-SNE可视化和生成实验验证结构性。进一步，提出新混合架构，将并行DiT处理的全局上下文信息用于动态调整RNN核心参数，使响应机制从固定变为上下文自适应。

Result: DREAMSTATE揭示了RWKV内部状态的表达潜力。新混合模型通过多目标损失稳定训练，实验验证了其设计可行性与有效性。

Conclusion: 本研究为RNN状态作为知识编辑单元的探索开辟新方向，并为未来模型架构设计提供了具体参考。

Abstract: Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: https://huggingface.co/2dgx41s/DreamState.

</details>


### [92] [RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering](https://arxiv.org/abs/2601.19225)
*Kaehyun Um,KyuHwan Yeom,Haerim Yang,Minyoung Choi,Hyeongjun Yang,Kyong-Ho Lee*

Main category: cs.CL

TL;DR: 本文提出了一种专为小型大语言模型（LLMs）设计的知识图谱增强检索-生成（RAG）框架RPO-RAG，有效提升了小模型在知识图谱问答（KGQA）上的推理和准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽推理能力突出，但在知识密集型任务中常出现幻觉问题。目前利用知识图谱外部检索增强虽能改进此问题，但现有方法对小模型提升有限，且缺乏对推理路径的有效利用与组织。此外，小于7B参数的模型在这一领域研究不足。

Method: RPO-RAG采用三大创新策略：（1）基于语义的路径采样，为训练提供有效监督信息；（2）关系感知的偏好优化，将训练过程与知识图谱推理信号更好对齐；（3）以答案为中心的提示设计，将实体和推理路径以可解释方式组织输入模型。

Result: RPO-RAG在WebQSP和CWQ两个KGQA基准上，显著提升了小型LLM表现，WebQSP集上F1最高提升8.8%，CWQ集在小于8B参数模型组中取得最新SOTA，显著缩小了与大模型的性能差距。

Conclusion: RPO-RAG极大提升了小型LLM在知识图谱问答任务的推理能力，即便在3B参数规模下也具备出色表现，展现了在算力受限、设备端实际应用的巨大潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.

</details>


### [93] [DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models](https://arxiv.org/abs/2601.19267)
*Xinlong Chen,Weihong Lin,Jingyun Hua,Linli Yao,Yue Ding,Bozhou Li,Bohan Zeng,Yang Shi,Qiang Liu,Yuanxing Zhang,Pengfei Wan,Liang Wang,Tieniu Tan*

Main category: cs.CL

TL;DR: DiaDem模型专注于提升视听视频字幕中对话描述的准确性，通过新数据集与分阶段策略训练，结合自建评测基准，实现了对现有模型在对话内容和角色归属方面的超越。


<details>
  <summary>Details</summary>
Motivation: 当前视听视频字幕模型对对话内容描述不够准确，影响后续理解与生成任务，因此亟需提升字幕中的对话描述质量。

Method: 1）合成高质量用于监督微调的数据集；2）提出基于难度分区的两阶段GRPO训练策略提升对话描述能力；3）构建DiaDemBench评测基准，系统性评测模型在对话场景下的表现，包括发言人归属与话语转录保真度。

Result: DiaDem在DiaDemBench基准下超过了包括Gemini系列在内的多个商用模型，尤其在对话描述准确率方面表现突出，同时在常规视听字幕任务中也具备竞争力。

Conclusion: DiaDem模型显著改善了字幕中的对话描述能力，并在整体字幕生成质量上保持优秀，有望推动对话感知视听字幕的相关研究与应用发展。

Abstract: Accurate dialogue description in audiovisual video captioning is crucial for downstream understanding and generation tasks. However, existing models generally struggle to produce faithful dialogue descriptions within audiovisual captions. To mitigate this limitation, we propose DiaDem, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions while maintaining strong overall performance. We first synthesize a high-quality dataset for SFT, then employ a difficulty-partitioned two-stage GRPO strategy to further enhance dialogue descriptions. To enable systematic evaluation of dialogue description capabilities, we introduce DiaDemBench, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.

</details>


### [94] [Riddle Quest : The Enigma of Words](https://arxiv.org/abs/2601.19273)
*Niharika Sri Parasa,Chaitali Diwan,Srinath Srinivasa*

Main category: cs.CL

TL;DR: 本文提出了一个基于类比的谜语生成与评估流程，并利用该流程探讨大语言模型在谜语解答中的表现，发现模型往往只能猜中主要答案，而遗漏其他合理解释，显示谜语测试对模型推理全面性和歧义处理的价值。


<details>
  <summary>Details</summary>
Motivation: 谜语因其简练巧妙、需进行间接推理被视为研究语言理解和推理能力的有趣工具。作者希望通过谜语考察大语言模型推理的广度和对歧义的处理能力。

Method: 设计了一个谜语处理系统流水线，包括：三元组创建器（描述事实）、语义映射器（筛选可类比属性）、风格化生成器（生成线索）、验证器（收集所有可能答案），并用其研究不同谜语类型下大语言模型能否完整复现所有答案。

Result: 研究发现，大语言模型通常能猜出谜语的主要答案，但对其他可能的正确答案往往遗漏，反映其在全面推理和歧义处理上的不足。

Conclusion: 谜语作为轻量级测试工具，能有效揭示语言模型推理覆盖面不足及对多重解答的歧义处理短板，具有一定应用价值。

Abstract: Riddles are concise linguistic puzzles that describe an object or idea through indirect, figurative, or playful clues. They are a longstanding form of creative expression, requiring the solver to interpret hints, recognize patterns, and draw inferences to identify the answers. In this work, we introduce a simple pipeline for creating and evaluating analogy-based riddles. The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues, and a validator that collects all possible answers the riddle could point to. We use this validator to study whether large language models can recover the full answer set for different riddle types. Our case study shows that while models often guess the main intended answer, they frequently miss other valid interpretations. This highlights the value of riddles as a lightweight tool for examining reasoning coverage and ambiguity handling in language models.

</details>


### [95] [DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference](https://arxiv.org/abs/2601.19278)
*Fuliang Liu,Xue Li,Ketai Zhao,Yinxi Gao,Ziyan Zhou,Zhonghui Zhang,Zhibin Wang,Wanchun Dou,Sheng Zhong,Chen Tian*

Main category: cs.CL

TL;DR: DART提出了一种通过并行生成预测多步未来token，从而大幅降低LLM推理时speculative decoding草稿阶段延迟的新方法，能明显超越现有EAGLE3方法。


<details>
  <summary>Details</summary>
Motivation: 现有speculative decoding如EAGLE3提升了推理精度，但因需多步自回归推理，导致草稿阶段成为性能瓶颈，需要降低此阶段延迟以进一步提升整体推理速度。

Method: DART借鉴扩散式大模型（dLLMs），用目标模型的隐藏状态，单次前向过程并行预测多个未来被mask的位置的logits，完全去除草稿模型自回归过程；同时提出高效的树剪枝算法，配合N-gram约束，构造语义连贯的高质量草稿树。

Result: 实验显示，DART在多个数据集上端到端推理加速2.03~3.44倍，比EAGLE3平均提升约30%。

Conclusion: DART大幅减少草稿阶段的开销，保持高准确率，极大提高整体推理速度，是一种实用的speculative decoding方法。

Abstract: Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the drafting stage itself a performance bottleneck. Inspired by diffusion-based large language models (dLLMs), we propose DART, which leverages parallel generation to reduce drafting latency. DART predicts logits for multiple future masked positions in parallel within a single forward pass based on hidden states of the target model, thereby eliminating autoregressive rollouts in the draft model while preserving a lightweight design. Based on these parallel logit predictions, we further introduce an efficient tree pruning algorithm that constructs high-quality draft token trees with N-gram-enforced semantic continuity. DART substantially reduces draft-stage overhead while preserving high draft accuracy, leading to significantly improved end-to-end decoding speed. Experimental results demonstrate that DART achieves a 2.03x--3.44x wall-clock time speedup across multiple datasets, surpassing EAGLE3 by 30% on average and offering a practical speculative decoding framework. Code is released at https://github.com/fvliang/DART.

</details>


### [96] [ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction](https://arxiv.org/abs/2601.19286)
*Jesus Lovon-Melgarejo,Jose G. Moreno,Christine Damase-Michel,Lynda Tamine*

Main category: cs.CL

TL;DR: 本文提出了ReToP框架，通过端到端训练EHR重写器和临床预测器，将大语言模型更好地应用于电子健康记录（EHR）预测任务，并在MIMIC-IV数据集等多任务上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够提升EHR数据在临床预测中的利用，但当前方法大多与任务无关，仅作为编码器或补全模块，缺乏与具体预测任务的深度结合。这一方式限制了预测准确性。

Method: 提出ReToP框架，将EHR重写和预测两个模块进行端到端联合训练。为解决无足够EHR重写训练数据问题，采用基于临床特征选择的策略生成合成伪标签，用于微调重写器。同时创新性地引入CSC（分类器监督贡献）分数，引导重写器生成有助预测的重写内容。

Result: ReToP在MIMIC-IV数据集上的三个临床预测任务上优于强基线模型。此外，ReToP在迁移到新任务和新数据集时表现出良好的泛化能力，仅需少量微调，同时保证重写内容的真实性和预测相关性。

Conclusion: ReToP框架通过任务导向的EHR重写，显著提升了基于EHR的临床预测效果，且具有良好的泛化和迁移能力，为大语言模型在健康医疗预测领域的应用带来了新的突破。

Abstract: Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.

</details>


### [97] [MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2601.19290)
*Yimeng Wang,Jiaxing Zhao,Hongbin Xie,Hexing Ma,Yuzhen Lei,Shuangxue Liu,Xuan Song,Zichen Zhang,Haoran Zhang*

Main category: cs.CL

TL;DR: MetaGen提出了一种无需训练即可在推理时自适应角色和协作拓扑结构的多智能体大语言模型系统，有效提升多智能体系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大模型系统通常依赖于固定的角色库和不可变的交互拓扑，这导致系统难以适应任务变化、新证据，且推理成本较高。

Method: MetaGen框架在无需更新大模型参数的情况下，在推理阶段动态生成和重写基于查询的角色设定，通过动态控制角色池和约束执行图，结合轻量级反馈信号调整结构和角色提示。

Result: 在代码生成与多步推理任务实验中，MetaGen在准确率和计算成本权衡方面优于现有多智能体基线。

Conclusion: MetaGen无需训练即可实现多智能体协作结构和角色的动态优化，有效提升多智能体大模型系统的自适应能力和推理效率。

Abstract: Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.

</details>


### [98] [Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics](https://arxiv.org/abs/2601.19302)
*Natapong Nitarach,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 提出F-1 Formula-One Prompting方法，通过先生成数学公式作为中间表达，再自适应选择推理路径，大幅提升大模型在应用数学问题（金融、物理等）上的推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有如CoT（思维链）和PoT（程序链）方法，通过让大模型用自然语言或代码分步推理提升数学推理能力，但这些方法没有专门针对应用数学（如金融、物理、密码学）中“回忆或推导关键公式”的需求。

Method: 提出Formula-One Prompting（F-1），分两个阶段：1）首先从问题描述中提取或推导出核心数学公式；2）再根据所得到的公式，智能选择采用CoT、PoT、或直接运算等多种策略进行求解，整个过程仅需一次模型调用。

Result: 在五个模型和四个基准测试上，F-1方法平均比CoT高5.76%、比PoT高8.42%。在应用领域上的提升更明显，对金融数学比CoT提升13.30%，奥林匹克题库中物理比纯数学提升更大（+2.55% vs +0.44%）。

Conclusion: F-1方法通过引入公式推导为中介，更契合应用数学问题的实际需求，显著优于传统思维链（CoT）等方法，特别适合需要公式抽象与分析的领域。

Abstract: Prompting techniques such as Chain-of-Thought (CoT) and Program-of-Thought (PoT) improve LLM mathematical reasoning by structuring intermediate steps in natural language or code. However, applied mathematics problems in domains like finance, physics, and cryptography often require recalling or deriving governing equations, a step that current approaches do not explicitly leverage. We propose Formula-One Prompting (F-1), a two-phase approach that uses mathematical equations as an intermediate representation before adaptive solving. F-1 first formulates governing equations from problem descriptions, then selects a solving strategy among CoT, PoT, or direct computation based on the generated equations, all within a single LLM call. Results across five models and four benchmarks show F-1 outperforms CoT by +5.76% and PoT by +8.42% on average. Crucially, gains are largest in applied domains: +13.30% on FinanceMath over CoT, and within OlympiadBench, larger gains on physics (+2.55%) than pure math (+0.44%). This demonstrates that F-1 is more effective than CoT in applied mathematics problems.

</details>


### [99] [When Benchmarks Leak: Inference-Time Decontamination for LLMs](https://arxiv.org/abs/2601.19334)
*Jianzhe Chai,Yu Zhe,Jun Sakuma*

Main category: cs.CL

TL;DR: 本文提出了一种全新的评价阶段去污染框架DeconIEP，可有效降低大语言模型（LLMs）在基准测试中因训练数据污染导致的异常高表现。该方法在保持评测集完整性的基础上，极大减少了评测结果的偏差。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的评测标准受测试集污染影响严重，即测试样本在训练集中存在，导致性能被人为抬高。以往的去污染方法要么损害评测集，要么影响模型正常推理，亟需新的解决方案平衡去污染效果和推理性能。

Method: 提出DeconIEP框架，在评测过程中对输入做小幅、受限的嵌入扰动，使用相对未污染的参考模型引导扰动生成器，促使被测模型避免走“记忆捷径”。该方法无须修改评测集，且只在推理阶段操作。

Result: 在多个开源权重的LLMs和基准集上，DeconIEP展示了极强的去污染能力，并且在正常（无污染）输入下对模型性能影响极小。

Conclusion: DeconIEP为评测LLMs提供了一种实用且高效的污染缓解方案，保证了性能评测的公正性与可信性。

Abstract: Benchmark-based evaluation is the de facto standard for comparing large language models (LLMs). However, its reliability is increasingly threatened by test set contamination, where test samples or their close variants leak into training data and artificially inflate reported performance. To address this issue, prior work has explored two main lines of mitigation. One line attempts to identify and remove contaminated benchmark items before evaluation, but this inevitably alters the evaluation set itself and becomes unreliable when contamination is moderate or severe. The other line preserves the benchmark and instead suppresses contaminated behavior at evaluation time; however, such interventions often interfere with normal inference and lead to noticeable performance degradation on clean inputs. We propose DeconIEP, a decontamination framework that operates entirely during evaluation by applying small, bounded perturbations in the input embedding space. Guided by a relatively less-contaminated reference model, DeconIEP learns an instance-adaptive perturbation generator that steers the evaluated model away from memorization-driven shortcut pathways. Across multiple open-weight LLMs and benchmarks, extensive empirical results show that DeconIEP achieves strong decontamination effectiveness while incurring only minimal degradation in benign utility.

</details>


### [100] [Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity in Text-to-Text Generation](https://arxiv.org/abs/2601.19350)
*Tathagata Raha,Clement Christophe,Nada Saadi,Hamza A Javed,Marco AF Pimentel,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 本文提出了一种无参考、多维评估生成式文本任务语义保真性的新方法，克服了现有BLEU和BERTScore的局限。


<details>
  <summary>Details</summary>
Motivation: 传统评测指标（如BLEU、BERTScore）在生成式文本任务（如翻译、摘要、临床记录生成等）无法有效捕捉语义保真性，容易遗漏内容遗漏与事实矛盾等关键错误。

Method: 作者将Cross-Examination Framework（CEF）进行了改进：将源文本和候选文本视为独立知识库，分别从中生成可验证性问题，并进行交叉检验，得出Coverage、Conformity、Consistency三类可解释性评分。同时进行了系统性稳健性分析以选择合适的“评审模型”，并对比了有无参考的两种模式。

Result: CEF方法在多种任务上被验证，能够发现标准指标遗漏的重要错误（如内容遗漏与事实矛盾）。无需参考答案时，CEF的评分与有参考时高度一致，且在人类专家验证下，CEF发现的语义错配问题与高影响力的语义错误强相关，尤其精于发现实体和关系的扭曲。

Conclusion: CEF能够高效并可靠地进行无参考多维语义评估，超越了传统指标，特别适用于识别高级语义错误与实体/关系失真，证明了其在生成式文本评估中的实用性和有效性。

Abstract: Traditional metrics like BLEU and BERTScore fail to capture semantic fidelity in generative text-to-text tasks. We adapt the Cross-Examination Framework (CEF) for a reference-free, multi-dimensional evaluation by treating the source and candidate as independent knowledge bases. CEF generates verifiable questions from each text and performs a cross-examination to derive three interpretable scores: Coverage, Conformity, and Consistency. Validated across translation, summarization and clinical note-generation, our framework identifies critical errors, such as content omissions and factual contradictions, missed by standard metrics. A key contribution is a systematic robustness analysis to select a stable judge model. Crucially, the strong correlation between our reference-free and with-reference modes validates CEF's reliability without gold references. Furthermore, human expert validation demonstrates that CEF mismatching questions align with meaning-altering semantic errors higher than with non-semantic errors, particularly excelling at identifying entity-based and relational distortions.

</details>


### [101] [Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement](https://arxiv.org/abs/2601.19360)
*Diego Rossini,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 本文提出了一种结合二分类标注、语言特征和数据增强的多词表达（MWE）识别方法，利用更小的DeBERTa-v3-large模型在CoAM和STREUSLE数据集上获得了显著优于超大语言模型(LLM)的效果。


<details>
  <summary>Details</summary>
Motivation: 在多词表达识别任务中，主流的大模型（如Qwen-72B）虽然性能高但参数巨大，计算资源消耗大。希望探索小模型通过合理设计也能取得甚至超越LLM的效果，特别是在资源受限的情况下。

Method: (1) 将MWE检测转化为二分类的START/END/INSIDE标注任务，而不是传统的跨度预测；(2) 引入NP分块和依存关系等语言特征，提升对不连续和名词型MWE的识别；(3) 利用过采样处理训练数据类别严重不均衡问题。

Result: 在CoAM数据集上，DeBERTa-v3-large模型F1得分为69.8%，较Qwen-72B（57.8%）提升了12个百分点，且参数量仅为其1/165。在STREUSLE数据集上也验证了方法的泛化性，F1达到78.9%。

Conclusion: 精心设计的小型模型不仅能大幅超过巨型LLM在结构化NLP任务上的表现，还更适合有限资源环境，具有重要的实际应用前景。

Abstract: We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165x fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. We confirm the generalization of our method on the STREUSLE dataset, achieving 78.9% F1. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.

</details>


### [102] [Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?](https://arxiv.org/abs/2601.19410)
*Ahrii Kim,Seong-heum Kim*

Main category: cs.CL

TL;DR: 本文系统性比较了专有和开源大语言模型（LLM）在文档级自动后编辑（APE）任务中的表现，发现专有LLM质量接近人工，但未能充分利用文档上下文，且成本和延迟较高。


<details>
  <summary>Details</summary>
Motivation: 当前，虽然大语言模型在机器翻译方面表现突出，但其在自动后编辑（APE），特别是涉及文档级上下文时的有效性还不明确。因此，需厘清不同类型LLM在此任务中的实际表现、优势及不足。

Method: 作者将多种专有和开源LLM应用于APE任务，采用朴素的文档级prompt方式，系统评估了模型的编辑质量、上下文利用能力、鲁棒性以及效率，并与人类编辑和自动评价指标进行对比。

Result: 实验发现，专有LLM即使用简单的单轮prompt，APE质量已接近人工，且比开源LLM对数据投毒攻击更鲁棒；但相关模型在利用文档级上下文进行语境纠错方面几乎没有提升。此外，自动评分指标无法充分反映模型实际改进，依赖人工评估仍然必要。

Conclusion: LLM推动APE性能显著提升，但当前模型对文档上下文利用有限，且高成本与高延迟限制了其实际应用前景。未来需要发展高效的长文本建模方法以进一步优化机器翻译后编辑任务。

Abstract: Automatic post-editing (APE) aims to refine machine translations by correcting residual errors. Although recent large language models (LLMs) demonstrate strong translation capabilities, their effectiveness for APE--especially under document-level context--remains insufficiently understood. We present a systematic comparison of proprietary and open-weight LLMs under a naive document-level prompting setup, analyzing APE quality, contextual behavior, robustness, and efficiency.
  Our results show that proprietary LLMs achieve near human-level APE quality even with simple one-shot prompting, regardless of whether document context is provided. While these models exhibit higher robustness to data poisoning attacks than open-weight counterparts, this robustness also reveals a limitation: they largely fail to exploit document-level context for contextual error correction. Furthermore, standard automatic metrics do not reliably reflect these qualitative improvements, highlighting the continued necessity of human evaluation. Despite their strong performance, the substantial cost and latency overheads of proprietary LLMs render them impractical for real-world APE deployment. Overall, our findings elucidate both the promise and current limitations of LLM-based document-aware APE, and point toward the need for more efficient long-context modeling approaches for translation refinement.

</details>


### [103] [KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking](https://arxiv.org/abs/2601.19447)
*Vítor N. Lourenço,Aline Paes,Tillman Weyde,Audrey Depeige,Mohnish Dubey*

Main category: cs.CL

TL;DR: KG-CRAFT方法通过结合知识图谱和对比性问题引导大型语言模型（LLM）进行更有效的事实核查，显著提升了模型在多项真实数据集上的验证能力。


<details>
  <summary>Details</summary>
Motivation: 自动事实核查中的声明验证任务依赖于模型能够根据权威证据（如文档或知识库）判断文本真实性。传统方法在如何充分利用结构化知识和增强证据推理方面存在不足，因此需要更高效的方法提升核查的准确性和解释性。

Method: KG-CRAFT首先基于声明及相关报告自动构建知识图谱，再借助知识图谱结构生成相关的对比性问题，以引导大语言模型关注和整合核心证据。这些对比性问题帮助模型提取并蒸馏基于证据的报告，最终合成为简明摘要，供LLM进行真实性判断。

Result: 在LIAR-RAW和RAWFC两个真实数据集上的广泛实验证明，KG-CRAFT在预测性能上达到了新的先进水平。详细分析也展现了基于知识图谱的对比性推理有助于提升LLM事实核查的能力。

Conclusion: 通过利用知识图谱和对比性问题，KG-CRAFT显著提升了大语言模型在自动事实核查任务中的表现，证明了这种基于结构化知识和推理框架的有效性。

Abstract: Claim verification is a core component of automated fact-checking systems, aimed at determining the truthfulness of a statement by assessing it against reliable evidence sources such as documents or knowledge bases. This work presents KG-CRAFT, a method that improves automatic claim verification by leveraging large language models (LLMs) augmented with contrastive questions grounded in a knowledge graph. KG-CRAFT first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary that is used for veracity assessment by LLMs. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.

</details>


### [104] [Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition](https://arxiv.org/abs/2601.19451)
*Isha Pandey,Ashish Mittal,Vartul Bahuguna,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 该论文提出了一种用于多语种自动语音识别（ASR）的稳定Mixture-of-Experts（MoE）投影器（SMEAR-MoE），在四种印度语言上优于单一投影器基线，能够有效提升识别效果并具备良好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型（LLM）与ASR结合的方法在多语言场景下表现不佳，主要因为单一投影器难以处理不同语言间复杂的声学与语义映射关系。

Method: 作者提出SMEAR-MoE方法，即采用稳定的Mixture-of-Experts投影器连接语音编码器和LLM，确保所有专家都能获得充分训练，防止部分专家“崩溃”失效，同时允许不同语言间共享专家以实现跨语言知识迁移。

Result: 在印地语、马拉地语、泰米尔语和泰卢固语四种印度语言的多语种ASR实验中，实现了最高达到7.6%的相对错误率（WER）降低，并且运行效率与单投影器基线相当。分析还发现相关语言确实倾向于使用相同的专家，表现出合理的语言聚类特性。

Conclusion: 论文证明了稳定的多专家投影器在多语种ASR任务中的有效性和可扩展性，为提升多语言ASR提供了新的思路和技术基础。

Abstract: Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.

</details>


### [105] [ClaimPT: A Portuguese Dataset of Annotated Claims in News Articles](https://arxiv.org/abs/2601.19490)
*Ricardo Campos,Raquel Sequeira,Sara Nerea,Inês Cantante,Diogo Folques,Luís Filipe Cunha,João Canavilhas,António Branco,Alípio Jorge,Sérgio Nunes,Nuno Guimarães,Purificação Silvano*

Main category: cs.CL

TL;DR: 本论文介绍了ClaimPT，这是一个针对欧洲葡萄牙语新闻报道中事实性主张的注释数据集，旨在推进该语言下的自动化事实核查研究。


<details>
  <summary>Details</summary>
Motivation: 当前人工查证事实仍费时费力，难以应对信息传播的速度。尤其葡萄牙语领域，缺乏可公开、标注良好的事实查证数据集，严重限制了相关NLP研究和应用。

Method: 作者与葡萄牙新闻社LUSA合作，收集并人工注释了1,308篇新闻文章，共计6,875条主张。每篇文章由两名熟练标注员注释，并由一名主审采用新标注方案进行验证。同时，论文也用此数据集训练并评估了主张检测的基线模型。

Result: 发布了高质量的ClaimPT数据集，并在该数据集上实现了主张检测的基线性能，建立了后续研究的初步基准。

Conclusion: ClaimPT的发布有望促进葡萄牙语等低资源语言的事实核查自动化，提升对新闻媒体虚假信息的研究和理解。

Abstract: Fact-checking remains a demanding and time-consuming task, still largely dependent on manual verification and unable to match the rapid spread of misinformation online. This is particularly important because debunking false information typically takes longer to reach consumers than the misinformation itself; accelerating corrections through automation can therefore help counter it more effectively. Although many organizations perform manual fact-checking, this approach is difficult to scale given the growing volume of digital content. These limitations have motivated interest in automating fact-checking, where identifying claims is a crucial first step. However, progress has been uneven across languages, with English dominating due to abundant annotated data. Portuguese, like other languages, still lacks accessible, licensed datasets, limiting research, NLP developments and applications. In this paper, we introduce ClaimPT, a dataset of European Portuguese news articles annotated for factual claims, comprising 1,308 articles and 6,875 individual annotations. Unlike most existing resources based on social media or parliamentary transcripts, ClaimPT focuses on journalistic content, collected through a partnership with LUSA, the Portuguese News Agency. To ensure annotation quality, two trained annotators labeled each article, with a curator validating all annotations according to a newly proposed scheme. We also provide baseline models for claim detection, establishing initial benchmarks and enabling future NLP and IR applications. By releasing ClaimPT, we aim to advance research on low-resource fact-checking and enhance understanding of misinformation in news media.

</details>


### [106] [GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs](https://arxiv.org/abs/2601.19503)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.CL

TL;DR: 本文提出了GradPruner方法，在大型语言模型微调初期基于梯度信息进行结构化剪枝，实现了训练和推理的高效化，能在只损失0.99%精度的情况下减少40%模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝多关注推理效率，但对训练（包括微调）往往开销大，如需搜索结构、蒸馏等，难以高效完成下游微调任务。因此，迫切需要一种在减少参数、加速推理的同时，也能提高微调训练效率的方法。

Method: GradPruner在微调初期通过累计参数梯度信息，构建初始梯度信息累计矩阵（IGIA-Matrix），评估各层重要性并进行剪枝。被剪枝层再基于该矩阵稀疏化，只合并符号相同元素以减少干扰，最后与保留层融合，实现结构压缩。

Result: 在两种LLM、八个下游数据集（涵盖医疗、金融等领域）上实验，GradPruner可减少40%参数，但仅损失0.99%准确率，验证了其高效性和有效性。

Conclusion: GradPruner为LLM微调提供了一种高效的结构化剪枝方案，在保证模型表现基本不变的前提下，大幅提升了训练和推理效率，对实际下游任务部署有重要价值。

Abstract: Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.

</details>


### [107] [Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs](https://arxiv.org/abs/2601.19507)
*Xiangyang Zhu,Yuan Tian,Zicheng Zhang,Qi Jia,Chunyi Li,Renrui Zhang,Heng Li,Zongrui Wang,Wei Sun*

Main category: cs.CL

TL;DR: 本文提出了一种自动化的大型视觉-语言模型（LVLM）安全性评测基准构建系统VLSafetyBencher，可以快速、低成本地生成高质量的安全性评测集。实验显示，该工具能有效区分模型安全性，并揭示不同LVLM安全表现的显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM在多模态任务中表现突出，但其安全问题严重威胁实际应用。现有的安全评测基准不仅构建成本高、复杂度固定且辨别力有限，也难以适应模型的快速发展和新出现的风险。

Method: 作者设计了VLSafetyBencher系统，包含数据预处理、生成、增强和筛选四个协同代理，以自动化方式构建并挑选高质量样本，显著提升测试集的效率和实用性。

Result: VLSafetyBencher可以在一周内低成本构建高质量安全基准。生成的基准测试显示，不同LVLM在安全性上的差异明显，最安全和最不安全模型之间的安全率差距达70%。

Conclusion: VLSafetyBencher能够有效自动化评测LVLM的安全性，为模型安全评估提供强大工具，有助于应对新风险并指导未来模型改进。

Abstract: Large vision-language models (LVLMs) exhibit remarkable capabilities in cross-modal tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process, static complexity, and limited discriminative power. Thus, they may fail to keep pace with rapidly evolving models and emerging risks. To address these limitations, we propose VLSafetyBencher, the first automated system for LVLM safety benchmarking. VLSafetyBencher introduces four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection agents to construct and select high-quality samples. Experiments validates that VLSafetyBencher can construct high-quality safety benchmarks within one week at a minimal cost. The generated benchmark effectively distinguish safety, with a safety rate disparity of 70% between the most and least safe models.

</details>


### [108] [Yunque DeepResearch Technical Report](https://arxiv.org/abs/2601.19578)
*Yuxuan Cai,Xinyi Lai,Peng Yuan,Weiting Liu,Huajian Li,Mingda Li,Xinghua Wang,Shengxie Zheng,Yanchao Hao,Yuyang Yin,Zheng Wei*

Main category: cs.CL

TL;DR: 本文提出了Yunque DeepResearch框架，通过分层、模块化、鲁棒的架构解决了自治智能体在深度研究任务中遇到的上下文噪声、易错性和扩展性问题，取得了在多项基准测试上的领先表现。


<details>
  <summary>Details</summary>
Motivation: 深度研究能力对于自治智能体处理复杂开放任务非常关键，但当前方法在长期任务中的上下文噪声、错误级联和模块化扩展受限等方面表现不足，因此有必要提出更有效的解决方案。

Method: Yunque DeepResearch通过三个主要模块实现：1）多智能体编排系统将子任务分配到工具与子智能体池；2）动态上下文管理机制将完成的子目标结构化总结，有效缓解信息超载；3）主动监督模块，检测异常并裁剪无用上下文，以增强系统鲁棒性。

Result: 该框架在GAIA、BrowseComp、BrowseComp-ZH和Humanity's Last Exam等深度研究基准上取得了SOTA效果，并通过开源方式提供了实现和应用案例。

Conclusion: Yunque DeepResearch为LLM驱动的深度研究任务提供了高效鲁棒的基础架构，显著提升了自治智能体的复杂任务执行能力，有助于推动相关研究和应用的发展。

Abstract: Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.

</details>


### [109] [Decompose-and-Formalise: Recursively Verifiable Natural Language Inference](https://arxiv.org/abs/2601.19605)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种新的将大语言模型与定理证明器结合的方法，用于提升自然语言推理中的可解释性和验证效率。通过分解、逐步验证与局部修正，显著提升了解释的正确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和定理证明器结合方法在面对真实世界复杂自然语言推理任务时，常因长文本和多步推理而产生自动形式化误差，且一处小错即可导致整体证明失败。当前方法多通过全局重生成修正错误，代价高昂且效率低，缺乏细粒度定位和修正能力。

Method: 作者提出一种分解与形式化的框架：1）将前提-假设对分解为原子推理步骤的蕴含树；2）自底向上验证该树，精确定位失败节点；3）在诊断的基础上仅对出错部分进行局部修正。此外，引入θ-替换以保证自动形式化过程中的一致性。该方法在多种推理任务和五个不同LLM上测试。

Result: 与当前最新方法相比，该方法在解释验证率上提升了21.6%~48.9%，同时降低了修正规模和计算时间，并保持了强劲的自然语言推理准确度。

Conclusion: 所提方法有效提升了自然语言推理任务中解释的可验证性和效率，展示了神经-符号混合方法在实际复杂推理场景下的优势。

Abstract: Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $θ$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.

</details>


### [110] [Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs](https://arxiv.org/abs/2601.19613)
*Xinzhong Wang,Ya Guo,Jing Li,Huan Chen,Yi Tu,Yijie Hong,Gongshen Liu,Huijia Zhu*

Main category: cs.CL

TL;DR: 该论文提出了一种并行推理范式（PIP），显著提升大语言模型在文档关键信息抽取中的推理速度。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在关键信息抽取任务上表现优秀，但由于自回归推理带来的输出依赖顺序，导致推理过程缓慢，不能高效处理需要同时抽取多个独立字段的场景。

Method: 本文将目标值均用“[mask]”占位，在单次前向传播中同时生成所有目标，提高推理效率，并针对性设计掩码预训练策略与大规模监督数据集以配合PIP范式。

Result: PIP模型在与传统自回归模型对比时，在保证表现基本持平的情况下，推理速度提升5-36倍。

Conclusion: PIP显著提高了KIE任务效率且保持高准确性，为实际大规模视觉文档关键信息抽取提供了可行途径。

Abstract: Key Information Extraction (KIE) from visually-rich documents (VrDs) is a critical task, for which recent Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have demonstrated strong potential. However, their reliance on autoregressive inference, which generates outputs sequentially, creates a significant efficiency bottleneck, especially as KIE tasks often involve extracting multiple, semantically independent fields. To overcome this limitation, we introduce PIP: a Parallel Inference Paradigm for KIE. Our approach reformulates the problem by using "[mask]" tokens as placeholders for all target values, enabling their simultaneous generation in a single forward pass. To facilitate this paradigm, we develop a tailored mask pre-training strategy and construct large-scale supervised datasets. Experimental results show that our PIP-models achieve a 5-36x inference speedup with negligible performance degradation compared to traditional autoregressive base models. By substantially improving efficiency while maintaining high accuracy, PIP paves the way for scalable and practical real-world KIE solutions.

</details>


### [111] [RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems](https://arxiv.org/abs/2601.19637)
*Weicong Liu,Zixuan Yang,Yibo Zhao,Xiang Li*

Main category: cs.CL

TL;DR: 本文提出了一个新的评审分配评测基准LR-bench，并引入了RATE模型来改进学术论文的评审人匹配效果，在最新数据集和强基线对比中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有论文评审人分配受到话题快速变化和代理信号准确性低的挑战，使得评估和方法落后且匹配效果不佳，亟需新的高保真数据和更有效的方法。

Method: 作者构建了LR-bench基准，基于2024-2025年AI/NLP新论文，通过专家自评熟悉度大规模采集人工标注数据；同时提出RATE方法，从评审人近期论文中提炼关键词简介，并基于启发式信号进行弱监督嵌入模型微调，实现论文与评审人的直接匹配。

Result: 在LR-bench和CMU金标准数据集上，RATE均超越了当前主流的嵌入模型基线，达到了最优性能。

Conclusion: LR-bench为评审分配评测带来了更符合当前实际的评测基准，RATE模型能够有效提升学术论文与评审人之间的匹配质量，推动了自动化评审分配系统的进步。

Abstract: Reviewer assignment is increasingly critical yet challenging in the LLM era, where rapid topic shifts render many pre-2023 benchmarks outdated and where proxy signals poorly reflect true reviewer familiarity. We address this evaluation bottleneck by introducing LR-bench, a high-fidelity, up-to-date benchmark curated from 2024-2025 AI/NLP manuscripts with five-level self-assessed familiarity ratings collected via a large-scale email survey, yielding 1055 expert-annotated paper-reviewer-score annotations. We further propose RATE, a reviewer-centric ranking framework that distills each reviewer's recent publications into compact keyword-based profiles and fine-tunes an embedding model with weak preference supervision constructed from heuristic retrieval signals, enabling matching each manuscript against a reviewer profile directly. Across LR-bench and the CMU gold-standard dataset, our approach consistently achieves state-of-the-art performance, outperforming strong embedding baselines by a clear margin. We release LR-bench at https://huggingface.co/datasets/Gnociew/LR-bench, and a GitHub repository at https://github.com/Gnociew/RATE-Reviewer-Assign.

</details>


### [112] [One Token Is Enough: Improving Diffusion Language Models with a Sink Token](https://arxiv.org/abs/2601.19657)
*Zihou Zhang,Zheyong Xie,Li Zhong,Haifeng Liu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文揭示了扩散语言模型（DLMs）在文本生成中存在的不稳定性问题（moving sink现象），并提出通过引入额外sink token来增强模型稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然实现了并行文本生成和性能上的竞争力，但在生成过程中出现了moving sink现象，导致推理鲁棒性下降，需要找到简单有效的方法加以解决。

Method: 分析Transformer中sink token在value空间的表征及其作用机制后，作者提出一种通过改进attention mask引入额外sink token的方法。具体是添加一个只能关注自身但对其他token可见的特殊token，以此来稳定sink token的位置和功能。

Result: 实验表明引入该额外sink token可以显著提升模型性能，且该token的位置无关其有效性，内容也无语义负担。

Conclusion: 通过结构化地添加一个特殊sink token，能够显著提升扩散语言模型的推理稳定性与生成质量，对后续结构设计具有借鉴价值。

Abstract: Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.

</details>


### [113] [SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking](https://arxiv.org/abs/2601.19667)
*Adam Remaki,Christel Gérardin,Eulàlia Farré-Maduell,Martin Krallinger,Xavier Tannier*

Main category: cs.CL

TL;DR: 提出了SynCABEL框架，通过生成合成的上下文语料解决生物医学实体链接标注数据稀缺问题，无需人工标注即可提升多语言领域性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学实体链接任务中，专家标注数据稀缺，获取高质量训练数据费用高、耗时大，严重限制了有监督方法的发展。

Method: 利用大型语言模型（LLM）为目标知识库中的所有候选概念生成包含丰富上下文的合成训练样本，从而扩充训练数据；并结合decoder-only模型和引导式推理方法；同时引入LLM评判协议，改善评测对实际有效预测的识别。

Result: 在MedMentions（英文）、QUAERO（法文）、SPACCC（西班牙文）三个多语种基准上实现新的SOTA表现，在获得与人工全监督相当效果时所需标注数据量减少达60%。LLM评判显示模型在临床上有效的预测比率有显著提升。

Conclusion: SynCABEL大大降低了对专家标注数据的依赖，提高了BEL模型的临床预测有效性，增强了数据利用效率，并为后续研究提供了资源和工具。

Abstract: We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.

</details>


### [114] [Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes](https://arxiv.org/abs/2601.19723)
*Yifan Wang,Jichen Zheng,Jingyuan Sun,Yunhao Zhang,Chunyu Ye,Jixing Li,Chengqing Zong,Shaonan Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种通过有针对性地扰动大型语言模型（LLMs）内部组件，模拟失语症语言障碍的新方法。实验证明，这种方法在不同模型架构和干预策略下，能生成更系统且接近真实失语症表现的语言退化。


<details>
  <summary>Details</summary>
Motivation: LLMs在语言行为和内部表达上越来越接近人类，本文希望探索LLMs是否能通过可控的方式，系统性地模拟因脑损伤引起的语言障碍（如失语症），以辅助康复假设的测试和语言功能组织的研究。

Method: 提出以临床为基础的组件层框架，通过有选择性地扰动LLMs内的功能模块，模拟Broca和Wernicke两种典型失语症的语言障碍。方法涵盖模块化MoE模型与密集型Transformer，通过统一干预接口，识别亚型相关模块、利用语言探针任务解释组件功能，并通过进阶扰动进行语言能力衰退评估，采用西方失语症量表（WAB）和Aphasia Quotient（AQ）量化结果。

Result: 不同架构和损伤方式下，针对性扰动导致的模型表现更接近失语症的系统性退化，优于随机扰动。其中，MoE模型的模块化特性带来更加局部且可解释的功能与障碍对应关系。

Conclusion: 模块化LLMs结合临床指导下的功能区扰动，为模拟失语症和研究语言功能如何在定向损伤下退化提供了有前景的平台。

Abstract: Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.

</details>


### [115] [TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching](https://arxiv.org/abs/2601.19739)
*Runjia Zeng,Qifan Wang,Qiang Guan,Ruixiang Tang,Lifu Huang,Zhenting Wang,Xueling Zhang,Cheng Han,Dongfang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为TokenSeek的插件方案，可以大幅降低大语言模型（LLMs）微调过程中的内存消耗，且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调方法虽然有效，但因内存消耗极高，降低了实际应用效率。尤其是在激活相关内存占用上，现有优化方法通常与数据无关，导致最终微调效果不佳且不稳定。

Method: 作者提出了TokenSeek方法，通过实例感知的Token筛选与丢弃，主动减少激活内存消耗。该方法可作为通用插件，无缝适配不同Transformer模型。

Result: TokenSeek在Llama3.2 1B等主流模型上，仅需原有内存的14.8%即可完成微调，同时还能保持甚至提升模型性能。

Conclusion: TokenSeek突破了内存瓶颈，实现了轻量化而有效的模型微调方式。其Token筛选机理具有可解释性，为未来提升Token效率提供了指导。

Abstract: Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/

</details>


### [116] [Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis](https://arxiv.org/abs/2601.19773)
*Zhuohan Long,Zhijie Bao,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本论文提出了一个用于交互式医疗问诊智能体评估的新框架，通过模拟病人与模拟报告者进行全面证据收集的过程建模，并用新指标ICR衡量信息覆盖完整性。还创建了多样化的基准EviMed，并提出策略REFINE提升智能体在证据收集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式医疗问诊智能体评估多侧重静态结果，忽视了证据收集过程，而有效证据收集对于实际临床具有关键作用。为补齐这一短板，作者设计了新评估框架，更真实地模拟问诊流程，并能细致量化信息收集能力。

Method: 作者提出基于“原子证据”的模拟患者与报告者系统，将问诊过程数字化建模，提出“信息覆盖率(ICR)”指标定量评估智能体证据采集的全面性，搭建了覆盖常见与罕见病情的EviMed基准，并比较了10种不同推理能力的模型。同时，作者提出REFINE策略，通过诊断验证机制，引导智能体主动补全不确定信息。

Result: 实验表明，推理能力强的模型在信息收集环节可能表现不足，是交互式任务的性能瓶颈。REFINE策略能有效改善这一短板，各数据集下都明显优于基准方法，还帮助小模型在强监督下取得更优成绩。

Conclusion: 有效的信息收集能力与推理能力同等重要，单靠优秀推理难以支撑高水平的交互式医疗问诊。REFINE策略和新评估框架为医疗AI的实用性和协作性提供了重要突破，对后续智能医疗系统开发具有指导价值。

Abstract: Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .

</details>


### [117] [LVLMs and Humans Ground Differently in Referential Communication](https://arxiv.org/abs/2601.19792)
*Peter Zeng,Weiling Li,Amie Paige,Zhengxiang Wang,Panagiotis Kaliosis,Dimitris Samaras,Gregory Zelinsky,Susan Brennan,Owen Rambow*

Main category: cs.CL

TL;DR: 本文通过多人多轮交流实验，揭示了当前生成式AI在理解人类意图及建立共同语境方面的不足，提出了用于分析该问题的实验工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 有效的人机合作需要AI能够准确预测人类意图，而目前AI在建构‘共同语境’（common ground）上存在明显短板，影响了其在实际交流中的协作能力。

Method: 作者进行了一个全因素设计的指称交流实验，涉及人-人、人-AI、AI-人和AI-AI多种配对。实验中，双方需多轮互动，通过描述无明显词汇标签的物体图片来完成匹配任务，系统采集了通信对话数据。

Result: 作者公布了在线数据采集流程、精度与效率分析工具及词汇重叠度指标，并建立了356组（89对×4轮）对话语料库。分析表明，大语言视觉模型（LVLMs）在互动式指称表达解析，尤其是建立共同语境方面存在明显局限。

Conclusion: LVLMs在模拟并理解人类交流、解析指称表述以建立共同语境时有明显不足，需要进一步改进，以提升AI在人机协作场景下的能力。

Abstract: For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.

</details>


### [118] [Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation](https://arxiv.org/abs/2601.19802)
*Aohua Li,Yuanshuo Zhang,Ge Gao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 该论文提出在复杂动态目标环境下进行零样本立场检测的新任务，并在中文社交媒体语境下创建数据集，通过细致评测，实验验证微调大模型有效性。


<details>
  <summary>Details</summary>
Motivation: 现有立场检测方法通常假设目标已知且静态，但真实社交媒体场景中目标复杂且动态，所以需要能自动识别多个未知目标并判断立场的新方法。

Method: 提出了“动态目标生成与多目标自适应”（DGTA）任务，无需事先目标知识自动抽取目标-立场对；构建中文社交媒体立场检测数据集，设计多维度评估指标；探索大模型端到端与两阶段微调策略，并评测多种基线。

Result: 实验表明，微调后的大模型在该任务表现最佳：两阶段微调Qwen2.5-7B的目标识别综合得分为66.99%，而端到端微调DeepSeek-R1-Distill-Qwen-7B的立场检测F1达到79.26%。

Conclusion: 提出的DGTA任务及相应大模型微调策略能够有效实现野外复杂情景下目标和立场的自动检测，为后续立场检测研究开辟新方向。

Abstract: Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.

</details>


### [119] [When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering](https://arxiv.org/abs/2601.19827)
*Mahdi Astaraki,Mohammad Arshi Saloot,Ali Shiraee Kasmaee,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TL;DR: 本论文评估了同步迭代检索与推理能否超越静态RAG（Gold Context），并发现分阶段检索通常优于单次供证，特别是在涉及多跳推理的科学场景。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG提升了LLM外部知识利用，但在需要多跳、稀疏知识和异构证据的科学任务中，尚不清楚迭代检索推理是否实际胜过静态RAG。

Method: 作者在ChemKGMultiHopQA数据集上，分别对11个LLM进行了无上下文、理想（Gold Context）上下文和迭代RAG三种测试，并用多种诊断指标分析模型表现和失败模式。

Result: 迭代RAG在多模型下持续优于Gold Context，提升最高达25.6个百分点。阶段式检索减少后期推理失败和证据过载，并动态纠正早期推理偏差。

Conclusion: 分阶段检索在科学RAG系统中比单纯理想证据更有效，对迭代检索推理的部署与诊断提供了实践指导，也为可靠的RAG框架奠定基础。

Abstract: Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.

</details>


### [120] [Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering](https://arxiv.org/abs/2601.19847)
*Fangan Dong,Zuming Yan,Xuri Ge,Zhiwei Xu,Mengqi Zhang,Xuanang Chen,Ben He,Xin Xin,Zhumin Chen,Ying Zhou*

Main category: cs.CL

TL;DR: 该论文提出AdaRAS方法，通过在大语言模型推理过程中有针对性地调控关键神经元激活状态，实现无需额外训练或高计算耗费即可提升模型推理可靠性。实验表明其能在多个数学和编程任务上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型推理能力强，但在复杂任务上表现可靠性不足，通常需要耗时的后处理或抽样方法，这影响了实际效率。因此，作者希望找到一种高效提升推理可靠性的方式。

Method: 作者发现LLM中部分神经元与推理正确性强相关，于是提出AdaRAS（Adaptive Reasoning Activation Steering）方法：用均值差分准则识别推理关键神经元，对其在推理时的激活加以调控，只在识别为错误推理时介入，从而提升整体推理表现。此方法无需再训练模型，也不增加推理采样次数。

Result: 在10个数学与编程基准测试上，AdaRAS均带来稳定提升。例如在AIME-24、AIME-25数据集上提升超过13%。此外，AdaRAS方法对数据集和更强模型具有良好可迁移性，性能优于常规后训练方案且无额外计算代价。

Conclusion: AdaRAS是一种简单高效、无需额外训练或计算开销即可提升大语言模型推理能力的方法，实验结果证明其泛化性和实用性，适用于多类任务。

Abstract: Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.

</details>


### [121] [Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection](https://arxiv.org/abs/2601.19871)
*Nicholas Cheng*

Main category: cs.CL

TL;DR: 该论文提出使用自反思机制提升低资源语言（如isiZulu和isiXhosa）机器翻译质量，通过模型自我批判和修正初译文，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言由于缺乏平行语料和语言资源，机器翻译效果一直很差，亟需无需额外数据和调优的新方法来提升翻译质量。

Method: 引入Reflective Translation框架，让大模型先生成初始翻译，再产出结构化自我批评，最后基于自我反思生成改进后的译文。在OPUS-100和NTREX-African英-祖鲁语、英-科萨语翻译任务上测试不同提示策略和置信度阈值。

Result: 对比初译文和反思后译文，BLEU和COMET分数均有提升，平均分别提升0.22 BLEU和0.18 COMET。统计检验显示提升显著。

Conclusion: 结构化自我反思是一种简单、无需微调且通用有效的方法，可实质性提升低资源语言机器翻译质量，并可扩展出带反思标注的数据集用于未来研究。

Abstract: Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.

</details>


### [122] [Evaluation of Oncotimia: An LLM based system for supporting tumour boards](https://arxiv.org/abs/2601.19899)
*Luis Lorenzo,Marcos Montana-Mendez,Sergio Figueiras,Miguel Boubeta,Cristobal Bernardo-Castineira*

Main category: cs.CL

TL;DR: 本研究提出了一种结合生成式人工智能（GenAI）的临床工具ONCOTIMIA，可自动化填报肺癌多学科肿瘤委员会表单，显著减轻肿瘤委员会成员的文档负担。


<details>
  <summary>Details</summary>
Motivation: 多学科肿瘤委员会（MDTBs）在肿瘤学决策中扮演关键角色，但需手动整理和记录大量异构临床信息，既耗时又增加了文档负担，亟需自动化与智能化解决方案。

Method: 开发了ONCOTIMIA系统，结合多层数据湖、混合关系与向量存储、增强型检索生成（RAG）及规则驱动的自适应表单模型，利用大语言模型（LLMs）将非结构化临床文档转为结构化、标准化表单。系统基于AWS Bedrock部署了6种LLM，对10例肺癌病例进行自动填表，评估字段填报准确率和整体时延。

Result: 所有LLM表现出较高的表单完成准确率，最佳配置下正确填充率达80%，同时大部分模型的应答时延临床可接受。模型越新、规模越大，准确率更高且时延未明显增大。

Conclusion: LLM辅助自动表单系统在肺癌多学科流程中技术上可行、操作上可用，有望大幅减少人工文档负担并保持数据质量。

Abstract: Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [123] [Learning the Pareto Space of Multi-Objective Autonomous Driving: A Modular, Data-Driven Approach](https://arxiv.org/abs/2601.18913)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.RO

TL;DR: 本文提出一个从真实轨迹数据中直接提取安全性、效率和交互性权衡关系的自主驾驶学习框架，并用Pareto前沿分析定义最优表现区间。结果显示，实现三者兼顾的工况极少，但为多目标优化研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统要在安全、效率和与人类交互等多重目标间平衡，但现有研究往往无法直接从实地数据中揭示这些目标的实际权衡关系，因此迫切需要一种实证化、可推广的方法来深入理解和优化自动驾驶车辆的实际表现。

Method: 作者提出了一种经验学习框架：利用自动驾驶车辆的自然轨迹数据，根据安全性、效率和交互性多维评分构建统一目标空间，再通过Pareto支配准则识别出非支配工况并形成多目标平衡表现的经验前沿。通过TGSIM真实路况数据进行实证分析。

Result: 分析发现，在所有自动驾驶实例中，只有0.23%实现了安全、效率、交互的Pareto最优，表明三者兼顾的场景极为罕见。Pareto最优状态下，这三项得分平均值均显著高于非最优工况，尤其交互性方面提升潜力最大。

Conclusion: 该框架可用极少的数据要求实现对自动驾驶多目标权衡的可视化和定量分析，有利于进一步优化自动驾驶算法，并可外推至其他多目标场景研究。

Abstract: Balancing safety, efficiency, and interaction is fundamental to designing autonomous driving agents and to understanding autonomous vehicle (AV) behavior in real-world operation. This study introduces an empirical learning framework that derives these trade-offs directly from naturalistic trajectory data. A unified objective space represents each AV timestep through composite scores of safety, efficiency, and interaction. Pareto dominance is applied to identify non-dominated states, forming an empirical frontier that defines the attainable region of balanced performance.
  The proposed framework was demonstrated using the Third Generation Simulation (TGSIM) datasets from Foggy Bottom and I-395. Results showed that only 0.23\% of AV driving instances were Pareto-optimal, underscoring the rarity of simultaneous optimization across objectives. Pareto-optimal states showed notably higher mean scores for safety, efficiency, and interaction compared to non-optimal cases, with interaction showing the greatest potential for improvement.
  This minimally invasive and modular framework, which requires only kinematic and positional data, can be directly applied beyond the scope of this study to derive and visualize multi-objective learning surfaces

</details>


### [124] [DeFM: Learning Foundation Representations from Depth for Robotics](https://arxiv.org/abs/2601.18923)
*Manthan Patel,Jonas Frey,Mayank Mittal,Fan Yang,Alexander Hansson,Amir Bar,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了DeFM，这是一个专为机器人应用训练的自监督深度图像基础模型，并在多个深度感知任务及平台上实现了最先进的性能，模型及代码已开源。


<details>
  <summary>Details</summary>
Motivation: 尽管深度传感器已广泛用于机器人，且仿真推动了深度视觉的迁移学习，但相比RGB领域的基础模型，面向深度模态的通用表征学习仍非常欠缺。因此，缺乏强大的、可泛化到不同任务和环境的深度视觉基础模型，成为制约机器人智能进步的瓶颈。

Method: 提出DeFM，利用DINO风格的自监督知识蒸馏，基于6000万张深度图像的高质量数据集进行训练。同时，创新性地引入输入归一化策略以保持多尺度度量感知，并进一步将大模型蒸馏为轻量版本以适应资源有限的设备。

Result: 在深度图像分类、分割、导航、行走、操作等多个基准数据集上，DeFM均取得了SOTA性能，且表现出从仿真到真实环境的极强泛化能力。

Conclusion: DeFM证明了深度图像基础模型在机器人领域的巨大潜力，不但实现了优秀性能，也兼具通用性和资源适应性，为后续深度模态的机器人自主学习提供了坚实基础。

Abstract: Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/

</details>


### [125] [Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot](https://arxiv.org/abs/2601.18963)
*Fauna Robotics,:,Diego Aldarondo,Ana Pervan,Daniel Corbalan,Dave Petrillo,Bolun Dai,Aadhithya Iyer,Nina Mortensen,Erik Pearson,Sridhar Pandian Arunachalam,Emma Reznick,David Weis,Jacob Davison,Samuel Patterson,Tess Carella,Michael Suguitan,David Ye,Oswaldo Ferro,Nilesh Suriyarachchi,Spencer Ling,Erik Su,Daniel Giebisch,Peter Traver,Sam Fonseca,Mack Mor,Rohan Singh,Sertac Guven,Kangni Liu,Yaswanth Kumar Orru,Ashiq Rahman Anwar Batcha,Shruthi Ravindranath,Silky Arora,Hugo Ponte,Dez Hernandez,Utsav Chaudhary,Zack Walker,Michael Kelberman,Ivan Veloz,Christina Santa Lucia,Kat Casale,Helen Han,Michael Gromis,Michael Mignatti,Jason Reisman,Kelleher Guerin,Dario Narvaez,Christopher Anderson,Anthony Moschella,Robert Cochran,Josh Merel*

Main category: cs.RO

TL;DR: 本文介绍了Sprout，一个专为安全、易用和可表达性设计的人形机器人开发平台，旨在促进机器人长期在人类环境中的应用和研发。


<details>
  <summary>Details</summary>
Motivation: 当前的人形机器人平台要么为封闭的工业系统，要么为学术原型，难以在真实的人类环境中安全、长期、便捷地部署和操作，这限制了领域发展。

Method: Sprout平台采用轻量化结构、合规控制、有限关节力矩和软包覆外观，实现了安全的人机协作。此外，整合了全身控制、集成夹爪的操作以及基于虚拟现实的遥操作，软件硬件一体化，还配备可表情头部以增强社交互动。

Result: Sprout平台降低了物理和技术障碍，使开发者能更方便地获得和部署高效能人形机器人。其安全性和表现力也推动了机器人在真实人类环境中的应用。

Conclusion: Sprout为开发具身智能和真实环境下人形机器人系统提供了切实可行的平台，有助于推动机器人技术的实际落地和社会互动等新方向的研究。

Abstract: Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.

</details>


### [126] [A Switching Nonlinear Model Predictive Control Strategy for Safe Collision Handling by an Underwater Vehicle-Manipulator System](https://arxiv.org/abs/2601.18971)
*Ioannis G. Polyzos,Konstantinos J. Kyriakopoulos*

Main category: cs.RO

TL;DR: 提出了一种用于水下机器人安全处理碰撞的切换式非线性模型预测控制策略，实现了在避免碰撞不可能时，通过机械臂与障碍物互动以减小损伤。


<details>
  <summary>Details</summary>
Motivation: 现有水下自主车辆面临操作过程中可能与障碍物发生碰撞的问题，缺乏有效的应急处理机制。尤其是在无法完全避免障碍物时，车辆如何降低结构损伤成为一个急需解决的问题。

Method: 提出了一种切换式非线性模型预测控制（NMPC）策略，在检测到不可避免碰撞时，控制算法切换为利用机械臂主动与障碍物接触，通过推、引导等动作，使车辆安全避让或减少对敏感部位的损害。该方法通过虚拟实验进行验证。

Result: 虚拟实验表明，该控制算法能够有效检测碰撞并做出相应反应，无论是主动避让还是利用机械臂进行适当处理，都能较好保护车辆敏感部位不受损伤。

Conclusion: 切换式NMPC控制策略为水下机器人在复杂环境下的碰撞处理提供了新的方法，使机器人即使在无法完全避免碰撞时，也能通过机械臂有效控制风险，提升了系统的安全性和智能性。

Abstract: For active intervention tasks in underwater environments, the use of autonomous vehicles is just now emerging as an active area of research. During operation, for various reasons, the robot might find itself on a collision course with an obstacle in its environment. In this paper, a switching Nonlinear Model Predictive Control (NMPC) strategy is proposed to safely handle collisions for an Underwater Vehicle-Manipulator System (UVMS). When avoiding the collision is impossible, the control algorithm takes advantage of the manipulator, using it to push against the obstacle, and deflect away from the collision. Virtual experiments are performed to demonstrate the algorithm's capability to successfully detect collisions and either avoid them, or use the manipulator to handle them appropriately without damaging sensitive areas of the vehicle.

</details>


### [127] [Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing](https://arxiv.org/abs/2601.19079)
*Naqash Afzal,Niklas Funk,Erik Helmut,Jan Peters,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 该论文提出了一种基于神经形态事件驱动触觉传感器的实时连续盲文识别方法，能够极大提升机器人读取盲文的速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人读取盲文方法需逐字符扫描，效率低且破坏自然阅读流畅性。视觉方案需大量计算，在实际环境中效果也易下降，亟需高效、实时且适应性强的新方法。

Method: 利用开源神经形态事件驱动触觉传感器Evetac，模拟人手指滑动识别盲文的机制，结合时空分割与轻量化ResNet分类器，对稀疏事件流进行处理，实现对不同压痕深度和滑动速度下的字符识别。

Result: 该系统在标准深度下字符识别准确率可达98%以上，具备多种盲文板类型适应性，并在语速快时亦保持高性能。在实物盲文板上进行词汇测试，词级别准确率超过90%，表现优于传统方法。

Conclusion: 神经形态触觉传感为机器人盲文识别提供了可扩展、低延迟的优选方案，对助残与机器人触觉感知等领域具有广泛的应用前景。

Abstract: Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.

</details>


### [128] [SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers](https://arxiv.org/abs/2601.19098)
*Kurt Enkera,Josh Pinskier,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: 本论文提出了一种自动化生成软体夹持器结构的方法，使其能更好地适应和抓取结构复杂的物体，显著提升了夹持性能。


<details>
  <summary>Details</summary>
Motivation: 传统软体夹持器无法有效抓取形态复杂、表面特征丰富的物体，这类物体在生产、医疗及农业等领域广泛存在。现有优化设计方法需要预先设定受力情况，但面对复杂物体时接触力难以预知，限制了个性化夹持器的设计能力。

Method: 提出SimTO框架，将基于物理仿真的接触分析与高分辨率拓扑优化结合，自动提取夹持过程中的载荷工况，并据此生成与具体物体特征高度匹配的软体夹持器结构，避免了人工指定力载荷的需求。

Result: 仿真结果显示，利用SimTO设计的软体夹持器不仅能针对具有复杂特征的对象生成高度定制化的结构，还能一定程度地泛化适应未见过的复杂物体。

Conclusion: SimTO方法能自动、准确地为复杂对象设计个性化的软体夹持器，克服了传统设计对人为准载荷输入的依赖，为软体夹持器多场景应用和自动化设计提供了新思路。

Abstract: Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear "optimal" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.

</details>


### [129] [Agree to Disagree: Consensus-Free Flocking under Constraints](https://arxiv.org/abs/2601.19119)
*Peter Travis Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 本文提出了一种无需全球信息或通讯即可灵活协调多机器人群体行为的新方法，适用于部分目标一致或存在冲突的场景。


<details>
  <summary>Details</summary>
Motivation: 随着多机器人系统的多样化和普及，实际应用中机器人常常需要在目标部分一致甚至冲突的情况下协作，而传统聚群算法假定机器人间目标一致（期望距离相同），难以满足实际需求。此外，缺乏信任和安全通讯亦是一大挑战。

Method: 通过引入一种新的受约束的集体势函数，方法允许不同机器人之间直接在本地感知基础上‘协商’彼此的期望距离参数，无需全球信息或机器人间通讯。

Result: 方法能够有效应对半信任环境下存在目标冲突的邻近机器人间的协作挑战，并通过仿真实验验证了算法的有效性。

Conclusion: 新方法为多机器人系统在实际应用中的灵活性和鲁棒性提升，尤其适用目标不完全一致、通信受限的复杂场景。

Abstract: Robots sometimes have to work together with a mixture of partially-aligned or conflicting goals. Flocking - coordinated motion through cohesion, alignment, and separation - traditionally assumes uniform desired inter-agent distances. Many practical applications demand greater flexibility, as the diversity of types and configurations grows with the popularity of multi-agent systems in society. Moreover, agents often operate without guarantees of trust or secure communication. Motivated by these challenges we update well-established frameworks by relaxing this assumption of shared inter-agent distances and constraints. Through a new form of constrained collective potential function, we introduce a solution that permits negotiation of these parameters. In the spirit of the traditional flocking control canon, this negotiation is achieved purely through local observations and does not require any global information or inter-agent communication. The approach is robust to semi-trust scenarios, where neighbouring agents pursue conflicting goals. We validate the effectiveness of the approach through a series of simulations.

</details>


### [130] [Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity](https://arxiv.org/abs/2601.19144)
*Tzvika Geft,William Zhang,Jingjin Yu,Kostas Bekris*

Main category: cs.RO

TL;DR: 本文提出了一种在不确定环境下提升自动化存储系统运营效率的框架。通过优化存储和检索序列，旨在最小化物品的搬移次数，尤其适用于物流中心等场景。该方法对于存储序列有扰动（k-bounded perturbations）时仍能保持极高效率。


<details>
  <summary>Details</summary>
Motivation: 在现实物流场景中，存储和取货顺序常因不确定因素变化，现有方案多针对理想序列，缺乏对实际扰动的鲁棒性需求支持。本文针对顺序扰动，设计鲁棒性更强的系统，减少搬移次数，提高效率。

Method: 论文分析了二维网格自动存储系统的结构和取货规律，提出$k$-bounded扰动模型，并通过理论分析和算法设计，得到消除搬移所需的最小网格宽度条件，开发高效求解器安排鲁棒存储。对于更大扰动时，设计了进一步减少搬移次数的策略。

Result: 理论上证明了消除搬移的网格宽度下界和最优性。在实验中，k不超过网格宽度一半时，几乎消除了全部搬移；k为全宽时，搬移次数减少了50%以上。

Conclusion: 提出的框架和算法能有效提升自动化存储系统在不确定检索序列下的运营效率，减少搬移，具备良好实际应用价值。

Abstract: This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty. It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid. The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards. The objective is to minimize the load relocations to ensure efficient retrieval. A previous result guarantees a zero-relocation solution for known storage and retrieval sequences, even for storage at full capacity, provided that the side of the grid through which loads are stored/retrieved is at least 3 cells wide. However, in practice, the retrieval sequence can change after the storage phase. To address such uncertainty, this work investigates \emph{$k$-bounded perturbations} during retrieval, under which any two loads may depart out of order if they are originally at most $k$ positions apart. We prove that a $Θ(k)$ grid width is necessary and sufficient for eliminating relocations at maximum capacity. We also provide an efficient solver for computing a storage arrangement that is robust to such perturbations. To address the higher-uncertainty case where perturbations exceed $k$, a strategy is introduced to effectively minimize relocations. Extensive experiments show that, for $k$ up to half the grid width, the proposed storage-retrieval framework essentially eliminates relocations. For $k$ values up to the full grid width, relocations are reduced by $50\%+$.

</details>


### [131] [iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations](https://arxiv.org/abs/2601.19234)
*Youndo Do,Chad Meece,Marc Zebrowitz,Spencer Banks,Myeongjun Choi,Xiaoxu Diao,Kai Tan,Michael Doran,Jason Reed,Fan Zhang*

Main category: cs.RO

TL;DR: 本文介绍了一个为核设施数字化和先进反应堆开发而打造的虚拟仿真平台iFAN，用于测试和验证包括AI、网络物理安全等前沿技术。


<details>
  <summary>Details</summary>
Motivation: 随着核设施的数字化转型和新一代反应堆的开发，集成AI、网络-物理安全，以及自主机器人等新技术愈发重要，但缺乏成熟的虚拟测试平台成为部署和评估的主要障碍。

Method: 作者开发了iFAN生态系统，一个以物理为基础、具有高保真三维仿真环境的数字孪生框架。它支持虚拟现实、强化学习、辐射仿真以及网络物理安全功能，能够用于核电厂运行、网络安全、物理安全和机器人操作的虚拟测试。

Result: iFAN生态系统实现了实时数据交换和高保真仿真，能够支持不同核设施操作场景下的新兴技术应用与验证。该平台在测试各类操作场景中表现出高度的适用性和安全性。

Conclusion: iFAN生态系统为新一代智能与网络安全核电站的开发、验证和安全性评估提供了一套多功能、可靠的数字仿真环境。

Abstract: As nuclear facilities experience digital transformation and advanced reactor development, AI integration, cyber-physical security, and other emerging technologies such as autonomous robot operations are increasingly developed. However, evaluation and deployment is challenged by the lack of dedicated virtual testbeds. The Immersive Framework for Advanced Nuclear (iFAN) ecosystem is developed, a comprehensive digital twin framework with a realistic 3D environment with physics-based simulations. The iFAN ecosystem serves as a high-fidelity virtual testbed for plant operation, cybersecurity, physical security, and robotic operation, as it provides real-time data exchange for pre-deployment verification. Core features include virtual reality, reinforcement learning, radiation simulation, and cyber-physical security. In addition, the paper investigates various applications through potential operational scenarios. The iFAN ecosystem provides a versatile and secure architecture for validating the next generation of autonomous and cyber-resilient nuclear operations.

</details>


### [132] [Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist](https://arxiv.org/abs/2601.19275)
*Tatsuya Kamijo,Mai Nishimura,Cristian C. Beltran-Hernandez,Nodoka Shibasaki,Masashi Hamaya*

Main category: cs.RO

TL;DR: 论文提出了TaMeSo-bot系统，通过结合软体机器人关节和触觉记忆实现接触丰富任务下的安全与高鲁棒性操作。核心算法MAT^3能有效建模多模态信息，显著提升了复杂插孔任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 触觉记忆对于机器人在不确定环境下安全、精确地进行如钥匙插入等接触性任务至关重要。当前基于触觉的操作系统在灵活适应新场景和任务鲁棒性方面仍有局限。作者希望通过更好地利用历史触觉数据，提高机器人技能力与泛化性。

Method: 提出TaMeSo-bot系统，包括能够安全探索的软腕，和利用历史操作经验进行触觉检索式控制的方案。核心是Masked Tactile Trajectory Transformer（MAT^3）模型，联合建模机器人动作、分布式触觉反馈、力-扭矩及本体信息，通过masked token预测实现无须任务分段的特征自主提取和缺失信息推断。

Result: 在真实机器人上进行了多种插孔任务实验，涵盖不同形状和条件的插销。实验结果显示，MAT^3在所有条件下的成功率均优于基线方法，并能高效适应新类型插销和未见过的环境。

Conclusion: MAT^3驱动的TaMeSo-bot系统能充分利用分布式多模态触觉记忆，显著增强了机器人在复杂接触场景下的鲁棒性与适应性，在现实任务中显示出良好推广能力。

Abstract: Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.

</details>


### [133] [Perception-to-Pursuit: Track-Centric Temporal Reasoning for Open-World Drone Detection and Autonomous Chasing](https://arxiv.org/abs/2601.19318)
*Venkatakrishna Reddy Oruganti*

Main category: cs.RO

TL;DR: 提出了一种面向无人机追击的新框架Perception-to-Pursuit（P2P），不仅提升了无人机轨迹预测的准确性，还极大增强了实际可追击性。


<details>
  <summary>Details</summary>
Motivation: 目前无人机自动追击系统往往只关注检测与轨迹预测准确性，忽略了追击动作的可行性，导致预测出的轨迹在绝大多数情况下难以实现有效拦截。该工作旨在提升预测轨迹的可追击性，使拦截动作变得实际可行。

Method: 提出了P2P（Perception-to-Pursuit）框架，以8维紧凑离散向量表示无人机运动状态（含速度、加速度、尺寸与平滑度），利用12帧因果Transformer模型进行时序推理预测未来轨迹。提出了新的可追击性评价指标ISR（Intercept Success Rate），并应用于实际无人机追踪数据集进行评测。

Result: 在Anti-UAV-RGBT数据集测试中，P2P平均像素位移误差28.12，ISR达到0.597，相比传统追踪基线算法分别提升77%和597倍；同时实现了100%的无人机分类准确率。

Conclusion: 针对无人机追击，P2P框架通过时序运动特征推理，将轨迹预测与可行拦截相结合，实现了更高准确性和可追击性，为无人机拦截等实际应用提供了有力支撑。

Abstract: Autonomous drone pursuit requires not only detecting drones but also predicting their trajectories in a manner that enables kinematically feasible interception. Existing tracking methods optimize for prediction accuracy but ignore pursuit feasibility, resulting in trajectories that are physically impossible to intercept 99.9% of the time. We propose Perception-to-Pursuit (P2P), a track-centric temporal reasoning framework that bridges detection and actionable pursuit planning. Our method represents drone motion as compact 8-dimensional tokens capturing velocity, acceleration, scale, and smoothness, enabling a 12-frame causal transformer to reason about future behavior. We introduce the Intercept Success Rate (ISR) metric to measure pursuit feasibility under realistic interceptor constraints. Evaluated on the Anti-UAV-RGBT dataset with 226 real drone sequences, P2P achieves 28.12 pixel average displacement error and 0.597 ISR, representing a 77% improvement in trajectory prediction and 597x improvement in pursuit feasibility over tracking-only baselines, while maintaining perfect drone classification accuracy (100%). Our work demonstrates that temporal reasoning over motion patterns enables both accurate prediction and actionable pursuit planning.

</details>


### [134] [Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection](https://arxiv.org/abs/2601.19354)
*Ziqian Wang,Chenxi Fang,Zhen Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种自监督框架，提高深度学习自主导航在非结构化环境中的安全性和实时性，通过引入可微硬约束投影层和人工势场，兼顾数据稀缺、计算资源有限等难题，并在多个实验中验证了所提方法的有效性和可部署性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习导航系统面临安全、数据稀缺及硬件资源不足等重大挑战，传统方法延迟高、学习方法缺乏确定性保障，因此亟需一种高效、可部署、具有安全保障的新型解决方案以适应实际无人系统应用。

Method: 1）提出自监督学习框架，利用无需人工标注的全局引导人工势场（G-APF）提供稠密监督信号；2）设计可微分硬约束投影层，通过自适应神经投影方法，将深度网络输出在运行时实时投影到可行解空间，动态约束下保持路径物理可实现性；3）架构在资源受限嵌入式硬件（如Jetson Orin NX）上实现。

Result: 在20000组测试场景中成功率达88.75%；CARLA环境中闭环实验进一步验证了路径的真实物理可执行性；在Jetson Orin NX上推理延迟仅为94ms，实现了实时性和资源受限条件下的高效部署。

Conclusion: 本文框架为将物理约束与神经网络深度模型有效结合开辟了新路径，为机电系统受约束优化等实际场景中的自主系统智能化部署提供了可行的通用方案。

Abstract: Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: https://github.com/wzq-13/SSHC.git.

</details>


### [135] [Teaching Machine Learning Fundamentals with LEGO Robotics](https://arxiv.org/abs/2601.19376)
*Viacheslav Sydora,Guner Dilsad Er,Michael Muehlebach*

Main category: cs.RO

TL;DR: 本论文介绍了“Machine Learning with Bricks”这个基于网页的开源平台，通过无需编程的乐高机器人活动，让12-17岁的学生学习机器学习核心算法，并验证其学习成效显著。


<details>
  <summary>Details</summary>
Motivation: 许多中学生对机器学习和人工智能有兴趣，但因缺乏编程基础难以入门。为让更多年轻学生能早期接触并理解机器学习概念，作者开发了一个无需编程即可动手实验的平台。

Method: 作者设计了网页平台“Machine Learning with Bricks”，结合交互式可视化和乐高机器人，教授KNN、线性回归和Q-learning三种算法。采用数据收集、模型训练和机器人交互的方式，通过两天课程，并采用前后测问卷评估学习效果。

Result: 14名学生参与课程，前后问卷显示在机器学习算法知识理解、AI认知态度、平台易用性和持续学习动力等方面均有显著提升。

Conclusion: 可视化、实物结合的机器学习平台能在不牺牲技术深度的前提下，有效提升青少年对机器学习的理解和兴趣，且平台免费开放，可供广泛使用。

Abstract: This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at https://learning-and-dynamics.github.io/ml-with-bricks/, with video tutorials guiding students through the experiments at https://youtube.com/playlist?list=PLx1grFu4zAcwfKKJZ1Ux4LwRqaePCOA2J.

</details>


### [136] [Judgelight: Trajectory-Level Post-Optimization for Multi-Agent Path Finding via Closed-Subwalk Collapsing](https://arxiv.org/abs/2601.19388)
*Yimin Tang,Sven Koenig,Erdem Bıyık*

Main category: cs.RO

TL;DR: 本文提出了Judgelight方法，作为多智能体路径规划（MAPF）中对已有解的后处理优化工具，通过移除轨迹中的冗余回路有效改善路径质量，并用实验证明优化效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的MAPF求解器能高效产出可行路径，但常包含不必要或来回波动的动作，影响实际应用。作者希望通过后处理优化使这些解更简洁、更适合现实部署。

Method: 提出Judgelight方法，通过分析并折叠轨迹中的闭合子路径（closed subwalk），删去多余动作，同时保证路径的可行性。该退化闭合子路径问题（MAPF-Collapse）被形式化并证明为NP-hard，并给出整数线性规划（ILP）形式的精确优化实现。

Result: 实验证明，Judgelight能让现有MAPF解（尤其是基于学习的解）平均成本降低约20%，路径更加优化，适用性更强。

Conclusion: Judgelight能显著优化MAPF生成的多智能体轨迹，提升路径质量，有利于多机器人系统的实际部署。

Abstract: Multi-Agent Path Finding (MAPF) is an NP-hard problem with applications in warehouse automation and multi-robot coordination. Learning-based MAPF solvers offer fast and scalable planning but often produce feasible trajectories that contain unnecessary or oscillatory movements. We propose Judgelight, a post-optimization method that improves trajectory quality after a MAPF solver generates a feasible schedule. Judgelight collapses closed subwalks in agents' trajectories to remove redundant movements while preserving all feasibility constraints. We formalize this process as MAPF-Collapse, prove that it is NP-hard, and present an exact optimization approach by formulating it as integer linear programming (ILP) problem. Experimental results show Judgelight consistently reduces solution cost by around 20%, particularly for learning-based solvers, producing trajectories that are better suited for real-world deployment.

</details>


### [137] [Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.19406)
*Kaipeng Fang,Weiqing Liang,Yuyang Li,Ji Zhang,Pengpeng Zeng,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本文提出SimHum框架，结合模拟机器人动作和真实人类观察信息，实现高效且泛化能力强的机器人操作策略。


<details>
  <summary>Details</summary>
Motivation: 收集真实机器人数据代价高昂，研究者尝试用合成模拟数据和人类数据作为替代，但前者存在'仿真到现实'视觉差距，后者有人体-机器人结构差异，限制了策略泛化能力。

Method: 提出SimHum联合训练框架：从仿真数据中提取机器人运动学先验，从真实人类数据中获得视觉先验，两者互补，共同促进机器人操作策略学习。

Result: 在相同数据预算下，SimHum方法比基线提升最高达40%；只用80条真实数据时，SimHum在分布外任务成功率达62.5%，是纯真实数据基线的7.1倍。

Conclusion: 通过融合仿真和人类数据的互补优势，SimHum框架显著提高了机器人在现实任务中的数据效率和泛化能力。

Abstract: Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\mathbf{40\%}$ under the same data collection budget, and achieves a $\mathbf{62.5\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\times$. Videos and additional information can be found at \href{https://kaipengfang.github.io/sim-and-human}{project website}.

</details>


### [138] [Task-Centric Policy Optimization from Misaligned Motion Priors](https://arxiv.org/abs/2601.19411)
*Ziang Zheng,Kai Feng,Yi Nie,Shentao Qin*

Main category: cs.RO

TL;DR: 该论文提出了一种新的仿人机器人运动学习框架TCMP，通过更有条件地融合模仿信号，有效提升仿真机器人既自然又完成任务的能力。


<details>
  <summary>Details</summary>
Motivation: 人类演示作为先验常用于提升仿人机器人的动作自然性，但由于身体结构差异等原因，直接模仿常常导致任务表现下降。而仅靠强化学习虽然能完成任务，却动作不自然。如何平衡两者，避免简单混合导致冲突，是当前仿人控制核心问题。

Method: 提出TCMP框架：将模仿作为有条件的正则项，仅在模仿信号与任务目标一致时引入，利用几何信息自适应调整优化方向，理论分析梯度冲突与收敛性，通过在仿人机器人上的实验验证方法有效性。

Result: 在含噪的人类演示上，TCMP在保持运动风格一致的前提下，实现了更为健壮和优越的任务表现，优于传统的线性奖励混合等方法。

Conclusion: TCMP突破了传统模仿学习和任务驱动强化学习在权衡运动自然性与任务完成度上的瓶颈，为仿人机器人自然高效运动提供了新路径。

Abstract: Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.

</details>


### [139] [Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots](https://arxiv.org/abs/2601.19496)
*Jie Gu,Hongrun Gao,Zhihao Xia,Yirun Sun,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: 本论文提出了一种新型模块化机器人自重构规划算法，能保证重构过程中的连接稳定性，并通过实际机器人平台验证了其效率和可行性。


<details>
  <summary>Details</summary>
Motivation: 模块化自重构机器人在结构重构过程中，如果连接不稳定会导致物理实现困难和部署受限，因此亟需一种保证连接稳定性的自重构方法。

Method: 该方法首先利用虚拟图来构建可行的连接/断开动作，再通过依赖反向树（DRTree）组织动作顺序，解决了动作间的依赖关系，并对任意形态（排除线性结构）下七个或以上模块的配置对证明了存在满足运动特性的重构序列。

Result: 与改进版BiRRT算法的对比表明，所提出算法在效率和稳定性方面具有明显优势，并在现实机器人平台上完成部署，验证了其实际可行性。

Conclusion: 本方法有效地提升了模块化自重构机器人在确保连接稳定性下的自重构能力，具备实际应用和推广价值。

Abstract: For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.

</details>


### [140] [Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots](https://arxiv.org/abs/2601.19499)
*Mehdi Heydari Shahna,Seyed Adel Alizadeh Kolagar,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习与形式化安全保障的方法，实现轮式移动机器人在非结构化环境中的高效且有形式保证的目标到达。创新点为引入Lyapunov型稳定器作为策略监督层，显著提升了RL策略的安全性与收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习虽然能学会到达目标的策略，但缺乏对安全到达目标的形式性保证。常采用防护机制实现安全约束，但易造成探索能力和学习效率下降。因此亟需一种既具备形式目标到达保证，又不影响探索的RL控制框架。

Method: 首先设计了包含15项奖励的实时RL控制策略，既鼓励目标达到，又保证平滑动作和安全规范。其次，在RL框架中集成了Lyapunov型稳定器作为策略监督器，强化对目标到达的形式性控制，并保持状态-动作空间的有效探索。

Result: 实验表明，集成Lyapunov型稳定器后，RL策略的目标到达率由84.6%提升至99.0%，显著减少失败，并提高了控制效率。

Conclusion: 所提出方法可实时部署于复杂环境下的移动机器人，兼顾目标到达的形式性保证和控制执行的现实约束，在提升安全性的同时维持学习与探索效率。

Abstract: Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.

</details>


### [141] [A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation](https://arxiv.org/abs/2601.19509)
*Jin Huang,Zichen Liu,Haoda Li,Zhikun Wang,Ying Chen*

Main category: cs.RO

TL;DR: 本文提出了两项互补的新方法，显著抑制了水下组合导航系统长期运行中的导航误差，提高了定位准确性。


<details>
  <summary>Details</summary>
Motivation: 基于SINS/DVL的水下自主导航系统中，惯性姿态误差会通过速度投影造成误差累积，显著影响中长期导航性能。因此需要新方法降低姿态误差对速度观测和位置估计的负面影响。

Method: 1) 构建了一种姿态误差感知的DVL速度变换模型，将姿态估计误差项引入观测方程以减少速度投影偏差；2) 提出基于协方差矩阵的方差传播方法，跨坐标系传播DVL测量不确定性，并引入基于期望的姿态误差补偿，实现一致性噪声建模。

Result: 仿真和实地实验显示，两项改进各自可提升导航精度，且联合应用时能有效抑制长期误差发散。在实验中，3D位置RMSE提升78.3%，最大分量误差降低71.8%。

Conclusion: 所提方法为提升水下组合导航系统的长期精度和鲁棒性提供了有效解决方案，并证实了姿态误差对速度观测及测量不确定性均有影响。

Abstract: In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.

</details>


### [142] [ALRM: Agentic LLM for Robotic Manipulation](https://arxiv.org/abs/2601.19510)
*Vitor Gaboardi dos Santos,Ibrahim Khadraoui,Ibrahim Farhat,Hamza Yous,Samy Teffahi,Hakim Hacid*

Main category: cs.RO

TL;DR: 提出了一种结合大语言模型（LLM）赋能的机器人操作新框架ALRM，同时构建了包含多语言指令的新基准，用于系统性测试多步推理与操作能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在机器人控制领域的应用有限，主要体现在缺乏模块化、反思与行动修正机制，以及现有基准不关注多步推理与语言多样性。

Method: 提出了一种名为ALRM的框架，将策略生成和通过ReAct推理机制实现的代理执行结合，支持直接代码生成（CaP）和基于工具的规划（TaP）。同时提出了一个包含56个任务的新仿真基准进行测试。

Result: 在十大LLM上的实验显示，ALRM支持可扩展、可解释和模块化的机器人语言推理及控制，其中Claude-4.1-Opus和Falcon-H1-7B分别在封闭和开放源码模型中表现最佳。

Conclusion: ALRM为自然语言推理与可靠机器人操作之间架起了桥梁，有效拓展了LLM在实际机器人任务中的应用能力。

Abstract: Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.

</details>


### [143] [PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment](https://arxiv.org/abs/2601.19514)
*Ruiyu Wang,Zheyu Zhuang,Danica Kragic,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 提出了PALM方法，有效提升了图像行为克隆在多种OOD（分布外）场景下的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像行为克隆方法难以应对分布外泛化（比如工作空间变换、视角变化、形态转移），且多数方法只关注单一泛化问题，复杂且实用性有限，需要一种统一且高效的解决方案。

Method: 提出PALM（Perception Alignment for Local Manipulation）方法，将操作策略拆分为局部和全局两部分，通过在局部层面对视觉关注和本体感知进行对齐，实现各类OOD泛化，同时无需引入额外输入、模型改动或新增数据。

Result: 在仿真和真实环境测试中，PALM使泛化性能下降分别控制在8%和24%，而基线方法下降分别为45%和77%，效果大幅优于现有方法。

Conclusion: PALM显著提升了行为克隆在各种场景下的鲁棒性和泛化能力，降低了复杂度，实现简单高效的OOD泛化。

Abstract: Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.

</details>


### [144] [Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion](https://arxiv.org/abs/2601.19529)
*Jie Gu,Yirui Sun,Zhihao Xia,Tin Lun Lam,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: 本文提出了Rhombot，一种新型可变形平面格子模块化自重构机器人，采用菱形模块结构，能够实现高效的形态变化和自重构。


<details>
  <summary>Details</summary>
Motivation: 传统模块化自重构机器人（MSRR）多存在控制复杂、重构过程不连续等缺陷，难以实现稳定多样的形态变化。为此，作者希望设计一种结构简单、控制简便，同时具备高效稳定重构能力的机器人。

Method: 提出了一种以平行四边形为骨架、仅需单一中心驱动器能沿对角线折叠/展开的菱形模块，并创新性地引入“morphpivoting”运动基本单元，制定其连续执行策略，简化系统控制复杂度。

Result: 通过一系列物理实验，验证了Rhombot模块的稳定重构能力，以及其定位、对接的高精度表现。

Conclusion: Rhombot可在控制简单的前提下，实现环境无关的连续稳定重构，展现出良好的形态变化与自重构性能，为模块化机器人设计与应用提供了新思路。

Abstract: In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.

</details>


### [145] [Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation](https://arxiv.org/abs/2601.19536)
*Hongji Liu,Linwei Zheng,Yongjian Li,Mingkai Tang,Xiaoyang Yan,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种基于增强逆透视映射（IPM）的低成本统一矢量化道路测绘框架，实现了对车道线及其它地面标识的高精度自动建图。


<details>
  <summary>Details</summary>
Motivation: 现有IPM方法受制于共面假设，存在定位误差，且往往仅适用于特定地面元素，精度和通用性有限，亟需更高效、通用、精确且低成本的道路矢量化测绘方法。

Method: 采用Catmull-Rom样条曲线描述车道线，用多边形统一表示其他地面标识，并利用实例分割结果对样条和多边形的三维控制点进行优化。同时，联合优化IPM的单应性矩阵和车辆位姿，有效补偿IPM带来的误差。

Result: 该方法在两个实际场景下测试可自动构建厘米级精度的高质量地图；优化后的IPM矩阵达到人工标定级别精度，车辆位姿估计也有显著提升。

Conclusion: 本文框架显著降低了基于IPM道路测绘的误差，提升了通用性和精度，是一种低成本高效的矢量化道路测绘方案。

Abstract: In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.

</details>


### [146] [AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation](https://arxiv.org/abs/2601.19634)
*Wenda Yu,Tianshi Wang,Fengling Li,Jingjing Li,Lei Zhu*

Main category: cs.RO

TL;DR: 本论文提出了一种高效的视-语言-动作模型AC^2-VLA，可根据动作上下文自适应调整计算量，实现计算加速且保持任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有视-语言-动作(VLA)模型虽然在机器人操作任务表现良好，但每个时间步都需全量计算大型视觉-语言主干网络，导致高延迟和高计算成本，限制了其实时、闭环部署。此外，主流的效率提升方法往往忽视了动作上下文的重要性。

Method: 提出AC^2-VLA框架，将当前视觉观测、语言指令与历史状态等动作相关信息作为条件，自适应进行计算。在此基础上，框架统一支持跨时间步的认知重用、token裁剪和组件选择性执行。为训练自适应策略，引入动作引导的自蒸馏机制以保持稠密VLA策略的表现，并推动稀疏化泛化。

Result: 在机器人操作基准测试中，AC^2-VLA在保持类似任务成功率前提下，速度提升达1.79倍，FLOPs降低至稠密基线的29.4%。

Conclusion: AC^2-VLA证明了面向动作上下文的自适应计算能有效减少开销并提升效率，为VLA模型的实际部署提供了新思路。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.

</details>


### [147] [Enhancing Worker Safety in Harbors Using Quadruped Robots](https://arxiv.org/abs/2601.19643)
*Zoe Betta,Davide Corongiu,Carmine Tommaso Recchiuto,Antonio Sgorbissa*

Main category: cs.RO

TL;DR: 本文提出了利用四足机器人对港口关键区域进行初步巡检的新思路。


<details>
  <summary>Details</summary>
Motivation: 港口环境复杂，传统基础设施巡检存在一定安全风险，提高工人安全迫切需要新的智能化巡检方案。

Method: 首先识别港口环境中的关键区域，然后分析应用四足机器人对这些区域进行初步巡检的可行性。

Result: 提出并验证了利用四足机器人开展港口环境巡检的初步方案，证明其具有一定的可行性。

Conclusion: 四足机器人有望在港口基础设施巡检领域发挥积极作用，为后续更深入的研究和实际部署奠定基础。

Abstract: Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.

</details>


### [148] [SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects](https://arxiv.org/abs/2601.19742)
*Ali Jnadi,Hadi Salloum,Yaroslav Kholodov,Alexander Gasnikov,Karam Almaghout*

Main category: cs.RO

TL;DR: SCOPE框架提供了对可变形线性物体(DLOs)的高效建模与操作，同时兼具速度与物理合理性，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统的基于能量的方法在处理可变形线性物体时计算开销较大，难以满足实时响应需求。

Method: SCOPE采用凸近似方法对DLOs进行建模与操作，有效降低了计算成本，同时保持平滑且物理合理的变形效果。

Result: 通过大量仿真实验，SCOPE展现出在几何和长度约束下生成平滑形状轨迹的能力，验证了方法的有效性。

Conclusion: SCOPE在实时或近实时对可变形线性物体建模与操作方面具有显著优势，实现了速度和准确性的良好平衡。

Abstract: We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.

</details>


### [149] [Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications](https://arxiv.org/abs/2601.19761)
*Jin Huang,Fethiye Irmak Doğan,Hatice Gunes*

Main category: cs.RO

TL;DR: 本文提出将推荐系统（RS）的核心技术整合到社交机器人中，以提升个性化水平，克服现有方法在全面捕捉和利用用户偏好方面的不足。


<details>
  <summary>Details</summary>
Motivation: 目前社交机器人主要依赖于大语言模型（LLMs）或强化学习（RL）进行个性化，但这些方法难以全面捕捉用户长期、短期和细粒度的偏好，也难以用这些信息驱动主动的、伦理负责的交互适应。

Method: 作者分析并比对了社交机器人和推荐系统的基础范式，筛选并识别出能够提升个性化的关键推荐技术，并将其设计为模块化、即插即用的组件，便于在社交机器人系统中全面集成。

Result: 构建了可将推荐系统技术无缝集成到社交机器人流程中的框架，并提出具体实现途径。

Conclusion: 此工作为RS和HRI（人机交互）两个领域的深度合作奠定了基础，能够显著加速社交机器人个性化创新，也有助于推动推荐系统技术在新场景（如社交机器人）中的应用。

Abstract: Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.

</details>


### [150] [Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse](https://arxiv.org/abs/2601.19826)
*Fan Yang,Renkai Ma,Yaxin Hu,Lingyao Li*

Main category: cs.RO

TL;DR: 本研究探讨了人们如何基于机器人的拟人化程度和自身的道德基础，对机器人被虐待的现象做出反应。通过对201名参与者观看不同类型机器人被虐待的视频，结合问卷测评，发现人们对机器人是否给予道德关注与拟人化程度有关，同时，道德基础影响他们做出相关判断的方式。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地融入日常生活，理解人们对机器人被虐待的反应，对于伦理和机器人设计具有重要意义。然而，目前尚不清楚机器人外观的拟人化程度以及个体道德基础如何影响人们对此类事件的看法。

Method: 采用混合方法，招募201名参与者，让其观看不同拟人化水平（蜘蛛型、双足型、拟人型）机器人被虐待的视频，并填写道德基础、愤怒程度和社会距离等量表。进行量化分析和定性分析两种方法。

Result: 研究发现：1）机器人越拟人化，人们越可能给予其道德关注；2）个体的道德基础影响其道德判断的推理方式。低进步型个体偏向于性格（本质）判断，高进步型个体更多采用面向未来的道德思考。

Conclusion: 拟人化设计能提升对机器人的道德关注。不同行为者的道德基础需考虑到未来机器人设计与政策沟通，推动更包容的人工智能社会规范。

Abstract: As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.

</details>


### [151] [Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation](https://arxiv.org/abs/2601.19832)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 论文提出了一种利用单个RGB视频演示，通过信息论与场景图分析自动生成双臂机器人任务执行计划的创新方法。


<details>
  <summary>Details</summary>
Motivation: 当前通过演示来编程的方式（Programming by demonstration）让非专业人士也能编程机器人，但双手协作任务因手部协调复杂，很少被涉及且数据采集困难。因此亟需简化双臂任务示教和计划生成的方法。

Method: 提出了一种“一次性”方法：输入仅为一段单RGB视频，通过香农信息论分析场景元素间信息流，结合场景图属性检测手部协调策略。最终将分析结果映射为模块化行为树，用于双臂机器人的协作规划。

Result: 方法在多组自采视频和一个公开数据集上进行了验证。对比现有方法，新方法在生成双臂协作中心化执行计划方面有明显提升。所用视频数据也开源。

Conclusion: 本框架有效解决了双臂协作示教任务中的执行计划自动生成难题，提高了方法的生成质量与实用性，并有望推动非专家用户在复杂机器人任务中的应用。

Abstract: Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.

</details>


### [152] [HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs](https://arxiv.org/abs/2601.19839)
*Jeanne Malécot,Hamed Rahimi,Jeanne Cattoni,Marie Samson,Mouad Abrini,Mahdi Khoramshahi,Maribel Pino,Mohamed Chetouani*

Main category: cs.RO

TL;DR: 该论文提出了HARMONI框架，通过融合多模态和大语言模型，实现社交辅助机器人在多用户场景下的个性化与动态适应。实验显示该框架优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统在多用户长期交互环境下缺乏有效的持续个性化和动态适应机制，影响实际部署和用户体验。

Method: 提出了HARMONI多模态个性化框架，集成了感知、世界建模、用户建模和生成四个模块，结合大语言模型管理和处理多用户长时交互和个性化响应。

Result: 通过四个数据集的评估和真实养老院场景的用户实验，验证了该框架在说话人识别、在线记忆更新和道德对齐个性化上的优越性，其用户建模精度、个性化质量和用户满意度均超越了基线方法。

Conclusion: HARMONI能有效支持社交辅助机器人在多用户环境下实现长期、个性化且动态的人机交互，在实际应用中具备显著优势。

Abstract: Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.

</details>


### [153] [Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability](https://arxiv.org/abs/2601.19856)
*Giulio Campagna,Marta Lagomarsino,Marta Lorenzini,Dimitrios Chrysostomou,Matthias Rehm,Arash Ajoudani*

Main category: cs.RO

TL;DR: 该论文提出了一个基于数据的框架，通过行为指标来评估和预测人机协作过程中的信任水平，提高了信任分类的准确率。


<details>
  <summary>Details</summary>
Motivation: 在Industry 5.0背景下，人机协作日益普遍，信任成为实现高效、安全合作的关键。传统信任评估方法主观性强、实时性差，因此亟需客观、数据驱动的方法来动态监测并促进人机之间的信任。

Method: 作者提出利用行为指标，通过Preference-Based Optimization（偏好优化）算法，根据操作员的反馈生成可增强信任的轨迹，并以反馈为‘真值’训练机器学习模型，用于从行为指标中实时预测信任水平。该方法在化工行业的机器人协助操作员混合化学品场景中进行了测试。

Result: 机器学习模型能够以80%以上的准确率对信任状态进行分类，Voting Classifier模型达到了84.07%的准确率，以及AUC-ROC为0.90，表明模型对不同信任状态区分能力很强。

Conclusion: 数据驱动的信任评估方法在实际人机协作中效果显著，行为指标在预测人类信任动态中具有重要作用，为智能人机协作系统的信任管理提供了可行技术。

Abstract: Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\% accuracy, with the Voting Classifier achieving 84.07\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.

</details>
