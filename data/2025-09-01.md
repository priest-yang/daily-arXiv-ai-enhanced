<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

TL;DR: 本论文介绍了2COOOL研讨会，专注于自动驾驶中未知和异常场景的研究，推动安全自动驾驶发展。


<details>
  <summary>Details</summary>
Motivation: 到目前为止，自动驾驶汽车尚未实现完全安全，主要难点之一在于应对新的、未知的异常场景。解决这些场景对于实现安全部署至关重要。

Method: 会议旨在整合视觉和传感器数据，聚焦于包括异常检测、开放集识别、视觉-语言模型、域适应等技术，从而提升自动驾驶系统对罕见和危险情景的感知和应对能力。

Result: 会议汇聚了学术界与产业界专家，推动了关于异常场景处理的前沿算法及系统的讨论和合作，同时促进了新基准和评测方法的开发。

Conclusion: 通过推动社区关注和创新与异常场景相关的理论和实际技术，2COOOL加速了安全、可靠自动驾驶的发展。

Abstract: As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [2] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: 本研究探索了深度学习方法在全景X光片中自动分类牙科疾病的表现，特别关注卷积神经网络(CNN)、混合模型和预训练模型的比较。通过实验发现，混合CNN与随机森林的模型表现最佳，优于自定义CNN和经典预训练模型。


<details>
  <summary>Details</summary>
Motivation: 牙科全景X光图像中自动、准确诊断多种常见牙科问题（如补牙、蛀牙、种植体和阻生牙）具有临床意义，可以大幅提升工作效率并减少人工误判。深度学习视觉算法有望成为辅助牙科诊断的有效工具。

Method: 作者整理了包含1,512张全景牙科X光图片、4种常见牙病共11,137个专家验证标注的数据集。对图像进行预处理和平衡。实验比较了自定义CNN、CNN特征提取+传统分类器的混合模型，以及迁移学习的预训练架构，并用5折交叉验证和多项评价指标进行分析。

Result: 混合CNN-随机森林模型获得最高准确率85.4%，优于自定义CNN（74.3%）和最佳预训练模型VGG16（82.3%）。混合模型在辨别形态相似病变方面优势显著。

Conclusion: 将CNN特征提取与集成分类器结合，可以实现高效可靠的自动牙科诊断支持。未来需扩展数据集并进一步临床验证。

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [3] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: 作者发现大型图像生成模型在零样本外观迁移任务中存在“Attention Leakage”（注意力泄漏）问题，提出Q-Align方法，通过Query-Query对齐和特征重排，提升了语义对齐和外观转移效果，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 零样本外观迁移是将一个图像的外观无监督地转移到另一个图像，需要精确的空间语义映射。现有基于Query-Key对齐的方法在这一任务中存在注意力泄漏问题，影响语义一致性，因此需针对这一现象提出解决方案。

Method: 提出Q-Align方法，核心包括：1）Query-Query对齐，实现两个图像间复杂的空间语义映射；2）Key-Value重排，提升特征点的一致性；3）利用重排后的key和value进行注意力细化，以维持语义上的一致性。

Result: 通过大量实验和分析，Q-Align在外观保真度上优于最先进的对比方法，同时在结构保持上也表现出较好的竞争力。

Conclusion: Q-Align有效缓解了零样本外观迁移中的注意力泄漏问题，提高了语义对齐和外观转移质量，具有实际应用前景。

Abstract: We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [4] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: 本文针对扩散模型推理阶段运算量大、速度慢的问题，提出一种高效缓存框架ERTACache，以在提升生成速度的同时保持甚至提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因其推理过程需要反复迭代，导致计算开销大，限制其实际应用。现有通过缓存特征来加速的方法会带来较大的画质损失，难以兼顾质量与速度。因此亟需找到一种兼顾推理效率与结果质量的方法。

Method: 作者对特征缓存引入的累计误差进行了理论分析，并将其分解为特征漂移误差和步长放大误差两部分。在此基础上，提出ERTACache：通过离线残差分析寻找可缓存步骤，利用轨迹感知的校正系数动态调整积分区间，并基于闭式线性化模型近似缓存引发的误差，从而合理修正两类误差，实现高频缓存下依然准确的取样推理。

Result: 在多个常用的图像与视频生成测试基准上，ERTACache推理速度可提升2倍，并在视觉质量上保持一致或有所提升。应用于先进的Wan2.1视频扩散模型时，同样实现2倍加速，VBench指标几乎无下降，有效维持基线画质。

Conclusion: ERTACache能在大幅加速扩散模型推理的同时，有效控制因缓存带来的质量损失，具备实际部署的潜力。

Abstract: Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>


### [5] [Video-LLMs with Temporal Visual Screening](https://arxiv.org/abs/2508.21094)
*Zheyu Fan,Jiateng Liu,Yuji Zhang,Zihan Wang,Yi R.,Fung,Manling Li,Heng Ji*

Main category: cs.CV

TL;DR: 提出了一种名为Temporal Visual Screening（TVS）的新任务，用于提升视频大模型对细粒度时序语义的理解，通过聚焦关键视频片段并重构问句，提高视频-语言理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型由于训练时采样帧稀疏且跨帧推理监督不足，难以像人类一样聚焦和理解关键时序信息，导致对复杂视频内容的理解能力有限。

Method: 作者受认知科学启发，提出了TVS任务，对视频问题回答和指令微调数据进行预处理，包括保留关键信息片段、同步重写问句保持一致性，并设计为可模块化插入训练与推理流程的前端适配器。并提出新基准和ReSimplifyIt基线模型。

Result: TVS显著优化了推理负担和认知负载，实验表明在视频修剪任务上优于现有方法0.47 F1分，在训练和推理阶段分别带来7.33%和34.6%的相对性能提升。

Conclusion: 整合TVS能够有效提升视频-语言大模型对时序信息的理解能力，是提升相关应用的有效手段。

Abstract: Humans naturally perform temporal screening by dragging the progress bar and
focusing on salient temporal segments, but current Video Large Language Models
(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse
frame sampling and insufficient inter-frame reasoning supervision during their
training. To address this, Inspired by well-established cognitive science
principles, we propose Temporal Visual Screening (TVS), a new task that
universally pre-processes video question answering and instruction tuning data
by: (1) retaining focus-critical video segments, (2) synchronously
reconstructing queries to their most direct form while preserving answer
consistency, and (3) keeping the invariance and consistency for any possible
answer. TVS is formulated as a modular front-end adapter task that can be
seamlessly integrated into both Video Instruction Tuning (training) and Video
Question Answering (inference) pipelines. TVS optimizes distribution of
reasoning burden and cognitive load; during training, it aligns queries with
focus-critical visual information; at inference, it enables query-aware segment
focus and streamlined query representations. In particular, we curate the first
benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior
approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming
while achieving competitive query rewriting performance. Experiments
demonstrate that incorporating TVS yields relative gains of 7.33% (training)
and 34.6% (inference), demonstrating the effectiveness of temporal information
screening for improving video-language understanding.

</details>


### [6] [ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments](https://arxiv.org/abs/2508.21096)
*Zhe Han,Charlie Budd,Gongyu Zhang,Huanyu Tian,Christos Bergeles,Tom Vercauteren*

Main category: cs.CV

TL;DR: 该论文提出了一种更高效的外科手术工具姿态标注方法，并基于已有数据集发布了包含姿态与实例分割信息的新数据集，用于推动相关技术的发展和应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在手术工具定位中依赖大量多样化的标注数据，但传统分割标注成本高。作者认为骨架姿态标注（skeletal pose annotation）相比工具实例分割标注更易于操作，同时能提供足够丰富的语义信息，因此可以更快扩展标注数据。

Method: 作者基于现有ROBUST-MIS数据集，结合姿态标注与实例分割，创建了新的ROBUST-MIPS数据集，并搭建了一个基准测试，采用流行的姿态估计方法评估标注效果。此外还公开了基准模型和自定义姿态标注软件。

Result: 实验结果表明，使用骨架姿态标注能实现高质量的手术工具定位效果，可作为现有分割方法的有效替代或补充。

Conclusion: 骨架姿态标注在外科手术工具定位任务中具备高效且可推广的优势，联合实例分割和姿态数据有助于推动自动化、智能化手术辅助相关技术发展。新数据集和工具将便利后续研究和实际应用。

Abstract: Localisation of surgical tools constitutes a foundational building block for
computer-assisted interventional technologies. Works in this field typically
focus on training deep learning models to perform segmentation tasks.
Performance of learning-based approaches is limited by the availability of
diverse annotated data. We argue that skeletal pose annotations are a more
efficient annotation approach for surgical tools, striking a balance between
richness of semantic information and ease of annotation, thus allowing for
accelerated growth of available annotated data. To encourage adoption of this
annotation style, we present, ROBUST-MIPS, a combined tool pose and tool
instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our
enriched dataset facilitates the joint study of these two annotation styles and
allow head-to-head comparison on various downstream tasks. To demonstrate the
adequacy of pose annotations for surgical tool localisation, we set up a simple
benchmark using popular pose estimation methods and observe high-quality
results. To ease adoption, together with the dataset, we release our benchmark
models and custom tool pose annotation software.

</details>


### [7] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Safe-Control的即插即用安全补丁，有效遏制了文本生成图像（T2I）模型中不安全内容的生成，在六种不同的T2I模型中显著优于现有的七种主流安全机制。


<details>
  <summary>Details</summary>
Motivation: 当前T2I生成模型易被滥用，造成严重的安全隐患，而现有安全机制要么容易被规避，要么需要大量模型定制，难以灵活高效应用。

Method: 作者提出Safe-Control，一种数据驱动、依赖安全感知条件的补丁式机制，在锁定的T2I模型中注入安全控制信号。其可扩展、可合并，且即插即用，适用于结构相似的其他T2I去噪模型。

Result: 在六种公开T2I模型上，Safe-Control显著降低了不安全内容的生成概率，同时保持正常图片的质量与文本对齐效果。与七种主流安全机制相比，无论应对危险提示还是最新对抗攻击，Safe-Control都大幅降低了安全风险（不安全内容生成概率降至7%，而基线为20%）。

Conclusion: Safe-Control是一种高效、灵活、兼容性强的T2I模型安全补丁，在多模型、多场景下能够更好地抑制风险内容生成，有效提升模型安全性。

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their
potential for misuse or even abuse raises serious safety concerns. Model
developers have made tremendous efforts to introduce safety mechanisms that can
address these concerns in T2I models. However, the existing safety mechanisms,
whether external or internal, either remain susceptible to evasion under
distribution shifts or require extensive model-specific adjustments. To address
these limitations, we introduce Safe-Control, an innovative plug-and-play
safety patch designed to mitigate unsafe content generation in T2I models.
Using data-driven strategies and safety-aware conditions, Safe-Control injects
safety control signals into the locked T2I model, acting as an update in a
patch-like manner. Model developers can also construct various safety patches
to meet the evolving safety requirements, which can be flexibly merged into a
single, unified patch. Its plug-and-play design further ensures adaptability,
making it compatible with other T2I models of similar denoising architecture.
We conduct extensive evaluations on six diverse and public T2I models.
Empirical results highlight that Safe-Control is effective in reducing unsafe
content generation across six diverse T2I models with similar generative
architectures, yet it successfully maintains the quality and text alignment of
benign images. Compared to seven state-of-the-art safety mechanisms, including
both external and internal defenses, Safe-Control significantly outperforms all
baselines in reducing unsafe content generation. For example, it reduces the
probability of unsafe content generation to 7%, compared to approximately 20%
for most baseline methods, under both unsafe prompts and the latest adversarial
attacks.

</details>


### [8] [GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions](https://arxiv.org/abs/2508.21102)
*Kei Katsumata,Yui Iioka,Naoki Hosomi,Teruhisa Misu,Kentaro Yamada,Komei Sugiura*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法GENNAV，用于根据自然语言指令和移动设备前置摄像头图像定位目标区域，尤其在处理边界模糊的stuff-type区域时表现突出，并通过新建立的GRiN-Drive基准和真实汽车实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 已有方法在定位stuff-type（如路面、草地等）目标区域时表现不佳，尤其难以处理目标缺失或多目标的情况，因此需要更优的模型来解决这些问题。

Method: 提出了GENNAV模型，能够预测目标是否存在并为多种stuff-type目标区域生成分割掩码。作者还构建了GRiN-Drive基准数据集，包含无目标、单目标和多目标三类样本，全面评估方法性能。

Result: GENNAV在标准评价指标上优于各类基线方法，并在五个不同城市的4辆车真实环境测试中，展现了较强的零样本迁移能力和鲁棒性。

Conclusion: GENNAV能够有效弥补现有方法在区域定位任务中的短板，适用于实际复杂环境，具有较好的泛化性和应用前景。

Abstract: We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.

</details>


### [9] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为R-4B的多模态大语言模型（MLLMs），该模型可以根据问题复杂度自适应地决定是否进行逐步推理，兼具“思考”和“非思考”两种模式，从而实现更高效的推理流程。实验表明，R-4B性能优越，在推理强度高的任务上以更低的计算成本达到与更大模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在复杂推理问题上效果显著，但对简单问题却存在计算冗余。动机在于解决模型无谓消耗资源，提升在多类型任务上的计算和推理效率。

Method: 提出R-4B模型，通过双模式退火训练方法（bi-mode annealing），使模型既能开启“思考模式”也可选择“非思考模式”，并基于Bi-mode Policy Optimization（BPO）优化，准确判断是否需要激活推理过程。整个流程包括两阶段训练：第一阶段用含有两种模式样本的数据进行训练，第二阶段在改进型GRPO框架下进一步训练，强制模型对每个输入都尝试两种模式。

Result: R-4B在25个具有挑战性的基准测试中均实现了业界最优性能，大多数任务上优于Qwen2.5-VL-7B，并在需要强推理的任务中，以更低计算成本达到与Kimi-VL-A3B-Thinking-2506 (16B)等更大模型相近的表现。

Conclusion: R-4B能有效平衡推理准确性和计算效率，在多模态复杂和简易任务场景下表现优异，有望推动MLLMs在高效推理方向的发展。

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [10] [HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection](https://arxiv.org/abs/2508.21135)
*Harris Song,Tuan-Anh Vu,Sanjith Menon,Sriram Narasimhan,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 该论文提出了一种融合RGB、热成像和深度数据的Mamba-based融合框架，有效提升了遮挡、伪装、光照等复杂环境下的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的RGB图像检测在遮挡、伪装和恶劣光照下性能显著下降，亟需能够联合多模态信息、提升鲁棒性的检测方法。

Method: 提出HiddenObject融合框架，通过Mamba机制对RGB、热成像和深度三种模态数据进行特征提取与统一融合，捕捉互补信号，生成适应复杂场景的通用表达。

Result: 在多个基准数据集上，该方法相较于现有方法取得了SOTA或有竞争力的结果，验证了方法的有效性并揭示了当前单模态或简单融合策略的不足。

Conclusion: Mamba-based多模态融合结构能够在多模态目标检测任务中显著提升性能，特别适用于视觉受限或复杂条件下的检测场景。

Abstract: Detecting hidden or partially concealed objects remains a fundamental
challenge in multimodal environments, where factors like occlusion, camouflage,
and lighting variations significantly hinder performance. Traditional RGB-based
detection methods often fail under such adverse conditions, motivating the need
for more robust, modality-agnostic approaches. In this work, we present
HiddenObject, a fusion framework that integrates RGB, thermal, and depth data
using a Mamba-based fusion mechanism. Our method captures complementary signals
across modalities, enabling enhanced detection of obscured or camouflaged
targets. Specifically, the proposed approach identifies modality-specific
features and fuses them in a unified representation that generalizes well
across challenging scenarios. We validate HiddenObject across multiple
benchmark datasets, demonstrating state-of-the-art or competitive performance
compared to existing methods. These results highlight the efficacy of our
fusion design and expose key limitations in current unimodal and na\"ive fusion
strategies. More broadly, our findings suggest that Mamba-based fusion
architectures can significantly advance the field of multimodal object
detection, especially under visually degraded or complex conditions.

</details>


### [11] [RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration](https://arxiv.org/abs/2508.21154)
*Ao Shen,Xueming Fu,Junfeng Jiang,Qiang Zeng,Ye Tang,Zhengming Chen,Luming Nong,Feng Wang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出RadGS-Reg，一个新颖的脊椎CT/X射线配准框架，通过联合三维Radiative Gaussians重建和三维配准，有效提升配准精度和实时性，超越已有方法。


<details>
  <summary>Details</summary>
Motivation: CT/X射线配准在图像导航中对精度和实时性要求极高，但现有方法易受空间信息丢失和信号噪声等影响，效果有限。

Method: RadGS-Reg包括两大创新模块：一是结合反事实注意力学习（CAL）的RadGS重建模块，能够在噪声很大的X射线下聚焦脊椎区域重建三维形态；二是基于患者的预训练策略，将模型从模拟数据逐步迁移到真实数据，并学习脊椎先验形态信息。最终通过三维/三维配准完成CT/X射线配准。

Result: 在真实临床数据集上的实验显示，RadGS-Reg在三维重建和配准任务上均取得了显著优于现有方法的高精度和高鲁棒性。

Conclusion: RadGS-Reg能有效提升CT/X射线配准的准确性和稳定性，尤其适用于脊椎等结构的导航手术场景，对实际医学图像配准具有实际价值。

Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation
remains challenging because of its stringent requirements for high accuracy and
real-time performance. Traditional "render and compare" methods, relying on
iterative projection and comparison, suffer from spatial information loss and
domain gap. 3D reconstruction from biplanar X-rays supplements spatial and
shape information for 2D/3D registration, but current methods are limited by
dense-view requirements and struggles with noisy X-rays. To address these
limitations, we introduce RadGS-Reg, a novel framework for vertebral-level
CT/X-ray registration through joint 3D Radiative Gaussians (RadGS)
reconstruction and 3D/3D registration. Specifically, our biplanar X-rays
vertebral RadGS reconstruction module explores learning-based RadGS
reconstruction method with a Counterfactual Attention Learning (CAL) mechanism,
focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific
pre-training strategy progressively adapts the RadGS-Reg from simulated to real
data while simultaneously learning vertebral shape prior knowledge. Experiments
on in-house datasets demonstrate the state-of-the-art performance for both
tasks, surpassing existing methods. The code is available at:
https://github.com/shenao1995/RadGS_Reg.

</details>


### [12] [SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4](https://arxiv.org/abs/2508.21169)
*Kevin Mayer,Alex Vesel,Xinyi Zhao,Martin Fischer*

Main category: cs.CV

TL;DR: 论文介绍了SYNBUILD-3D，这是一个包含620万余个合成3D住宅建筑的大型、多样化、多模态数据集，旨在推动3D建筑模型自动生成相关研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模公开标注数据集，自动生成高精度且语义丰富的3D建筑模型仍为难题。作者受到合成数据在计算机视觉领域成功应用的启发，提出该数据集以促进相关AI算法发展。

Method: SYNBUILD-3D数据集包含三种模态：1) 语义丰富的LoD 4级别3D线框图，2) 对应的建筑平面图像，3) 类似激光雷达的屋顶点云。每个建筑的语义标注源自平面图像，涵盖房间、门、窗等信息。

Result: 该数据集覆盖620万余个LoD 4住宅建筑，且具备丰富的语义和多模态信息，可对齐实际AI研究需求。数据与代码也已对外公开。

Conclusion: SYNBUILD-3D为自动化3D建筑建模提供了强有力的数据基础，将促进新型生成式AI算法的发展，推动建筑建模领域实现更高的语义-几何一致性。

Abstract: 3D building models are critical for applications in architecture, energy
simulation, and navigation. Yet, generating accurate and semantically rich 3D
buildings automatically remains a major challenge due to the lack of
large-scale annotated datasets in the public domain. Inspired by the success of
synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,
and multi-modal dataset of over 6.2 million synthetic 3D residential buildings
at Level of Detail (LoD) 4. In the dataset, each building is represented
through three distinct modalities: a semantically enriched 3D wireframe graph
at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a
LiDAR-like roof point cloud (Modality III). The semantic annotations for each
building wireframe are derived from the corresponding floor plan images and
include information on rooms, doors, and windows. Through its tri-modal nature,
future work can use SYNBUILD-3D to develop novel generative AI algorithms that
automate the creation of 3D building models at LoD 4, subject to predefined
floor plan layouts and roof geometries, while enforcing semantic-geometric
consistency. Dataset and code samples are publicly available at
https://github.com/kdmayer/SYNBUILD-3D.

</details>


### [13] [Radially Distorted Homographies, Revisited](https://arxiv.org/abs/2508.21190)
*Mårten Wadenbäck,Marcus Valtonen Örnhag,Johan Edstedt*

Main category: cs.CV

TL;DR: 本文提出了一种统一方法，可以同时处理三种径向畸变配置下的单应性矩阵（homography）估计问题，并提出了快速稳定的极小求解器。相比现有方法更快，在主流基准上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 在实际计算机视觉任务中，受镜头径向畸变影响，单应性矩阵估计十分困难，现有方法对畸变的不同情况往往分开处理，效率与通用性不足。

Method: 作者提出一个统一框架，能够支持三种径向畸变配置：（1）仅一幅图像存在畸变，（2）两幅图像畸变相同，（3）两幅图像畸变独立。基于此框架，构建了新的极小求解器，提升了求解速度和稳定性。

Result: 新方法在三种情形下的求解速度超过业内现有主流极小求解器，精度相当。通过常用基准数据集（含鱼眼镜头拍摄图片）进行了评测验证。

Conclusion: 统一方法简化了流程，并显著提升了求解效率，适用于含径向畸变的实际单应性估计任务，具有较强工程应用价值。

Abstract: Homographies are among the most prevalent transformations occurring in
geometric computer vision and projective geometry, and homography estimation is
consequently a crucial step in a wide assortment of computer vision tasks. When
working with real images, which are often afflicted with geometric distortions
caused by the camera lens, it may be necessary to determine both the homography
and the lens distortion-particularly the radial component, called radial
distortion-simultaneously to obtain anything resembling useful estimates. When
considering a homography with radial distortion between two images, there are
three conceptually distinct configurations for the radial distortion; (i)
distortion in only one image, (ii) identical distortion in the two images, and
(iii) independent distortion in the two images. While these cases have been
addressed separately in the past, the present paper provides a novel and
unified approach to solve all three cases. We demonstrate how the proposed
approach can be used to construct new fast, stable, and accurate minimal
solvers for radially distorted homographies. In all three cases, our proposed
solvers are faster than the existing state-of-the-art solvers while maintaining
similar accuracy. The solvers are tested on well-established benchmarks
including images taken with fisheye cameras. The source code for our solvers
will be made available in the event our paper is accepted for publication.

</details>


### [14] [GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](https://arxiv.org/abs/2508.21197)
*Zhenghao He,Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为GCAV（Global Concept Activation Vector）的新方法，通过对比学习和注意力机制，将深度神经网络不同层的概念向量统一为语义一致的表示，从而提高解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的CAV（Concept Activation Vectors）能解释神经网络对人类定义概念的敏感性，但在网络不同层计算得到的CAV容易出现不一致，导致跨层比较不可靠。该问题限制了CAV方法对神经网络整体概念表达能力的分析。

Method: 提出GCAV框架：1）用对比学习将不同层的同一概念表征对齐；2）用注意力融合机制将跨层的CAV整合为全局一致的GCAV；3）开发TGCAV方法，配合GCAV应用于模型评估。

Result: GCAV大幅降低了TCAV分数的方差，提升了概念相关性的稳定性和解释性。实验验证GCAV在多种神经网络上都能缓解层间概念不一致、提升概念定位能力，并增强对于对抗扰动的鲁棒性。

Conclusion: GCAV通过整合跨层信息，为深度学习模型的概念编码提供了更全面、一致、可解释的理解，有助于推进模型可解释性研究。源码已公开。

Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for
interpreting deep neural networks by quantifying their sensitivity to
human-defined concepts. However, when computed independently at different
layers, CAVs often exhibit inconsistencies, making cross-layer comparisons
unreliable. To address this issue, we propose the Global Concept Activation
Vector (GCAV), a novel framework that unifies CAVs into a single, semantically
consistent representation. Our method leverages contrastive learning to align
concept representations across layers and employs an attention-based fusion
mechanism to construct a globally integrated CAV. By doing so, our method
significantly reduces the variance in TCAV scores while preserving concept
relevance, ensuring more stable and reliable concept attributions. To evaluate
the effectiveness of GCAV, we introduce Testing with Global Concept Activation
Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We
conduct extensive experiments on multiple deep neural networks, demonstrating
that our method effectively mitigates concept inconsistency across layers,
enhances concept localization, and improves robustness against adversarial
perturbations. By integrating cross-layer information into a coherent
framework, our method offers a more comprehensive and interpretable
understanding of how deep learning models encode human-defined concepts. Code
and models are available at https://github.com/Zhenghao-He/GCAV.

</details>


### [15] [Generalizable Object Re-Identification via Visual In-Context Prompting](https://arxiv.org/abs/2508.21222)
*Zhizhong Huang,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉对象重识别（ReID）方法VICP，无需针对每类对象重新训练模型，而是通过视觉上下文提示，实现对新类别的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有ReID方法通常需要为每种对象（如行人、车辆）训练特定模型，泛化能力差，且新类别需大量标注数据。自监督学习虽减少注释需求，但缺乏身份敏感特征的捕获。

Method: 提出VICP框架，利用大语言模型（LLM）和视觉基础模型（VFM）协同工作。通过少量正负样本对，LLM生成身份语义规则，作为动态视觉提示引导VFM（如DINO）提取ID区分特征，无需对模型参数适配。

Result: 在新引入的ShopID10K数据集和多个ReID基准测试上，VICP在未见类别上明显优于现有方法，实现了更好的泛化能力。

Conclusion: VICP无需数据集特定的再训练，通过视觉上下文提示即可实现对新类别的泛化，提升了ReID的实用性和易用性。

Abstract: Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.

</details>


### [16] [Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg](https://arxiv.org/abs/2508.21227)
*Keshav Jha,William Sharp,Dominic LaBella*

Main category: cs.CV

TL;DR: 本文在2025 PANTHER挑战赛中，利用SegResNet模型，对两组MRI胰腺肿瘤分割任务进行了训练和评估，分别针对91例T1增强MRI和50例T2 MR-Linac，结果显示模型性能有限，表现受数据规模和MRI序列差异影响明显。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤的准确分割对于诊断、治疗计划和疗效评估至关重要，而现有的自动分割方法受限于解剖结构的多样性和数据集规模不足。本文旨在探索深度学习自动分割在不同MRI序列及小样本数据条件下的可行性与挑战。

Method: 采用了Auto3DSeg框架下的SegResNet模型，对任务1（T1增强MRI:91例）和任务2（T2 MR-Linac:50例）进行5折交叉验证训练，并基于STAPLE集成模型，聚焦于解剖相关的ROI区域。分割结果采用DSC、5mm DSC、HD95、MASD及RMSE等多项指标评估。

Result: 在任务1上，模型DSC为0.56，任务2性能下降，DSC仅为0.33。其他评估指标如HD95、MASD、RMSE等也显示任务2更难。整体上，MRI序列差异及样本量小对分割效果影响较大。

Conclusion: 尽管分割性能有限，但结果显示自动胰腺肿瘤分割具有潜力。要提升模型鲁棒性和临床实用性，亟需更大规模、标准化的MRI数据集支持。

Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis,
treatment planning, and outcome assessment, yet automated segmentation remains
challenging due to anatomical variability and limited dataset availability. In
this study, SegResNet models, as part of the Auto3DSeg architecture, were
trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as
part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold
cross-validation with STAPLE ensembling after focusing on an anatomically
relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic
MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI
with expert annotated pancreas and tumor labels. The Pancreatic Tumor
Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases
with expert annotated pancreas and tumor labels. Algorithm-automated
segmentation performance of pancreatic tumor was assessed using Dice Similarity
Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean
Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,
the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD
of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC
of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203
mm. These findings illustrate the challenges of MRI-based pancreatic tumor
segmentation with small datasets, highlighting variability introduced by
different MRI sequences. Despite modest performance, the results demonstrate
potential for automated delineation and emphasize the need for larger,
standardized MRI datasets to improve model robustness and clinical utility.

</details>


### [17] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: 本论文提出了一种称为Reverse Imaging的物理驱动方法，通过反向推断心脏MRI图像下的自旋属性，利用生成扩散模型实现跨成像协议的泛化与数据增强。实验表明，该方法显著提升了心脏MRI分割在不同图像对比度和协议下的准确性，实现了高度泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的心脏MRI分割预训练模型在面对不同成像协议和图像对比度时泛化能力较差，主要原因是成像协议变化带来了显著的图像对比变化，而现有方法难以适应这种差异。尽管成像协议不同，但所有MRI图像的基础物理自旋属性（如质子密度、T1、T2值）是一致的。作者希望利用这一医学物理基础，突破图像对比变化带来的域泛化障碍。

Method: 作者提出Reverse Imaging方法，通过解反问题推断MR图像的底层自旋属性（质子密度、T1、T2），该过程由自旋属性的先验（通过对mSASHA数据集上的扩散生成模型训练得到）进行正则化。推断得到的自旋参数可以作为“潜变量”驱动生成任意成像协议的新图像，实现数据增强和域适应。

Result: 实验表明，Reverse Imaging方法显著提升了心脏MRI自动分割模型在不同对比度、不同成像协议下的准确性和泛化能力。该方法能有效地实现跨序列、跨协议的心脏MRI图像分割。

Conclusion: Reverse Imaging方法以医学物理为基础，通过潜在自旋属性建模，实现了不同成像对比和协议下心脏MRI分割的广谱泛化，有望成为心脏MRI领域中分割模型泛化的重要解决方案。

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)
struggle to generalize across different imaging sequences due to significant
variations in image contrast. These variations arise from changes in imaging
protocols, yet the same fundamental spin properties, including proton density,
T1, and T2 values, govern all acquired images. With this core principle, we
introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data
augmentation and domain adaptation to fundamentally solve the generalization
problem. Our method reversely infers the underlying spin properties from
observed cardiac MRI images, by solving ill-posed nonlinear inverse problems
regularized by the prior distribution of spin properties. We acquire this "spin
prior" by learning a generative diffusion model from the multiparametric
SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which
offers joint cardiac T1 and T2 maps. Our method enables approximate but
meaningful spin-property estimates from MR images, which provide an
interpretable "latent variable" that lead to highly flexible image synthesis of
arbitrary novel sequences. We show that Reverse Imaging enables highly accurate
segmentation across vastly different image contrasts and imaging protocols,
realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [18] [PHD: Personalized 3D Human Body Fitting with Point Diffusion](https://arxiv.org/abs/2508.21257)
*Hsuan-I Ho,Chen Guo,Po-Chen Wu,Ivan Shugurov,Chengcheng Tang,Abhay Mittal,Sizhe An,Manuel Kaufmann,Linguang Zhang*

Main category: cs.CV

TL;DR: PHD提出了一种结合个体体型信息的3D人体重建新方法，提高了个性化人体姿态估计的准确性。方法通过先校准用户体型，再根据体型进行个性化姿态拟合，利用Point Diffusion Transformer作为3D姿态先验，提升了绝对姿态精度，只需使用合成数据训练，并可无缝集成到现有估计算法中增强表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体重建方法通常不考虑用户个体差异，主要针对通用性进行优化，因此在拟合过程中过度依赖2D信息，忽略了3D真实体型和姿态的合理性，影响了3D姿态估计的准确度。

Method: PHD方法先校准用户特定的体型信息，然后进行个性化姿态拟合。其核心为基于体型的3D姿态先验（以Point Diffusion Transformer实现），通过Point Distillation Sampling loss逐步优化3D姿态结果，减小因2D约束带来的误差。

Result: PHD显著提升了盆骨对齐和绝对姿态精度。即便只用合成数据训练，也能达到高效准确的姿态恢复效果，并且该模块可直接集成到各类3D姿态估计器中，带来性能提升。

Conclusion: PHD方法兼顾了个体体型和3D姿态合理性，有效改善了3D人体重建的准确性和实用性，并具备较好的数据效率和模块集成能力。

Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery
(HMR) and body fitting that leverages user-specific shape information to
improve pose estimation accuracy from videos. Traditional HMR methods are
designed to be user-agnostic and optimized for generalization. While these
methods often refine poses using constraints derived from the 2D image to
improve alignment, this process compromises 3D accuracy by failing to jointly
account for person-specific body shapes and the plausibility of 3D poses. In
contrast, our pipeline decouples this process by first calibrating the user's
body shape and then employing a personalized pose fitting process conditioned
on that shape. To achieve this, we develop a body shape-conditioned 3D pose
prior, implemented as a Point Diffusion Transformer, which iteratively guides
the pose fitting via a Point Distillation Sampling loss. This learned 3D pose
prior effectively mitigates errors arising from an over-reliance on 2D
constraints. Consequently, our approach improves not only pelvis-aligned pose
accuracy but also absolute pose accuracy -- an important metric often
overlooked by prior work. Furthermore, our method is highly data-efficient,
requiring only synthetic data for training, and serves as a versatile
plug-and-play module that can be seamlessly integrated with existing 3D pose
estimators to enhance their performance. Project page:
https://phd-pose.github.io/

</details>


### [19] [Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning](https://arxiv.org/abs/2508.21363)
*Yuquan Bi,Hongsong Wang,Xinli Shi,Zhipeng Gui,Jie Gui,Yuan Yan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的基于扩散模型的3D人体姿态估计算法，通过三级时序剪枝策略，大幅减少计算量并提升推理速度，同时保持甚至提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在3D人体姿态生成方面表现出色，但其多次迭代和多假设需求导致计算开销大，实际应用受限。因此，作者希望降低扩散模型的计算负担，提高其在姿态估计任务上的效率。

Method: 提出三级时序剪枝（Hierarchical Temporal Pruning, HTP）策略，包括：（1）TCEP模块，通过自适应时序图来分析帧间关系，筛选重要帧；（2）SFT MHSA，利用帧层面的稀疏性，减少注意力计算；（3）MGPTP，通过聚类进行精细化语义剪枝，只保留信息最丰富的姿态token。整个流程自顶向下进行。

Result: 在Human3.6M和MPI-INF-3DHP数据集上的实验结果显示，所提方法相比前沿扩散法训练MACs降低38.5%、推理MACs降低56.8%、平均推理速度提升81.1%，且精度达到SOTA水平。

Conclusion: 本文的方法显著提升了扩散模型3D人体姿态估计的效率和实用性，既降低了计算资源消耗又保持了精度，证明了分级剪枝策略的有效性。

Abstract: Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.

</details>


### [20] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: 本论文提出了Print2Volume框架，可以利用2D指纹图像合成高质量OCT风格的3D指纹数据，有效缓解OCT指纹数据匮乏问题，极大提升了深度学习识别模型性能。


<details>
  <summary>Details</summary>
Motivation: OCT指纹数据因获取成本高且费时，导致公开大数据集稀缺，严重制约了数据驱动算法尤其是深度学习方法的发展。

Method: 方法分三步：1）2D风格迁移模块将二值指纹图像转换成模仿OCT投影风格的灰度图；2）3D结构扩展网络将该2D图像推断成合理的3D解剖体积；3）通过3D GAN驱动的OCT真实感修饰器，进一步赋予真实OCT成像的纹理和噪声等特征。最终批量生成了42万张OCT风格的3D合成指纹样本。

Result: 大量合成数据质量高，通过预训练识别模型后显著提升识别准确率。在ZJUT-EIFD基准集上，模型预训练后EER由15.62%降至2.50%。

Conclusion: 利用Print2Volume解决了OCT指纹数据稀缺难题，为深度学习模型创造了丰富数据环境，大幅提升了指纹识别系统的性能和实用性。

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of
high-resolution, three-dimensional fingerprint data, capturing rich subsurface
structures for robust biometric recognition. However, the high cost and
time-consuming nature of OCT data acquisition have led to a scarcity of
large-scale public datasets, significantly hindering the development of
advanced algorithms, particularly data-hungry deep learning models. To address
this critical bottleneck, this paper introduces Print2Volume, a novel framework
for generating realistic, synthetic OCT-based 3D fingerprints from 2D
fingerprint image. Our framework operates in three sequential stages: (1) a 2D
style transfer module that converts a binary fingerprint into a grayscale
images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D
Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D
anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that
renders the structural volume with authentic textures, speckle noise, and other
imaging characteristics. Using Print2Volume, we generated a large-scale
synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the
high quality of our synthetic data and its significant impact on recognition
performance. By pre-training a recognition model on our synthetic data and
fine-tuning it on a small real-world dataset, we achieved a remarkable
reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD
benchmark, proving the effectiveness of our approach in overcoming data
scarcity.

</details>


### [21] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: 该论文介绍了妇科腹腔镜手术影像数据集GLENDA，用于支持内异症相关的医学图像分析和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 手术影像录制给术后分析、治疗规划等提供便利，但人工分析录像费时费力。目前缺乏公开的、可用于计算机视觉与机器学习研究的标注医学腹腔镜图像数据，尤其是内异症（子宫内膜异位症）相关的。

Method: 研究团队与医学专家合作，采集和整理了妇科腹腔镜手术中的影像数据，并对其中的内异症区域进行了区域级别的标注，最终形成GLENDA数据集。

Result: 发布了首个内异症区域标注的妇科腹腔镜影像数据集（GLENDA），提供了重要的基础数据资源。

Conclusion: GLENDA数据集的发布将推动医学计算机视觉和机器学习方法在妇科腹腔镜手术，特别是内异症识别与分析领域的研发进展。

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [22] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: 本文针对腹腔镜妇科手术视频中的手术器械，利用区域型全卷积网络，实现了器械的分割与识别。实验证明方法对于器械分割有较高准确率，但器械具体类别的识别仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 手术录像日益成为医学内镜领域重要信息源，然而视频内容特殊，自动内容检索面临挑战。解决视频中手术器械的自动分割和识别有助于医学视频归档和内容检索。

Method: 采用区域型全卷积神经网络（fully convolutional network），对腹腔镜妇科手术视频中的手术器械进行实例级分割和识别。一部分专注于器械与背景的二值分割，另一部分进行器械多类别识别。

Result: 在训练样本数量适中的条件下，方法能够准确定位和分割手术器械区域。但由于手术器械本身形状和结构高度相似，具体类别识别的准确性仍有待提升。

Conclusion: 基于深度学习的分割方法对于器械定位和分割效果良好，但器械类别识别存在较大难度，表明未来需要更强的算法或辅助信息以提升多类别识别性能。

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [23] [SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing](https://arxiv.org/abs/2508.21402)
*Jakub Straka,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文提出了SatDINO，一种专门针对遥感影像自监督表示学习的模型，在多个数据集和测试下，优于主流的MAE方法。


<details>
  <summary>Details</summary>
Motivation: 遥感领域拥有大量未标注数据，如何高效利用这些数据进行特征学习，是提升任务表现的重要研究方向。

Method: 采用DINO（一种对比自监督方法）进行遥感影像的预训练，并在此基础上提出SatDINO，结合了自创新的GSD编码方式和自适应视角采样。

Result: SatDINO在多个遥感数据集和不同测试设置下，其性能超过了基于MAE的现有方法，在多项基准任务上取得了有竞争力的成果。同时，经过消融实验验证了模型各组件的有效性。

Conclusion: SatDINO在遥感自监督领域表现优越，且提出的新方法可独立应用于相关模型，为后续工作提供了有价值的技术与代码资源。

Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing,
where large amounts of unlabeled data are available. In this work, we
investigate the use of DINO, a contrastive self-supervised method, for
pretraining on remote sensing imagery. We introduce SatDINO, a model tailored
for representation learning in satellite imagery. Through extensive experiments
on multiple datasets in multiple testing setups, we demonstrate that SatDINO
outperforms other state-of-the-art methods based on much more common masked
autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual
components. Finally, we propose a few novel enhancements, such as a new way to
incorporate ground sample distance (GSD) encoding and adaptive view sampling.
These enhancements can be used independently on our SatDINO model. Our code and
trained models are available at: https://github.com/strakaj/SatDINO.

</details>


### [24] [Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives](https://arxiv.org/abs/2508.21418)
*Gernot Fiala,Markus Plass,Robert Harb,Peter Regitnig,Kristijan Skok,Wael Al Zoughbi,Carmen Zerner,Paul Torke,Michaela Kargl,Heimo Müller,Tomas Brazdil,Matej Gallo,Jaroslav Kubín,Roman Stoklasa,Rudolf Nenutil,Norman Zerbe,Andreas Holzinger,Petr Holub*

Main category: cs.CV

TL;DR: 论文提出了一种生成和标准化全扫描切片图像（WSI）的二维索引与内容描述框架，以自动化和细致地对WSI内容进行标注和分类，提升AI算法数据集准备的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏WSI内容标准化的元数据描述方法，导致大规模WSI集合在AI算法开发时难以高效检索和利用，主要依赖手动检视，严重制约了自动化分析和大数据挖掘。

Method: 提出了一个通用的二维索引地图生成框架及面向具体领域的细化机制，并以临床病理学为例，定义三层组织映射：来源、组织类型、病理改变，每层对WSI分区赋予标准类别，实现多目录和系统的互操作。

Result: 该方法可为每个WSI集合自动生成细粒度的内容地图和多层标准化注释，极大提升了数据管理、检索和机器学习建模的效率。通过典型示例展示了在目录、机器学习和基于图的WSI表示中的优势与应用前景。

Conclusion: 所提出的WSI内容索引与标准化框架为AI和相关领域提供了强有力的工具支持，有助于推进不同平台和系统间的互操作和数据共享，适用于大规模WSI数据的高效利用与自动化处理。

Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by
scanning an entire glass slide containing a biological specimen, such as tissue
sections or cell samples, at multiple magnifications. These images can be
viewed, analyzed, shared digitally, and are used today for Artificial
Intelligence (AI) algorithm development. WSIs are used in a variety of fields,
including pathology for diagnosing diseases and oncology for cancer research.
They are also utilized in neurology, veterinary medicine, hematology,
microbiology, dermatology, pharmacology, toxicology, immunology, and forensic
science.
  When assembling cohorts for the training or validation of an AI algorithm, it
is essential to know what is present on such a WSI. However, there is currently
no standard for this metadata, so such selection has mainly been done through
manual inspection, which is not suitable for large collections with several
million objects.
  We propose a general framework to generate a 2D index map for WSI and a
profiling mechanism for specific application domains. We demonstrate this
approach in the field of clinical pathology, using common syntax and semantics
to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that
provides fine-grained information about the WSI content. The tissue map is
organized into three layers: source, tissue type, and pathological alterations,
with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard
through specific examples in WSI catalogs, Machine Learning (ML), and
graph-based WSI representations.

</details>


### [25] [Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels](https://arxiv.org/abs/2508.21424)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督增量学习方法，用置信度伪标签替代人工标注，实现在无标注数据集上增量学习，并取得与主流有监督方法接近甚至超越的性能。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，经常会遇到新的类别数据，但大多数增量学习方法假设新增数据已经全部标注，这在实际应用中难以实现。因此，需要一种无需人工标注的新类别增量学习方法。

Method: 作者提出了基于置信度伪标签的无监督增量学习方法（ICPL），利用模型生成的伪标签替换人工标注，通过置信度筛选高质量伪标签，并整合到各类增量学习方法中，适用于无标注的增量数据场景。

Result: 在CIFAR100、ImageNet100以及细粒度数据集上的实验表明，ICPL与有监督方法相比取得了有竞争力的结果，并且在最终准确率上比当前主流class-iNCD方法高5%以上。同时还验证了该方法在资源受限环境下的计算效率。

Conclusion: ICPL方法无需人工标注数据，能够有效进行类别增量学习，兼具准确率高和计算效率高，适用于实际应用场景，有望推动无监督增量学习的发展。

Abstract: Deep learning models have achieved state-of-the-art performance in many
computer vision tasks. However, in real-world scenarios, novel classes that
were unseen during training often emerge, requiring models to acquire new
knowledge incrementally. Class-Incremental Learning (CIL) methods enable a
model to learn novel classes while retaining knowledge of previous classes.
However, these methods make the strong assumption that the incremental dataset
is fully labeled, which is unrealistic in practice. In this work, we propose an
unsupervised Incremental Learning method using Confidence-based Pseudo-labels
(ICPL), which replaces human annotations with pseudo-labels, enabling
incremental learning from unlabeled datasets. We integrate these pseudo-labels
into various CIL methods with confidence-based selection and evaluate
performance degradation on CIFAR100 and ImageNet100. Then, we compare our
approach to popular Class Incremental Novel Category Discovery (class-iNCD)
methods addressing similar challenges. Additionally, we apply our method to
fine-grained datasets to demonstrate its real-world practicality and measure
its computational complexity to validate its suitability for
resource-constrained environments. ICPL achieves competitive results compared
to supervised methods and outperforms state-of-the-art class-iNCD methods by
more than 5% in final accuracy.

</details>


### [26] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: 本论文提出了一种基于生成扩散模型的3D高斯点渲染方法，可根据单张图像重建完整场景（包括不可见部分），并克服传统方法的模糊和不合理问题。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点渲染方法依赖稠密采样，难以恢复遮挡和未观测区域，且传统回归式方法只能预测单一解，导致重建效果模糊且缺乏多样性。该工作旨在解决单视图下的3D完整场景高质量重建问题。

Method: 通过条件生成扩散模型，学习输入单张图像下对应3D高斯点分布。为克服缺乏真实3D标注的训练数据，设计变分自动重建器（Variational AutoReconstructor），利用2D图像自监督学习潜在空间，然后在该空间上训练扩散模型。

Result: 方法能够针对单张图像生成可信且多样化的3D重建结果，能弥补遮挡区域，实现高质量的360°全景渲染。

Conclusion: 基于生成扩散模型的方法有效提升了单视图3D重建的质量与多样性，尤其在遮挡和不可见部分的补全方面显著优于现有方法。

Abstract: Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [27] [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](https://arxiv.org/abs/2508.21435)
*Francisco Caetano,Christiaan Viviers,Peter H. H. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨域X光影像翻译方法MedShift，并发布了X-DigiSkull数据集，实现了合成与真实医学X光影像之间的高质量、灵活转换。


<details>
  <summary>Details</summary>
Motivation: 合成医学影像用于模型训练具有可扩展性，但合成影像与真实临床影像在衰减、噪声、软组织等方面存在明显域差，严重影响模型泛化能力。破解合成与真实之间的域迁移问题对于提升医学AI模型落地意义重大。

Method: 提出MedShift方法，这是一种基于Flow Matching与薛定谔桥（Schrodinger Bridges）的统一类条件生成模型，能够无需配对数据并以统一潜在空间实现多域间高保真影像翻译。引入新数据集X-DigiSkull，含不同剂量下配准的合成及真实颅骨X光数据，并作为跨域翻译基准。

Result: MedShift相比扩散模型具备更小模型规模，但在合成与真实X光影像翻译任务中却取得了强性能。模型还能在推理阶段兼顾感知保真度和结构一致性，展现出极强的灵活性。

Conclusion: MedShift为医学影像领域提供了一个可扩展、泛化性强的跨域适应解决方案，无需领域专属训练或数据配对，对实际临床应用和研究具有推广潜力。

Abstract: Synthetic medical data offers a scalable solution for training robust models,
but significant domain gaps limit its generalizability to real-world clinical
settings. This paper addresses the challenge of cross-domain translation
between synthetic and real X-ray images of the head, focusing on bridging
discrepancies in attenuation behavior, noise characteristics, and soft tissue
representation. We propose MedShift, a unified class-conditional generative
model based on Flow Matching and Schrodinger Bridges, which enables
high-fidelity, unpaired image translation across multiple domains. Unlike prior
approaches that require domain-specific training or rely on paired data,
MedShift learns a shared domain-agnostic latent space and supports seamless
translation between any pair of domains seen during training. We introduce
X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays
under varying radiation doses, to benchmark domain translation models.
Experimental results demonstrate that, despite its smaller model size compared
to diffusion-based approaches, MedShift offers strong performance and remains
flexible at inference time, as it can be tuned to prioritize either perceptual
fidelity or structural consistency, making it a scalable and generalizable
solution for domain adaptation in medical imaging. The code and dataset are
available at https://caetas.github.io/medshift.html

</details>


### [28] [Trees as Gaussians: Large-Scale Individual Tree Mapping](https://arxiv.org/abs/2508.21437)
*Dimitri Gominski,Martin Brandt,Xiaoye Tong,Siyu Liu,Maurice Mugabowindekwe,Sizhuo Li,Florian Reiner,Andrew Davies,Rasmus Fensholt*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的新方法，在全球范围内利用3米分辨率的卫星影像检测单棵大树，实现了更精细的树木监测。


<details>
  <summary>Details</summary>
Motivation: 当前全球树木监测主要限于树冠覆盖或高度的粗略估算，缺乏对单棵树的精确识别和监控，限制了生态系统、气候调节等相关研究和应用的发展。

Method: 本方法用高斯核模拟树冠，提取树冠中心并生成二值树冠覆盖图，模型训练依赖于从机载激光雷达自动提取的十亿级点云数据，有效支持对森林内部和外部的树木检测。

Result: 与现有树冠覆盖图和机载激光雷达数据相比，本方法表现优异（与激光雷达的R^2为0.81），在不同生物群落间检测表现均衡，并通过人工标注微调提升了检测精度。

Conclusion: 该研究方法为全球高分辨率树木监测提供了可扩展的技术框架，未来可适配更高质量的卫星影像，有助于生态、气候和生物经济等领域的研究与应用。

Abstract: Trees are key components of the terrestrial biosphere, playing vital roles in
ecosystem function, climate regulation, and the bioeconomy. However,
large-scale monitoring of individual trees remains limited by inadequate
modelling. Available global products have focused on binary tree cover or
canopy height, which do not explicitely identify trees at individual level. In
this study, we present a deep learning approach for detecting large individual
trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree
crowns with Gaussian kernels of scalable size, allowing the extraction of crown
centers and the generation of binary tree cover maps. Training is based on
billions of points automatically extracted from airborne lidar data, enabling
the model to successfully identify trees both inside and outside forests. We
compare against existing tree cover maps and airborne lidar with
state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial
lidar), report balanced detection metrics across biomes, and demonstrate how
detection can be further improved through fine-tuning with manual labels. Our
method offers a scalable framework for global, high-resolution tree monitoring,
and is adaptable to future satellite missions offering improved imagery.

</details>


### [29] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出了一种高效可扩展的3D高斯溅射(3DGS)方法，解决动态图像训练中高数据量与训练时间长的问题。通过分层、高斯体激活、混合变形与自适应掩码机制，大幅加速了动态场景的高保真渲染。


<details>
  <summary>Details</summary>
Motivation: 高保真实时渲染在沉浸式应用中非常重要，但现有3D高斯溅射方法在动态场景下面临高数据量和每帧训练耗时过长的瓶颈，限制了其实用性。

Method: 提出了一种分层锚点结构，将高斯体按尺度组织；通过粗到细层级激活以分辨不同的细节层次，并采用混合变形与孵化（spawning）策略对运动表现建模，配合双向自适应掩码机制聚焦有效训练区域。

Result: 实验证明该方法在动态场景下能显著提升渲染质量，同时较现有方法大幅减少训练时间。

Conclusion: 本方法可高效扩展到动态场景，有效提升高保真渲染质量并优化训练速度，为实时沉浸式应用提供了新的解决思路。

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key
requirement for immersive applications. However, the extension of 3DGS to
dynamic scenes remains limitations on the substantial data volume of dense
Gaussians and the prolonged training time required for each frame. This paper
presents \M, a scalable Gaussian Splatting framework designed for efficient
training in streaming tasks. Specifically, Gaussian spheres are hierarchically
organized by scale within an anchor-based structure. Coarser-level Gaussians
represent the low-resolution structure of the scene, while finer-level
Gaussians, responsible for detailed high-fidelity rendering, are selectively
activated by the coarser-level Gaussians. To further reduce computational
overhead, we introduce a hybrid deformation and spawning strategy that models
motion of inter-frame through Gaussian deformation and triggers Gaussian
spawning to characterize wide-range motion. Additionally, a bidirectional
adaptive masking mechanism enhances training efficiency by removing static
regions and prioritizing informative viewpoints. Extensive experiments
demonstrate that \M~ achieves superior visual quality while significantly
reducing training time compared to state-of-the-art methods.

</details>


### [30] [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](https://arxiv.org/abs/2508.21451)
*Junha Song,Yongsik Jo,So Yeon Min,Quanting Xie,Taehwan Kim,Yonatan Bisk,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出了一种用于设备端轻量级图像描述的新方法，模型参数仅为大型MLLM（如LLaMA-7B）的1/56，但在单句和详细描述任务中仍表现出色。结合新颖的视觉细化机制，进一步提升了生成的描述质量。


<details>
  <summary>Details</summary>
Motivation: 现有主流多模态大模型由于计算量大，难以在本地设备执行，制约了图像描述技术在实际应用（如探索机器人、视频讲解等）中的普及。该研究旨在探索更小模型的可行性，以及提升小模型描述准确性的机制。

Method: 首先用一个125M参数的小型语言模型作为图像描述专家，在不同描述任务上进行对比，并分析其局限性（如视觉失明）。发现主要问题在于注意力机制和视觉表征有限。为此，提出名为“Sharp-Eyed Refinement”的新框架，通过DeepLens机制在初步注视后聚焦关键区域，从而提取更丰富的视觉信息，提升描述准确性。

Result: 实验表明，所提出的小模型在效果上能媲美大规模多模态泛化模型，并优于传统小模型。新框架在减少语义错误和提升描述细致度方面表现突出。

Conclusion: 小模型有望用于设备端图像描述任务，通过改进的视觉聚焦与表征机制，能够有效弥补视觉失明问题，兼具轻量与高效，适合实际部署。

Abstract: Image captioning is fundamental for applications like video instruction
systems and exploration robots, yet deploying such models on local devices is
challenging due to the high computational demands of multimodal large language
models (MLLMs). To address this, we first explore lightweight captioning by
implementing a specialist based on a 125M-parameter language model, 56 times
smaller than LLaMA-7B, and evaluating its performance on both single-sentence
and detailed captioning tasks. Surprisingly, we find that our model can achieve
performance comparable to large multimodal generalists, suggesting its
potential to serve as a strong visual specialist for on-device applications.
While promising, our model also exhibits a limitation: like other MLLMs, it
suffers from visual blindness, occasionally resulting in semantic captioning
errors. We carry out toy experiments and investigate the underlying causes,
where we observe that the problems arise from ineffective attention mechanisms
and limited visual representations. To alleviate them, we develop a novel
captioning framework, Sharp-Eyed Refinement, which enhances caption quality
through improved visual grounding. At its core, our DeepLens extracts detailed
visual representations by concentrating on informative regions identified
during the initial glance. Our experiments confirm both the advantages of our
specialist over prior small captioning models and large generalists and the
effectiveness of our framework.

</details>


### [31] [Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification](https://arxiv.org/abs/2508.21458)
*Kaouther Mouheb,Marawan Elbatel,Janne Papma,Geert Jan Biessels,Jurgen Claassen,Huub Middelkoop,Barbara van Munster,Wiesje van der Flier,Inez Ramakers,Stefan Klein,Esther E. Bron*

Main category: cs.CV

TL;DR: 本文系统评估了在脑部MRI数据的联邦学习环境下，基础模型（FM）微调的设计选择对性能和效率的影响，提供了关键的实证对比和部署建议。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FM）在AI辅助痴呆症诊断方面具有巨大潜力，但其与联邦学习（FL）的结合尚缺乏深入研究。由于医疗数据敏感且分布分散，亟需摸索在隐私保护等实际限制下，如何有效部署FM。

Method: 作者围绕分类头架构、微调策略和参数聚合方法三大关键设计，基于大规模多队列脑MRI数据，系统对比不同配置下联邦基础模型微调的表现。

Result: 关键发现包括：分类头结构对模型性能影响很大；冻结FM编码器的效果与完全微调相当；复杂的聚合方法优于标准联邦平均方法。

Conclusion: 该研究为基础模型在去中心化临床场景中的应用提供了实用见解，并强调了相关设计权衡，对未来方法开发具有指导意义。

Abstract: While foundation models (FMs) offer strong potential for AI-based dementia
diagnosis, their integration into federated learning (FL) systems remains
underexplored. In this benchmarking study, we systematically evaluate the
impact of key design choices: classification head architecture, fine-tuning
strategy, and aggregation method, on the performance and efficiency of
federated FM tuning using brain MRI data. Using a large multi-cohort dataset,
we find that the architecture of the classification head substantially
influences performance, freezing the FM encoder achieves comparable results to
full fine-tuning, and advanced aggregation methods outperform standard
federated averaging. Our results offer practical insights for deploying FMs in
decentralized clinical settings and highlight trade-offs that should guide
future method development.

</details>


### [32] [Multi-Method Ensemble for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21463)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 本文提出将特征截断与多种评分函数结合，通过集成多种当前主流的OOD检测方法，提出了一种更强健的Multi-Method Ensemble（MME）评分方法，在各类标准数据集上显著超越当前最优。


<details>
  <summary>Details</summary>
Motivation: 许多现有OOD检测方法只关注某一种技术或特定类型数据集，没能充分挖掘、融合已有多种方案的潜力。作者希望理论与实证结合，探索多种主流检测技术集成提升表现的可能性。

Method: 理论和实验证明特征截断与各种评分函数能高效结合，同时多评分函数集成能有效提升鲁棒性。提出了MME评分，将多种主流OOD检测方法统一到一个集成评分函数框架。

Result: 在大规模和小规模基准、涵盖near-OOD与far-OOD场景的实验中，MME方法在所有基准上显著优于最新方法。例如，在ImageNet-1K上，MME基于BiT模型平均FPR95为27.57%，比当前最好基线提升6%。

Conclusion: 集成当前主流OOD检测方案比单一方案更有效，MME方法在各类任务中均有显著优势，为安全关键应用中的开放世界神经网络提供更为可靠的OOD检测能力。

Abstract: Detecting out-of-distribution (OOD) samples is essential for neural networks
operating in open-world settings, particularly in safety-critical applications.
Existing methods have improved OOD detection by leveraging two main techniques:
feature truncation, which increases the separation between in-distribution (ID)
and OOD samples, and scoring functions, which assign scores to distinguish
between ID and OOD data. However, most approaches either focus on a single
family of techniques or evaluate their effectiveness on a specific type of OOD
dataset, overlooking the potential of combining multiple existing solutions.
Motivated by this observation, we theoretically and empirically demonstrate
that state-of-the-art feature truncation and scoring functions can be
effectively combined. Moreover, we show that aggregating multiple scoring
functions enhances robustness against various types of OOD samples. Based on
these insights, we propose the Multi-Method Ensemble (MME) score, which unifies
state-of-the-art OOD detectors into a single, more effective scoring function.
Extensive experiments on both large-scale and small-scale benchmarks, covering
near-OOD and far-OOD scenarios, show that MME significantly outperforms recent
state-of-the-art methods across all benchmarks. Notably, using the BiT model,
our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K
benchmark, improving performance by 6% over the best existing baseline.

</details>


### [33] [Adversarial Patch Attack for Ship Detection via Localized Augmentation](https://arxiv.org/abs/2508.21472)
*Chun Liu,Panpan Ding,Zheng Zheng,Hailong Wang,Bingqian Zhu,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种仅对目标区域进行增强的数据增强方法，提高了对遥感影像中船只的对抗性攻击效果。此方法减少了背景干扰，实验结果显示攻击成功率和迁移性均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于遥感影像的船只目标检测依赖深度神经网络，但这些模型易受对抗性补丁攻击。已有的增强方法常对整幅图像进行处理，导致目标检测出现干扰和误检，因此需要一种只针对目标区域的增强方法。

Method: 本文提出了一种局部增强方法，仅对目标（船只）区域进行数据增强，对背景和非目标区域保持不变。这样可以减少不必要的背景干扰，并让损失函数聚焦于补丁对检测模型的影响。

Result: 在HRSC2016数据集上进行的实验表明，所提方法能有效提升对抗性补丁攻击的成功率，并增强其在不同模型上的迁移能力。

Conclusion: 本文的局部增强方法能够有效提升遥感影像下船只检测的对抗性攻击效果，同时减少了因过度数据增强带来的误检问题。

Abstract: Current ship detection techniques based on remote sensing imagery primarily
rely on the object detection capabilities of deep neural networks (DNNs).
However, DNNs are vulnerable to adversarial patch attacks, which can lead to
misclassification by the detection model or complete evasion of the targets.
Numerous studies have demonstrated that data transformation-based methods can
improve the transferability of adversarial examples. However, excessive
augmentation of image backgrounds or irrelevant regions may introduce
unnecessary interference, resulting in false detections of the object detection
model. These errors are not caused by the adversarial patches themselves but
rather by the over-augmentation of background and non-target areas. This paper
proposes a localized augmentation method that applies augmentation only to the
target regions, avoiding any influence on non-target areas. By reducing
background interference, this approach enables the loss function to focus more
directly on the impact of the adversarial patch on the detection model, thereby
improving the attack success rate. Experiments conducted on the HRSC2016
dataset demonstrate that the proposed method effectively increases the success
rate of adversarial patch attacks and enhances their transferability.

</details>


### [34] [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](https://arxiv.org/abs/2508.21496)
*Hao Lu,Jiahao Wang,Yaolun Zhang,Ruohui Wang,Xuanyu Zheng,Yepeng Tang,Dahua Lin,Lewei Lu*

Main category: cs.CV

TL;DR: 本文提出了首个专注于长视频幻觉检测的基准ELV-Halluc，深入分析聚合帧级语义时产生的新型幻觉（语义聚合型幻觉，SAH），并验证了现有Video-MLLM在长视频场景下的不足及改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有Video-MLLM在处理长视频时，会由于多事件、复杂语义的聚合过程导致新的幻觉现象（SAH），而已有基准多聚焦于短视频，未能系统研究SAH问题。因此，作者希望填补这一漏洞，推动长视频理解中幻觉问题的研究。

Method: 作者构建了ELV-Halluc长视频幻觉基准，设计8K对抗性数据对，系统研究SAH现象。同时，探讨了缓解方法，如优化位置编码和采用DPO策略，以提升模型区分事件内外语义的能力。

Result: 实验证明：SAH随语义复杂度和事件变化速度提升而加重，现有模型在这类复杂场景下易犯SAH。通过优化位置编码与DPO技术，SAH比率可显著降低，减少幅度达27.7%。

Conclusion: 长视频场景下，模型语义聚合过程中的幻觉（SAH）尤为关键，需区分并单独应对。针对性基准和改进策略可有效提升模型可靠性，并为后续研究提供了参考。

Abstract: Video multimodal large language models (Video-MLLMs) have achieved remarkable
progress in video understanding. However, they remain vulnerable to
hallucination-producing content inconsistent with or unrelated to video inputs.
Previous video hallucination benchmarks primarily focus on short-videos. They
attribute hallucinations to factors such as strong language priors, missing
frames, or vision-language biases introduced by the visual encoder. While these
causes indeed account for most hallucinations in short videos, they still
oversimplify the cause of hallucinations. Sometimes, models generate incorrect
outputs but with correct frame-level semantics. We refer to this type of
hallucination as Semantic Aggregation Hallucination (SAH), which arises during
the process of aggregating frame-level semantics into event-level semantic
groups. Given that SAH becomes particularly critical in long videos due to
increased semantic complexity across multiple events, it is essential to
separate and thoroughly investigate the causes of this type of hallucination.
To address the above issues, we introduce ELV-Halluc, the first benchmark
dedicated to long-video hallucination, enabling a systematic investigation of
SAH. Our experiments confirm the existence of SAH and show that it increases
with semantic complexity. Additionally, we find that models are more prone to
SAH on rapidly changing semantics. Moreover, we discuss potential approaches to
mitigate SAH. We demonstrate that positional encoding strategy contributes to
alleviating SAH, and further adopt DPO strategy to enhance the model's ability
to distinguish semantics within and across events. To support this, we curate a
dataset of 8K adversarial data pairs and achieve improvements on both
ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.

</details>


### [35] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 本论文提出了一种用于微观图像分割和对象检测的新方法，通过训练CNN对视觉基础模型的低分辨率特征进行上采样，以提升对微观细节的表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（如Vision Transformers）由于计算效率通常采用patch-based方法，导致难以捕捉显微图像中的精细特征，而且不适合处理大尺寸图像，因此需要提升这些模型对高分辨率、细节丰富图像的适应能力。

Method: 作者训练了一个卷积神经网络（CNN），用以参考原始输入图像对基础模型的低分辨率特征进行上采样增强。该上采样网络应用于多类显微图像，且无需进一步训练。

Result: 该方法能高效地对多种显微图像（如植物细胞、锂电池阴极、有机晶体等）特征化和分割，尤其提升了例如发丝裂缝等难分割相的分离能力。

Conclusion: 通过该深度特征的交互式分割，可在远少于传统卷积网络训练/微调所需标签和时间的情况下，获得高质量的分割效果。

Abstract: Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.

</details>


### [36] [HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones](https://arxiv.org/abs/2508.21539)
*Hao Ruan,Jinliang Lin,Yingxin Lai,Zhiming Luo,Shaozi Li*

Main category: cs.CV

TL;DR: 提出了一种面向自然语言引导无人机任务的视觉语言理解新框架HCCM，以解决现有方法在复杂无人机场景下的语义对齐和泛化能力不足问题，并在公开数据集上取得了最新最好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有主流视觉-语言模型在无人机场景中对全局与细粒度语义的处理有限，难以适应无人机视角下视野广阔、语义复杂的场景，同时依赖严格划分和完整文本描述，影响动态环境中应用效果，因此需创新方法提升对局部-全局语义的层次化理解和对不完整语言的鲁棒性。

Method: 提出了HCCM框架，包括（1）区域-全局图文对比学习（RG-ITC），通过局部视觉区域与全局文本，以及全局视觉与局部文本对比，实现层次化语义提取，无需精确划分场景；（2）区域-全局图文匹配（RG-ITM），通过在全局跨模态表征下评估局部语义一致性，增强组合推理能力。此外，引入动量对比与蒸馏（MCD）机制，提升对文本描述不完整或模糊时的鲁棒性。

Result: 在GeoText-1652数据集上，HCCM在图像检索Recall@1达28.8%，文本检索Recall@1达14.7%，均为当前最高。在未见过的ERA数据集上，HCCM实现了39.93%的平均召回率（mR），零样本泛化性能超越多种微调基线方法。

Conclusion: HCCM为无人机视觉语言理解任务提供了层次化、高鲁棒性的建模方案，显著提升了复杂动态环境下的语义对齐与检索效果，具有很好的泛化潜力，有望推动相关跨模态智能系统发展。

Abstract: Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such
as target matching and navigation. However, the wide field of view and complex
compositional semantics in drone scenarios pose challenges for vision-language
understanding. Mainstream Vision-Language Models (VLMs) emphasize global
alignment while lacking fine-grained semantics, and existing hierarchical
methods depend on precise entity partitioning and strict containment, limiting
effectiveness in dynamic environments. To address this, we propose the
Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM)
framework with two components: (1) Region-Global Image-Text Contrastive
Learning (RG-ITC), which avoids precise scene partitioning and captures
hierarchical local-to-global semantics by contrasting local visual regions with
global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),
which dispenses with rigid constraints and instead evaluates local semantic
consistency within global cross-modal representations, enhancing compositional
reasoning. Moreover, drone text descriptions are often incomplete or ambiguous,
destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation
(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM
achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text
retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot
generalization with 39.93% mean recall (mR), outperforming fine-tuned
baselines.

</details>


### [37] [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](https://arxiv.org/abs/2508.21550)
*Yujin Park,Haejun Chung,Ikbeom Jang*

Main category: cs.CV

TL;DR: 本文提出了一种新的人机结合排序方法EZ-Sort，大幅减少主观任务中的配对标注成本，在保证标注一致性的前提下提高了效率。


<details>
  <summary>Details</summary>
Motivation: 在许多主观或困难的标注任务中，两两比较比绝对打分或排序更可靠，但传统方法需要大量的人工标注，成本极高。尽管已有采样和排序算法可以降低部分成本，但仍需进一步提升效率。

Method: EZ-Sort方法包括三步：首先利用CLIP模型无监督地对数据进行初步分组排序，然后用Bucket-aware Elo分数初始化项排序，最后用不确定性引导人类标注者在合并排序（MergeSort）流程中只介入关键难判别项，其余易判项由自动系统完成。

Result: 在多个数据集（如FGNET脸龄估计、历史图像年代排序、视网膜图像质量评估）上的实验证明，EZ-Sort的人类标注成本比穷尽两两比较减少了90.5%，比已有的方法减少了19.8%，同时标注一致性还得到保持或提升。

Conclusion: 结合CLIP无监督预排序和不确定性感知采样能显著提高两两比较式排序方法的效率与可扩展性，为主观判别类任务的人机协同标注提供了高效方案。

Abstract: Pairwise comparison is often favored over absolute rating or ordinal
classification in subjective or difficult annotation tasks due to its improved
reliability. However, exhaustive comparisons require a massive number of
annotations (O(n^2)). Recent work has greatly reduced the annotation burden
(O(n log n)) by actively sampling pairwise comparisons using a sorting
algorithm. We further improve annotation efficiency by (1) roughly pre-ordering
items using the Contrastive Language-Image Pre-training (CLIP) model
hierarchically without training, and (2) replacing easy, obvious human
comparisons with automated comparisons. The proposed EZ-Sort first produces a
CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,
and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation
was conducted using various datasets: face-age estimation (FGNET), historical
image chronology (DHCI), and retinal image quality assessment (EyePACS). It
showed that EZ-Sort reduced human annotation cost by 90.5% compared to
exhaustive pairwise comparisons and by 19.8% compared to prior work (when n =
100), while improving or maintaining inter-rater reliability. These results
demonstrate that combining CLIP-based priors with uncertainty-aware sampling
yields an efficient and scalable solution for pairwise ranking.

</details>


### [38] [ECHO: Ego-Centric modeling of Human-Object interactions](https://arxiv.org/abs/2508.21556)
*Ilya A. Petrov,Vladimir Guzov,Riccardo Marin,Emre Aksan,Xu Chen,Daniel Cremers,Thabo Beeler,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 该论文提出了ECHO框架，仅基于头部和手腕追踪，实现了对第一视角下的人体—物体交互的重建，包括人体姿态、物体运动和接触关系三类信息。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备普及，如何在几乎只依赖头部和手腕追踪的极简条件下，准确建模第一视角的人体与物体交互，对于智能设备的发展具有重要意义。但这一方向此前研究较少，需相关方法创新。

Method: 提出ECHO（Ego-Centric modeling of Human-Object interactions）统一框架，首次能从头、手腕追踪数据恢复人体姿态、物体运动、接触关系三种模态。核心为Diffusion Transformer结构和三变量扩散过程，联合建模人、物体和接触序列。采用head-centric空间表达，加强模型对全局方向的稳健性；通过conveyor-based inference机制，实现任意长度序列灵活处理。

Result: 在大量实验中，ECHO展现出优于以往方法的性能，尤其是在方法灵活性和准确性方面，达到了第一视角人体-物体交互重建任务的新SOTA水平。

Conclusion: ECHO首次实现了在极简观测下对第一视角人体-物体交互三模态的高效联合建模和重建，结果优异，有望推动可穿戴设备及相关交互分析的发展。

Abstract: Modeling human-object interactions (HOI) from an egocentric perspective is a
largely unexplored yet important problem due to the increasing adoption of
wearable devices, such as smart glasses and watches. We investigate how much
information about interaction can be recovered from only head and wrists
tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object
interactions), which, for the first time, proposes a unified framework to
recover three modalities: human pose, object motion, and contact from such
minimal observation. ECHO employs a Diffusion Transformer architecture and a
unique three-variate diffusion process, which jointly models human motion,
object trajectory, and contact sequence, allowing for flexible input
configurations. Our method operates in a head-centric canonical space,
enhancing robustness to global orientation. We propose a conveyor-based
inference, which progressively increases the diffusion timestamp with the frame
position, allowing us to process sequences of any length. Through extensive
evaluation, we demonstrate that ECHO outperforms existing methods that do not
offer the same flexibility, setting a state-of-the-art in egocentric HOI
reconstruction.

</details>


### [39] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 本文提出将OCR从单词级提升到行级，通过跳过单词检测，直接识别整行文本，提高了准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的OCR方法大多专注于单词级识别，虽然解决了字符切分的问题，但使单词切分成为新的瓶颈。本文旨在通过提升识别颗粒度，进一步提升OCR整体表现。

Method: 提出了一种行级OCR方法，输入整行文本而非单词，结合语言模型进行识别，并构建了251页带行级标注的英语数据集用于训练和评估。

Result: 实验结果显示，行级OCR在端到端准确率上提升5.4%，效率提高4倍。

Conclusion: 行级OCR能够进一步提升识别准确率和处理效率，未来有望结合大语言模型获得更好效果。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


### [40] [How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](https://arxiv.org/abs/2508.21565)
*Juneyoung Ro,Namwoo Kim,Yoonjin Yoon*

Main category: cs.CV

TL;DR: 本文评估了三种主流视觉-语言模型（BLIP-2、InstructBLIP 和 LLaVA-1.5）在城市场景理解，尤其是空间推理任务上的迁移能力，并引入了基于合成VQA数据集的微调方法来提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型大多在通用场景中预训练，对于城市场景下对象、布局和深度线索的细粒度空间推理能力迁移情况尚不明确，因此需要系统评估并探索相应提升方法。

Method: 作者对三种主流VLM在城市场景VQA任务下的零样本效果及微调后效果进行对比，所用微调数据集由街景图像的分割、深度、目标检测结果合成，并配有LLM生成的链式思考答案用于推理监督。

Result: 三种VLM在零样本条件下已有一定表现，通过合成数据集及CoT监督微调后，性能显著提升，特别是在否定和反事实等复杂问题类型上的提升较大。

Conclusion: 本研究提出了城市空间推理作为VLM新的挑战任务，并展示了通过合成数据集进行链式思考监督微调是将通用模型适配至专业领域的有效路径。

Abstract: Effectively understanding urban scenes requires fine-grained spatial
reasoning about objects, layouts, and depth cues. However, how well current
vision-language models (VLMs), pretrained on general scenes, transfer these
abilities to urban domain remains underexplored. To address this gap, we
conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,
and LLaVA-1.5-evaluating both zero-shot performance and the effects of
fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct
such dataset from segmentation, depth, and object detection predictions of
street-view images, pairing each question with LLM-generated Chain-of-Thought
(CoT) answers for step-by-step reasoning supervision. Results show that while
VLMs perform reasonably well in zero-shot settings, fine-tuning with our
synthetic CoT-supervised dataset substantially boosts performance, especially
for challenging question types such as negation and counterfactuals. This study
introduces urban spatial reasoning as a new challenge for VLMs and demonstrates
synthetic dataset construction as a practical path for adapting general-purpose
models to specialized domains.

</details>


### [41] [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](https://arxiv.org/abs/2508.21580)
*Nico Albert Disch,Yannick Kirchhoff,Robin Peretzke,Maximilian Rokuss,Saikat Roy,Constantin Ulrich,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 本论文提出了一种新的生成式时序预测方法，能够更好地预测和建模医学影像的时间动态，对比现有方法在公开数据集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在医学影像时序分析上多只使用单一时间上下文，且聚焦于分类或回归任务，难以进行精细空间预测。同时，已有方法局限于单一时点、特定疾病或存在技术限制，无法全面刻画时间动态。

Method: 作者提出了Temporal Flow Matching (TFM)方法，这是一种统一的生成式轨迹学习方法，能够学习影像随时间的分布变化。TFM可以在特定情况下退化为最近图像预测器，支持3D体积、多历史扫描及不规则取样，提升了方法的通用性与鲁棒性。

Result: 在三个公共纵向医学影像数据集上，TFM均优于当前先进的时空方法，设立了医学4D图像预测的新基准。

Conclusion: TFM为4D医学影像预测提供了新的、效果更佳且更可靠的方法，有望推进疾病进展建模和相关医学应用的发展。

Abstract: Understanding temporal dynamics in medical imaging is crucial for
applications such as disease progression modeling, treatment planning and
anatomical development tracking. However, most deep learning methods either
consider only single temporal contexts, or focus on tasks like classification
or regression, limiting their ability for fine-grained spatial predictions.
While some approaches have been explored, they are often limited to single
timepoints, specific diseases or have other technical restrictions. To address
this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified
generative trajectory method that (i) aims to learn the underlying temporal
distribution, (ii) by design can fall back to a nearest image predictor, i.e.
predicting the last context image (LCI), as a special case, and (iii) supports
$3D$ volumes, multiple prior scans, and irregular sampling. Extensive
benchmarks on three public longitudinal datasets show that TFM consistently
surpasses spatio-temporal methods from natural imaging, establishing a new
state-of-the-art and robust baseline for $4D$ medical image prediction.

</details>


### [42] [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](https://arxiv.org/abs/2508.21581)
*Daniël Boeke,Cedrik Blommestijn,Rebecca N. Wray,Kalina Chupetlovska,Shangqi Gao,Zeyu Gao,Regina G. H. Beets-Tan,Mireia Crispin-Ortuzar,James O. Jones,Wilson Silva,Ines P. Machado*

Main category: cs.CV

TL;DR: 本研究提出并测试了一种结合术前CT与术后病理切片（WSI）的多模态深度学习预测框架，以提升肾透明细胞癌（ccRCC）的复发风险评估准确性。结果显示，基于病理的模型优于纯CT模型，融合多模态信息进一步提升表现，总体接近临床使用的Leibovich评分。


<details>
  <summary>Details</summary>
Motivation: 目前临床上常用的Leibovich评分在分层预测ccRCC复发风险时存在精度有限、缺乏影像信息整合的问题。亟需更精细化、个性化的复发风险预测方法，提升术后随访和治疗决策的科学性。

Method: 采用预训练编码器与Cox多元回归生存分析模型，分别对CT影像数据和全切片病理图像进行特征提取和风险建模，并通过单模态、晚期融合和中间融合等策略进行多模态整合与性能对比。

Result: 病理WSI模型在复发风险预测上表现优于仅用CT的模型；通过中间融合的多模态模型进一步提升效果，最佳模型表现已与Leibovich评分接近。影像数据通过多模态融合提供额外预测价值。

Conclusion: 多模态深度学习方法可以实现ccRCC复发风险的个性化预测，并有望超越传统评分类工具。未来应发展更高效的融合策略、扩大数据集规模，并增强CT建模能力，以进一步提升性能和临床应用价值。

Abstract: Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is
essential for guiding postoperative surveillance and treatment. The Leibovich
score remains widely used for stratifying distant recurrence risk but offers
limited patient-level resolution and excludes imaging information. This study
evaluates multimodal recurrence prediction by integrating preoperative computed
tomography (CT) and postoperative histopathology whole-slide images (WSIs). A
modular deep learning framework with pretrained encoders and Cox-based survival
modeling was tested across unimodal, late fusion, and intermediate fusion
setups. In a real-world ccRCC cohort, WSI-based models consistently
outperformed CT-only models, underscoring the prognostic strength of pathology.
Intermediate fusion further improved performance, with the best model
(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random
tie-breaking narrowed the gap between the clinical baseline and learned models,
suggesting discretization may overstate individualized performance. Using
simple embedding concatenation, radiology added value primarily through fusion.
These findings demonstrate the feasibility of foundation model-based multimodal
integration for personalized ccRCC risk prediction. Future work should explore
more expressive fusion strategies, larger multimodal datasets, and
general-purpose CT encoders to better match pathology modeling capacity.

</details>


### [43] [Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation](https://arxiv.org/abs/2508.21657)
*Haomiao Zhang,Zhangyuan Li,Yanling Piao,Zhi Li,Xiaodong Wang,Miao Cao,Xiongfei Su,Qiang Song,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种深度展开网络（DUN），利用自适应带宽保持模型和相位域复杂值去噪模块，有效提升了计算机生成全息(CGH)的重建质量，实现了35 dB以上的PSNR，并在多组实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往基于深度学习的CGH方法存在物理可解释性差、全局依赖捕捉能力弱及适用距离有限等问题。本文旨在解决这些非线性与不适定性障碍，提升模型性能与泛化能力。

Method: 作者提出将梯度下降过程拆分为两个模块：自适应带宽保持模型（ABPM），扩展工作距离能力；相位域复杂值去噪模块（PCD），用可变形自注意力机制捕获全局特征并提升去噪效果。整体采用深度展开框架，将物理模型与深度学习结合。

Result: 在模拟与真实数据上均取得最新最优性能，尤其是PSNR超过35 dB，在工作距离和重建稳定性方面大幅优于传统ASM方法和基于CNN的端到端网络。

Conclusion: 结合物理建模和深度学习的DUN方法解决了CGH中的多项关键挑战，实现了更高质量、更远距离的全息重建，展示了很强的实际应用前景和推广价值。

Abstract: Computer-generated holography (CGH) has gained wide attention with deep
learning-based algorithms. However, due to its nonlinear and ill-posed nature,
challenges remain in achieving accurate and stable reconstruction.
Specifically, ($i$) the widely used end-to-end networks treat the
reconstruction model as a black box, ignoring underlying physical
relationships, which reduces interpretability and flexibility. ($ii$) CNN-based
CGH algorithms have limited receptive fields, hindering their ability to
capture long-range dependencies and global context. ($iii$) Angular spectrum
method (ASM)-based models are constrained to finite near-fields.In this paper,
we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into
two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain
complex-valued denoiser (PCD), providing more flexibility. ABPM allows for
wider working distances compared to ASM-based methods. At the same time, PCD
leverages its complex-valued deformable self-attention module to capture global
features and enhance performance, achieving a PSNR over 35 dB. Experiments on
simulated and real data show state-of-the-art results.

</details>


### [44] [Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models](https://arxiv.org/abs/2508.21680)
*Maximilian Rokuss,Yannick Kirchhoff,Fabian Isensee,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 本文提出了一种可交互的PET/CT图像分割方法，将用户点击（前景/背景）编码为模型的额外输入通道，并证明了这种方法能提高分割效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 全身PET/CT在肿瘤影像中极为重要，但由于示踪剂异质性、生理摄取及多中心扫描带来的差异，精准分割病灶极具挑战。虽然全自动方法进步显著，临床中仍需结合人工交互以高效优化分割结果，提升实际工作流程。

Method: 基于获奖的autoPET III nnU-Net管线，将用户前景和背景点击编码为额外的模型输入通道。系统性评估了不同空间提示（spatial prompt）的表示方式，证明欧氏距离变换（EDT）优于高斯核。此外，提出在线仿真用户交互及自定义点采样策略，以在实际提示条件下增强模型鲁棒性。采用基于EDT的多模型集成，并分别在有/无外部数据下训练。

Result: EDT编码方法下的模型集成在交叉验证中取得最佳表现，相较于基线模型减少了假阳性和假阴性，实现更高的分割准确度和鲁棒性。

Conclusion: 提示增强型（promptable）模型能在多示踪剂、多中心PET/CT应用中实现高效、用户驱动的分割流程，有助于PET/CT临床落地。代码已公开。

Abstract: Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate
lesion segmentation remains challenging due to tracer heterogeneity,
physiological uptake, and multi-center variability. While fully automated
methods have advanced substantially, clinical practice benefits from approaches
that keep humans in the loop to efficiently refine predicted masks. The
autoPET/CT IV challenge addresses this need by introducing interactive
segmentation tasks based on simulated user prompts. In this work, we present
our submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,
we extend the framework with promptable capabilities by encoding user-provided
foreground and background clicks as additional input channels. We
systematically investigate representations for spatial prompts and demonstrate
that Euclidean Distance Transform (EDT) encodings consistently outperform
Gaussian kernels. Furthermore, we propose online simulation of user
interactions and a custom point sampling strategy to improve robustness under
realistic prompting conditions. Our ensemble of EDT-based models, trained with
and without external data, achieves the strongest cross-validation performance,
reducing both false positives and false negatives compared to baseline models.
These results highlight the potential of promptable models to enable efficient,
user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code
is publicly available at https://github.com/MIC-DKFZ/autoPET-interactive

</details>


### [45] [Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping](https://arxiv.org/abs/2508.21689)
*Fatih Erdoğan,Merve Rabia Barın,Fatma Güney*

Main category: cs.CV

TL;DR: 本文提出了一种基于概率投影与置信分数的新方法，用于提升从图像空间到鸟瞰图空间的高精度地图构建，并在多个数据集上取得显著领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有鸟瞰图高精地图构建方法多依赖通用投影或注意力机制，但在准确性和泛化能力上存在困难，容易出现虚假道路元素。作者旨在通过结合几何映射与场景自适应过滤，提升地图要素的投影精度和鲁棒性。

Method: 方法首先依据摄像头参数做几何映射，将图像空间中的信息转换至鸟瞰图空间。随后引入概率性投影机制，为每个要素分配置信分数，动态调整映射以过滤无关或不可靠的要素，并通过置信分数指导跨时刻信息积累，从而提升整体建图质量。

Result: 在nuScenes和Argoverse2数据集的新划分上，所提方法在精度和泛化能力方面均超越了现有先进技术，尤其在nuScenes和长距离感知任务中提升显著。

Conclusion: 基于置信分数的概率性投影方法有效克服了注意力等方法的准确性和泛化瓶颈，为高精地图构建提供了更可靠的方案，具有较好的实用前景和推广价值。

Abstract: Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .

</details>


### [46] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: 本文提出了一种名为FLORA的轻量级合成数据生成方法，用于提升目标检测数据增强效率，显著降低计算资源消耗，并取得了较以往方法更优的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的合成数据增强方法依赖大量计算资源（如企业级显卡）和成千上万的合成图像，导致实际应用门槛高、成本大。为此，作者希望开发一种低资源消耗、易于应用且高效的数据生成方法。

Method: 作者提出FLORA方法，基于Flux 1.1 Dev扩散模型，仅采用Low-Rank Adaptation（LoRA）进行微调。该方法可以在消费级显卡（如NVIDIA RTX 4090）上生成高质量的合成数据，大大降低了硬件和数据需求。

Result: 在七个不同的目标检测数据集上进行实验，结果显示：仅用500张FLORA生成的合成图像训练检测器，其性能就优于使用ODGEN基线生成的5000张图像（提升最高可达21.3%的mAP@.50:.95）。

Conclusion: FLORA方法能够以更少的数据和更低的计算成本超过主流方法的性能，验证了专注于质量与效率的数据生成方案比粗暴大量生成更为有效，为实际应用中合成数据增强提供了更实用和可行的选择。

Abstract: Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.

</details>


### [47] [Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks](https://arxiv.org/abs/2508.21715)
*Amirhossein Nazeri,Wael Hafez*

Main category: cs.CV

TL;DR: 该论文提出了一种通过监控CNN激活层熵来检测对抗性扰动的方法，无需修改网络结构或影响模型性能。实验证明该方法能高效区分正常样本与对抗样本。


<details>
  <summary>Details</summary>
Motivation: 虽然CNN在图像识别任务上表现优异，但它们对对抗性样本十分脆弱。现有对抗样本检测方法往往需重训练、修改模型结构，或牺牲性能。因此需要一种轻量、实用、且不影响原模型的方法来检测对抗输入。

Method: 通过并行监控CNN（以VGG-16为例）早期卷积层的激活熵变化，统计分析正常输入与对抗输入在激活熵上的差异，实现无侵入、高效的对抗样本检测。

Result: 对抗样本会使VGG-16前几层的激活熵平均变化7%，借此能实现90%的检测准确率，假阳性率和假阴性率低于20%。正常和对抗样本间熵分布出现完全分离。

Conclusion: CNN在激活模式中固有地编码了分布转变，通过监控激活熵即可实时检测对抗输入且不影响模型本身性能，为实际视觉系统部署提供实用自诊断工具。

Abstract: Convolutional Neural Networks (CNNs) have become the foundation of modern
computer vision, achieving unprecedented accuracy across diverse image
recognition tasks. While these networks excel on in-distribution data, they
remain vulnerable to adversarial perturbations imperceptible input
modifications that cause misclassification with high confidence. However,
existing detection methods either require expensive retraining, modify network
architecture, or degrade performance on clean inputs. Here we show that
adversarial perturbations create immediate, detectable entropy signatures in
CNN activations that can be monitored without any model modification. Using
parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs
consistently shift activation entropy by 7% in early convolutional layers,
enabling 90% detection accuracy with false positives and false negative rates
below 20%. The complete separation between clean and adversarial entropy
distributions reveals that CNNs inherently encode distribution shifts in their
activation patterns. This work establishes that CNN reliability can be assessed
through activation entropy alone, enabling practical deployment of
self-diagnostic vision systems that detect adversarial inputs in real-time
without compromising original model performance.

</details>


### [48] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）在复杂环境下读取数字测量设备（DMD）上的数值表现不佳。本文提出了CAD2DMD-SET合成数据生成工具，以及用于验证的DMDBench真实数据集，通过生成高质量的合成数据集，有效提升了LVLMs在该任务中的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务上取得进展，但在真实场景下（如杂乱、遮挡、极端角度、运动模糊等）读取数字测量设备DMD的数值时表现不理想，尤其对于可穿戴设备和AR应用尤为突出。因此，亟需构建高质量数据和新方法，提升LVLMs在这些实际任务下的表现。

Method: 作者开发了CAD2DMD-SET，一种基于3D CAD模型、高级渲染和精细图像合成的合成数据工具，自动生成多样且带VQA标签的DMD数据集，支持LVLMs的微调。还构建了DMDBench验证集（含1000张真实标注图片）用于实际评测。作者用该工具生成的数据对三种主流LVLMs进行微调，并通过ANLS指标评测效果。

Result: 利用CAD2DMD-SET生成的数据微调后，InternVL模型在ANLS指标上提升了200%，且在其它任务表现无下降。整体上，大部分主流LVLMs在极端复杂场景下读取DMD数值的能力得到了明显提升。

Conclusion: CAD2DMD-SET极大改善了LVLMs在应对复杂现实环境下数字测量设备读数任务的能力，提高了模型的鲁棒性与实际可用性，工具计划开源供社区拓展和使用。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.

</details>


### [49] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 本文提出改进的音频视觉源定位方法SSL-SaN，能更好应对无声、噪声、离屏等负音频场景，并提出新指标与数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理无关视觉内容的负音频（如无声、噪声、离屏声源）时表现不佳，且评测仅限于单一正样本（场景中有唯一可见声源），不能反映实际复杂性。

Method: 1) 提出含负音频（静音和噪音）的新训练策略，使模型对负音频场景更鲁棒，正样本效果也兼顾。2) 构建SSL-SaN自监督模型，提升声音定位和跨模态检索表现。3) 提出新评测指标，平衡正负声画配对下特征对齐与分离能力。4) 构建IS3+扩展合成数据集，包含负音频。

Result: 新模型SSL-SaN在声音定位和跨模态检索任务中优于其他自监督方法；新指标能更全面评价模型能力；IS3+数据集和代码已发布。

Conclusion: 引入负音频训练、创新指标和数据集能有效提升视频声源定位在实际复杂场景下的性能与评估能力，推动领域进步。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


### [50] [UItron: Foundational GUI Agent with Advanced Perception and Planning](https://arxiv.org/abs/2508.21767)
*Zhixiong Zeng,Jing Huang,Liming Zheng,Wenkang Han,Yufeng Zhong,Lei Chen,Longrong Yang,Yingjie Chu,Yuzhi He,Lin Ma*

Main category: cs.CV

TL;DR: 本论文提出了UItron，这是一个针对移动端和PC端的自动化GUI代理（GUI agent）基础模型，具备高级的界面感知、语义对齐和任务规划能力，尤其在中文App场景下取得显著突破。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）推动了GUI代理的发展，但由于操作轨迹数据稀缺、交互基础设施有限以及底层模型初始能力不足，GUI代理的自动化和泛化能力依然面临重大挑战，特别是在中文环境下。

Method: UItron通过系统性数据工程，包括从前100个热门App手动收集超百万步操作轨迹，结合构建与移动端和PC端设备相连的交互环境。其训练策略为监督微调“界面感知”和“任务规划”，并发展课程式强化学习框架以提升复杂推理和探索能力。

Result: UItron在GUI界面感知、元素定位和任务规划等多个基准测试中取得了优异成绩。尤其在中文头部App的实际交互表现显著优于现有其它方案。

Conclusion: UItron推动了GUI代理在实际应用中的发展步伐，尤其优化了中文App场景下的自动交互能力，为实现更通用的智能界面操作奠定了技术和数据基础。

Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.

</details>


### [51] [Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations](https://arxiv.org/abs/2508.21769)
*Ha Min Son,Zhe Zhao,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: 本文指出传统CLIP等基础模型的领域泛化（DG）评估不够严格，因此提出了更具挑战性的评估方式，并提出了新方法CLIP-DCA以提升模型在高分布外（OOD）场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP等大规模基础模型的评估中，训练数据覆盖了太多现有测试集，导致模型泛化能力（DG）难以被准确评估，特别是在遇到真正未见过的分布（out-of-distribution, OOD）数据时。因此，有必要提出更具挑战性的DG评估方法，并改进模型处理高OOD数据的能力。

Method: 作者提出两种更有挑战性的CLIP领域泛化评估方式：1）对CLIP在ImageNet微调后，在33个具有量化OOD分数的多样化数据集上测试性能；2）通过unlearning技术让CLIP“遗忘”部分领域以模拟更极端的未知场景。之后，提出了CLIP-DCA方法，它通过添加独立的领域识别头并引入合成多样领域数据，提升编码器领域感知能力，同时通过特征解耦促进领域不变的分类表现。

Result: 实验表明：在更具挑战性的OOD数据集上，CLIP的表现明显下降；采用CLIP-DCA方法相比现有方法，模型在这些高难度OOD数据集上表现出显著提升，泛化能力增强。

Conclusion: 现有的领域泛化评估对CLIP等模型而言可能不够严格。通过提升领域感知并解耦分类和领域特征，CLIP-DCA能更好适应难以预测的新领域，提高基础模型的泛化能力。

Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.

</details>


### [52] [What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos](https://arxiv.org/abs/2508.21770)
*Qiyue Sun,Qiming Huang,Yang Yang,Hongjun Wang,Jianbo Jiao*

Main category: cs.CV

TL;DR: 该论文探讨了在视觉领域开放世界学习中，使用非典型、异常类型的视频数据进行特征表示学习，并提出了一个包含科幻、动画等不同非典型类别的新视频数据集。实验证明，融入这些数据显著提升了OOD检测、新类别发现和零样本动作识别等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉学习多聚焦于封闭集中的典型样本，对开放世界下异常、非典型数据的泛化和发现能力研究不足。作者希望探究：在学习过程中引入不寻常的视频样本，能否提升模型的开放世界适应性和泛化能力。

Method: 作者收集了涵盖科幻、动画等多类非典型视频数据的新数据集，并将其作为训练数据引入特征表示学习。通过实验证明其对开放世界三大任务（OOD检测、新类别发现和零样本动作识别）的帮助。同时分析了非典型样本的语义和类别多样性对性能的影响。

Result: 实验证明，简单地引入非典型样本即可在不同开放世界任务和设置下显著提升性能；增加非典型样本的类别多样性可进一步增强OOD检测效果，语义多样性高的一小撮非典型样本优于大量典型样本。多样化的非典型视频还能提升零样本动作识别泛化能力。

Conclusion: 非典型视频数据有助于开放世界中的视觉表征学习，能够显著提升OOD检测、新类别发现和零样本识别等任务的表现。此发现和新数据集有望促进该方向的进一步研究。

Abstract: Humans usually show exceptional generalisation and discovery ability in the
open world, when being shown uncommon new concepts. Whereas most existing
studies in the literature focus on common typical data from closed sets,
open-world novel discovery is under-explored in videos. In this paper, we are
interested in asking: \textit{What if atypical unusual videos are exposed in
the learning process?} To this end, we collect a new video dataset consisting
of various types of unusual atypical data (\eg sci-fi, animation, \etc). To
study how such atypical data may benefit open-world learning, we feed them into
the model training process for representation learning. Focusing on three key
tasks in open-world learning: out-of-distribution (OOD) detection, novel
category discovery (NCD), and zero-shot action recognition (ZSAR), we found
that even straightforward learning approaches with atypical data consistently
improve performance across various settings. Furthermore, we found that
increasing the categorical diversity of the atypical samples further boosts OOD
detection performance. Additionally, in the NCD task, using a smaller yet more
semantically diverse set of atypical samples leads to better performance
compared to using a larger but more typical dataset. In the ZSAR setting, the
semantic diversity of atypical videos helps the model generalise better to
unseen action classes. These observations in our extensive experimental
evaluations reveal the benefits of atypical videos for visual representation
learning in the open world, together with the newly proposed dataset,
encouraging further studies in this direction.

</details>


### [53] [Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering](https://arxiv.org/abs/2508.21773)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出了一种无监督视频连续学习的现实场景和对应的非参数学习方法，能在无任务边界和无标签的情况下，对接连出现的新视频任务进行学习，并显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 视频数据应用广泛但连续学习领域大多聚焦在有监督场景，需要昂贵的标签，缺乏对复杂视频数据的无监督连续学习探索。作者希望填补持续视频学习无监督方法的空白，降低实际应用成本。

Method: 作者制定了新的uVCL基准协议，在每个任务学习无结构的视频数据类别；利用无监督视频transformer提取特征，再采用核密度估计（KDE）进行数据概率表示；通过新颖性检测动态扩展内存聚类以捕获新知识，并结合迁移学习实现知识传递。

Result: 所提方法在UCF101、HMDB51和Something-to-Something V2这三个标准动作识别数据集上做了深入实验，不使用任何标签或类别边界，实验结果显示该方法在连续任务学习上性能提升明显。

Conclusion: 该研究首次解决了无监督视频连续学习的多个关键挑战，提出的非参数方法和新颖性检测机制可有效扩展到多任务场景，并为未来无标签视频理解与实际部署打开新思路。

Abstract: We propose a realistic scenario for the unsupervised video learning where
neither task boundaries nor labels are provided when learning a succession of
tasks. We also provide a non-parametric learning solution for the
under-explored problem of unsupervised video continual learning. Videos
represent a complex and rich spatio-temporal media information, widely used in
many applications, but which have not been sufficiently explored in
unsupervised continual learning. Prior studies have only focused on supervised
continual learning, relying on the knowledge of labels and task boundaries,
while having labeled data is costly and not practical. To address this gap, we
study the unsupervised video continual learning (uVCL). uVCL raises more
challenges due to the additional computational and memory requirements of
processing videos when compared to images. We introduce a general benchmark
experimental protocol for uVCL by considering the learning of unstructured
video data categories during each task. We propose to use the Kernel Density
Estimation (KDE) of deep embedded video features extracted by unsupervised
video transformer networks as a non-parametric probabilistic representation of
the data. We introduce a novelty detection criterion for the incoming new task
data, dynamically enabling the expansion of memory clusters, aiming to capture
new knowledge when learning a succession of tasks. We leverage the use of
transfer learning from the previous tasks as an initial state for the knowledge
transfer to the current learning task. We found that the proposed methodology
substantially enhances the performance of the model when successively learning
many tasks. We perform in-depth evaluations on three standard video action
recognition datasets, including UCF101, HMDB51, and Something-to-Something V2,
without using any labels or class boundaries.

</details>


### [54] [A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI](https://arxiv.org/abs/2508.21775)
*Omer Faruk Durugol,Maximilian Rokuss,Yannick Kirchhoff,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 本文提出了一种针对胰腺导管腺癌（PDAC）在MRI上的自动分割方法，并在PANTHER挑战赛中取得了领先成绩，尤其是在数据受限与影像对比度低的难题下。方法基于nnU-Net，并通过多阶段预训练与个性化集成策略，提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: PDAC的MRI分割对临床诊疗至关重要，但面临肿瘤与周围组织对比度低、标注数据稀缺等挑战。解决这些难题有助于推动自动分割技术在实际临床流程中的应用。

Method: 方法以nnU-Net为基础，采用深度多阶段级联预训练，自通用解剖基础模型开始，持续微调至CT胰腺病灶数据集和目标MRI模态。采用五折交叉验证系统性评估数据增强和训练流程，并针对表现得分进行指标导向的集成专科模型混合策略。

Result: 分析发现：激进数据增强提高体积精度，而默认增强提升边界精度。最终通过自定义专家模型混合集成，有效兼顾二者，获得Task 1肿瘤Dice得分0.661和Task 2得分0.523，以及最优MASD 5.46 mm、HD95 17.33 mm的边界表现，体现了方法的高效性和鲁棒性。

Conclusion: 本文提出的方法能在有限数据及复杂医学影像分割任务中，构建出高性能、专科化的自动模型，对自动医学图像分割领域有显著推动作用。

Abstract: Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is
critical for clinical workflows but is hindered by poor tumor-tissue contrast
and a scarcity of annotated data. This paper details our submission to the
PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and
therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the
nnU-Net framework and leverages a deep, multi-stage cascaded pre-training
strategy, starting from a general anatomical foundation model and sequentially
fine-tuning on CT pancreatic lesion datasets and the target MRI modalities.
Through extensive five-fold cross-validation, we systematically evaluated data
augmentation schemes and training schedules. Our analysis revealed a critical
trade-off, where aggressive data augmentation produced the highest volumetric
accuracy, while default augmentations yielded superior boundary precision
(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).
For our final submission, we exploited this finding by constructing custom,
heterogeneous ensembles of specialist models, essentially creating a mix of
experts. This metric-aware ensembling strategy proved highly effective,
achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523
for Task 2. Our work presents a robust methodology for developing specialized,
high-performance models in the context of limited data and complex medical
imaging tasks (Team MIC-DKFZ).

</details>


### [55] [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)
*Ugur Dinc,Jibak Sarkar,Philipp Schubert,Sabine Semrau,Thomas Weissmann,Andre Karius,Johann Brand,Bernd-Niklas Axer,Ahmed Gomaa,Pluvio Stephan,Ishita Sheth,Sogand Beirami,Annette Schwarz,Udo Gaipl,Benjamin Frey,Christoph Bert,Stefanie Corradini,Rainer Fietkau,Florian Putz*

Main category: cs.CV

TL;DR: 本文评估了GPT-5在放射肿瘤学领域临床决策支持中的表现，显示其在标准化测试和实际病例推荐中均优于早期模型，但仍需专家监督。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（如GPT-4等）在医学领域的应用增加，作者希望检验新一代GPT-5在专用于肿瘤治疗场景下的能力，以推动其临床应用价值评估。

Method: 本研究采用两项基准测试：（1）2021年ACR放射肿瘤学在职考试（TXIT，300道选择题）；（2）60个实际放射肿瘤病例描述，请GPT-5生成治疗方案。随后由四位放疗专科医师对方案的正确性、全面性及幻觉（不实内容）进行评分，并用Fleiss Kappa评估评分一致性。

Result: TXIT测试中，GPT-5准确率92.8%，高于GPT-4（78.8%）和GPT-3.5（62.1%）；在病例推荐中，对正确性（均分3.24/4）、全面性（3.59/4）评价较高，幻觉极少见。评分者之间一致性较低（Kappa=0.083），错误多集中在需要专业知识和临床细节适应的复杂场景。

Conclusion: GPT-5在放射肿瘤学基准测试和实际应用中明显超越前代模型，但仍存在明显不足。其临床应用前需严密专家审核及监督，避免潜在错误对患者安全造成影响。

Abstract: Introduction: Large language models (LLM) have shown great potential in
clinical decision support. GPT-5 is a novel LLM system that has been
specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the
ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300
multiple-choice items, and (ii) a curated set of 60 authentic radiation
oncologic vignettes representing diverse disease sites and treatment
indications. For the vignette evaluation, GPT-5 was instructed to generate
concise therapeutic plans. Four board-certified radiation oncologists rated
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was
quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were
most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's
treatment recommendations were rated highly for correctness (mean 3.24/4, 95%
CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).
Hallucinations were rare with no case reaching majority consensus for their
presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for
correctness), reflecting inherent variability in clinical judgment. Errors
clustered in complex scenarios requiring precise trial knowledge or detailed
clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable
performance in generating real-world radiation oncology treatment
recommendations, correctness ratings indicate room for further improvement.
While hallucinations were infrequent, the presence of substantive errors
underscores that GPT-5-generated recommendations require rigorous expert
oversight before clinical implementation.

</details>


### [56] [TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank](https://arxiv.org/abs/2508.21795)
*Jiawei Liu,Jiahe Hou,Wei Wang,Jinsong Du,Yang Cong,Huijie Fan*

Main category: cs.CV

TL;DR: 该论文提出一种结合结构和逻辑异常检测的三重记忆库框架（TMUAD），通过引入文本记忆库与经典图像记忆库融合，在工业和医学领域公开数据集上获得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测依赖于图像特征提取和图像记忆库，难以有效识别逻辑异常，且在标注数据稀缺时表现有限。本文动机在于更好地识别不止于外观异常，还能识别难以发现的逻辑型异常（如物体间存在不合理关系）。

Method: 提出TMUAD框架，包含三种互补记忆库：（1）对象类别级文本记忆库，利用逻辑感知文本提取器从图像中获取对象之间的丰富逻辑描述；（2）对象级图像记忆库，提取分割后对象的完整轮廓特征；（3）图像局部块级记忆库，采用视觉编码器提取图像块特征。三个记忆库为输入查询图像分别在逻辑与结构层面检索最相似的正常图像，并多层次融合异常评分。

Result: 在七个工业与医学领域公开数据集上，TMUAD的异常检测性能超越现有方法，达到了最优的效果。

Conclusion: 论文验证了结构与逻辑记忆库协同异常检测的有效性，突破了仅依靠图像特征局限，提升了综合异常检测能力，特别是在逻辑异常场景下具有优势。

Abstract: Anomaly detection, which aims to identify anomalies deviating from normal
patterns, is challenging due to the limited amount of normal data available.
Unlike most existing unified methods that rely on carefully designed image
feature extractors and memory banks to capture logical relationships between
objects, we introduce a text memory bank to enhance the detection of logical
anomalies. Specifically, we propose a Three-Memory framework for Unified
structural and logical Anomaly Detection (TMUAD). First, we build a class-level
text memory bank for logical anomaly detection by the proposed logic-aware text
extractor, which can capture rich logical descriptions of objects from input
images. Second, we construct an object-level image memory bank that preserves
complete object contours by extracting features from segmented objects. Third,
we employ visual encoders to extract patch-level image features for
constructing a patch-level memory bank for structural anomaly detection. These
three complementary memory banks are used to retrieve and compare normal images
that are most similar to the query image, compute anomaly scores at multiple
levels, and fuse them into a final anomaly score. By unifying structural and
logical anomaly detection through collaborative memory banks, TMUAD achieves
state-of-the-art performance across seven publicly available datasets involving
industrial and medical domains. The model and code are available at
https://github.com/SIA-IDE/TMUAD.

</details>


### [57] [VoCap: Video Object Captioning and Segmentation from Any Prompt](https://arxiv.org/abs/2508.21809)
*Jasper Uijlings,Xingyi Zhou,Xiuye Gu,Arsha Nagrani,Anurag Arnab,Alireza Fathi,David Ross,Cordelia Schmid*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频理解模型VoCap，能接收视频及多模态提示（文本、框或掩码），输出时空对象掩码及其描述，实现了对象分割和标注一体化，并提出了新的数据集SAV-Caption。


<details>
  <summary>Details</summary>
Motivation: 目前视频中细粒度对象定位和详细语义理解任务（如对象分割与描述）难以统一，且相关数据集昂贵且稀缺，现有方法难以兼顾多任务和多模态提示。

Method: 提出了VoCap模型，能够处理不同形式的提示（文本、框、掩码），输出时空对象掩码及对象描述。作者通过利用大规模视频分割数据集SAV并用伪标签生成“对象字幕”，预处理后用VLM生成伪描述，扩充并手动标注部分验证集，最终构建SAV-Caption数据集。VoCap在该数据集及其它数据集上混合训练。

Result: VoCap在指代表达式视频对象分割任务上达到SOTA，在半监督视频对象分割方面也具备竞争力，并首次在视频对象字幕任务上建立了基准。

Conclusion: VoCap模型在多模态对象分割与标注中表现优秀，并提供了新数据集SAV-Caption以推动相关研究。

Abstract: Understanding objects in videos in terms of fine-grained localization masks
and detailed semantic properties is a fundamental task in video understanding.
In this paper, we propose VoCap, a flexible video model that consumes a video
and a prompt of various modalities (text, box or mask), and produces a
spatio-temporal masklet with a corresponding object-centric caption. As such
our model addresses simultaneously the tasks of promptable video object
segmentation, referring expression segmentation, and object captioning. Since
obtaining data for this task is tedious and expensive, we propose to annotate
an existing large-scale segmentation dataset (SAV) with pseudo object captions.
We do so by preprocessing videos with their ground-truth masks to highlight the
object of interest and feed this to a large Vision Language Model (VLM). For an
unbiased evaluation, we collect manual annotations on the validation set. We
call the resulting dataset SAV-Caption. We train our VoCap model at scale on a
SAV-Caption together with a mix of other image and video datasets. Our model
yields state-of-the-art results on referring expression video object
segmentation, is competitive on semi-supervised video object segmentation, and
establishes a benchmark for video object captioning. Our dataset will be made
available at https://github.com/google-deepmind/vocap.

</details>


### [58] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 本文解决了语境识别（SR）任务中动词分类的多标签本质问题，提出用单正例多标签学习（SPMLL）新范式，并设计了新的评测指标与基线方法，有效提升了多标签识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像中的动词分类视为单标签问题，但作者发现同一张图片可被多个动词语义同时描述，单标签假设无法反映实际语义模糊性，因此需新的多标签识别方案。

Method: 作者将动词分类重塑为单正例多标签学习（SPMLL）问题，并设计了Graph Enhanced Verb Multilayer Perceptron（GE-VerbMLP）方法，通过图神经网络捕捉标签相关性，并结合对抗训练优化决策边界。同时，构建了多标签评测基准。

Result: 实验证明，所提方法在真实数据集上提升了超过3%的MAP（平均准确率），同时在传统top-1与top-5指标上也保持较强竞争力。

Conclusion: 将动词分类作为多标签问题更符合图像语境识别现实，所提SPMLL新范式和GE-VerbMLP方法有效提升了模型性能，有助于推动语境识别发展。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims
to extract structured semantic summaries from images by identifying key events
and their associated entities. Specifically, given an input image, the model
must first classify the main visual events (verb classification), then identify
the participating entities and their semantic roles (semantic role labeling),
and finally localize these entities in the image (semantic role localization).
Existing methods treat verb classification as a single-label problem, but we
show through a comprehensive analysis that this formulation fails to address
the inherent ambiguity in visual event recognition, as multiple verb categories
may reasonably describe the same image. This paper makes three key
contributions: First, we reveal through empirical analysis that verb
classification is inherently a multi-label problem due to the ubiquitous
semantic overlap between verb categories. Second, given the impracticality of
fully annotating large-scale datasets with multiple labels, we propose to
reformulate verb classification as a single positive multi-label learning
(SPMLL) problem - a novel perspective in SR research. Third, we design a
comprehensive multi-label evaluation benchmark for SR that is carefully
designed to fairly evaluate model performance in a multi-label setting. To
address the challenges of SPMLL, we futher develop the Graph Enhanced Verb
Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to
capture label correlations and adversarial training to optimize decision
boundaries. Extensive experiments on real-world datasets show that our approach
achieves more than 3\% MAP improvement while remaining competitive on
traditional top-1 and top-5 accuracy metrics.

</details>


### [59] [DriveQA: Passing the Driving Knowledge Test](https://arxiv.org/abs/2508.21824)
*Maolin Wei,Wanzhou Liu,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: 本文提出了DriveQA，一个覆盖交通法规和驾驶场景的开源文本与视觉基准，用于评估和提升大语言模型（LLM）与多模态LLM在驾驶知识上的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管当前自动驾驶基准测评关注空间和视觉问答任务，但现实中的驾驶笔试需要模型对所有交通规则、标志和通行权有完整理解。而许多极端情况和细节往往不在现有数据集中出现。因此，急需一个能全面覆盖交通法规与场景的新基准，来衡量和提升模型的实际驾驶知识。

Method: 作者构建了DriveQA，一个包含丰富交通法规、标志、场景和复杂交互情况的开放基准，涵盖文本与视觉输入，并设计实验系统性评测LLM与多模态模型在基准各方面的表现。研究内容包括基础规则、复杂通行权、标志变异和空间布局的问答，还扩展了不同环境变化要素如光照、视角、距离和天气（DriveQA-V模块），进一步分析模型对这些因素的敏感性。

Result: 实验发现：1) 先进LLM及多模态模型能较好处理基础交通规则，但在数值推理、复杂通行权、变异交通标志及空间场景上表现不足；2) 在DriveQA上微调显著提升多类别准确率，尤其是标志识别与路口决策；3) DriveQA-V揭示了模型对环境变化的敏感度；4) 在DriveQA预训练提升了下游真实驾驶数据集（如nuScenes和BDD）的表现，表明模型可内化文本与合成知识，并有效泛化至实际QA任务。

Conclusion: DriveQA作为全面的文本与视觉基准，为评估与提升大模型在驾驶知识、规则理解及复杂场景推理的能力提供了新标准。该工作为基于知识的驾驶AI开发及泛化能力测试奠定了基础。

Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [60] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CoBA（CounterBias Augmentation）的新型反偏数据增强方法，通过在语义三元组层面对文本进行重构，减少深度学习模型对非目标特征（如偏见）的依赖，从而提升模型泛化能力和抗偏鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在训练过程中经常借助数据中的伪相关性（如性别偏见、简单性偏见）来做出预测，这会导致模型在遇到未见数据时性能大幅下降。为了解决模型依赖伪相关性导致泛化性差的问题，论文提出了新的反偏增强方法。

Method: 提出了一套通用的反偏数据增强框架CoBA。首先将文本信息分解为语义三元组（如主语-谓语-宾语），然后有针对性地修改这些三元组以打破伪相关性，最后重构文本以生成新的增强数据，用于训练模型。此法可同时处理多种偏见问题。

Result: 通过大量实验，CoBA方法不仅在下游任务中提升了模型性能，而且有效减少了模型的多种偏见，提高了模型对未见分布数据的鲁棒性。

Conclusion: CoBA为解决伪相关性导致的泛化性与偏见问题提供了通用且有效的解决方案，有助于提升深度学习模型在实际环境下的适应性和公正性。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [61] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 本研究构建了首个包含年龄信息的大规模德语有毒言论数据集，分析不同年龄群体的网络交流差异，为更公平、具备年龄敏感性的内容审核系统提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 现有有毒言论数据集缺乏人口统计背景，尤其是年龄信息，限制了对不同年龄群体网络交流差异的理解。因此需要一个结合年龄估算的新数据集，促进更细致的研究和管理。

Method: 数据集来源于Instagram、TikTok、YouTube的匿名评论，结合平台提供的年龄估算，通过预定义的有毒关键词筛选评论，由人类和大型语言模型进行标注，共计3,024条人工标注和30,024条模型标注。对有毒言论按类别细分，确保数据多样性和相关性。

Result: 数据集中16.7%的评论被标注为有问题。有明确年龄估算，分析发现年轻用户更偏向情绪表达，而年长用户更常涉及虚假信息传播和贬低行为，揭示了年龄相关的语言和有毒言论差异。

Conclusion: 该数据集丰富了德语有毒言论研究资源，为研究群体间语言变异，以及开发更具公平性和年龄敏感度的内容审核系统提供了重要基础。

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [62] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: 本文提出了Granite Embedding R2模型家族，是高性能英文编码嵌入模型，特别适用于企业级密集检索应用，在多个领域超越现有开源模型。模型已开源，方便企业和研究界使用。


<details>
  <summary>Details</summary>
Motivation: 企业级密集检索应用对嵌入模型的检索速度、上下文长度和准确性提出了更高要求，现有开源模型存在上下文受限、速度较慢或准确性不足的问题。

Method: 基于第一代Granite模型升级，提升了上下文长度至8192 tokens，采用双路和交叉编码器架构，分别提供了22层和12层高效检索模型，并配备高质量reranker，全部在企业级治理数据上训练并监控。

Result: 在多种检索任务如文本、代码、长文档、多轮对话、表格数据上，模型表现优于主流竞品，检索速度提升19-44%，准确率更高，且在多个标准基准和真实企业场景下表现优异。

Conclusion: Granite R2模型在开源嵌入领域树立新标准，兼具领先性能、企业级许可和数据透明性，适合关键业务场景。所有模型以Apache 2.0协议免费开放使用。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [63] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的墨迹生成模型TrInk，并在字词识别准确率方面超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的墨迹（手写）生成模型在捕捉全局依赖和文本与手写轨迹对齐方面存在不足，需要更有效的模型提升生成效果。

Method: 提出TrInk（基于Transformer），在cross-attention模块中引入了缩放位置嵌入和高斯记忆掩码来更好地对齐输入文本与生成的笔迹点。设计了主观与客观评价流程，以全面评估生成手写体的可读性和风格一致性。

Result: 在IAM-OnDB数据集上，TrInk模型相比以往方法将字符错误率（CER）降低了35.56%，单词错误率（WER）降低了29.66%。

Conclusion: TrInk在手写体生成任务中比以往方法表现更优，生成的手写字体在可读性和风格一致性上有显著提升。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [64] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在价格谈判中的“锚定效应”偏见，发现其与人类类似，且推理过程越复杂模型越不易受影响。


<details>
  <summary>Details</summary>
Motivation: 认知偏见长期被研究于人类，但LLM在实际应用中也会受其影响，影响可靠性。本研究聚焦于谈判场景下的锚定效应，探讨它在LLM中的表现及相关影响因素。

Method: 作者设计了卖方LLM代理，通过引导这些代理在价格谈判中使用锚定效应，并用客观和主观指标评估其表现。同时，进一步分析了推理能力与性格等因素对锚定效应的影响。

Result: 实验发现LLM确实会表现出锚定效应，与人类类似。同时，推理能力较强的模型对锚定效应不太敏感，而人格特质与受锚定效应的程度无显著相关性。

Conclusion: LLM同样会出现认知偏见，尤其是在谈判等任务中。然而，增强模型推理链条可减弱此类偏见。研究有助于理解和减少LLM在实际应用中的认知偏见，促进其安全、负责任地应用。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [65] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 本文提出了一个名为Percept-V的新数据集，用于评测多模态大模型（MLLMs）在基础视觉感知任务上的能力，并发现这些模型在复杂度提升时表现明显下滑。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs在编码、数学、科学等复杂任务上取得了显著进展，但它们在未被干扰的基础图形感知任务上的能力尚未被系统评估。因此作者希望通过新数据集深入了解MLLMs基本视觉感知能力的局限。

Method: 作者构建了Percept-V数据集，包含7200个由程序生成的基础图形图像，分为30类，每类测试不同视觉感知技能。利用这个数据集，评估了主流MLLMs（如GPT-4o、Gemini、Claude）以及LRMs（如OpenAI o4-mini和DeepSeek R1）的表现，并分析了不同模型和不同认知技能类别的准确率变化。

Result: 实验发现，随着问题复杂性的增加，所有测试系统的表现都明显下降。不同MLLMs的准确率变化趋势类似，其中某些类别的视觉技能比其他技能明显更难以掌握。

Conclusion: 现有的MLLMs在基础视觉感知能力上还存在明显不足，尤其是在多技能复合和复杂感知任务上有待提高。作者的数据集和评测方法有助于进一步推动多模态模型在基础视觉认知方向的发展。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [66] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文综述了科学大语言模型（Sci-LLMs）与科学数据互相促进发展的现状与挑战，并提出了以数据为核心的分析与发展路线图。


<details>
  <summary>Details</summary>
Motivation: 随着Sci-LLMs逐渐改变科学研究的数据表达、集成和应用方式，本文旨在系统梳理其发展现状、面临的挑战及未来趋势，特别强调科学数据的复杂性和与通用NLP数据的区别。

Method: 作者提出了科学数据的统一分类法和科学知识的分层模型，全面评述了从通用到专业的各种Sci-LLMs及其涉及的270多个训练数据集，通过对190多个基准评测数据集的分析，揭示了对评估方式的转变，并探讨了提升数据质量的自动化和专家验证等方法。

Result: 发现目前Sci-LLMs需要处理异质、多尺度且不确定性高的科学语料，并对跨模态推理和领域不变性提出更高要求。评测方式正在从静态测试向过程和探索驱动的评估进化。论文还梳理了自动化标注管道和专家参与的解决思路。

Conclusion: 本文强调未来需要闭环AI系统，让基于Sci-LLMs的自主体参与实验、验证与知识库更新，并为建立可信、持续进化的科学AI伙伴系统提供了路线图和研究指引。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [67] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 该研究发现，大型语言模型在自评和互评时，模型身份标签会显著影响评判结果，不同标签下打分和偏好存在明显偏见。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型常被用于自动化评测，但评测是否存在模型偏见尚未彻底探讨。作者希望揭示模型身份标签对其所作评判的影响与潜在偏见情况。

Method: 让ChatGPT、Gemini和Claude三大模型对由各自撰写的博客文章进行评分。在无标签、真实标签和两种错误标签条件下，分别让三模型打偏好票并给出连贯性、信息性和简洁性评分，全部结果转化为百分比便于比较。

Result: 实验发现，“Claude”身份标签显著提高其被评内容得分，而“Gemini”标签则导致内容得分降低，无论实际内容如何。错误标签条件下，偏好票和质量分数大幅波动。此外，Gemini自评遇真标签时分数骤降，而Claude自评分数则进一步上升。

Conclusion: 模型身份标签会严重影响高级判断，并对细致的质量评分产生微妙影响。因此，为确保评测公正，建议采用盲评或多模型联合评测机制。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [68] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 本文提出了一种新方法（BED-LLM），通过贝叶斯实验设计框架，提升大语言模型（LLM）在多轮对话和信息获取任务中的表现，并在多种测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在与用户或外部环境交互过程中，对信息主动获取和交互式推理能力有限。该研究旨在通过贝叶斯实验设计（BED）框架，提升LLM的自适应问询与信息搜集能力，使其能更有效地进行多轮对话、小样本学习和用户偏好推断。

Method: 作者提出BED-LLM方法，通过最大化每轮提问的期望信息增益（EIG），引导LLM智能地设计问题，并用概率模型（基于LLM的信念分布）来公式化EIG。为保证有效性，引入了专门的EIG估算器，避免仅靠上下文更新，并采用有针对性的候选问题生成策略。

Result: 在经典的20问游戏以及用户偏好推断实验中，BED-LLM在主动信息获取和用户偏好推断任务上，明显优于直接提示法和其它自适应设计策略，展现了更高效的信息获取能力。

Conclusion: BED-LLM为提升LLM主动信息搜集和多轮对话任务性能提供了有效框架，并通过创新EIG估算、上下文处理和问题生成方法，显著扩展了LLM与环境交互的智能水平。

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [69] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习（GRPO）微调Llama-3.1 8B语言模型的自动HFACS分类框架，用于航空安全分析，实现了比当前主流大模型更高的多标签分类精度和资源效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于HFACS的人为因素航空事故分析方法存在可扩展性和一致性问题，难以满足大规模、自动化和高效的事故预防需求。

Method: 作者提出利用带有多组件奖励机制的GRPO（Group Relative Policy Optimization）强化学习方法，微调Llama-3.1 8B模型。为解决事故数据类别不平衡问题，还引入了合成数据生成。该方法专门为航空安全分析设计奖励体系并设定新评价基准。

Result: 优化后的GRPO模型在多标签HFACS分类任务中，实现精确匹配准确率由0.0400提升至0.1800（增长350%），局部匹配准确率提升到0.8800。性能在多项关键指标上超越GPT-5-mini等主流大模型。

Conclusion: 实验表明，经过领域优化的小型模型不仅提升了航空事故安全分析准确性，还显著降低了计算资源和部署延迟，为边缘设备上的实际快速部署提供了可行性。

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [70] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文提出了一种基于像素的生成式语言模型，通过将文本单词渲染为图片来增强对字符扰动攻击的鲁棒性，在多语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自回归式语言模型容易受到“正字法攻击”，即输入文本被用多语言字符扰动，导致性能大幅下降，主要由于子词分词器和嵌入存在词汇覆盖问题。亟需方法提升模型对这些噪音输入的鲁棒性，增强多语言支持。

Method: 作者将传统基于文本的嵌入替换为基于像素的表示，把单词渲染为独立的图像，通过图像嵌入进行语言建模，天然具备对字符扰动和多语言文本的兼容性。

Result: 在多语言LAMBADA数据集、WMT24和SST-2基准数据集上，所提方法在面对正字法噪音时显著提升了鲁棒性，并在多语言环境下效果良好。

Conclusion: 基于像素的嵌入解决了传统语言模型对字符扰动的敏感性和子词分词器的局限性，对多语言文本处理更具兼容性和鲁棒性。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [71] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本文评估了自监督语音模型（S3Ms）在语音习得过程中是否表现出关键期（CP）效应，发现这些模型未能显现出与人类类似的关键期效应。


<details>
  <summary>Details</summary>
Motivation: 人类语言习得存在关键期效应，即推迟第二语言接触会导致习得更困难，而延长第一语言暴露则有助于第一语言保持。此前大多研究集中在文本语言模型，缺乏对语音模型在这一问题上的探索。

Method: 作者使用以儿童为对象的话语（child-directed speech）训练S3Ms，分别操控第二语言（L2）输入的开始时间和第一语言（L1）输入的终止时间，然后以音素区分能力为标准评估模型表现。

Result: 实验发现，S3Ms并未展现出与人类相同的关键期效应。相反，推迟L2接触的模型在L2表现更好，推迟L1结束则导致L1遗忘。

Conclusion: 自监督语音模型在语音习得任务中未能重现人类的关键期效应，提示其与人类语言习得机制存在差异。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [72] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 该论文提出了一种新的加速大型语言模型自洽性（self-consistency）方法的推理框架，显著提升了生成速度且不影响表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在生成文本时，常出现幻觉（hallucination）问题。现有检测方法在句子级别效果不佳，或依赖特定领域知识。自洽性方法一定程度缓解了这些限制，但需多次生成，导致计算开销大。

Method: 作者首次研究了自洽性方法中生成结果的冗余（表现在多次生成中存在相同前缀），并发现非精确答案的token对语义贡献很小。据此提出了“Decoding Memory Pipeline”（DMP）：通过选择性推理和退火解码，加速生成；该方法对模型、数据集、解码策略及自洽性基线均通用。

Result: 在大量实验证明下，DMP方法在不降低AUROC表现的前提下，实现了高达3倍的推理加速。

Conclusion: DMP方法可大幅提高多响应生成效率，并有潜力扩展至模型对齐与推理等任务，提升LLMs实际应用性能。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [73] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种全新的代码嵌入模型jina-code-embeddings，能够高效支持代码检索、技术问答和多语言代码相似性识别，且性能优异，模型规模较小。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言与代码检索、问答等场景对高质量代码表示需求迫切，尤其是在模型小型化和多编程语言兼容性方面仍存挑战。因此，作者提出一种新的代码嵌入模型以满足这些需求。

Method: 模型采用自回归主干网络，结合文本与代码双模态预训练，通过last-token pooling生成最终向量表示。文中还详细介绍了模型训练流程。

Result: 尽管模型规模较小，其在相关任务上达到了当前最佳性能，优于现有主流代码嵌入方法。

Conclusion: 作者验证了所提出的代码嵌入模型方案在模型体积和性能之间取得了良好平衡，证明该方法的有效性和先进性。

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [74] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: 论文发布了更新版的BLUEX数据集，涵盖2024-2025年考试题目和自动生成的图片描述，用于增强多语言LLM的评测和数据污染检测。


<details>
  <summary>Details</summary>
Motivation: LLM在多语言、非英文场景的能力提升，但相应评测方法不足，尤其缺少对数据污染（训练集与测试集重合）和视觉文本混合任务的评估。

Method: 对BLUEX数据集进行扩展，新增2024-2025考题和用先进模型自动生成的图片描述；通过caption策略使仅支持文本输入的LLM也能参与图片理解评测；将原始可用问题数翻倍。对多种商用和开源LLM展开测试，评估其通过caption理解视觉信息的能力。

Result: caption技术使文本模型可访问性提升超40%，生成1,422道可用题目，远超原始BLUEX。实测多种LLM对图片caption的利用能力，有助量化其多模态水平。

Conclusion: 增强版BLUEX数据集显著提升多语言LLM的评测广度和深度，caption方案为文本型模型提供了公平的多模态能力评测基础，有助揭示LLM在视觉理解和数据污染下的实际表现。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [75] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本文综述了构建和应用大语言模型（LLM）面临的16项关键挑战，并通过比较最新的闭源GPT-4o和开源DeepSeek-V3-0324模型，分析其应对挑战的不同路径、优缺点与应用适配性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在AI领域取得突破，但其开发与部署依旧复杂多变。论文旨在系统梳理LLM开发/应用过程中的核心挑战，并比较代表性的闭源与开源模型解决问题的方式，为业界提供参考。

Method: 作者基于文献和实际案例，总结LLM的16项挑战，并以GPT-4o和DeepSeek-V3-0324为代表，详细比较各自在安全性、可靠性、效率和适应性等方面的表现，同时剖析它们在不同应用场景中的优势。

Result: 分析表明，闭源模型在安全性和可靠性上表现突出，适合高要求场景；开源模型则更高效、易于定制，适用于灵活多变的需求。不同模型在聊天机器人、编程工具、医疗、教育等领域有各自的适配性。

Conclusion: 本论文帮助AI相关人员系统了解LLM的前沿进展、主要难点和应用适配建议，便于更科学地选择和运用大语言模型。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [76] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文重新审视了图灵测试，通过“正常性”概念探讨其意义，认为图灵测试本质上是对“普通智能”的测试，而不是对卓越智能的测试，并指出大型语言模型（如ChatGPT）难以通过图灵测试，因为它们追求的是“人工聪明”而非“人工智能”。


<details>
  <summary>Details</summary>
Motivation: 作者试图厘清图灵测试的真正内涵，并指出当前人工智能模型在目标和本质上与图灵测试的初衷存在偏差，借此探讨何为“人工智能”。

Method: 作者以统计学和规范性视角重新解读“正常性”，分析图灵测试中的“平均人类智能”与“普通评判者”，提出图灵测试乃基于模型化的群体评判与常态智能。

Result: 作者认为，图灵测试关注的不是极端或卓越智能，而是“普通人类智能”。而如今的大型语言模型追求异常出色的表现，因而未必能通过着重普通性的图灵测试。

Conclusion: 本文得出两个结论：一是ChatGPT等大模型更像“人工聪明”而非“人工智能”，难以真正通过图灵测试；二是图灵测试能否揭示人类认知本质，关键在于人类心智是否可简化为“普通/平均心智”，这一问题超越了图灵测试范畴。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [77] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究了自动文本摘要评估中指标可复现性的问题，发现文献报告的性能与实际实验存在较大出入，强调当前主流指标在对齐人工判断和计算效率之间存在权衡，并指出LLM评估的不稳定与不可复现性。作者提出一个统一开源评测框架，呼吁更规范的评估协议和详尽记录以提升评测可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动文本摘要评估依赖多种自动化指标，但指标的可复现性受到质疑。文献中报告的指标性能存在不一致，尤其是近期兴起的LLM评估方法，相关流程缺乏标准化和透明度，影响学术公信力和实验可重复性。

Method: 本文选取包含ROUGE等经典方法和G-Eval、SEval-Ex等LLM指标在内的六种代表性评测方法，在SummEval数据集上进行统一实验，并构建一个开源框架以实现不同指标的公平、透明比较。同时，分析各指标对人工判断的一致性与计算性能及稳定性的权衡。

Result: 发现不同自动评测指标在标称与实际表现上有显著差异。最能对齐人工判断的LLM类指标计算耗时且多次运行结果不稳定，不如传统指标易于重现。同时总结出了主流评测方法的一些局限及风险。

Conclusion: 结合实验发现，作者建议自动摘要评测需采用更严格、透明的实验协议与标准化流程，包括详尽纪录以增强结果可靠性。不应盲目依赖LLM指标，需关注其技术依赖与可复现性风险，促进未来评测方法的健全发展。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [78] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文评估了大语言模型自动化评审生成器（ARG）在检测科研逻辑缺陷方面的能力，结果显示其检测能力有限，因此提出改进建议并公开相关评测数据集和框架。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）自动化评审有巨大潜力，但其潜在偏见和系统性失误可能危及科学诚信。了解这些自动评审工具在核心评审技能上的表现，尤其是检测研究逻辑错误的能力，具有重要的现实意义。

Method: 作者设计了一个全自动反事实评测框架，能够在可控条件下隔离并测试ARG检测论文研究逻辑缺陷的能力，主要检测论文结果、解释和结论之间的一致性。

Result: 测试多种ARG方法后发现，研究逻辑中存在的缺陷对其输出评审几乎没有影响，说明现有ARG难以有效识别研究逻辑问题。

Conclusion: 大语言模型作为自动评审工具在检测科研逻辑错误方面存在明显局限。作者据此提出三项未来改进建议，同时公开了相应的数据集和评测框架，便于后续研究。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [79] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 本文提出了首个专为医学奖励模型（MRMs）和评判者设计的多模态基准Med-RewardBench，填补了医学场景下准确与专业评测的空白，并评估了32个多模态大模型在临床任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 医学应用需求高度专业、准确和具备临床背景的大模型输出，但目前缺乏能够准确评估医学场景奖励模型和评判者的专门基准，导致模型的临床实用性难以量化和改进。

Method: 作者构建了Med-RewardBench基准，涵盖13个器官系统和8个临床科室，共1026个专家注释案例，并设定六大临床关键评估维度，通过严格的三步流程确保数据质量。利用该基准，对32种开源、专有及医学领域的多模态大模型进行评测，并开发并微调了基线模型以测试提升潜力。

Result: 基准数据显示，当前多模态大模型在医学任务上的专业性与专家一致性方面存在巨大挑战。基线模型通过微调后，性能有显著提升。

Conclusion: Med-RewardBench为医学奖励模型和评判者的系统评估提供了新标准，揭示了现有模型在临床医学应用中尚存巨大提升空间，并为后续模型优化与研究奠定基础。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [80] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，通过分解大语言模型的词嵌入，挖掘语义维度下更细致的子维度，并用脑成像数据验证其神经合理性。


<details>
  <summary>Details</summary>
Motivation: 现有研究常用宽泛预定义语义维度，难以捕捉更精细的概念区分，因此需要发现更细粒度、可解释的语义结构。

Method: 提出Disentangled Continuous Semantic Representation Model（DCSRM），将词嵌入分解为多个子嵌入，各自编码特定语义信息，并利用体素级编码模型，将这些子维度映射到大脑激活数据。

Result: 识别出一组可解释的语义子维度，并发现这些维度的分解与极性等结构性原则紧密相关。脑影像实验表明这些子维度具有认知与神经科学的合理性。

Conclusion: 该方法能获得更细粒度、可解释的语义子维度，有助于理解语言与大脑中意义的组织方式。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [81] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 本论文提出并研究了大语言模型（LLMs）的“意识形态深度”概念，通过实验证明不同LLM在政治立场可控性和内部表征复杂性上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现LLMs展现出明显的意识形态倾向，但目前尚不清楚这种倾向是表层现象还是源自稳定且复杂的内部结构。探索这一问题有助于理解模型决策的本质、增进模型的可控性，并规避潜在偏见风险。

Method: 作者使用两种方法考察两个著名开源LLM：1）利用指令提示和激活引导实验其政治立场的可控性（steerability）；2）用稀疏自编码器（SAEs）分析其内部的政治特征表征。

Result: 实验发现，不同模型的政治立场可控性差异较大，有些容易在自由主义和保守主义间切换，有些则表现出抗拒或拒绝率上升。抽象和深层的政治特征在低可控性模型中更加突出。一个模型的政治特征数量可达另一个同规模模型的7.3倍。定向消融核心政治特征后，深度模型的相关推理出现一致逻辑变化，而浅层模型则更倾向于拒答。

Conclusion: LLM的“意识形态深度”可以量化，政治立场的可控性可作为观察其潜在政治结构的窗口，对未来构建可解释和可控的大模型具有重要启示。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [82] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 本文研究如何通过AI反馈强化学习(RLAIF)提升小型语言模型(SLM)的创意写作能力。提出并比较了两种AI驱动的奖励策略，对比后发现基于原则引导的LLM-裁判机制优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备强大创意写作能力，却受制于高昂算力资源。提升小模型(SLM)的生成能力是更经济的方向，但现有监督微调(SFT)难以激发新意，基于人类反馈的强化学习(RLHF)又成本高昂。论文旨在寻求更高效且更具创意的SLM训练方法。

Method: 提出两种RLAIF训练策略：1) 训练一个奖励模型(RM)，用高质量、多代理拒绝采样策略获得的偏好数据进行训练，2) 创新的基于原则的LLM裁判（Judge）机制，根据对抗式训练和反思机制直接输出奖励信号。两者都用于引导7B参数中文问候语生成。

Result: 实验证明，两种方案均显著提升了生成内容的创意性和质量。其中，基于原则裁判的LLM方案不仅生成质量更高，在训练效率及对人类标注依赖性方面也有明显优势。自动化评价指标与人工评测高度一致。

Conclusion: 基于LLM原则裁判的RLAIF方法，为提升小模型创意能力提供了高效、可扩展的解决方案，降低了对人工数据的依赖，对未来创意型SLM开发具有重要意义。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [83] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 本文提出了一种关注多样性的自动分类器选择方法，用于提升多分类器集成系统检测假新闻的效果，并在多个数据集上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 假新闻在社交媒体上的盛行部分源于人们的认知偏见，如确认偏误，给公共健康和政治等领域带来严重影响。现有的机器学习事实核查系统依赖集成方法提升准确率，但集成性能高度依赖于选用分类器的多样性，现有方法在挑选多样分类器方面存在挑战。

Method: 研究提出一种新颖的自动分类器选择方法。具体做法为：首先计算多个分类器之间的两两多样性，根据结果用层次聚类法将分类器分组，不同层次代表不同粒度的多样性。HierachySelect算法随后在不同分组层级中寻找具有最大内部多样性的分类器池，并结合分类器性能评价指标选择最终的集成分类器组合。

Result: 作者在六个不同领域和类别数的数据集上，使用40个异构分类器进行实验。与Elbow启发法和现有最先进的方法相比，本方法在两个数据集上取得了最高准确率。

Conclusion: 该方法能有效选择多样且性能优良的分类器进行集成，提升假新闻等领域的自动检测能力，优于部分现有主流方法，有应用推广价值。

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [84] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文提出了MahaSTS，这是首个马拉地语大规模人工标注的句子文本相似度（STS）数据集，并基于该数据集微调了MahaSBERT句子编码模型，用于高质量句子相似度回归任务。


<details>
  <summary>Details</summary>
Motivation: 马拉地语作为一种低资源语言，缺乏高质量、公开的文本语义相似度数据集，导致NLP任务中相关模型较难训练和评估。作者希望通过构建标准化数据集和模型促进该领域发展。

Method: 创建了包含16,860对带有0-5连续相似度分数的马拉地语句子对的数据集，并确保分布均匀。用该数据集微调了Sentence-BERT模型，得到MahaSBERT-STS-v2，并与现有多种模型（MahaBERT、MuRIL等）进行了对比实验。

Result: MahaSBERT-STS-v2在句子相似度任务中，表现优于其他主流马拉地语和多语种模型。实验还表明人工标注和均匀分布的数据集设计有助于提升模型性能和稳定性。

Conclusion: 该论文证明了高质量人工标注和结构化监督对于低资源语言句子相似度任务的重要性。数据集与模型已开源，为未来马拉地语NLP研究和应用提供了重要资源。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [85] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文是对文本去标识化（匿名化）技术的全面综述，涵盖基础方法、最新进展、领域应用及挑战，并对未来研究趋势提出建议。


<details>
  <summary>Details</summary>
Motivation: 随着各种领域包含敏感个人信息的文本数据激增，对强大的去标识化技术的需求变得迫切。这既是为了保护隐私与符合法规，也是为了保留数据在后续任务中的可用性。

Method: 综述首先探讨了以命名实体识别（NER）为核心的传统方法，随后详细讨论了大语言模型（LLM）在匿名化和去匿名化中的双重作用。文章还分析了医疗、法律、金融、教育等关键领域中的特定挑战和定制化解决方案，并探讨了融合正式隐私模型和风险感知框架的先进方法，以及作者身份匿名化等子领域。

Result: 对现有评估框架、指标、基准和实际工具进行回顾，系统归纳了当前知识、涌现的趋势和持续存在的挑战，比如隐私与可用性的权衡、准识别符问题、LLM能力带来的新威胁等。

Conclusion: 该综述为学术界和实践界提供了对现有技术的系统梳理，明确了未来在平衡隐私保护与数据可用性、应对新型威胁等方面的研究方向与待解难题。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [86] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Middo的自我进化、动态数据优化框架，用于提升大型语言模型监督微调时的数据质量。该框架通过模型自知的数据诊断与优化，能持续提升训练数据和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态数据构建的数据选择与合成方法，无法适应模型能力的演进，从而限制了训练效果。因此需要一种能与模型能力动态协同进化的数据优化策略。

Method: Middo采用模型感知的数据选择及上下文保持的数据修正。具体地，首先通过三维模型信号（损失模式、嵌入聚类、模型自对齐分数）主动识别次优样本；然后利用自适应优化引擎，在保持语义一致性的前提下将这些样本转化为更有教学价值的数据；整个过程基于动态学习原理，能随着模型能力提升自我迭代。

Result: 在多个基准测试中，Middo显著提升了种子数据质量，并在不扩展数据规模的情况下，使LLM性能平均提升7.15%。

Conclusion: Middo展示了数据与模型之间可持续的动态协同优化范式，为大型语言模型训练带来新的思路，并展示了具备长远实际应用前景。

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [87] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 本研究探讨不同性格类型的用户在与大型语言模型（LLM）协作时的偏好差异，发现性格影响对模型的选择，传统评估可能忽视了这些细粒度差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于日常协作，不同性格类型的用户在实际互动中对特定模型是否存在偏好尚不清楚。了解这一点有助于更好地匹配用户与模型，提高用户体验。

Method: 研究招募了32名参与者，按Keirsey性格类型分为四组。每组与GPT-4和Claude 3.5分别在数据分析、创意写作、信息检索、写作辅助四项任务中多轮互动。通过偏好选择及情感分析定性反馈，比较不同性格用户对模型的选择倾向。

Result: Rational类型显著偏好GPT-4，尤其在目标导向任务上；Idealist则更喜欢Claude 3.5，特别在创意与分析任务中。其他类型的用户偏好与任务相关。虽然总体上两模型的“有用性”评分差异不大，性格分层后偏好差异明显。

Conclusion: 性格类型显著影响用户对LLM的选择，传统总体评分可能掩盖了这些细节，个性化模型匹配或评估对提升用户满意度具有重要意义。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [88] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: 本文提出了QZhou-Embedding，一种通用的上下文文本嵌入模型，在MTEB和CMTEB等基准数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型在表示能力和下游任务表现上仍有限，亟需更强大的模型和高质量多样化的数据，以推动检索类任务的发展。

Method: 作者基于Qwen2.5-7B-Instruct模型，设计了统一多任务框架和专门的数据转换与训练策略。提出了多样化数据转换方案，提升训练数据的丰富性，并利用任务特定训练提升模型学习效率。采用了依托LLM-API的数据合成管道，包括同义改写、数据增强及生成困难负样本。训练流程分为检索预训练和全任务微调两个阶段。

Result: 模型在MTEB和CMTEB基准集上均排名第一，且在重排序、聚类等多个任务上取得SOTA成绩。

Conclusion: 高质量和多样化数据对提升检索模型至关重要，利用大模型的生成能力能进一步优化嵌入模型表现。QZhou-Embedding模型及其权重、评测代码均已开放。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [89] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 该论文提出并公开了一个用于检测具有误导性可视化图表的数据集Misviz及其合成版Misviz-synth，同时评估了多种检测方法，并提供了相关代码。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和网络上的误导性可视化容易导致信息误导，而现有AI模型缺乏大规模、真实、公开的训练数据集，限制了其检测能力。

Method: 作者提出了Misviz（2604个真实可视化实例，带有12种误导类型注释）和Misviz-synth（基于真实数据表合成的81814个图表），并用多模态大模型、基于规则的方法及微调分类器等对其进行了全面评测。

Result: 实验结果表明，即使是最新的多模态大型语言模型、基于规则的方法和专用分类器，在检测误导性可视化任务上依然面临很大挑战。

Conclusion: Misviz及Misviz-synth为该领域提供了宝贵的资源，可推动误导性可视化检测相关AI模型和方法的发展。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [90] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 提出了一种新的大模型多任务微调方法Core Parameter Isolation Fine-Tuning（CPI-FT），可以缓解任务间的互相干扰和遗忘问题，相比传统方法有更优表现。


<details>
  <summary>Details</summary>
Motivation: 传统SFT在多任务适应时，参数更新往往导致'跷跷板现象'，即优化某些任务时会损害其它任务表现；因此需要更精细的参数管理方法以减少任务间干扰。

Method: 1. 单独对每个任务微调LLM，量化参数更新幅度，识别该任务的核心参数区域；2. 基于核心参数重叠度对任务聚类分组；3. 提出参数融合技术，将每个任务的核心参数直接移植到统一主干模型，非核心参数则用球面线性插值（SLERP）融合，减少干扰；4. 使用混合任务数据的轻量级流水线微调阶段，同时冻结已识别的核心区域，防止遗忘。

Result: 在多个公开基准测试上实验证明，该方法能够显著缓解任务间的干扰和遗忘问题，性能优于传统SFT基线。

Conclusion: CPI-FT方法为多任务大模型微调提供了一种有效的参数隔离与融合策略，有效提升模型整体性能，可作为多任务SFT的新范式。

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [91] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出了推理密集型回归（RiR）的概念，并建立了初步基准，提出MENTAT方法，显著提升了模型在RiR任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在简单的文本回归任务中表现良好，但在需要复杂推理与有限标注数据的场景（如评分、专域信息提取）上，其能力尚未得到充分验证。论文希望对这类更具挑战性的推理密集任务进行专项研究。

Method: 作者将三种现实问题建模为RiR任务，作为基准进行实验，比较了直接LLM提示与Transformer微调的效果。针对两者均表现有限的现象，提出了MENTAT方法，将批量反思型提示优化与神经集成学习结合，以提升推理能力。

Result: MENTAT在多项RiR任务上比传统的LLM推理和微调方法最高提升了65%。

Conclusion: MENTAT在推理密集型回归任务上显著优于传统方法，但RiR仍具有很大发展空间，未来需要持续优化。

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [92] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: 本文提出了一种新的候选解评分方法PiCSAR，用于提升大模型在推理任务中的表现，无需额外训练，通过计算推理过程和最终答案的联合概率打分，有效提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 在大模型推理任务中，如何无须标签答案而选出正确的推理链和答案，是提升模型效果的关键难题。现有评分方法受限于评分函数设计并且采样效率较低。

Method: 作者提出PiCSAR方法，对每个候选解，根据其推理链和最终答案的联合对数似然打分，将推理置信度与答案置信度结合起来，最终选择得分最高的结果；该方法无需额外训练。

Result: PiCSAR在MATH500和AIME2025等多个基准集上，准确率提升显著（如MATH500提升+10.18分），且能用更少的样本数超过现有方法，在20组实验中的16组表现更优。

Conclusion: 通过分析发现，正确推理链在推理和答案置信度上都更高，验证了PiCSAR方法的有效性和合理性。

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [93] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出基于ElasticSearch的数据分析框架，实现了对大型LLM训练数据集的高效索引与快速查询。


<details>
  <summary>Details</summary>
Motivation: 目前主流大语言模型（LLM）大量依赖如Common Crawl等网络数据集，但网络抓取数据存在质量、安全和伦理问题。以往相关有害内容的分析仅限于小样本，不能反映整体数据质量，亟需更高效的分析工具。

Method: 作者设计并实现了一个基于ElasticSearch的数据管道框架，对瑞士AI的FineWeb-2（1.5TB，四语言）数据集进行索引和高效查询。该框架支持实时分析，查询响应时间大部分在毫秒级，全部小于2秒。

Result: 实验在FineWeb-2数据集上验证，系统能够实时高效地进行数据查询和分析，提升了对大规模训练集的可控性和透明度。

Conclusion: 该框架为LLM训练数据的质量、安全和可追溯性分析提供了实用工具，有助于构建更安全、更负责任的AI系统。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [94] [EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](https://arxiv.org/abs/2508.21112)
*Delin Qu,Haoming Song,Qizhi Chen,Zhaoqing Chen,Xianqiang Gao,Xinyi Ye,Qi Lv,Modi Shi,Guanghui Ren,Cheng Ruan,Maoqing Yao,Haoran Yang,Jiacheng Bao,Bin Zhao,Dong Wang*

Main category: cs.RO

TL;DR: 提出了一种新的统一多模态机器人基础模型EO-1和配套的大规模数据集EO-Data1.5M，显著提升了机器人在开放世界环境下的推理与操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型虽然在机器人领域取得进展，但在多模态、交错推理与灵活互动方面距离人类能力仍有较大差距，需要更强大、统一的基础模型支持。

Method: 提出EO-Robotics框架，包括：1）EO-1，一个可以无差别处理图像、文本、视频、动作等多模态输入的统一架构；2）EO-Data1.5M，包含超过150万条注重视觉、文本与动作交错理解的大规模、高质量多模态数据集。采用自回归解码和流匹配去噪相结合的训练策略。

Result: EO-1模型在多种长时序、灵巧操作任务和多种机器人形态上均展现出优越的泛化与推理能力，通过多项实验验证了视觉-文本-动作交错学习带来的效果提升。

Conclusion: 论文为多模态机器人推理与操控提出了强大的基础模型和数据平台，为后续通用型智能机器体的开发提供了新思路和有价值的实践指导。

Abstract: The human ability to seamlessly perform multimodal reasoning and physical
interaction in the open world is a core goal for general-purpose embodied
intelligent systems. Recent vision-language-action (VLA) models, which are
co-trained on large-scale robot and visual-text data, have demonstrated notable
progress in general robot control. However, they still fail to achieve
human-level flexibility in interleaved reasoning and interaction. In this work,
introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is
a unified embodied foundation model that achieves superior performance in
multimodal embodied reasoning and robot control through interleaved
vision-text-action pre-training. The development of EO-1 is based on two key
pillars: (i) a unified architecture that processes multimodal inputs
indiscriminately (image, text, video, and action), and (ii) a massive,
high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains
over 1.5 million samples with emphasis on interleaved vision-text-action
comprehension. EO-1 is trained through synergies between auto-regressive
decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot
action generation and multimodal embodied reasoning. Extensive experiments
demonstrate the effectiveness of interleaved vision-text-action learning for
open-world understanding and generalization, validated through a variety of
long-horizon, dexterous manipulation tasks across multiple embodiments. This
paper details the architecture of EO-1, the data construction strategy of
EO-Data1.5M, and the training methodology, offering valuable insights for
developing advanced embodied foundation models.

</details>


### [95] [Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence](https://arxiv.org/abs/2508.21163)
*Tarek Bouazza,Soulaimane Berkane,Minh-Duc Hua,Tarek Hamel*

Main category: cs.RO

TL;DR: 本论文提出了一种级联观测器架构，通过融合光流和IMU数据，实现单目的持续视觉-惯性里程计（VIO）。该方法能同步估计机体系速度和重力方向，并保证全局指数稳定性。仿真验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉-惯性里程计需要同时估计姿态与速度，但单目系统存在尺度和方向的不确定性，同时姿态估计易受漂移影响，因此需要更可靠和稳定的融合方法。

Method: 1. 使用光流测量提供的速度方向，与IMU中的陀螺仪和加速度计数据进行融合。2. 应用全局指数稳定的Riccati观测器，实现在持续激励的平移运动条件下对速度和重力方向的估计。3. 估计得到的重力方向，结合可选的磁力计数据，设计了基于SO(3)的互补观测器，用于姿态估计。4. 为了从稀疏光流数据中提取速度方向，开发了基于单位球约束的梯度下降算法。5. 通过仿真进行效果验证。

Result: 所提方法在模糊光流和IMU测量条件下实现了几乎全局渐近稳定的估计。仿真结果表明，级联观测器结构性能优异，能够准确融合光流和惯性数据。

Conclusion: 该级联观测器架构能有效提升单目VIO的稳定性和估计精度，尤其适用于实际运动条件下对速度与姿态的实时连续估计，具有良好应用前景。

Abstract: This paper presents a novel cascaded observer architecture that combines
optical flow and IMU measurements to perform continuous monocular
visual-inertial odometry (VIO). The proposed solution estimates body-frame
velocity and gravity direction simultaneously by fusing velocity direction
information from optical flow measurements with gyro and accelerometer data.
This fusion is achieved using a globally exponentially stable Riccati observer,
which operates under persistently exciting translational motion conditions. The
estimated gravity direction in the body frame is then employed, along with an
optional magnetometer measurement, to design a complementary observer on
$\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer
architecture is shown to be almost globally asymptotically stable. To extract
the velocity direction from sparse optical flow data, a gradient descent
algorithm is developed to solve a constrained minimization problem on the unit
sphere. The effectiveness of the proposed algorithms is validated through
simulation results.

</details>


### [96] [Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)](https://arxiv.org/abs/2508.21205)
*Usman A. Khan,Mouhacine Benosman,Wenliang Liu,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: 本文提出了一种基于最优输运理论和模型预测控制的多机器人导航路径规划与调度新方法，可实现最优且无重叠的轨迹。针对具有障碍的空间，将空间离散为若干网格，通过特定代价结构与最优输运，实现机器人到目标的非重叠路径。同时，通过重规划与模型预测控制，兼顾动态情况和必要的轨迹重叠。


<details>
  <summary>Details</summary>
Motivation: 多机器人分配目标后直接路径规划会导致路径重叠乃至死锁问题。为此，未来实际部署亟需一种能高效提供无重叠最优导航轨迹的调度与路径算法。

Method: 首先基于最优输运理论，将空间离散为K个网格单元，设定${K×K}$的转移代价结构，通过最优输运提供每个机器人到目标的最小代价且不重叠的路径分配。如遇轨迹不可避免重叠，通过引入重规划与模型预测控制，兼顾机器人动力学和时间因素。

Result: 该算法能保证机器人获得最小代价且不重叠的路径，避免死锁。算法复杂度最坏为O(K^3 log K)，良好情况下为O(K^2 log K)。在部分场景下可通过时序优化结合模型预测控制，兼容必然发生的轨迹重叠和动力学约束。

Conclusion: 本文方法有效解决了多机器人分配与调度中的路径重叠与死锁问题，提供了一种理论上最优且具备工程实用性的规划调度框架，兼顾代价、动力学和时序调度问题。

Abstract: In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.

</details>


### [97] [Uncertainty-Aware Ankle Exoskeleton Control](https://arxiv.org/abs/2508.21221)
*Fatima Mumtaza Tourk,Bishoy Galoaa,Sanat Shajan,Aaron J. Young,Michael Everett,Max K. Shepherd*

Main category: cs.RO

TL;DR: 本文提出了一种具备不确定性感知能力的踝关节外骨骼控制框架，能够根据用户的动作自动判别是否安全辅助，显著提升外骨骼在真实复杂环境中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的下肢外骨骼主要依赖于为特定、预先设定的动作设计的控制器，这限制了其在真实、动态环境中的应用，因为无法安全应对未见过的动作类型。

Method: 本研究提出了结合不确定性估计器的控制框架，该框架能够识别用户动作是否在训练集已知范围（分布内）或超出已知范围（分布外），从而自动切换外骨骼辅助模式。作者评估了三种不确定性估计架构（模型集成、自编码器和生成对抗网络），并在离线数据集上进行了测试，最终选取性能最好的模型集成方法在线上进行了实验。

Result: 在线实验显示，模型集成的不确定性估计器能够准确地在用户动作切换于分布内/外任务时自动启停外骨骼辅助，F1分数为89.2，表现出高效的判别能力。

Conclusion: 该工作首次实现了基于不确定性感知的外骨骼自动安全辅助控制，为外骨骼在日常复杂环境下的智能自主应用提供了实际可能。

Abstract: Lower limb exoskeletons show promise to assist human movement, but their
utility is limited by controllers designed for discrete, predefined actions in
controlled environments, restricting their real-world applicability. We present
an uncertainty-aware control framework that enables ankle exoskeletons to
operate safely across diverse scenarios by automatically disengaging when
encountering unfamiliar movements. Our approach uses an uncertainty estimator
to classify movements as similar (in-distribution) or different
(out-of-distribution) relative to actions in the training set. We evaluated
three architectures (model ensembles, autoencoders, and generative adversarial
networks) on an offline dataset and tested the strongest performing
architecture (ensemble of gait phase estimators) online. The online test
demonstrated the ability of our uncertainty estimator to turn assistance on and
off as the user transitioned between in-distribution and out-of-distribution
tasks (F1: 89.2). This new framework provides a path for exoskeletons to safely
and autonomously support human movement in unstructured, everyday environments.

</details>


### [98] [Remarks on stochastic cloning and delayed-state filtering](https://arxiv.org/abs/2508.21260)
*Tara Mina,Lindsey Marinello,John Christian*

Main category: cs.RO

TL;DR: 本文重新探讨了在机器人和导航中的延时状态估计问题，通过分析发现，适当设计的延时卡尔曼滤波器（delayed-state Kalman filter）无需扩展状态向量即可与随机克隆法（stochastic cloning）达到相同效果，并具备更高的计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 在机器人导航等领域中，诸如里程计等测量依赖于之前的状态，需要处理由延迟测量引入的相关性。传统的随机克隆法虽然能解决这一问题，但伴随显著的计算和内存开销。作者希望寻找无需扩展状态的方法，并澄清关于卡尔曼滤波器不能高效处理延迟测量的误解。

Method: 作者系统性地分析了延时卡尔曼滤波器的推导过程，并与随机克隆法进行了理论上的精确比较，证明二者在状态和协方差的更新结果完全等价。同时评估了该方法在高维状态下的计算与存储需求。

Result: 经过严格的理论推导与实验对比，作者表明延时卡尔曼滤波器在不增加状态维数的情况下，能实现与随机克隆法完全相同的状态与协方差更新。此外，该方法在高维场景下展现出明显的计算和内存优势。

Conclusion: 作者打破了关于卡尔曼滤波器不能高效处理延迟相关测量的误区，提出了一种更为高效且等价的滤波方法，为复杂机器人与导航应用中的延时状态估计提供了理论与工程上的新思路。

Abstract: Many estimation problems in robotics and navigation involve measurements that
depend on prior states. A prominent example is odometry, which measures the
relative change between states over time. Accurately handling these
delayed-state measurements requires capturing their correlations with prior
state estimates, and a widely used approach is stochastic cloning (SC), which
augments the state vector to account for these correlations.
  This work revisits a long-established but often overlooked alternative--the
delayed-state Kalman filter--and demonstrates that a properly derived filter
yields exactly the same state and covariance update as SC, without requiring
state augmentation. Moreover, the generalized Kalman filter formulation
provides computational advantages, while also reducing memory requirements for
higher-dimensional states.
  Our findings clarify a common misconception that Kalman filter variants are
inherently unable to handle correlated delayed-state measurements,
demonstrating that an alternative formulation achieves the same results more
efficiently.

</details>


### [99] [Mini Autonomous Car Driving based on 3D Convolutional Neural Networks](https://arxiv.org/abs/2508.21271)
*Pablo Moraes,Monica Rodriguez,Kristofer S. Kappel,Hiago Sodre,Santiago Fernandez,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: 本论文提出了一种基于RGB-D信息和三维卷积神经网络（3D CNNs）的自主控制方法，应用于Mini Autonomous Cars (MACs)在仿真环境下的自动驾驶测试，并与RNN方法进行了比较。该方法在任务完成率、圈速和驾驶一致性等方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着人们对智能驾驶辅助功能需求的增长，自动驾驶的安全性、效率和用户体验变得日益重要。然而，研发可靠的自动驾驶系统面临高复杂度、训练周期长和不确定性大等挑战。为了便于测试和验证，MACs作为小规模、经济高效的实验平台被广泛采用。

Method: 本文采用RGB-D传感数据和3D卷积神经网络实现MAC仿真自动驾驶，并将其与传统的循环神经网络（RNN）方法在两个不同仿真轨道上进行了训练和测试，通过任务完成率、圈速、驾驶一致性等指标对模型性能进行评估。

Result: 结果表明，3D CNN在自动驾驶任务中表现出较好的泛化能力和控制表现，并且在某些方面优于RNN。轨道复杂度和网络架构调整对模型的性能有显著影响。

Conclusion: 基于3D CNN和RGB-D输入的自主控制方法在MAC自动驾驶仿真环境中效果良好，可作为未来自动驾驶控制方法的有力候选。

Abstract: Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.

</details>


### [100] [Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609](https://arxiv.org/abs/2508.21272)
*Jaehong Oh,Seungjun Jung,Sawoong Kim*

Main category: cs.RO

TL;DR: 本文首次在带有欠驱动夹爪的6自由度协作机器人上，综合应用了法合法动作屏蔽的DQN与安全再抓取策略，实现了索玛积木的自主装配学习，并系统性集成了约束感知强化学习与奇异点安全运动规划。通过分层Q函数估算及合理奖励设计，大幅提升了训练效率和装配成功率。


<details>
  <summary>Details</summary>
Motivation: 协作机器人在自动装配过程中面临组合动作空间爆炸、不安全运动规划与系统性装配策略学习等挑战，尤其在处理欠驱动末端执行器时难度更大，因此亟需更安全、高效的算法解决这些实际问题。

Method: 作者提出了结合法合法动作屏蔽DQN与安全ZYZ再抓策略的方法。以层次化Q函数估算分别处理位姿与位置，降低动作空间复杂度，并引入符合法规约束的奖励函数，引导装配顺序。采用课程学习法，分三阶段递进学习索玛积木装配任务。

Result: 在2件、3件、7件装配任务下系统取得了高效训练成果：500轮即可在第一阶段达到100%成功率，第二阶段达92.9%，第三阶段达39.9%。同时显著降低了计算与探索复杂度。

Conclusion: 本方法实现了协作机器人索玛积木自动装配的高效、安全与可扩展自主学习。所提系统在处理装配类高维动作场景及欠驱动夹爪时具备良好泛化性，为未来复杂协作自动装配任务提供了新思路。

Abstract: This paper presents the first comprehensive application of legal-action
masked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated
gripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly
learning. Our approach represents the first systematic integration of
constraint-aware reinforcement learning with singularity-safe motion planning
on a Doosan M0609 collaborative robot. We address critical challenges in
robotic manipulation: combinatorial action space explosion, unsafe motion
planning, and systematic assembly strategy learning. Our system integrates a
legal-action masked DQN with hierarchical architecture that decomposes
Q-function estimation into orientation and position components, reducing
computational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining
solution completeness. The robot-friendly reward function encourages
ground-first, vertically accessible assembly sequences aligned with
manipulation constraints. Curriculum learning across three progressive
difficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training
efficiency: 100\% success rate for Level 1 within 500 episodes, 92.9\% for
Level 2, and 39.9\% for Level 3 over 105,300 total training episodes.

</details>


### [101] [Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking](https://arxiv.org/abs/2508.21309)
*Seyed Ali Rakhshan,Mehdi Golestani,He Kong*

Main category: cs.RO

TL;DR: 该论文提出了一种针对异质传感器多机器人协同分配以实现多目标跟踪的分配算法，对不同能力的机器人进行分类，并采用贪心算法动态分配以提升跟踪质量。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪任务中，异质（能力不同）的机器人如何更高效协作以最小化目标状态估计的不确定性，是实际场景中亟需解决的问题。尤其是部分机器人仅具备单一感知能力，需协作方能完成目标跟踪。

Method: 作者将机器人分为具备全套（距离+方位）传感器的‘充分感知’机器人和只能单独或结对使用的‘有限感知’机器人。采用垒集理论，设计了一个贪心分配算法，将机器人动态分配给目标，提升跟踪质量。该算法理论上在任意质量函数下有1/3近似保证，在子模函数下达1/2，并保持多项式时间复杂度。

Result: 大量仿真实验证明，所提算法能够长时间地准确估计和跟踪目标，数值结果显示，算法性能接近最优分配方案，具有较强稳健性。

Conclusion: 该动态分配算法在理论和实验上均能有效提升多目标跟踪的分配效率和质量，在实际的机器人多目标跟踪场景中具有良好的应用前景。

Abstract: This paper addresses the challenge of assigning heterogeneous sensors (i.e.,
robots with varying sensing capabilities) for multi-target tracking. We
classify robots into two categories: (1) sufficient sensing robots, equipped
with range and bearing sensors, capable of independently tracking targets, and
(2) limited sensing robots, which are equipped with only range or bearing
sensors and need to at least form a pair to collaboratively track a target. Our
objective is to optimize tracking quality by minimizing uncertainty in target
state estimation through efficient robot-to-target assignment. By leveraging
matroid theory, we propose a greedy assignment algorithm that dynamically
allocates robots to targets to maximize tracking quality. The algorithm
guarantees constant-factor approximation bounds of 1/3 for arbitrary tracking
quality functions and 1/2 for submodular functions, while maintaining
polynomial-time complexity. Extensive simulations demonstrate the algorithm's
effectiveness in accurately estimating and tracking targets over extended
periods. Furthermore, numerical results confirm that the algorithm's
performance is close to that of the optimal assignment, highlighting its
robustness and practical applicability.

</details>


### [102] [Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty](https://arxiv.org/abs/2508.21322)
*Haojie Bai,Yang Wang,Cong Guo,Xiongwei Zhao,Hai Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的协同车辆协调框架，采用分布式优化与注意力机制兼顾安全性和实时性能，显著提升了多车协同效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在动态与不确定环境下，多辆车辆协同时很难同时保证安全性和实时性；现有方法往往在两者之间难以兼顾。因此，需要一种能同时优化安全与效率的新框架。

Method: 提出了三项创新：(1) 直接控制车辆轨迹分布，将鲁棒规划与自适应安全约束结合，实现对不确定性轨迹的指定安全水平。(2) 设计了完全并行的基于ADMM的分布式轨迹协商算法（ADMM-DTN），可配置协商轮数以平衡解的质量与计算资源。(3) 融入交互注意力机制，聚焦关键参与者，提升计算效率。方法通过仿真和真实实验验证。

Result: 该框架在多种场景下将碰撞率降低了最高40.79%，计算需求也因注意力机制减少了14.1%，在车辆数量增加下亦保持良好扩展性。仿真和真实动态障碍实验均证明了方法有效。

Conclusion: 文中系统验证了所提方法的安全性、实时性和可扩展性均优于现有技术，适合复杂多车环境下的高效安全协同。

Abstract: Achieving both safety guarantees and real-time performance in cooperative
vehicle coordination remains a fundamental challenge, particularly in dynamic
and uncertain environments. This paper presents a novel coordination framework
that resolves this challenge through three key innovations: 1) direct control
of vehicles' trajectory distributions during coordination, formulated as a
robust cooperative planning problem with adaptive enhanced safety constraints,
ensuring a specified level of safety regarding the uncertainty of the
interactive trajectory, 2) a fully parallel ADMM-based distributed trajectory
negotiation (ADMM-DTN) algorithm that efficiently solves the optimization
problem while allowing configurable negotiation rounds to balance solution
quality and computational resources, and 3) an interactive attention mechanism
that selectively focuses on critical interactive participants to further
enhance computational efficiency. Both simulation results and practical
experiments demonstrate that our framework achieves significant advantages in
safety (reducing collision rates by up to 40.79\% in various scenarios) and
real-time performance compared to state-of-the-art methods, while maintaining
strong scalability with increasing vehicle numbers. The proposed interactive
attention mechanism further reduces the computational demand by 14.1\%. The
framework's effectiveness is further validated through real-world experiments
with unexpected dynamic obstacles, demonstrating robust coordination in complex
environments. The experiment demo could be found at
https://youtu.be/4PZwBnCsb6Q.

</details>


### [103] [Multi-Modal Model Predictive Path Integral Control for Collision Avoidance](https://arxiv.org/abs/2508.21364)
*Alberto Bertipaglia,Dariu M. Gavrila,Barys Shyrokau*

Main category: cs.RO

TL;DR: 本文提出了一种基于多模态模型预测路劲积分控制（MPPI）的自动驾驶车辆运动规划与决策方法，通过对前一输入使用Sobol序列采样，并引入解析避碰方案，实现避障和安全停车等多样化驾驶策略，显著提升现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂环境下容易面临本地最优、避障不充分和车辆动力学不稳定等难题，现有单模态方法在应对多种突发情况（如动态障碍物和多变路面条件）时表现有限。因此，亟需一种能探索多种应对策略并确保车辆稳定性的运动规划方法。

Method: 本文提出采用多模态MPPI方法，利用Sobol序列围绕先前控制输入高效采样，集成解析避碰策略，通过非线性单轨车辆—Fiala轮胎模型进行轨迹预测，并强制摩擦圆内轮胎力约束以确保操稳性，最终优化转向角和纵向加速度生成无碰撞轨迹。

Result: 在高保真仿真环境下，该方法在多种路面摩擦条件、动态障碍物遮挡情况下的双车道变换测试中，成功避障并保持车辆稳定性，性能优于标准单模态MPPI方法。

Conclusion: 多模态MPPI算法能有效提升自动驾驶车辆对障碍物和复杂环境的适应能力，在避障性能和车辆稳定性方面明显优于传统方法，具有良好的推广应用前景。

Abstract: This paper proposes a novel approach to motion planning and decision-making
for automated vehicles, using a multi-modal Model Predictive Path Integral
control algorithm. The method samples with Sobol sequences around the prior
input and incorporates analytical solutions for collision avoidance. By
leveraging multiple modes, the multi-modal control algorithm explores diverse
trajectories, such as manoeuvring around obstacles or stopping safely before
them, mitigating the risk of sub-optimal solutions. A non-linear single-track
vehicle model with a Fiala tyre serves as the prediction model, and tyre force
constraints within the friction circle are enforced to ensure vehicle stability
during evasive manoeuvres. The optimised steering angle and longitudinal
acceleration are computed to generate a collision-free trajectory and to
control the vehicle. In a high-fidelity simulation environment, we demonstrate
that the proposed algorithm can successfully avoid obstacles, keeping the
vehicle stable while driving a double lane change manoeuvre on high and
low-friction road surfaces and occlusion scenarios with moving obstacles,
outperforming a standard Model Predictive Path Integral approach.

</details>


### [104] [Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation](https://arxiv.org/abs/2508.21375)
*Anuj Pasricha,Joewie Koh,Jay Vakil,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文指出目前机器人标称负载能力评估过于保守，导致资源浪费，并提出了一种基于扩散模型的新型轨迹生成方法，可在保证动力学约束下实时生成高效轨迹，使机器人在大范围工作空间内处理大于标称负载的物体成为可能。


<details>
  <summary>Details</summary>
Motivation: 现有机械臂负载能力评估采用最保守的工况，造成整个工作空间的负载限制过于严格，实际能力被极大低估，从而影响了机械臂的高效利用。作者希望突破这一限制，实现合理评估并充分利用机器人能力。

Method: 提出利用去噪扩散模型，将负载约束直接融入轨迹规划过程。与传统基于采样或优化的方法相比，该方法能在恒定时间内生成满足力学和动力学约束的关节空间轨迹，无需后处理，且效率更高。

Result: 在Franka Emika Panda 7自由度机械臂上的实验证明，即使负载超过标称值3倍，仍有高达67.6%的工作空间可达，显著提高了设备的应用范围。

Conclusion: 该工作展示了通过动态约束下的轨迹生成，可显著拓展机械臂的负载和工作空间边界，强调运动规划算法中对负载动力学更细致考量的必要性。

Abstract: Nominal payload ratings for articulated robots are typically derived from
worst-case configurations, resulting in uniform payload constraints across the
entire workspace. This conservative approach severely underutilizes the robot's
inherent capabilities -- our analysis demonstrates that manipulators can safely
handle payloads well above nominal capacity across broad regions of their
workspace while staying within joint angle, velocity, acceleration, and torque
limits. To address this gap between assumed and actual capability, we propose a
novel trajectory generation approach using denoising diffusion models that
explicitly incorporates payload constraints into the planning process. Unlike
traditional sampling-based methods that rely on inefficient trial-and-error,
optimization-based methods that are prohibitively slow, or kinodynamic planners
that struggle with problem dimensionality, our approach generates dynamically
feasible joint-space trajectories in constant time that can be directly
executed on physical hardware without post-processing. Experimental validation
on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the
workspace remains accessible even with payloads exceeding 3 times the nominal
capacity. This expanded operational envelope highlights the importance of a
more nuanced consideration of payload dynamics in motion planning algorithms.

</details>


### [105] [RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation](https://arxiv.org/abs/2508.21378)
*Chenduo Ying,Linkang Du,Peng Cheng,Yuanchao Shu*

Main category: cs.RO

TL;DR: 本文提出了RoboInspector，用于分析和提升大语言模型驱动机器人操作时策略代码生成的可靠性。通过168种任务和指令的实验发现并归纳了四种导致失败的不可靠行为，并提出针对性的反馈改进方法，将策略代码的可靠性提升了35%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备强大的代码生成和推理能力，但自动生成的机器人操作策略代码在现实任务中经常因任务复杂性和用户指令多样性导致不可靠，影响实际应用。因此，研究如何揭示、分析并改善这些不可靠性至关重要。

Method: 设计了RoboInspector流程，从任务复杂性和指令粒度两个角度揭示和表征策略代码的不可靠性，通过对两大框架下168组任务、指令和模型的实验，总结出策略代码失效的四种主要行为，并分析其原因。同时，提出基于失败代码反馈的细化方法，对策略代码进行改进。

Result: 通过RoboInspector，系统性识别出策略代码生成的四类主要不可靠行为，并详述其成因。引入的基于失败反馈的改进方法，在仿真和实际场景下将机器人策略代码的可靠性提升了最高35%。

Conclusion: 大语言模型在机器人操作代码生成上存在显著的不可靠性，通过RoboInspector能够有效揭示问题类型，并通过失败反馈推动代码优化，大幅提升自动策略代码的可靠性，为未来实际部署和进一步研究提供了理论与实践基础。

Abstract: Large language models (LLMs) demonstrate remarkable capabilities in reasoning
and code generation, enabling robotic manipulation to be initiated with just a
single instruction. The LLM carries out various tasks by generating policy code
required to control the robot. Despite advances in LLMs, achieving reliable
policy code generation remains a significant challenge due to the diverse
requirements of real-world tasks and the inherent complexity of user
instructions. In practice, different users may provide distinct instructions to
drive the robot for the same task, which may cause the unreliability of policy
code generation. To bridge this gap, we design RoboInspector, a pipeline to
unveil and characterize the unreliability of the policy code for LLM-enabled
robotic manipulation from two perspectives: the complexity of the manipulation
task and the granularity of the instruction. We perform comprehensive
experiments with 168 distinct combinations of tasks, instructions, and LLMs in
two prominent frameworks. The RoboInspector identifies four main unreliable
behaviors that lead to manipulation failure. We provide a detailed
characterization of these behaviors and their underlying causes, giving insight
for practical development to reduce unreliability. Furthermore, we introduce a
refinement approach guided by failure policy code feedback that improves the
reliability of policy code generation by up to 35% in LLM-enabled robotic
manipulation, evaluated in both simulation and real-world environments.

</details>


### [106] [Assessing Human Cooperation for Enhancing Social Robot Navigation](https://arxiv.org/abs/2508.21455)
*Hariharan Arunachalam,Phani Teja Singamaneni,Rachid Alami*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，通过在合适时机与人进行有效交流，提高机器人在社交环境中的导航能力，尤其是在面对面交错时。


<details>
  <summary>Details</summary>
Motivation: 现有的社交导航主要依赖于人类轨迹预测，但在遇到人类行为异常时效果有限，原因是机器人无法理解人类意图，且人类也不了解机器人的计划。为弥补这一认知与互动的空白，提升安全与协作，文章探讨了通过交流弥补这一不足的可能性。

Method: 本文针对面对面交错场景，通过对环境几何特征和人类合作意愿的分析，制定交互时机，并采用幾何推理来判断并生成合适的语言或动作反馈。提出了一套评估方法和指标，用于区分合作型和非合作型人类。

Result: 研究展示了所提方法能够有效识别人类合作程度，帮助机器人实现适时有效的交流，以改善社交导航效果。并通过几何推理自动生成合适的机器人响应。

Conclusion: 通过在恰当时机主动交流并结合几何情境分析，机器人可更好地理解人类意图和合作性，从而提升社交导航的鲁棒性和人机交互体验。

Abstract: Socially aware robot navigation is a planning paradigm where the robot
navigates in human environments and tries to adhere to social constraints while
interacting with the humans in the scene. These navigation strategies were
further improved using human prediction models, where the robot takes the
potential future trajectory of humans while computing its own. Though these
strategies significantly improve the robot's behavior, it faces difficulties
from time to time when the human behaves in an unexpected manner. This happens
as the robot fails to understand human intentions and cooperativeness, and the
human does not have a clear idea of what the robot is planning to do. In this
paper, we aim to address this gap through effective communication at an
appropriate time based on a geometric analysis of the context and human
cooperativeness in head-on crossing scenarios. We provide an assessment
methodology and propose some evaluation metrics that could distinguish a
cooperative human from a non-cooperative one. Further, we also show how
geometric reasoning can be used to generate appropriate verbal responses or
robot actions.

</details>


### [107] [Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting](https://arxiv.org/abs/2508.21501)
*Pierrick Lorang,Hong Lu,Johannes Huemer,Patrik Zips,Matthias Scheutz*

Main category: cs.RO

TL;DR: 本文提出了一种神经符号融合框架，仅需少量演示即可让智能体习得复杂行为，并在多个任务和环境下展现出极高的数据效率及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习方法通常只适用于短时任务，且依赖大量数据，难以应对长时任务或任务变异、分布漂移。这促使作者寻求一种既能高效泛化，又能高效利用数据的新框架。

Method: 本方法将高层任务结构抽象为图，通过Answer Set Programming求解器发掘符号规则，利用扩散策略模仿学习训练底层连续控制策略。高层oracle筛选任务相关信息，从而极大减少每个控制器所需的观测和动作空间。

Result: 在堆叠、厨房、装配、河内塔及自动化叉车等六个不同领域的多个环境下实验，五次示范即可取得优异表现，展现出零次或少次样本下的强泛化能力和解释性决策。

Conclusion: 新框架不仅实现了极高数据效率和优良泛化能力，还能学到传统无监督方法难以捕捉的复杂状态转移关系，推动了模仿学习的应用边界。

Abstract: Imitation learning enables intelligent systems to acquire complex behaviors
with minimal supervision. However, existing methods often focus on
short-horizon skills, require large datasets, and struggle to solve
long-horizon tasks or generalize across task variations and distribution
shifts. We propose a novel neuro-symbolic framework that jointly learns
continuous control policies and symbolic domain abstractions from a few skill
demonstrations. Our method abstracts high-level task structures into a graph,
discovers symbolic rules via an Answer Set Programming solver, and trains
low-level controllers using diffusion policy imitation learning. A high-level
oracle filters task-relevant information to focus each controller on a minimal
observation and action space. Our graph-based neuro-symbolic framework enables
capturing complex state transitions, including non-spatial and temporal
relations, that data-driven learning or clustering techniques often fail to
discover in limited demonstration datasets. We validate our approach in six
domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers
of Hanoi environments, and a distinct Automated Forklift domain with two
environments. The results demonstrate high data efficiency with as few as five
skill demonstrations, strong zero- and few-shot generalizations, and
interpretable decision making.

</details>


### [108] [Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler](https://arxiv.org/abs/2508.21549)
*Liding Zhang,Kuanqi Cai,Yu Zhang,Zhenshan Bing,Chaoqun Wang,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 该论文提出了一种新型路径规划算法MIT*，通过利用多重启发式信息集和自适应采样策略，提高高维空间中路径规划的收敛速度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于采样的路径规划算法，在初始未找到解时需要重新采样整个空间，效率低下，特别是在高维和受限场景下。为了提升初始解的收敛速度并降低计算消耗，亟需新的方法。

Method: 论文提出了MIT*算法，在找到初始解之前利用先验可容许解的成本构建估计的启发式信息集，加快路径搜索。MIT*还引入了动态自适应采样器，采样策略随探索过程自适应调整，并使用基于路径长度的稀疏碰撞检测来辅助搜索，提高搜索效率。

Result: 通过仿真和实际机器人操作实验，MIT*在4维到16维问题上均优于现有单次查询的采样规划器，实现了更快的收敛、更优的路径代价和更高的成功率，已成功应用于真实机器人操作任务。

Conclusion: MIT*适用于高维空间中复杂、受限的路径规划任务，能显著提升求解效率和路径质量，是较现有采样规划器更有效的替代方案。

Abstract: Path planning in robotics often involves solving continuously valued,
high-dimensional problems. Popular informed approaches include graph-based
searches, such as A*, and sampling-based methods, such as Informed RRT*, which
utilize informed set and anytime strategies to expedite path optimization
incrementally. Informed sampling-based planners define informed sets as subsets
of the problem domain based on the current best solution cost. However, when no
solution is found, these planners re-sample and explore the entire
configuration space, which is time-consuming and computationally expensive.
This article introduces Multi-Informed Trees (MIT*), a novel planner that
constructs estimated informed sets based on prior admissible solution costs
before finding the initial solution, thereby accelerating the initial
convergence rate. Moreover, MIT* employs an adaptive sampler that dynamically
adjusts the sampling strategy based on the exploration process. Furthermore,
MIT* utilizes length-related adaptive sparse collision checks to guide lazy
reverse search. These features enhance path cost efficiency and computation
times while ensuring high success rates in confined scenarios. Through a series
of simulations and real-world experiments, it is confirmed that MIT*
outperforms existing single-query, sampling-based planners for problems in R^4
to R^16 and has been successfully applied to real-world robot manipulation
tasks. A video showcasing our experimental results is available at:
https://youtu.be/30RsBIdexTU

</details>


### [109] [Learning Agile Gate Traversal via Analytical Optimal Policy Gradient](https://arxiv.org/abs/2508.21592)
*Tianchen Sun,Bingheng Wang,Longbin Tang,Yichao Gao,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种结合模型预测控制（MPC）和神经网络（NN）的混合方法，能高效且精确地引导四旋翼飞行器穿越狭窄门道，且比端到端强化学习方法在样本效率上有巨大提升。


<details>
  <summary>Details</summary>
Motivation: 传统自主飞行方案需要大量人工设计与参数调优，而端到端强化学习方法又普遍存在样本效率低和可解释性差的问题。因此，亟需一种高效且可控的新方法，以实现快速、精准、灵活的四旋翼穿门飞行。

Method: 作者提出了一种混合框架：离线训练神经网络，用于在线自适应调整MPC参数。神经网络依据当前无人机状态和门的位置，联合预测MPC参考点和代价函数权重。还引入基于优化的门检测模块，以及简明的姿态跟踪误差表达，通过解析梯度提升整体训练效率。

Result: 实验表明，该方法能让四旋翼准确、快速地穿越受限环境下的狭窄门道，且在样本效率上较传统端到端强化学习实现了数量级的提升。

Conclusion: 本文方法有效融合了传统模型控制和深度学习的优势，实现了高效、精准且实用的自主飞行，无需烦琐的手动参数调优，具有良好的应用前景。

Abstract: Traversing narrow gates presents a significant challenge and has become a
standard benchmark for evaluating agile and precise quadrotor flight.
Traditional modularized autonomous flight stacks require extensive design and
parameter tuning, while end-to-end reinforcement learning (RL) methods often
suffer from low sample efficiency and limited interpretability. In this work,
we present a novel hybrid framework that adaptively fine-tunes model predictive
control (MPC) parameters online using outputs from a neural network (NN)
trained offline. The NN jointly predicts a reference pose and cost-function
weights, conditioned on the coordinates of the gate corners and the current
drone state. To achieve efficient training, we derive analytical policy
gradients not only for the MPC module but also for an optimization-based gate
traversal detection module. Furthermore, we introduce a new formulation of the
attitude tracking error that admits a simplified representation, facilitating
effective learning with bounded gradients. Hardware experiments demonstrate
that our method enables fast and accurate quadrotor traversal through narrow
gates in confined environments. It achieves several orders of magnitude
improvement in sample efficiency compared to naive end-to-end RL approaches.

</details>


### [110] [The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics](https://arxiv.org/abs/2508.21635)
*Nicolas Soncini,Javier Cremona,Erica Vidal,Maximiliano García,Gastón Castro,Taihú Pire*

Main category: cs.RO

TL;DR: 本文公开了一个专为大豆农田场景采集的多模态数据集，包含多种传感器，旨在推进农田机器人技术发展。


<details>
  <summary>Details</summary>
Motivation: 农业机器人在复杂农田环境下对定位、导航、感知和地图构建算法提出了极高的要求。目前在农业领域缺乏高质量的多模态公开数据集，且现有的SLAM（同步定位与地图构建）方法在农田环境中表现有限。因此，作者希望通过新数据集促进相关算法的发展与评测。

Method: 搭建并部署了一个多传感器平台，在大豆农田实际环境中采集了两个多小时的数据。传感器包括立体红外相机、彩色相机、惯性测量单元（加速度计、陀螺仪、磁力计）、GNSS（单点定位、RTK和后处理）、及轮速计。数据集保证了各传感器数据的时钟同步，并提供6自由度的真实轨迹作为基准。作者还用现有主流多模态SLAM方法在该数据集上进行测试与分析。

Result: 测试结果显示当前多模态SLAM方法在应对农田环境中的自然光变、运动模糊、崎岖地形和长距离感知混淆等挑战时存在明显局限性。

Conclusion: 本数据集为农田机器人领域提供了有价值的基准资源，有助于推动多模态SLAM和相关算法在农业场景下的研究和改进。数据集及相关工具已公开发布，便于社区使用和进一步开发。

Abstract: We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.

</details>


### [111] [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](https://arxiv.org/abs/2508.21677)
*Bernhard Wullt,Johannes Köhler,Per Mattsson,Mikeal Norrlöf,Thomas B. Schön*

Main category: cs.RO

TL;DR: 本文提出了一种针对工业机械臂在复杂环境下的高效安全运动规划新方法，实现了在存在模型不确定性的情况下的快速且安全的机械臂运动。


<details>
  <summary>Details</summary>
Motivation: 工业机械臂常在杂乱环境中操作，存在碰撞风险且模型参数存在不确定性。目前实际应用中为保证安全通常牺牲速度，因此亟需可兼顾安全与速度的控制方法。

Method: 作者提出一种新型的模型预测控制（MPC）方案，融合了稳健的tube型MPC与走廊规划算法，使机械臂能生成无碰撞的运动轨迹。该MPC问题被设计为凸优化，便于快速求解。

Result: 在六自由度工业机械臂的仿真实验中，所提方法在复杂环境和模型不确定性下展现出优异性能。与基线方法相比，能承受更高的模型不确定性，且运动速度更快。

Conclusion: 该方法有效提升了机械臂在复杂和不确定环境下的工作速度和安全性，为工业机器人实现高效安全作业提供了可行的解决方案。

Abstract: Industrial manipulators are normally operated in cluttered environments,
making safe motion planning important. Furthermore, the presence of
model-uncertainties make safe motion planning more difficult. Therefore, in
practice the speed is limited in order to reduce the effect of disturbances.
There is a need for control methods that can guarantee safe motions that can be
executed fast. We address this need by suggesting a novel model predictive
control (MPC) solution for manipulators, where our two main components are a
robust tube MPC and a corridor planning algorithm to obtain collision-free
motion. Our solution results in a convex MPC, which we can solve fast, making
our method practically useful. We demonstrate the efficacy of our method in a
simulated environment with a 6 DOF industrial robot operating in cluttered
environments with uncertainties in model parameters. We outperform benchmark
methods, both in terms of being able to work under higher levels of model
uncertainties, while also yielding faster motion.

</details>


### [112] [Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?](https://arxiv.org/abs/2508.21690)
*Olger Siebinga,David Abbink*

Main category: cs.RO

TL;DR: 本文探讨了行人在人行道上相遇时出现“sidewalk salsa”尴尬情境，即双方反复同时向同一侧让路，结合机器人交互，利用RL强化学习方法来改进机器人与行人互动效果。


<details>
  <summary>Details</summary>
Motivation: 人和行人在日常生活中通过隐式沟通避免重复移动（sidewalk salsa）带来的尴尬。理解这种隐式沟通失败的原因，有助于为移动机器人设计更安全、被接受的行为策略。

Method: 在以往基于CEI（Communication-Enabled Interaction）框架的人行人行为建模基础上，本文提出用强化学习（RL）方法训练智能体与该模型互动。设置了普通RL智能体和具备风险感知的RL智能体，通过学习来减少人机互动中的沟通失败。

Result: 基本RL智能体可以学会与CEI模型良好互动；而具备风险感知的RL智能体能有效表达其运动意图，降低模型化行人的感知风险和努力度。

Conclusion: 基于CEI与强化学习结合的方法在提升机器人的行人互动能力方面前景很好，值得深入研究。

Abstract: Pedestrians approaching each other on a sidewalk sometimes end up in an
awkward interaction known as the "sidewalk salsa": they both (repeatedly)
deviate to the same side to avoid a collision. This provides an interesting use
case to study interactions between pedestrians and mobile robots because, in
the vast majority of cases, this phenomenon is avoided through a negotiation
based on implicit communication. Understanding how it goes wrong and how
pedestrians end up in the sidewalk salsa will therefore provide insight into
the implicit communication. This understanding can be used to design safe and
acceptable robotic behaviour. In a previous attempt to gain this understanding,
a model of pedestrian behaviour based on the Communication-Enabled Interaction
(CEI) framework was developed that can replicate the sidewalk salsa. However,
it is unclear how to leverage this model in robotic planning and
decision-making since it violates the assumptions of game theory, a much-used
framework in planning and decision-making. Here, we present a proof-of-concept
for an approach where a Reinforcement Learning (RL) agent leverages the model
to learn how to interact with pedestrians. The results show that a basic RL
agent successfully learned to interact with the CEI model. Furthermore, a
risk-averse RL agent that had access to the perceived risk of the CEI model
learned how to effectively communicate its intention through its motion and
thereby substantially lowered the perceived risk, and displayed effort by the
modelled pedestrian. These results show this is a promising approach and
encourage further exploration.

</details>


### [113] [QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning](https://arxiv.org/abs/2508.19153)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉和本体感知的四足机器人运动控制方法QuadKAN，利用Spline参数化的Kolmogorov-Arnold网络实现跨模态策略，提升了机器人的运动效率和稳定性，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人运动控制通常仅依赖单一感知（如本体感知或视觉），在复杂或不确定环境下鲁棒性有限。作者认识到结合本体感知与视觉信息是提升四足机器人运动能力的关键。

Method: 提出QuadKAN框架：1）采用Spline参数化的Kolmogorov-Arnold网络作为策略模型；2）设计了Spline编码器对本体信息进行编码，并通过融合头将本体和视觉信息结合；3）引入多模态延迟扰动（MMDR）增强训练，使用PPO进行端到端强化学习。

Result: 在多种地形（包括平坦、不平坦、有静态和动态障碍物）上测试，QuadKAN相比最新方法(SOTA)表现更优：获得更高的回报，运动距离更远，碰撞次数更少。

Conclusion: 基于Spline参数化的跨模态策略为鲁棒的视觉引导四足运动控制提供了简单、高效、可解释性好的新选择，优于当前主流方法。

Abstract: We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.

</details>
