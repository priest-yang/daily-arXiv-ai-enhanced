<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 112]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 本文提出了一种结合基于规则的口罩变形与无配对GAN图像转换的两步生成式数据增强框架，有效提升了口罩人脸检测与识别中的数据质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 面对口罩人脸检测与识别时数据稀缺和分布偏移的问题，现有方法难以生成真实且多样的带口罩人脸数据，因此需要新的数据增强方法。

Method: 该方法分为两步，首先采用基于规则的口罩变形技术，然后结合无配对GAN图像到图像的转换，以生成更加真实的带口罩人脸样本。同时引入非口罩保持损失和随机噪声注入，来稳定训练并增加样本多样性。

Result: 实验结果显示，相较于仅使用基于规则的变形，该方法在生成带口罩样本的质量和多样性上有一致提升，并能有效补充现有的GAN类数据生成方法（如IAMGAN）。各部分新引入的组件在实验中体现出积极作用。

Conclusion: 该研究证明了两步数据增强框架在带口罩人脸数据生成上的有效性，为基于数据增强的人脸识别研究提供了新思路，并指出未来数据增强技术的改进方向。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: JARVIS 是一种用于提升多模态大模型视觉理解能力的自监督增强方法，通过引入JEPA自监督学习范式，改善了现有模型在视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）虽能连接视觉与语言，但在基础的视觉推理任务上表现有限。现有MLLMs主要靠文本描述学习视觉内容，这种方式本身主观且片面，导致监督信号不完整，加之多模态指令微调规模小于大规模文本预训练，导致模型容易过拟合语言先验，忽略视觉细节。

Method: 提出JARVIS框架，将JEPA自监督学习范式融入MLLMs的标准视觉-语言对齐训练流程。使用冻结的视觉基础模型分别作为上下文和目标编码器，而将LLM的前层作为预测器，学习从图像中抽取结构和语义规律，无需完全依赖语言信号进行监督。

Result: 在多个标准MLLMs基准测试上，大量实验证明JARVIS能在不同LLM家族下持续提升以视觉为主的基准任务表现，并且不会减弱多模态推理能力。

Conclusion: JARVIS 有效增强了MLLMs的视觉理解与推理能力，为结合视觉与语言学习提供了更优方法。源代码已开放，有利于学界复现和进一步研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出并构建了CityNav基准，评估多模态大模型（MLLMs）在真实城市环境下仅凭视觉和自身推理完成连续导航任务的能力，发现现有方法表现不佳，并提出了Verbalization of Path（VoP）方法显著提升导航表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在实际场景中的推理和决策评估不足，尤其缺乏复杂、知识密集型任务的针对性测试，通常测试侧重于语言或虚拟环境，难以反映现实问题。作者希望通过更贴近实际的多模态导航任务，推动MLLM能力发展。

Method: 提出Sparsely Grounded Visual Navigation（稀疏锚定视觉导航）任务，并设计了CityNav基准，涵盖全球四个不同城市，测试MLLMs驱动的智能体仅凭视觉输入和内部推理在无注释、无专用结构辅助下完成多步城市导航。进一步，作者提出了Verbalization of Path (VoP) 方法，通过显式提取认知地图（地标及方向）增强内部推理能力。

Result: 实验表明，目前主流的MLLMs及常见推理技术（如Chain-of-Thought、Reflection）在此任务中的表现远低于预期。采用VoP后，导航成功率和推理表现显著提升。

Conclusion: CityNav基准为评估和推动MLLMs在知识密集型、真实场景下的决策和推理能力提供了新的标准，Verbalization of Path为提升其导航表现提供了有效方法，有助于智能体更好地应对复杂现实任务。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: 提出了R4框架，让视觉-语言模型具备结构化的4D（空间与时间）记忆，无需训练即可进行检索增强推理，并在相关任务上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 人类通过持久化、结构化的多模态记忆在四维空间中进行感知和推理。现有的视觉-语言模型缺乏类似的人类4D推理与记忆能力。

Method: R4利用语义、空间和时间信息持续建立4D知识库，将目标级语义锚定在时间和度量空间中，并在推理时把自然语言查询分解为多维检索键进行相关观测检索和集成，整个过程无需训练。

Result: 在具身问答和导航基准任务中，R4对时空信息的检索和推理能力相比基线方法有实质提升。

Conclusion: R4开启了具身4D推理新范式，为动态环境下视觉语言模型的结构化长期记忆和推理能力提供了新的可能。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了The Perceptual Observatory框架，用于全面评估和分析多模态大语言模型(MLLMs)的感知能力，关注其在扰动下的感知扎根和推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型能力日益增强，但其视觉感知能力缺乏系统表征。目前多数模型系列只是扩展了语言部分，但视觉编码器基本相同，导致难以判断模型进步究竟源于视觉理解还是文本世界知识。现有评测关注任务准确率，忽视了鲁棒性、归因能力及在扰动下的推理要求，因此需要更系统的分析方法。

Method: 作者提出The Perceptual Observatory评测框架，覆盖如人脸匹配、视觉中文字理解（简单视觉任务）、图像匹配、网格指点、属性定位（从局部到全局的视觉扎根），测试MLLMs的视觉理解能力。所有任务基于真实人脸和词语数据集，并通过像素级和风格扩散等方法系统扰动，测试模型鲁棒性。

Result: 该框架能够揭示现有多模态模型在面对不同视觉和扰动条件时的优势与不足，提供了比传统排行榜准确率更详尽的性能画像。

Conclusion: The Perceptual Observatory为MLLMs感知能力的分析建立了系统方法基础，为现有和未来模型的发展与改进提供了分析工具和理论支持。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型的新方法CAMP-VLM，实现了多人的第三视角行为预测，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往行为预测多聚焦在单人的第一视角，现实中机器人常需理解多人行为且在第三视角，因此亟需相关方法与数据。

Method: 提出CAMP-VLM：融合视觉输入的上下文特征与场景图空间信息，实现多人的行为与场景交互预测。由于真实第三视角数据缺乏，采用高真实度仿真器生成合成数据，结合SFT和DPO对模型进行微调与优化，并在合成与真实序列上评估泛化能力。

Result: CAMP-VLM在预测准确率上比最佳基线提升最高66.9%。

Conclusion: 使用多模态上下文特征可显著提升机器人对多人的行为预测能力，特别是在缺乏真实数据时，合成数据与微调可有效增强模型泛化。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文探索了在多光谱目标检测中利用视觉-语言模型(VLM)的可能性，表明在标注数据稀缺时，VLM模型能有效提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测对于自动驾驶和安防等领域具有重要意义，但有限的标注数据制约了深度模型训练。文本类信息可作为额外的语义监督以弥补这一不足。

Method: 作者将两种典型的基于VLM的检测器（Grounding DINO和YOLO-World）适配为可处理多光谱输入，并提出了一种有效的多模态融合机制，将文本、可见光和热成像信息结合。

Result: 在FLIR和M3FD两个多光谱图像基准上进行大量实验，结果显示VLM驱动的检测器在小样本下远超同等数据训练的多光谱专用模型，在全监督下也具备竞争力甚至优越性。

Conclusion: 大规模VLM学到的语义先验能够很好地迁移到未见过的光谱模态，为高效获取多光谱感知能力提供了新途径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 本文对当前主流视觉语言模型（VLMs）在农业视觉识别任务中的表现进行了系统基准测试，结果显示其远逊于传统基于监督学习的专用模型，尚不能作为独立决策支持系统应用于农业领域。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）因其通用性与强大能力受到关注，但其在农业场景（如植物病害识别、作物和杂草分类等）中的实际可靠性和有效性尚未充分验证。

Method: 作者选取了包括Gemini-3 Pro、Qwen-VL-72B等多种开源和闭源VLMs，在AgML包含的27个农业分类数据集（162个类别）上，通过零样本、多选和开放式提示等多种评测方式进行基准测试，并与有监督模型YOLOv11的表现进行了对比。

Result: 所有零样本VLMs在准确率上远低于YOLOv11；受限提示下表现最好的闭源VLM准确率约为62%，开放式提示下普遍低于25%；加入LLM语义评判可提升部分准确率（如从21%提升至30%）；Qwen-VL-72B为开源模型表现最佳。各任务中，植物及杂草分类相对简单，虫害和损伤识别最具挑战。

Conclusion: 当前开箱即用的VLMs尚难胜任农业诊断独立应用，但在配合受限界面、明确标签本体及领域化评测体系下，可作为辅助组件发挥作用。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 本论文提出了一种通过视觉感知和自适应决策主动提升花园生物多样性的机器人割草框架。系统利用深度特征分析识别并保护多样化植被，从而选择性停用割草刀片，已在真实场景中验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统的被动复野（如停止割草）推动生物多样性的方法往往效率低且控制力弱，缺乏精细的自动化和智能化手段。当前缺乏能主动促进城市草坪生物多样性的智能割草解决方案。

Method: 系统采用预训练ResNet50（基于PlantNet300K数据集）对摄像头图像中植被进行特征嵌入，无需物种级监督，通过全局离散度度量生物多样性。割草算法动态切换割草和保护行为，在检测到多样性较高区域时暂停割草。该方案通过改装商用机器人割草机，在模拟草坪和真实花园数据集上进行了实验验证。

Result: 实验显示，特征嵌入空间的分布离散度与专家的生物多样性评估呈强相关性，表明深度视觉多样性可作为生态丰富度的代理，提出的割草决策方法有效提升了区域生物多样性。

Conclusion: 该方法证实了利用深度视觉技术促进生物多样性的可行性。广泛应用此类系统，有望将单一草坪转变为充满活力、生态有价值的生物多样性栖息地，对城市生态恢复至关重要。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，能根据文本指令从初始图像和机器人状态生成视频-动作对，为视频扩散模型自动提供动作标注，从而促进机器人学习。相比现有方法，本文实现了更好的跨模态信息融合和动作精度提升，并在多项基准和实际数据集上取得显著优越表现。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型在机器人学习中有很大潜力，但普遍缺乏动作标注，使其无法充分发挥针对机器人策略学习的作用。现有方法或信息融合不足，或无法充分迁移预训练知识，限制了模型应用。

Method: （1）将预训练的视频扩散模型扩展为包含一个并行的、专用的动作扩散模型，充分保留视频知识；（2）引入Bridge Attention机制实现有效的跨模态交互；（3）设计动作细化模块，将粗略动作转化为在低分辨率数据集上的精确控制。

Result: 在多个公开基准和真实机器人数据集上进行大量评估，所提方法在生成视频质量和动作准确性方面都明显优于现有基线模型。

Conclusion: 本方法为机器人学习充分利用大规模视频数据提供了可扩展框架，有效提升了视频特性和动作标注的质量，在相关领域具有显著应用前景。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

</details>


### [11] [Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.16055)
*Jiaheng Geng,Jiatong Du,Xinyu Zhang,Ye Li,Panqu Wang,Yanjun Huang*

Main category: cs.CV

TL;DR: 本文提出了一个用于端到端自动驾驶的闭环对抗性评测平台，能够在真实世界场景中生成安全关键的极端案例，从而有效检测自动驾驶模型在极端情况下的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的安全关键极端事件难以采集，但对自动驾驶系统的评估至关重要。已有方法多基于简化的仿真场景，缺乏在真实场景对端到端自动驾驶模型的对抗性评测。

Method: 设计了一个闭环评测平台：1）利用基于流匹配的图像生成器，根据交通环境信息高效地生成真实世界的交通图像；2）结合设计的对抗性交通策略，自动生成周围车辆的行为，制造自动驾驶系统难以应对的极端交互场景，用于评测主流端到端模型。

Result: 平台能够高效生成真实感强的驾驶图像。在对UniAD、VAD等端到端模型评测中，平台能检测模型在极端情况下的性能下降，验证了其有效性。

Conclusion: 该平台能够有效发现自动驾驶模型的潜在问题，有助于提升端到端自动驾驶系统的安全性与鲁棒性。

Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.

</details>


### [12] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种3D多通道patch扩散模型，用于从单壳低角分辨率dMRI（LAR-FOD）预测多壳高角分辨率FOD（HAR-FOD），以提升白质纤维定向分布估计的准确性，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前从低角分辨率dMRI估算FOD存在精度不足的问题，而高角分辨率dMRI虽然能更准确地估计FOD，但扫描时间过长限制了其应用。为此，亟需能够利用已有低分辨率数据准确预测高分辨率FOD的方法。

Method: 提出一种3D多通道patch扩散模型，该模型利用FOD-patch adapter融合脑解剖先验，提高patch学习效率，并引入体素级条件协调模块增强模型全局理解，同时设计SH注意力模块学习SH系数的复杂相关性。

Result: 实验结果表明，该方法在HAR-FOD预测任务上取得了最佳表现，优于其他先进方法。

Conclusion: 所提出的方法能有效实现从LAR-FOD向HAR-FOD的转换，在精度上优于现有技术，具有较大的临床价值和推广前景。

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [13] [Auto-Vocabulary 3D Object Detection](https://arxiv.org/abs/2512.16077)
*Haomeng Zhang,Kuan-Chuan Peng,Suhas Lohit,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 该论文提出了一种无需用户指定类别、自动生成检测目标类别的3D开放词汇物体检测方法，并在ScanNetV2和SUNRGB-D数据集上取得了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D物体检测方法虽然名为开放词汇，实则仍需用户在训练和推理时提供类别信息，限制了自动化和泛化能力。因此作者希望实现真正自动化、无需用户指定类别的3D目标检测。

Method: 作者提出了Auto-Vocabulary 3D Object Detection (AV3DOD)，可自动为检测到的目标生成类别名。他们引入Semantic Score (SS)来衡量生成类别名的语义质量。具体方法包括：利用2D视觉-语言模型进行图片描述生成丰富语义候选，伪3D框生成，以及特征空间的语义扩展。

Result: 该方法在ScanNetV2和SUNRGB-D数据集上实现了定位和语义质量SOTA。相较于SOTA方法CoDA，整体mAP提升3.48，在ScanNetV2上的SS指标相对提升24.5%。

Conclusion: AV3DOD实现了真正自动化的3D开放词汇物体检测，提升了检测精度和类别语义表达能力，推动了领域发展。

Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.

</details>


### [14] [LAPX: Lightweight Hourglass Network with Global Context](https://arxiv.org/abs/2512.16089)
*Haopeng Zhao,Marsha Mariya Kappan,Mahdi Bamdad,Francisco Cruz*

Main category: cs.CV

TL;DR: 该论文提出了LAPX，一种基于Hourglass结构并结合自注意力机制的人体姿态估计算法。LAPX在保证准确率的同时，极大地减少了参数量，实现了高效、适用于边缘设备的实时姿态估计。


<details>
  <summary>Details</summary>
Motivation: 当前最优算法参数众多，计算开销大，不适合在边缘设备上部署；一些轻量化方法虽然简化了模型结构，但准确率下降明显，难以兼顾速度与精度。

Method: 基于已有的LAP网络，提出LAPX，采用自注意力机制获取全局信息，优化网络阶段设计，并改进轻量级注意力模块，减小模型复杂度。

Result: LAPX在MPII和COCO两个数据集上取得了有竞争力的精度，参数量仅2.3M，能够实现实时推理，适合在边缘设备部署。

Conclusion: LAPX兼顾了模型轻量化和准确性，实现了在边缘设备上的高效部署，是人体姿态估计领域轻量化网络设计的有力尝试。

Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.

</details>


### [15] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于光准直仪和闪烁星型标定板的新型事件相机标定方法，实验证明该方法在精度和可靠性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高动态范围和高时间分辨率方面有独特优势，但其几何标定（尤其是远距离测量）一直存在挑战。迫切需要解决在远距离、高精度要求下的标定问题。

Method: 提出利用带有闪烁星型标定板的光准直仪进行标定。先通过光准直仪的球面运动模型线性求解相机参数，再通过非线性优化进一步提升参数精度。

Result: 经过不同条件下的大量真实实验，结果显示该方法在精度与可靠性方面持续优于现有事件相机标定方法。

Conclusion: 本文方法有效提升了事件相机在远距离高精度应用场景下的标定精度和可靠性，为事件相机相关测量应用提供了新的技术手段。

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [16] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: TurboDiffusion是一种针对扩散式视频生成的加速框架，能在保持视频质量的同时，实现100-200倍的生成速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式视频生成模型虽然效果优秀，但生成速度缓慢，限制了其实际应用。因此，急需提升扩散模型在视频生成任务中的速度。

Method: TurboDiffusion主要有三大加速组件：1）基于低比特SageAttention和可训练稀疏线性注意力（SLA）加速注意力计算；2）采用rCM实现高效步长蒸馏；3）模型参数及激活量8比特量化（W8A8）以加速线性层及压缩模型。此外还集成了多项工程优化。

Result: 在Wan2系列多个型号的视频生成模型上实验表明，TurboDiffusion即使在单张RTX 5090 GPU上，也能实现100-200倍加速，同时生成的视频质量近似未加速模型。

Conclusion: TurboDiffusion极大提升了扩散式视频生成的速度，并未显著损失输出质量，为扩散模型在实际视频生成应用中提供了突破性的解决方案。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [17] [Flexible Camera Calibration using a Collimator System](https://arxiv.org/abs/2512.16113)
*Shunkun Liang,Banglei Guan,Zhenbao Yu,Dongcai Tan,Pengju Sun,Zibin Liu,Qifeng Yu,Yang Shang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于专用平行光管（collimator）系统的新型相机标定方法，通过独特的几何约束简化标定过程，并在合成与真实实验中取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统相机标定往往需要复杂的环境和多个自由度变化，受限于运动控制精度和表观噪声，存在灵活性不足和操作复杂的问题。为此，作者希望利用 collimator 提供稳定、受控光学环境，设计更简洁高效的标定方法。

Method: 设计了专用 collimator 标定系统，并首次引入角度不变约束。理论上将标定靶与相机的相对运动由6自由度约束在球面3自由度旋转。基于此，提出了多图像的闭式线性解算器和双图像的最小解算器，并创新性地提出了只需单张 collimator 图像的标定方法，无需相机运动。

Result: 在合成数据和真实世界标定实验中，作者方法均取得了高精度，验证了 collimator 系统的有效性，并在标定精度和灵活性上均优于现有主流方法。

Conclusion: 本文提出的 collimator 相机标定法实现了环境可控、过程灵活且高精度的快速标定，为实际摄影测量和三维视觉任务提供了更优新方案。

Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration

</details>


### [18] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 本文提出了一种可以通过单张图片自动检测放牧牛只行为互动的方法，对智能畜牧管理如发情检测等场景非常关键。该方法利用分解牛只互动为个体动作，并在大规模动作数据集基础上，结合对稀有互动的对比学习微调，实现了动作与互动的统一表征，大幅提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 牛只间互动（如发情期行为）对于畜牧业管理至关重要，但当前缺少包含互动的全面数据集，加之此类行为本身较为罕见，导致检测存在很大难度。人类的互动检测已经研究较多，但直接类推用于牛只并不可行，因此需要新的、高效的数据驱动方法来解决这一问题。

Method: 作者提出CattleAct方法，将复杂的互动行为分解为单头牛的动作组合。首先，从大规模牛只动作数据集中学习动作隐空间，然后利用对比学习微调，将稀有互动嵌入至已有表征空间，形成统一的动作-互动隐空间。该方法还集成了视频与GPS数据，实现了实际可用的互动检测系统。

Result: 在商用规模牧场数据上的实验结果显示，本方法在互动检测的准确性上优于现有基线方法；系统能够在实际应用场景下准确检测牛只之间的互动行为。

Conclusion: CattleAct证明了通过动作分解与表征微调可以有效解决牛群中稀有互动检测的问题，并具备较高的实际应用价值，为智能畜牧管理提供了可行的技术路线。源码已公开。

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [19] [ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT](https://arxiv.org/abs/2512.16140)
*Ze Yuan,Wenbin Li,Shusen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合知识驱动与数据驱动的双谱CT重建框架，利用快速的OPMT方法和改进型深度学习网络ResDynUNet++协同提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 双谱CT在医学成像领域有广泛应用，但传统重建方法在去除伪影和提升分辨率方面有限。现有方法难以兼顾物理模型知识与数据适应能力，且深度学习方法容易产生通道不平衡和界面伪影等问题。因此亟需融合物理与数据驱动的新方法，以提升重建的准确性和质量。

Method: 框架包括两部分：1）知识驱动模块，采用OPMT算法从投影数据快速生成初步的基底材料图像；2）数据驱动模块，引入基于UNet++、采用残差动态卷积的新型神经网络ResDynUNet++，对中间结果进行精细修正，充分结合动态卷积的自适应特性和残差结构的稳定性。

Result: 在合成和真实临床数据集上的大量实验表明，该方法在去除通道不平衡与界面伪影、提升图像清晰度和准确率上均优于现有方法。

Conclusion: 提出的混合重建框架成功结合了物理知识和深度学习优势，在DSCT重建任务中表现出更高鲁棒性和更优重建质量，具有良好应用前景。

Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.

</details>


### [20] [SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation](https://arxiv.org/abs/2512.16143)
*Yueyang Hu,Haiyong Jiang,Haoxuan Song,Jun Xiao,Hao Pan*

Main category: cs.CV

TL;DR: 本文提出了一种用于小样本3D部件分割的新型框架SegGraph，通过利用SAM分割图显式建模几何关系，提升了3D分割性能，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D基础模型的小样本3D部件分割方法难以有效整合2D模型知识到3D，忽视了3D几何结构和高质量分组线索，导致分割不足和部分标签不一致。

Method: 提出SegGraph方法，基于SAM的2D分割面具构建分割图，节点为分割片段，边代表空间关系（重叠/邻接），用图神经网络传播并融合2D模型特征，实现全局几何结构学习。通过新的视角加权融合，实现高质量3D点特征映射。

Result: 在PartNet-E数据集上，SegGraph的mIoU至少超越所有基线6.9个百分点，特别在小部件和边界表现突出。

Conclusion: SegGraph有效增强了3D分割的几何理解能力，在少样本情况下显著优于现有方法。

Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.

</details>


### [21] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 本论文提出了一种新的无监督领域自适应方法C-DGPA，用于解决视觉-语言模型在下游任务中的领域差异问题，显著提升了各大数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型通过提示调优（prompt tuning）在无监督领域自适应任务中存在领域分布差异，导致模型性能下降，特别是现有方法忽略了条件分布不一致问题，造成类别语义对齐困难、语义区分性降低。

Method: 提出C-DGPA（Class-Centric Dual Alignment Generative Prompt Adaptation）框架，采用双分支结构：一方面通过动态对抗训练对齐边缘分布，另一方面引入类别映射机制，对齐条件分布，标准化语义提示，防止过度依赖源域。

Result: 在OfficeHome、Office31和VisDA-2017等多个主流领域自适应数据集上进行了大量实验，C-DGPA在所有基准上均取得最新的最优结果。

Conclusion: C-DGPA方法能够协同优化边缘和条件分布对齐，把领域知识高效融入提示学习，获得领域无关且语义区分性强的表示，显著提升了视觉-语言模型的无监督领域自适应能力。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [22] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 本文提出使用事件相机替代传统相机，以提升自动驾驶系统在不同光照条件下（尤其是昼夜切换）的一致性和鲁棒性。结果显示事件相机在不同光照域下表现更稳定，迁移惩罚更小。


<details>
  <summary>Details</summary>
Motivation: 传统相机由于“领域差异”（如昼夜变换）导致训练环境与实际部署环境不符时性能显著下降，亟需寻找跨领域更鲁棒的感知传感器。

Method: 作者将传统相机与事件相机在自动驾驶场景下进行对比，分析在不同光照条件（如白天与夜晚）下的驱动车端到端性能，评估事件相机在领域迁移中的优势。

Result: 实验结果表明，事件相机在昼夜等不同光照条件下，性能更加一致，对领域迁移惩罚更小，且相比于灰度帧基线，在跨域场景下表现更优。

Conclusion: 事件相机无需复杂适应即可实现光照条件下的稳定感知，是跨光照领域下有前景的替代方案。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [23] [Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation](https://arxiv.org/abs/2512.16199)
*Jerrin Bright,Zhibo Wang,Dmytro Klepachevskyi,Yuhao Chen,Sirisha Rambhatla,David Clausi,John Zelek*

Main category: cs.CV

TL;DR: 本文提出Avatar4D，一个可定制的合成人体动作数据生成流水线，增强了在特定领域（如体育）中的灵活性和适用性，并展示了其在合成数据集Syn2Sport上的应用和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的合成人体动作数据集大多仅覆盖日常动作，灵活性有限，难以满足特定领域如体育类动作分析的需求，因此作者希望构建一个更具可控性和适应性的生成流程。

Method: 作者设计了Avatar4D流水线，可以精细控制人体姿态、外观、摄像角度和环境，生成高保真的4D人体动作序列，并应用于体育领域（如棒球、冰球），构建了大规模多样化的合成数据集Syn2Sport。

Result: 作者在Syn2Sport上对多种当前最先进姿态估计算法进行基准测试，验证了其对监督学习、零样本迁移与跨运动泛化的有效性，并评估了合成数据与真实数据在特征空间中的相似度。

Conclusion: 该系统能够在无需真实领域数据的前提下，生成可扩展、可控且具领域适应性的人体动作数据集，促进特定应用领域（如体育）的动作理解与算法开发。

Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.

</details>


### [24] [Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation](https://arxiv.org/abs/2512.16201)
*Sarosij Bose,Ravi K. Rajendran,Biplob Debnath,Konstantinos Karydis,Amit K. Roy-Chowdhury,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 本文提出了VALOR方法，通过视觉与语言对齐，提升了放射学报告生成的准确性和视觉基础，实现了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前医学大模型在报告生成中容易因为视觉和语言模态对齐不足，导致虚假内容（hallucination），且现有方法依赖大规模标注数据和高成本的偏好数据，难以解决这一问题。本文旨在提升报告的视觉和临床准确性，减少虚假内容。

Method: 提出了VALOR，一种基于强化学习后对齐的新框架。方法包括两个阶段：（1）用文本奖励提升医学大模型对临床术语的精准性；（2）将视觉投影模块和疾病发现内容对齐，引导模型关注诊断相关的图像区域。核心技术为Group-Relative Proximal Optimization（GRPO）。

Result: 在多个数据集上广泛实验证明，VALOR在事实准确性和视觉对齐上都有显著提升，整体效果明显优于现有主流放射学报告生成方法。

Conclusion: VALOR有效解决了医学视觉-语言模型报告生成中的虚假内容和对齐挑战，为医学影像报告自动化提供了新方向，并在实际中展现了更高的可靠性和应用潜力。

Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.

</details>


### [25] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 本文提出了OAK模型，通过引入可学习的上下文token，结合CLIP与视觉聚类目标，实现了开放式即席分类任务的最优表现。


<details>
  <summary>Details</summary>
Motivation: AI系统需具备适应性，将视觉场景动态划分到任务相关的新颖类别，传统静态类别难以满足不同场景按需分类的需求。

Method: 作者提出OAK方法，在固定CLIP模型前端引入少量可学习的上下文token，通过CLIP的图文对齐损失和GCD的聚类损失联合训练，从少量标注样本和大量未标注数据中，自动挖掘并扩展即席类别。

Result: OAK在Stanford和Clevr-4数据集上实现了多分类任务的最优表现，Stanford Mood任务新颖类别准确率达87.4%，比传统CLIP和GCD方法高出50%以上。

Conclusion: OAK能实现更透明、可解释的视觉特征关注区域，并具备自适应和可泛化的类别划分能力，有助于提升AI在复杂任务中的可靠性和应用价值。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [26] [SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning](https://arxiv.org/abs/2512.16461)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: 本文提出了SNOW框架，将视觉-语言模型得到的语义信息与点云3D几何和时序动态相结合，实现统一的4D场景理解，显著提升了自主机器人场景感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人在动态环境中的空间-时序感知面临挑战：视觉-语言模型虽提供开放世界语义，但缺乏3D和动态信息；传统几何感知则语义稀缺。因此亟需一种将二者优势融合的方法。

Method: 提出SNOW框架：无需训练、主干网络无关，处理同步RGB图像与3D点云。利用HDBSCAN聚类生成目标级建议，用SAM2分割，采用新提出的STEP编码，每一分割区域被编码为多模态Token，融合语义、几何和时间特征。这些Token增量式集成至4D场景图（4DSG），由轻量SLAM模块空间锚定，确保全局空间一致性。最终4DSG可直接支持VLM对空间结构和动态的推理。

Result: 在多个基准数据集上，SNOW展现出精确的4D场景理解与空间语义推理能力，在多个任务上取得最新最优表现。

Conclusion: 结构化4D先验对于具身推理和自主机器人意义重大。SNOW框架有效打通了语义、几何和时间，推进了机器人场景理解发展。

Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.

</details>


### [27] [Enhanced 3D Shape Analysis via Information Geometry](https://arxiv.org/abs/2512.16213)
*Amit Vishwakarma,K. S. Subrahamanian Moosath*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D点云形状分析的信息几何方法，通过将点云表示为高斯混合模型（GMM），利用修改后的对称KL散度（MSKL）实现了数值稳定的形状比较。实验表明，该方法在几何变化反映和数值稳定性方面优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 3D点云作为数字化对象的高精度表示，在计算机视觉等领域应用广泛。然而，由于其无结构性和复杂几何，点云间的比较一直存在挑战，传统度量对离群点敏感且难以刻画全局结构，现有KL散度的GMM近似也存在数值不稳定的问题。

Method: 作者将点云表示为统计流形上的高斯混合模型，证明GMM构成统计流形，并提出了具有理论上下界、数值稳定的修改对称KL散度（MSKL）。该方法用于计算任意GMM间的距离，并且在实际数据集上进行了大量实验验证。

Result: 在人体姿势识别（MPI-FAUST）和动物形状比较（G-PCD）等多个数据集上的实验显示，MSKL能够稳定反映几何变化，且在数值稳定性和准确性上均优于Hausdorff距离、Chamfer距离和传统KL近似方法。

Conclusion: MSKL为3D点云的形状分析提供了理论有保证、数值稳定的新度量，能够更准确地捕捉全局和局部几何结构，并且在多个实际任务中表现优越，有望成为点云分析的新标准。

Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.

</details>


### [28] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一种结合几何感知的新型视觉-语言-动作模型，强化机器人操作中的3D推理能力，在多个任务中效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型主要依赖2D特征，缺乏对精确3D几何关系的建模，导致在复杂空间操作任务中的泛化和可靠性不足。

Method: 提出GeoPredict框架，在现有VLA模型基础上，加入运动历史编码器和多步3D关键点轨迹预测模块，以及预测性3D高斯几何模块。训练时，利用深度渲染作为监督信号。推理时，仅需极轻量级查询token，避免了繁重的3D推理开销。

Result: 在RoboCasa Human-50、LIBERO等仿真与真实世界任务上，GeoPredict在涉及复杂3D几何和空间操作需求的任务中，性能均超越主流VLA基线方法。

Conclusion: 引入几何预测与运动先验，可显著提升VLA模型在几何和空间高度相关任务中的表现，为机器人操作领域带来新的突破。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [29] [Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models](https://arxiv.org/abs/2512.16219)
*Zhihao Zhang,Xuejun Yang,Weihua Liu,Mouquan Shen*

Main category: cs.CV

TL;DR: 本文提出了一种提升单视角新视图合成（NVS）扩散模型生成质量的方法，通过引入高质量噪声增强图像生成效果，并提出了相应的噪声学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的NVS方法依赖随机噪声初始化，不同的噪声对生成效果影响很大，但缺乏对高质量噪声的专门学习方法。作者希望改进这一不足，实现更优质的新视图合成。

Method: 首先，设计了离散欧拉反演方法，将图像语义信息注入随机噪声，构建用于学习的成对噪声数据集；其次，提出基于编码器-解码器(EDN)的深度学习框架，将随机噪声直接转换为高质量噪声。

Result: 实验表明，所提EDN框架可无缝集成到多种NVS模型如SV3D和MV-Adapter中，并在多个数据集上显著提升了生成性能。

Conclusion: EDN框架能够高效提升新视图合成的表现，具备通用性和可扩展性，有望推动扩散模型下NVS的发展。

Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.

</details>


### [30] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本论文提出了OpenTouch，这是首个包含野外环境下第一人称全手触觉数据集，为视觉和物理交互的研究提供了有力工具。数据集包含视频、触觉和手势同步信息，并配有详尽注释。研究证明触觉信号对理解抓握和跨模态对齐有重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有的可穿戴触觉传感器有限，且缺乏同步的第一人称全手触觉与视频数据，这极大限制了对手部物理交互的理解与研究。

Method: 作者构建了OpenTouch数据集，采集了长达5.1小时的同步视频、触觉、手势数据，并挑选了2,900个带有文本注释的片段。基于该数据集，作者设计了检索和分类基准任务，评估触觉信号在知觉和动作中的作用，并分析其与视觉信号的对齐能力。

Result: 结果显示，触觉信号为理解手部抓握方式提供了有效线索，加强了视觉与其他模态（如动作、触觉）之间的协同，并能够通过视频查询可靠地检索到触觉片段。

Conclusion: OpenTouch数据集和相关基准的发布，有望推动多模态的第一人称感知、具身学习以及机器人丰富接触操作等领域的研究进展。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [31] [Image Compression Using Singular Value Decomposition](https://arxiv.org/abs/2512.16226)
*Justin Jiang*

Main category: cs.CV

TL;DR: 该论文评估了利用奇异值分解（SVD）和低秩矩阵近似进行图像压缩的方法，并与主流压缩格式对比，发现其压缩效率远逊于JPEG、JPEG2000和WEBP等标准格式。


<details>
  <summary>Details</summary>
Motivation: 随着互联网图片数量庞大，高效的图像压缩技术对减小存储与带宽压力至关重要。作者希望探索SVD和低秩矩阵近似在图像压缩中的实际应用效果。

Method: 作者采用SVD与低秩矩阵近似的方法，对灰度图像和彩色多通道图像进行了压缩测试，使用相对Frobenius误差和压缩率作为性能评价指标，并与JPEG、JPEG2000和WEBP等现有标准格式进行对比。

Result: 结果表明，低秩近似压缩后图像在视觉上几乎与原图无异，但在相同误差水平下的压缩率显著低于主流压缩标准；对于低误差容忍度时，SVD法甚至可能产生比原图还大的文件。

Conclusion: SVD与低秩矩阵近似虽然在保持图像质量上表现不错，但由于压缩效率差，不适合实际大规模图像压缩，与现有行业标准无法竞争。

Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.

</details>


### [32] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 本文提出EgoMAN数据集和EgoMAN模型，通过将语义推理与动作生成紧密结合，提升3D手部轨迹预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D手部轨迹预测数据集将动作与语义监督分离，模型也未能有效将推理与动作衔接，限制了预测效果。

Method: 作者构建了大规模头戴视角的数据集EgoMAN，包含21.9万个6DoF手部轨迹及300万个结构化问答对。提出EgoMAN模型，利用“轨迹-令牌”接口，把视觉-语言推理与动作生成关联，通过分阶段训练对齐推理与动作动态。

Result: 该方法能够生成准确且对交互阶段敏感的手部轨迹，在现实场景中具有较强的泛化能力。

Conclusion: 紧密结合推理与动作的EgoMAN框架有效提升了3D手部轨迹预测的性能，为相关任务提供了更优的模型和数据基础。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [33] [ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation](https://arxiv.org/abs/2512.16234)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Hesheng Wang,Ajmal Mian*

Main category: cs.CV

TL;DR: 论文提出了一种新的3D人类反应生成方法ARMFlow，能同时实现高动作保真度、实时推理和自回归的在线适应性，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D人类反应生成在高动作质量、实时性和自回归在线适应性上存在挑战，现有方法无法三者兼得，急需新的方法突破。

Method: 提出ARMFlow框架，基于MeanFlow实现自回归，建模角色与反应者之间的时序依赖，包括因果上下文编码器和MLP速度预测器。训练中引入Bootstrap Contextual Encoding（BSCE），用生成历史而非真实历史编码，减少自回归误差积累。同时提出离线版本ReMFlow。

Result: ARMFlow在InterHuman和InterX数据集上，在线推理FID指标优于现有方法40%以上，且只用部分序列条件即可达到离线SOTA水平。ReMFlow推断速度为同类最快。

Conclusion: ARMFlow有效解决了3D人反应生成的三大挑战，在准确率、低延迟和误差控制方面表现突出，推理速度快、泛化能力强，在在线与离线场景下均取得SOTA效果。

Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.

</details>


### [34] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 本文提出MomaGraph，一种用于家庭服务机器人（移动机械臂）的统一场景表示方法，结合空间和功能关系，用于更好地完成任务。并发布了相关数据集和评测基准，还训练了多模态大模型，实验显示在相关任务中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 传统针对家庭服务机器人的场景图往往只考虑空间信息或功能信息之一，忽视对象状态、时序动态和任务相关性，难以满足导航与操作一体的需求。因此亟需一个能够融合空间-功能关系、对象可交互部分，并易于任务规划和评估的场景表示。

Method: 作者提出MomaGraph，一种集成空间与功能、支持动作部件的统一场景图。为支撑研究，作者建立了大规模带有丰富标签的MomaGraph-Scenes数据集，并开发了MomaGraph-Bench评测套件。基于此，训练了7B参数的视觉-语言模型MomaGraph-R1，采用强化学习动态图场景驱动训练，并在Graph-then-Plan范式下进行零样本任务规划。

Result: MomaGraph-R1模型在MomaGraph-Bench基准上达到71.6%准确率，提升11.4个百分点，领先于现有开源模型，并在多个公开基准和真实机器人实验中表现出良好的泛化与迁移能力。

Conclusion: MomaGraph方法及其支持的数据集、评测套件与大模型显著推动了家庭机器人场景理解与任务规划研究，为后续智能体在真实生活环境中的应用提供了重要基础。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [35] [AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection](https://arxiv.org/abs/2512.16235)
*Satya Narayana Panda,Vaishnavi Kukkala,Spandana Iyer*

Main category: cs.CV

TL;DR: 本文提出了一种结合临床影像与家族史信息的多模态AI系统，以提升皮肤科疾病的诊断准确性，尤其针对遗传性皮肤病，在初步验证中显示出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 皮肤病全球影响广泛，但诊断受限于专家数量不足和症状复杂，同时家族史对疾病易感性及治疗反应有重要影响，然而实际诊断中常被忽视。因此，如何有效将家族史与影像等临床数据结合，是亟需解决的难题。

Method: 作者开发了一个多模态AI框架，利用可解释的卷积神经网络处理影像数据，并与包含详细家族史等结构化数据的决策树结合。系统设计中还包含了多种临床环境下的前瞻性临床试验方案来验证其有效性。

Result: 将家族史数据整合入AI系统可显著提高某些遗传性皮肤疾病（如黑色素瘤、银屑病、特应性皮炎）的诊断准确率。专家反馈显示该系统有助于疾病早期发现和个性化诊疗建议，虽然大规模临床试验尚在规划中。

Conclusion: 该多模态AI系统结合家族史和影像数据，对提升皮肤病诊断具有明显潜力，且具备良好的可解释性，适合融入现有临床工作流程，值得在临床领域进一步推广和验证。

Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.

</details>


### [36] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种新型的方法——驾驶视觉几何变换器（DVGT），可以从多视角视频序列重建3D场景点云地图，无需已知相机参数，并在多个主流自动驾驶数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺少一种能适应不同场景和相机配置、专为自动驾驶设计的高密度几何感知模型。传统方法对相机参数依赖较大，限制了其实际应用能力。

Method: DVGT利用DINO骨干网络提取每帧特征，通过交替的视内局部注意力、跨视角空间注意力与跨帧时序注意力推理帧间几何关系。使用多头机制在第一帧自车坐标系中直接解码整体点云与每帧自车位姿，无需精确的相机几何先验。

Result: DVGT在nuScenes、OpenScene、Waymo、KITTI及DDAD等多个大规模驾驶数据集上进行训练，并在多种场景下的3D重建精度上显著优于其他现有模型。

Conclusion: DVGT能灵活地适应各种相机配置和场景，摆脱了对外部传感器对齐和精确几何先验的依赖，为自动驾驶中的3D感知和场景重建提供了更高效与精准的新范式。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [37] [Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models](https://arxiv.org/abs/2512.16243)
*Qi Zhang,Yunfei Gong,Zhidan Xie,Zhizi Wang,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出两种半监督多视角人群计数方法，通过排序多视角融合模型的预测或不确定性来提升有限标注数据下的计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视角人群计数受限于样本采集和标注的高成本，导致数据集有限，训练深度模型表现受限，因此需要依赖更少标注或半监督方法提升模型效果。

Method: 提出两种多视角融合排序机制的半监督方法：（1）基于模型不同相机视角输入下预测数量的排序约束；（2）基于模型预测不确定性随视角数变化的排序约束，利用这些约束进行半监督训练。

Result: 实验证明，所提的多视角模型排序方法在有限标注样本下优于其他半监督计数方法。

Conclusion: 通过多视角模型排序机制显著提升了有限数据下的人群计数性能，为今后多视角场景下的半监督计数提供了新思路。

Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.

</details>


### [38] [Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning](https://arxiv.org/abs/2512.16266)
*Paloma Casteleiro Costa,Parnian Ghapandar Kashani,Xuhui Liu,Alexander Chen,Ary Portes,Julien Bec,Laura Marcu,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的多通道像素超分辨框架FLIM_PSR_k，可大幅提升荧光寿命成像显微镜（FLIM）的图像分辨率和采集速度，在保持重要图像特征的同时，有效缓解信噪比（SNR）和分辨率-速度权衡的难题。


<details>
  <summary>Details</summary>
Motivation: 临床上FLIM因像素驻留时间长和信噪比低，导致分辨率和速度难以兼得，从而限制了其应用。现有超分辨方法推理速度慢或兼容性差，亟需更高效、实用的新方法。

Method: 作者提出FLIM_PSR_k，通过条件生成对抗网络（cGAN）训练，可从大像素采集数据重建高分辨率图像，推断速度远优于扩散模型。测试时从低分辨率输入恢复高分辨率FLIM图像，提升空间带宽积。

Result: 在盲测的肿瘤组织切片样本上，FLIM_PSR_k能实现k=5的超分辨（空间带宽积提升25倍），显著改进多项图像质量指标，有效恢复丢失细节。

Conclusion: FLIM_PSR_k提升了FLIM的有效分辨率，实现更快采集与高度灵活的硬件兼容性，为FLIM的临床转化提供了关键技术基础。

Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.

</details>


### [39] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了TextEditBench，一个专注于图像中文字区域编辑的综合评测基准，并引入了新的评估维度以推动多模态生成模型在文本编辑和推理中的发展。


<details>
  <summary>Details</summary>
Motivation: 目前大规模扩散模型与多模态生成模型在图像内文本渲染方面取得了一定进展，但针对图像中已有文本的编辑依然面临字符可读性、语义一致性和上下文连贯性挑战，相关评测工具缺乏。作者旨在推动该领域的发展，填补有效评测标准的空白。

Method: 作者设计了TextEditBench评测基准，对图像内的文字编辑能力进行全面评估，并设置了强调推理和跨模态理解的复杂编辑场景。同时提出新评估指标Semantic Expectation (SE)，用于衡量模型在语义一致性、上下文连贯性及跨模态对齐等方面的推理能力。

Result: 实验表明，尽管现有主流编辑系统可以完成简单的文本编辑，但在需要上下文推理、物理一致性和布局感知等方面仍有明显不足。TextEditBench不仅揭示了当前技术瓶颈，也能够细致区分不同系统的优劣。

Conclusion: TextEditBench拓展并推动了图像中基于文本的编辑与推理能力评测，为多模态生成等前沿领域提供了新的测试标准和研究方向，有望促进更加智能的文本编辑系统发展。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [40] [GFLAN: Generative Functional Layouts](https://arxiv.org/abs/2512.16275)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动化户型生成框架（GFLAN），将生成过程分为拓扑规划和几何实现两个阶段，以更好地捕捉建筑设计中的复杂约束和布局逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法虽然提升了自动化户型生成的水平，但依然难以表达建筑设计中的本质推理，比如拓扑关系优先于几何细节、功能约束在邻接网络中的传播，以及交通流线的自发形成。这造成生成的户型在实际应用中存在功能性和合理性不足。

Method: GFLAN将户型生成分为两个阶段：A阶段用双编码器卷积网络通过概率地图依次分配房间质心，专门将空间上下文和布局状态分离表达；B阶段用异构图连接房间节点与边界顶点，再结合带有Transformer模块的图神经网络回归具体房间边界，实现几何细化。

Result: GFLAN能够在给定外部边界与入户门位置时，有效生成兼顾拓扑合理性与几何可行性的高质量户型，其分阶段设计明显优于端到端的像素或墙线生成方法。

Conclusion: 将户型生成问题分解为明确的拓扑与几何两个阶段，有助于模型学习和推理建筑设计的深层逻辑约束，从而提升生成结果的实际可用性和建筑意义。

Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.

</details>


### [41] [MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.16294)
*Amna Amir,Erchan Aptoula*

Main category: cs.CV

TL;DR: 本文提出了多标签自适应对比学习（MACL）方法，用于解决多标签遥感图像检索中的语义重叠、标签分布不均与复杂类别共现等难题。MACL在主流数据集上明显优于传统对比损失基线方法。


<details>
  <summary>Details</summary>
Motivation: 多标签遥感图像检索面临语义重叠、标签极不均衡和复杂类别共现，这导致现有方法性能受限。作者希望提出一种新方法以改善对稀有与常见类别的表征能力，提升检索性能。

Method: 提出了MACL方法，结合标签感知采样、频率敏感加权和动态温度缩放三大机制，以实现类间学习的均衡。该方法基于对比学习框架扩展，专门针对多标签场景进行设计。

Result: 在DLRSD、ML-AID和WHDLD三个主流基准数据集上的大量实验表明，MACL明显优于基于传统对比损失的同类方法，能更好地缓解语义不均衡，并提升大规模遥感数据检索的准确性。

Conclusion: MACL是一种有效的多标签自适应对比学习方法，对遥感图像检索中的多标签失衡问题有显著改善效果，具有应用和研究推广价值。

Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.

</details>


### [42] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: 该论文提出了PixelArena基准，以像素级语义分割任务，评估多模态大模型的细粒度图像生成能力，发现Gemini 3 Pro展示了出色的零样本生成表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型图像生成评测大多关注美学而非细粒度生成能力，缺少针对像素级生成质量的客观评价方法。

Method: 作者提出PixelArena基准，以语义分割任务为切入点，用像素级精度考察模型的细粒度图像生成能力，并以此衡量多模态大模型的视觉理解与泛化表现。

Result: Gemini 3 Pro Image模型在零样本设置下能够生成高保真度的语义分割掩码，表现出前所未有的视觉智能和泛化能力，并与其他模型进行了定性与定量对比，还展示了部分失败案例。

Conclusion: PixelArena为多模态模型细粒度生成能力的评估提供了新思路和工具，相关结果推动了多模态通用智能发展，并为多模态推理、可解释性及评测体系研究提供了重要参考。

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [43] [LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation](https://arxiv.org/abs/2512.16313)
*Haiyu Zhao,Yiwen Shan,Yuanbiao Gou,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了LaverNet，一个仅有36.2万参数的轻量级全能视频复原网络，通过创新的特征传播机制，有效处理多种时间变化退化问题，性能优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 现有的全能视频复原方法在面对时间变化退化时，模型容易被退化影响而忽略视频内容，并且通常需要大规模模型来实现，效率和应用受限。

Method: 提出LaverNet小型网络，并引入了一种新颖的退化无关特征选择性传播机制，使跨帧传递信息不受退化干扰，从而提升对多种退化的处理能力。

Result: LaverNet在参数量远小于现有同类模型（不到1%）的情况下，依然在多个标准基准上达到或超越现有大模型的性能。

Conclusion: LaverNet网络展示了轻量级模型同样可以实现高效的多退化视频复原，为模型小型化和实际落地提供了新的思路。

Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.

</details>


### [44] [Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs](https://arxiv.org/abs/2512.16314)
*Huayu Huang,Chen Chen,Banglei Guan,Ze Tan,Yang Shang,Zhang Li,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于岭估计的融合定位方法，通过结合序列图像丰富的场景信息和激光测距的高精度，提高了目标定位的准确性，尤其在观测条件受限时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无人机多传感器目标定位方法，在远距离、小交会角、大倾斜角等极限条件下，常因最小二乘算法设计矩阵的多重共线性导致定位不稳定和鲁棒性差。

Method: 通过将岭估计引入多传感器数据融合过程中，缓解最小二乘法在观测条件受限下的多重共线性问题，融合了序列图像与激光测距信息，实现鲁棒、高精度的目标定位。

Result: 实验证明，所提方法在定位精度上优于基于单一信息的地面定位算法，并且岭估计有效增强了在观测条件受限下的鲁棒性和稳定性。

Conclusion: 基于岭估计的融合定位方法显著提升了无人机多传感器目标定位的精度和鲁棒性，特别适用于数据观测受限的复杂环境。

Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.

</details>


### [45] [QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing](https://arxiv.org/abs/2512.16325)
*Nan Zhou,Zuxin Li,Fanhang Man,Xuecheng Chen,Susu Xu,Fan Dang,Chaopeng Hong,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种提升非专用车载移动众包感知系统信息质量（QoI）的调度系统QUIDS，通过优化激励与调度，多维度提升感知覆盖率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 非专用车载众包感知系统存在感知覆盖率不足、感知可靠性低以及车辆动态参与等问题，导致最终信息质量不高。

Method: 提出QUIDS系统，引入聚合感知质量（ASQ）指标，综合覆盖率和可靠性。采用互助式基于置信度的车辆调度算法，在不确定条件下分配激励并估算感知可靠性，动态优化派遣策略。

Result: 在真实城市数据集上，QUIDS系统的ASQ提升达38%（对比无调度场景），较现有方法提升10%。地图重构误差降低39-74%。

Conclusion: QUIDS有效提升了非专用车载众包感知系统下的感知质量，实现了低成本、高质量的城市监控，适用多种智慧城市感知场景。

Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.

</details>


### [46] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种边缘设备与服务器协同推理的视觉-语言模型（VLM）框架，能够显著减少通信成本并保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，边缘设备需将原始视觉数据上传至服务器做VLM推理，但图像下采样常导致细节丢失，影响识别精度。作者旨在兼顾带宽成本与推理准确性。

Method: 提出两阶段推理方法：第一阶段服务器用下采样全局图像推理并利用内部注意力机制找出兴趣区域（RoI），通过输出token的min-entropy评估信心，若不满足要求，则请求边缘设备发送该区域的高细节局部图像。服务器融合全局与局部图像重新推理，只传输必要的内容。

Result: 在多种VLM架构上实验证明，该框架能大幅降低通信成本且基本保持推理精度。

Conclusion: 所提方法有效实现了在带宽受限场景下的高效协同VLM推理，是实际应用落地的重要补充。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [47] [GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction](https://arxiv.org/abs/2512.16357)
*Tao Hu,Weiyu Zhou,Yanjie Tu,Peng Wu,Wei Dong,Qingsen Yan,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于一步扩散的新方法GMODiff，有效用于多曝光HDR重建，比现有LDM方法快100倍，质量优异。


<details>
  <summary>Details</summary>
Motivation: LDM在低层视觉任务表现良好，但直接用于多曝光HDR重建存在动态范围受限、推理耗时高、生成内容幻觉等问题。亟需新方法解决这些挑战。

Method: 提出将HDR重建任务重构为条件引导的增益图（Gain Map, GM）估计，由GM表达扩展动态范围。通过基于回归的初始估计取代纯噪声启动去噪过程，仅用一步完成高质量GM生成，同时结合回归先验引导LDM去噪和解码，兼顾内容保真和感知质量。

Result: GMODiff在多个实验中优于现有方法，且推理速度提升百倍，显著减少生成内容幻觉，保持结构准确性。

Conclusion: GMODiff有效解决了LDM用于HDR重建时的动态范围受限、推理慢和内容幻觉等关键问题，兼具高感知质量与结构保真，为HDR重建提供了高效高质的解决思路。

Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.

</details>


### [48] [EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation](https://arxiv.org/abs/2512.16360)
*Haotian Ling,Zequn Chen,Qiuying Chen,Donglin Di,Yongjia Ma,Hao Li,Chen Wei,Zhulin Tao,Xun Yang*

Main category: cs.CV

TL;DR: 本文针对多角色动画中的身份对应问题，提出EverybodyDance方法，通过构建身份匹配图优化角色对应准确性，显著提升多角色生成动画的身份一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 在单角色动作驱动动画中已取得较大进展，但多角色环境下，尤其涉及角色位置互换时，身份正确对应成为一大挑战，单纯扩展现有方法难以解决。本文旨在解决多角色场景下，参考与生成帧的身份一致性问题。

Method: 引入EverybodyDance系统，核心为身份匹配图（IMG），通过构建带权完全二分图，将参考帧与生成帧中的角色节点间的亲和度作为边权，亲和度由提出的Mask-Query Attention（MQA）计算。将身份对应准确性形式化为图结构度量，并在训练期间优化。此外提出身份嵌入引导、多尺度匹配、预分类采样等多项配套策略协同提升表现。最后构建了多角色身份对应评测基准。

Result: EverybodyDance方法在身份对应准确性和视觉保真度上，相较现有最优基线方案，均取得显著提升。

Conclusion: EverybodyDance有效解决了多角色动画中身份对应难题，可作为多角色动作驱动生成的新基准方法。

Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.

</details>


### [49] [Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.16371)
*Mariam Hassan,Bastien Van Delft,Wuyang Li,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出了一种新的Text-to-Video（T2V）扩散生成流程，通过分解生成任务，在多个基准上取得了SOTA表现，并大幅提升采样速度。


<details>
  <summary>Details</summary>
Motivation: 现有T2V生成模型虽能生成高质量画面，但在复杂场景拼接及时序推理方面表现不佳。本研究认为主要原因在于模型不能生成语义正确或逻辑一致的初始帧。

Method: 作者提出Factorized Video Generation (FVG)方法，将T2V分为三个阶段：1）Reasoning，利用大语言模型（LLM）重写并明确描述初始场景；2）Composition，借助文本到图像模型（T2I）生成高质量锚帧；3）Temporal Synthesis，微调的视频模型以该锚帧为基础完成动画和理解时序。

Result: FVG方法在T2V CompBench基准上刷新了最新最好成绩，并显著提升了所有在VBench2测试的模型表现。采用anchor机制还能使采样步骤数减少70%，而不损失性能，带来明显速度提升。

Conclusion: Factorized Video Generation方法能够有效提升视频生成的效率、鲁棒性与可控性，为更高效实用的视频合成指明了方向。

Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis

</details>


### [50] [Adaptive Frequency Domain Alignment Network for Medical image segmentation](https://arxiv.org/abs/2512.16393)
*Zhanwei Li,Liang Li,Jiawan Zhang*

Main category: cs.CV

TL;DR: 该论文提出了AFDAN，一个通过在频域对齐特征实现跨域适应的神经网络框架，用于缓解医学图像分割中高质量标注数据不足的问题，并在多个数据集上取得了优异的分割效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割依赖高质量标注数据，但手工标注数据获取十分困难且耗时费力。因此，需要开发能充分利用有限数据并增强跨域泛化能力的方法。

Method: 提出了自适应频域对齐网络（AFDAN）。该方法包含：1）对抗性域学习模块，实现源域和目标域特征迁移；2）源-目标频域融合模块，实现跨域频域特征融合；3）时空频联合模块，整合空间和频域特征，提高分割精度。

Result: 在VITILIGO2025皮肤白斑分割数据集上IoU达90.9%；在 DRIVE 视网膜血管分割基准上IoU达82.6%，均优于现有SOTA方法。

Conclusion: AFDAN框架能够有效缓解医学图像分割中因标注数据稀缺造成的性能下降问题，具备良好的跨域泛化能力和实用前景。

Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.

</details>


### [51] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Gaussian Splatting的三维人脸重建方法，能从少量未标定的人脸图像中重建高质量中性姿势的人脸模型，并生成可用于标准图形管线的几何和纹理。


<details>
  <summary>Details</summary>
Motivation: 目前三维神经表示（如NeRFs）在还原三维场景方面具有较大优势，但对约束条件的适应性有限。此外，用于人脸重建时往往需要大量输入（如视频），且与现有图形管线的集成性不高。本文旨在解决输入数据量大和兼容性差的问题。

Method: 本文采用Gaussian Splatting替代NeRF作为三维表示，通过利用分割标注对人脸语义区域进行对齐，仅用11张图片即可重建中性姿态的人脸模型。同时，将高斯分布软约束到一个三角网格表面，以提升结构性，并基于几何准确性进一步优化。作者还提出如何将高斯点变换到纹理空间，进而实现基于视点的神经纹理。此外，使用具备重光能力的高斯模型，将纹理与光照分离，得到可在标准管线中直接应用的高分辨率本征纹理。最后，方法可适应不同照明条件，提高训练的鲁棒性。

Result: 实验表明，该方法能以极少的输入图片获得高保真的人脸重建结果，生成的三角网格及纹理可直接导入标准图形管线，无需修改现有资产或渲染流程。同时展示了其在文本驱动资产创建流程中的有效应用。

Conclusion: 本文方法能够高效、灵活地用极少图片进行高质量三维人脸重建，极大提高了与标准计算机图形管线的兼容性，推进了神经表示在人脸建模领域的实用化与落地。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [52] [BrepLLM: Native Boundary Representation Understanding with Large Language Models](https://arxiv.org/abs/2512.16413)
*Liyuan Deng,Hao Guo,Yunpeng Bai,Yongkang Dai,Huaxi Huang,Yilei Shi*

Main category: cs.CV

TL;DR: BrepLLM是一种能让大模型处理与理解3D几何Breps数据的框架，采用了跨模态对齐和多阶段微调训练，在3D对象分类和描述任务上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型主要针对文本序列而设计，难以直接处理复杂的3D模型，特别是包含详细几何和拓扑信息的边界表示（Brep）模型。为拓展LLM在3D领域的应用，需要新的方法实现3D几何数据与自然语言的桥接。

Method: 提出BrepLLM框架，包含两阶段训练流程：第一阶段采用自适应UV采样，将Brep数据转为包含几何与拓扑信息的图结构，并用层次化BrepEncoder编码为token，与冻结的CLIP文本编码进行对比学习实现模态对齐；第二阶段将BrepEncoder集成进LLM，采用三步式对齐与微调策略，包括MLP语义映射、LLM微调和Query专家混合增强多样性建模，并构建了包含26万多问答对的数据集Brep2Text。

Result: BrepLLM在3D对象分类和文本描述等多项任务上达到了最先进水平，相较于已有方法取得更优效果。

Conclusion: BrepLLM首次实现了LLM对原始3D Brep数据的解析与推理，桥接了结构化几何与自然语言的模态鸿沟，并推动了多模态大模型在3D设计与推理领域的能力边界。

Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.

</details>


### [53] [CountZES: Counting via Zero-Shot Exemplar Selection](https://arxiv.org/abs/2512.16415)
*Muhammad Ibraheem Siddiqui,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练框架 CountZES，用于在零样本设置下针对复杂场景中的新类别目标进行计数，成果优于现有同类方法，在多领域表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 零样本目标计数需要无需训练即可识别并计数仅由类别名称指定的未见类别目标。但当前方法要么依赖于开放词汇检测器导致多实例候选，要么随机采样补丁无法准确区分单个目标实例，精度受限，因此需要更精准、泛化性强的方法。

Method: CountZES 框架分为三步：（1）DAE 利用检测结果精炼得到精确的单个实例样本；（2）DGE 通过密度驱动的自监督方式筛选统计一致、语义紧凑的样本；（3）FCE 进一步通过特征聚类强化样本的视觉一致性，三者协同构建多样且互补的样本集合。

Result: 在多个自然、航空及医疗等领域的数据集上，CountZES 在零样本计数任务中取得了领先于现有方法的表现，展现了高度的跨领域泛化能力。

Conclusion: CountZES 无需训练即可在未见类别和新领域下高效实现目标计数，为复杂场景下的零样本目标计数提供了准确、泛化性强的解决方案。

Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.

</details>


### [54] [Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt](https://arxiv.org/abs/2512.16443)
*Shangxun Li,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一种新的、无需训练的方法，通过几何方式优化文本嵌入，有效提升了生成图像时主体一致性及文本对齐度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的文本到图像扩散模型在多图输出场景下，难以保持主体一致性，已有方法如模型微调或图像条件化计算成本高，需要针对每个主体优化；而现有免训练方法如1Prompt1Story又存在语义串扰问题，影响文本和图像的对齐。

Method: 作者提出一种基于几何视角优化文本嵌入的免训练方法。通过精炼文本嵌入，抑制非目标语义，从而解决语义交缠和文本对齐偏差，具体实现为对所有场景描述的嵌入进行几何优化而无需模型微调。

Result: 大量实验表明，该方法在提高图像主体一致性和文本对齐方面均优于当前最先进的基线方法。

Conclusion: 几何优化文本嵌入的免训练方法在提升视觉故事生成中的主体一致性与文本对齐方面效果明显，为相关领域提供了高效、易用的新思路。

Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.

</details>


### [55] [Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach](https://arxiv.org/abs/2512.16456)
*Masashi Hatano,Saptarshi Sinha,Jacob Chalk,Wei-Hong Li,Hideo Saito,Dima Damen*

Main category: cs.CV

TL;DR: 该论文研究了通过生成模型实现具备目光引导（gaze priming）特征的人体到目标物体或位置的动作序列生成，并首次整理并利用了一批包含目光引导行为的动作数据集。提出的方法在HD-EPIC数据集上获得了较高的动作生成成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作生成方法难以真实还原人类在实际任务中执行“寻找目标-接近目标-操作目标”这一序列动作时的自然细节，尤其缺乏对“目光引导”这一关键前置行为的描述与建模。作者希望实现更自然、更符合实际人类行为的动作生成。

Method: 首先从五个公开数据集整理获得了总计23,700条具备“目光引导”特征的人体动作序列。随后，使用文本条件扩散模型，在这些序列上进行预训练并以目标姿态或位置为条件微调。最终，结合‘Reach Success’与新引入的‘Prime Success’等指标对生成结果进行评估。

Result: 在最大的HD-EPIC数据集上，模型在以目标位置为条件生成时取得了60%的Prime Success（目光引导动作成功率）和89%的Reach Success（到达目标动作成功率）。

Conclusion: 方法有效结合了目光引导和目标动作生成，能够生成更加自然且符合人类动作习惯的行为序列，在任务导向的人体动作生成领域具有应用前景。

Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.

</details>


### [56] [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)
*Senmao Li,Kai Wang,Salman Khan,Fahad Shahbaz Khan,Jian Yang,Yaxing Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为StageVAR的视觉自回归（VAR）加速框架，能在保持高质量图像的同时大幅提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自回归模型通过预测不同尺度带来高质量图像，但大规模步进时计算复杂度和运行时间急剧上升，现有加速方法依赖人工划分阶段，忽视各阶段作用的重要性，因此迫切需要更智能和系统的加速方法。

Method: 作者首先分析了图像生成早期步骤对语义和结构一致性的关键作用，以及后期步骤主要细化细节的特性，据此提出StageVAR框架。在保证早期关键步骤完整的同时，对后期步骤利用其语义无关性和低秩特性进行裁剪或近似加速，且无需额外训练，策略为即插即用。

Result: StageVAR在GenEval和DPG指标上分别只损失0.01和0.26的精度，但速度提升最高可达3.4倍，整体表现优于同类主流加速基线。

Conclusion: 阶段感知设计是一种有效提升VAR模型生成效率的原则，StageVAR为高效视觉自回归生成提供了新的思路和强有力的实证支持。

Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.

</details>


### [57] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 该论文提出了一种结合人类感知与推理能力的无参考图像质量评估（BIQA）方法，通过引入人的评估数据和自描述推理过程，提高模型与人类一致性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 人类在评价图像质量时会融合感知线索与隐性推理，现有BIQA方法难以同时获得类人感知和自洽推理能力。作者希望让模型具备更加人性化和可解释的评价过程，更好地与人类的判断标准对齐。

Method: 作者首先收集了能反映人类感知-推理过程的评估数据。随后采用强化学习，把人的标注作为奖励信号，引导模型学习类人的感知和推理机制。为增强模型的自洽推理能力，设计了让模型仅凭自生成描述来推断图像质量的奖励策略。

Result: 实验结果显示，模型在分数预测（Pearson和Spearman相关系数等指标）上与现有最优BIQA方法相当。在模型与人类推理链的一致性（ROUGE-1分数）上，模型在千余个人工标注样本中达到0.512，明显优于基线（0.443）。

Conclusion: 该方法不仅提升了BIQA的可解释性和类人推理能力，还提升了模型与人类评判标准的一致性，迈出了向人类可解释推理方向发展的重要一步。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [58] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 该论文提出基于眼动行为的多模态情感识别（EMER）数据集和增强模型，通过融合眼动数据与面部表情显著提升了情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别主要依赖面部表情，但面部表情常用于社交伪装，未必真实反映内在情感，因此需要更多真实反映情感的信号。

Method: 作者采集了结合非侵入式眼动行为（如眼动序列、凝视热图）与面部表情视频的自然情感数据，构建EMER数据集；提出EMERT模型，利用模态对抗特征解耦与多任务Transformer，将眼动与面部表情特征有效融合；并引入多视角情感标注，制定七项多模态基准协议进行实验评估。

Result: EMERT模型在所有基准协议上大幅超越现有多模态方法，验证了眼动特征对鲁棒情感识别的重要性。

Conclusion: 研究系统分析了眼动行为在情感识别中的作用，提出的数据集和模型有效弥补了面部表达与真实情感之间的鸿沟，为更可靠的情感识别提供了新途径，相关资源已开源。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [59] [YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images](https://arxiv.org/abs/2512.16493)
*Huma Hafeez,Matthew Garratt,Jo Plested,Sankaran Iyer,Arcot Sowmya*

Main category: cs.CV

TL;DR: 论文提出了YOLO11-4K，一个高效实时的4K全景图像目标检测框架，有效应对了360度图像中的空间畸变和高分辨率问题。


<details>
  <summary>Details</summary>
Motivation: 由于全景360度图像存在空间畸变、广视域和超高分辨率等问题，常规目标检测器（如YOLO）难以兼顾高效性和准确性，尤其在4K及以上分辨率下计算压力巨大。

Method: YOLO11-4K在网络结构中引入了多尺度检测头，并加入P2层增强对小目标的检测能力。同时，采用GhostConv主干网络以降低计算复杂性并保持特征表达力。此外，研究者还手工标注并公开了CVIP360数据集，包含6876个4K全景场景下的目标边框。

Result: YOLO11-4K在0.50 IoU下获得了0.95 mAP，单帧推理时延仅28.3毫秒，比原始YOLO11的时延减少了75%，精度也有所提升（0.95 vs 0.908 mAP）。

Conclusion: YOLO11-4K在高分辨率全景目标检测中实现了高效和高精度的平衡，适用于现实世界中的全景应用。另外，该方法可拓展至自动驾驶、安防和增强现实等高分辨率目标检测场景。

Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.

</details>


### [60] [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2512.16494)
*Mengyuan Liu,Jiajie Liu,Jinyan Zhang,Wenhao Li,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的单目3D人体姿态估计方法PoseMoE，通过专家混合网络分别处理2D姿态和深度特征，并提出跨专家知识聚合模块，有效提升了3D姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于提升(lifting-based)的3D人体姿态估计方法通常将2D检测姿态作为中间表示，但将二维姿态和未知深度特征混合编码会导致深度不确定性对整体估计精度产生负面影响。作者意识到深度表征的独立性和准确性对精度提升至关重要。

Method: 1）提出混合专家网络，将2D姿态特征和深度特征分开由不同专家模块精细处理。2）设计跨专家知识聚合模块，实现2D与深度特征的双向信息交互，有助于特征融合。3）采用网络先对深度进行初步估计再融合2D特征，减少深度不确定性的影响。

Result: PoseMoE方法在Human3.6M、MPI-INF-3DHP和3DPW三个主流数据集上进行大量实验，结果优于传统提升式方法。

Conclusion: 本文的方法通过解耦2D姿态与深度特征的编码，有效降低了深度不确定性对2D姿态的负面影响，提升了单目3D人体姿态估计的整体准确性。

Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.

</details>


### [61] [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
*Beitong Zhou,Zhexiao Huang,Yuan Guo,Zhangxuan Gu,Tianyu Xia,Zichen Luo,Fei Tang,Dehan Kong,Yanyi Shang,Suling Ou,Zhenlin Guo,Changhua Meng,Shuheng Shen*

Main category: cs.CV

TL;DR: 本文提出了VenusBench-GD，这是一个跨平台、双语的大规模GUI实体定位基准，具有丰富的标注和分层任务，用于更全面评估GUI智能体对界面元素的理解与定位能力。


<details>
  <summary>Details</summary>
Motivation: 目前已有的GUI grounding基准存在数据规模小、应用场景单一、过度依赖特定平台或需专业知识等问题，导致模型评测和实际应用的局限性。本文旨在构建更大规模、适用范围更广、层次结构更丰富的基准数据集，提升模型的泛化能力和评测科学性。

Method: 本文提出VenusBench-GD基准：1) 涵盖多平台、多种应用场景和多样UI元素，数据量丰富且双语标注；2) 构建高质量数据标注流程，保证高准确性；3) 将grounding任务细分为基础和高级两大类六个子任务，实现分层次、多角度评估。

Result: 实验显示：基础任务上，多模态通用模型已能达到或超过专业GUI模型的表现；而在高级任务上，仍需专用模型，但这些模型存在过拟合严重和泛化能力差等问题。

Conclusion: 综合、分层的评测基准对于推动GUI grounding技术发展至关重要。VenusBench-GD为实际应用中多层次评估提供了工具，实验也印证了通用模型在基础任务的能力与专业模型在高级任务上的局限，强调了多维评测的必要性。

Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.

</details>


### [62] [Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization](https://arxiv.org/abs/2512.16504)
*Qiushuo Cheng,Jingjing Liu,Catherine Morgan,Alan Whone,Majid Mirmehdi*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督预训练方法，针对骨架动作时序定位任务设计了片段判别预训练任务，并融合U型模块提升帧级定位能力，在多个公开数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督对比学习主要针对动作识别任务，对于需要精确定位动作边界的骨架动作时序定位任务，现有方法在特征表达上存在不足，因此需设计更具时序敏感性的特征学习方法。

Method: 1）设计了片段判别的自监督预训练任务，将骨架动作序列划分为非重叠片段，通过对比学习促使不同片段的特征区分；2）融合动作识别骨干模型，利用U型结构模块，将中间层特征进行多尺度融合，以增强帧级特征的分辨能力。

Result: 所提出方法在BABEL数据集多个子集及评测协议下均提升了骨架对比学习方法的时序动作定位表现，同时在PKUMMD数据集上采用NTU RGB+D和BABEL预训练也取得了业界领先的迁移性能。

Conclusion: 本文的方法显著提升了骨架动作时序定位任务的特征表达能力和帧级动作边界检测精度，在多项指标和数据集上均优于现有主流方法，展示了较强的泛化和迁移能力。

Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.

</details>


### [63] [Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images](https://arxiv.org/abs/2512.16511)
*Hossein Javidnia*

Main category: cs.CV

TL;DR: 本论文提出了一种用于面部图像内在分解的新方法MAGINet，可从单张自拍照高精度预测无照明色散反照率，辅以一系列物理渲染通道，实现对真实人脸的高质量重光照和材质编辑。


<details>
  <summary>Details</summary>
Motivation: 在各种应用场景（如真实感重光照、数字替身、增强现实特效）中，需要将面部图像分解为照明无关的内在属性。但受限于单张图片以及复杂照明，准确内在分解长期具有挑战性。

Method: 提出多尺度注意力引导的MAGINet框架，包含分层残差编码、空间与通道注意力瓶颈、多尺度特征融合解码，先预测初始反照率，经RefinementNet细化，再利用Pix2PixHD预测五种物理渲染通道，组合形成完整内在分解。采用多种损失联合训练。

Result: 在FFHQ-UV-Intrinsics数据集上，MAGINet及其分解流程在反照率估计和整体渲染栈上达到了最优精度，优于以往U-Net及衍生模型，边界更锐利，光照不变性更好。

Conclusion: MAGINet实现了对单张面部照片的全面高质量内在分解，所生成的通道为真实人脸的高质量重光照与材质编辑提供了有效工具，推动了相关应用的发展。

Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.

</details>


### [64] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Test-Time Padding (TTP)的新型对抗样本防御方法，无需重新训练和标注数据，在推理阶段通过空间填充检测与自适应修复对抗输入，提高模型鲁棒性且不损失原有准确率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型如CLIP在零样本识别表现优异，但对对抗扰动非常脆弱，威胁安全关键应用。现有防御方法要么需高昂的重新训练和标注成本（训练时防御），要么在推理时无法有效区分干净样本和对抗样本，导致鲁棒性和准确率难以两全。

Method: TTP方法在推理阶段首先通过前后空间填充的CLIP特征余弦相似性变化来检测对抗输入，为不同架构和数据集提供通用阈值；检测到的对抗样本，采用可训练的填充修复注意力，结合相似性感知集成策略做最终判断；正常样本则默认不改变，或可选用现有推理时自适应技术提升准确率。

Result: 在多种CLIP骨干模型和细粒度基准测试上，TTP在推理时防御性能优于现有最优方法，在提升对抗鲁棒性的同时不降低干净样本的准确率。

Conclusion: TTP是一种高效、简单、无需标签和额外训练的数据推理时防御框架，既提升了对抗鲁棒性，也保留了原有识别准确率，实用性强。代码即将公布。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [65] [N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.16561)
*Yuxin Wang,Lei Ke,Boqiang Zhang,Tianyuan Qu,Hanxun Yu,Zhenpeng Huang,Meng Yu,Dan Xu,Dong Yu*

Main category: cs.CV

TL;DR: 该论文提出了N3D-VLM，一个将原生3D物体感知与3D视觉推理无缝融合的统一框架，实现了更精准的三维定位及空间理解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型只能基于2D图像回答问题，缺乏对3D场景空间关系和深度的本质理解，限制了实际应用。

Method: 该方法赋予模型原生3D物体感知能力，能基于文本直接在3D空间中定位物体；构建高效数据生成流程，通过深度估计将大规模2D标注提升至3D，大幅扩充训练数据，涵盖三维物体定位和空间推理。模型进一步基于3D定位进行结构化推理，提升理解能力。

Result: 实验显示N3D-VLM在3D定位和视觉语言3D空间推理任务上都达到了SOTA（最佳）水平，并显著超越现有方法。

Conclusion: N3D-VLM实现了统一的原生3D感知和推理框架，是面向空间理解的多模态模型发展的重要进步，为后续3D场景感知与推理任务奠定了基础。

Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

</details>


### [66] [4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction](https://arxiv.org/abs/2512.16564)
*Kirill Mazur,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 本文提出了一种能够从普通单目RGB视频中动态重建完整场景的系统，不仅重建当前可见部分，还可回放历史时刻的全部场景。该方法实现了高质量4D时空重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目视频重建多用于静态部分，无法持续、完整地重建包含多个运动物体的全场景，限制了在多对象动态场景中的应用，如多物体扫描与对象持久化。

Method: 方法将场景分解为刚性3D基元，通过密集2D对应联合优化基元刚性运动，得到随时间变化的4D（时空）重建。对于变得不可见的对象，引入基于运动分组的运动外推机制以保持重建连续性。

Result: 系统能实现时空范畴内的4D重建，包括可回放的多物体与关节物体的动态三维结构。实验表明在对象扫描及多对象数据集上，无论定量还是定性评测均优于主流方法。

Conclusion: 该系统拓展了单目视频的动态场景重建能力，实现了场景的时空持续追踪，提升了多物体场景下的重建质量及应用潜力。

Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.

</details>


### [67] [Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2512.16567)
*Yin Zhang,Yongqiang Zhang,Yaoyue Zheng,Bogdan Raducanu,Dan Liu*

Main category: cs.CV

TL;DR: 本文发现视觉基础模型（VFM）微调时常由于预训练带来的伪影影响泛化能力，提出了一种基于因果机制的新颖微调方法，有效改善跨域语义分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有VFM微调工作多关注轻量化参数适配或特征细化，忽略了VFM长期预训练易存在伪影与非因果因素。这些伪影会阻碍有用表征利用，降低泛化能力，尤其在未见域表现差。作者希望剔除非因果因素，提升跨域语义分割的鲁棒性。

Method: 提出Causal-Tune微调策略。用离散余弦变换（DCT）对每层特征做频谱分析，采用高斯带通滤波分离因果与非因果分量。引入因果感知可学习token以进一步精化频谱域内的因果特征，丢弃非因果分量。最终经逆DCT还原至空间域传给下一层。

Result: 在多项跨域语义分割任务上，Causal-Tune均表现优越，特别是在恶劣天气如雪天条件下，mIoU比基线提升4.8%。

Conclusion: Causal-Tune显式区分与处理因果/非因果因素，能有效抑制VFM伪影，推动DGSS领域泛化能力提升。该方法简单高效，对现有VFM适配具有实际价值。

Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.

</details>


### [68] [CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series](https://arxiv.org/abs/2512.16577)
*Nico Albert Disch,Saikat Roy,Constantin Ulrich,Yannick Kirchhoff,Maximilian Rokuss,Robin Peretzke,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 本文提出了CRONOS模型，一种能够实现3D医学扫描序列连续时间点预测的新方法，支持不规则采样和多时间点数据输入，超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学影像预测模型，要么只能从单次扫描推断，要么只能处理固定时间网格，或者只做全局标签预测，导致难以在不规则采样下进行体素级的前景预测。

Method: CRONOS提出了统一的many-to-one架构，可以从多条历史扫描中预测任意未来时刻的扫描结果，并同时兼容离散和连续时间戳。其核心为空间-时间速度场的学习，直接于体素空间进行预测。

Result: CRONOS在Cine-MRI、perf. CT以及纵向MRI三个公开3D医学图像数据集上超过主流基线方法，同时保持较强的计算效率。

Conclusion: CRONOS首次实现了3D医学影像的多上下文、连续时间点预测，有更强的灵活性和准确性，推动了不规则采样下3D医学影像演变预测技术的发展。

Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.

</details>


### [69] [Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs](https://arxiv.org/abs/2512.16584)
*Jintao Tong,Jiaqi Gu,Yujing Lou,Lubin Fan,Yixiong Zou,Yue Wu,Jieping Ye,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Sketch-in-Latents (SkiLa)的新范式，使多模态大模型能够在推理过程中自动生成和利用视觉想象（潜在素描token），显著提升视觉相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视觉理解任务上虽表现出色，但缺乏“视觉想象”能力，即在推理过程中无法像人类一样自发地在思维中生成、操作视觉信息。这是因为人类可在大脑统一空间中进行视觉-文本想象，而现有方法往往依赖外部工具，缺乏这种灵活性。作者希望赋予模型原生视觉想象能力，实现更自然的多模态推理。

Method: 作者提出了SkiLa框架，将视觉token无缝插入到由文本token承载的推理过程中。具体做法是让多模态模型在自回归推理时，动态切换文本思考模式（生成think token）和视觉素描模式（生成latent sketch token），并通过潜在视觉语义重建机制确保这些视觉token具备语义基础，增强视觉想象能力。

Result: 大量实验表明，SkiLa在以视觉为核心的多模态任务上取得了优越表现，并展示出对多种常规多模态基准任务的出色泛化能力。

Conclusion: SkiLa创新性地扩展了现有MLLMs的推理能力，实现了统一空间下的视觉-文本思维过程，为提升模型类人的“视觉想象”与推理能力开辟新路径。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

</details>


### [70] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种将Swin-transformer应用于扩散模型的架构（Yuan-TecSwin），提升了图像生成的质量和推理性能，并在ImageNet上取得了极低的FID分数。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的扩散模型因其局部操作限制了对全局语义的建模能力。为此，作者尝试引入更擅长处理长距离依赖的Swin-transformer，以提升生成效果。

Method: 用Swin-transformer模块替换了扩散模型中的编码器与解码器中的CNN模块，以增强对全局信息的建模。同时，改进文本编码器和文本条件融合机制，优化文本-图像对齐。在推理层面，通过调整扩散过程中的时间步长提升生成效率和质量。

Result: 提出的模型在ImageNet生成任务中达到了1.37的最优FID分数，相比现有方法性能更优。同时，对比实验证明人类难以区分模型生成与真实人类绘制的图像。

Conclusion: Swin-transformer在扩散模型中的应用有效提升了图像的生成质量和效率，且改进的文本条件机制带来了更优的文本-图像对齐效果。该方法有潜力推动生成模型的进一步发展。

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [71] [Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment](https://arxiv.org/abs/2512.16609)
*Ayush Bhavsar*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hazedefy的轻量级去雾算法，可实时提升视频和摄像头画面质量，具备高效、易于部署的优点，适用于普通消费级硬件。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法普遍计算复杂，难以在移动终端或嵌入式设备上实时运行。因此，需要一种既高效又便于实际应用的去雾方法，以满足实际场景下对实时处理和设备资源有限的需求。

Method: 本文基于暗通道先验与大气散射模型，设计了一套去雾流程，包括伽马自适应重建、快速透射率近似（引入下界保障数值稳定）、基于分数化顶端像素均值的气氛光稳定估计器，并可选用色彩平衡模块。整个流程计算量低，无需GPU支持。

Result: 在实拍图像和视频上的实验结果显示，该方法有效提升了画面对比度和能见度，同时能够在移动及嵌入式设备上流畅运行。

Conclusion: Hazedefy去雾流程在保证去雾效果的同时，大幅降低了运算复杂度，具有很高的实际部署价值，尤其适用于对硬件和实时性有要求的应用场景。

Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.

</details>


### [72] [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)
*Yifan Zhou,Zeqi Xiao,Tianyi Wei,Shuai Yang,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了Log-linear Sparse Attention (LLSA)，一种适用于极长序列的可训练稀疏注意力机制，极大提升了Diffusion Transformers在长序列视觉生成任务中的效率，并显著加速了推理与训练速度。


<details>
  <summary>Details</summary>
Motivation: 目前DiT在视觉生成任务表现优异，但自注意力的计算复杂度与序列长度平方正相关，限制了对长序列的扩展。现有稀疏注意力方法在压缩token和选择相关块时仍存在瓶颈，需更高效方法提升可扩展性。

Method: LLSA采用层次化稀疏选择机制，将原始块逐层Top-K筛选，逐步利用前一级的索引。同时引入层次化KV增强机制，在利用更少token的情况下最大限度保留全局上下文。提供高效的GPU实现，仅用稀疏索引实现正反向传播，无需密集注意力掩码。

Result: 在无需patch划分和VAE编码的高分辨率像素空间上，LLSA在256x256像素token序列下实现了28.27倍的注意力推理加速、6.09倍的DiT训练加速，同时生成质量保持不变。

Conclusion: LLSA展现了在训练和推理极长序列Diffusion Transformers中的巨大优势，为高效训练长序列视觉生成模型提供了有前景的方法。

Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

</details>


### [73] [Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation](https://arxiv.org/abs/2512.16620)
*Kanwal Aftab,Graham Adams,Mark Scanlon*

Main category: cs.CV

TL;DR: 该论文提出了一个利用电源插座作为一致性室内地理标记的多阶段深度学习管道，用于提升室内多媒体地理定位能力，尤其服务于数字取证和打击重大犯罪。


<details>
  <summary>Details</summary>
Motivation: 室外图像地理定位领域发展迅速，但室内定位因房间布局相似、光照变化大、GPS信号弱等问题发展受限。作者希望提升室内图片定位能力，以助力执法机关在打击人口贩运、儿童剥削等犯罪中的取证效率。

Method: 论文提出三阶段深度学习管道：阶段一用YOLOv11检测插座（mAP@0.5=0.843）；阶段二用Xception分类12类插座类型（准确率0.912）；阶段三将插座类型映射至国家（准确率0.96，置信度>90%）。为此构建并增强了插座检测与分类数据集，并在真实条件下的Hotels-50K（TraffickCam子集）进行评测。

Result: 本方法在TraffickCam数据集上表现良好，即便在非理想光照和非专业角度下，插座检测与分类都取得较高准确率。插座类型与国家映射准确率达0.96。数据与代码已开源。

Conclusion: 电源插座作为室内一致标记特征，有助于提升室内地理定位精度。该流程为数字取证实用落地迈出了重要一步，有望服务于真实犯罪调查与执法场景。

Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.

</details>


### [74] [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)
*Linghui Shen,Mingyue Cui,Xingyi Yang*

Main category: cs.CV

TL;DR: DeContext是一种保护个人图像防止被未经授权编辑的新方法，通过在扩散模型的多模态注意力机制中注入微小扰动，有效阻止不当的图像篡改，实验表现优秀。


<details>
  <summary>Details</summary>
Motivation: 当前强大的上下文扩散模型带来了用户极易编辑且真实感很强的图像编辑能力，但也加剧了隐私风险，个人图像容易被恶意篡改用于冒充身份、误导信息等。已有针对文本到图像生成的防护方法，但对新型、大规模的DiT架构的上下文模型防护仍未充分研究。

Method: 作者提出了DeContext方法，通过向模型多模态交叉注意力层中注入针对性的微小扰动，削弱源图像语境在输出中的传播，从而切断输入与输出的关键关联。还发现初期去噪步骤和特定的变换器模块是语境传播的关键，针对性集中扰动以达到高效防护。

Result: 在Flux Kontext和Step1X-Edit等实际数据集上，DeContext能有效阻止未经授权的图像编辑，同时保持较高的视觉质量，表现优越。

Conclusion: 通过基于注意力的扰动方式能够强有力地防御当下流行的图像生成模型中的不当篡改，DeContext方法高效、稳健且实用，提升了对个人图像隐私的保护能力。

Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.

</details>


### [75] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 提出了SARMAE方法，通过引入噪声感知的mask自编码器和光学先验，实现了SAR图像的自监督表征学习，并建立了首个百万级配对SAR数据集，显著提升了SAR任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前SAR图像的深度学习由于缺乏大规模数据集以及固有斑点噪声，导致难以获得鲁棒、细粒度的语义特征。需要新的方法和数据资源来提升SAR遥感图像的理解能力。

Method: 1）构建首个百万级的SAR-1M数据集，并配有光学图像作为配对先验；2）提出SARMAE模型：一方面，在自编码过程中显式注入SAR专有的斑点噪声（SARE），提高模型对噪声鲁棒性；另一方面，引入语义锚定约束（SARC）利用光学先验对齐和约束SAR特征，使其更符合语义一致性。

Result: SARMAE模型在多个SAR公开数据集的分类、检测和分割任务上均大幅度优于现有方法，取得了最新的性能水平，充分验证了方法的有效性和通用性。

Conclusion: SARMAE解决了SAR遥感领域数据短缺与噪声干扰难题，依托大规模数据与创新的自监督方法，为SAR下游任务提供了更强表征基础，对遥感智能解译具有重要推动意义。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [76] [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)
*Giorgos Petsangourakis,Christos Sgouropoulos,Bill Psomas,Theodoros Giannakopoulos,Giorgos Sfikas,Ioannis Kakogeorgiou*

Main category: cs.CV

TL;DR: 本文提出REGLUE框架，通过全局-局部统一编码，将VAE隐变量、局部VFM语义及全局[CLS] token在单一主干网络中联合建模，实现了更高效的图像合成。


<details>
  <summary>Details</summary>
Motivation: 现有LDMs在图像生成上有效，但其降噪目标仅能间接提供语义监督，语义学习慢且样本质量受限。利用VFM注入语义的已有工作要么仅对齐特征，要么只建模少量VFM特征，未充分利用VFM的空间语义信息。

Method: 提出REGLUE方法，将VAE隐变量、本地（patch级）VFM语义和全局（图像级[CLS]）token以轻量非线性卷积压缩聚合形成低维空间结构表征，并与VAE隐变量在扩散过程中耦合，并用外部对齐损失进一步正则化内部表征；所有信息通过统一的SiT主干整合。

Result: 在ImageNet 256x256实验中，相比先进基线（SiT-B/2、SiT-XL/2、REPA、ReDi、REG），REGLUE在FID和收敛速度上均有提升。消融和对比实验表明空间VFM信息和非线性压缩关键，且全局token与外部对齐为框架带来互补增强。

Conclusion: REGLUE有效整合全局与局部VFM语义至扩散模型，为高效高质图像生成提供了新路径，并证明多层空间特征的联合建模和非线性聚合重要性。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .

</details>


### [77] [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)
*Ole Beisswenger,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: FrameDiffuser是一种自回归神经渲染框架，能依赖G-buffer数据及前一帧输出高效生成时序一致、光照真实的图像，适合交互式应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的神经渲染方法，要么无法保证时序一致性，要么计算资源消耗过大，不适用于需要用户实时交互的应用。为解决这些问题，亟需既高效又时序稳定的渲染方案。

Method: 提出FrameDiffuser框架，采用自回归结构，通过G-buffer和模型自身生成的前一帧进行双重条件引导，包括结构引导（ControlNet）和时序连贯（ControlLoRA），结合三阶段训练策略以实现高效稳定的序列生成。此外，模型针对单一环境进行专门训练以提升效果。

Result: 与通用化模型相比，FrameDiffuser在特定场景下显著提升了光照、阴影和反射等细节的保真度，实现了高推理速度和长期时序稳定性，适合交互性强的应用。

Conclusion: 专用于单环境的新自回归神经渲染方法FrameDiffuser解决了现有模型的时序一致性与高效性难题，可为交互式应用提供高质量、实时、稳定的图像生成效果。

Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.

</details>


### [78] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 本文提出了一种利用“主体指纹”方法，通过深度学习将同一人的影像在潜在空间中聚集，实现跨数据集的主体再识别，有效解决公开医学影像集混用可能产生的数据泄露问题。


<details>
  <summary>Details</summary>
Motivation: 多开源医学影像数据集的合并使用在AI模型训练中很常见，但同一受试者出现在不同数据集会导致数据泄露，带来模型评估结果虚高。因此亟需一种能够发现跨数据集同源主体的方法。

Method: 作者基于ResNet-50神经网络，采用triplet margin loss，将同一主体影像映射到潜在空间的一个区域，实现主体再识别。方法在ChestXray-14、BraTS-2021等数据集上，采用few-shot学习进行20-way~1000-way的主体指纹实验。

Result: 模型在不同情形下表现优异：在ChestXray-14上20-way 1-shot场景下Mean-Recall-@-K高达99.10%，在500-way 5-shot下为90.06%；在BraTS-2021上20-way 1-shot为99.20%，100-way 3-shot为98.86%。

Conclusion: 提出的方法能够可靠地区分和识别跨数据集的同一主体，有效检测和预防医学影像数据泄露，提高后续AI模型的评估可信度。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [79] [Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?](https://arxiv.org/abs/2512.16688)
*Serafino Pandolfini,Lorenzo Pellegrini,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文系统评估了当前主流的深度伪造检测器在局部图像修补（inpainting）检测方面的表现。尽管它们原本训练于检测全合成图像，但在检测局部编辑时表现有限。结果显示，经过多种生成器训练的模型在检测中大面积或再生成型修补时效果较好，优于许多现有检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，图像修补与局部编辑技术日益逼真，被用于越来越多网络安全威胁场景。目前关于检测全合成图像的技术已较成熟，但这些方法在检测局部图像编辑时的能力尚不明确，因此需要系统评估其泛化能力。

Method: 作者选取并系统性地评估了多种最先进的深度伪造检测器，将其应用于局部修补检测任务，利用包含多种生成器、遮罩尺寸、修补技术的数据集进行对比实验。

Result: 实验发现，针对不同生成器大规模训练的模型对局部修补操作有一定检测能力，尤其在中到大面积的修补或再生成式修补下表现更优，超过很多现有专门针对局部操作的检测方法。

Conclusion: 当前主流深度伪造检测模型具备一定的局部修补检测泛化能力，特别是在较显著的编辑语境下表现优异，但在更细粒度或隐蔽的编辑检测方面仍有待提升。

Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.

</details>


### [80] [SDFoam: Signed-Distance Foam for explicit surface reconstruction](https://arxiv.org/abs/2512.16706)
*Antonella Rech,Nicola Conci,Nicola Garau*

Main category: cs.CV

TL;DR: 本论文提出了SDFoam方法，将显式Voronoi图与隐式有符号距离场（SDF）结合，提升3D神经渲染技术中的精确网格重建能力，同时保持高效的渲染速度和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的体积渲染和3D高斯撒点（3DGS）等方法在视点合成和渲染速度上取得了一定进展，但在精确网格重建方面表现仍有不足，容易出现浮点碎片和拓扑错误。本文旨在填补高效渲染与高质量网格重建之间的技术空白。

Method: 提出SDFoam框架，将显式Voronoi图与隐式有符号距离场（SDF）联合学习，通过射线追踪优化场景并采用Eikonal项进行正则化。SDF用于提供度量一致的等值面，引导Voronoi单元面与零等值集对齐，从而提升网格重建的清晰度与一致性。

Result: SDFoam在多个不同场景下显著提升了网格重建的准确度（Chamfer距离更优），同时保持与RadiantFoam类似的PSNR和SSIM，渲染质量高且效率未降低。

Conclusion: SDFoam在不牺牲渲染效率和外观保真的前提下，极大提升了3D神经场的网格重建能力，为高效、精细的三维重建和渲染提供了新的解决方案。

Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.

</details>


### [81] [A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry](https://arxiv.org/abs/2512.16710)
*Chiara Di Vece,Zhehua Mao,Netanell Avisdris,Brian Dromey,Raffaele Napolitano,Dafna Ben Bashat,Francisco Vasconcelos,Danail Stoyanov,Leo Joskowicz,Sophia Bano*

Main category: cs.CV

TL;DR: 本文构建并公开了一个多中心、多设备的胎儿超声图像数据集，并配套提供专家标注的关键解剖点，支持更可靠和泛化能力更强的AI辅助胎儿生长评估方法的开发。


<details>
  <summary>Details</summary>
Motivation: 现有胎儿超声生长评估依赖人工在标准切面上标注解剖点进行测量，费时并易受操作员及设备差异影响，导致自动化方法的复现性受限。目前缺乏覆盖多中心、多设备的高质量带标注的公开数据集，限制了AI方法的研究和临床转化。

Method: 作者收集了来自3个临床中心7台设备的4513张胎儿超声去标识化图像，包含1904名受试者。对于临床常用的胎儿双顶径、枕额径、腹部横径及前后径、股骨长度等测量点均由专家进行精确标注。数据集按标准方式划分了训练集和测试集，同时开源了评估代码和基线模型，并通过自动测量模型评估了域间迁移影响。

Result: 实验证明，仅在单中心内训练和评估的AI方法会显著高估性能，而多中心测试结果能更真实反映模型的泛化能力。数据、标注、训练代码和评测流程均已对外公开，便于学术界公平对比方法。

Conclusion: 该数据集是首个涵盖所有主要胎儿生物测量指标的多中心、多设备、精确定标的公开数据集，为领域适应性研究和具备多中心泛化能力的AI辅助胎儿生长评估方法提供了有力支撑，有望推动临床实际应用的发展。

Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.

</details>


### [82] [OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition](https://arxiv.org/abs/2512.16727)
*Haochen Chang,Pengfei Ren,Buyuan Zhang,Da Li,Tianhao Han,Haoyang Zhang,Liang Xie,Hongbo Chen,Erwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种自动生成微手势骨架数据的多视角自监督流程，并基于此构建了首个大规模在线微手势识别骨架数据集OMG-Bench。同时提出了新颖的层次化记忆增强Transformer（HMATr）模型，在手势检测和分类任务上显著超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 微手势识别在虚拟现实和增强现实交互中具有重要意义。然而，现有公开数据集稀缺、标注困难，且现有方法大多针对特定任务，难以推广。因此，需要新的数据集和高效统一的识别方法。

Method: 作者提出了一套多视角自监督的数据生成与半自动标注流程，并由专家修正。基于此流程构建了大规模OMG-Bench数据集。模型方面，提出层次化记忆增强Transformer(HMATr), 利用多级记忆库存储帧级细节和窗口级语义，通过可学习的位置信息查询隐式编码手势位置与语义，实现手势检测与分类的端到端统一。

Result: OMG-Bench数据集包含40类细微手势，13,948个实例，反映了手势微小、动态快、连续性强等挑战。实验结果显示，所提HMATr框架在检测率上超过当前最佳方法7.6%。

Conclusion: 该研究打造了首个骨架级微手势大规模公开数据集，并提出了统一检测与分类的新模型，在微手势识别方向建立了强有力的新基线，有助于促进该领域发展。

Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/

</details>


### [83] [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2512.16740)
*Yunkai Yang,Yudong Zhang,Kunquan Zhang,Jinxiao Zhang,Xinying Chen,Haohuan Fu,Runmin Dong*

Main category: cs.CV

TL;DR: 针对遥感图像语义分割中合成训练数据存在控制难和质量不稳定的问题，提出了TODSynth框架，通过多模态扩散Transformer和任务反馈引导的采样策略，显著提升了合成数据的效果。


<details>
  <summary>Details</summary>
Motivation: 遥感领域人工数据标注成本高，合成数据被视为扩充数据集和降低标注负担的有效途径。然而，现有合成数据在语义掩码的精确控制和生成样本质量方面存在瓶颈，影响其在下游分割任务中的应用价值。

Method: 提出了TODSynth任务导向数据合成框架，核心包括：1）基于DiT的多模态扩散Transformer（MM-DiT），实现文本-图像-掩码的三重注意力联合控制，2）基于任务反馈的可插拔采样策略，3）提出了控制-校正流匹配（CRFM）方法，通过语义损失动态调整早期采样方向，提升生成稳定性。

Result: 系统评估了不同控制策略，证实文本-图像-掩码联合注意力及分支全微调显著提升了遥感语义分割数据合成的有效性，特别是在小样本和复杂场景下。CRFM方法有效减缓了合成图片的不稳定性和分布差异。在多项实验中，TODSynth生成的数据在分割任务上全面优于现有方法。

Conclusion: TODSynth框架结合多模态控制和任务反馈驱动的采样，在遥感语义分割中能够持续生成更稳定、更具任务导向性的高质量合成数据，推动了可控合成方法在实际应用中的实用性和性能提升。

Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.

</details>


### [84] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: 本文提出了TreeNet，一种采用二叉树结构的低复杂度图像压缩神经网络，在保持高压缩性能的同时大幅降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前学习型图像压缩方法在性能上优于传统方法，但高计算复杂度阻碍了其广泛应用，故需要开发低复杂度的高效模型。

Method: TreeNet采用二叉树状的编码-解码架构，通过注意力特征融合机制整合多分支特征，实现高效的数据表示和重建。同时，通过与最新标准JPEG AI等方法对比，并在多个基准数据集上验证性能。还通过消融实验分析了潜在特征对重建质量的影响。

Result: 在低比特率下，TreeNet相较于JPEG AI在BD-rate上平均提升4.83%，同时模型复杂度降低87.82%。

Conclusion: TreeNet在大幅降低模型复杂度的前提下，实现了优于当前最新学习型压缩标准的压缩性能，适合实际部署，有望推动学习型图像压缩算法的应用。

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [85] [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)
*Zhiyang Guo,Ori Zhang,Jax Xiang,Alan Zhao,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出Make-It-Poseable框架，通过操控3D角色的潜在空间表示实现高质量变形，提升了姿态拟合和拓扑适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D角色自动绑定与姿态生成方法在蒙皮权重预测、拓扑一致性和姿态还原方面表现有限，影响了其健壮性与泛化能力，实际场景应用受限。

Method: 作者重新定义角色变形为潜在空间（latent space）中的变换问题。采用潜在姿态变换器（latent posing transformer）对形状token进行基于骨骼运动的操控，并引入密集姿态表达实现精细控制。为获得高精度几何及处理拓扑变化，提出潜在空间监督策略和自适应补全模块。

Result: 方法在姿态质量、拓扑处理等方面效果优异，并可自然扩展至局部替换与角色细化等三维编辑任务。

Conclusion: Make-It-Poseable在3D角色姿态处理上表现出更强的健壮性、适应性和高精度几何重建潜力，有望推广至更广泛的三维编辑和动画制作场景。

Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.

</details>


### [86] [FlowDet: Unifying Object Detection and Generative Transport Flows](https://arxiv.org/abs/2512.16771)
*Enis Baty,C. P. Bridges,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出了FlowDet，这是首个采用条件流匹配（Conditional Flow Matching）技术进行目标检测的方法。该方法改进了DiffusionDet的扩散式生成思路，达到了更高效和更优的检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有的DiffusionDet方法通过扩散过程完成目标检测，但其随机转换路径较为复杂，推理速度和性能提升受限。希望找到更高效、表现更强的生成式检测方法。

Method: 作者将目标检测重新表述为条件流匹配问题，改变了生成式检测的传输路径，从曲线、随机性较大的扩散路径变为更直接、简单的路径。该方法允许在不重新训练的情况下自由调整检测框数量和推理步数。

Result: FlowDet在COCO和LVIS等多个数据集、不同主干网络、不同精确率/召回率下均超越了DiffusionDet及非生成式基线方法。特别是在受召回率约束时，表现更明显。精度提升最高达到+3.6% AP和+4.2% AP_rare。

Conclusion: 条件流匹配方法为目标检测带来更高效、性能更优的生成式路径，是对扩散模型目标检测的一次关键改进。

Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.

</details>


### [87] [Kling-Omni Technical Report](https://arxiv.org/abs/2512.16776)
*Kling Team,Jialu Chen,Yuanzheng Ci,Xiangyu Du,Zipeng Feng,Kun Gai,Sainan Guo,Feng Han,Jingbin He,Kang He,Xiao Hu,Xiaohua Hu,Boyuan Jiang,Fangyuan Kong,Hang Li,Jie Li,Qingyu Li,Shen Li,Xiaohan Li,Yan Li,Jiajun Liang,Borui Liao,Yiqiao Liao,Weihong Lin,Quande Liu,Xiaokun Liu,Yilun Liu,Yuliang Liu,Shun Lu,Hangyu Mao,Yunyao Mao,Haodong Ouyang,Wenyu Qin,Wanqi Shi,Xiaoyu Shi,Lianghao Su,Haozhi Sun,Peiqin Sun,Pengfei Wan,Chao Wang,Chenyu Wang,Meng Wang,Qiulin Wang,Runqi Wang,Xintao Wang,Xuebo Wang,Zekun Wang,Min Wei,Tiancheng Wen,Guohao Wu,Xiaoshi Wu,Zhenhua Wu,Da Xie,Yingtong Xiong,Yulong Xu,Sile Yang,Zikang Yang,Weicai Ye,Ziyang Yuan,Shenglong Zhang,Shuaiyu Zhang,Yuanxing Zhang,Yufan Zhang,Wenzheng Zhao,Ruiliang Zhou,Yan Zhou,Guosheng Zhu,Yongjie Zhu*

Main category: cs.CV

TL;DR: 本文提出Kling-Omni，一种通用生成框架，可基于多模态视觉与语言输入直接生成高保真视频，集成生成、编辑和智能推理等功能，支持文本、图片、视频等多种输入，最终实现高质量视频内容的智能创作。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成与编辑工具多为分离式流程，难以实现多模态、智能化、一体化的视频创作需求。随着用户需求和应用场景的复杂化，亟需一个能统一多输入类型、具备推理和高质量生成能力的综合框架。

Method: Kling-Omni采用端到端架构，将文本、图片、视频等多模态输入转化为统一的表示，集成视频生成、编辑与推理能力。依托全面的数据系统、大规模高效预训练及推理优化构建而成，实现多输入、高质量视频创作。

Result: 实验证明，Kling-Omni在上下文生成、推理驱动编辑、多模态指令执行等方面表现卓越，生成的视频内容具备电影级画质与智能特性。

Conclusion: Kling-Omni不仅是一个内容创作工具，更代表了向多模态世界模拟器的重大进步，为智能感知、推理、生成和交互提供了强大基础。

Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.

</details>


### [88] [R3ST: A Synthetic 3D Dataset With Realistic Trajectories](https://arxiv.org/abs/2512.16784)
*Simone Teglia,Claudia Melis Tonti,Francesco Pro,Leonardo Russo,Andrea Alfarano,Leonardo Pentassuglia,Irene Amerini*

Main category: cs.CV

TL;DR: 本文介绍了一个名为R3ST的新型合成数据集，融合了真实世界的交通轨迹，以提升模型的训练效果和评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有真实交通数据集缺乏精确标注，而传统合成数据集虽然易于标注、生成效率高，但其车辆轨迹往往缺乏真实性。研究人员希望创建一个兼具真实行驶轨迹与高质量标注的新型数据集，以更好地推进交通分析与安全相关的视觉模型研究。

Method: R3ST通过在合成的3D环境中，融合来自SinD（通过无人机视角记录的真实轨迹）的车辆运动轨迹，从而生成兼具多模态准确标注与高度真实交通行为的合成数据集。

Result: 新数据集既提供了真实的人类驾驶轨迹，又拥有准确的多模态标注，有效缩小了合成数据与现实交通轨迹之间的差距。

Conclusion: R3ST为道路交通轨迹预测等研究方向提供了高质量的新工具，有望提升交通安全相关视觉模型的表现，推动该领域的发展。

Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.

</details>


### [89] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的全身运动追踪方法KineST，在只需从头戴式设备获得稀疏信号的情况下，实现了高精度、时序连贯、且高效的全身姿态重建。


<details>
  <summary>Details</summary>
Motivation: 头戴式设备是AR/VR应用的主流硬件，但这类设备仅能获取稀疏信号，导致实时、真实的人体全身姿态重建极具挑战。现有方法在性能、连贯性和效率之间难以平衡。

Method: KineST方法采用运动学先验指导的双向扫描，重构状态空间，并利用混合时空表征紧密结合空间与时间信息。此外，引入几何角速度损失，强化对运动的物理约束，提高重建姿态的稳定性。

Result: 实验表明，KineST在保持轻量级框架的同时，准确性和时序一致性都优于当前主流方法。

Conclusion: KineST方法实现了在稀疏输入下高效、准确地重建全身姿态，在AR/VR场景的人体运动追踪方面具有广阔应用前景。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [90] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出DenseBEV方法，将BEV特征单元直接用作anchor，实现多摄像头3D目标检测，提升效率和精度，并在nuScenes与Waymo数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前BEV Transformers多摄像头3D目标检测依赖随机queries作为anchor或用辅助网络检测结果替代，这些方法在效率、直观性与性能提升方面存在不足。作者希望用更自然、端到端的方法提升检测性能，尤其改善小目标检测。

Method: 提出将BEV特征单元直接用作anchor，并提出两阶段锚点生成方法。为应对大量queries导致attention扩展问题，使用基于BEV的NMS，仅对未抑制目标回传梯度，实现高效训练。结合直接来自BEV编码器的特征，天然集成时间信息，并用混合时序建模方式融合先验检测结果进一步增强性能。

Result: 在nuScenes数据集上，DenseBEV取得了NDS和mAP的一致且显著提升，尤其在BEV网格稀疏、锚点较少时更显著。对小目标（如行人检测）提升尤为明显，在nuScenes行人mAP提升3.8%，Waymo数据集LET-mAP提升8%。在Waymo Open数据集上，DenseBEV得到60.7%的LET-mAP，超过此前最佳5.4%。

Conclusion: DenseBEV通过端到端将BEV特征用作anchors，结合有效两阶段锚点生成和时序建模，显著提升了多摄像头3D检测的效果，尤其对小目标。该方法具有良好的扩展性和高效率，对实际应用具有重要价值。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [91] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 该论文研究了YOLOv8不同变体在车牌识别和字符识别任务中的表现，并提出新方案提升了在多环境下的实时准确性及部署效率。


<details>
  <summary>Details</summary>
Motivation: 随着城市交通智能化，对高效、准确的车牌检测与识别需求不断增长。虽然以往方法有所进展，但在实际多样化场景下的实时、高精度识别仍具挑战。作者旨在提升智能交通系统中车牌与字符识别的效果。

Method: 作者基于YOLOv8的Nano和Small两个变体，分别在车牌识别和字符识别上进行训练和实验，利用两个不同数据集验证。创新性地提出了基于x轴位置的字符排序方法，并构建了以YOLOv8 Nano和YOLOv8 Small为核心的优化识别流程。

Result: YOLOv8 Nano在车牌识别上取得0.964精度和0.918的mAP50，YOLOv8 Small在字符识别上取得0.92精度和0.91的mAP50。基于x轴的新字符排序方法表现良好。整体流程在保证高精度的同时具备较高的计算效率。

Conclusion: 提出的优化流程不仅提升了识别准确率，还具有良好的计算效率，易于实际部署，为未来智能交通系统及边缘设备车牌识别奠定了坚实基础。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [92] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 本文提出一个紧凑型的图像到文本架构，只需单张胸片即可自动生成胸部X光报告中的“发现（Findings）”部分，无需大规模多模态数据和复杂的训练资源。该方法在MIMIC-CXR数据集上取得了远超当前主流系统的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的自动放射报告生成系统需要大量多模态训练数据、临床元数据和多视角成像，造成高昂资源消耗，使其难以普及。为降低成本和复杂性，解决“资源门槛高、应用受限”的问题，本文探索了能广泛适用的轻量化方法。

Method: 采用冻结的DINOv3 Vision Transformer作为图像编码器，结合GPT-2文本解码器，并通过分层高斯平滑集成肺部和心脏分割掩码，实现层级解剖注意力，无需增加可训练参数，使模型重点关注临床相关区域。

Result: 在MIMIC-CXR数据集上，五大关键病变的CheXpert Macro-F1从0.083提升到0.238（提升168%），Micro-F1提高146%。针对14项观察指标整体F1提升86%，报告结构一致性（RadGraph F1）提升9.7%。

Conclusion: 仅依靠单张胸片和结构紧凑的模型，通过解码器层面引入解剖引导，能有效提升报告空间定位与结构一致性。这为资源有限环境中的自动放射报告生成提供了更可行的解决方案，源码已公开。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [93] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 现有文本到图像（T2I）模型评测基准GenEval因未能随模型进步及时更新，出现了严重的基准偏移问题。为此，作者提出了更难、覆盖更广的新基准GenEval 2，并配套更贴合人类评判的Soft-TIFA评测方法，强调评估基准需持续审查和优化。


<details>
  <summary>Details</summary>
Motivation: 自动化评价T2I模型时，评测基准需要兼顾模型能力与评测公正性。现有流行基准GenEval未能随着T2I模型能力提升而更新，导致其评判结果与人类偏离严重，影响公正、权威的模型比较。因此，研究者希望提出新的评测基准，桥接模型进步带来的评测误差。

Method: 通过对现有GenEval基准的深入分析以及人类大规模评测对比，作者识别了基准漂移现象。随后，作者设计了覆盖基本视觉概念且具更高组合复杂度的新基准GenEval 2，以及结合不同视觉原语判断结果的新评测方法Soft-TIFA，并评估了其对比之前基准的人类一致性。

Result: 实验证明，GenEval 2比旧基准显著更具挑战性，对现有T2I模型的区分度更高；Soft-TIFA方法在人类一致性上优于现有整体性评测方法（如VQAScore）。此外，实验证明，旧版GenEval基准对新模型的评测误差高达17.7%。

Conclusion: GenEval 2与Soft-TIFA为T2I模型自动化评测提供了更精准且难度更高的新基准，但作者指出任何静态基准最终都难免漂移，因此需持续审查和迭代以保障评测公平性。这对于所有自动化模型评测领域都具有重要参考借鉴意义。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [94] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 该论文提出了RePlan框架，能够在复杂指令和场景下进行精确的多区域图像编辑，并在新基准IV-Edit上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑模型在面对复杂指令和混乱场景（IV-Complexity）时表现不佳，尤其在需要多步推理和对场景多个区域进行精细化操作时，难以保证准确性和编辑效果。

Method: 提出了Plan-then-Execute框架RePlan，分为两步：一、用视觉-语言规划器对编辑指令逐步推理并落实到具体图像区域（region-aligned）；二、利用扩散编辑器，通过无训练的attention-region注入机制对多个区域并行进行编辑，不需反复嵌入。规划部分还用GRPO强化学习，仅靠1K条纯指令显著提升推理和格式表现。

Result: 提出了新的IV-Edit基准数据集，专注于细粒度和知识密集型编辑。在各种高IV-Complex背景下，RePlan模型在区域精度和整体效果上均大幅超越了基于更大数据集训练的主流方法。

Conclusion: RePlan显著增强了复杂指令下的多区域图像编辑能力，是应对指令和视觉复杂性的有效方法，并推动了该领域的发展。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [95] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出了Pixel Seal，一个在图像和视频中实现不可见水印的新方法，显著优于现有技术，同时兼顾了鲁棒性和不可见性。


<details>
  <summary>Details</summary>
Motivation: 现有不可见水印方法在实现真正的不可见性和鲁棒性间难以平衡，而且在高分辨率图像/视频场景下问题更加突出；具体包括：依赖代理感知损失（如MSE、LPIPS）导致水印可见、优化目标冲突造成训练不稳定、高分辨率时鲁棒性和不可见性降低。

Method: 1）提出仅用对抗式训练方法，去掉像素级不可见性损失；2）设计三阶段训练流程，将鲁棒性目标与不可见性目标解耦提升收敛性；3）针对高分辨率，采用基于JND的衰减和训练时推理模拟方法，消除上采样伪影。

Result: 在不同图像类型和多种变换下，Pixel Seal的鲁棒性和不可见性均优于现有方法；并且能够通过时序水印池化高效拓展到视频应用。

Conclusion: Pixel Seal为现实世界高分辨率图像和视频的可靠溯源提供了一种实用、可扩展的水印解决方案，明显提升了不可见性与鲁棒性的平衡。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [96] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ReMeDI-SAM3的新方法，有效提升了内镜手术视频中手术器械分割的精度，特别是在遮挡和快速运动等复杂场景下，比以往的方法提升显著。


<details>
  <summary>Details</summary>
Motivation: 手术器械在内镜视频中的精确分割对手术辅助系统至关重要，但遮挡、运动、反光和长时间再入等问题使分割任务非常有挑战性。目前的SAM3框架尽管强大，但在手术场景下容易因无差别记忆更新、固定记忆容量、遮挡后身份恢复能力弱等问题导致性能受限。

Method: 提出了一种无需训练的增强版SAM3（ReMeDI-SAM3），主要包含三大创新：(1) 引入相关性感知的记忆过滤和专用遮挡记忆单元用于保存遮挡前帧；(2) 采用分段插值方法扩展实际记忆容量；(3) 设计基于特征的再识别模块和时间投票机制以保证遮挡后的身份区分。

Result: 在EndoVis17和EndoVis18数据集上，在零样本设定下对基线SAM3有7%和16%左右的mcIoU提升，甚至优于以往需要训练的先进方法。

Conclusion: ReMeDI-SAM3通过引入相关性感知记忆、容量增强和身份恢复机制，显著提升了内镜手术场景下的器械分割鲁棒性和准确性，尤其在复杂遮挡和运动情境下优势明显。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [97] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多材料物理参数估算方法，能够从视频中估计真实世界复杂物体（如花朵）的材料组成和物理参数，并通过实验验证了其准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多假设物体为单一材料或结构简单，难以处理现实中由多种材料和复杂形状构成的物体，因此需要新方法来解决更真实情境下的多材料物理参数估算问题。

Method: 作者提出了Multi-material Physical Gaussians（M-PhyGs）模型，从自然环境下的视频入手，通过联合分割和参数恢复，估算物体各部分的材料组成及连续介质力学参数。该方法引入级联的3D/2D损失函数和时间批处理技术，以更高效准确地实现多材料估算。同时，制作并公开了人与花朵互动的视频数据集Phlowers，为方法评测提供平台。

Result: 在Phlowers数据集上的实验结果表明，M-PhyGs及其各组成模块能够高效且准确地估算具有复杂多材料组成自然物体（如花朵）的物理参数。

Conclusion: 该方法突破了以往对物体材料单一和结构简单的假设，能够适应更复杂的自然情景，多材料物理参数估算任务得到了有效解决，对现实世界中物体属性的分析更具实用意义。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [98] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了LinkedOut，一种基于视频大语言模型(VLLM)的新型视频推荐方法，能够直接从原始帧中提取具备世界知识的视频表示，实现高效且可解释的多视频推荐，无需手动标签，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLLM在视频理解和问答等任务上表现出色，但实际视频推荐应用面临多视频输入、低延迟、高效率等瓶颈，现有方法无法充分利用VLLM的视觉细节和世界知识，因此需要创新的视频表征方式。

Method: 提出LinkedOut，直接由VLLM从视频帧中抽取具有世界知识的语义token，通过可提示查询和多模态辅助，利用跨层知识融合MoE机制，自动选择特征抽象层级，实现个性化、可解释、低延迟的视频推荐。

Result: LinkedOut能够直接在原始视频帧（无需人工标签）上运行，在多个公开推荐基准数据集上取得了最优结果。解释性实验和消融分析验证了方法中层多样性和分层融合的有效性。

Conclusion: LinkedOut为利用VLLM实现高效、多视频、细粒度和具世界知识先验的视频推荐提供了实用路径，超越传统模型，并为相关下游视觉任务提供了更具解释性和性能的新方案。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [99] [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/abs/2512.16893)
*Kaiwen Jiang,Xueting Li,Seonwook Park,Ravi Ramamoorthi,Shalini De Mello,Koki Nagano*

Main category: cs.CV

TL;DR: 本论文提出了一种结合2D扩散模型和3D快速结构的高效人脸动画方法，实现了高质量、三维一致性且可实时生成的人脸动画。


<details>
  <summary>Details</summary>
Motivation: 现有2D肖像动画方法虽然提升了画质，但牺牲了3D一致性与速度，难以用于数字孪生、远程展示等实际场景；而现有基于显式3D表示（如神经辐射场、Gaussian splatting）的方法虽然保证了3D一致性和推理速度，但动画细节表现不足。作者希望结合两者优点，解决3D一致性、速度与表现力难以兼得的问题。

Method: 本方法采用“知识蒸馏”思想，从2D扩散模型中蒸馏动画能力到一个前馈编码器，使其能将单张真实环境图片即时转化为3D一致、速度快且高细节的动画表现。模型结构上，动画表达与三维结构解耦，并通过轻量级的局部融合策略（替代以往多层注意力的全局融合），更加高效地结合3D结构和动画信息，无需依赖预定义参数模型。

Result: 方法可以实现107.31 FPS的推理速度，动画质量与当前最强方法相当，且比其他只能从速度或质量中取舍的方案表现更优。

Conclusion: 本方法有效融合2D扩散模型的动画细节与3D方法的速度及一致性，推动了三维肖像动画的实际应用落地，具有很好的实时性和高动画表现力。

Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

</details>


### [100] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait是一种能加速并保障人物肖像一致性的长动画生成新方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的长肖像动画加速方法在保证人物身份一致性时存在困难，导致生成视频中的人物形象易变。

Method: FlashPortrait提出一种端到端的视频扩散变换器。首先用预训练特征提取器提取与身份无关的面部表情特征，然后通过归一化面部表达模块将特征按均值方差规整，增加身份稳定性。在推理时，FlashPortrait使用动态滑动窗口并对重叠区加权融合以保证平滑过渡和ID一致。还将当前时刻的高阶隐变量导数用于直接预测未来时刻的隐变量，跳过部分去噪步骤，从而实现6倍加速。

Result: 在多项基准测试上，FlashPortrait在保证人物身份一致性的同时大幅提升了推理速度，实现了6倍加速，且生成的动画在定性和定量评测中表现优越。

Conclusion: FlashPortrait有效实现了高保真、身份一致、任意时长的肖像动画生成，同时显著提升了生成速度，为人物视频生成领域提供了新方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [101] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出一种名为Alchemist的元梯度驱动方法，自动从大规模文本-图像配对数据中选择高质量样本，提高文生图模型的视觉质量与下游任务表现，且用一半数据超越全量数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型（如Stable Diffusion）性能受限于训练数据质量，而现有采样与筛选方法效率低或效果有限。提升数据筛选的自动化、有效性成为关键。

Method: 提出Alchemist框架，包括数据评分与数据剪枝两阶段。首先训练轻量级评分器，利用多粒度梯度信息评估每个样本对模型训练的贡献，随后采用Shift-Gsampling策略挑选最具信息量的数据子集。

Result: 在合成及网页爬取的数据上，Alchemist显著提升训练模型的视觉质量与下游性能。只用50%的筛选数据，其训练结果优于全量数据训练。

Conclusion: Alchemist作为首个可扩展的元梯度驱动自动数据选择框架，有效提高文生图模型的数据利用效率与表现，验证了数据质量选优的重要性及方法有效性。

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [102] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频编辑框架VIVA，通过引入视觉语言模型（VLM）和奖励优化机制，实现了更强的自然语言指令视频编辑能力，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型视频编辑方法主要训练于简单编辑配对数据，导致对复杂和实际指令泛化能力有限。因此，亟需一种能够理解更复杂指令且具有更强泛化能力的编辑框架。

Method: 作者提出VIVA框架，核心方法包括：(1)利用VLM编码指令、源视频首帧及参考图片，获得细致的视觉语义指令描述，增强扩散Transformer骨干的信息感知能力；(2)引入Edit-GRPO，通过相对奖励优化策略对生成结果在多维度上进行直接优化；(3)提出合成多样高保真指令-视频配对数据集的生成流程以增强调优。

Result: 在多项实验中，VIVA在指令遵循性、泛化能力和编辑质量等方面显著优于当前先进方法。

Conclusion: VIVA有效提升了基于自然语言指令的视频编辑能力，并在编辑效果和泛化性方面树立了新基准。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [103] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本论文提出了一个多视角变更检测新基准（SceneDiff Benchmark）和一种无需训练的新方法（SceneDiff），能有效识别场景中对象的增删移动变化，并在多个数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人整理、施工进度监测等应用中，准确检测场景中对象的变化（如增加、移除、移动）至关重要。但由于不同视角采集的图像存在巨大差异，易导致误检，因此亟需鲁棒的多视角对象变化检测方法和数据集。

Method: 作者构建了SceneDiff Benchmark，这是第一个具有对象实例标注的多视角变更检测基准数据集，包含350对视频和数千个变更对象。提出了SceneDiff方法，利用预训练的3D建模、分割和编码模型，将图像在3D空间校准，提取对象区域后，通过空间与语义特征进行比较检测对象变化。此外，该方法无需额外训练，泛化能力强。

Result: 在多视角和双视角变更检测任务上，SceneDiff方法分别实现了94%和37.4%的AP提升，显著优于现有方法。

Conclusion: SceneDiff基准和方法为多视角对象变更检测提供了新工具，数据集和代码将公开，有望推动相关研究和现实应用。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [104] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的离散图像分词器SFTok，通过多步迭代机制和自引导重建方法，在高压缩比下实现了高质量图像重建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离散分词器虽然更适合自回归生成范式，但在图像重建质量上落后于连续分词器，限制了其在多模态任务中的应用。为提升离散分词器表现，迫切需要解决高压缩下的重建质量问题。

Method: 本文提出SFTok分词器，采用多步迭代重建机制，设计了自引导视觉重建（self-forcing guided visual reconstruction）和去偏拟合训练策略（debias-and-fitting training strategy），同步优化训练与推断的一致性，提升分词器训练效果。

Result: 在ImageNet上，SFTok在每张图片仅用64个token的高压缩率下，实现了领先的重建质量（rFID=1.21），在类别-图像生成任务上表现同样优异（gFID=2.29），优于已有方法。

Conclusion: SFTok有效提升了离散分词器在高压缩比下的图像重建质量和生成性能，推动了其在高效多模态生成系统中的应用前景。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [105] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: 本文提出了一种能够泛化到不同场景距离的全景度量深度基础模型，结合多源数据和创新优化方法，在多个基准上实现了零样本泛化和鲁棒预测。


<details>
  <summary>Details</summary>
Motivation: 现有度量深度估计模型在不同场景距离（如室内外、真实与合成数据间）泛化能力有限，且缺乏大规模高质量全景数据支持。作者希望解决模型泛化性弱和数据域差异大的问题。

Method: 1）通过整合公开数据集、高质量合成数据（来自UE5模拟器与文本到图像生成模型）、网络爬取的真实全景图像，构建大规模多样性数据集；2）提出三阶段伪标签生成流程，优化无标签图像的伪真值，缓解不同域间的差异；3）模型采用DINOv3-Large主干，并新增可插拔距离掩码头，采用以清晰度为核心的优化和以几何为核心的优化策略，提升不同距离下的鲁棒性及视角几何一致性。

Result: 在Stanford2D3D、Matterport3D、Deep360等多个基准上，提出的方法都获得了优异表现，并实现了零样本泛化，在复杂且多样的真实世界场景中预测稳定可靠。

Conclusion: 该工作展示了通过数据和模型创新，实现了极具泛化能力和鲁棒性的全景度量深度模型，对实际应用具备重要意义。

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [106] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本论文提出了用于单目转立体视频的新方法StereoPilot，并构建了第一个统一的大规模立体视频数据集UniStereo，显著提升了视觉质量与计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前3D视频内容需求激增，但高质量立体视频制作成本高、流程复杂，且现有自动转换方法（如深度-变形-修补DWI流程）存在误差传播、深度模糊、格式不兼容等难题。

Method: 作者首先构建了UniStereo数据集，涵盖了并行及会聚两种主流立体视频格式，方便公平评测及模型训练。随后提出StereoPilot模型，通过高效前馈网络，采用可学习的域切换器和循环一致性损失，无需显式深度图和迭代采样，直接合成目标视角，并可自适应不同立体格式。

Result: StereoPilot在大量立体视频转换实验中，不论是在视觉保真度还是推理速度上均显著超越了现有先进方法。

Conclusion: 本研究为自动立体视频生成提供了高效且更鲁棒的技术路线，UniStereo数据集也为后续研究提供了标准基线，推动了3D视频转换领域的发展。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [107] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出AdaTooler-V，使多模态大模型在视觉推理任务中能根据实际需求自适应地决定是否调用视觉工具，从而提升效率和性能，并在多个基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大模型在视觉推理时往往“盲目”调用视觉工具，导致推理效率低和性能下降，因此需要一种方法，使模型仅在必要时才使用工具。

Method: 提出AdaTooler-V模型，通过自适应决策是否使用视觉工具。核心算法为AT-GRPO，一种基于每个样本Tool Benefit Score自适应调整奖励尺度的强化学习方法，鼓励模型仅在工具确实带来提升时调用。模型训练包括两个数据集：用于微调的AdaTooler-V-CoT-100k和用于强化学习的AdaTooler-V-300k，覆盖单图、多图、视频等多种任务。

Result: AdaTooler-V在12项视觉推理基准测试中表现优异，其中AdaTooler-V-7B在高分辨率V*基准测试中获得89.8%的准确率，超过了GPT-4o与Gemini 1.5 Pro等商业闭源模型。

Conclusion: AdaTooler-V能够有效提升多模态大模型视觉推理的效率和性能，其自适应工具调用机制显著减少了不必要的推理开销。所有代码、模型和数据均已开源，有助于多模态推理领域的发展。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [108] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: 该论文提出了EasyV2V，一种简单高效的指令式视频编辑框架，在视频编辑领域取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 虽然图像编辑技术发展迅速，但视频编辑在一致性、控制性和泛化方面仍面临挑战，相关研究相对较少。因此，作者希望设计一种方法以提升视频编辑的泛化性与可控性。

Method: 作者在数据、模型结构和控制机制三个维度进行了系统设计：1）数据方面，通过组合已有专家模型及其逆过程生成多样化视频对，利用单帧监督将图像编辑迁移至视频，并利用密集标注片段和过渡监督增强编辑能力；2）模型方面，基于预训练文本生成视频模型，采用序列级联和轻量级LoRA微调，保证编辑性能的同时简化了训练流程；3）控制方面，统一使用掩码机制实现时空控制，并支持可选参考图像。

Result: EasyV2V模型可以灵活接收多种输入类型（如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本），并在多个公开与商用系统中实现了领先的视频编辑性能。

Conclusion: EasyV2V不仅实现了灵活的输入与强大的编辑能力，还在一致性与泛化性方面大幅超越现有方法，为视频编辑提供了简单实用的新范式。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [109] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 本文提出了一个名为AuditDM的自动化审计框架，通过主动发现和修正多模态大模型（MLLM）中的失败模式，提高模型诊断和改进的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的评估方法缺乏可解释性，且难以充分揭示不同模型间的“短板”，为了更有效地识别和修正模型缺陷，作者提出了新的自动化框架。

Method: AuditDM 通过强化学习微调LLM，让其作为审计者，主动生成能使不同目标模型产生分歧的问题与反事实图像，从而发现模型薄弱点。这些示例无需人工标注便可用于模型纠正。

Result: 在Gemma-3、PaliGemma-2等主流模型上，AuditDM发现了20多种独特的失败类型。基于这些失败类型微调后，16项基准测试上全部模型都有提升，甚至一个3B参数的模型超过了28B参数的对应物。

Conclusion: 随着数据量扩充带来的效益递减，有针对性的模型审计成为诊断和优化多模态大模型的有效途径。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [110] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出了一种视觉自监督学习新方法NEPA，通过自回归地预测图像patch的embedding，而不是传统的像素重建或对比损失，取得了较好的效果。


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成式预训练的启发，作者探索是否可以用类似原则提升视觉自监督学习效果，减少架构复杂性并提高可扩展性。

Method: 提出Next-Embedding Predictive Autoregression (NEPA)方法，让模型在预训练时通过Transformer架构预测下一个patch的embedding，采用因果遮蔽与stop-gradient策略。该方法无需像素级重建、离散tokens、对比损失或定制任务头。

Result: NEPA方法仅用ImageNet-1k数据进行embedding预测预训练，在ViT-B和ViT-L主干网络下分别达到ImageNet top-1精度83.8%和85.3%。在ADE20K上做语义分割迁移也表现优秀。

Conclusion: 生成式embedding预测为视觉自监督学习带来了简洁、可扩展且跨模态的潜力，提供了一种有前景的替代路线。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [111] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种无需高端设备即可单张图片重聚焦的新方法，显著提升了去模糊与虚化效果，并允许更灵活的景深和光圈控制。


<details>
  <summary>Details</summary>
Motivation: 当前的单图像重聚焦方法存在诸多限制，如必须全焦清晰输入、依赖合成数据、光圈调节受限，不能真实反映相机的光学特性。作者希望解决单图像重聚焦中的去模糊和真实虚化难题，提升操作灵活性和效果逼真度。

Method: 提出Generative Refocusing方法，包括两个步骤：（1）利用DeblurNet从各种输入恢复全焦清晰图像；（2）通过BokehNet生成可控光圈效果的虚化效果。创新点在于引入半监督训练，混合合成配对数据与真实未配对虚化照片，并用EXIF元数据捕捉真实相机光学特性。还支持文本引导的调整与自定义光圈形状。

Result: 实验结果表明，该方法在去模糊、虚化合成及重聚焦等基准测试上取得了最佳性能。不仅提高了输出质量，还提升了应用的灵活性与逼真度。

Conclusion: Generative Refocusing方法克服了以往方法的局限，既能恢复高质量清晰图像，又能真实模拟不同光圈下的虚化效果，为单图像重聚焦任务提供了更实用的解决方案。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [112] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个结合文本、运动轨迹和参考图片的多模态世界事件生成框架，实现用户可控、高一致性的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的视频生成方法或仅用轨迹控制的图像生成视频方法存在表现力和可控性有限、语义和过程难以结合等问题，难以生成复杂、多主体互动事件。

Method: WorldCanvas提出结合三模态：用运动轨迹编码动作、时序与可见性，文本表达语义意图，参考图片实现外观锚定，在多模态条件下生成世界事件。

Result: 该方法能生成多主体互动、目标进出与参考引导外观等复杂事件视频，在时序连贯、身份和场景一致性方面表现出色，即使对象暂时消失也能保持一致性。

Conclusion: WorldCanvas推动世界模型从被动预测迈向交互式、用户主导的模拟器，提升了世界事件生成的表现力和可控性。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [113] [TabReX : Tabular Referenceless eXplainable Evaluation](https://arxiv.org/abs/2512.15907)
*Tejas Anvekar,Juhna Park,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种无参考、基于属性的方法TabReX，用于评估大语言模型生成表格的质量。该方法将文本和表格转为知识图谱，通过LLM引导比对，计算可解释的评分。实验表明TabReX与专家评价高度相关，稳定且易于追踪误差。


<details>
  <summary>Details</summary>
Motivation: 现有的表格质量评估方法无法充分考虑表格结构或泛化性弱，难以对复杂生成任务做出可靠评价，需要新的高信度、可解释方法。

Method: 提出TabReX：先将源文本和生成表格转换为标准化知识图谱；再利用LLM辅助完成节点/属性对齐；最后基于比对结果给出结构与事实准确度评分。此外，建立了TabReX-Bench基准库，设计多种干扰测试其鲁棒性。

Result: TabReX在6个领域、12种干扰及3个难度下的评测中，与专家排名的一致性最高，对复杂情况仍表现稳定，支持细粒度分析，并能提供单元级误差溯源。

Conclusion: TabReX为表格生成评价提供了一种无参考、高解释性、泛化良好的新范式，为可信、透明的结构化生成系统评价奠定基础。

Abstract: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.

</details>


### [114] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出了一个新的形式化框架SocialStoryFrames以及配套模型，用来分析读者对故事的复杂反应，并通过社交媒体大规模数据集展示其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前关于读者阅读故事时的复杂、细致反应（如推断作者意图、情感判断等）的计算模型能力有限，影响了对读者反应的深入分析和理解。为弥补这一研究空白，作者希望建立可以系统且细致刻画读者反应的新方法和工具。

Method: 作者开发了SocialStoryFrames这个形式化框架，并基于它推出了两个模型SSF-Generator和SSF-Classifier，能够利用对话语境和以叙事理论、语言语用学、心理学为基础的分类体系来识别和归纳读者的推理、情感及价值判断。模型通过382名参与者的人类调研和专家标注验证；他们还打造了SSF-Corpus（含6140个社交媒体真实故事），对框架的实际应用效果进行了展示和初步分析。

Result: 实验表明，该框架及模型能够有效提取和分析读者对故事的多维度反应，并揭示了不同社区和语境下叙事实践的多样性和共性。作者系统性地描绘了社交媒体上故事叙述意图的频度及其相互依赖关系。

Conclusion: SocialStoryFrames与配套模型能够实现细粒度、情境敏感化的读者反应建模，为规模化探究社群故事讲述方式和反应提供新工具，对网络社区叙事研究具有开创性意义。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [115] [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)
*Armağan Amcalar,Eyup Cinar*

Main category: cs.CL

TL;DR: 本文提出了一种结构化提示方法BRAID，在多个GPT模型上进行评估，实现了提升推理准确率并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在性能、成本和Token使用上的关系表现为非线性，提升推理能力时容易带来高成本。作者希望通过结构化提示提升推理效率，降低实际应用中的计算开销。

Method: 提出BRAID（Bounded Reasoning for Autonomous Inference and Decisions）框架，用基于Mermaid的指令图对模型进行结构化提示，引导模型以结构化而非自然语言全面展开方式推理。多组GPT模型在AdvancedIF、GSM-Hard以及SCALE MultiChallenge数据集上进行了定量评估。

Result: 实验结果显示，使用结构化的机器可读提示，可以显著提升自动化代理推理的准确率，同时节省成本。

Conclusion: BRAID是一种高效可扩展的结构化推理提示方案，能有效优化自主代理系统的推理效率。

Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.

</details>


### [116] [Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms](https://arxiv.org/abs/2512.16034)
*Kieran Henderson,Kian Omoomi,Vasudha Varadarajan,Allison Lahnala,Charles Welch*

Main category: cs.CL

TL;DR: 本文研究自我揭露类句子在预测主观任务中标注者标签的作用，发现人口统计信息最为关键。


<details>
  <summary>Details</summary>
Motivation: 尽管以往研究表明自我揭露信息能帮助改善主观任务标签的预测，但很少关注哪些类型的信息最有价值，尤其是在个人信息充足的情况下。因此，本文旨在深入分析不同类型的自我揭露信息对预测社会规范判断的影响。

Method: 作者将自我揭露句子进行细致分类（如人口统计、态度、关系、经历），并据此构建标注者预测模型。采用消融实验和多种分析手段，比较不同信息类型对模型预测效果的影响，并探究理论驱动方法与自动聚类方法的差异。

Result: 实验表明，人口统计信息对预测标注者标签的提升作用最大，优于态度、关系或经历信息。理论驱动的方法效果好于自动聚类。与以往研究不同，仅需极少量的关联评论即可获得良好预测表现。标注者自我揭露信息多样性越高，预测性能最佳。

Conclusion: 为了更好预测主观性任务中标注者标签，应优先收集多样且包含人口统计信息的自我揭露内容，少量高相关内容即可获得优秀表现。

Abstract: Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.

</details>


### [117] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: 本文提出了Sage，一个无需人工标注的新型评测套件，用于更客观地评价大语言模型（LLM）作为判断者时的可靠性与一致性。实验显示，当前主流LLM作为评判者在复杂任务下仍存在显著不一致，且人类标注本身也不总是可靠。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-as-a-Judge评测方法依赖人工标注的标准答案，存在主观偏见和不可扩展的问题，难以准确、公正地衡量模型表现。亟需无需人工干预、能自动评估LLM裁判能力的新方法。

Method: 作者借鉴理性选择理论，设计了本地自洽性（局部对比稳定性）和全局逻辑一致性（跨偏好传递性）两个指标，构建Sage评测框架，并制备了包含650个问题的数据集，既涵盖标准测试题，又囊括真实用户问题。通过Sage，无需人工标注即可自动评判LLM裁判的一致性和鲁棒性。

Result: 实验表明，Sage的评测指标稳定且与现有标注基准（如LLMBar、RewardBench2）高度相关。通过Sage发现，即使是最先进的LLM，如Gemini-2.5-Pro和GPT-5，在难题上仍有近四分之一判定不一致，表现出“情境偏好”现象；人工标注的一致性也存在较大问题。

Conclusion: Sage为无偏、可扩展地评估LLM裁判能力提供了新思路。当前LLM裁判在复杂判别中仍不够可靠；微调模型、基于小组和深度推理的判官系统能提升一致性。人工标注不是绝对的金标准，后续评测方法应减少对其依赖。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [118] [Convolutional Lie Operator for Sentence Classification](https://arxiv.org/abs/2512.16125)
*Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TL;DR: 本论文提出将李群卷积（Lie Convolutions）用于基于卷积的句子分类器，并显示新方法在准确率上优于传统卷积模型。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络（CNN）能够捕捉文本中的局部特征，但对语言内部复杂变换的建模能力有限。作者希望通过探索更能表达非欧几里得对称性的结构，提升语言建模的表现。

Method: 将李群运算引入卷积网络，提出SCLie和DPCLie两种新模型，利用李群卷积捕捉更复杂的语言变换特征，用于句子分类任务。

Result: SCLie和DPCLie模型在实证测试中准确率优于传统卷积句子分类器，表明李群卷积能够捕捉传统方法难以建模的语言变换。

Conclusion: 李群卷积为句子分类任务带来了性能提升，激励了对新型语言建模范式的进一步探索。

Abstract: Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.

</details>


### [119] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义驱动强化学习（SRL）的医学报告生成方法，显著提升了报告的临床正确性，并在IU X-Ray和MIMIC-CXR数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统医学报告生成方法侧重模仿放射科医生的文本风格，但往往忽视了医学内容的准确性，导致生成报告缺乏临床正确性。这是由于现有方法多用词级（token-level）训练目标，优化的是语句结构和词语选择，而不是医学内容本身。

Method: 作者提出一种基于语义强化学习的新方法，将其应用于大型视觉-语言模型（LVLM）。具体做法是通过组相对策略优化（GRPO），最大化约束在生成报告与参考报告中的关键放射学发现的语义相似性分数（MCCS），直接对报告级别的临床标签一致性进行优化。同时，添加结构化的推理格式约束引导模型产生更具逻辑性的“思考报告”。

Result: 该方法（MRG-R1）在IU X-Ray与MIMIC-CXR数据集上，用临床效能指标（CE-F1）验证，分别取得51.88和40.39的最新最优表现。实验还发现，语义标签强化比传统的token-level监督更优。

Conclusion: 直接优化与医学标签一致性的报告级奖励，而非词级重叠度，可以显著提升生成报告的临床正确性。这为通过语义强化辅助医学视觉-语言大模型训练，提高医学生成内容的正确性提供了新思路。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [120] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文针对社交媒体中由虚假叙事驱动的仇恨言论（Faux-Hate）进行检测，提出了结合自然语言处理与领域预训练的多任务学习系统，获得了有竞争力的效果。


<details>
  <summary>Details</summary>
Motivation: 社交平台已成为仇恨言论与虚假叙事快速传播的温床，检测由虚假叙述驱动的仇恨言论对遏制网络有害信息传播具有重要意义。该研究旨在应对Faux-Hate现象，提升相关文本的识别能力。

Method: 作者提出的系统结合了先进的自然语言处理技术和领域特定的预训练方法，通过多任务学习，分别进行Faux-Hate的二分类检测（真假仇恨言论区分）以及仇恨内容的目标与严重性预测。主要任务针对印地语-英语夹杂的社交媒体文本。

Result: 系统在Faux-Hate识别相关任务中获得了有竞争力的表现，验证了多任务学习和领域预训练的有效性。

Conclusion: 采用多任务学习和领域自适应预训练，可在复杂的仇恨与虚假信息识别问题中带来性能提升，为检测多语言、混合编码社交媒体内容中的有害信息提供了新思路。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [121] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 本文提出了一种针对警情通报公告结构化信息抽取的解决方案，通过模型微调和Prompt工程实现对社交媒体噪声文本的高效信息提取，精度达到业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 警情通报信息的及时结构化对于公共安全和社会科学研究至关重要，但社交媒体中的文本高度非结构化、来源繁杂，传统抽取方法难以应对。

Method: 作者提出一种基于Qwen2.5-7B模型的低秩适配（LoRA）高效微调方法，结合域相关的Prompt工程，用人工标注的高质量数据集（4,933条，来自微博）对模型进行定向优化，能够抽取15类核心信息。

Result: LoRA微调后模型在死亡人数检测上准确率超98.36%，死亡人数抽取及省级地点抽取实现95%以上的精准匹配率，显著优于基本模型和其他指令调优模型。

Conclusion: 所提出的方法为警情文本领域内多任务结构化信息抽取提供了经验证且高效的技术路径，有助于社科领域将非结构化文本转化为可靠数据。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


### [122] [Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation](https://arxiv.org/abs/2512.16189)
*Musarrat Zeba,Abdullah Al Mamun,Kishoar Jahan Tithee,Debopom Sutradhar,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Reem E. Mohamed,Md Rafiqul Islam,Yakub Sebastian,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 该论文提出了一种在医疗领域中减少LLM幻觉输出的方法，包括独立的事实校验模块和领域特定的摘要模型，以提升医疗文本生成的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在医疗决策和患者安全相关任务中，虽然能生成文本，却常常产生不可靠甚至错误的信息（幻觉），存在严重隐患。需要有效机制确保LLM输出内容的真实性。

Method: 提出了一个独立于LLM的事实校验模块，结合基于MIMIC III数据集LoRA微调后的医疗领域专用摘要模型。事实校验模块采用数值测试和自然语言处理中的离散逻辑检验，将LLM生成的信息与电子健康记录（EHR）中的事实进行对比验证。

Result: 事实校验模块获得0.8904的精确率，0.8234的召回率和0.8556的F1分数，摘要模型的ROUGE-1分数为0.5797，BERTScore为0.9120，显示出系统在事实验证和高质量摘要上的有效性。

Conclusion: 该方法显著提升了医疗领域中LLM生成文本的可靠性，事实校验与专用摘要模型的结合有效减少了幻觉输出，有助于在实际医疗环境中部署更安全的自动化文本生成系统。

Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.

</details>


### [123] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息瓶颈理论的大语言模型（LLM）知识编辑新框架，通过更高效、广泛且可控的方式纠正模型中的错误或过时信息。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型中的错误和过时知识影响模型准确性和安全性，而目前的模型编辑方法在推广和避免副作用方面存在局限，需要一种更高效、可泛化且不会产生负面影响的编辑机制。

Method: 提出了基于信息瓶颈理论的信息瓶颈知识编辑器（IBKE），通过紧凑的潜在表示引导梯度更新，实现对知识精准、低干扰的编辑。该方法在多个LLM架构和标准任务上进行了验证。

Result: IBKE在多种架构和标准任务上表现出色，实现了当前最优的准确率，并提升了编辑的泛化性和针对性。

Conclusion: IBKE为开放域知识编辑建立了理论坚实且实用的范式，提升了大语言模型在实际应用中的可靠性和效用。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [124] [LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding](https://arxiv.org/abs/2512.16229)
*Chenkai Xu,Yijie Jin,Jiajun Li,Yi Tu,Guoping Long,Dandan Tu,Tianqi Hou,Junchi Yan,Zhijie Deng*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoPA的新型推理算法，通过优化Token填充顺序，大幅提升了扩散型大语言模型（dLLM）推理的并行解码速度，显著提高了tokens-per-forward(TPF)和整体推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的dLLM推理方法在高效率并行解码（即每次前向推理能输出的Token数量）上表现有限，仅能做到每次1～3个Token，成为模型应用和落地的瓶颈。作者注意到Token填充顺序对并行解码效率影响很大，迫切需要在无需额外训练下提升解码并行性。

Method: 作者提出了LoPA算法，可以在推理时并行探索多种不同Token填充顺序（TFO），选出最有利于后续并行的顺序。此外，配合开发了多设备多分支并行（Branch Parallelism, BP）系统，利用多GPU提升处理速度。

Result: 在将LoPA应用于最先进的D2F模型后，TPF指标显著提升——在GSM8K任务上从原有的低TPF提升到10.1，且保持了比基线更高的性能。多GPU系统下，单样本能达到1073.9 token每秒的推理速度。

Conclusion: LoPA方法无须额外训练、可直接集成使用，在提升dLLM推理并行度和效率上效果显著，有望加速diffusion大模型的实际部署和应用。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.

</details>


### [125] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 本文提出了Sigma-MoE-Tiny，一种基于Mixture-of-Experts（MoE）的高稀疏性（sparse）语言模型，在同类开源模型中稀疏性最高，仅激活极少参数却能保持高性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型因可高效扩展而受到关注，但极高稀疏性带来专家负载均衡难题，尤其在底层结构中，主流的均衡损失函数效果不佳，亟需解决。

Method: 提出精细的专家划分（单层多达96个专家，每个token只用1个），配合进阶稀疏化调度方法，保障专家负载均衡与训练稳定性，并采用高质量、多样化数据进行预训练和后训练。

Result: 尽管只有0.5B激活参数（总参数20B），该模型在同等或更大规模模型中表现优异；训练过程稳定无严重损失波动。

Conclusion: Sigma-MoE-Tiny在极高稀疏性下实现高效率和强性能，并为未来高稀疏MoE架构的负载均衡与设计提供了有价值的见解。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [126] [Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures](https://arxiv.org/abs/2512.16287)
*Yehor Tereshchenko,Mika Hämäläinen,Svitlana Myroniuk*

Main category: cs.CL

TL;DR: 本文评估了OpenAI GPT模型在高加索低资源语言翻译中的表现，比较了带推理能力与无推理能力模型在翻译芬兰语与4种乌拉尔语系低资源语言时的差异。带推理能力模型拒绝翻译的比例显著较低。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的翻译评测主要集中在高资源语言，缺乏对低资源及濒危语言的评估，尤其在乌拉尔语系语言上存在研究空白。作者希望了解不同GPT模型在这些语言翻译任务中的表现，为濒危语言保护提供参考。

Method: 采用包含文学文本的平行语料库，对芬兰语与Komi-Zyrian、Moksha、Erzya、Udmurt四种低资源乌拉尔语进行相互翻译。比较OpenAI GPT系列推理模型与非推理模型的翻译表现，特别分析其在遇到翻译请求时的拒绝率。

Result: 带推理能力的GPT模型在低资源乌拉尔语的翻译任务中，平均拒绝翻译的概率比无推理模型低16个百分点，表现差异显著。

Conclusion: 推理型大语言模型在低资源语言翻译中展现出更高的积极性与能力，有助于这些濒危语言的数字化与保护。研究为相关领域工作者提供了重要参考，也扩展了对推理型语言模型在非主流语言应用能力的认知。

Abstract: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.

</details>


### [127] [Hacking Neural Evaluation Metrics with Single Hub Text](https://arxiv.org/abs/2512.16323)
*Hiroyuki Deguchi,Katsuki Chousa,Yusuke Sakai*

Main category: cs.CL

TL;DR: 本文提出了一种方法，能够在离散文本空间中找到一个在各种测试情况下都被评价为高质量的对抗性文本，从而揭示当前主流神经网络文本评价指标（如COMET）的漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前用于生成模型评价的神经网络型指标（如COMET）虽被广泛应用，但由于其“黑箱”特性，结果不总是可靠与稳健。因此，作者旨在揭示这些指标的潜在漏洞和安全风险，提升社区对评价指标可靠性的重视。

Method: 作者提出了一种在离散文本空间搜索单个“对抗性文本”的方法，该文本能够在多种测试情况下被COMET这类指标判定为高质量，并以WMT'24的英日和英德翻译任务作为实验场景。

Result: 利用该方法发现的“hub text”在WMT'24英日、英德任务上分别取得79.1%和67.8%的COMET分数，超过了M2M100模型针对每个源句分别生成的译文。此外，该hub text对其它语对（如日--英、德--英）同样有效。

Conclusion: 现有的神经网络型文本自动评价标准（如COMET）存在可被攻击的脆弱点，说明其可靠性和安全性值得进一步警惕和改进。

Abstract: Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.

</details>


### [128] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: 本文系统评测了5个最新SpeechLLMs与16个强大的直接和级联语音到文本翻译系统。结果显示，级联系统整体表现更为可靠，目前的SpeechLLMs只在部分任务上可与级联系统媲美，单独的语音基础模型（SFM）则表现较弱。融合大语言模型对于高质量语音翻译至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向多模态拓展，将语音作为原生输入模态已成为新趋势，SpeechLLMs的出现意在直接处理语音翻译，从而跳过传统的转录流程。本文旨在系统评测该新兴范式是否确实优于已经成熟的级联架构。

Method: 作者提出了Hearing to Translate评测套件，对5个主流SpeechLLMs和16个直接及级联翻译系统进行系统性对比。评测覆盖16个基准、13种语言对及9种复杂场景，包括口齿不清、噪声和长文本等情形。

Result: 实验显示，级联系统整体依然最为稳健，SpeechLLMs仅在有限情形下与级联系统相当，语音基础模型单独使用时表现最差。同时，结合大语言模型，无论协议内整合还是流水线嵌入，都是获得高质量语音翻译的关键。

Conclusion: 目前，SpeechLLMs虽具潜力，但在大多数场景下尚无法全面超越级联系统。高性能语音翻译依然需依赖大语言模型的深度集成。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [129] [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)
*Darshil Chauhan,Adityasinh Solanki,Vansh Patel,Kanav Kapoor,Ritvik Jain,Aditya Bansal,Dhruv Kumar,Prateek Narang*

Main category: cs.CL

TL;DR: 本文提出了一种高效而注重隐私的ASR自适应方法，大幅提升了医疗场景下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 医学场景中急需自动语音识别（ASR）来提升文档处理效率，但现有技术由于数据隐私、算力与声学适应性等问题，难以应用于实际部署。

Method: 作者提出在边缘设备上结合LoRA（低秩适应）以实现持续学习，并采用多领域经验回放机制，有效缓解遗忘问题，同时保障数据隐私。

Result: 在真实医疗音频数据集上，该框架使ASR模型的词错误率相对降低17.1%，且灾难性遗忘降低47%。

Conclusion: 所提出的自适应ASR方案显著提升在资源受限医疗场景下的实用性和可靠性，为实际部署ASR系统提供新路径。

Abstract: Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.

</details>


### [130] [Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics](https://arxiv.org/abs/2512.16530)
*Primoz Kocbek,Leon Kopitar,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型（LLMs）在简化生物医学文本、提升健康素养方面的应用，比较了多种LLM简化方法的表现。


<details>
  <summary>Details</summary>
Motivation: 医疗健康文本专业性强，普通民众难以理解，限制了健康知识普及。通过大模型简化文本，有助于提升大众健康素养，因此有必要探究LLM在此任务中的具体效果及有效方案。

Method: 使用公开生物医学文本简化数据集，实验了三种方法：基础prompt模板法、双AI代理法、微调法，并以gpt-4o与gpt-4o mini为基线。评估指标包括Flesch-Kincaid年级水平、SMOG、SARI、BERTScore、G-Eval等定量指标及Likert量表定性评价。

Result: gpt-4o-mini模型在文本简化任务上表现最好，微调方法效果较差。G-Eval定量评价与人工质性评价的排序基本一致，表现较好。

Conclusion: LLMs对生物医学文本简化具备较强能力，gpt-4o-mini更为出色，复杂微调方案表现反而一般。G-Eval工具值得关注和推广。

Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.

</details>


### [131] [UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification](https://arxiv.org/abs/2512.16541)
*Primoz Kocbek,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本文介绍了作者在CLEF 2025 SimpleText任务中，使用不同版本的GPT-4.1模型进行科学文本简化的实验及结果比较。


<details>
  <summary>Details</summary>
Motivation: 科学文本通常晦涩难懂，简化科学文本有助于更广泛的公众理解和知识传播。该任务旨在自动化地简化科技内容。

Method: 作者采用了OpenAI的gpt-4.1、gpt-4.1mini、gpt-4.1-nano模型，分别测试了无上下文（仅提示工程）和微调（FT，fine-tuned）两种简化方式，在句子和文档层面进行效果对比。

Result: gpt-4.1-mini在无上下文方法下，在两种简化任务中均表现出色。微调后的模型表现参差不齐，尤其在不同粒度的简化任务中，gpt-4.1-nano-ft在某些文档级简化任务中表现突出。

Conclusion: 简化任务在不同粒度上存在挑战，复杂性高。gpt-4.1-mini无需上下文便表现优秀，微调模型在特定场景下有独特优势。

Abstract: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.

</details>


### [132] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: 本文提出了一种无需重新训练即可在推理阶段精细控制大语言模型在政治敏感话题上拒答行为的方法 —— Refusal Steering。该方法利用LLM判别拒答信心分数，通过改进的向量操控技术，一方面削弱模型对政治敏感问题的拒答，另一方面保持对有害内容的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理政治敏感话题时常有过度拒答现象，影响实用性，但直接去除拒答又可能损害安全性，且现有拒答检测方法易碎、不泛化。作者期望实现对模型拒答行为的细粒度可控，既能灵活应对敏感内容，又能保持整体安全。

Method: 方法上，作者以LLM自身（rather than pattern-matching）作为“法官”，计算对回答请求的拒答信心分数；然后，提出带ridge正则的向量操控方法（steering vectors），更好地提取“拒答-合规”方向，实现对模型拒答行为的定向强化或抑制，且无须重新训练模型。

Result: 在Qwen3-Next-80B-A3B-Thinking等大模型上实验证明，该方法能够有效去除对政治敏感话题的无效拒答，同时在JailbreakBench和其他通用基准上保持原有安全性和性能。该方法还支持在需要时对特定内容诱导拒答，且结果在不同规模的大模型（4B到80B）上均有很强泛化。进一步分析表明，拒答信号多集中于transformer较深层，且分布于多个维度。

Conclusion: Activation steering为大模型在不丧失安全性的前提下，提供了灵活透明的拒答行为可控手段，实现了推理时的高效可控内容审核，为大模型实际部署中的细粒度内容调节和合规安全提供了可行方法。

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [133] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种简单的单阶段、固定超参数的RL训练方法JustRL，在数学推理基准上实现了SOTA性能，用算力仅为复杂方法的一半。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习训练日益复杂，包括多阶段流程、动态超参和课程学习，但并不清楚这些复杂性是否必须。研究疑问：增加的训练复杂性真的有必要吗？

Method: 提出JustRL方法：仅用单阶段训练、固定超参数，无需常见的复杂技巧，在两个1.5B参数模型上测试，无调参迁移学习，4000步以上持续改进。还做了消融，对比常规tricks（如长度惩罚、稳健验证器）的效果。

Result: JustRL在9个数学基准测试中平均准确率分别达到54.9%和64.3%，明显优于复杂流程，用计算量仅为后者一半。消融显示常规tricks反而降低性能。

Conclusion: 增加训练复杂度并非必需，稳定、可扩展的简单基线足以实现强性能，并能减少探索崩溃风险。建议领域社区重视简单、高效的RL训练基线。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [134] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出GinSign框架，提升了自然语言到时序逻辑（Temporal Logic，TL）翻译中的原子命题对齐（grounding）准确性，实现了更高的逻辑等价性能。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到时序逻辑的翻译方法，在原子命题与系统签名的对齐上存在准确率不足，导致翻译虽正确但语义无效，难以直接应用于系统验证等工程实践。提升对齐准确率是提升自动化可信系统工程实用性的关键。

Method: GinSign框架提出了分层grounding方法，先预测谓词标签，再选择对应类型的常量参数。将原本自由生成式的任务转化为结构化的分类任务，使得模型可以采用更小的mask语言模型，无需依赖大型LLM，提高了效率和可推广性。

Result: 实验覆盖多个领域，GinSign实现了95.5%的逻辑等价准确率，比当前最优方法提升了1.4倍。常规方法未做有效对齐时产生的TL表达，在语法上虽正确，但语义上与目标表达通常不等价。GinSign有效支持实际系统的模型检验。

Conclusion: GinSign显著提升了自然语言到时序逻辑翻译中的grounding准确率，为自动化系统规范制定和验证提供了新的更实用的技术路径。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [135] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种带有推理追踪的增强型检索生成（RAG）框架，有效解决了传统RAG在处理冲突、过时或主观信息时的不足，并通过实验展示了其在准确性和行为一致性上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在检索到冲突、有偏见或过时信息时，难以做出合理、可解释的决策，而之前的改进方案多是针对单一问题，缺乏统一、可解释的推理监督机制。

Method: 作者提出了一个三阶段框架：（1）文档级裁决；（2）冲突分析；（3）有根据的综合归纳，形成带有引用的答案或合理拒答。同时引入了冲突感知信任分数（CATS）评价流水线，利用LLM作为评判，对答案的依据、事实准确性、拒答合理性以及冲突行为进行评价。

Result: 在539组推理数据和新评价流水线下，提出的方法（尤其结合Supervised Fine-Tuning）大幅提升了Qwen模型的端到端答案正确率（从0.069提至0.883）和行为一致性（从0.074提至0.722），大大优于基线。

Conclusion: 推理追踪增强型RAG系统及其评价体系为构建具冲突感知和可解释性的RAG奠定了基础，对现实应用中依赖多源证据的问答系统具有重要意义。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [136] [Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology](https://arxiv.org/abs/2512.16802)
*Primož Kocbek,Azra Frkatović-Hodžić,Dora Lalić,Vivian Hui,Gordan Lauc,Gregor Štiglic*

Main category: cs.CL

TL;DR: 本论文评估了多模态RAG方法在生物医学领域复杂视觉数据环境下的应用效果，针对将图表转换为文本或直接用视觉检索两种方案进行了实验对比。


<details>
  <summary>Details</summary>
Motivation: 生物医学问题解答常常涉及大量视觉数据（如图像、表格），如何有效地将这些信息融入生成式问答管道以提升正确率，特别是在检索策略选择（视觉转文本vs. OCR-free视觉检索）上尚无定论。该研究旨在明确不同模型能力下的最佳检索-生成管道配置。

Method: 以糖生物学为实验领域，建立了覆盖多种难度的120题多项选择题基准数据集。分别构建无增强、文本RAG增强、多模态转换、OCR-free视觉检索（ColPali）四种检索增强方式，采用Docling解析与Qdrant索引，评估包括Gemma-3-27B-IT、GPT-4o家族及GPT-5系列等不同规模大模型，反复试验计算准确率及置信区间。

Result: 中等规模模型（如Gemma-3-27B-IT）下，文本和多模态转换增强准确率高于OCR-free视觉检索（0.722-0.740 vs 0.510）。强大模型（如GPT-4o）下，多模态转换表现最佳（0.808），文本和OCR-free检索差距缩小。更强GPT-5系列下，各视觉检索器间无显著性差异，ColFlor以较小模型体量达到与大模型检索器相同效果。

Conclusion: 检索-生成管道选择依赖于模型容量：中等模型优先将视觉信息转化为文本，大模型下OCR-free视觉检索表现更佳。对于强生成器模型，ColFlor等轻量级视觉检索器在效率和准确率之间达到了良好平衡，推荐优先使用。

Abstract: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.

</details>


### [137] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出了一种新的自然语言转时序逻辑（NL到TL）翻译框架GraFT，通过减少每一步输出词汇空间，提升了翻译准确率和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到时序逻辑的翻译方法在原子命题抽取、代词指代消解以及小样本学习等方面表现欠佳。人机交互和自动系统需要更准确高效的翻译方法。

Method: GraFT框架通过限制每一步输出的合法词汇，仅在有效集合中进行选择，从而大大缩小了搜索空间。该方法利用了NL到TL任务的特殊结构，分阶段减少难度，并提供了理论分析解释其有效性。

Result: 在CW、GLTL和Navi等基准上测试，GraFT在端到端翻译准确率上比现有最优方法高5.49%，在域外泛化准确率上提升了14.06%。

Conclusion: GraFT通过缩减输出空间简化了自然语言到时序逻辑的翻译，提升了准确性和学习效率，为人机交流提供了更优的技术路径。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [138] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息论的方法，用以量化语音中韵律独立于文本所传递的信息量，并分析这些信息具体指向哪些维度。


<details>
  <summary>Details</summary>
Motivation: 语音中的韵律能够传达比文本更多的关键信息，例如情感、讽刺等，而这些信息在传统的文本分析中常被忽略。因此，迫切需要一种系统性的方法来量化韵律所承载的信息，并区分其与文本的差异。

Method: 基于大规模语音和语言模型，估算语音不同通道（如音频与文本）之间与特定含义维度（如情感、讽刺、疑问）的互信息。分析对象包括电视和播客中的语音数据，分别评估韵律与文本对讽刺、情感和疑问性的表达信息量。

Result: 实验表明，对于讽刺和情感维度，音频通道（即韵律）所传递的信息量远超文本通道，尤其是在缺少长时上下文时最为显著。而在表达问题（疑问性）时，韵律增加的信息量则相对较少。

Conclusion: 所提出的方法有效量化了韵律与文本在不同维度上的信息贡献，为后续拓展更多意义维度、交流通道及语言类型的研究奠定了基础。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [139] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: 本文提出了一种新的Transformer加速机制——LLMCache，通过在模型的多个层级缓存中间激活状态，实现高效推理加速，且准确率损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型虽然性能优越，但推理延迟高，难以满足实时或大规模应用的要求。虽然有些方法能通过缓存机制提升自回归解码速度，但应用范围有限。

Method: LLMCache为一种新颖的层级缓存框架，能在输入序列语义相似时重用Transformer中间层激活，不依赖具体模型类型，适配编解码器结构且支持任意层缓存。为此，作者设计了轻量级指纹匹配机制判断语义相似，并针对缓存过时提出了自适应淘汰策略。

Result: 在BERT和GPT-2模型上，针对SQuAD、WikiText-103以及OpenBookQA等任务进行实验，LLMCache推理加速最高可达3.1倍，准确率损失小于0.5%。

Conclusion: LLMCache是一种实用且通用的Transformer推理加速解决方案，可望广泛应用于实际场景中，显著降低推理延迟且基本不损失准确率。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [140] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: 该论文提出了一种使大语言模型更加智能地决定何时调用外部搜索的新方法AdaSearch，显著提升了模型对自身知识边界的感知能力，同时减少无谓搜索，让模型决策过程更透明、可信。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的搜索代理往往面临两难：过度依赖外部搜索会增加成本和风险，仅依赖参数化知识又容易出现幻觉（hallucination）。已有方法通过奖惩机制抑制过量搜索，但这些方法实现繁琐难以推广，无法有效区分必要与无谓的搜索。

Method: 作者首先用F1指标量化当前搜索代理对自身知识的感知力，发现主流方法往往忽视自身已知内容。随后，作者提出AdaSearch方法，将问题求解与“是否调用搜索”的决策过程分离为两个阶段，使搜索决策过程更清晰和可解释。

Result: 在不同模型结构和规模下的实验显示，AdaSearch显著提高了模型衡量自身知识边界的能力，有效减少不必要的搜索调用，同时保持了强任务表现。

Conclusion: AdaSearch不仅提高了代理适应性及知识边界感知，还使决策过程更透明，这对于金融、医疗等高风险领域尤为重要。

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


### [141] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 本文提出了MMRB2，这是第一个针对多模态理解与生成奖励模型的综合基准，包括文本生成、图像编辑、交错生成和多模态推理等任务，共有4000个人工标注偏好对。通过MMRB2评测多模态奖励模型发现，目前主流大模型准确率显著低于人类。


<details>
  <summary>Details</summary>
Motivation: 当前多模态奖励模型研究不足，评价方法缺乏，尤其是在处理图文交错序列和多模态生成方面。缺乏全面且高质量的基准阻碍了相关模型的进步和客观比较。

Method: 作者构建了MMRB2基准，涵盖四种多模态任务，采集了23个模型和智能体在21个任务上的响应，每项任务都包含1000个人工偏好对。标注过程采用专家共识并结合集成过滤策略。同时，分析了现有的多模态奖励模型在这些任务上的表现，比较了不同模型的准确率。

Result: 最新的Gemini 3 Pro模型在基准上的准确率为75-80%，GPT-5和Gemini 2.5 Pro为66-75%，人类准确率超过90%。开源模型Qwen3-VL-32B与Gemini 2.5 Flash表现相当，为64%。GPT-4o的表现较差，仅为59%。此外，奖励模型在此基准上的表现与下游任务的效果高度相关。

Conclusion: MMRB2为多模态奖励模型提供了一个系统评测平台，能够有效反映模型实际能力。目前多模态奖励模型与人类尚有明显差距，未来需要关注奖励模型的进一步提升。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [142] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 本文研究了transformer在变量意味动态变化的代数运算任务中的机制，发现无论符号含义如何变换，模型都能学习到接近完美正确率，并发展出多种符号推理机制。


<details>
  <summary>Details</summary>
Motivation: 以往关于transformer理解算术的研究，主要是符号意义固定的情境下，发现其嵌入空间会反映代数结构。本工作希望探究当符号（变量）意义随序列动态变化时，transformer如何适应与推理。

Method: 作者设计了一种新任务，每个输入序列变量的代数群值分配都不同，迫使模型不能依赖固定的符号含义。模型还在未见过的群结构上测试泛化能力。通过针对模型内部机制假设设计特殊数据分布，进行因果验证，分析transformer到底学会哪些具体机制。

Result: transformer在上述困难设置下，依然能学到接近完美的准确率，并能泛化到新的代数群。通过机制分析，发现模型主要学到三种机制：交换复制（复制答案）、单位元识别（识别单位元相关事实）、基于闭包的抵消（追踪群成员以限制答案）。

Conclusion: 与符号意义固定时偏向几何嵌入的机制不同，本文发现transformer在变量意义随环境变化时，会发展出更具符号推理能力的机制，表明其具备一定的灵活推理能力。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [143] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: 本文提出了一种名为“结构性电路放大（Constructive Circuit Amplification）”的新方法，仅通过修改极少量与特定任务高度相关的模型组件，对大模型的能力进行精确和定向的提升，实现了数学推理能力最高11.4%的提升，并且不影响模型的其他能力。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现，大模型内部存在稀疏的“电路”，它们负责特定任务的实现，而微调提升性能往往是通过增强这些已有电路。该现象启发作者，希望能通过直接对相关电路进行干预，实现能力的精确提升，而不是盲目的全模型微调。

Method: 所提方法名为Constructive Circuit Amplification：首先，根据模型推理轨迹识别关键token，再锁定与目标任务相关的模型组件，然后仅对这些极少部分组件进行更新。

Result: 在数学推理任务上，经过该方法处理的模型准确率提升最高达到+11.4%，同时仅有1.59%的模型组件被修改，并且在其他常见评测集（如MMLU、TriviaQA和TruthfulQA）中的表现几乎未受影响。

Conclusion: 通过针对性地识别和更新与特定任务相关的模型电路，可以高效且可靠地增强大模型的目标能力，而且不会带来明显的副作用。这为模型精细化、定向能力提升提供了新途径。

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [144] [Large Video Planner Enables Generalizable Robot Control](https://arxiv.org/abs/2512.15840)
*Boyuan Chen,Tianyuan Zhang,Haoran Geng,Kiwhan Song,Caiyi Zhang,Peihao Li,William T. Freeman,Jitendra Malik,Pieter Abbeel,Russ Tedrake,Vincent Sitzmann,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种新的通用机器人基础模型，采用大规模视频预训练方法，能够直接从视频生成机器人可执行的动作计划，并在多任务和真实机器人平台上展示了广泛的泛化能力和执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人决策模型多依赖多模态大语言模型（MLLMs）的图像和语言预训练，期望将其泛化能力迁移到动作输出，但这忽略了视频数据在捕捉机器人行为时更自然的时空序列优势。本文针对机器人行为的本质特征，探索以大规模视频预训练为主的建模范式，以提高泛化与可操作性。

Method: 1. 构建包含人类活动和任务演示的大型互联网视频数据集。2. 首次在基础模型规模上训练开源视频生成模型，用于机器人任务规划。3. 在推理阶段，模型针对新场景和任务生成“零样本”视频方案，通过后处理技术提取可执行机器人动作。4. 在第三方任务和真实机器人平台上进行泛化与实际执行验证。

Result: 模型能够针对未见过的场景和任务生成合理的执行视频方案，并被有效转化为机器人的动作指令，在多样化任务和真实环境下实现了稳健的执行与良好的泛化表现。

Conclusion: 基于视频预训练的机器人基础模型在泛化和实用性上优于传统以MLLM为核心的方案。该工作开源了模型和数据集，为视频驱动的通用机器人学习提供了基础设施与新范式。

Abstract: General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.

</details>


### [145] [SORS: A Modular, High-Fidelity Simulator for Soft Robots](https://arxiv.org/abs/2512.15994)
*Manuel Mekkattu,Mike Y. Michelis,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出了一种高保真、模块化的软体机器人仿真器SORS，能准确模拟大变形、材料不可压缩性及接触等复杂物理行为，并在多项真实实验中验证了其实用性和仿真与现实的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前软体机器人在多物理场环境中的部署，对仿真准确性和可扩展性提出了更高要求，但现有仿真器在大变形、材料间复杂相互作用及接触建模等方面存在不足，难以满足实际应用需求。

Method: SORS采用基于有限元法的能量驱动框架，支持模块化扩展自定义材料和驱动模型。为保证接触建模的物理一致性，集成了基于顺序二次规划的受约束非线性优化方法，实现了稳定、精准的接触模拟。

Result: 通过悬臂梁挠曲、软臂压力驱动和PokeFlex数据集中的接触测试等实验，SORS成功实现了高一致性仿真，并在软体机器腿的控制优化中展现了方法优势。

Conclusion: SORS有效填补了软体机器人仿真在可扩展性、高保真度和实用性方面的空白，为新一代软体机器人原型设计和控制优化提供了可靠工具。

Abstract: The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.

</details>


### [146] [dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection](https://arxiv.org/abs/2512.16011)
*Jack Naylor,Raghav Mishra,Nicholas H. Barbara,Donald G. Dansereau*

Main category: cs.RO

TL;DR: 本论文提出了一个端到端可微分的在轨检测仿真管道（∂LITE），可自动优化和设计卫星巡视路径以提升成像质量，在类似低轨环境下提升空间资产的可视化检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着高价值卫星维护、损伤评估和寿命延长的需求增加，在轨资产的可视化检测变得日益重要。然而，低地轨道（LEO）环境中反射、阴影和动态光照等因素，以及巡视期间航天器之间的相对运动，严重影响了成像数据的质量。如何基于成像质量自动规划最佳巡视轨迹，尚未得到充分研究。

Method: 论文提出了∂LITE，一个端到端、可微分的在轨巡视仿真管道。它结合了先进的可微渲染工具与自定义轨道传播器，可以根据视觉传感器数据自动化优化巡视轨迹，实现对轨道参数的端到端优化，以提升巡视成像质量。

Result: ∂LITE 能够自动设计出许多非直观但成像质量显著提升的巡视轨迹，有效地提高了巡视数据的质量与有用性。

Conclusion: 本文首次提出了利用可微分渲染与轨道仿真结合的巡视规划方法，为空间任务规划带来了新的计算方法和见解，对提升空间资产检测质量具有重要意义。

Abstract: Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/

</details>


### [147] [Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios](https://arxiv.org/abs/2512.16019)
*Qiping Zhang,Nathan Tsoi,Mofeed Nagib,Hao-Tien Lewis Chiang,Marynel Vázquez*

Main category: cs.RO

TL;DR: 本研究提出利用大型语言模型（LLM）的少样本学习能力，高效预测用户对机器人行为的感知表现，实验表明该方法数据需求更低且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何评价机器人行为对于开发符合人类期望的社交机器人至关重要。现有基于数据驱动的方法需要大量标注数据，限制了实际应用，因此需要更高效的方法捕捉用户反馈。

Method: 扩展了SEAN TOGETHER数据集，收集更多人机导航场景与用户反馈，使用LLM通过少量上下文示例，基于机器人与人类运动的时空特征，预测用户对机器人性能的感知，并与传统监督模型对比。还进行了特征消融分析和个性化示例实验。

Result: LLM在极少需标注数据的情况下就可达到甚至超过传统监督学习模型的表现；随着上下文示例增多，预测效果提升，表明方法具备良好扩展性。个性化示例进一步提升预测精度。

Conclusion: 利用LLM进行少样本用户反馈理解，不仅能高效准确预测用户感知，还能通过个性化示例等方式进一步提升表现，为大规模以用户为中心的机器人行为优化提供了新途径。

Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.

</details>


### [148] [Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface](https://arxiv.org/abs/2512.16024)
*Rishabh Dev Yadav,Shrey Agrawal,Kamalakar Karlapalem*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人协作运输系统，能在未知且不平坦的地形中运输并保持载荷的预期姿态，通过仿真验证了其控制方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前多机器人运输载荷在复杂地形中，保持载荷稳定和正确姿态是挑战，现有方法多对地形有假设，缺乏通用性。该研究希望解决在未知、非均匀坡度环境下载荷稳定运输的问题。

Method: 采用定制机器人，每个机器人顶部均装有线性致动器（活塞），系统持续监测载荷姿态，并计算每个机器人的活塞高度以维持载荷姿态。控制方法为开环控制器与闭环PID控制器结合，对地形类型不做假设。

Result: 在多种仿真环境及复杂崎岖地形下测试，验证了所提控制器的有效性，能够实现载荷的姿态保持和稳定运输。

Conclusion: 提出的多机器人运输系统与控制方法无需地形先验，能在任意未知、不均匀地形下有效地运输载荷并保持其姿态，具有较强的通用性与实际应用潜力。

Abstract: In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.

</details>


### [149] [SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments](https://arxiv.org/abs/2512.16027)
*Shuaidong Ji,Mahdi Bamdad,Francisco Cruz*

Main category: cs.RO

TL;DR: 提出了一种结合TD3强化学习、模糊逻辑和优先经验回放的新型无人机（UAV）导航方法，使无人机能在复杂和动态环境中高效、自主地避障导航，并在仿真测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在有障碍物且动态变化的环境中导航时，传统方法常因环境复杂和实时性要求高而表现有限，需要更高效、可靠的解决方案。作者希望通过改进学习机制及安全策略，实现更稳定、强泛化能力的自主导航系统。

Method: 方法以TD3为核心，加入优先经验回放（关注高误差事件）、递减epsilon贪婪探索（提升探索效率），融合轻量级模糊逻辑安全层（根据传感器信息和安全得分限制不安全动作），前端用激光雷达数据生成带信心权重的安全地图。奖励函数结合目标推进度、安全间隙及切换经济性等，引导强化学习加速收敛。系统在Webots仿真平台实现，并采用邻近碰撞检测机制。

Result: 与主流基线方法相比，该方法在轨迹平滑度和对新环境的泛化能力上表现更优，同时保持了实时响应能力。

Conclusion: 融合TD3强化学习、经验优先回放、模糊安全控制及合理的探索策略，可实现在复杂环境下稳定、可部署且表现优越的无人机导航系统。

Abstract: Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.

</details>


### [150] [A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators](https://arxiv.org/abs/2512.16069)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Rui Dai,Matteo Dalle Vedove,Jiatao Ding,Daniele Fontanelli,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: 该论文提出了一种模块化机械臂一体化设计与运动优化框架，实现了多分支结构机械臂的高效任务适应能力。


<details>
  <summary>Details</summary>
Motivation: 模块化机械臂具有高度适应性，但在任务执行中需要同时优化结构形态和安装姿态，并满足运动学、动力学及物理约束，传统的单分支结构在扩展时存在扭矩过载问题。

Method: 作者提出了统一的任务驱动型计算框架，集成了多形态下的轨迹规划及结构/安装姿态的协同优化。方法上，采用层次化模型预测控制（HMPC）进行运动规划，并利用CMA-ES算法联合优化离散结构配置与连续安装姿态。引入虚拟模块，实现双分支结构，辅助分支可分担主分支的扭矩，有效扩展工作空间。

Result: 通过仿真与实物实验（如抛光、钻孔与搬运等任务），证明了所提框架的有效性。结果显示，框架能产生满足各项约束的多种可行设计，并可自定义目标函数灵活实现如最大操作性、最小关节负载和最少模块数等目标。同时，实现了在不增加模块功率下的大工作空间双分支结构。

Conclusion: 该框架为模块化机械臂设计、部署和任务自适应提供了高效方案，可根据任务自定义优化目标，实现灵活多分支结构扩展和运动优化，在工业机器人实际应用中具有重要价值。

Abstract: Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.

</details>


### [151] [A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level](https://arxiv.org/abs/2512.16076)
*Jia Hu,Junqi Li,Xuerun Yan,Jintao Lai,Lianhua An*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自动驾驶车辆仿真平台的全自动校准方法，能够高效精准地复现车辆与背景交通的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真校准方法无法同时在车辆与整体交通流层面精准复现自动驾驶车辆与背景交通的交互，影响了仿真测试的可靠性，因此需要更高效且准确的校准方法。

Method: 论文提出了一种新型仿真平台校准方法，具备：1）支持车-车交互级别校准；2）保证校准精度；3）提高校准效率；4）支持自动化流水线校准。该方法与未校准方法及当前主流校准方法进行了对比评测。

Result: 结果显示，该方法的交互复现准确率提升了83.53%，校准效率提升了76.75%，在车辆层面和交通流层面准确率提升了51.9%。同时，整个校准流程全自动化，无需人工干预。

Conclusion: 所提出的方法能大幅提升仿真复现的准确度和效率，保障对自动驾驶车辆的更可靠仿真评估，且自动化水平高，具备实际应用潜力。

Abstract: Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.

</details>


### [152] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: 提出了一种新的One-shot仿生学习框架ManiLong-Shot，实现了机器人对长时序操作任务的一次性学习，显著提升了泛化能力和执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有的一次性仿生学习方法主要适用于短时序任务，对复杂的长时序操作（如多步抓取、放置等）效果有限，难以实现高效泛化和实际应用。研究者希望突破当前OSIL仅能应对简单任务的局限，使机器人能一次性模仿并完成更复杂的操作任务。

Method: ManiLong-Shot通过将长时序任务按物理交互事件划分为多个原子操作（primitive），利用视觉-语言大模型(VLM)或基于规则的启发方式进行高层次分解。每个原子操作，预测关键不变量区域、建立示范与当前观测间的对应关系，并计算目标末端执行器的位置，从而分段实现整体复杂任务。

Result: 在仅使用10个短时序任务训练的情况下，ManiLong-Shot在仿真中可以对20个难度不同的长时序任务进行一次性仿生学习，实现了比当前最优方法(SOTA)高22.8%的相对性能提升。实物机器人上，也可稳定执行3个长时序任务，显示出真实应用潜力。

Conclusion: ManiLong-Shot有效扩展了一次性仿生学习在复杂长时序操作中的应用边界，提升了学习效率和可泛化能力，为机器人实际使用带来了新的可能性。

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>


### [153] [A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion](https://arxiv.org/abs/2512.16367)
*Sijia Chen,Wei Dong*

Main category: cs.RO

TL;DR: 本文提出了一种地面-空中协同系统，实现飞行机器人在复杂环境下的鲁棒定位，尤其是在视觉传感器失效的情况下。通过整合主动视觉、单点测距、惯性里程计和光流信息，显著增强了定位精度和系统韧性。实验证明系统在多种干扰条件下都能稳定工作。


<details>
  <summary>Details</summary>
Motivation: 传统基于固定相机和标记物的定位方法易受距离和视觉捕捉失败影响，特别是在视觉信号退化时，定位鲁棒性大幅降低，限制了飞行机器人在复杂环境中的应用。本文旨在解决这些局限，提高定位系统的鲁棒性和适用性。

Method: 改进后的定位框架整合了主动视觉、单点测距、惯性里程计和光流信息。地面车辆上安装的可动相机通过红外标记主动识别飞行机器人，并通过单点测距技术提升远距离捕捉和失效恢复能力。采用多源数据融合的降维估计算法和自适应置信度评价机制，动态优化测量权重，提升精度和效率。

Result: 在烟雾干扰、光照变化、障碍遮挡、长时间视觉丢失和远距离等恶劣条件下，实验显示系统平均均方根误差约为0.09米，表现出优秀的鲁棒性和抗失效能力。

Conclusion: 所提地空协同定位方法在极端环境下显著提高了飞行机器人定位的精度和鲁棒性，验证了其在多种实际场景中的应用潜力，改善了捕捉丢失和传感器失效下的表现。

Abstract: It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.

</details>


### [154] [E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion](https://arxiv.org/abs/2512.16446)
*Enis Yalcin,Joshua O'Hara,Maria Stamatopoulou,Chengxu Zhou,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: E-SDS 框架结合视觉语言模型和实时地形传感分析，实现了环境感知的自动奖励函数生成，大幅提升了类人行走机器人的自主运动能力，减少了人工设计奖励的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉语言模型的方法在奖励函数设计上依赖人工，并且缺乏对复杂地形的环境感知，无法完成如下楼梯等复杂任务。如何自动化且高效地产生感知增强的奖励函数是该领域的重要问题。

Method: 本文提出 E-SDS 框架，将视觉语言模型与实时地形传感器分析融合，根据示例视频自动生成奖励函数，从而训练鲁棒的环境感知行走策略，不再依赖手工调优或单纯基于视觉语言的盲目奖励生成。

Result: 在 Unitree G1 类人机器人和四种地形（简单、缝隙、障碍、楼梯）上评估，E-SDS 成功实现下楼梯等复杂运动任务，人工奖励设计和无环境感知基线方法均失败。所有地形下，E-SDS 令速度跟踪误差降低 51.9-82.6%，显著优于对照组。奖励设计时间由数天下降至2小时内。

Conclusion: E-SDS 显著简化了奖励函数设计流程，提升了机器人在复杂地形下的自主运动能力，具备高效性和鲁棒性，对机器人运动控制领域具有广泛应用前景。

Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.

</details>


### [155] [Single-View Shape Completion for Robotic Grasping in Clutter](https://arxiv.org/abs/2512.16449)
*Abhishek Kashyap,Yuxuan Yang,Henrik Andreasson,Todor Stoyanov*

Main category: cs.RO

TL;DR: 该论文提出利用扩散模型进行单视角下的3D形状补全，提高机器人在实际杂乱场景中抓取成功率。实验证明在复杂场景中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉抓取方法受限于单摄像头视角与物体遮挡，导致可获取的物体几何信息不完整，影响抓取效果。因此需要一种能够补全3D物体形状的方法来提升抓取算法表现。

Method: 作者提出利用扩散模型，从单视角深度图恢复物体的完整三维几何结构。针对常见家居物品，形状补全结果被用于后续抓取推断网络，并且与以往只考虑孤立物体或低杂乱度场景的工作不同，论文聚焦于现实中杂乱环境下的形状补全和抓取任务。

Result: 在实际杂乱家居场景的预实验中，该方法比没有形状补全的朴素基线提升抓取成功率23%，比最新形状补全方法提升19%。

Conclusion: 利用扩散模型实现的3D形状补全能显著提升机器人在复杂场景中的抓取成功率，展示出明显优于现有方法的潜力。

Abstract: In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.

</details>


### [156] [AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems](https://arxiv.org/abs/2512.16454)
*Tianhao Shao,Kaixing Zhao,Feng Liu,Lixin Yang,Bin Guo*

Main category: cs.RO

TL;DR: 提出了一种适用于无人系统的任务招募框架MPBS，有效提升无人设备参与任务的效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着无人机、无人车在城市感知和应急响应等领域的重要性提升，如何高效调度这些设备执行敏感任务成为亟需解决的问题。

Method: MPBS框架包含三个核心模块：行为感知KNN分类器、时变马尔可夫预测模型（用于设备移动预测）、动态优先级调度机制（结合任务紧急度与基站性能）。这使得MPBS能够基于设备行为和移动性进行实时、适配性任务分配。

Result: 基于真实GeoLife数据集的实验显示，MPBS显著提升了任务完成效率和无人系统资源利用率。

Conclusion: MPBS提供了一种预测性和行为感知兼具的无人系统智能协作调度新方案，有助于实现更高效的无人设备协作。

Abstract: As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable "user". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.

</details>


### [157] [Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing](https://arxiv.org/abs/2512.16469)
*Jiayu Zhang,Kaixing Zhao,Tianhao Shao,Bin Guo,Liang He*

Main category: cs.RO

TL;DR: 本文提出了一种名为Tri-Select的多阶段视觉数据选择框架，有效过滤冗余和低质量的移动众包环境监测图像，并显著提升了数据选择效率和数据集质量。


<details>
  <summary>Details</summary>
Motivation: 移动视觉众包环境监测会产生大量来自不同设备的图像数据，这些数据往往冗余严重、质量不一，不利于后续分析和系统扩展，因此亟需有效的数据选择与过滤方法。

Method: Tri-Select框架分为三步：首先利用元数据过滤掉无关样本；然后基于空间相似性进行谱聚类，组织候选图像；最后通过最大独立集搜索与视觉特征引导，选择代表性且高质量的图片。

Result: 通过在真实世界和公开数据集上的实验，Tri-Select在提升图像选择效率的同时，也显著提升了数据集的整体质量。

Conclusion: Tri-Select框架能够高效地应对移动视觉众包数据的冗余与异质性问题，为大规模众包应用提供高质量数据支持，具有良好的扩展性和实用前景。

Abstract: Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.

</details>


### [158] [A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots](https://arxiv.org/abs/2512.16555)
*Marcelo Rosa,José E. R. Cury,Fabio L. Baldissera*

Main category: cs.RO

TL;DR: 本文提出了一种利用监督控制理论协调多机器人自主建造三维结构的方法，实现了多机器人协作完成预定义结构任务。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作构建三维结构能够提升自动化建筑效率和灵活性，但如何保证各机器人协同、避免冲突，是实现自主建造的关键难题。

Method: 作者采用监督控制理论，基于单机器人建模和目标结构模型，自动合成正确可靠的反应式控制器（监督者），并将该监督者应用于所有机器人，实现多机器人自主协作建造。

Result: 通过将合成的监督者复制应用于各机器人，使得所有机器人能够协同完成目标三维结构的建造，验证了该方法的有效性。

Conclusion: 采用监督控制理论及模型合成方法可行有效，实现了多机器人安全有序地协作建造三维结构，具备一定的推广应用价值。

Abstract: In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots

</details>


### [159] [Olaf: Bringing an Animated Character to Life in the Physical World](https://arxiv.org/abs/2512.16705)
*David Müller,Espen Knoop,Dario Mylonopoulos,Agon Serifi,Michael A. Hopkins,Ruben Grandia,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文通过强化学习控制，将动画角色Olaf以近似原始动画风格的方式实体化，并通过创新机械设计、噪声控制和温控等多项优化，实现了高度逼真的机器人角色表演。


<details>
  <summary>Details</summary>
Motivation: 动画角色往往有非物理比例和运动风格，这对于机器人仿真提出了新的挑战和创新空间。该论文旨在让非典型比例和步态的动画角色Olaf在现实中变得“鲜活”，突破传统机器人形态限制，提升动画人物机器人的表现力和逼真度。

Method: 采用以动画为参考的强化学习策略指导机器人动作控制；通过创新机械结构（如泡沫裙下隐藏非对称腿及球面/平面连杆驱动手臂、嘴巴和眼睛）满足外观和功能需求；为降低走路时的冲击噪音，设计了奖励机制以减少声音；为防止颈部小型执行器过热，将温度作为策略输入，并引入温度奖励调控发热。

Result: 通过仿真和实物测试，系统有效控制了冲击噪音和执行器温度，Olaf机器人实现了极具动画风格和高度逼真的动作表现，验证了模型和上述优化措施的有效性。

Conclusion: 创新的控制方法与机械设计，可以让动画角色在现实机器人中展现前所未有的自然动作与拟真度。本方法对未来动画机器人角色的设计和控制具有重要借鉴意义。

Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.

</details>


### [160] [VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation](https://arxiv.org/abs/2512.16724)
*Yixiang Chen,Yan Huang,Keji He,Peiyan Li,Liang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为VERM的三维操作新方法，通过多摄像头信息进行3D感知，借助基础模型知识生成虚拟、自适应任务视角，有效提取关键信息，提升了操作效率和速度。实验验证了方法的先进性。


<details>
  <summary>Details</summary>
Motivation: 多摄像头用于机器人3D操作时，带来大量冗余与无关信息，导致计算成本高、训练慢，急需一种高效筛选、利用关键特征的新方法。

Method: 作者提出了VERM方法，利用基础模型知识，从多摄像头构建的点云中‘想象’出虚拟、与任务自适应的视角，提取关键特征，还设计了深度感知模块和动态由粗到细的操作流程。

Result: 在仿真平台RLBench和真实环境上，作者的方法显著优于现有SOTA（最优）方法，训练时间提升1.89倍，推理速度提升1.54倍。

Conclusion: VERM有效解决了多摄像头带来的冗余问题，在3D操作任务中实现了高效、准确的感知与规划，具有实际应用潜力。

Abstract: When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .

</details>


### [161] [Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future](https://arxiv.org/abs/2512.16760)
*Tianshuai Hu,Xiaolu Liu,Song Wang,Yiyao Zhu,Ao Liang,Lingdong Kong,Guoyang Zhao,Zeying Gong,Jun Cen,Zhiyu Huang,Xiaoshuai Hao,Linfeng Li,Hang Song,Xiangtai Li,Jun Ma,Shaojie Shen,Jianke Zhu,Dacheng Tao,Ziwei Liu,Junwei Liang*

Main category: cs.RO

TL;DR: 该论文综述了自动驾驶领域中融合视觉、语言和动作决策（VLA）模型的最新进展，并提出了对现有方法的系统分类框架。


<details>
  <summary>Details</summary>
Motivation: 传统的“感知-决策-动作”模块化自动驾驶系统在复杂或长尾场景中易失效，且感知误差会传递至后续模块。同时，直接“视觉到动作”(VA) 模型存在可解释性差、泛化能力弱等问题。新近多模态大模型（视觉+语言）的突破为提升自动驾驶决策的人类兼容性和可解释性提供了新方向。

Method: 作者梳理了VLA的发展脉络，将当前方法划分为“端到端VLA模型”和“双系统VLA模型”两类。端到端整合感知、推理、规划于一体；双系统则分为慢速深度推理（语言大模型）和快速安全控制（传统规划器）。进一步细化了各类子方向及其实现方式，并汇总了代表性数据集和评测基准。

Result: 系统整理了VLA领域的主流模型、分类方法、子类别特征，以及对应的数据集和评测手段，全面总结了当前的研究现状。

Conclusion: VLA为自动驾驶决策带来了更好的可解释性、泛化性及人机对齐能力。该综述为推动更安全、智能和人类兼容的自动驾驶研究奠定了理论和方法基础。

Abstract: Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.

</details>


### [162] [PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence](https://arxiv.org/abs/2512.16793)
*Xiaopeng Lin,Shijie Lian,Bin Yu,Ruoqi Yang,Changti Wu,Yuzhuo Miao,Yurun Jin,Yukun Shi,Cong Huang,Bojun Cheng,Kai Chen*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法，将人体第一视角视频转化为可用于机器人训练的结构化监督数据，从而提升机器人对自身视角下任务的理解和决策能力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数视觉语言模型（VLMs）主要基于第三人称视角数据训练，这与仿人机器人实际操作时的第一人称视角存在视角不匹配问题。而大规模采集机器人的第一人称数据成本高且难以覆盖多样场景，因此急需探索低成本、丰富的训练监督来源。

Method: 作者提出Egocentric2Embodiment翻译流程，将海量人类第一人称视频转化为多层次、结构化的问答形式监督（VQA），并构建大规模E2E-3M数据集。随后在该数据集上训练新模型PhysBrain，使其具备自我视角的感知与规划能力。

Result: PhysBrain模型展现出了更强的第一人称理解能力，尤其在EgoThink上的规划表现大幅提升。同时，PhysBrain作为初始化方法有助于实现更高效的视频-语言适配与微调，在SimperEnv任务中的成功率达到53.9%，显著优于以往方法。

Conclusion: 通过利用大规模人类第一视角视频，作者实现了以低成本获得高质量自我视角监督，有效提升了机器人控制和泛化能力，验证了人类视频监督到机器人任务迁移的可行性和效果。

Abstract: Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.

</details>


### [163] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: ReinforceGen 提出了一种结合任务分解、数据生成、模仿学习和运动规划，再通过强化学习微调的系统，有效提升了长时序机器操作任务的表现。


<details>
  <summary>Details</summary>
Motivation: 长时序操控任务在机器人领域一直是难点，现有方法在关联多个动作及任务分解实现、泛化能力方面有限，需要新的方法提升整体表现与应用范围。

Method: ReinforceGen 首先将复杂任务分割为多个局部技能，通过运动规划衔接。技能和运动目标均以模仿学习进行训练，训练数据源自10次人工演示。最后采用在线自适应和强化学习对整个系统进行微调优化。

Result: 在 Robosuite 数据集最高难度下，ReinforceGen 实现了 80% 的成功率。消融实验显示强化微调对性能有 89% 的提升贡献。

Conclusion: 将任务分解、模仿学习与强化学习结合并对系统进行微调，能极大提升长时序操控任务的表现。ReinforceGen 为复杂机器人任务控制提供了有效路径。

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [164] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: 本文提出了PolaRiS框架，通过对现实场景的短视频扫描，自动重建高保真的交互式仿真环境，用于更可靠地评估机器人策略。该方法有效缩小了现实与仿真之间的差距，提高了仿真评估的可信度。


<details>
  <summary>Details</summary>
Motivation: 传统机器人策略评估因现实环境中的随机性、不可复现性和耗时等问题而极具挑战，现有仿真基准与现实存在较大视觉与物理差异，难以真实反映实际表现，尤其是对于多任务、通用型的策略。

Method: 提出了一种新颖的框架PolaRiS，基于神经重建技术，将真实世界的场景视频扫描转化为可交互的高保真仿真环境，并设计了一套联合仿真数据的简单协同训练方法，缩小了仿真到现实的性能差距，实现对未见过的仿真环境的零样本评估。

Result: 大量仿真与现实环境的（评测）对比实验表明，PolaRiS在预测通用机器人策略的现实表现时，比现有仿真基准相关性更高，并且支持快速生成多样化仿真环境。

Conclusion: PolaRiS框架显著提升了仿真评测与现实表现的吻合度，简化了仿真环境创建过程，有望推动机器人领域基础模型的分布式和普惠式评估。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [165] [Sceniris: A Fast Procedural Scene Generation Framework](https://arxiv.org/abs/2512.16896)
*Jinghuan Shang,Harsh Patel,Ran Gong,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: 本文提出了Sceniris，一种高效的场景生成框架，可以极大提高3D合成场景的生成速度，助力大规模数据集创建。


<details>
  <summary>Details</summary>
Motivation: 在物理智能和生成模型的发展中，需要大量高质量的3D合成场景。现有的程序化生成方法速度较慢，成为数据集扩展的瓶颈。因此，亟需一种能够高效大规模生成场景的方法。

Method: 作者设计了Sceniris框架，相较以往方法（如Scene Synthesizer），通过批量采样与更快的碰撞检测（基于cuRobo）提升生成效率。同时，Sceniris扩展了对象空间关系，并可选提供机器人可达性的检测，避免碰撞并兼顾机器人操作需求。

Result: Sceniris生成3D场景的速度比Scene Synthesizer快至少234倍，并能生成多样化且可用于机器人操作的无碰撞场景。

Conclusion: Sceniris显著提升了3D场景生成的效率和多样性，尤其适用于大规模物理AI和机器人研究场景。该框架已开源，方便社区共同推进相关研究。

Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris

</details>
