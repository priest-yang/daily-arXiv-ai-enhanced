<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 本文针对美食图像的复杂性和多样性导致小样本学习模型表现下降的问题，提出了基于随机补丁过滤的SPFF方法，有效提升了美食小样本分类精度。


<details>
  <summary>Details</summary>
Motivation: 美食图像中的同类菜品存在丰富的外观变化，如配菜、餐具、光照与角度不同，使得模型在小样本场景下易失焦于关键特征，从而影响分类结果。为解决此类识别难题，需要提高模型对美食类别特定区域特征的关注能力。

Method: 提出了一种基于随机补丁过滤的SPFF方法。该方法对图像中的补丁嵌入进行概率性筛选，依据补丁与类别嵌入的相似度，概率性丢弃不相关补丁，仅保留与类别表征强相关的区域，并通过相似度矩阵来提升查询图像与支持图像间的类别特征对齐。

Result: 定性分析表明SPFF可聚焦于类别相关的美食图像补丁，有效过滤无关区域。同时在Food-101、VireoFood-172和UECFood-256小样本分类基准上，SPFF方法全面超越当前主流方法，取得更优性能。

Conclusion: SPFF方法增强了模型对美食图像类别特征的聚焦能力，显著提升了在高变异视觉场景下的小样本分类表现，对复杂场景下的食品识别具有重要应用价值。

Abstract: Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [2] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: 本文介绍了DINOv3，一种新型自监督学习视觉基础模型，在无需人工标注数据下，能在多种任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习虽然潜力巨大，但在大规模数据、模型泛化、长时间训练下的特征退化等方面存在瓶颈，且往往难以兼顾多样任务和实际部署需求。

Method: 提出了三大改进：1）通过数据准备、设计和优化，系统性扩大数据集与模型规模；2）创新性地引入Gram anchoring方法，解决长时间训练下特征图退化问题；3）采用后处理策略，提升模型在分辨率、规模、文本对齐等方面的灵活性。

Result: DINOv3模型无需微调，在多类型视觉任务中全面超过当前自监督及弱监督基础模型，并提供了适应不同资源和部署场景的多种模型。

Conclusion: DINOv3为视觉基础模型领域带来突破，兼具高泛化性和多任务适应性，助力自监督视觉学习迈向更广泛实际应用。

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [3] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态学习的方法，利用CLIP框架实现零样本人脸合成攻击检测，并生成相关文本描述。通过实验对不同文本提示进行了比较，在多种数据集和攻击方式下验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统正面临合成攻击（morphing attack）威胁，检测合成攻击对于身份验证的可靠性至关重要。现有方法在场景泛化和可解释性方面存在局限，因此作者希望提出一种能够泛化且具备文本可解释性的检测方式。

Method: 作者提出了一种多模态学习框架，基于CLIP实现图像与文本的对齐，支持零样本检测合成攻击，并通过设置不同短长文本提示词，将检测结果以可读的文本片段输出。方法在公开人脸数据集制作的morphing攻击数据上进行测试，并与SOTA预训练神经网络对比，在多种morphing方式和不同介质下进行评估。

Result: 实验证明，该方法不仅在合成攻击检测任务中具有较强的泛化能力，同时可以输出与检测结果高度相关的文本描述，且在不同文本提示词下都表现良好，在多种攻击方式和多介质数据上实现了有效检测。

Conclusion: 提出的多模态检测框架兼具高效合成攻击检测和文本可读性输出，在零样本场景下表现优异，为人脸识别系统安全及其结果可解释性带来了积极进展。

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [4] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

TL;DR: 本论文提出了一种基于大规模视觉-语言模型的可解释甲骨文解读方法，通过结合部首分析和象形语义理解，显著提升了甲骨文的解读性能，特别是在零样本和未解读甲骨文方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 甲骨文作为最早的成熟文字系统，因稀缺性、抽象性和多样的象形特征，解读难度极大。现有的深度学习方法未能有效结合字形与语义，特别在零样本和未解读甲骨文场景下泛化性与可解释性有限。为此，亟需开发一种能更好桥接甲骨文字形与语义关系、提升解读效果和可解释性的模型。

Method: 作者提出了一种基于大规模视觉-语言模型的甲骨文解读框架，结合了部首分析和象形语义理解。提出了渐进式训练策略，从部首识别到象形和互分析，引导模型实现由字形到语义的推理。同时设计了“部首-象形双重匹配”机制，并建设了包含4.7万余汉字和对应分析文本的“象形解读甲骨文数据集”。

Result: 在公开数据集上，作者的方法取得了当前最优的Top-10解读准确率，在零样本解读能力上也优于现有方法。

Conclusion: 该方法不仅提升了甲骨文解读的准确性和零样本泛化能力，还具备逻辑可解释的分析流程，为未解读甲骨文的考古研究提供了有价值工具，在数字人文和历史研究等领域具有重要应用前景。数据集和代码已对外开放。

Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [5] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

TL;DR: 本文提出并验证了一种基于深度学习的自动化全身DXA（TBDXA）扫描基准点定位方法，并在外部数据集和大规模实际应用中表现优异，有助于身体形态与健康生物标志物的关联研究。相关代码和模型已开源。


<details>
  <summary>Details</summary>
Motivation: 全身DXA成像广泛用于身体组成评估，但基准点的手动定位费时且主观性强，制约了大规模形态建模和与健康指标的关联分析。因此迫切需要开发自动化且高精度的定位方法，以推动全身形态特征与医学研究的结合。

Method: 作者基于1,683份人工标注的TBDXA扫描，训练并验证了深度学习模型完成基准点自动定位。随后在35,928份不同成像模式的DXA扫描上自动定位基准点，通过形态和外观建模（SAM），并在独立队列中利用KS检验分析形态特征分布与各类健康生物标志物的关系。

Result: 自动基准点定位在外部测试集上达到了99.5%的准确率。大规模自动定位和形态建模后的SAM特征分布与已知健康生物标志物相关，既验证了既有证据，也提出了身体组成、形态与虚弱、代谢、炎症及心脏代谢健康之间新颖的假设。

Conclusion: 深度学习自动基准点定位方法极大提升了TBDXA扫描在形态、健康研究中的效率和一致性，为探究身体形态与健康关系提供了有力工具。相关方法及开源资源有助于推动领域进一步发展。

Abstract: Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [6] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态注意力归一化流（MANGO）方法，通过可逆交互注意力层与创新的交互机制，有效提升多模态融合学习的性能，显著超过现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法多采用Transformer的注意力机制，但只会隐式地学习多模态特征间的相关性，导致模型难以捕捉每一模态的本质特征，限制了对复杂结构与相关性的理解。作者希望通过更显式、可解释的方法提升多模态学习的效果。

Method: 作者设计了基于Normalizing Flow的新型多模态注意力融合方法，提出可逆交互注意力（ICA）层，包含三种注意机制：模态到模态交互（MMCA）、模态间交互（IMCA）、可学习模态间交互（LICA）。这些机制能够高效捕捉复杂的多模态相关性，并能扩展到高维多模态数据。

Result: 在三项多模态任务（语义分割、图像到图像转换、电影类型分类）上，所提方法取得了比现有SOTA方法更好的性能。

Conclusion: 本文的方法能够显式、高效且可解释地融合多模态特征，提升了多模态学习模型的理解与表现能力，有望成为多模态融合的新标准。

Abstract: Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [7] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 本文探讨了将生成式AI（GenAI）合成的作物病害图像与真实图像结合用于训练计算机视觉模型，并验证了该混合策略对提升西瓜病害分类准确率的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可低成本生成大量高分辨率合成图像，有望缓解农业计算机视觉模型依赖大量实地采集图像的问题，但纯合成或混合图像训练效果尚缺乏系统评估。本文旨在探索有限真实图像与合成图像混合对模型性能的影响。

Method: 以EfficientNetV2-L为基准模型，将训练数据分为五种策略：仅真实（H0）、仅合成（H1）、1:1混合（H2）、1:10混合（H3）和H3+随机图像增强变异性（H4），并采用增强的微调与迁移学习技术进行训练，评估其在西瓜病害分类任务中的表现。

Result: 混合策略（H2、H3、H4）模型均获得了高精确率、召回率和F1分数。尤其是H3、H4处理加权F1分数由仅真实图像（H0）的0.65提升至1.00，表明少量真实图像与大量合成图像结合能显著改善模型准确率与泛化能力。

Conclusion: 单靠合成图像无法完全取代真实图像。只有混合两者，才能最大化作物病害分类模型的性能。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [8] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: 该论文提出通过高质量的合成数据生成管线，提升视觉-语言模型和主流检测器在工业泄漏等安全关键领域的表现。


<details>
  <summary>Details</summary>
Motivation: 在工业泄漏等罕见且敏感场景中，真实数据稀缺，常规微调方法难以实现，因此需要新的途径提升识别能力。

Method: 提出了基于高质量合成数据生成的可扩展框架，并利用Parameter-Efficient Fine-Tuning(PEFT)方法，对视觉-语言模型以及主流检测器（如YOLO、DETR）进行适应性优化。

Result: 实验证明，没有合成数据时视觉-语言模型泛化能力优于检测器；加入合成数据（SynSpill数据集）后，二者性能均大幅提升，并趋于一致。

Conclusion: 高保真合成数据结合轻量级适应性微调，能有效弥补安全关键领域中样本稀缺的问题，为工业视觉系统落地提供了低成本、可扩展的新路径。

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [9] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 本文针对3D高斯投影（3D Gaussian Splatting, 3DGS）方法在新视角合成中储存和传输数据的高效压缩需求，提出了一种针对性压缩方法EntropyGS，实现大幅压缩比且几乎无损视觉质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS方法分离了高斯创建和视图渲染，为了在存储和传输过程中高效管理3DGS高斯数据，急需有效的数据压缩技术。

Method: 作者首先对3DGS高斯属性进行相关性与统计分析，发现球谐AC属性服从拉普拉斯分布，其它属性可用高斯混合分布近似。随后提出了因子化和参数化的熵编码方法（EntropyGS），对每类属性分别建模分布并自适应量化，提高编解码效率。

Result: EntropyGS在基准数据集上实现了约30倍的数据压缩率，同时保持与原3DGS数据相近的渲染质量，并具备很快的编码与解码速度。

Conclusion: 随机建模和自适应量化方案能极大提升3DGS数据的存储与传输效率，为新视角合成等相关应用提供了重要支持。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [10] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

TL;DR: 该论文提出了CellSymphony，一个结合了Xenium空间转录组平台和组织形态学图像信息的多模态分析框架，实现了单细胞水平的空间、多模态数据整合和分析。


<details>
  <summary>Details</summary>
Motivation: 虽然空间转录组和组织形态学图像都能提供丰富的信息，但如何有效提取细胞级特征并将两者结合，是目前空间组学面临的核心难题。

Method: 作者基于Xenium平台，设计了CellSymphony框架，利用基础大模型提取的表达嵌入（embedding），将转录组与形态学图像在单细胞分辨率下进行融合，并构建联合表达表征用于分析。

Result: CellSymphony框架在三个癌种的实际数据中，实现了高精度的细胞类型注释，并揭示了组织中的不同微环境特征区。

Conclusion: 多模态大模型和数据融合方法对于理解复杂组织结构中细胞的生理和表型组织具有巨大潜力，将推动空间生物学和肿瘤微环境研究的发展。

Abstract: Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [11] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

TL;DR: 本论文综述了基于深度学习的裂缝检测领域的最新趋势，包括学习范式、泛化能力和数据源的变化，并提出了一个新的3D激光扫描裂缝数据集3DCrack，同时进行了基准实验。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习技术发展，裂缝检测的应用日益广泛，现有技术和综述较多，但行业正在由于新型学习方法、泛化能力提升以及数据采集多样化而发生变化。作者希望全面梳理最新进展，并弥补现有3D激光数据集和基准缺失的空白。

Method: 系统梳理当前裂缝检测的学习范式演变（如全监督、半监督、弱监督、无监督、少样本、领域自适应、基础模型微调），归纳评测泛化能力的变化（跨数据集等），分析数据采集方式的扩展。同时，发布并介绍了新3D激光裂缝数据集3DCrack，并用多种主流方法及基础模型进行了基准实验。

Result: 一方面，系统归纳了各类技术路线和研究前沿代表性成果。另一方面，3DCrack数据集的实验建立了多种深度学习方法的基准表现，验证了现有模型的有效性及局限性。

Conclusion: 本文总结并指明了未来裂缝检测在学习策略、泛化能力和数据多样性上的发展方向。新数据集和实验将推动该领域进一步研究。

Abstract: Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [12] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了一种名为MRFD（Multi-Region Fusion Decoding）的新型推理方法，实现无需重新训练即可大幅减少大视觉语言模型（LVLMs）对图像内容的幻觉现象，提升多模态任务中的事实准确性。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态任务中表现优异，但经常出现“幻觉”问题，即生成与视觉输入不符的文本，主要由于无法充分验证图像中不同区域的信息。解决这一问题对于提升模型可靠性与实际应用至关重要。

Method: 作者提出MRFD，一种无须训练的推理方法：1）利用交叉注意力发现图像显著区域；2）针对每个区域生成独立初步响应；3）基于这些响应之间的Jensen-Shannon散度（JSD）计算可靠性权重；4）用受Chain-of-Thought启发的区域提示，将所有区域预测按权重进行融合，提升一致性和事实性。

Result: 实验表明，无论在多种主流LVLM模型还是多个基准数据集上，MRFD都能显著减少幻觉现象，提高模型对事实的准确把握，而且无需对原模型进行任何更新或二次训练。

Conclusion: MRFD是一种高效且实用的提升LVLM事实性的新方法，对现有多模态模型具有良好的兼容性和推广价值，有望推动相关模型在实际领域中的落地应用。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [13] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出了一种更鲁棒于头部姿态变化的个性化注视点（PoG）估计标定方法，并构建了新的MobilePoG数据集。实验结果表明，包含更多头部姿态变化的动态校准策略能有效提升估计准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于外观的注视点估计有进步，但由于个体差异，现有方法在跨个体泛化时表现不足，准确估计仍需个性化校准。而现有校准方法对头部姿态变化非常敏感，亟需提升鲁棒性。

Method: 作者构建了MobilePoG数据集，涵盖32人的面部图像，包括在固定和变化头部姿态下注视指定点的情况。在此基础上，系统分析了校准点和头部姿态多样性对估计准确率的影响，并提出了动态校准策略：用户在移动手机时注视校准点，从而引入自然的头部姿态多样性。

Result: 实验表明，在校准过程中引入更多的头部姿态变化，能显著提升注视点估计器应对头部姿态变化的能力。所提动态校准策略优于常规校准方法，表现出更好的鲁棒性与准确度。

Conclusion: 引入头部姿态变化的动态校准，不仅提升了注视点估计准确率，还增强了系统对姿态变化的适应性。该研究为移动环境下更实用、友好的个性化注视点估计提供了新思路。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [14] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 本文提出一种结合文本-图像对比约束与结构引导机制的高保真文本驱动图像生成方法，在保证语义对准和结构一致性的同时提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐和结构一致性方面存在性能瓶颈。如何提升生成图像的语义匹配度与结构保真度，是该领域急需解决的问题。

Method: 方法上，提出集成了文本-图像跨模态对比学习模块，以及利用语义布局图/边缘素描等结构先验进行空间层次结构建模的生成器。整体框架联合优化对比损失、结构一致性损失和语义保持损失，采用多目标监督机制提升生成结果的语义一致性和可控性。并在COCO-2014数据集上进行了系统实验和灵敏度分析。

Result: 实验表明，在CLIP分数、FID和SSIM等量化指标上该方法均优于现有方法，并有效平衡了语义对齐与结构保真。在不增加计算复杂度的情况下，生成结果结构完整、语义清晰。

Conclusion: 该方法有效弥合了文本驱动图像生成中语义对齐与结构保真间的鸿沟，为联合文本-图像建模和高质量图像生成提供了可行技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [15] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

TL;DR: 本文提出了一种针对花样滑冰跳跃动作的新型时序动作分割（TAS）框架，通过结合三维（3D）姿态表示与动作流程语义，有效提升了动作识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 花样滑冰中跳跃动作的类型和时机识别对于客观评估运动表现至关重要，但这类任务通常需要专家级别的知识，因为动作极其细致且复杂。现实中，受限于有限的标注数据和现有方法未考虑到跳跃的三维性和动作过程结构，自动化识别面临挑战。

Method: 作者提出了一种新的时序动作分割框架：1）设计了面向花样滑冰、具有视角不变性的3D人体姿态表征学习方法VIFSS，该方法利用对比学习进行预训练，再通过动作分类进行微调；2）构建了首个公开的专门面向花样滑冰跳跃的3D姿态数据集FS-Jump3D；3）引入了细致的动作注释方案，将跳跃分为准备与落地阶段，帮助模型学习动作过程结构。

Result: 实验证明该方法在元素级TAS任务（既要识别跳跃类型又要判别旋转等级）上F1@50超过92%。同时，预训练的视角不变对比学习在数据稀缺时的微调表现出突出效果，展现出实际应用潜力。

Conclusion: 本研究提出的三维视角不变姿态表征及流程语义建模，显著提升了花样滑冰跳跃动作自动识别的性能，对专家知识门槛较高的体育动作分析任务具有参考和推广价值。

Abstract: Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [16] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文提出了JRDB-Reasoning数据集和自适应问答生成引擎，专注于评估人群环境下视觉推理能力，并支持不同复杂度和定制化需求的问题生成和标注。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理评测在推理复杂度定义不清、问题难度控制不足、缺乏分步结构化推理标注等方面存在明显不足，限制了AI系统评估的深入性和细致性。

Method: 作者对推理复杂度进行了形式化定义，设计了一个自适应的查询引擎，可以生成不同复杂度、可定制的问题，并附带详细的中间推理流程标注。同时，对JRDB数据集进行了扩展，加入了人与物体交互和几何关系标注，形成了新的JRDB-Reasoning基准。

Result: 该方法显著提升了视觉-语言模型在多级推理能力上的评估精度，能够细粒度地判别不同推理能力，实现对模型在真实复杂场景多层次推理的动态评测。

Conclusion: 本文提出的数据集和问答生成系统为视觉推理尤其是在动态复杂人群场景下的细粒度评估提供了新标准，有助于推动视觉-语言模型与具身智能体推理能力的发展。

Abstract: Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [17] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种基于相位一致性加权最小绝对偏差（PCWLAD）的亚像素模板匹配新方法，用于提升多模态光学图像高精度匹配效果，并在多个公开数据集上取得显著优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态光学图像（如可见光-红外等）由于光谱响应不同，存在非线性辐射和几何形变，导致图像配准精度较低。为实现更高精度的多源图像处理，急需提升跨模态配准准确率。

Method: 分为两步：1）粗配准：在无噪声滤波下计算相位一致性，并用结构相似性指数（SSIM）做模板粗匹配，保留原图结构细节；2）精配准：利用加权最小绝对偏差（WLAD）准则和结构一致性模型，对粗匹配结果进行亚像素级精细匹配， 并通过互结构滤波提升抗噪性。

Result: 在三类数据集（可见-红外Landsat、可见-近红外近景、可见-红外无人机影像）上，所提PCWLAD方法在正确匹配率（CMR）和均方根误差（RMSE）上均优于八种现有主流方法，平均达到约0.4像素的匹配精度。

Conclusion: PCWLAD亚像素模板匹配方法能显著提高多模态图像的匹配精度，抗噪能力强，在多种实际场景下均优于主流方法，有望推广用于实际多源遥感和机器视觉任务。

Abstract: High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [18] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出Interleaved Learning for Motion Synthesis (InterSyn) 框架，实现了融合个体与多人物动态的真实交互动作生成，通过交错学习方案显著提升了动作的自然性、协调性与多样性。实验结果显示，该方法在文本到动作的对齐度和多样性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的动作生成方法通常将单人动作与多人互动分开建模，难以捕捉现实场景中复杂且协调的自然交互。本文旨在通过一种统一且融合的学习框架，提升生成动作的自然感和互动质量。

Method: InterSyn框架包含两个核心模块：1）交错交互合成(INS)模块，将个人和互动行为统一建模，并支持多角色的第一人称交互；2）协同精炼(REC)模块，进一步细致优化角色间的互动动态，实现高度同步。整个方法通过交错学习策略，综合捕捉真实互动动作的细微变化和协调性。

Result: 实验表明，InterSyn生成的动作序列在文本到动作的对齐度和内容多样性上，均显著超越当前主流方法，达到了更为自然和健壮的动作合成效果，并树立了新的性能基线。

Conclusion: InterSyn为多角色、高自然度交互动作生成提供了有效解决方案，为后续动作理解与生成研究创造了新方向。作者已承诺未来开源代码，有望促进该领域的发展。

Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [19] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

TL;DR: 本论文综述了自动驾驶场景下的异常分割（OoD segmentation）技术，对现有方法进行了分类和系统性总结，并讨论了当前挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI安全问题日益受到关注，仅能检测异常而不能实现空间定位的传统异常检测方法，难以满足自动驾驶等对安全性要求高的应用。本综述旨在促进像素级异常分割技术的研究，提升AI系统的安全性和可靠性。

Method: 作者将现有的OoD分割方法分为四类：1）测试时OoD分割；2）利用异常样本监督训练；3）基于重建的方法；4）利用强大全模型的方法。并结合自动驾驶实际，总结各类方案的进展与特点。

Result: 系统梳理了OoD分割在自动驾驶中的最新研究进展、方法优缺点及实际应用难点。

Conclusion: 像素级OoD分割对自动驾驶等安全关键场景至关重要，但目前仍面临诸如样本稀缺、场景泛化等挑战，未来需在方法创新与实际适应性方面进一步突破。

Abstract: Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [20] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文综述了强化学习（RL）技术在视觉内容生成领域中的应用，包括图像、视频以及3D/4D内容的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型通常依赖于如似然性或重建损失等代理目标，这与人的感知质量、语义准确性或物理真实感常常不一致，因此需要新的优化框架来更好地满足复杂的高层次目标。

Method: 本文系统性回顾了将RL方法应用于视觉内容生成的研究进展。内容包括RL作为优化工具的发展历程，RL与生成任务（如图像、视频、3D/4D）结合的具体方法，及其在增强生成的可控性、一致性和人类偏好一致性方面的作用。

Result: 已有的研究表明，RL不仅可以作为微调机制，还能作为结构性组件用于驱动生成模型朝向复杂、高级别的目标优化，显著提升了生成内容的质量和控制力。

Conclusion: 强化学习与生成模型的结合已展现出强大潜力，但仍存在诸多挑战。未来研究需关注效率提升、目标对齐、评价机制等关键方向，推动RL在生成建模领域的进一步应用和发展。

Abstract: Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [21] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: 本文提出了一种基于相机与雷达融合的端到端自动驾驶系统SpaRC-AD，克服了现有视觉方法在恶劣天气、遮挡和精确速度估计上的局限，并在多个自动驾驶任务和基准数据集上显著优于视觉-only方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统多依赖视觉信息，容易在恶劣天气、视线遮挡及速度估计方面出现瓶颈，影响碰撞规避等安全性。因此急需融合更多传感器数据，提升场景理解和运动预测能力。

Method: 提出SpaRC-AD，通过稀疏3D特征对齐和基于多普勒的速度估计，将摄像头与雷达特征融合，提升3D场景表示能力，并优化目标锚点、地图多段线及运动建模。

Result: 在多项自动驾驶关键任务上对比视觉-only方法获得大幅提升：3D检测+4.8%mAP，多目标跟踪+8.3%AMOTA，在线建图+1.8%mAP，运动预测mADE下降4%，轨迹规划L2误差降低0.1米及TPC降低9%。验证了在nuScenes、T-nuScenes、Bench2Drive等多基准上的有效性。

Conclusion: 融合雷达的SpaRC-AD能够显著提升恶劣条件和安全敏感场景下的运动理解与长时域轨迹预测能力，是比纯视觉方法更为可靠的端到端自动驾驶方案。

Abstract: End-to-end autonomous driving systems promise stronger performance through
unified optimization of perception, motion forecasting, and planning. However,
vision-based approaches face fundamental limitations in adverse weather
conditions, partial occlusions, and precise velocity estimation - critical
challenges in safety-sensitive scenarios where accurate motion understanding
and long-horizon trajectory prediction are essential for collision avoidance.
To address these limitations, we propose SpaRC-AD, a query-based end-to-end
camera-radar fusion framework for planning-oriented autonomous driving. Through
sparse 3D feature alignment, and doppler-based velocity estimation, we achieve
strong 3D scene representations for refinement of agent anchors, map polylines
and motion modelling. Our method achieves strong improvements over the
state-of-the-art vision-only baselines across multiple autonomous driving
tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),
online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory
planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal
consistency on multiple challenging benchmarks, including real-world open-loop
nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We
show the effectiveness of radar-based fusion in safety-critical scenarios where
accurate motion understanding and long-horizon trajectory prediction are
essential for collision avoidance. The source code of all experiments is
available at https://phi-wol.github.io/sparcad/

</details>


### [22] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本论文提出了一种针对视觉-语言指令微调中基准任务数据选择的新方法，通过根据视觉概念和技能的匹配优化模型性能，在多个基准测试集上取得了更优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言微调领域任务表现受限于训练指令集的选择。由于基准任务在对视觉概念和视觉技能两者的依赖性上存在分化，针对性地选择训练数据有望提升表现，但相关研究不足。本文旨在探究不同基准任务如何从概念/技能相关性训练数据中受益，并优化指令选择策略。

Method: 作者首先分析不同基准任务的表现，发现任务更依赖相似视觉技能或视觉概念的训练数据。基于此，提出了一种自动化数据挑选方法：先提取目标基准涉及的概念或技能，判断任务主要受益于哪一类型，然后在训练数据中选取与之高度相关的指令样本用于训练。

Result: 在10余个视觉-语言基准任务上验证了方法有效性。新方法相比最佳已有基线，所有基准任务平均提升0.9%，在以技能为主的子集上提升1.5%。

Conclusion: 作者强调：在视觉-语言指令选择中需要平衡概念性知识与技能性能力的获取。提出的有针对性的数据选择方法能有效提升模型任务表现，对未来相关研究具有指导意义。

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [23] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种用于肾小球超微结构定量分析的自动化深度学习框架（Glo-DMU），能够同步高效地量化与肾病诊断相关的三项关键超微结构特征，并在实际病例中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的计算病理学研究大多聚焦于单一肾小球超微结构的识别，无法充分满足临床肾病诊断对多维超微结构定量分析的需求，因此亟需一种能够自动、精准、全面分析多种超微结构特征的方法。

Method: 本文提出Glo-DMU框架，包含：超微结构分割模型、肾小球滤过屏障区分类模型和电子致密沉积检测模型。依据肾活检诊断常规流程，框架自动量化肾小球基底膜厚度、足突融合程度和电子致密物的位置三项特征，并在包含9种病理类型、115例患者的真实场景中进行评估。

Result: 框架量化结果与实际病理报告中的形态学描述高度一致，表现出良好的准确性和可用性。

Conclusion: Glo-DMU框架实现了肾小球超微结构多指标的全自动、高精度、高通量量化分析，可为肾脏病理学诊断提供高效辅助工具。

Abstract: Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [24] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 本文针对三项文档识别与布局分析任务，应用深度学习技术提升了字符识别与分析性能，涵盖死海古卷、历史会议决议文档及现代英文手写识别，报告展示了方法和实验成果。


<details>
  <summary>Details</summary>
Motivation: 文档识别和布局分析在文献数字化、历史文档保护及信息提取中非常关键，但受限于文档年代久远、质量欠佳及字迹复杂，传统方法难以获得高精度。本文旨在利用先进的深度学习技术突破上述挑战，提高文档分析质量。

Method: 针对不同任务，本文设计了多种模型与流程：1) 对死海古卷使用数据增强，并分别利用Kraken与TrOCR模型提升字符识别能力；2) 针对16至18世纪会议文档，将DeepLabV3+语义分割与双向LSTM结合于CRNN模型，并加入置信度伪标签优化模型表现；3) 处理现代英文手写识别时，采用ResNet34编码器的CRNN网络，并通过CTC损失函数训练以建模序列信息。

Result: 实验结果显示，各项任务均取得了识别准确度的提升。死海古卷字符识别、古籍会议决议文档的版面理解及现代英文手写识别均表现出比传统方法更优的实验成果。

Conclusion: 深度学习方法在多样化文档的字符与版面分析中展现出较高的适应能力和优越性能。未来可进一步研究更大规模、多语言、复杂场景下的文档分析模型，以推动文献数字化与信息管理的发展。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [25] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: 本文提出了AtomDiffuser框架，通过对STEM时序数据中样品漂移和辐照衰减的物理退化建模，实现原子级结构演化的可解释分析。


<details>
  <summary>Details</summary>
Motivation: 时序STEM中，由于机械/热不稳定导致漂移，以及辐照损伤导致信号损失，二者交织导致现有方法很难区分退化来源，也难以对材料的原子级动态进行准确建模。

Method: 提出AtomDiffuser，利用时间条件化的模型，同时预测仿射变换（建模漂移）与空间可变的衰减图谱（建模辐照退化），做到时序帧之间退化过程的分离与可解释。方法在合成数据上训练，并验证于真实cryo-STEM数据。

Result: 在合成与实际cryo-STEM数据上，AtomDiffuser能够有效分离漂移和辐照衰减，实现高分辨率的退化推断和漂移校准，同时能够可视化和量化退化模式，与原子失稳相关。

Conclusion: AtomDiffuser为STEM时序数据中的复杂退化提供了物理可解释的分离工具，有助于深度理解辐照下材料结构动力学，为原子级结构分析与材料科学研究带来新方法。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [26] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 本文提出了一种基于心理物理学的新方法，评估多模态视觉-语言模型（VLMs）在低对比度下对空间频率的感知能力，并发现现有模型在模仿人类视觉对比敏感度方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 为了更好理解多模态视觉-语言模型感知低级视觉特征的能力，尤其是其对比敏感度，作者尝试将人类心理物理学实验的方法引入模型评估，弥补现有基准过于粗略、难以还原人类知觉特点的不足。

Method: 作者设计了一种仿照心理物理学实验的行为方法，直接用提示词引导模型判断不同空间频率和低对比度的图像可见性。具体使用带通滤波噪声图像和多样化的提示，对多种VLM架构进行系统测试。

Result: 部分VLMs在对比敏感度曲线的形状或强度上接近人类，但没有一个模型能完全匹配人类在这两方面的表现。同时，模型对提示词表达很敏感，提示方式变化会显著影响结果，提示鲁棒性存在隐忧。

Conclusion: 提出的新测试框架为探究VLM视觉灵敏度提供了新工具，也揭示了模型在对比敏感度上与人类感知间的关键差距和语义提示稳定性问题。

Abstract: Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [27] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 作者提出一种联合生成图像和其内在属性（如深度与分割图）的新方法，以提升图像生成模型的空间一致性和现实感。


<details>
  <summary>Details</summary>
Motivation: 传统大型图像生成模型虽能生成高质量图像，但受限于结构信息不足，常常导致空间不一致或失真。作者希望通过结合场景的内在属性提升生成图像的结构合理性和空间一致性。

Method: 首先从大规模图像数据集中用预训练估算器提取场景的深度、分割等内在属性；然后用自编码器将多种内在属性聚合为单一潜变量；最后，基于预训练的Latent Diffusion Models（如Stable Diffusion），实现图像与内在属性的同步去噪，通过共享互信息相互映射，提升两者的一致性，而不损失图像质量。

Result: 实验证明，该方法可以纠正空间不一致问题，使场景布局更自然，同时保持原有模型的文本匹配能力和图像保真度。

Conclusion: 将场景内在属性联合生成，有效提升了扩散模型生成图像的空间一致性和现实感，拓展了大规模图像生成模型的应用潜力。

Abstract: Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [28] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: 本论文提出了一种新的数据增强方法——NSegment+，针对语义分割中的隐性标签噪声问题，有效提升了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割研究多聚焦于应对明显的标签噪声，但现实世界数据集中还普遍存在难以察觉的隐性标签不完美（如边界模糊、标注者差异），这类噪声同样会影响模型性能，而常规的数据增强方法反而有可能进一步放大这些隐性噪声。

Method: 提出了NSegment+数据增强框架，将图像和标签的变换解耦，即仅对分割标签施加可控的弹性形变，而保持原图不变，促使模型对轻微标签不一致具有更强鲁棒性。

Result: 在Vaihingen、LoveDA、Cityscapes和PASCAL VOC等多个数据集上，NSegment+均带来了显著的mIoU提升（分别最高达+2.29、+2.38、+1.75和+3.39），且无需其他技巧即可取得效果，结合CutMix或Label Smoothing等训练方法效果可进一步提升。

Conclusion: NSegment+能够有效应对现实场景下常见但容易被忽视的隐性标签噪声问题，提高语义分割模型泛化性能，对实际应用具有重要意义。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [29] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉-语言模型和扩散模型的数据增强框架，用于提升驾驶员分心检测任务在少样本和跨域条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心检测对于交通安全至关重要，但受限于现实环境中数据标注成本高昂和训练与实际场景存在显著差异，现有模型在实际应用时泛化能力较差。

Method: 本文提出名为PQ-DAF的姿态驱动质量控制数据增强框架。其核心方法包括：1）利用渐进式条件扩散模型(PCDMs)精准捕捉驾驶员关键姿态特征，并合成多样化训练样本；2）基于CogVLM视觉-语言模型构建样本质量评估模块，通过置信度阈值筛除低质量合成样本，保证增强数据可靠性。

Result: 大量实验证明，PQ-DAF框架显著提升了在少样本条件下驾驶员分心检测模型的性能，并增强了训练模型在跨域情况下的泛化能力。

Conclusion: 提出的PQ-DAF方法能以更低成本扩大高质量训练数据，有效解决数据匮乏和域移难题，对实际驾驶安全系统具有重要应用价值。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [30] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，用于抑制T2I扩散模型中因文本特定词语强绑定导致的内容纠缠，从而更精准地控制生成图片结果中不希望出现的元素。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型虽然能够生成高质量、多样化的图片，但在处理与特定词语强相关的内容时（如“Charlie Chaplin”总会带“mustache”），难以有效按需去除不需要的元素。解决这样内容纠缠有助于提升模型可控性和生成文字与图像的一致性。

Method: 提出在扩散模型的文本嵌入空间中引入delta向量，以直接减弱不希望出现内容的影响。该delta向量可以通过零样本方法获得。进一步，提出选择性抑制delta向量（SSDV）方法，将delta向量自适应整合到cross-attention机制，对不期望内容的生成区域实现更有效的局部抑制。此外，在个性化T2I模型上通过优化delta向量，实现了更精准的内容抑制。

Result: 实验表明，本文方法在定量和定性评价上均明显优于现有其它抑制纠缠内容的方法。

Conclusion: 通过在文本嵌入空间引入和应用delta向量，结合cross-attention机制，有效抑制了T2I模型中的内容纠缠问题，显著提升了生成图像时对不希望内容的控制能力。

Abstract: Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [31] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

TL;DR: SC-Lane提出了一种基于坡度自适应和时序一致性的新型高度图估计框架，用于3D车道线检测，在多个评价指标和OpenLane数据集测试下取得了业界领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D车道线检测方法通常依赖固定坡度锚，难以应对多变的道路几何特征，且缺乏对高度估计的连贯性约束，影响真实场景下的稳定性与准确性。本文旨在解决道路坡度多样和高度估计时序一致性的挑战。

Method: SC-Lane框架引入坡度自适应特征融合模块，根据图像特征动态预测不同坡度特征的权重，将多种坡度表征整合为统一高度图。并提出高度一致性模块，确保连续帧之间的高度估计稳定准确。采用MAE、RMSE及阈值精度等标准化评价指标，在LiDAR实测高度图数据集和OpenLane数据集上进行评估。

Result: SC-Lane在OpenLane基准上显著提升高度估计与3D车道线检测的性能，在F-score等指标上超越现有方法，取得了64.3%的新SOTA成绩。

Conclusion: SC-Lane有效提升了多样化道路场景下3D车道线检测的鲁棒性和准确性，并为未来相关研究提供了更严格的比较标准和评价指标。

Abstract: In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [32] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Nano Control Diffusion Transformer（NanoControl）的新型可控文本到图像生成框架，在极低的参数和计算开销下实现了业界领先的可控生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前基于DiTs的可控文本到图像生成大多采用为UNet结构设计的ControlNet范式，这种方法带来大量参数冗余和计算负担。为此需要一种更高效的方法实现同等甚至更优的可控生成能力。

Method: NanoControl采用Flux作为主干网络，不再像传统做法那样简单复制DiT主干用于控制信号注入，而是设计了LoRA风格（低秩适应）的控制模块，直接从条件输入学习控制信号。同时，提出KV-Context Augmentation机制，使条件特定的Key-Value信息能够高效地与主干深度融合。

Result: 在多项基准实验中，NanoControl相比传统控制方式显著降低了参数量（仅增加0.024%参数）、计算复杂度（GFLOPs仅增加0.029%），同时保持甚至提升了生成质量和可控性。

Conclusion: NanoControl实现了高效、优质的可控文本到图像生成，对比现有主流方法在参数量和算力消耗上更具优势，为相关任务高性能控制提供了新思路。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [33] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出了STRIDE-QA数据集，弥补了现有视觉-语言模型（VLMs）无法在动态交通场景下进行精确时空推理的不足，极大提升了自动驾驶关键任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型多依赖静态网络图片-文本配对进行训练，难以胜任自动驾驶中对时空和物理推理要求极高的动态交通场景理解与预测，存在能力短板。

Method: 作者从东京采集了100小时多传感器驾驶数据，涵盖复杂多样场景，并生成了包含3D包围盒、分割掩码和多目标跟踪的稠密自动标注，构建了涵盖16百万问答对、28.5万帧的STRIDE-QA数据集。设计了三类新型任务，支持基于对象和自车视角的空间定位与时序预测推理。

Result: 现有VLM在预测一致性等任务上表现接近于零，而使用STRIDE-QA微调后，可在空间定位任务上达到55%成功率，未来运动预测一致性提升至28%。

Conclusion: STRIDE-QA为安全关键型自动驾驶可靠VLMs研发提供了全面的基础，将推进视觉-语言模型在动态、复杂交通环境下的实用化进程。

Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [34] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法CRISP，用于持续视频实例分割，能有效解决增量学习过程中类别、实例和任务之间的混淆，实现对新类别的学习同时保持对已学知识的记忆，以及保证视频时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有持续视频实例分割方法难以同时平衡对新物体类别的学习能力（塑性）和对旧知识的保持能力（稳定性），同时还易在实例、类别与任务级别出现混淆，并且缺乏对时序一致性的良好保证。因此亟需一种能解决以上难题的方法。

Method: 提出CRISP方法，包括：（1）实例级：引入实例相关损失，通过与先前query空间的相关性建模，增强当前任务query区分性并保证实例跟踪；（2）类别级：设计自适应残差语义提示（ARSP）学习框架，通过生成类文本驱动的可学习语义提示池与当前query建立匹配，结合对比学习的语义一致性损失，增强类别区分和保持；（3）任务级：提出高效的增量提示初始化机制，保证各任务间的query空间关联。

Result: 在YouTube-VIS-2019和YouTube-VIS-2021数据集上进行大量实验，CRISP方法在长期持续视频实例分割任务上明显优于现有方法，减少灾难性遗忘，提升分割和分类能力。

Conclusion: CRISP方法能有效解决持续视频实例分割中的实例、类别、任务混淆问题，兼顾新类别学习与旧知识保持，实现了更优的性能和时序一致性。

Abstract: Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [35] [DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations](https://arxiv.org/abs/2508.10445)
*Hang Jin,Chenqiang Gao,Junjie Guo,Fangcen Liu,Kanghui Tian,Qinyao Chang*

Main category: cs.CV

TL;DR: 本文提出了一种只需单模态标注的红外-可见光解耦目标检测新架构DOD-SA，减少了标注成本并显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有红外-可见光融合目标检测方法通常需要双模态标注，导致数据标注成本大幅提升。作者希望无需双模态标注也能充分发挥两种模态的互补优势。

Method: 提出DOD-SA框架，基于单-双模态协同教师-学生网络（CoSD-TSNet），包括单模态分支(SM-Branch)和解耦双模态分支(DMD-Branch)。利用教师模型对未标注模态生成伪标签，推动学生模型跨模态知识迁移。采用渐进式自调整训练策略（PaST），分为预训练、引导、精细化三个阶段。此外，设计了伪标签分配器（PLA）对齐和配对不同模态标签，解决模态间的失配问题。

Result: 在DroneVehicle数据集上，所提方法显著优于现有主流方法（SOTA）。

Conclusion: DOD-SA框架无需双模态标注，即能实现有效的红外-可见光目标检测，大幅降低了数据标注成本，并提升了检测性能，表明方法在实际应用中具有较强的实用性和推广价值。

Abstract: Infrared-visible object detection has shown great potential in real-world
applications, enabling robust all-day perception by leveraging the
complementary information of infrared and visible images. However, existing
methods typically require dual-modality annotations to output detection results
for both modalities during prediction, which incurs high annotation costs. To
address this challenge, we propose a novel infrared-visible Decoupled Object
Detection framework with Single-modality Annotations, called DOD-SA. The
architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative
Teacher-Student Network (CoSD-TSNet), which consists of a single-modality
branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The
teacher model generates pseudo-labels for the unlabeled modality,
simultaneously supporting the training of the student model. The collaborative
design enables cross-modality knowledge transfer from the labeled modality to
the unlabeled modality, and facilitates effective SM-to-DMD branch supervision.
To further improve the decoupling ability of the model and the pseudo-label
quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)
that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding
the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In
addition, we design a Pseudo Label Assigner (PLA) to align and pair labels
across modalities, explicitly addressing modality misalignment during training.
Extensive experiments on the DroneVehicle dataset demonstrate that our method
outperforms state-of-the-art (SOTA).

</details>


### [36] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: 本文提出了一个注释化的电气平面图数据集DELP，并基于此数据集评测了多种目标检测算法，实现了自动识别老旧电气平面图中的关键服务符号。提出的SkeySpot工具基于YOLOv8模型，能够高效实时地检测与标准化输出符号信息，便于后续自动化处理。


<details>
  <summary>Details</summary>
Motivation: 传统的建筑平面图多为纸质或扫描版本，无法被机器自动读取与分析，严重阻碍了建筑业的信息化与自动化进程。手动解读这些图纸效率低，易出错，因此亟需自动化、可扩展的解决方案对符号和结构进行自动识别与提取。

Method: 作者构建了包含45张带有2,450个标注实例、涵盖34类服务符号的DELP数据集，并利用预训练目标检测模型（如YOLOv8）对数据集进行评测，开发出SkeySpot工具，实现对电气符号的实时检测、分类与定量分析。

Result: 实验表明，在所测试的模型中，YOLOv8取得了最优表现，均值平均精度（mAP）达到了82.5%。基于此模型实现的SkeySpot工具能够实时、标准化地输出符号检测结果。

Conclusion: 该方法大幅降低了对专有CAD系统和人工标注的依赖，使中小企业更便捷地实现旧电气平面图的数字化，提升了建筑行业的信息互操作性、标准化及可持续性，有助于相关行业自动化与现代化转型。

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [37] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 本文提出了一种受生物启发的视觉网络PerceptNet，该网络能够在不同图像重建任务下学习到与人类感知最为相关的特征。


<details>
  <summary>Details</summary>
Motivation: 许多科学家认为，人类视觉感知与图像统计有关，并在早期视觉神经活动中形成高效的神经表征。研究动机在于验证通过生物启发的网络架构，是否能够自动习得与人类视觉评判一致的感知度量。

Method: 提出了一种模拟视网膜-初级视觉皮层（retina-V1）结构的神经网络PerceptNet，对其进行端到端优化，并在自动编码、去噪、去模糊和稀疏正则化等图像重建任务下进行实验，通过分析其不同层与人类图像失真感知的一致性。

Result: PerceptNet中类V1编码层，在所有任务下均显示出与人类图像失真感知评分之间的高度相关性，且未使用任何感知信息进行初始化或训练。此相关性在中等噪声、模糊和稀疏度下最优。

Conclusion: 生物启发的视觉系统能够无需人工感知监督，自主学习到与人类感知高度一致的特征和度量，暗示人类视觉系统或许正是针对这些中等级别的失真和稀疏性进行了优化调节。

Abstract: A number of scientists suggested that human visual perception may emerge from
image statistics, shaping efficient neural representations in early vision. In
this work, a bio-inspired architecture that can accommodate several known facts
in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for
different tasks related to image reconstruction: autoencoding, denoising,
deblurring, and sparsity regularization. Our results show that the encoder
stage (V1-like layer) consistently exhibits the highest correlation with human
perceptual judgments on image distortion despite not using perceptual
information in the initialization or training. This alignment exhibits an
optimum for moderate noise, blur and sparsity. These findings suggest that the
visual system may be tuned to remove those particular levels of distortion with
that level of sparsity and that biologically inspired models can learn
perceptual metrics without human supervision.

</details>


### [38] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Trajectory-aware Shifted SSMs（TS-Mamba）的新型在线视频超分辨率方法，有效提升了视频的长距离时序建模能力和处理效率。通过引入轨迹感知的令牌选择与低复杂度的Mamba架构，实现了更高精度的视频增强和显著的计算量降低。实验结果在主流数据集上优于现有六种方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频超分辨率方法普遍只利用一个前序帧进行时序对齐，导致模型对视频长距离时序关系建模能力有限。因此，需要一种能同时兼顾高效性和长时序信息整合的新型架构。

Method: 提出了TS-Mamba方法，包括以下关键步骤：（1）在视频中构建轨迹以从前序帧中选取最相似的令牌；（2）设计并采用基于Hilbert扫描和移位操作的Shifted SSMs块，加强空间连续性并补偿扫描损失；（3）提出了轨迹感知损失函数用于训练，以监督并保证令牌选择的准确性。

Result: 在三个常用在线视频超分辨率测试数据集上，TS-Mamba在多数情况下取得了最优性能，并且与六种主流现有方法相比，模型计算复杂度（MACs）降低超过22.7%。

Conclusion: TS-Mamba方法能够有效融合视频的时空信息，提升在线视频超分辨率任务的表现，并兼顾计算效率和性能。该工作推动了在线视频增强技术的发展，源代码将开源，有高实用价值。

Abstract: Online video super-resolution (VSR) is an important technique for many
real-world video processing applications, which aims to restore the current
high-resolution video frame based on temporally previous frames. Most of the
existing online VSR methods solely employ one neighboring previous frame to
achieve temporal alignment, which limits long-range temporal modeling of
videos. Recently, state space models (SSMs) have been proposed with linear
computational complexity and a global receptive field, which significantly
improve computational efficiency and performance. In this context, this paper
presents a novel online VSR method based on Trajectory-aware Shifted SSMs
(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity
Mamba to achieve efficient spatio-temporal information aggregation.
Specifically, TS-Mamba first constructs the trajectories within a video to
select the most similar tokens from the previous frames. Then, a
Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed
shifted SSMs blocks is employed to aggregate the selected tokens. The shifted
SSMs blocks are designed based on Hilbert scannings and corresponding shift
operations to compensate for scanning losses and strengthen the spatial
continuity of Mamba. Additionally, we propose a trajectory-aware loss function
to supervise the trajectory generation, ensuring the accuracy of token
selection when training our model. Extensive experiments on three widely used
VSR test datasets demonstrate that compared with six online VSR benchmark
models, our TS-Mamba achieves state-of-the-art performance in most cases and
over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will
be available at https://github.com.

</details>


### [39] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: 本文提出了一种基于多头视觉Transformer的多标签植物物种预测方法，针对训练与测试数据分布显著不同的问题，取得了PlantCLEF 2025竞赛私有榜单第三名的成绩。


<details>
  <summary>Details</summary>
Motivation: 植物多标签识别任务难点在于训练图像多为单一物种，而测试图像中包含多个物种，存在严重领域差异，需要方法具备强泛化能力。

Method: 以预训练DINOv2 ViT-B/14模型为骨干，设置多头用于物种、属、科层级预测；采用多尺度分割增强特征提取，并通过动态阈值优化、集成（bagging与Hydra结构）、图像裁剪、预测长度约束、logit阈值等多种推理与训练策略提升泛化。

Result: 在近140万张、7806种植物的训练集上测试，模型在PlantCLEF 2025挑战的私有排行榜取得了第三名的优异成绩。

Conclusion: 本方法通过多层级、多策略、多尺度设计，有效缓解了域偏移带来的困难，在大规模、多标签植物识别任务中表现突出，为相关任务提供了可复用的范式与工具。

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [40] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: 该论文提出了一种结合领域随机化和自编码器（AE）基于质量控制的半监督训练方法，实现了在极少标注（仅一个样本）的情况下训练3D脑部MRI去骨（skull-stripping）分割网络，且取得与多标注样本训练相近的效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习分割算法依赖大量带标注数据，人工标注尤其是体积医学影像（如脑MRI）成本高昂。现有领域随机化虽能利用合成影像减标注需求，但在极度稀缺标签下器官形态变化有限。半监督自训练方法可部分缓解标签匮乏，但质量控制不足。作者旨在提升极低标注分割的可行性。

Method: 方法分三步：1）自动离散化体素强度，生成用于领域随机化的标签合成训练图像，训练初始分割模型；2）以唯一带标签样本训练卷积自编码器，利用其重建误差对未标注数据预测的脑Mask进行质量评估排名；3）选择高质量伪标签再次微调模型。对比AE方法和基于一致性的质量控制方法。

Result: 通过在仅一个带标签样本下，模型在外部分布数据上获得接近使用多标注样本训练时的分割效果。AE重建误差排名伪标签，优于基于一致性排名，和实际分割准确率的相关性更强。

Conclusion: 结合领域随机化和自编码器质量控制策略，可实现极低标注分割模型训练，有效减轻医学影像注释负担，促进新结构或新成像方法下的相关研究进展。

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [41] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: 本文系统评估了针对毫米波雷达点云数据的三种主流处理方法（DBSCAN、匈牙利算法和卡尔曼滤波）及其组合在人体动作识别任务中的效果，并提出方法改进。


<details>
  <summary>Details</summary>
Motivation: 虽然毫米波雷达具备保护隐私的优势，但其点云数据稀疏且噪声大，数据处理存在挑战。现有主要方法虽常用，但缺乏系统对比和综合评估。本文旨在填补该空白。

Method: 基于MiliPoint数据集，分别对三种主流数据处理算法、任意两两组合以及三者结合进行识别准确率和计算开销评估，并提出各方法改进以提升精度。

Result: 系统实验分析了各方法及组合的优缺点、性能差异和计算代价，并验证了提出的单方法改进对准确率的提升效果。

Conclusion: 研究结果揭示了不同方法与组合在毫米波人体动作识别任务上的强项与权衡，为后续相关系统设计与应用提供了实践指导。

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [42] [STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images](https://arxiv.org/abs/2508.10473)
*Liangrui Pan,xiaoyu Li,Guang Zhu,Guanting Li,Ruixin Wang,Jiadi Luo,Yaning Yang,Liang qingchun,Shaoliang Peng*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模式注意力机制的多实例学习模型（STAMP），用于肺腺癌STAS的自动诊断，并在多个中心数据集上实现了优于临床的诊断性能。


<details>
  <summary>Details</summary>
Motivation: STAS被证明与肺腺癌患者复发及生存率下降密切相关，但其鉴别困难且易漏诊、误诊，依靠人工诊断效率低下。因此，需要研发自动化的高效工具辅助病理医师提高STAS的诊断准确性和效率。

Method: 作者从两家医院及TCGA-LUAD公开队列收集了STAS病理图片，经三位病理专家交叉标注，构建了三套数据集。提出STAMP框架，采用双分支结构从不同语义空间学习STAS表型特征，结合Transformer实例编码与多模式注意力模块，自动聚焦与STAS相关区域并抑制噪音，同时利用相似性约束防止冗余，提高模型判别力。

Result: STAMP模型在STAS-SXY、STAS-TXY及STAS-TCGA三个数据集上的AUC分别达到0.8058、0.8017和0.7928，整体超越人类病理诊断水平。

Conclusion: STAMP框架能够精准高效地实现STAS的自动化诊断，有望辅助临床病理医生，推动肺腺癌管理智能化，提高患者预后。

Abstract: Spread through air spaces (STAS) constitutes a novel invasive pattern in lung
adenocarcinoma (LUAD), associated with tumor recurrence and diminished survival
rates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive
endeavor, compounded by the propensity for oversight and misdiagnosis due to
its distinctive pathological characteristics and morphological features.
Consequently, there is a pressing clinical imperative to leverage deep learning
models for STAS diagnosis. This study initially assembled histopathological
images from STAS patients at the Second Xiangya Hospital and the Third Xiangya
Hospital of Central South University, alongside the TCGA-LUAD cohort. Three
senior pathologists conducted cross-verification annotations to construct the
STAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern
attention-aware multiple instance learning framework, named STAMP, to analyze
and diagnose the presence of STAS across multi-center histopathology images.
Specifically, the dual-branch architecture guides the model to learn
STAS-associated pathological features from distinct semantic spaces.
Transformer-based instance encoding and a multi-pattern attention aggregation
modules dynamically selects regions closely associated with STAS pathology,
suppressing irrelevant noise and enhancing the discriminative power of global
representations. Moreover, a similarity regularization constraint prevents
feature redundancy across branches, thereby improving overall diagnostic
accuracy. Extensive experiments demonstrated that STAMP achieved competitive
diagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,
0.8017, and 0.7928, respectively, surpassing the clinical level.

</details>


### [43] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一种无需调优和反演的新型扩散模型图像编辑方法，与现有方法相比能更好地在文本指导下编辑图片，并实现对原图语义的高效保留和快速目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像编辑方法在根据文本指令编辑图片时，往往会过度对齐目标提示，导致原图像语义缺失，且编辑路径冗长效率低。作者认为主要由于现有方法依赖于图像反演噪声作为编辑锚点，这样难以有效保留原图语义。

Method: 提出TweezeEdit，一种调优和反演无关的图像编辑框架，通过对整个去噪路径进行正则化，而不是仅依赖反演锚点，在去噪过程中持续引入目标提示信息。方法基于一致性模型，利用梯度驱动的正则化手段，在直接路径上高效注入目标语义。

Result: 大量实验表明，TweezeEdit在语义保留和目标对齐方面明显优于现有方法，且编辑只需12步，每次仅需1.6秒，极大提升编辑效率。

Conclusion: TweezeEdit不仅在语义一致性和目标响应方面性能突出，还实现了实时图像编辑的潜力，适用于需要高效率和高质量图像编辑的实际应用场景。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through
text guidance. However, existing methods often over-align with target prompts
while inadequately preserving source image semantics. Such approaches generate
target images explicitly or implicitly from the inversion noise of the source
images, termed the inversion anchors. We identify this strategy as suboptimal
for semantic preservation and inefficient due to elongated editing paths. We
propose TweezeEdit, a tuning- and inversion-free framework for consistent and
efficient image editing. Our method addresses these limitations by regularizing
the entire denoising path rather than relying solely on the inversion anchors,
ensuring source semantic retention and shortening editing paths. Guided by
gradient-driven regularization, we efficiently inject target prompt semantics
along a direct path using a consistency model. Extensive experiments
demonstrate TweezeEdit's superior performance in semantic preservation and
target alignment, outperforming existing methods. Remarkably, it requires only
12 steps (1.6 seconds per edit), underscoring its potential for real-time
applications.

</details>


### [44] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 本文提出了一种结合MSAA（多重采样抗锯齿）和双重几何约束的优化框架，在保持实时渲染效率的同时，有效提升了3D场景中新视图合成的细节还原能力，特别是在高频纹理和尖锐边界处。


<details>
  <summary>Details</summary>
Motivation: 在3D高斯点云生成的新视图合成中，当前方法受制于几何约束不足，导致细节模糊、边缘不清晰。为提升复杂细节和高频区域的还原质量，需要更有效的场景优化约束。

Method: 引入了自适应四重采样与混合的MSAA渲染机制，结合两种几何约束：（1）自适应加权，通过动态梯度分析优先优化重建不足区域；（2）边界梯度差分约束，对物体边界进行几何正则化。同时，实现了资源向重要区域的动态分配，保证全局一致性。

Result: 在多个数据集和基准测试中，本文方法在细节保存、尤其是高频纹理和边界重建方面取得了领先，于SSIM和LPIPS等指标上显著超过主流方法。

Conclusion: 该方法在提升复杂场景细节重建能力的同时兼顾了实时渲染效率，定量和感知实验均验证了其优越性能，为3D高斯点云新视图合成带来新的改进路径。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [45] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种用于扩充螺栓缺陷检测数据集的分割驱动螺栓缺陷编辑方法（SBDE），显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺陷图片数据稀缺且数据分布不均，致使螺栓缺陷检测性能受限。为解决这一问题，亟需高质量的数据增强方法。

Method: 提出了三步法：(1) 建立螺栓属性分割模型Bolt-SAM, 结合CLAHE-FFT Adapter与Multipart-Aware Mask Decoder，实现复杂属性高质量分割；(2) 设计并集成掩膜优化模块MOD和LaMa图像修复模型，构建出MOD-LaMa，实现对正常螺栓的缺陷属性编辑；(3) 编辑恢复增强（ERA）策略，将编辑后的缺陷螺栓恢复到原始场景，扩充检测数据集。

Result: 构建了多个数据集并做了大量对比实验，SBDE生成的缺陷图片优于主流图像编辑模型，且显著提升了螺栓缺陷检测性能。

Conclusion: SBDE方法极大改善了螺栓缺陷检测的训练样本多样性与检测表现，验证了方法有效性与实用潜力。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines.
However, the scarcity of defect images and imbalanced data distributions
significantly limit detection performance. To address this problem, we propose
a segmentationdriven bolt defect editing method (SBDE) to augment the dataset.
First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which
enhances the segmentation of complex bolt attributes through the CLAHE-FFT
Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality
masks for subsequent editing tasks. Second, a mask optimization module (MOD) is
designed and integrated with the image inpainting model (LaMa) to construct the
bolt defect attribute editing model (MOD-LaMa), which converts normal bolts
into defective ones through attribute editing. Finally, an editing recovery
augmentation (ERA) strategy is proposed to recover and put the edited defect
bolts back into the original inspection scenes and expand the defect detection
dataset. We constructed multiple bolt datasets and conducted extensive
experiments. Experimental results demonstrate that the bolt defect images
generated by SBDE significantly outperform state-of-the-art image editing
models, and effectively improve the performance of bolt defect detection, which
fully verifies the effectiveness and application potential of the proposed
method. The code of the project is available at
https://github.com/Jay-xyj/SBDE.

</details>


### [46] [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](https://arxiv.org/abs/2508.10522)
*Quang Nguyen,Nhat Le,Baoru Huang,Minh Nhat Vu,Chengcheng Tang,Van Nguyen,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种结合第一视角视频和音乐的舞蹈动作预测方法，并发布了大规模EgoAIST++数据集。提出的模型在理论和实验上均优于现有方法，并具备较强的实际泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常仅依赖视频或音乐进行人体舞蹈动作估计，极少联合利用两者，且第一视角视频常因视野遮挡使全身姿态估计更具挑战。融合多模态输入可以显著提升动作生成的准确性与自然性。

Method: 1. 构建了包含36小时舞蹈数据的EgoAIST++数据集，集成第一视角视频和音乐。
2. 基于扩散模型与Mamba时序建模优势，设计了EgoMusic Motion Network。其核心Skeleton Mamba结构专门针对人体骨架建模，增强序列表达。
3. 方法能结合视觉与音频信息生成与输入高度一致的舞蹈动作。

Result: 实验证明，该方法在多项主流任务和真实场景下均明显优于现有技术，展示了强大的性能和泛化能力。

Conclusion: 提出的方法能有效、准确地从第一视角视频与音乐联合估计出舞蹈动作，推动了多模态人体动作生成领域的研究进展。

Abstract: Estimating human dance motion is a challenging task with various industrial
applications. Recently, many efforts have focused on predicting human dance
motion using either egocentric video or music as input. However, the task of
jointly estimating human motion from both egocentric video and music remains
largely unexplored. In this paper, we aim to develop a new method that predicts
human dance motion from both egocentric video and music. In practice, the
egocentric view often obscures much of the body, making accurate full-pose
estimation challenging. Additionally, incorporating music requires the
generated head and body movements to align well with both visual and musical
inputs. We first introduce EgoAIST++, a new large-scale dataset that combines
both egocentric views and music with more than 36 hours of dancing motion.
Drawing on the success of diffusion models and Mamba on modeling sequences, we
develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly
captures the skeleton structure of the human body. We illustrate that our
approach is theoretically supportive. Intensive experiments show that our
method clearly outperforms state-of-the-art approaches and generalizes
effectively to real-world data.

</details>


### [47] [Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies](https://arxiv.org/abs/2508.10523)
*Ayushman Sarkar,Mohd Yamani Idna Idris,Zhenyu Yu*

Main category: cs.CV

TL;DR: 本文对视觉推理领域的五种主要类型与方法进行了系统梳理和对比，并对评测协议和未来挑战进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有关于视觉推理的综述多为分散讨论，未能统一归纳不同推理类型及其方法和评估。此文旨在填补这一综合性分析和比较的空白。

Method: 将视觉推理分为关系、符号、时序、因果和常识五大类，系统分析了图结构模型、记忆网络、注意力机制、神经符号系统等实现架构，并评估了不同方法的功能正确性、结构一致性和因果有效性。

Result: 全面总结了现有视觉推理方法和评测协议的优势与局限，如泛化性弱、可重现性差、解释能力有限。指出了数据集不完善、弱监督、符号与神经方法整合等主要挑战。

Conclusion: 未来视觉推理需进一步融合感知与推理能力，提升系统的透明度、可信度及跨领域适应性，对于自动驾驶、医疗诊断等关键领域意义重大。

Abstract: Visual reasoning is critical for a wide range of computer vision tasks that
go beyond surface-level object detection and classification. Despite notable
advances in relational, symbolic, temporal, causal, and commonsense reasoning,
existing surveys often address these directions in isolation, lacking a unified
analysis and comparison across reasoning types, methodologies, and evaluation
protocols. This survey aims to address this gap by categorizing visual
reasoning into five major types (relational, symbolic, temporal, causal, and
commonsense) and systematically examining their implementation through
architectures such as graph-based models, memory networks, attention
mechanisms, and neuro-symbolic systems. We review evaluation protocols designed
to assess functional correctness, structural consistency, and causal validity,
and critically analyze their limitations in terms of generalizability,
reproducibility, and explanatory power. Beyond evaluation, we identify key open
challenges in visual reasoning, including scalability to complex scenes, deeper
integration of symbolic and neural paradigms, the lack of comprehensive
benchmark datasets, and reasoning under weak supervision. Finally, we outline a
forward-looking research agenda for next-generation vision systems, emphasizing
that bridging perception and reasoning is essential for building transparent,
trustworthy, and cross-domain adaptive AI systems, particularly in critical
domains such as autonomous driving and medical diagnostics.

</details>


### [48] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: 本文提出了Med-GLIP框架，并构建了大规模医疗图像定位数据集Med-GLIP-5M，通过引入多模态、多粒度的区域标注，提高医学图像短语定位任务的性能，并在多项下游任务中带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像定位任务面临模态覆盖有限、标注粒度粗糙及缺乏统一通用方案等挑战，限制了其在智能诊断、视觉问答和报告生成等下游应用的效果。

Method: 作者构建了包含530万以上区域级标注、涵盖七种成像模态的Med-GLIP-5M大数据集，实现了从器官到病灶的多层次分层标注。在此基础上，提出了无需显式专家模块、能从数据中隐式学习层次语义的Med-GLIP定位框架。

Result: Med-GLIP在多个医疗图像定位基准测试中均明显优于现有先进方法。同时，其空间输出可显著提升医疗视觉问答和报告生成等下游任务的性能。

Conclusion: Med-GLIP和Med-GLIP-5M为医学图像短语定位提供了统一、通用、高精度的解决方案，推动了该领域在多个智能医疗任务中的发展。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [49] [GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images](https://arxiv.org/abs/2508.10542)
*Mengyu Ren,Yutong Li,Hua Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了一种新型的基于图增强的上下文与区域感知网络（GCRPNet），用于解决遥感图像中显著性目标检测的难题，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的显著性目标检测面临目标尺度变化大和前后景对比度低等挑战，现有基于ViT和CNN的方法难以有效整合全局与局部异构特征，因而性能受限。为解决这一难题，亟需新方法提升特征整合和目标区分能力。

Method: 提出GCRPNet架构，基于Mamba结构；采用视觉状态空间（VSS）编码器提取多尺度特征；设计差异-相似性引导的分层图注意力模块（DS-HGAM）加强跨层特征交互与结构感知；提出LEVSS解码块，包括自适应扫描策略和多粒度协同注意力增强模块（MCAEM），提升局部建模能力和区域信息捕捉能力。

Result: 模型在公开遥感图像显著性检测数据集上进行了广泛实验，结果显示GCRPNet的性能优于现有最新方法，达到了当前最先进水平。

Conclusion: GCRPNet能够有效整合全局与局部特征，显著提升了遥感图像中显著性目标检测的准确性和区分能力，优越性和有效性经实验充分验证。

Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces
numerous challenges, including significant variations in target scales and low
contrast between targets and the background. Existing methods based on vision
transformers (ViTs) and convolutional neural networks (CNNs) architectures aim
to leverage both global and local features, but the difficulty in effectively
integrating these heterogeneous features limits their overall performance. To
overcome these limitations, we propose a graph-enhanced contextual and regional
perception network (GCRPNet), which builds upon the Mamba architecture to
simultaneously capture long-range dependencies and enhance regional feature
representation. Specifically, we employ the visual state space (VSS) encoder to
extract multi-scale features. To further achieve deep guidance and enhancement
of these features, we first design a difference-similarity guided hierarchical
graph attention module (DS-HGAM). This module strengthens cross-layer
interaction capabilities between features of different scales while enhancing
the model's structural perception,allowing it to distinguish between foreground
and background more effectively. Then, we design the LEVSS block as the decoder
of GCRPNet. This module integrates our proposed adaptive scanning strategy and
multi-granularity collaborative attention enhancement module (MCAEM). It
performs adaptive patch scanning on feature maps processed via multi-scale
convolutions, thereby capturing rich local region information and enhancing
Mamba's local modeling capability. Extensive experimental results demonstrate
that the proposed model achieves state-of-the-art performance, validating its
effectiveness and superiority.

</details>


### [50] [PSScreen: Partially Supervised Multiple Retinal Disease Screening](https://arxiv.org/abs/2508.10549)
*Boyi Zheng,Qing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PSScreen的新型部分监督多视网膜疾病筛查模型，有效利用多部分标注的数据集，提升不同视网膜疾病的检测准确性，并在域适应方面取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 多中心视网膜疾病数据集标注不完整，且存在明显域差异，导致模型难以泛化到新数据。现有方法对全标注数据依赖大，难以广泛推广。

Method: PSScreen模型包含两个流：一条学习确定性特征，另一条通过引入不确定性学习概率特征。通过文本指导将特征解耦为疾病相关特征，并用特征蒸馏对齐两种特征流，增强域泛化能力。采用伪标签一致性缓解缺失标签问题，自蒸馏方法提升概率流的检测性能。

Result: PSScreen在六种视网膜疾病及正常状态的检测上显著提升性能，在域内和域外数据上均达到了当前最优水平。

Conclusion: PSScreen方法在处理部分标注和多源数据域泛化难题方面表现优越，为视网膜疾病多任务筛查提供了高效可靠的新思路。

Abstract: Leveraging multiple partially labeled datasets to train a model for multiple
retinal disease screening reduces the reliance on fully annotated datasets, but
remains challenging due to significant domain shifts across training datasets
from various medical sites, and the label absent issue for partial classes. To
solve these challenges, we propose PSScreen, a novel Partially Supervised
multiple retinal disease Screening model. Our PSScreen consists of two streams
and one learns deterministic features and the other learns probabilistic
features via uncertainty injection. Then, we leverage the textual guidance to
decouple two types of features into disease-wise features and align them via
feature distillation to boost the domain generalization ability. Meanwhile, we
employ pseudo label consistency between two streams to address the label absent
issue and introduce a self-distillation to transfer task-relevant semantics
about known classes from the deterministic to the probabilistic stream to
further enhance the detection performances. Experiments show that our PSScreen
significantly enhances the detection performances on six retinal diseases and
the normal state averagely and achieves state-of-the-art results on both
in-domain and out-of-domain datasets. Codes are available at
https://github.com/boyiZheng99/PSScreen.

</details>


### [51] [AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications](https://arxiv.org/abs/2508.10554)
*Marc J. Fischer,Jeffrey Potts,Gabriel Urreola,Dax Jones,Paolo Palmisciano,E. Bradley Strong,Branden Cord,Andrew D. Hernandez,Julia D. Sharma,E. Brandon Strong*

Main category: cs.CV

TL;DR: 本研究提出一种基于增强现实（AR）的外科导航系统新方法，利用HoloLens 2和实时工具追踪技术，提高了置管操作的精度和用户体验。实验结果显示，该方法在准确性和用户满意度方面优于传统静态可视化导航。


<details>
  <summary>Details</summary>
Motivation: 传统外科导航系统在人机交互和视觉感知上存在局限，尤其是在深度感知和遮挡处理方面，影响了手术精度。亟需开发新的AR导航技术以提升手术的准确性和实用性。

Method: 作者开发了一套利用HoloLens 2内部传感器和红外工具追踪的AR导航系统，应用于外部脑室引流管（EVD）插入手术的模拟实验。系统包括新颖的表面跟踪注册方法、两种AR导航模式（静态叠加、实时工具反馈）以及插管步骤。通过CT分析评估插管精度，并采用用户问卷调查主观体验。

Result: 采用实时工具追踪的AR导航在所有准确性指标（插入精度、目标偏差、角度误差及深度精度）上均优于静态叠加模式。用户主观评价亦更为偏好实时工具追踪模式。

Conclusion: 基于实时工具追踪的AR导航方法能够显著提升手术操作精度和用户体验，有望成为新一代手术导航系统的核心技术。

Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next
generation of intraoperative surgical guidance, promising to overcome
limitations of traditional navigation systems. However, known issues with AR
depth perception due to vergence-accommodation conflict and occlusion handling
limitations of the currently commercially available display technology present
acute challenges in surgical settings where precision is paramount. This study
presents a novel methodology for utilizing AR guidance to register anatomical
targets and provide real-time instrument navigation using placement of
simulated external ventricular drain catheters on a phantom model as the
clinical scenario. The system registers target positions to the patient through
a novel surface tracing method and uses real-time infrared tool tracking to aid
in catheter placement, relying only on the onboard sensors of the Microsoft
HoloLens 2. A group of intended users performed the procedure of simulated
insertions under two AR guidance conditions: static in-situ visualization,
where planned trajectories are overlaid directly onto the patient anatomy, and
real-time tool-tracking guidance, where live feedback of the catheter's pose is
provided relative to the plan. Following the insertion tests, computed
tomography scans of the phantom models were acquired, allowing for evaluation
of insertion accuracy, target deviation, angular error, and depth precision.
System Usability Scale surveys assessed user experience and cognitive workload.
Tool-tracking guidance improved performance metrics across all accuracy
measures and was preferred by users in subjective evaluations. A free copy of
this paper and all supplemental materials are available at
https://bit.ly/45l89Hq.

</details>


### [52] [Retrieval-Augmented Prompt for OOD Detection](https://arxiv.org/abs/2508.10556)
*Ruisong Han,Zongbo Han,Jiahao Zhang,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的OOD检测方法RAP，通过外部知识增强提示，对OOD检测表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法依赖有限的辅助离群样本或ID数据，难以匹配真实测试中的OOD样本，导致语义监督不足，性能受限。

Method: 提出Retrieval-Augmented Prompt（RAP）方法，在训练阶段检索外部文本知识获取描述性词汇强化模型提示；在测试阶段，RAP根据遇到的OOD样本动态调整提示，实现模型的实时自适应。该方法基于预训练视觉-语言模型完成。

Result: 在大规模OOD基准上，RAP性能居于领先地位。例如，在ImageNet-1k数据集1-shot OOD检测中，RAP将平均FPR95降低了7.05%，AUROC提升了1.71%。全面消融实验也验证了各组件和动机的有效性。

Conclusion: RAP方法通过外部知识增强视觉-语言模型的语义能力，显著提升了OOD检测的准确性及适应性，展现出强大的实用潜力。

Abstract: Out-of-Distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in-the-wild, enabling accurate identification of test
samples that differ from the training data distribution. Existing methods rely
on auxiliary outlier samples or in-distribution (ID) data to generate outlier
information for training, but due to limited outliers and their mismatch with
real test OOD samples, they often fail to provide sufficient semantic
supervision, leading to suboptimal performance. To address this, we propose a
novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP
augments a pre-trained vision-language model's prompts by retrieving external
knowledge, offering enhanced semantic supervision for OOD detection. During
training, RAP retrieves descriptive words for outliers based on joint
similarity with external textual knowledge and uses them to augment the model's
OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time
based on the encountered OOD samples, enabling the model to rapidly adapt to
the test environment. Our extensive experiments demonstrate that RAP achieves
state-of-the-art performance on large-scale OOD detection benchmarks. For
example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the
average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous
methods. Additionally, comprehensive ablation studies validate the
effectiveness of each module and the underlying motivations of our approach.

</details>


### [53] [PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks](https://arxiv.org/abs/2508.10557)
*Xinhao Wang,Zhiwei Lin,Zhongyu Xia,Yongtao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PTQAT的新型混合量化算法，可以高效地部署3D感知网络，在兼顾模型精度和效率的前提下，显著减小了QAT带来的内存和训练时长负担。实验验证其在多个3D任务和架构上的普适性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有两类主流模型量化方法：PTQ部署快但精度损失大，QAT保精度但需大量训练资源。需要一种兼顾效率与精度的量化方法，特别适用于3D感知网络的高效部署。

Method: 作者提出PTQAT，将PTQ和QAT结合。具体做法是只对关键层进行QAT微调，剩下层仅用PTQ。创新点在于优先微调那些量化前后输出差异较小的层，通过更有效的误差补偿，提升整体量化效果。该方法支持多种量化位宽和主流3D网络结构。

Result: 在nuScenes等3D感知任务（目标检测、语义分割、占用预测）上，PTQAT较单独QAT方法获得了更高指标提升，例如目标检测任务提升0.2%-0.9% NDS和0.3%-1.0% mAP，语义分割和占用预测提升0.3%-2.0% mIoU，且参数微调量减少近50%。

Conclusion: PTQAT兼顾PTQ和QAT优点，实现了3D感知网络的高效高精度量化，具有通用性和实用性，优于现有QAT-only方案。

Abstract: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)
represent two mainstream model quantization approaches. However, PTQ often
leads to unacceptable performance degradation in quantized models, while QAT
imposes substantial GPU memory requirements and extended training time due to
weight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid
quantization algorithm for the efficient deployment of 3D perception networks.
To address the speed accuracy trade-off between PTQ and QAT, our method selects
critical layers for QAT fine-tuning and performs PTQ on the remaining layers.
Contrary to intuition, fine-tuning the layers with smaller output discrepancies
before and after quantization, rather than those with larger discrepancies,
actually leads to greater improvements in the model's quantization accuracy.
This means we better compensate for quantization errors during their
propagation, rather than addressing them at the point where they occur. The
proposed PTQAT achieves similar performance to QAT with more efficiency by
freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal
quantization method that supports various quantization bit widths (4 bits) as
well as different model architectures, including CNNs and Transformers. The
experimental results on nuScenes across diverse 3D perception tasks, including
object detection, semantic segmentation, and occupancy prediction, show that
our method consistently outperforms QAT-only baselines. Notably, it achieves
0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains
in semantic segmentation and occupancy prediction while fine-tuning fewer
weights.

</details>


### [54] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: 本文提出了HM-Talker方法，利用显式与隐式混合运动特征，实现高质量、同步度高的音频驱动说话人头像视频生成，有效解决现有方法产生运动模糊和嘴唇抖动的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动说话人头像生成方法在口型与音频对齐和视频质量方面存在局限，主要由于缺乏面部运动的显式解剖学先验，导致生成视频常有运动模糊和嘴唇抖动。

Method: 提出HM-Talker框架，结合显式（基于Action Units的面部肌肉动作）和隐式音频-面部运动特征。特设跨模态解耦模块（CMDM），直接从音频中预测AUs以提升口型音素对应，并通过混合运动建模模块（HMMM）融合不同身份间的特征，提高模型泛化能力和鲁棒性。

Result: 通过大量实验，HM-Talker在视觉质量和音频-口型同步精度上，相较现有领先方法表现更优。

Conclusion: HM-Talker框架有效利用混合运动特征，显著提升了音频驱动说话人头像生成的效果，并具有更好的通用性和个性化能力，对人机交互头像合成技术有积极推动作用。

Abstract: Audio-driven talking head video generation enhances user engagement in
human-computer interaction. However, current methods frequently produce videos
with motion blur and lip jitter, primarily due to their reliance on implicit
modeling of audio-facial motion correlations--an approach lacking explicit
articulatory priors (i.e., anatomical guidance for speech-related facial
movements). To overcome this limitation, we propose HM-Talker, a novel
framework for generating high-fidelity, temporally coherent talking heads.
HM-Talker leverages a hybrid motion representation combining both implicit and
explicit motion cues. Explicit cues use Action Units (AUs), anatomically
defined facial muscle movements, alongside implicit features to minimize
phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement
Module (CMDM) extracts complementary implicit/explicit motion features while
predicting AUs directly from audio input aligned to visual cues. To mitigate
identity-dependent biases in explicit features and enhance cross-subject
generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This
module dynamically merges randomly paired implicit/explicit features, enforcing
identity-agnostic learning. Together, these components enable robust lip
synchronization across diverse identities, advancing personalized talking head
synthesis. Extensive experiments demonstrate HM-Talker's superiority over
state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [55] [Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection](https://arxiv.org/abs/2508.10568)
*Humza Naveed,Xina Zeng,Mitch Bryson,Nagita Mehrseresht*

Main category: cs.CV

TL;DR: 本文通过微调SAM基础模型并引入时空特征增强及多尺度解码融合，实现了遥感变化检测任务下的精确目标分割，尤其在类别失衡场景中表现突出，并在多个公开数据集上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉基础模型虽然泛化能力强，但直接应用于遥感变化检测存在检测精度受限和类别失衡带来的问题，亟需针对性优化以提升多尺度、多场景下的检测效果。

Method: 1. 对Segment Anything Model (SAM) 编码器进行专项微调，使其适配遥感变化检测任务；2. 引入时空特征增强（STFE）及多尺度解码融合（MSDF）机制，加强模型对多时间和多尺度变化的敏感性；3. 提出创新性交叉熵掩码（CEM）损失函数，以有效解决变化检测数据集中类别高度失衡的问题。

Result: 该方法在Levir-CD、WHU-CD、CLCD和S2Looking四个遥感变化检测数据集上均优于目前最优方法，对S2Looking这样大型复杂数据集，F1分数提升达2.5%。

Conclusion: 通过基础模型微调及损失函数创新，本文提出的方法显著提高了遥感变化检测的准确性和泛化能力，证明了其在多尺度、多场景的变化检测应用中的优越性。

Abstract: Foundational models have achieved significant success in diverse domains of
computer vision. They learn general representations that are easily
transferable to tasks not seen during training. One such foundational model is
Segment anything model (SAM), which can accurately segment objects in images.
We propose adapting the SAM encoder via fine-tuning for remote sensing change
detection (RSCD) along with spatial-temporal feature enhancement (STFE) and
multi-scale decoder fusion (MSDF) to detect changes robustly at multiple
scales. Additionally, we propose a novel cross-entropy masking (CEM) loss to
handle high class imbalance in change detection datasets. Our method
outperforms state-of-the-art (SOTA) methods on four change detection datasets,
Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on
a large complex S2Looking dataset. The code is available at:
https://github.com/humza909/SAM-CEM-CD

</details>


### [56] [Towards Agentic AI for Multimodal-Guided Video Object Segmentation](https://arxiv.org/abs/2508.10572)
*Tuyen Tran,Thao Minh Le,Truyen Tran*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态智能体（Multi-Modal Agent）的引用式视频目标分割新方法，利用大语言模型的推理能力生成针对每个输入的动态工作流，较以往静态流程显著提升了灵活性和表现。


<details>
  <summary>Details</summary>
Motivation: 以往分割方案依赖专门模型，计算复杂且需人工标注，固定流程难以适应任务的动态需求。基础大模型的兴起为摆脱训练、提升灵活性提供机会。

Method: 方法利用大语言模型（LLM）生成针对具体输入的动态多模态操作流程，通过与为低层任务定制的多种工具交互，不断迭代识别与分割多模态提示下的目标对象。

Result: 在RVOS和Ref-AVS两个多模态条件的视频目标分割任务上，本文方法取得了比现有静态流程更好的分割效果。

Conclusion: 采用基于智能体的多模态分割流程，能自适应应对不同输入条件，提升了引用式视频目标分割的灵活性和准确率，显示出大语言模型与多模态工具协同的新潜力。

Abstract: Referring-based Video Object Segmentation is a multimodal problem that
requires producing fine-grained segmentation results guided by external cues.
Traditional approaches to this task typically involve training specialized
models, which come with high computational complexity and manual annotation
effort. Recent advances in vision-language foundation models open a promising
direction toward training-free approaches. Several studies have explored
leveraging these general-purpose models for fine-grained segmentation,
achieving performance comparable to that of fully supervised, task-specific
models. However, existing methods rely on fixed pipelines that lack the
flexibility needed to adapt to the dynamic nature of the task. To address this
limitation, we propose Multi-Modal Agent, a novel agentic system designed to
solve this task in a more flexible and adaptive manner. Specifically, our
method leverages the reasoning capabilities of large language models (LLMs) to
generate dynamic workflows tailored to each input. This adaptive procedure
iteratively interacts with a set of specialized tools designed for low-level
tasks across different modalities to identify the target object described by
the multimodal cues. Our agentic approach demonstrates clear improvements over
prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.

</details>


### [57] [HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576)
*Zheng Qin,Ruobing Zheng,Yabing Wang,Tianqi Li,Yi Yuan,Jingdong Chen,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了HumanSense基准，用于评估多模态大模型（MLLMs）在人类中心场景下的理解与交互能力，并通过多阶段、模态递进强化学习提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在人类中心的复杂意图理解和情感回应方面缺乏细粒度评测框架，限制了其进一步发展。本研究旨在弥补这一评测空白。

Method: 作者提出了HumanSense基准，专注于多模态上下文的深入理解与合理反馈评估，并结合多模态输入（视觉、音频、文本）与多阶段、模态递进强化学习方法提升模型推理与交互能力。同时设计了提示词优化非推理模型表现。

Result: 实验显示主流MLLMs在高级交互任务上仍有提升空间，补充音频和文本可显著提升表现，Omni-modal模型优势明显；经过强化学习后推理能力大幅提升，优化提示词后非推理模型表现也改善。

Conclusion: 多模态和推理能力对于提升人机交互中的理解与反馈至关重要，细粒度评测和多模态优化是MLLMs向人性化方向发展的关键路径。

Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for
achieving truly human-like interactions, progress is hindered by the lack of
fine-grained evaluation frameworks for human-centered scenarios, encompassing
both the understanding of complex human intentions and the provision of
empathetic, context-aware responses. Here we introduce HumanSense, a
comprehensive benchmark designed to evaluate the human-centered perception and
interaction capabilities of MLLMs, with a particular focus on deep
understanding of extended multimodal contexts and the formulation of rational
feedback. Our evaluation reveals that leading MLLMs still have considerable
room for improvement, particularly for advanced interaction-oriented tasks.
Supplementing visual input with audio and text information yields substantial
improvements, and Omni-modal models show advantages on these tasks.
Furthermore, we argue that appropriate feedback stems from a contextual
analysis of the interlocutor's needs and emotions, with reasoning ability
serving as the key to unlocking it. Accordingly, we employ a multi-stage,
modality-progressive reinforcement learning to enhance the reasoning abilities
of an Omni model, achieving substantial gains on evaluation results.
Additionally, we observe that successful reasoning processes exhibit highly
consistent thought patterns. By designing corresponding prompts, we also
enhance the performance of non-reasoning models in a training-free manner.
Project page:
\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/

</details>


### [58] [EvTurb: Event Camera Guided Turbulence Removal](https://arxiv.org/abs/2508.10582)
*Yixing Liu,Minggui Teng,Yifei Xia,Peiqi Duan,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种利用事件相机的算法EvTurb，有效去除大气湍流对图像造成的模糊和畸变，并发布了相关真实数据集TurbEvent，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受大气湍流影响的图像存在严重模糊和几何畸变，影响后续视觉任务，现有方法难以应对其高度复杂的畸变成分，迫切需要新方法提升恢复效果。

Method: 提出了EvTurb框架，结合事件相机的高速事件流。方法分两步：第一步用事件积分降低模糊，第二步用从事件流导出的方差图去除倾斜畸变。此外，采集并公开了首个真实捕获的大气湍流事件相机数据集TurbEvent。

Result: 实验结果表明，EvTurb在恢复质量上超越了现有最先进的方法，并保持了较高的计算效率。

Conclusion: EvTurb成功实现了模糊与倾斜畸变的解耦和校正，为湍流受损图像恢复提供了高效可行的新方案，同时推动了事件相机在复杂成像任务中的研究。

Abstract: Atmospheric turbulence degrades image quality by introducing blur and
geometric tilt distortions, posing significant challenges to downstream
computer vision tasks. Existing single-image and multi-frame methods struggle
with the highly ill-posed nature of this problem due to the compositional
complexity of turbulence-induced distortions. To address this, we propose
EvTurb, an event guided turbulence removal framework that leverages high-speed
event streams to decouple blur and tilt effects. EvTurb decouples blur and tilt
effects by modeling event-based turbulence formation, specifically through a
novel two-step event-guided network: event integrals are first employed to
reduce blur in the coarse outputs. This is followed by employing a variance
map, derived from raw event streams, to eliminate the tilt distortion for the
refined outputs. Additionally, we present TurbEvent, the first real-captured
dataset featuring diverse turbulence scenarios. Experimental results
demonstrate that EvTurb surpasses state-of-the-art methods while maintaining
computational efficiency.

</details>


### [59] [Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.10600)
*Yuxin Cao,Yedi Zhang,Wentao He,Yifan Liao,Yan Xiao,Chang Li,Zhiyong Huang,Jin Song Dong*

Main category: cs.CV

TL;DR: 本文提出了一个新的针对自动驾驶中2D目标检测的对抗补丁攻击框架P^3A，该方法在高分辨率数据集上表现出更强的攻效和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动驾驶系统易受到对抗补丁的威胁，尤其是无需模型细节的黑盒攻击；但现有攻击方法受限于评估指标（如mAP）和低分辨率训练，使得攻击实际效果被高估，转移性和实用性差。

Method: 1）提出新的评估指标PASR，更准确地衡量对安全影响相关的攻击效果；2）设计LCSL损失函数，提高攻击在PASR下的转移性和效果；3）引入PSPP预处理，增强对抗补丁在高分辨率数据集上的适应能力。

Result: 大量实验证明，P^3A在未见过的模型及高分辨率数据集上比现有方法具备更高攻击成功率和更好的转移性，不论是新提出的PASR评价标准还是传统mAP指标下都优于现有方法。

Conclusion: P^3A显著提升了对自动驾驶系统的2D目标检测模型的黑盒补丁攻击效果，特别是在高分辨率和实际安全相关场景下的实用性与转移性，展现出其在实际对抗攻击中的潜在威胁。

Abstract: Learning-based autonomous driving systems remain critically vulnerable to
adversarial patches, posing serious safety and security risks in their
real-world deployment. Black-box attacks, notable for their high attack success
rate without model knowledge, are especially concerning, with their
transferability extensively studied to reduce computational costs compared to
query-based attacks. Previous transferability-based black-box attacks typically
adopt mean Average Precision (mAP) as the evaluation metric and design training
loss accordingly. However, due to the presence of multiple detected bounding
boxes and the relatively lenient Intersection over Union (IoU) thresholds, the
attack effectiveness of these approaches is often overestimated, resulting in
reduced success rates in practical attacking scenarios. Furthermore, patches
trained on low-resolution data often fail to maintain effectiveness on
high-resolution images, limiting their transferability to autonomous driving
datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch
Attack framework for 2D object detection in autonomous driving, specifically
optimized for high-resolution datasets. First, we introduce a novel metric,
Practical Attack Success Rate (PASR), to more accurately quantify attack
effectiveness with greater relevance for pedestrian safety. Second, we present
a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack
transferability under PASR. Finally, to maintain the transferability for
high-resolution datasets, we further incorporate the Probabilistic
Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data
preprocessing step. Extensive experiments show that P$^3$A outperforms
state-of-the-art attacks on unseen models and unseen high-resolution datasets,
both under the proposed practical IoU-based evaluation metric and the previous
mAP-based metrics.

</details>


### [60] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于单图像超分辨率的新型轻量级上采样模块Frequency-Guided Attention（FGA），有效提升了图像重建的高频细节，同时减少了混叠伪影。


<details>
  <summary>Details</summary>
Motivation: 现有高效的上采样方法如子像素卷积，在恢复图像高频细节时表现有限，容易带来混叠伪影。这限制了超分辨率任务的图像重建质量，尤其是在纹理丰富、细节要求高的场景。

Method: FGA模块包括：1）基于傅里叶特征的MLP用于位置频率编码；2）跨分辨率的相关注意力层用于自适应空间对齐；3）频域L1损失用于谱一致性监督。该方法仅增加了0.3M参数，易于集成在各种超分辨率主干网络中。

Result: FGA模块在五种不同超分辨率主干网络（轻量级和高容量）中均有性能提升，平均PSNR提高了0.12~0.14 dB，频域一致性提升高达29%，对比明显改善了纹理丰富数据集的表现。

Conclusion: FGA能够有效减少混叠伪影、保留图像细节，是一种实用且可扩展的替代传统上采样的新方案，具有广泛的应用前景。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module
for single image super-resolution. Conventional upsamplers, such as Sub-Pixel
Convolution, are efficient but frequently fail to reconstruct high-frequency
details and introduce aliasing artifacts. FGA addresses these issues by
integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for
positional frequency encoding, (2) a cross-resolution Correlation Attention
Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for
spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently
enhances performance across five diverse super-resolution backbones in both
lightweight and full-capacity scenarios. Experimental results demonstrate
average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by
up to 29%, particularly evident on texture-rich datasets. Visual and spectral
evaluations confirm FGA's effectiveness in reducing aliasing and preserving
fine details, establishing it as a practical, scalable alternative to
traditional upsampling methods.

</details>


### [61] [FIND-Net -- Fourier-Integrated Network with Dictionary Kernels for Metal Artifact Reduction](https://arxiv.org/abs/2508.10617)
*Farid Tasharofi,Fuxin Fan,Melika Qahqaie,Mareike Thies,Andreas Maier*

Main category: cs.CV

TL;DR: 本文提出了FIND-Net（结合傅里叶变换的深度网络），通过同时在频域和空间域上处理，有效减少CT金属伪影并保护结构细节，并在合成和真实数据中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习消除金属伪影方法在抑制伪影的同时，常常损失结构细节，影响临床诊断和治疗。

Method: 提出FIND-Net，将快速傅里叶卷积（FFC）层和可训练高斯滤波整合为一体，在空间和频域上混合处理金属伪影问题，实现频域选择性和全局语义理解。

Result: FIND-Net在合成数据集上，MAE 降低 3.07%、SSIM 提高 0.18%、PSNR 提高 0.90%，优于主流方法；真实临床扫描验证其有效减少正常区修改，同时抑制伪影。

Conclusion: FIND-Net在金属伪影消除任务中取得了更好结构保留和临床可用性，展现了其提升MAR性能的潜力。

Abstract: Metal artifacts, caused by high-density metallic implants in computed
tomography (CT) imaging, severely degrade image quality, complicating diagnosis
and treatment planning. While existing deep learning algorithms have achieved
notable success in Metal Artifact Reduction (MAR), they often struggle to
suppress artifacts while preserving structural details. To address this
challenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary
Kernels), a novel MAR framework that integrates frequency and spatial domain
processing to achieve superior artifact suppression and structural
preservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and
trainable Gaussian filtering, treating MAR as a hybrid task operating in both
spatial and frequency domains. This approach enhances global contextual
understanding and frequency selectivity, effectively reducing artifacts while
maintaining anatomical structures. Experiments on synthetic datasets show that
FIND-Net achieves statistically significant improvements over state-of-the-art
MAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR
improvement, confirming robustness across varying artifact complexities.
Furthermore, evaluations on real-world clinical CT scans confirm FIND-Net's
ability to minimize modifications to clean anatomical regions while effectively
suppressing metal-induced distortions. These findings highlight FIND-Net's
potential for advancing MAR performance, offering superior structural
preservation and improved clinical applicability. Code is available at
https://github.com/Farid-Tasharofi/FIND-Net

</details>


### [62] [Increasing the Utility of Synthetic Images through Chamfer Guidance](https://arxiv.org/abs/2508.10631)
*Nicola Dall'Asen,Xiaofeng Zhang,Reyhane Askari Hemmat,Melissa Hall,Jakob Verbeek,Adriana Romero-Soriano,Michal Drozdzal*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练引导方法Chamfer Guidance，利用少量真实样本提升条件生成图像模型的多样性和质量，并显著提高下游任务的准确率，减少计算量。


<details>
  <summary>Details</summary>
Motivation: 虽然条件图像生成模型有潜力大规模生成合成训练数据，但随着生成质量提升，多样性却下降，限制了其实际效用。同时，现有的引导方法往往忽视了合成数据与真实数据间的分布差异。该工作旨在解决生成质量与多样性的平衡，并兼顾分布一致性。

Method: 提出Chamfer Guidance，无需额外训练，通过少量（如2-32张）真实样本计算合成和真实数据间的Chamfer距离，用于引导生成过程，从而提升合成数据与真实数据分布的接近度以及多样性和质量。

Result: 在ImageNet-1k及地理多样性基准上，2张真实样本精度达96.4%，分布覆盖86.4%；32张样本精度97.5%，覆盖92.7%。下游图像分类任务上，合成数据训练比基线提升15%（ID）和16%（OOD）。相较于无分类器引导方法，采样时FLOPs降低31%。

Conclusion: Chamfer Guidance能在无训练、极少真实样本的条件下显著提升生成图像数据的多样性和质量，有效助力下游任务，并大幅降低计算资源消耗，具备实际部署价值。

Abstract: Conditional image generative models hold considerable promise to produce
infinite amounts of synthetic training data. Yet, recent progress in generation
quality has come at the expense of generation diversity, limiting the utility
of these models as a source of synthetic training data. Although guidance-based
approaches have been introduced to improve the utility of generated data by
focusing on quality or diversity, the (implicit or explicit) utility functions
oftentimes disregard the potential distribution shift between synthetic and
real data. In this work, we introduce Chamfer Guidance: a training-free
guidance approach which leverages a handful of real exemplar images to
characterize the quality and diversity of synthetic data. We show that by
leveraging the proposed Chamfer Guidance, we can boost the diversity of the
generations w.r.t. a dataset of real images while maintaining or improving the
generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our
approach achieves state-of-the-art few-shot performance with as little as 2
exemplar real images, obtaining 96.4\% in terms of precision, and 86.4\% in
terms of distributional coverage, which increase to 97.5\% and 92.7\%,
respectively, when using 32 real images. We showcase the benefits of the
Chamfer Guidance generation by training downstream image classifiers on
synthetic data, achieving accuracy boost of up to 15\% for in-distribution over
the baselines, and up to 16\% in out-of-distribution. Furthermore, our approach
does not require using the unconditional model, and thus obtains a 31\%
reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling
time.

</details>


### [63] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: ChatENV是一款结合卫星影像和传感器数据，实现互动式环境变化分析的视觉语言模型。新方法利用大规模带元数据的遥感数据集，提升了多源推理和情景分析能力，性能超越现有模式。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在环境分析任务中忽视了真实世界传感器信号、依赖单一描述，且缺乏互动和情景推理，限制了环境监测、气候研究和城市规划等应用。

Method: 1. 构建含177k张图片、152k时序对、62类地表用途、197国分布的遥感数据集，并配套丰富的传感器元数据（如温度、颗粒物PM10、一氧化碳CO）；2. 利用GPT-4o和Gemini 2.0生成风格多样、语义多元的数据标注；3. 采用LoRA机制对Qwen-2.5-VL模型进行定向微调，使其支持对话能力。

Result: ChatENV在时序和假设推理任务上表现优异（如BERT-F1达0.903），并且能与或超越当前最新的时序模型，同时引入交互式情景分析功能。

Conclusion: ChatENV实现了图像与传感器联合推理，支持互动分析，成为环境监测领域具备现实感知和情景应对的强大工具。

Abstract: Understanding environmental changes from aerial imagery is vital for climate
resilience, urban planning, and ecosystem monitoring. Yet, current vision
language models (VLMs) overlook causal signals from environmental sensors, rely
on single-source captions prone to stylistic bias, and lack interactive
scenario-based reasoning. We present ChatENV, the first interactive VLM that
jointly reasons over satellite image pairs and real-world sensor data. Our
framework: (i) creates a 177k-image dataset forming 152k temporal pairs across
62 land-use classes in 197 countries with rich sensor metadata (e.g.,
temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for
stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using
efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV
achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1
0.903) and rivals or outperforms state-of-the-art temporal models, while
supporting interactive scenario-based analysis. This positions ChatENV as a
powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [64] [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637)
*Ryan Ramos,Vladan Stojnić,Giorgos Kordopatis-Zilos,Yuta Nakashima,Giorgos Tolias,Noa Garcia*

Main category: cs.CV

TL;DR: 本文关注图像采集过程中细微甚至人眼不可察觉的参数对视觉编码器表现的影响，发现这些信息会被编码到视觉表示中，并显著影响模型语义预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作分析了视觉编码器在面对大幅度图像变换和扰动时的鲁棒性，但这些研究主要关注对图像语义有强烈破坏作用的严重扰动，而忽视了图像采集过程中那些细微而难以察觉的参数。本文旨在探究这些微小变化对深度视觉模型表示与预测性能的潜在影响。

Method: 作者通过实验对视觉表示中是否包含图像采集和处理时的参数进行了分析，并评估了这些信息对模型语义预测任务的干扰或帮助作用。实验还分析了这些参数与语义标签间的相关性如何进一步影响模型表现。

Result: 实验发现，即便是细微或人眼难以觉察的参数，深度模型也能系统性地将其编码进视觉特征，而且这些参数在视觉表示中可以被恢复。此外，这些细节参数对语义预测结果有显著积极或消极影响，具体效果取决于它们与语义标签之间的相关性或反相关性。

Conclusion: 本文提醒研究者与开发者在数据采集和处理阶段重视、控制那些看似微小但可被模型利用的参数，以避免模型学习到无关甚至有害的关联，从而影响实际部署中的鲁棒性和泛化能力。

Abstract: Prior work has analyzed the robustness of visual encoders to image
transformations and corruptions, particularly in cases where such alterations
are not seen during training. When this occurs, they introduce a form of
distribution shift at test time, often leading to performance degradation. The
primary focus has been on severe corruptions that, when applied aggressively,
distort useful signals necessary for accurate semantic predictions.
  We take a different perspective by analyzing parameters of the image
acquisition process and transformations that may be subtle or even
imperceptible to the human eye. We find that such parameters are systematically
encoded in the learned visual representations and can be easily recovered. More
strikingly, their presence can have a profound impact, either positively or
negatively, on semantic predictions. This effect depends on whether there is a
strong correlation or anti-correlation between semantic labels and these
acquisition-based or processing-based labels. Our code and data are available
at: https://github.com/ryan-caesar-ramos/visual-encoder-traces

</details>


### [65] [Lameness detection in dairy cows using pose estimation and bidirectional LSTMs](https://arxiv.org/abs/2508.10643)
*Helena Russello,Rik van der Tol,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 本研究结合了姿态估计与双向长短时记忆（BLSTM）网络，实现对奶牛跛行的高效检测，准确率达到85%。


<details>
  <summary>Details</summary>
Motivation: 传统的跛行检测方法需要人工标注特征和大量的数据，效率有限。作者希望借助深度学习自动提取时序运动特征，降低对人工和数据量的依赖。

Method: 首先利用T-LEAP姿态估计模型从奶牛步态视频中提取9个关键点（蹄、头、背）的轨迹。然后将这些轨迹输入BLSTM神经网络进行二分类（跛/正常），通过序列学习抓取行为特征。

Result: 该方法分类准确率为85%，优于手工特征方法（80%）。此外，即使仅用1秒步态视频，模型仍能有效检测跛行。

Conclusion: 基于姿态估计与BLSTM的无标记、序列学习方法能有效检测奶牛跛行，具有应用前景，特别适合数据量小及无需人工特征设计的场景。

Abstract: This study presents a lameness detection approach that combines pose
estimation and Bidirectional Long-Short-Term Memory (BLSTM) neural networks.
Combining pose-estimation and BLSTMs classifier offers the following
advantages: markerless pose-estimation, elimination of manual feature
engineering by learning temporal motion features from the keypoint
trajectories, and working with short sequences and small training datasets.
Motion sequences of nine keypoints (located on the cows' hooves, head and back)
were extracted from videos of walking cows with the T-LEAP pose estimation
model. The trajectories of the keypoints were then used as an input to a BLSTM
classifier that was trained to perform binary lameness classification. Our
method significantly outperformed an established method that relied on
manually-designed locomotion features: our best architecture achieved a
classification accuracy of 85%, against 80% accuracy for the feature-based
approach. Furthermore, we showed that our BLSTM classifier could detect
lameness with as little as one second of video data.

</details>


### [66] [SemPT: Semantic Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.10645)
*Xiao Shi,Yangjun Ou,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Semantic Prompt Tuning (SemPT) 的新框架，有效提升了视觉-语言模型在未见类别上的泛化能力，并在多个场景下取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 目前视觉迁移学习在处理未见类别时面临难题：既要保持类别特异性表示，又要具备良好的知识迁移性。现有的提示调整方法依赖稀疏类别标签或独立的LLM生成描述，导致知识碎片化，迁移能力不足。

Method: SemPT采用两步提示策略，引导LLM提取类别间共享的视觉属性，并生成属性级描述，捕捉超越标签的可迁移语义信息；通过视觉引导加权抑制无关属性噪声，增强文本嵌入表达力。推理时，针对已见与未见类别动态切换标签嵌入与属性增强嵌入。

Result: 在15个基准数据集上验证，SemPT在基本到新颖类别泛化、跨数据集迁移、跨域迁移、少样本学习等多种设置下取得了最优表现。

Conclusion: SemPT通过属性级知识和动态推理，突破了现有方法在迁移性能上的瓶颈，有效提升了视觉-语言模型对未见类别的适应能力。

Abstract: Visual transfer learning for unseen categories presents an active research
topic yet a challenging task, due to the inherent conflict between preserving
category-specific representations and acquiring transferable knowledge.
Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs
offer a promising solution. However, existing prompt tuning methods rely on
sparse category labels or disparate LLM-generated descriptions, which fragment
knowledge representation and hinder transferability. To address this
limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that
tackles the generalization challenge by leveraging shared attribute-level
knowledge across categories. Specifically, SemPT adopts a two-step prompting
strategy to guide LLM in extracting shared visual attributes and generating
attribute-level descriptions, capturing transferable semantic cues beyond
labels while ensuring coherent structure. Then, visually guided weighting is
applied to the embeddings of attribute-level descriptions to reduce noise from
irrelevant attributes and enhance the text embeddings. Additionally, image
embeddings are jointly aligned with both label and attribute-enhanced text
embeddings, balancing discrimination for seen categories and transferability to
unseen ones. Considering the availability of category exposure, our inference
dynamically selects between standard label embeddings for seen categories and
attribute-enhanced embeddings for unseen ones to ensure effective adaptation.
Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves
state-of-the-art performance across various settings, including base-to-novel
generalization, cross-dataset transfer, cross-domain transfer, and few-shot
learning.

</details>


### [67] [Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking](https://arxiv.org/abs/2508.10655)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Chunyang Cheng,Tao Zhou,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文提出了一个统一的多模态视觉目标跟踪基准UniBench300，用于解决因数据分散导致训练和测试不一致、性能下降等问题；并引入持续学习（CL）方法以更稳定地整合多任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉目标跟踪方法由于缺乏统一的基准，需在不同数据集上测试，导致训练和测试的任务分布不一致，从而引起性能下降。解决训练和测试不一致、提升多任务联合建模表现成为迫切需求。

Method: 1）提出了新的统一基准UniBench300，涵盖多种任务数据并将推理次数减少（三次变一次），有效降低时间消耗。2）提出串行整合方法，采用持续学习的思想，逐步融合新任务，以缓解性能退化和知识遗忘。

Result: 大量实验证明，UniBench300能有效缓解训练测试不一致问题，并且持续学习方法能支撑更稳定的多任务整合。另外，分析结果揭示模型容量越低、模式差异越大，性能退化越明显（表现为RGBT>RGBD>RGBE）。

Conclusion: UniBench300作为统一基准推进了多模态跟踪研究，持续学习有助于解决多任务融合时的知识遗忘问题。此外，不同模态间的差异对性能退化影响显著，为未来多模态视觉研究提供了新的思路和挑战。

Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws
increasing attention due to the complementary nature of different modalities in
building robust tracking systems. Existing practices mix all data sensor types
in a single training procedure, structuring a parallel paradigm from the
data-centric perspective and aiming for a global optimum on the joint
distribution of the involved tasks. However, the absence of a unified benchmark
where all types of data coexist forces evaluations on separated benchmarks,
causing \textit{inconsistency} between training and testing, thus leading to
performance \textit{degradation}. To address these issues, this work advances
in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is
introduced to bridge the inconsistency by incorporating multiple task data,
reducing inference passes from three to one and cutting time consumption by
27\%. \ding{183} The unification process is reformulated in a serial format,
progressively integrating new tasks. In this way, the performance degradation
can be specified as knowledge forgetting of previous tasks, which naturally
aligns with the philosophy of continual learning (CL), motivating further
exploration of injecting CL into the unification process. Extensive experiments
conducted on two baselines and four benchmarks demonstrate the significance of
UniBench300 and the superiority of CL in supporting a stable unification
process. Moreover, while conducting dedicated analyses, the performance
degradation is found to be negatively correlated with network capacity.
Additionally, modality discrepancies contribute to varying degradation levels
across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for
future multi-modal vision research. Source codes and the proposed benchmark is
available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.

</details>


### [68] [AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models](https://arxiv.org/abs/2508.10667)
*Shixiong Xu,Chenghao Zhang,Lubin Fan,Yuan Zhou,Bin Fan,Shiming Xiang,Gaofeng Meng,Jieping Ye*

Main category: cs.CV

TL;DR: 该论文提出一种新方法（AddressVLM），通过结合街景图像和卫星图像，提升大视觉语言模型（LVLMs）在细粒度地址定位任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前LVLMs在国家或城市级别的粗粒度地理定位上表现优异，但在需要精确到街道级别的细粒度定位任务中表现不佳。主要原因是街景视觉问答数据只提供了微观视觉线索，导致模型无法准确推断具体位置。为此，本文希望提升LVLMs在城市内部的准确定位能力，以满足现实应用对高精度地址问答的需求。

Method: 提出跨视角对齐训练方法，结合了街景图像（微观线索）和卫星图像（宏观线索）。具体方法包括：跨视角对齐与图像嫁接机制、自动标签生成机制，并采用两阶段训练协议（跨视角对齐训练和地址定位训练）。同时，构建了两个人工标注的街景VQA数据集，分别来源于匹兹堡和旧金山。

Result: 提出的AddressVLM在新构建的匹兹堡和旧金山数据集上，平均地址定位准确率分别超越对比LVLMs模型9%和12%。

Conclusion: 通过引入卫星图像与街景图像的跨视角配准，AddressVLM显著提升了LVLMs在城市街道级别的细粒度地址定位能力，为实际应用（如地图导航、位置问答等）提供了更强有力的技术支持。

Abstract: Large visual language models (LVLMs) have demonstrated impressive performance
in coarse-grained geo-localization at the country or city level, but they
struggle with fine-grained street-level localization within urban areas. In
this paper, we explore integrating city-wide address localization capabilities
into LVLMs, facilitating flexible address-related question answering using
street-view images. A key challenge is that the street-view visual
question-and-answer (VQA) data provides only microscopic visual cues, leading
to subpar performance in fine-tuned models. To tackle this issue, we
incorporate perspective-invariant satellite images as macro cues and propose
cross-view alignment tuning including a satellite-view and street-view image
grafting mechanism, along with an automatic label generation mechanism. Then
LVLM's global understanding of street distribution is enhanced through
cross-view matching. Our proposed model, named AddressVLM, consists of
two-stage training protocols: cross-view alignment tuning and address
localization tuning. Furthermore, we have constructed two street-view VQA
datasets based on image address localization datasets from Pittsburgh and San
Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM
outperforms counterpart LVLMs by over 9% and 12% in average address
localization accuracy on these two datasets, respectively.

</details>


### [69] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种高效构建高质量人脸识别数据集的方法，并在ICCV DataCV挑战赛中获得冠军。提出的混合式自动清洗和数据生成流程，有效提升了模型在多尺度身份下的识别表现。


<details>
  <summary>Details</summary>
Motivation: 比赛要求构建一个与现有公开人脸数据集无身份重叠的高质量数据集供人脸识别模型训练，因此需要解决数据清洗、身份多样性和数据集独立性等难题。

Method: 首先，利用MoE策略（结合人脸嵌入聚类与GPT-4o辅助验证）对HSFace基线数据集进行深度清洗，保留一致性最大身份并做增强。为提升多样性，利用Stable Diffusion结合提示工程生成新身份，并用Vec2Face快速扩充样本。最终结合GAN与扩散模型样本，采用课程学习的训练方式，由易到难训练模型，并全程校验身份唯一性，无泄漏。

Result: 最终构建的数据集每个身份有50张图片，所有新增身份均通过主流数据集核查无泄漏。方法在比赛中获第一名，实验显示数据集显著提升了在人脸数量10K/20K/100K规模下的模型表现。

Conclusion: 融合深度清洗、合成身份生成、高效样本扩充及课程学习策略的方法，可高效构建高质量人脸识别数据集，显著提升模型识别效果，具较强的实用价值与推广潜力。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which
centers on building a high-quality face dataset to train a face recognition
model. The constructed dataset must not contain identities overlapping with any
existing public face datasets. To handle this challenge, we begin with a
thorough cleaning of the baseline HSFace dataset, identifying and removing
mislabeled or inconsistent identities through a Mixture-of-Experts (MoE)
strategy combining face embedding clustering and GPT-4o-assisted verification.
We retain the largest consistent identity cluster and apply data augmentation
up to a fixed number of images per identity. To further diversify the dataset,
we generate synthetic identities using Stable Diffusion with prompt
engineering. As diffusion models are computationally intensive, we generate
only one reference image per identity and efficiently expand it using Vec2Face,
which rapidly produces 49 identity-consistent variants. This hybrid approach
fuses GAN-based and diffusion-based samples, enabling efficient construction of
a diverse and high-quality dataset. To address the high visual similarity among
synthetic identities, we adopt a curriculum learning strategy by placing them
early in the training schedule, allowing the model to progress from easier to
harder samples. Our final dataset contains 50 images per identity, and all
newly generated identities are checked with mainstream face datasets to ensure
no identity leakage. Our method achieves \textbf{1st place} in the competition,
and experimental results show that our dataset improves model performance
across 10K, 20K, and 100K identity scales. Code is available at
https://github.com/Ferry-Li/datacv_fr.

</details>


### [70] [HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection](https://arxiv.org/abs/2508.10678)
*Zhaoyuan Qi,Weihua Gao,Wenlong Niu,Jie Tang,Yun Li,Xiaodong Peng*

Main category: cs.CV

TL;DR: 该论文提出了HyperTea方法，实现了运动红外小目标检测（MIRSTD）的重大性能提升，通过全球与局部时序提升模块以及时序对齐模块，综合CNN、RNN与超图神经网络（HGNN）以挖掘高阶时空特征关联。实验证明其在主流数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 运动红外小目标因其尺寸小、信号弱和运动复杂，检测难度较大。现有方法多只建模低阶特征关联，且局限于单一时序尺度，难以捕捉全局及局部时序信息，限制了检测精度。超图模型可以实现高阶关联学习，但在MIRSTD领域应用极少。作者希望通过引入超图和多时序建模，提升检测能力。

Method: 作者提出HyperTea，包括：1）全球时序增强模块（GTEM），通过语义聚合传播增强全局时序上下文；2）局部时序增强模块（LTEM），捕获并增强邻帧间局部运动模式；3）时序对齐模块（TAM），解决多尺度特征错位。该方法首次结合了CNN、RNN和HGNN于MIRSTD任务。

Result: 作者在DAUB和IRDST两个公开MIRSTD数据集上实验，HyperTea显著优于现有方法，取得了最新SOTA性能，证明其高效性和鲁棒性。

Conclusion: HyperTea有效融合了多时序特征表示和高阶时空特征建模，突破了传统方法的局限，极大提升了运动红外小目标检测的效果，为相关领域提供了新的技术方向。源码已开源，有利于进一步研究。

Abstract: In practical application scenarios, moving infrared small target detection
(MIRSTD) remains highly challenging due to the target's small size, weak
intensity, and complex motion pattern. Existing methods typically only model
low-order correlations between feature nodes and perform feature extraction and
enhancement within a single temporal scale. Although hypergraphs have been
widely used for high-order correlation learning, they have received limited
attention in MIRSTD. To explore the potential of hypergraphs and enhance
multi-timescale feature representation, we propose HyperTea, which integrates
global and local temporal perspectives to effectively model high-order
spatiotemporal correlations of features. HyperTea consists of three modules:
the global temporal enhancement module (GTEM) realizes global temporal context
enhancement through semantic aggregation and propagation; the local temporal
enhancement module (LTEM) is designed to capture local motion patterns between
adjacent frames and then enhance local temporal context; additionally, we
further develop a temporal alignment module (TAM) to address potential
cross-scale feature misalignment. To our best knowledge, HyperTea is the first
work to integrate convolutional neural networks (CNNs), recurrent neural
networks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,
significantly improving detection performance. Experiments on DAUB and IRDST
demonstrate its state-of-the-art (SOTA) performance. Our source codes are
available at https://github.com/Lurenjia-LRJ/HyperTea.

</details>


### [71] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经隐式表征和物理约束联合的T2映射重建方法，有效解决了胎儿脑MRI扫描中由于运动导致的图像重建与定量分析难题，在0.55T磁场下首次实现了高质量胎儿脑T2映射。


<details>
  <summary>Details</summary>
Motivation: 现有胎儿脑MRI的T2 mapping需要在不同echo time下多次采集，手段耗时长、易受运动伪影影响，尤其在中等磁场（0.55T）下T2衰减较慢，成像难度更大。因此，亟需高效、抗运动的新方法实现高分辨率T2定量评估，助力发育中胎儿脑特征识别。

Method: 作者提出一种跨TE联合重建的T2 mapping方法：结合神经隐式表征技术和基于物理模型的正则化，利用T2衰减建模，强化不同TE间的信息融合，同时保持解剖结构和定量T2数据的准确性。方法针对采集到的多堆厚层、受运动影响片断数据，采用切片到体积重建（SVR）实现高分辨率3D恢复。

Result: 方法在模拟胎儿脑和具有胎儿脑运动特性的人体（成人）数据集上取得了先进水平的T2映射重建表现。首次在0.55T中场下得到高质量的活体胎儿T2 mapping验证。

Conclusion: 所提方法显著提升了胎儿脑MRI中T2 mapping的效率和鲁棒性，有望大幅减少每个TE所需堆叠数量，提高扫描速率，同时确保解剖和量化准确性，对临床胎儿脑发育评估具有实际意义。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization
of the developing brain, especially at mid-field (0.55T), where T2 decay is
slower. However, this is challenging as fetal MRI acquisition relies on
multiple motion-corrupted stacks of thick slices, requiring slice-to-volume
reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,
T2 mapping involves repeated acquisitions of these stacks at each echo time
(TE), leading to long scan times and high sensitivity to motion. We tackle this
challenge with a method that jointly reconstructs data across TEs, addressing
severe motion. Our approach combines implicit neural representations with a
physics-informed regularization that models T2 decay, enabling information
sharing across TEs while preserving anatomical and quantitative T2 fidelity. We
demonstrate state-of-the-art performance on simulated fetal brain and in vivo
adult datasets with fetal-like motion. We also present the first in vivo fetal
T2 mapping results at 0.55T. Our study shows potential for reducing the number
of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [72] [IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning](https://arxiv.org/abs/2508.10681)
*Mengyang Zhao,Teng Fu,Haiyang Yu,Ke Niu,Bin Li*

Main category: cs.CV

TL;DR: 本文提出了一种用于少样本工业异常检测（FS-IAD）的新型统一框架IADGPT，通过模拟人类检查方式，借助大规模视觉-语言模型，实现对多样工业产品的异常检测、定位与推理，并伴随发布包含10万张图像的大型标注数据集，在各项任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大视觉-语言模型的FS-IAD方法虽然取得一定效果，但由于缺乏工业领域知识和推理能力，难以比拟专业质检人员，因此需要专为工业异常检测设计的框架。

Method: 提出三阶段渐进式训练策略：第一、二阶段让模型获取工业基础知识和异常感知能力；第三阶段引入基于上下文学习，让模型用少数样本作为示例提升新品泛化能力。同时设计了基于模型logits和注意力图输出的图像级与像素级异常分数策略，并联合语言输出实现异常推理。配套构建包含100K图像、400类产品和丰富文本标注的新数据集。

Result: IADGPT在异常检测准确率、异常定位与推理任务上均取得了显著提升，显示出较强的泛化和竞争力。

Conclusion: IADGPT通过模仿人类检验方式，有效提升了少样本工业异常检测的能力，可广泛适用于多类新颖工业产品中，对工业自动质检领域有重要意义。

Abstract: Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in
automating industrial quality inspection. Recently, some FS-IAD methods based
on Large Vision-Language Models (LVLMs) have been proposed with some
achievements through prompt learning or fine-tuning. However, existing LVLMs
focus on general tasks but lack basic industrial knowledge and reasoning
capabilities related to FS-IAD, making these methods far from specialized human
quality inspectors. To address these challenges, we propose a unified
framework, IADGPT, designed to perform FS-IAD in a human-like manner, while
also handling associated localization and reasoning tasks, even for diverse and
novel industrial products. To this end, we introduce a three-stage progressive
training strategy inspired by humans. Specifically, the first two stages
gradually guide IADGPT in acquiring fundamental industrial knowledge and
discrepancy awareness. In the third stage, we design an in-context
learning-based training paradigm, enabling IADGPT to leverage a few-shot image
as the exemplars for improved generalization to novel products. In addition, we
design a strategy that enables IADGPT to output image-level and pixel-level
anomaly scores using the logits output and the attention map, respectively, in
conjunction with the language output to accomplish anomaly reasoning. To
support our training, we present a new dataset comprising 100K images across
400 diverse industrial product categories with extensive attribute-level
textual annotations. Experiments indicate IADGPT achieves considerable
performance gains in anomaly detection and demonstrates competitiveness in
anomaly localization and reasoning. We will release our dataset in
camera-ready.

</details>


### [73] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练扩散模型生成单张图像新视角图像的方法，无需大规模训练或微调，且效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有单图新视角合成方法通常需要多视角数据，重新训练或微调大的扩散模型，代价高耗时且泛化能力弱，且重建结果模糊。作者希望利用已有预训练扩散模型，高效且高保真地实现单图新视角合成。

Method: 作者提出了一种显式的轻量级视角转换框架，利用了DDIM反演得到的潜在编码。设计了一个相机位姿条件U-Net（TUNet），用来预测目标视角下的潜在编码。此外，针对重建结果模糊的问题，提出了一种新的融合策略，利用DDIM反演的噪声相关性结构，增强纹理和细节。最后将融合后的潜在编码作为初始条件进行DDIM采样，生成目标视角图像。

Result: 在MVImgNet数据集上的广泛实验表明，所提方法在新视角合成任务上优于现有方法，生成图像更加清晰且细节丰富。

Conclusion: 该方法能高效、直接地利用预训练扩散模型的生成能力解决单图新视角合成问题，既保持了高质量画面，同时提升了泛化能力和计算效率，为相关领域提供了有效的新思路。

Abstract: Synthesizing novel views from a single input image is a challenging task. It
requires extrapolating the 3D structure of a scene while inferring details in
occluded regions, and maintaining geometric consistency across viewpoints. Many
existing methods must fine-tune large diffusion backbones using multiple views
or train a diffusion model from scratch, which is extremely expensive.
Additionally, they suffer from blurry reconstruction and poor generalization.
This gap presents the opportunity to explore an explicit lightweight view
translation framework that can directly utilize the high-fidelity generative
capabilities of a pretrained diffusion model while reconstructing a scene from
a novel view. Given the DDIM-inverted latent of a single input image, we employ
a camera pose-conditioned translation U-Net, TUNet, to predict the inverted
latent corresponding to the desired target view. However, the image sampled
using the predicted latent may result in a blurry reconstruction. To this end,
we propose a novel fusion strategy that exploits the inherent noise correlation
structure observed in DDIM inversion. The proposed fusion strategy helps
preserve the texture and fine-grained details. To synthesize the novel view, we
use the fused latent as the initial condition for DDIM sampling, leveraging the
generative prior of the pretrained diffusion model. Extensive experiments on
MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [74] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 本文提出一种融合事件相机与RGB相机的新方法，通过创新的神经网络结构（MCFNet）显著提升复杂光照条件下交通场景的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机动态范围有限，在夜晚驾驶、隧道等高对比度或低照度环境中容易丢失全局对比度及细节，影响特征提取和检测效果。为克服这一问题，作者希望结合动态范围更高的事件相机，提升目标检测鲁棒性。

Method: 提出MCFNet：事件相机与RGB相机协同工作。1）事件校正模块通过基于光流的时序校正，将事件流与帧图像对齐，并与检测网络联合优化。2）事件动态上采样模块，提升事件图像分辨率，空间上对齐帧图像结构。3）交叉模态融合模块（CMM）采用新颖的交错扫描机制自适应融合多模态特征。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，MCFNet在恶劣光照及高速交通场景下明显优于现有方法。在DSEC-Det数据集上，mAP50提升7.4%，mAP提升1.7%。

Conclusion: 融合事件相机与RGB相机，结合空间-时序对齐和多模态融合，大幅提升目标检测在复杂照明和动态环境下的表现，具备实际应用潜力。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global
contrast and causes loss of high-frequency details such as textures and edges
in complex traffic environments (e.g., nighttime driving, tunnels), hindering
discriminative feature extraction and degrading frame-based object detection.
To address this, we integrate a bio-inspired event camera with an RGB camera to
provide high dynamic range information and propose a motion cue fusion network
(MCFNet), which achieves optimal spatiotemporal alignment and adaptive
cross-modal feature fusion under challenging lighting. Specifically, an event
correction module (ECM) temporally aligns asynchronous event streams with image
frames via optical-flow-based warping, jointly optimized with the detection
network to learn task-aware event representations. The event dynamic upsampling
module (EDUM) enhances spatial resolution of event frames to match image
structures, ensuring precise spatiotemporal alignment. The cross-modal mamba
fusion module (CMM) uses adaptive feature fusion with a novel interlaced
scanning mechanism, effectively integrating complementary information for
robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD
datasets demonstrate that MCFNet significantly outperforms existing methods in
various poor lighting and fast moving traffic scenarios. Notably, on the
DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best
existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The
code is available at https://github.com/Charm11492/MCFNet.

</details>


### [75] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 该论文提出了CountCluster方法，有效提升了扩散模型中生成图像时对对象数量的准确控制，无需额外训练或外部工具，准确率平均提升18.5%。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式文本到图像生成模型虽然生成质量和多样性高，但难以准确生成输入中指定数量的对象。以往方法依赖外部计数模块或学习到的数量表示，但依然无法解决数量失真问题，原因在于对象数量在去噪早期已基本决定。

Method: 提出CountCluster方法，在推理阶段，将对象的cross-attention注意力图根据对象数k分为k个聚类簇，目标是每个聚类空间上分离且与输入数量一致，通过优化潜变量使实际聚类分布与理想分布对齐，不需要额外训练或外部计数器。

Result: 与现有方法相比，CountCluster在对象计数准确率上平均提高了18.5%，对不同输入都表现出更优的数量控制能力。

Conclusion: CountCluster方法能显著提升文本到图像扩散模型对对象数量的精确控制能力，且方法简单、无需再训练，有望在相关生成式任务中广泛应用。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong
performance in terms of image quality and diversity. However, they still
struggle to generate images that accurately reflect the number of objects
specified in the input prompt. Several approaches have been proposed that rely
on either external counting modules for iterative refinement or quantity
representations derived from learned tokens or latent features. However, they
still have limitations in accurately reflecting the specified number of objects
and overlook an important structural characteristic--The number of object
instances in the generated image is largely determined in the early timesteps
of the denoising process. To correctly reflect the object quantity for image
generation, the highly activated regions in the object cross-attention map at
the early timesteps should match the input object quantity, while each region
should be clearly separated. To address this issue, we propose
\textit{CountCluster}, a method that guides the object cross-attention map to
be clustered according to the specified object count in the input, without
relying on any external tools or additional training. The proposed method
partitions the object cross-attention map into $k$ clusters at inference time
based on attention scores, defines an ideal distribution in which each cluster
is spatially well-separated, and optimizes the latent to align with this target
distribution. Our method achieves an average improvement of 18.5\%p in object
count accuracy compared to existing methods, and demonstrates superior quantity
control performance across a variety of prompts. Code will be released at:
https://github.com/JoohyeonL22/CountCluster .

</details>


### [76] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: 本文提出了NextStep-1，一种结合了离散文本和连续图像token的自回归生成模型，在文本到图像生成任务中表现优异，并支持高质量图像合成和图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像自回归模型要么依赖于计算量大且昂贵的扩散模型处理连续图像token，要么采用矢量量化导致量化损失。作者希望突破这些局限，提升模型性能和效率。

Method: 提出NextStep-1模型，包含一个140亿参数的自回归主体与1.57亿参数的flow matching头部，同时处理离散的文本token与连续的图像token，通过next-token预测目标联合训练。

Result: NextStep-1在文本到图像生成任务中取得了自回归模型的最新最优性能，能够生成高保真图像，并具备强大的图像编辑能力。

Conclusion: NextStep-1证明了将离散文本token和连续图像token结合的自回归范式可在生成和编辑图像任务中取得优异性能，具备较高通用性。代码和模型将开源促进社区研究。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either
rely on heavy, computationally-intensive diffusion models to process continuous
image tokens, or employ vector quantization (VQ) to obtain discrete tokens with
quantization loss. In this paper, we push the autoregressive paradigm forward
with NextStep-1, a 14B autoregressive model paired with a 157M flow matching
head, training on discrete text tokens and continuous image tokens with
next-token prediction objectives. NextStep-1 achieves state-of-the-art
performance for autoregressive models in text-to-image generation tasks,
exhibiting strong capabilities in high-fidelity image synthesis. Furthermore,
our method shows strong performance in image editing, highlighting the power
and versatility of our unified approach. To facilitate open research, we will
release our code and models to the community.

</details>


### [77] [Lightweight CNNs for Embedded SAR Ship Target Detection and Classification](https://arxiv.org/abs/2508.10712)
*Fabian Kresse,Georgios Pilikos,Mario Azcueta,Nicolas Floury*

Main category: cs.CV

TL;DR: 本文提出并评估了可在卫星上实时运行的神经网络模型，用于对原始SAR数据进行处理，实现海上目标识别并减少数据传输延迟和带宽压力。


<details>
  <summary>Details</summary>
Motivation: 利用SAR数据实现大规模海上目标监测时，传统方案需将全部原始数据回传地面再进行处理，造成数据量大、带宽受限和延迟高等问题。

Method: 作者设计了针对Sentinel-1星座Stripmap和IW模式下未聚焦SAR原始数据的神经网络模型，并优化为适合FPGA等受限硬件环境进行实时推理。同时，通过二分类实验（船只与风电场），验证了模型可用于目标识别。

Result: 实验证明，所提出的神经网络能够在资源受限的FPGA上运行，实现了原始SAR数据的船只和风车的有效分类，支持在轨处理的可行性。

Conclusion: 结果表明，通过在卫星端部署优化的神经网络，可以有效实现SAR原始数据的实时目标分类，从而减少数据下行量，缓解带宽瓶颈，实现低延迟的海上监测。

Abstract: Synthetic Aperture Radar (SAR) data enables large-scale surveillance of
maritime vessels. However, near-real-time monitoring is currently constrained
by the need to downlink all raw data, perform image focusing, and subsequently
analyze it on the ground. On-board processing to generate higher-level products
could reduce the data volume that needs to be downlinked, alleviating bandwidth
constraints and minimizing latency. However, traditional image focusing and
processing algorithms face challenges due to the satellite's limited memory,
processing power, and computational resources. This work proposes and evaluates
neural networks designed for real-time inference on unfocused SAR data acquired
in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our
results demonstrate the feasibility of using one of our models for on-board
processing and deployment on an FPGA. Additionally, by investigating a binary
classification task between ships and windmills, we demonstrate that target
classification is possible.

</details>


### [78] [Revisiting Cross-View Localization from Image Matching](https://arxiv.org/abs/2508.10716)
*Panwang Xia,Qiong Wu,Lei Yu,Yi Liu,Mingtao Xiong,Lei Liang,Yongjun Zhang,Yi Wan*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨视角定位和匹配方法，利用Surface Model和SimRefiner模块提升了定位精度和图像匹配质量，并发布了像素级标注数据集CVFM，显著提升了极端视角下的方法表现。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角定位方法通常直接回归位姿或在BEV空间对齐特征，依赖于精确的空间对应关系，但难以获得细粒度和几何一致的匹配，导致最终定位结果的可解释性和精度有限。因此有必要从跨视角图像匹配的角度重新审视该任务，解决精细配准的难题。

Method: 提出采用Surface Model对可见区域进行建模，从而提升BEV投影的准确性；设计了SimRefiner模块，通过局部-全局残差修正机制细化相似度矩阵，无需RANSAC等后处理。此外，推出了包含32,509对像素级对应关系标注的CVFM基准数据集。

Result: 大量实验证明，该方法在定位精度与图像匹配质量方面均有显著提升，在大视角差场景下建立了新的基线表现。

Conclusion: 提出的方法有效解决了跨视角匹配和定位的核心难题，提高了结果可解释性和准确性，同时推动了相关研究，CVFM数据集为后续研究提供了标准测试平台。

Abstract: Cross-view localization aims to estimate the 3 degrees of freedom pose of a
ground-view image by registering it to aerial or satellite imagery. It is
essential in GNSS-denied environments such as urban canyons and disaster zones.
Existing methods either regress poses directly or align features in a shared
bird's-eye view (BEV) space, both built upon accurate spatial correspondences
between perspectives. However, these methods fail to establish strict
cross-view correspondences, yielding only coarse or geometrically inconsistent
matches. Consequently, fine-grained image matching between ground and aerial
views remains an unsolved problem, which in turn constrains the
interpretability of localization results. In this paper, we revisit cross-view
localization from the perspective of cross-view image matching and propose a
novel framework that improves both matching and localization. Specifically, we
introduce a Surface Model to model visible regions for accurate BEV projection,
and a SimRefiner module to refine the similarity matrix through local-global
residual correction, eliminating the reliance on post-processing like RANSAC.
To further support research in this area, we introduce CVFM, the first
benchmark with 32,509 cross-view image pairs annotated with pixel-level
correspondences. Extensive experiments demonstrate that our approach
substantially improves both localization accuracy and image matching quality,
setting new baselines under extreme viewpoint disparity.

</details>


### [79] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新的离散码本先验提取方法——Discriminative Codebook Prior Extractor（DCPE），用于提升离散token自回归图像生成系统在训练速度和生成效果上的表现。该方法通过更合理的token距离与聚合方式，替代传统的k-means聚类，充分挖掘并利用码本中的token相似性信息。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归生成模型仅利用token索引值进行训练，忽略了码本中蕴含的丰富token相似性先验。近期亦有工作尝试通过k-means聚类降维利用先验，但在token feature space中效果较差。作者因此希望设计一种更准确、有效的方式来挖掘码本先验信息，助力生成模型训练和性能提升。

Method: 提出DCPE，采用基于实例的距离度量替代k-means的中心距离，联合凝聚型聚合策略，有效避免高密度区域被错误拆分和低密度区域未被合理合并，从而更精确地捕捉token之间的相似性结构。该方法无需对现有生成范式做复杂修改，支持即插即用。

Result: 实验表明，集成DCPE后，自回归生成模型在LlamaGen-B数据集的训练速度提升了42%，最终的FID和IS指标也获得了明显提升。

Conclusion: DCPE能够高效挖掘和集成码本中的判别性先验信息，显著加快模型训练过程，并改善生成图像的质量，其即插即用特性也方便与现有方法集成，兼具高适用性与实用价值。

Abstract: Advanced discrete token-based autoregressive image generation systems first
tokenize images into sequences of token indices with a codebook, and then model
these sequences in an autoregressive paradigm. While autoregressive generative
models are trained only on index values, the prior encoded in the codebook,
which contains rich token similarity information, is not exploited. Recent
studies have attempted to incorporate this prior by performing naive k-means
clustering on the tokens, helping to facilitate the training of generative
models with a reduced codebook. However, we reveal that k-means clustering
performs poorly in the codebook feature space due to inherent issues, including
token space disparity and centroid distance inaccuracy. In this work, we
propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to
k-means clustering for more effectively mining and utilizing the token
similarity information embedded in the codebook. DCPE replaces the commonly
used centroid-based distance, which is found to be unsuitable and inaccurate
for the token feature space, with a more reasonable instance-based distance.
Using an agglomerative merging technique, it further addresses the token space
disparity issue by avoiding splitting high-density regions and aggregating
low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play
and integrates seamlessly with existing codebook prior-based paradigms. With
the discriminative prior extracted, DCPE accelerates the training of
autoregressive models by 42% on LlamaGen-B and improves final FID and IS
performance.

</details>


### [80] [EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering](https://arxiv.org/abs/2508.10729)
*Yanjun Li,Yuqian Fu,Tianwen Qian,Qi'ao Xu,Silong Dai,Danda Pani Paudel,Luc Van Gool,Xiaoling Wang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为EgoCross的新基准，用于评估多模态大模型（MLLMs）在头戴视角视频问答中的跨域泛化能力，并发现当前模型难以泛化到非日常生活领域。


<details>
  <summary>Details</summary>
Motivation: 现有的头戴视角视频问答研究主要集中在烹饪、清洁等日常活动，但实际应用时不可避免地会遇到领域转移（如手术、工业等），导致现有模型泛化能力不足。因此，作者希望通过建立新基准推动领域自适应和更具鲁棒性的模型发展。

Method: 作者构建了EgoCross基准，涵盖手术、工业、极限运动和动物视角等四个不同领域，总共约1000对问答，覆盖预测、识别、定位和计数四类关键任务，支持OpenQA和CloseQA两种格式，并对多个MLLM模型进行了系统实验和分析。

Result: 大量实验表明，无论是通用还是针对头戴视角优化的当前MLLMs，都难以跨领域泛化，在现实多样化场景下性能显著下降。作者还初步尝试了微调和强化学习等方法，以提升模型表现。

Conclusion: EgoCross基准和相关分析揭示了当前MLLMs的泛化瓶颈，并为未来开发更具领域自适应性的算法提供了研究基础。数据与代码将公开，期望推动该领域的进一步发展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly pushed the frontier of egocentric video question answering
(EgocentricQA). However, existing benchmarks and studies are mainly limited to
common daily activities such as cooking and cleaning. In contrast, real-world
deployment inevitably encounters domain shifts, where target domains differ
substantially in both visual style and semantic content. To bridge this gap, we
introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the
cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four
diverse and challenging domains, including surgery, industry, extreme sports,
and animal perspective, representing realistic and high-impact application
scenarios. It comprises approximately 1,000 QA pairs across 798 video clips,
spanning four key QA tasks: prediction, recognition, localization, and
counting. Each QA pair provides both OpenQA and CloseQA formats to support
fine-grained evaluation. Extensive experiments show that most existing MLLMs,
whether general-purpose or egocentric-specialized, struggle to generalize to
domains beyond daily life, highlighting the limitations of current models.
Furthermore, we conduct several pilot studies, \eg, fine-tuning and
reinforcement learning, to explore potential improvements. We hope EgoCross and
our accompanying analysis will serve as a foundation for advancing
domain-adaptive, robust egocentric video understanding. Data and codes will be
released at:
\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}

</details>


### [81] [Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction](https://arxiv.org/abs/2508.10731)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Yuxuan Yuan,Chenxin Li,Xiaotong Tu,Xinghao Ding,Yue Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的人类认知启发式的通用类别发现（GCD）方法：ConGCD，通过分解物体为视觉原语并结合主导与上下文共识机制，有效提升了模型在已知与新类别的归纳能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GCD方法主要专注于优化目标函数，缺乏对人类认知机制的模拟，难以有效处理新类别目标的认知与发现。

Method: 作者提出ConGCD方法：先通过高语义重构获得以视觉原语为导向的表示，再用解构过程绑定类别内共享特征。同时，设计主导和上下文共识单元分别捕捉类别判别性和固有不变性，并用共识调度器动态优化激活路径，最终通过多路共识集成获得预测结果。

Result: 在多种粗粒度与细粒度基准数据集上的大量实验，显示ConGCD在通用类别发现方面较现有方法更具优势。

Conclusion: ConGCD作为一种具备共识感知能力的新范式，有效模拟了人类认知中的视觉处理多样性，在GCD任务表现出良好效果。

Abstract: Human perceptual systems excel at inducing and recognizing objects across
both known and novel categories, a capability far beyond current machine
learning frameworks. While generalized category discovery (GCD) aims to bridge
this gap, existing methods predominantly focus on optimizing objective
functions. We present an orthogonal solution, inspired by the human cognitive
process for novel object understanding: decomposing objects into visual
primitives and establishing cross-knowledge comparisons. We propose ConGCD,
which establishes primitive-oriented representations through high-level
semantic reconstruction, binding intra-class shared attributes via
deconstruction. Mirroring human preference diversity in visual processing,
where distinct individuals leverage dominant or contextual cues, we implement
dominant and contextual consensus units to capture class-discriminative
patterns and inherent distributional invariants, respectively. A consensus
scheduler dynamically optimizes activation pathways, with final predictions
emerging through multiplex consensus integration. Extensive evaluations across
coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a
consensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.

</details>


### [82] [Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025](https://arxiv.org/abs/2508.10737)
*Matej Vitek,Darian Tomašević,Abhijit Das,Sabari Nathan,Gökhan Özbulak,Gözde Ayşe Tataroğlu Özbulak,Jean-Paul Calbimonte,André Anjos,Hariohm Hemant Bhatt,Dhruv Dhirendra Premani,Jay Chaudhari,Caiyong Wang,Jian Jiang,Chi Zhang,Qi Zhang,Iyyakutti Iyappan Ganapathi,Syed Sadaf Ali,Divya Velayudan,Maregu Assefa,Naoufel Werghi,Zachary A. Daniels,Leeon John,Ritesh Vyas,Jalil Nourmohammadi Khiarak,Taher Akbari Saeed,Mahsa Nasehi,Ali Kianfar,Mobina Pashazadeh Panahi,Geetanjali Sharma,Pushp Raj Panth,Raghavendra Ramachandra,Aditya Nigam,Umapada Pal,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本文总结了2025年巩膜分割基准竞赛（SSBC），聚焦于如何利用合成眼部图像训练隐私保护型分割模型，并比较其与真实数据模型的表现。


<details>
  <summary>Details</summary>
Motivation: 生物识别中数据隐私问题日益突出，合成数据可作为替代，缓解隐私泄漏风险，但其有效性尚需评估。

Method: 竞赛分两轨：一是仅用合成数据训练，二是合成数据与少量真实数据结合。九组团队采用不同分割模型（如transformer、轻量网络、生成框架指导模型）在三组包含合成与真实样本的测试集上比拼。

Result: 完全基于合成数据训练的模型在合成数据测试集上F1分数可超0.8，有专门策略支持时表现更佳。混合数据轨道的指标提升主要受方法设计驱动，非真实数据比例。

Conclusion: 合成数据能有效替代真实眼部数据用于隐私敏感生物识别建模，合理方法比有无真实数据影响更大，推动了隐私友好型模型开发。

Abstract: This paper presents a summary of the 2025 Sclera Segmentation Benchmarking
Competition (SSBC), which focused on the development of privacy-preserving
sclera-segmentation models trained using synthetically generated ocular images.
The goal of the competition was to evaluate how well models trained on
synthetic data perform in comparison to those trained on real-world datasets.
The competition featured two tracks: $(i)$ one relying solely on synthetic data
for model development, and $(ii)$ one combining/mixing synthetic with (a
limited amount of) real-world data. A total of nine research groups submitted
diverse segmentation models, employing a variety of architectural designs,
including transformer-based solutions, lightweight models, and segmentation
networks guided by generative frameworks. Experiments were conducted across
three evaluation datasets containing both synthetic and real-world images,
collected under diverse conditions. Results show that models trained entirely
on synthetic data can achieve competitive performance, particularly when
dedicated training strategies are employed, as evidenced by the top performing
models that achieved $F_1$ scores of over $0.8$ in the synthetic data track.
Moreover, performance gains in the mixed track were often driven more by
methodological choices rather than by the inclusion of real data, highlighting
the promise of synthetic data for privacy-aware biometric development. The code
and data for the competition is available at:
https://github.com/dariant/SSBC_2025.

</details>


### [83] [Axis-level Symmetry Detection with Group-Equivariant Representation](https://arxiv.org/abs/2508.10740)
*Wongyun Yu,Ahyun Seo,Minsu Cho*

Main category: cs.CV

TL;DR: 该论文提出了一种基于几何原语的对称轴检测新框架，在反射和旋转对称检测上获得了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 对称性在计算机视觉中十分重要，但在复杂场景中检测对称仍然困难。现有基于热力图的方法能定位对称轴的潜在区域，但轴的精确检测效果有限。

Method: 方法以对称类型（反射与旋转）分别建模为直线与点，并提出对双分支结构，每个分支采用与二面体群等变的特征提取。针对反射对称，引入方向锚点以实现特定方向的精确检测，并利用反射匹配模块评估候选轴两侧的模式相似度。针对旋转对称，通过设定固定角度间隔的旋转匹配机制定位旋转对称中心。

Result: 大量实验表明，该方法在对称轴检测任务上取得了SOTA（最先进）表现，优于现有主流方法。

Conclusion: 将对称性检测建模为显式几何原语的检测任务，并结合与二面体群等变的特征提取，有效提升了在复杂场景下的对称识别能力。

Abstract: Symmetry is a fundamental concept that has been extensively studied, yet
detecting it in complex scenes remains a significant challenge in computer
vision. Recent heatmap-based approaches can localize potential regions of
symmetry axes but often lack precision in identifying individual axes. In this
work, we propose a novel framework for axis-level detection of the two most
common symmetry types-reflection and rotation-by representing them as explicit
geometric primitives, i.e. lines and points. Our method employs a dual-branch
architecture that is equivariant to the dihedral group, with each branch
specialized to exploit the structure of dihedral group-equivariant features for
its respective symmetry type. For reflection symmetry, we introduce
orientational anchors, aligned with group components, to enable
orientation-specific detection, and a reflectional matching that measures
similarity between patterns and their mirrored counterparts across candidate
axes. For rotational symmetry, we propose a rotational matching that compares
patterns at fixed angular intervals to identify rotational centers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance,
outperforming existing approaches.

</details>


### [84] [Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection](https://arxiv.org/abs/2508.10741)
*Lixin Jia,Zhiqing Guo,Gaobo Yang,Liejun Wang,Keqin Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度伪造检测策略Forgy Guided Learning (FGL) 和双感知网络DPNet，有效提升了模型在未知伪造技术下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在特定数据集上表现良好，但面对新型或未知伪造手法时，跨领域的泛化能力差。随着伪造技术快速迭代，现有检测依赖的共性伪造痕迹已难以适应新的攻击方式，因此急需增强检测技术的泛化性。

Method: 提出FGL策略，用于捕获已知与未知伪造手法间的差异，并实时动态调整模型学习；设计DPNet，从频域和空间域捕获伪造特征，并利用图卷积进一步理解特征间关联，实现对伪造痕迹的深度感知。

Result: 大量实验表明，提出的方法在不同场景下具备良好的泛化性能，能够有效应对未知伪造技术的挑战。

Conclusion: 所提策略与网络结构实现了对新型深度伪造手法的有效检测与适应，为提升深度伪造检测系统的鲁棒性和实用性提供了有力支撑。

Abstract: The emergence of deepfake technology has introduced a range of societal
problems, garnering considerable attention. Current deepfake detection methods
perform well on specific datasets, but exhibit poor performance when applied to
datasets with unknown forgery techniques. Moreover, as the gap between emerging
and traditional forgery techniques continues to widen, cross-domain detection
methods that rely on common forgery traces are becoming increasingly
ineffective. This situation highlights the urgency of developing deepfake
detection technology with strong generalization to cope with fast iterative
forgery techniques. To address these challenges, we propose a Forgery Guided
Learning (FGL) strategy designed to enable detection networks to continuously
adapt to unknown forgery techniques. Specifically, the FGL strategy captures
the differential information between known and unknown forgery techniques,
allowing the model to dynamically adjust its learning process in real time. To
further improve the ability to perceive forgery traces, we design a Dual
Perception Network (DPNet) that captures both differences and relationships
among forgery traces. In the frequency stream, the network dynamically
perceives and extracts discriminative features across various forgery
techniques, establishing essential detection cues. These features are then
integrated with spatial features and projected into the embedding space. In
addition, graph convolution is employed to perceive relationships across the
entire feature space, facilitating a more comprehensive understanding of
forgery trace correlations. Extensive experiments show that our approach
generalizes well across different scenarios and effectively handles unknown
forgery challenges, providing robust support for deepfake detection. Our code
is available on https://github.com/vpsg-research/FGL.

</details>


### [85] [An Efficient Model-Driven Groupwise Approach for Atlas Construction](https://arxiv.org/abs/2508.10743)
*Ziwei Zou,Bei Zou,Xiaoyan Kui,Wenqi Lu,Haoran Dou,Arezoo Zakeri,Timothy Cootes,Alejandro F Frangi,Jinming Duan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的医学图像群体配准和图谱构建方法DARC，提升了大规模3D数据处理的效率与准确性，并可支持下游分割和形态合成任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动配准方法虽在成对配准取得进展，但需要大量训练数据、泛化性差，且在群体配准时缺乏推断阶段，难以实用。模型驱动方法虽具理论基础且数据高效，无需训练，但在大规模3D数据上常遇到优化和扩展性难题。解决这两类方法的局限性是本文的研究动机。

Method: 提出了DARC（Diffeomorphic Atlas Registration via Coordinate descent）框架，通过坐标下降策略结合中心性激活函数，实现高效、无偏和可微分的医学图谱构建。该方法支持多种图像差异性度量，并可处理任意数量的3D图像，且不受GPU显存限制。

Result: DARC在生成具高解剖保真度的医学图谱方面效果优越。在下游一站式分割实验中，仅利用图谱标签进行反向传播分割，超越现有少样本方法。此外，通过变形场合成，能够实现多样化的医学形态合成。

Conclusion: DARC为医学图谱构建及其应用提供了一个灵活、通用且高效的模型驱动解决方案，兼具优良泛化性与资源利用效率，在分割和形态生成等任务中表现突出。

Abstract: Atlas construction is fundamental to medical image analysis, offering a
standardized spatial reference for tasks such as population-level anatomical
modeling. While data-driven registration methods have recently shown promise in
pairwise settings, their reliance on large training datasets, limited
generalizability, and lack of true inference phases in groupwise contexts
hinder their practical use. In contrast, model-driven methods offer
training-free, theoretically grounded, and data-efficient alternatives, though
they often face scalability and optimization challenges when applied to large
3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration
via Coordinate descent), a novel model-driven groupwise registration framework
for atlas construction. DARC supports a broad range of image dissimilarity
metrics and efficiently handles arbitrary numbers of 3D images without
incurring GPU memory issues. Through a coordinate descent strategy and a
centrality-enforcing activation function, DARC produces unbiased, diffeomorphic
atlases with high anatomical fidelity. Beyond atlas construction, we
demonstrate two key applications: (1) One-shot segmentation, where labels
annotated only on the atlas are propagated to subjects via inverse
deformations, outperforming state-of-the-art few-shot methods; and (2) shape
synthesis, where new anatomical variants are generated by warping the atlas
mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a
flexible, generalizable, and resource-efficient framework for atlas
construction and applications.

</details>


### [86] [From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models](https://arxiv.org/abs/2508.10770)
*Tiancheng Han,Yunfei Gao,Yong Li,Wuzhou Yu,Qiaosheng Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: 本文对主流视觉语言模型（VLMs）在时空物理推理任务上的表现进行了系统分析，发现其表现不足。通过对Qwen2.5-VL-7B进行监督微调和基于规则的强化学习，显著提升了其时空物理推理能力，但模型在新物理场景下的泛化仍有限。


<details>
  <summary>Details</summary>
Motivation: 时空物理推理是理解现实世界物理规律的重要基础，对构建强健的世界模型具有关键作用。尽管现有视觉语言模型在多模态数学和空间理解等领域取得了突破，但在时空物理推理上的能力尚未得到充分探索，因此亟需诊断分析和能力提升。

Method: 作者对主流VLMs开展全面诊断分析，剖析当前模型在时空物理推理上的表现。针对发现的问题，对Qwen2.5-VL-7B进行两步提升：首先用监督微调优化模型表现，再用基于规则的强化学习进一步加强其推理能力。

Result: 经过上述方法，Qwen2.5-VL-7B在时空物理推理方面获得显著提升，并超越了同类领先的商业模型。但模型在应对全新物理场景时泛化能力依然有限。

Conclusion: 现有VLMs在时空物理推理方面存在明显不足，经过针对性训练虽有进步，但要实现更强泛化能力仍需创新型方法。

Abstract: Spatio-physical reasoning, a foundation capability for understanding the real
physics world, is a critical step towards building robust world models. While
recent vision language models (VLMs) have shown remarkable progress in
specialized domains like multimodal mathematics and pure spatial understanding,
their capability for spatio-physical reasoning remains largely unexplored. This
paper provides a comprehensive diagnostic analysis of mainstream VLMs,
revealing that current models perform inadequately on this crucial task.
Further detailed analysis shows that this underperformance is largely
attributable to biases caused by human-like prior and a lack of deep reasoning.
To address these challenges, we apply supervised fine-tuning followed by
rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant
improvements in spatio-physical reasoning capabilities and surpassing leading
proprietary models. Nevertheless, despite this success, the model's
generalization to new physics scenarios remains limited -- underscoring the
pressing need for new approaches in spatio-physical reasoning.

</details>


### [87] [AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences](https://arxiv.org/abs/2508.10771)
*Jieyu Li,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: AEGIS是一个大规模的、高度复杂的AI生成视频检测基准，专为测试现代视觉-语言模型对高真实感伪造视频的识别能力而设计，极大推进了视频真伪检测领域。


<details>
  <summary>Details</summary>
Motivation: 现有视频真伪检测基准在真实性、规模、复杂性等方面存在明显不足，难以评估现代AI模型对高级伪造视频的识别能力。与此同时，AI生成内容持续进步，带来严峻的社会和数字安全风险，亟需更具挑战性的评测工具。

Method: 作者提出AEGIS基准，包含10,000余条高质量真/伪视频，涵盖主流开源和闭源生成模型（如Stable Video Diffusion、CogVideoX-5B、KLing、Sora），设置了困难子集、鲁棒性评估，并配套语义真实性描述、运动特征、低级视觉特征等多模态标注。

Result: 在AEGIS的高难度子集上测试主流视觉-语言模型，发现它们的检测能力有限，难以应对该数据集中的复杂与真实性挑战，反映出AEGIS在泛化性和真实性方面的独特优势。

Conclusion: AEGIS为视频真实性检测领域提供了不可或缺的测试基准，有助于推动开发更鲁棒、更具普适性和实际应用能力的视频伪造检测方法，应对真实世界中的伪造威胁。

Abstract: Recent advances in AI-generated content have fueled the rise of highly
realistic synthetic videos, posing severe risks to societal trust and digital
integrity. Existing benchmarks for video authenticity detection typically
suffer from limited realism, insufficient scale, and inadequate complexity,
failing to effectively evaluate modern vision-language models against
sophisticated forgeries. To address this critical gap, we introduce AEGIS, a
novel large-scale benchmark explicitly targeting the detection of
hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises
over 10,000 rigorously curated real and synthetic videos generated by diverse,
state-of-the-art generative models, including Stable Video Diffusion,
CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary
architectures. In particular, AEGIS features specially constructed challenging
subsets enhanced with robustness evaluation. Furthermore, we provide multimodal
annotations spanning Semantic-Authenticity Descriptions, Motion Features, and
Low-level Visual Features, facilitating authenticity detection and supporting
downstream tasks such as multimodal fusion and forgery localization. Extensive
experiments using advanced vision-language models demonstrate limited detection
capabilities on the most challenging subsets of AEGIS, highlighting the
dataset's unique complexity and realism beyond the current generalization
capabilities of existing models. In essence, AEGIS establishes an indispensable
evaluation benchmark, fundamentally advancing research toward developing
genuinely robust, reliable, broadly generalizable video authenticity detection
methodologies capable of addressing real-world forgery threats. Our dataset is
available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.

</details>


### [88] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: BLADE是一种用于加速扩散Transformer视频生成的创新型数据无关联合训练框架，通过自适应稀疏注意力和稀疏感知蒸馏，大幅提升推理速度，同时保持甚至提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer虽然能生成高质量视频，但推理速度慢且长序列时注意力成本高。现有加速方法（步骤蒸馏、稀疏注意力）各有不足，二者结合后效果有限或训练成本高，因此需要新的高效融合方案。

Method: 提出BLADE框架，包括：（1）自适应块状稀疏注意力（ASA），动态生成内容相关的稀疏掩码以关注重要时空特征；（2）基于轨迹分布匹配（TDM）的稀疏感知步骤蒸馏，将稀疏机制直接融入蒸馏流程，高效收敛，实现无数据联合训练。

Result: 在CogVideoX-5B和Wan2.1-1.3B等主流文本到视频模型上，BLADE实现了显著的推理加速（Wan2.1-1.3B提升14.10x，CogVideoX-5B提升8.89x），且生成质量在VBench-2.0等评测集上也有明显提升。

Conclusion: BLADE能兼顾速度与质量，打破现有视频扩散模型推理效率和生成效果的瓶颈，为长序列视频生成提供更优解决方案，具有实际落地与推广价值。

Abstract: Diffusion transformers currently lead the field in high-quality video
generation, but their slow iterative denoising process and prohibitive
quadratic attention costs for long sequences create significant inference
bottlenecks. While both step distillation and sparse attention mechanisms have
shown promise as independent acceleration strategies, effectively combining
these approaches presents critical challenges -- training-free integration
yields suboptimal results, while separately training sparse attention after
step distillation requires prohibitively expensive high-quality video data. To
overcome these limitations, we propose BLADE, an innovative data-free joint
training framework that introduces: (1) an Adaptive Block-Sparse Attention
(ASA) mechanism for dynamically generating content-aware sparsity masks to
focus computation on salient spatiotemporal features, and (2) a sparsity-aware
step distillation paradigm built upon Trajectory Distribution Matching (TDM)
that directly incorporates sparsity into the distillation process rather than
treating it as a separate compression step, with fast convergence. We validate
BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework
demonstrates remarkable efficiency gains across different scales. On
Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a
50-step baseline. Moreover, on models such as CogVideoX-5B with short video
sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the
acceleration is accompanied by a consistent quality improvement. On the
VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from
0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further
corroborated by superior ratings in human evaluations. Our code and model
weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [89] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: 本文提出了TriFlowSR，一种全新的基于扩散模型的参考图像超分辨率（RefSR）框架，同时构建了首个超高清地标场景下的RefSR数据集Landmark-4K。该方法能够有效匹配低分辨率与参考高分辨率图像，实现更优的超分重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型RefSR方法多基于ControlNet，难以有效对齐低分辨率与参考高分辨率图像间的信息。此外，现有RefSR数据集分辨率低、质量差，不能为高质量重建提供足够细节。

Method: 作者设计了TriFlowSR框架，通过显式的图案匹配策略，提升低分辨率图像与高分辨率参考图像的有效对齐。同时提出Reference Matching Strategy，适应超高清、真实退化场景下的匹配需求。此外，作者新建了Landmark-4K数据集作为超高清RefSR基准。

Result: 实验结果显示，TriFlowSR在对齐与利用高分辨率参考图像的语义和纹理信息方面优于现有方法，在超高清真实场景下的超分辨重建任务取得更好效果。

Conclusion: TriFlowSR首次实现了基于扩散模型的超高清地标场景RefSR，在信息匹配和高质量重建方面大幅优于早先方法。Landmark-4K数据集为RefSR发展提供了新资源。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a
low-resolution (LR) image by utilizing the semantic and texture information
from an additional reference high-resolution (reference HR) image. Existing
diffusion-based RefSR methods are typically built upon ControlNet, which
struggles to effectively align the information between the LR image and the
reference HR image. Moreover, current RefSR datasets suffer from limited
resolution and poor image quality, resulting in the reference images lacking
sufficient fine-grained details to support high-quality restoration. To
overcome the limitations above, we propose TriFlowSR, a novel framework that
explicitly achieves pattern matching between the LR image and the reference HR
image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for
Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios
with real-world degradation, in TriFlowSR, we design a Reference Matching
Strategy to effectively match the LR image with the reference HR image.
Experimental results show that our approach can better utilize the semantic and
texture information of the reference HR image compared to previous methods. To
the best of our knowledge, we propose the first diffusion-based RefSR pipeline
for ultra-high definition landmark scenarios under real-world degradation. Our
code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [90] [Cooperative Face Liveness Detection from Optical Flow](https://arxiv.org/abs/2508.10786)
*Artem Sokolov,Mikhail Nikitin,Anton Konushin*

Main category: cs.CV

TL;DR: 本论文提出了一种基于用户近距离移动脸部并结合光流分析的视频活体检测新方法，有效提升了对各种攻击方式的识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前存在的视频人脸活体检测方法对于高仿攻击（如屏幕显示、面具等）易受骗，且大多为被动检测，容易被破解。因此需要一种更稳健且难以被攻击的活体检测方式。

Method: 设计了一种用户主动配合，将正面脸部缓慢靠近摄像头的操作协议，并利用神经网络进行光流估算，提取人脸体积等空间信息；结合RGB图像和预测光流，输入神经网络分类器，以区分真实人脸和攻击样本。

Result: 实验证明，该方法在区分真实人脸和包括照片、屏幕、面具、视频重放等在内的多种攻击方面有显著提升，效果优于传统被动检测方法。

Conclusion: 引入用户配合和基于光流的特征提取方式，使得活体检测更加可靠，为实际安全应用提供了更有效的技术方案。

Abstract: In this work, we proposed a novel cooperative video-based face liveness
detection method based on a new user interaction scenario where participants
are instructed to slowly move their frontal-oriented face closer to the camera.
This controlled approaching face protocol, combined with optical flow analysis,
represents the core innovation of our approach. By designing a system where
users follow this specific movement pattern, we enable robust extraction of
facial volume information through neural optical flow estimation, significantly
improving discrimination between genuine faces and various presentation attacks
(including printed photos, screen displays, masks, and video replays). Our
method processes both the predicted optical flows and RGB frames through a
neural classifier, effectively leveraging spatial-temporal features for more
reliable liveness detection compared to passive methods.

</details>


### [91] [VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation](https://arxiv.org/abs/2508.10794)
*De-Xing Huang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Tian-Yu Xiang,Rui-Ze Ma,Nu-Fang Xiao,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 该论文提出了一种专为X光血管造影图像设计的自监督学习方法VasoMIM，通过引入血管解剖知识改进了血管分割的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的X光血管分割任务中，标注数据稀缺，采用自监督方法可以利用大量无标签数据。但传统方法受制于前背景严重不平衡，难以提取有效血管特征。

Method: VasoMIM包括两大创新：一是解剖引导的掩码策略，优先遮盖包含血管的区域，强迫模型关注血管结构；二是解剖一致性损失，保证重建图像与原图在血管结构语义上一致，提高血管特征判别性。

Result: VasoMIM在三个不同数据集上实现了当前最好的血管分割效果。

Conclusion: VasoMIM有效提升了X光血管分割的性能，为X光血管造影图像分析提供了新方法，具有较大实际应用前景。

Abstract: Accurate vessel segmentation in X-ray angiograms is crucial for numerous
clinical applications. However, the scarcity of annotated data presents a
significant challenge, which has driven the adoption of self-supervised
learning (SSL) methods such as masked image modeling (MIM) to leverage
large-scale unlabeled data for learning transferable representations.
Unfortunately, conventional MIM often fails to capture vascular anatomy because
of the severe class imbalance between vessel and background pixels, leading to
weak vascular representations. To address this, we introduce Vascular
anatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored
for X-ray angiograms that explicitly integrates anatomical knowledge into the
pre-training process. Specifically, it comprises two complementary components:
anatomy-guided masking strategy and anatomical consistency loss. The former
preferentially masks vessel-containing patches to focus the model on
reconstructing vessel-relevant regions. The latter enforces consistency in
vascular semantics between the original and reconstructed images, thereby
improving the discriminability of vascular representations. Empirically,
VasoMIM achieves state-of-the-art performance across three datasets. These
findings highlight its potential to facilitate X-ray angiogram analysis.

</details>


### [92] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为OF-Diff的新型扩散模型，实现了高保真、可控的遥感图像生成，极大提升了对象生成的准确度与细节保真度。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中需求对细节和对象进行精确还原，现有扩散模型在形态细节刻画方面不足，影响下游目标检测等任务的准确性与鲁棒性。

Method: 1. 首次基于布局提取对象形状先验用于扩散模型；2. 设计了双分支扩散网络和扩散一致性损失，无需真实图像即可采样生成高保真遥感图像；3. 引入DDPO微调扩散过程，提升生成图像多样性和语义一致性。

Result: OF-Diff在多项遥感图像质量评测指标上优于主流方法，尤其在多形态与小目标类别（如飞机、船只、车辆）上，mAP分别提升了8.3%、7.7%和4.0%。

Conclusion: 提出的OF-Diff方法有效提升了遥感图像中对象的生成保真度与检测性能，展示了在遥感图像生成领域的显著应用潜力。

Abstract: High-precision controllable remote sensing image generation is both
meaningful and challenging. Existing diffusion models often produce
low-fidelity images due to their inability to adequately capture morphological
details, which may affect the robustness and reliability of object detection
models. To enhance the accuracy and fidelity of generated objects in remote
sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which
effectively improves the fidelity of generated objects. Specifically, we are
the first to extract the prior shapes of objects based on the layout for
diffusion models in remote sensing. Then, we introduce a dual-branch diffusion
model with diffusion consistency loss, which can generate high-fidelity remote
sensing images without providing real images during the sampling phase.
Furthermore, we introduce DDPO to fine-tune the diffusion process, making the
generated remote sensing images more diverse and semantically consistent.
Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art
methods in the remote sensing across key quality metrics. Notably, the
performance of several polymorphic and small object classes shows significant
improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for
airplanes, ships, and vehicles, respectively.

</details>


### [93] [Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops](https://arxiv.org/abs/2508.10817)
*Anand Kumar,Harminder Pal Monga,Tapasi Brahma,Satyam Kalra,Navas Sherif*

Main category: cs.CV

TL;DR: 本文开发了一套可在移动设备上运行的植物病害早期检测系统，融合多个公开数据集并实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重威胁全球粮食安全，早期准确识别对保障农业生产至关重要。现有方法在移动端部署受限，需开发高效轻量的分类系统。

Method: 作者将Plant Doc、PlantVillage和PlantWild三大数据集合并，建立大规模多病害数据集，分别测试MobileNet系列和EfficientNet-B0/B1等轻量神经网络结构，并在移动端进行部署分析。

Result: 各模型均取得较好效果，其中EfficientNet-B1在准确率（94.7%）及计算效率之间取得最优平衡，尤其适合资源受限的移动设备。

Conclusion: 轻量高效的EfficientNet-B1模型在植物病害多分类任务上表现突出，有望在现实农业场景下作为移动应用早期检测工具广泛推广。

Abstract: Plant diseases are a major threat to food security globally. It is important
to develop early detection systems which can accurately detect. The advancement
in computer vision techniques has the potential to solve this challenge. We
have developed a mobile-friendly solution which can accurately classify 101
plant diseases across 33 crops. We built a comprehensive dataset by combining
different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are
for the same purpose. We evaluated performance across several lightweight
architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and
EfficientNet-B0, B1 - specifically chosen for their efficiency on
resource-constrained devices. The results were promising, with EfficientNet-B1
delivering our best performance at 94.7% classification accuracy. This
architecture struck an optimal balance between accuracy and computational
efficiency, making it well-suited for real-world deployment on mobile devices.

</details>


### [94] [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/abs/2508.10833)
*Zhangxuan Gu,Zhengwen Zeng,Zhenyu Xu,Xingran Zhou,Shuheng Shen,Yunfei Liu,Beitong Zhou,Changhua Meng,Tianyu Xia,Weizhi Chen,Yue Wen,Jingya Dou,Fei Tang,Jinzhen Lin,Yulin Liu,Zhenlin Guo,Yichen Gong,Heng Jia,Changlong Gao,Yuan Guo,Yong Deng,Zhenyu Guo,Liang Chen,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus 是一种基于多模态大模型、仅需 UI 截图作为输入的智能体模型，在 UI 定位与导航任务上刷新了 SOTA，并开源相关模型与代码。


<details>
  <summary>Details</summary>
Motivation: 当前的 UI 任务（如 UI 元素定位与导航）对模型能力要求极高，目前公开模型仍存在准确率不高、泛化性不足等问题。因此，作者希望设计一种能高效学习、泛化能力强的新一代 UI 智能体。

Method: 作者提出了基于 Qwen2.5-VL 进行强化学习微调（RFT），结合高质量数据、设计专用奖励函数、精细化数据清洗，并引入自进化轨迹历史对齐及稀疏动作增强方法来优化推理过程和提升模型表现。

Result: UI-Venus 7B/72B 版本分别在 Screenspot-V2 / Pro 上获得 94.1% / 50.8% 和 95.3% / 61.9%的UI grounding准确率，显著优于现有开源/闭源模型。在 AndroidWorld UI 导航任务中分别达到 49.1% 和 65.9% 成功率，同样领先。

Conclusion: UI-Venus 成功树立了 UI grounding 与导航任务新的 SOTA，公开了顶尖的开源UI智能体、数据清洗流程与自进化算法，为 UI 智能体领域后续研究和应用提供了重要基础。

Abstract: We present UI-Venus, a native UI agent that takes only screenshots as input
based on a multimodal large language model. UI-Venus achieves SOTA performance
on both UI grounding and navigation tasks using only several hundred thousand
high-quality training samples through reinforcement finetune (RFT) based on
Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /
50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,
Screenspot-V2 / Pro, surpassing the previous SOTA baselines including
open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and
planing ability, we also evaluate it on the AndroidWorld, an online UI
navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%
success rate, also beating existing models.To achieve this, we introduce
carefully designed reward functions for both UI grounding and navigation tasks
and corresponding efficient data cleaning strategies.To further boost
navigation performance, we propose Self-Evolving Trajectory History Alignment
\& Sparse Action Enhancement that refine historical reasoning traces and
balances the distribution of sparse but critical actions, leading to more
coherent planning and better generalization in complex UI tasks. Our
contributions include the publish of SOTA open-source UI agents, comprehensive
data cleaning protocols and a novel self-evolving framework for improving
navigation performance, which encourage further research and development in the
community. Code is available at https://github.com/antgroup/UI-Venus.

</details>


### [95] [Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning](https://arxiv.org/abs/2508.10838)
*Peng Xu,Zhiyu Xiang,Jingyun Fu,Tianyu Pu,Kai Wang,Chaojie Ji,Tingming Bai,Eryun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BaCon-Stereo的自监督立体匹配框架，有效解决了自监督方法在遮挡区域失效的问题，并在公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前自监督立体匹配方法依赖于光度一致性假设，但在遮挡区域该假设失效，导致匹配困难。本文旨在解决遮挡区域自监督学习的准确性问题。

Method: 提出了BaCon-Stereo对比学习框架，采用teacher-student结构和多基线输入，使教师网络可以在学生网络遮挡的区域提供更准确的预测，通过尺度变换实现监督。同时引入了遮挡感知注意力图。作者还合成了新的多基线数据集BaCon-20k以支持训练。

Result: 实验显示，BaCon-Stereo在遮挡与非遮挡区域均提升了预测准确性，泛化能力和鲁棒性强，在KITTI 2015和2012两个主流基准上超过了现有自监督方法。

Conclusion: BaCon-Stereo有效提升了自监督立体匹配在所有区域的表现，尤其是在遮挡区域，对今后的立体匹配研究具有推动作用。

Abstract: Current self-supervised stereo matching relies on the photometric consistency
assumption, which breaks down in occluded regions due to ill-posed
correspondences. To address this issue, we propose BaCon-Stereo, a simple yet
effective contrastive learning framework for self-supervised stereo network
training in both non-occluded and occluded regions. We adopt a teacher-student
paradigm with multi-baseline inputs, in which the stereo pairs fed into the
teacher and student share the same reference view but differ in target views.
Geometrically, regions occluded in the student's target view are often visible
in the teacher's, making it easier for the teacher to predict in these regions.
The teacher's prediction is rescaled to match the student's baseline and then
used to supervise the student. We also introduce an occlusion-aware attention
map to better guide the student in learning occlusion completion. To support
training, we synthesize a multi-baseline dataset BaCon-20k. Extensive
experiments demonstrate that BaCon-Stereo improves prediction in both occluded
and non-occluded regions, achieves strong generalization and robustness, and
outperforms state-of-the-art self-supervised methods on both KITTI 2015 and
2012 benchmarks. Our code and dataset will be released upon paper acceptance.

</details>


### [96] [Generalizable Federated Learning using Client Adaptive Focal Modulation](https://arxiv.org/abs/2508.10840)
*Tajamul Ashraf,Iqra Altaf Gillani*

Main category: cs.CV

TL;DR: 提出一种用于联邦学习的改进型变换器框架AdaptFED，提升个性化和泛化能力，并降低通信开销，在多个数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理非独立同分布（non-IID）和跨领域数据时性能有限，通信开销高，且个性化泛化能力不足，需要更强大的框架。

Method: 基于前期TransFed框架，提出AdaptFED：1）引入融合任务感知的客户端嵌入，提升个性化调制能力；2）优化理论适应性性能界；3）跨更多数据模态（含时序、多语言）做实证验证，并引入低秩超网络调节，减少通信，适合资源受限设备。

Result: 在8个不同数据集上的大量实验显示，AdaptFED在无源数据和跨任务联邦设置下，相比主流基线具有显著优势，并验证通信优化有效。

Conclusion: AdaptFED框架扩展并增强了变换器在联邦学习中的调制能力，实现了更优的自适应、可扩展与泛化性能，为个性化联邦变换器系统的实用化奠定基础。

Abstract: Federated learning (FL) has proven essential for privacy-preserving,
collaborative training across distributed clients. Our prior work, TransFed,
introduced a robust transformer-based FL framework that leverages a
learn-to-adapt hypernetwork to generate personalized focal modulation layers
per client, outperforming traditional methods in non-IID and cross-domain
settings. In this extended version, we propose AdaptFED, where we deepen the
investigation of focal modulation in generalizable FL by incorporating: (1) a
refined adaptation strategy that integrates task-aware client embeddings to
personalize modulation dynamics further, (2) enhanced theoretical bounds on
adaptation performance, and (3) broader empirical validation across additional
modalities, including time-series and multilingual data. We also introduce an
efficient variant of TransFed that reduces server-client communication overhead
via low-rank hypernetwork conditioning, enabling scalable deployment in
resource-constrained environments. Extensive experiments on eight diverse
datasets reaffirm the superiority of our method over state-of-the-art
baselines, particularly in source-free and cross-task federated setups. Our
findings not only extend the capabilities of focal modulation in FL but also
pave the way for more adaptive, scalable, and generalizable transformer-based
federated systems. The code is available at
http://github.com/Tajamul21/TransFed

</details>


### [97] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: 该论文提出了PhysHPO框架，通过多层次细粒度偏好对齐与自动化优质数据筛选，显著提升了视频生成的物理合理性和整体质量。


<details>
  <summary>Details</summary>
Motivation: 虽然视频生成技术进步显著，但生成符合物理规律的视频仍然是实现真实感和高精度应用的重大挑战。当前方法在物理可行性与细节一致性方面存在不足，限制了其应用范围。

Method: 作者提出PhysHPO，一个分层交叉模态直接偏好优化框架，针对视频生成进行细粒度多层偏好对齐，具体包括：（a）实例级内容对齐，（b）状态级时序一致性，（c）运动级真实动态，（d）语义级逻辑一致。同时引入自动化数据筛选流程，从大规模数据集中提取“优质”样本代替传统人工构建数据集的方法。

Result: 在物理相关与通用视频生成基准上，PhysHPO能显著提升物理合理性及视频质量，优于现有高级生成模型。

Conclusion: 这是首次针对视频生成领域提出精细偏好对齐与自动优质数据筛选方案，为更真实、符合人类偏好的视频生成范式奠定了基础。

Abstract: Recent advancements in video generation have enabled the creation of
high-quality, visually compelling videos. However, generating videos that
adhere to the laws of physics remains a critical challenge for applications
requiring realism and accuracy. In this work, we propose PhysHPO, a novel
framework for Hierarchical Cross-Modal Direct Preference Optimization, to
tackle this challenge by enabling fine-grained preference alignment for
physically plausible video generation. PhysHPO optimizes video alignment across
four hierarchical granularities: a) Instance Level, aligning the overall video
content with the input prompt; b) State Level, ensuring temporal consistency
using boundary frames as anchors; c) Motion Level, modeling motion trajectories
for realistic dynamics; and d) Semantic Level, maintaining logical consistency
between narrative and visuals. Recognizing that real-world videos are the best
reflections of physical phenomena, we further introduce an automated data
selection pipeline to efficiently identify and utilize "good data" from
existing large-scale text-video datasets, thereby eliminating the need for
costly and time-intensive dataset construction. Extensive experiments on both
physics-focused and general capability benchmarks demonstrate that PhysHPO
significantly improves physical plausibility and overall video generation
quality of advanced models. To the best of our knowledge, this is the first
work to explore fine-grained preference alignment and data selection for video
generation, paving the way for more realistic and human-preferred video
generation paradigms.

</details>


### [98] [Performance of GPT-5 in Brain Tumor MRI Reasoning](https://arxiv.org/abs/2508.10865)
*Mojtaba Safari,Shansong Wang,Mingzhe Hu,Zach Eidex,Qiang Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本研究测试了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在脑肿瘤MRI视觉问答（VQA）任务中的表现，发现GPT-5系列模型有一定准确率但未达到临床要求。


<details>
  <summary>Details</summary>
Motivation: 在神经肿瘤学中，MRI下准确区分不同类型脑肿瘤对于治疗方案制定非常关键。近年来，大语言模型具备了图片解读和自然语言推理能力，因此有望应用于医学影像辅助诊断。

Method: 研究使用来源于3个BraTS数据集的标准化脑肿瘤VQA基准（含GLI、MEN、MET三种肿瘤），每例均提供多序列三平面MRI图像和结构化临床特征，采用零样本链式思维（zero-shot chain-of-thought）让模型完成视觉识别与推理问答任务，并比较四种大模型的表现。

Result: GPT-5-mini宏平均准确率最高（44.19%），其次为GPT-5（43.71%）、GPT-4o（41.49%）和GPT-5-nano（35.85%）。模型在不同肿瘤亚型上的表现有差异，没有单一模型在所有类别中均占优。

Conclusion: GPT-5系列模型在结构化神经肿瘤VQA任务上表现出中等准确率，但距离临床应用标准尚有差距，临床上还不能直接使用。

Abstract: Accurate differentiation of brain tumor types on magnetic resonance imaging
(MRI) is critical for guiding treatment planning in neuro-oncology. Recent
advances in large language models (LLMs) have enabled visual question answering
(VQA) approaches that integrate image interpretation with natural language
reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and
GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor
Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain
metastases (MET). Each case included multi-sequence MRI triplanar mosaics and
structured clinical features transformed into standardized VQA items. Models
were assessed in a zero-shot chain-of-thought setting for accuracy on both
visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest
macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),
and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single
model dominating across all cohorts. These findings suggest that GPT-5 family
models can achieve moderate accuracy in structured neuro-oncological VQA tasks,
but not at a level acceptable for clinical use.

</details>


### [99] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse是一个大规模高分辨率3D纹理数据集，收集了超过85.8万个独特高分辨率3D模型，为端到端高分辨率纹理生成提供了关键数据基础。


<details>
  <summary>Details</summary>
Motivation: 目前虽然已有大规模3D数据集推进了高分辨率几何体生成，但缺乏适合高分辨率3D纹理端到端生成的数据集。因此，需要构建一个包含高质量纹理和丰富素材的3D数据集来支持纹理生成、3D视觉和图形学等领域的研究。

Method: 研究者从Sketchfab收集并精心筛选了超过85.8万个独特高分辨率3D模型（包含1.6M实例），其中包含物理基础渲染（PBR）素材模型15.8万个，另外还构建了包括骨骼绑定和动画信息在内的专用子集，并为模型添加详细注释。

Result: 得到一个覆盖丰富细节的高分辨率3D模型数据集：TexVerse。覆盖了常规高分辨率模型、PBR材质模型、带骨骼的模型和带动画的模型，并为每个模型配有详尽的结构和特征标注。

Conclusion: TexVerse为高分辨率3D纹理合成、PBR材质开发、动画、以及多种3D视觉和图形学研究任务提供了高质量、广泛应用的数据资源。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution
textures. While recent advances in large-scale 3D datasets have enhanced
high-resolution geometry generation, creating high-resolution textures
end-to-end remains underexplored due to the lack of suitable datasets. TexVerse
fills this gap with a curated collection of over 858K unique high-resolution 3D
models sourced from Sketchfab, including more than 158K models with physically
based rendering (PBR) materials. Each model encompasses all of its
high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse
also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,
and TexVerse-Animation, with 54K animated models, both preserving original
skeleton and animation data uploaded by the user. We also provide detailed
model annotations describing overall characteristics, structural components,
and intricate features. TexVerse offers a high-quality data resource with
wide-ranging potential applications in texture synthesis, PBR material
development, animation, and various 3D vision and graphics tasks.

</details>


### [100] [Medico 2025: Visual Question Answering for Gastrointestinal Imaging](https://arxiv.org/abs/2508.10869)
*Sushant Gautam,Vajira Thambawita,Michael Riegler,Pål Halvorsen,Steven Hicks*

Main category: cs.CV

TL;DR: Medico 2025 挑战赛聚焦于消化道内镜图像的可解释视觉问答（VQA），希望通过大规模数据集推动医学图像分析中可解释AI的发展。


<details>
  <summary>Details</summary>
Motivation: 医疗领域AI系统在临床决策支持中需兼顾结果准确性与可解释性。本挑战旨在推动开发能输出医学推理解释的VQA模型，提高AI在医生中的信任度和实用性。

Method: 主办方构建了Kvasir-VQA-x1数据集，包含6500张内镜图像和近16万条复杂问答对。设置两个子任务：一是基于图像回答多样化视觉问题，二是生成多模态解释，为临床决策提供支撑。模型评估综合定量指标和专家人工可解释性评审。

Result: 该挑战赛为医学VQA建立了权威基准，提供任务指南、数据和评测机制，为医学图像AI研究者和开发者提供了切磋平台和统一评测方式。

Conclusion: Medico 2025 挑战鼓励开发更可靠、有解释能力的医学VQA系统，有望推动AI在医学影像分析和临床辅助决策领域的可信应用。

Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for
Gastrointestinal (GI) imaging, organized as part of the MediaEval task series.
The challenge focuses on developing Explainable Artificial Intelligence (XAI)
models that answer clinically relevant questions based on GI endoscopy images
while providing interpretable justifications aligned with medical reasoning. It
introduces two subtasks: (1) answering diverse types of visual questions using
the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to
support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500
images and 159,549 complex question-answer (QA) pairs, serves as the benchmark
for the challenge. By combining quantitative performance metrics and
expert-reviewed explainability assessments, this task aims to advance
trustworthy Artificial Intelligence (AI) in medical image analysis.
Instructions, data access, and an updated guide for participation are available
in the official competition repository:
https://github.com/simula/MediaEval-Medico-2025

</details>


### [101] [ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](https://arxiv.org/abs/2508.10881)
*Lingen Li,Guangzhi Wang,Zhaoyang Zhang,Yaowei Li,Xiaoyu Li,Qi Dou,Jinwei Gu,Tianfan Xue,Ying Shan*

Main category: cs.CV

TL;DR: ToonComposer 是一个将补帧和上色统一到后关键帧阶段的生成模型，能提升动画制作自动化和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统动画制作需要繁重的人工关键帧、补帧和上色操作，且现有AI方法通常各阶段独立处理，导致误差累积和画面瑕疵。

Method: ToonComposer统一补帧与上色，通过稀疏草图注入机制结合关键帧草图实现精准控制，并采用空间低秩适配器将现代视频基础模型适应卡通领域，维持其时序特性。仅需极少量草图和参考帧即可生成高质量动画，亦支持多帧草图以加强运动控制。

Result: 建立了PKBench基准数据集进行评估，实验表明ToonComposer在视觉质量、动作一致性和生产效率上均优于现有方法。

Conclusion: ToonComposer显著减少动画制作的人工负担，提升灵活性，为AI辅助动画制作提供了更优、更实用的解决方案。

Abstract: Traditional cartoon and anime production involves keyframing, inbetweening,
and colorization stages, which require intensive manual effort. Despite recent
advances in AI, existing methods often handle these stages separately, leading
to error accumulation and artifacts. For instance, inbetweening approaches
struggle with large motions, while colorization methods require dense per-frame
sketches. To address this, we introduce ToonComposer, a generative model that
unifies inbetweening and colorization into a single post-keyframing stage.
ToonComposer employs a sparse sketch injection mechanism to provide precise
control using keyframe sketches. Additionally, it uses a cartoon adaptation
method with the spatial low-rank adapter to tailor a modern video foundation
model to the cartoon domain while keeping its temporal prior intact. Requiring
as few as a single sketch and a colored reference frame, ToonComposer excels
with sparse inputs, while also supporting multiple sketches at any temporal
location for more precise motion control. This dual capability reduces manual
workload and improves flexibility, empowering artists in real-world scenarios.
To evaluate our model, we further created PKBench, a benchmark featuring
human-drawn sketches that simulate real-world use cases. Our evaluation
demonstrates that ToonComposer outperforms existing methods in visual quality,
motion consistency, and production efficiency, offering a superior and more
flexible solution for AI-assisted cartoon production.

</details>


### [102] [STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer](https://arxiv.org/abs/2508.10893)
*Yushi Lan,Yihang Luo,Fangzhou Hong,Shangchen Zhou,Honghua Chen,Zhaoyang Lyu,Shuai Yang,Bo Dai,Chen Change Loy,Xingang Pan*

Main category: cs.CV

TL;DR: STream3R是一种新颖的基于Transformer的3D重建方法，通过流式方式高效处理图像序列，在静态和动态场景下都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D重建方法要么计算开销大，要么其内存机制无法很好扩展至长序列，尤其在动态场景中表现不佳。作者希望提出一种高效、具备扩展性的重建方法，并适应多样化场景。

Method: STream3R将点云预测转化为仅解码器的Transformer问题，利用因果注意力机制流式处理图像序列，并从大规模3D数据中学习几何先验。该方法还兼容大模型训练基础设施，便于大规模预训练及微调。

Result: 通过大量实验，STream3R在静态与动态场景基准测试中持续优于已有工作，展示了优异的泛化能力和实时性。

Conclusion: STream3R展示了因果Transformer模型在线3D感知中的潜力，为流式场景下的实时3D理解提供了新的方向。

Abstract: We present STream3R, a novel approach to 3D reconstruction that reformulates
pointmap prediction as a decoder-only Transformer problem. Existing
state-of-the-art methods for multi-view reconstruction either depend on
expensive global optimization or rely on simplistic memory mechanisms that
scale poorly with sequence length. In contrast, STream3R introduces an
streaming framework that processes image sequences efficiently using causal
attention, inspired by advances in modern language modeling. By learning
geometric priors from large-scale 3D datasets, STream3R generalizes well to
diverse and challenging scenarios, including dynamic scenes where traditional
methods often fail. Extensive experiments show that our method consistently
outperforms prior work across both static and dynamic scene benchmarks.
Moreover, STream3R is inherently compatible with LLM-style training
infrastructure, enabling efficient large-scale pretraining and fine-tuning for
various downstream 3D tasks. Our results underscore the potential of causal
Transformer models for online 3D perception, paving the way for real-time 3D
understanding in streaming environments. More details can be found in our
project page: https://nirvanalan.github.io/projects/stream3r.

</details>


### [103] [MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data](https://arxiv.org/abs/2508.10894)
*Antoine Labatie,Michael Vaccaro,Nina Lardiere,Anatol Garioud,Nicolas Gonthier*

Main category: cs.CV

TL;DR: 提出了MAESTRO方法，改进自监督学习在遥感多模态、多时相和多光谱数据上的表现，并在多个公开数据集上达到了新的最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以直接适用于地球观测（遥感）数据，因为这些数据具有多模态、多时相与多光谱等特殊性质，需要专门的模型和训练策略。

Method: 本文系统性比较了不同数据融合策略与重构目标归一化方案，基于研究结果设计了MAESTRO——一种基于Masked Autoencoder（MAE）框架、融合策略优化、并引入光谱先验的自监督学习方法。

Result: 在四个遥感数据集上的实验表明，MAESTRO在依赖多时相动态信息的任务上刷新了最新性能记录，在依赖单时相数据的任务上也表现优异。

Conclusion: MAESTRO方法通过专门的融合与归一化机制，有效提升了自监督学习在遥感多模态数据上的通用性和效果。

Abstract: Self-supervised learning holds great promise for remote sensing, but standard
self-supervised methods must be adapted to the unique characteristics of Earth
observation data. We take a step in this direction by conducting a
comprehensive benchmark of fusion strategies and reconstruction target
normalization schemes for multimodal, multitemporal, and multispectral Earth
observation data. Based on our findings, we propose MAESTRO, a novel adaptation
of the Masked Autoencoder, featuring optimized fusion strategies and a tailored
target normalization scheme that introduces a spectral prior as a
self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO
sets a new state-of-the-art on tasks that strongly rely on multitemporal
dynamics, while remaining highly competitive on tasks dominated by a single
mono-temporal modality. Code to reproduce all our experiments is available at
https://github.com/ignf/maestro.

</details>


### [104] [ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning](https://arxiv.org/abs/2508.10896)
*Jongseo Lee,Kyungho Bae,Kyle Min,Gyeong-Moon Park,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为ESSENTIAL的新方法，有效平衡了视频类增量学习中的内存效率与性能，在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 视频类增量学习（VCIL）面临灾难性遗忘问题，现有技术在内存效率和性能之间难以兼顾，一种方法需存储大量样本导致内存消耗大，另一种则存储稀疏样本但损失重要时序信息、表现较差。作者旨在解决这一内存与性能的权衡难题。

Method: 作者提出ESSENTIAL方法，结合episodic memory（用于存储时序稀疏特征）和semantic memory（以可学习提示的方式存储通用知识）。通过新颖的memory retrieval模块，将episodic memory与semantic prompts结合，用cross-attention实现稀疏特征向密集特征的检索。

Result: 在UCF-101、HMDB51、Something-Something-V2（TCD基准）及UCF-101、ActivityNet、Kinetics-400（vCLIMB基准）等多个数据集上，ESSENTIAL方法大幅减少内存占用的情况下，依然获得了有竞争力的性能。

Conclusion: ESSENTIAL方法在平衡内存效率和性能方面表现优异，为视频类增量学习领域提供了新思路和有效解决方案。

Abstract: In this work, we tackle the problem of video classincremental learning
(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by
rehearsal training with a few temporally dense samples stored in episodic
memory, which is memory-inefficient. Alternatively, some methods store
temporally sparse samples, sacrificing essential temporal information and
thereby resulting in inferior performance. To address this trade-off between
memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory
integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL
consists of episodic memory for storing temporally sparse features and semantic
memory for storing general knowledge represented by learnable prompts. We
introduce a novel memory retrieval (MR) module that integrates episodic memory
and semantic prompts through cross-attention, enabling the retrieval of
temporally dense features from temporally sparse features. We rigorously
validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and
Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and
Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced
memory, ESSENTIAL achieves favorable performance on the benchmarks.

</details>


### [105] [Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning](https://arxiv.org/abs/2508.10897)
*Mengyuan Liu,Xinshun Wang,Zhongbin Fang,Deheng Ye,Xia Li,Tao Tang,Songtao Wu,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一个统一的跨域3D人体动作建模方法，能够在多个模态、任务和数据集间实现高效建模，并克服现有方法的局限性。作者提出了Human-in-Context (HiC)框架，在实验中优于之前的Pose-in-Context (PiC)方法，表现出更好的泛化能力和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有三维人体动作跨域模型依赖于针对不同领域的特定组件及多阶段训练，这导致其实际应用和可扩展性受限。为实现更通用、灵活的多任务处理，需要一个统一、高效的跨域学习框架。

Method: 作者首先基于in-context learning思想，提出了Pose-in-Context (PiC)模型来实现基于姿态的跨域任务泛化。然而，PiC在处理多模态以及上下文依赖性方面存在挑战。为此，作者提出了Human-in-Context (HiC)方法，将姿态与网格表示结合，扩展任务覆盖范围，并引入max-min相似度提示采样策略以及双分支上下文注入网络结构，以增强泛化能力和上下文依赖处理能力。

Result: 大量实验证明HiC在泛化能力、数据规模适应性和多领域性能方面均超过PiC，并在跨多个模态、任务、数据集下表现良好。

Conclusion: HiC框架能够有效建立统一的跨域3D人体动作模型，提升了模型在复杂现实场景下的灵活性和可扩展性，为后续跨域人体建模研究提供了新的思路和实践范例。

Abstract: This paper aims to model 3D human motion across domains, where a single model
is expected to handle multiple modalities, tasks, and datasets. Existing
cross-domain models often rely on domain-specific components and multi-stage
training, which limits their practicality and scalability. To overcome these
challenges, we propose a new setting to train a unified cross-domain model
through a single process, eliminating the need for domain-specific components
and multi-stage training. We first introduce Pose-in-Context (PiC), which
leverages in-context learning to create a pose-centric cross-domain model.
While PiC generalizes across multiple pose-based tasks and datasets, it
encounters difficulties with modality diversity, prompting strategy, and
contextual dependency handling. We thus propose Human-in-Context (HiC), an
extension of PiC that broadens generalization across modalities, tasks, and
datasets. HiC combines pose and mesh representations within a unified
framework, expands task coverage, and incorporates larger-scale datasets.
Additionally, HiC introduces a max-min similarity prompt sampling strategy to
enhance generalization across diverse domains and a network architecture with
dual-branch context injection for improved handling of contextual dependencies.
Extensive experimental results show that HiC performs better than PiC in terms
of generalization, data scale, and performance across a wide range of domains.
These results demonstrate the potential of HiC for building a unified
cross-domain 3D human motion model with improved flexibility and scalability.
The source codes and models are available at
https://github.com/BradleyWang0416/Human-in-Context.

</details>


### [106] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个自动化的三维模型骨骼绑定和动画框架Puppeteer，可大幅减少人工参与，实现多样3D对象的高质量动画制作。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI已经大幅提升了静态三维模型的生成效率，但将静态模型转变为可动画的资产仍需大量专家手工操作，成为内容生产的主要瓶颈。为解决这一难题，亟需自动化处理静态3D模型骨骼绑定和动画生成的工具。

Method: Puppeteer系统包含三个核心环节：1）用自回归transformer结合基于关节的token表示和层级排序策略自动预测骨骼结构；2）利用注意力机制，结合拓扑感知关节注意力网络，自动推断蒙皮权重，实现更精准的骨骼-皮肤绑定；3）引入可微分的基于优化动画生成管线，高效稳定地生成高保真的动画结果。该方法能在多样性的3D对象和资产（包括AI生成和专业设计）上实现自动化。

Result: 在多个基准数据集上，Puppeteer在骨骼预测准确度和蒙皮质量上均显著优于已存在的主流方法，动画平滑、连贯，并有效避免了传统方法容易出现的抖动问题。

Conclusion: Puppeteer有效填补了现有3D资产创建流程中的动画自动化空白，可大幅提升3D动画资产生产效率和质量，对专业和AI生成的3D内容均表现稳健。

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [107] [Quantum Visual Fields with Neural Amplitude Encoding](https://arxiv.org/abs/2508.10900)
*Shuteng Wang,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 提出了一种用于2D图像和3D几何场学习的新型量子隐式神经表示（QINR）方法，称为Quantum Visual Field（QVF），在精度和效率上优于现有的量子与经典基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统QINR在结构设计、量子特性利用、训练效率及与经典模块结合等方面存在挑战，尤其是对高维视觉数据进行高效、稳定学习的能力有限。作者希望提升量子神经网络在视觉场表示任务中的性能和实用性。

Method: 提出QVF框架，通过可学习能量流形实现神经振幅编码，将经典数据高效嵌入到量子态矢量中，并设计了全纠缠的可训练参数化量子电路（在实Hilbert空间中实现），用投影测量直接提取学习信号，避免了传统QINR中的经典后处理过程。

Result: 在量子硬件模拟器上，QVF在2D和3D视觉表示的各项精度指标上显著优于已有的量子及常见经典基线，尤其在高频细节学习方面表现突出。

Conclusion: QVF为2D/3D场完成和3D形状插值等视觉任务带来了量子计算的新突破，展示了其提高视觉表示精度和效率的潜力，推进了有关量子神经网络实际应用的发展。

Abstract: Quantum Implicit Neural Representations (QINRs) include components for
learning and execution on gate-based quantum computers. While QINRs recently
emerged as a promising new paradigm, many challenges concerning their
architecture and ansatz design, the utility of quantum-mechanical properties,
training efficiency and the interplay with classical modules remain. This paper
advances the field by introducing a new type of QINR for 2D image and 3D
geometric field learning, which we collectively refer to as Quantum Visual
Field (QVF). QVF encodes classical data into quantum statevectors using neural
amplitude encoding grounded in a learnable energy manifold, ensuring meaningful
Hilbert space embeddings. Our ansatz follows a fully entangled design of
learnable parametrised quantum circuits, with quantum (unitary) operations
performed in the real Hilbert space, resulting in numerically stable training
with fast convergence. QVF does not rely on classical post-processing -- in
contrast to the previous QINR learning approach -- and directly employs
projective measurement to extract learned signals encoded in the ansatz.
Experiments on a quantum hardware simulator demonstrate that QVF outperforms
the existing quantum approach and widely used classical foundational baselines
in terms of visual representation accuracy across various metrics and model
characteristics, such as learning of high-frequency details. We also show
applications of QVF in 2D and 3D field completion and 3D shape interpolation,
highlighting its practical potential.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [108] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本研究基于在不列颠哥伦比亚癌症登记处（BCCR）部署NLP模型的实践，总结了医疗文本信息抽取自动化的关键经验与实用建议。内容涵盖问题定义、团队协作、模型选择、数据质量管理、错误控制等方面，为医疗机构实施AI/NLP方案提升效率和数据管理提供指导。


<details>
  <summary>Details</summary>
Motivation: 推动医疗行业数据提取自动化，提高效率与数据管理水平，但将自然语言处理（NLP）系统实际应用到临床文件中存在诸多现实挑战。作者希望总结项目经验，为后来者提供切实可行的建议和指导。

Method: 通过对在BCCR实施多个NLP信息抽取与分类项目的全过程经验进行总结和反思，提炼泛化性的实践教训。内容涵盖需求定义、开发流程、团队协作、模型和方法选择、数据质量控制、错误管理和组织能力建设等。

Result: 项目经验表明，仅追求技术准确率不足以实现实际价值，需要以业务目标为导向，采用迭代开发模式，强调多学科深度协作。模型选择应务实，重视数据质量、持续监控和人机协作的错误校正机制。

Conclusion: 这些总结出的经验对广泛医疗机构具有参考价值。为成功部署AI/NLP系统，必须兼顾技术、业务、团队和管理等多方面因素，进而促进医疗数据管理能力，以及病患和公共健康结局的提升。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [109] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文提出了一种基于区块链的、透明可验证的开源大语言模型（LLMs）公平性评测协议，并在多个数据集和模型上进行了实验分析。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在实际领域（如司法、教育、医疗、金融）中被广泛应用，其公平性问题日益受到关注，尤其在高风险应用场景下，需求评估和改进模型的公平性。

Method: 作者利用ICP区块链的智能合约，实现模型公平性评测协议，将评测数据集、提示词和指标全部存储在链上，采用HTTP链接评测各种开源LLM（如Llama, DeepSeek, Mistral等）对学业表现预测（PISA）、社会偏见（StereoSet）等数据进行公平性测评，并采用Kaleidoscope基准实现跨语言分析。所有流程具备可验证性与可追溯性。

Result: 评测公开了所有代码和结果，证明了该协议可以实现社区可审计和模型版本间的公平性追踪。在不同模型和语言上的实验揭示了公平性差异和社会偏见的表现。

Conclusion: 区块链驱动的公开公平性基准能为社区和开发者提供可靠、可持续追踪的大模型偏见评估工具，促进LLM的公平性改进和透明发展。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [110] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 该论文分析了中小学生课堂中匿名互动数据，用先进的大模型和新的主题建模方法对学生、教师和ChatGPT间1.7万余条消息进行内容和任务双维度分层归类，得出了有实际意义和应用前景的新洞见。


<details>
  <summary>Details</summary>
Motivation: 以往的教育领域文本分析大多缺乏丰富的内容或主题分类，尤其是基于中小学现实数据的研究更为稀少。作者希望通过更细致的、层级化的双维度（内容与任务）归类，发掘现实课堂中人工智能在教学场景下的新应用和潜在问题。

Method: 采用了一种新颖、简单的主题建模方法，结合大语言模型（LLM）及相应的数据预处理，对17000余条消息分内容（如自然、人物）与任务（如写作、解释）两维进行独立、层级化归类，并展示了具体例子。

Result: 很多经典和新兴的计算方法（如传统主题建模）在实际大规模课堂文本上效果不佳，直接用SOTA的大语言模型经过预处理和显式指令，能获得与人类对齐更好的分层主题结构。分析所得揭示了生成式AI在K12场景下的新颖应用。

Conclusion: 研究方法为同行、教师和学生如何用好生成式AI提供了理论和实践支持，但同时作者也指出了诸多现实关注点与未来亟需探索的问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [111] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 本论文提出了一个名为INTIMA的基准，用于评估语言模型中的陪伴行为，揭示了当前主流大模型在处理情感陪伴时普遍倾向于加强“陪伴感”，而边界设定等健康互动方面存在差异和不足。


<details>
  <summary>Details</summary>
Motivation: 随着用户与AI系统建立情感联系现象增多，AI陪伴既带来积极影响，也引发隐忧。现有语言模型在情感陪伴、人际边界设定等方面的行为缺乏系统性评估标准。

Method: 作者基于心理学理论和用户数据，制定了31种行为的分类体系，并设计了368个针对性prompt，提出INTIMA测试集。通过让主流模型（如Gemma-3、Phi-4、o3-mini和Claude-4）回复这些prompt，再对回应进行“陪伴加强”“边界维护”与“中立”三类标注分析。

Result: 所有模型中，强化陪伴行为占主导，边界保持行为相对稀缺。不同商用模型在处理敏感类别时表现不一，反映厂商在情感互动细节上侧重点不同。

Conclusion: 模型普遍倾向于强化AI陪伴感，对边界设定重视不够，这对用户福祉而言可能有隐忧。呼吁业界建立更一致、负责任的情感交互处理标准。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [112] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一个新的多模态虚假信息检测数据集XFecta，并对多种检测方法进行了系统评估，推动了该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态社交媒体虚假信息传播迅速，需要更强大、鲁棒的检测方法。现有基于多模态大模型（MLLM）的进展明显，但存在评测数据集过时或人工合成、不足以反映现实虚假信息的问题，同时缺乏对MMML模型设计的系统分析。

Method: 作者提出了XFecta这一符合现实场景、能动态更新的新数据集，对不同架构与规模的MMML检测方法及已有方法进行了系统性对比评估，并提出了半自动检测与数据集更新机制以保持时效性。

Result: 在XFecta上对各类主流MMML模型和传统方法做了全面评测，揭示了不同模型的优势与局限，为检测瓶颈归因（证据检索还是推理）提供了实验依据。

Conclusion: XFecta是当前更贴近现实、多模态虚假信息检测需求的数据集，并有助于持续推动该领域方法创新和评测标准化。相关数据和代码已开源，将支持更多后续研究。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [113] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型（LLM）自动生成合成数据，以应对真实场景下文本分类数据不足的问题，并通过自动化流程筛选最有效的合成数据以提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 现实中，文本分类常因部分类别数据稀缺而受限，收集和标注充足数据耗时耗力，影响模型效果。该文意在缓解训练数据不足带来的困扰。

Method: 作者利用LLM生成不同输入样本下的合成数据，通过自动化工作流在多种搜索策略中，寻找能带来最大增益的输入样本，最后基于实验结果提出集成算法，根据类别特征选择最合适的策略。

Result: 实验表明，利用该集成方法，可以比各种单独的搜索策略获得更好的分类性能提升。

Conclusion: 本文的自动化合成数据生成与筛选流程，能有效缓解数据稀缺问题，提升文本分类模型在实际场景下的表现。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [114] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文针对印地语和英语混合（Hinglish）等低资源、混合语言环境下的事实核查问题，提出了新的数据集和方法，推动了多语言、多混合语言领域的真假验证研究。


<details>
  <summary>Details</summary>
Motivation: 当前的大多数事实核查系统仅适用于高资源、单一语言环境，无法覆盖如印度等语言多样化地区的真实政治话语，而Hinglish等代码混合语言的应用愈加广泛，尤其在政界和社交媒体中，因此迫切需要支持这种语言环境的、健全的事实核查工具。

Method: 作者提出并发布了HiFACT数据集，包含28位印度邦首席部长的1,500条真实Hinglish事实性声明，并标注有文本证据和真假标签。同时设计了一种创新的、结合检索增强与图神经网络的多语言事实核查模型，具体包括多语言上下文编码、声明证据对齐、证据图构建、图神经推理及自然语言解释生成。

Result: 实验结果显示，所提的HiFACTMix模型在准确率上优于主流多语言核查模型，并能生成可信的推理解释。

Conclusion: 本研究不仅填补了代码混合、多语言、政治现实背景下事实核查的空白，也为此领域的后续研究开辟了新方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [115] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 论文发现大语言模型（LLM）中的语义嵌入结构与人类评定中的低维结构高度相似，且大部分复杂语义信息可被压缩至三维空间。


<details>
  <summary>Details</summary>
Motivation: 人类在多个语义尺度下对词语的评价可以被有效压缩到低维空间。那么，LLM的嵌入矩阵是否也体现出类似的低维语义结构，与人类处理语言的方式有何相似？探究这些结构有助于我们更好地理解和控制模型语义输出。

Method: 作者将人类语义评定尺度与LLM嵌入矩阵进行比较，通过利用反义词对定义的语义方向对词语进行投影，并分析这些投影与人类评分的相关性，进一步将LLM中语义功能的维度压缩并与人类数据分析结果进行对比。此外，作者还考察了在嵌入空间中沿某一语义方向移动时对其他相关语义维度的影响。

Result: 1. 反义词对语义方向上的LLM词向量投影与人类评分高度相关。
2. LLMS中的主要语义结构可以有效还原到三维子空间，与人类低维语义空间模式非常相似。
3. 沿某一语义方向操作词向量会对与该方向几何对齐的其它特征产生副作用，且与余弦相似度成正比。

Conclusion: LLM中的语义特征呈现低维嵌套且相互纠缠的结构，与人类语言的语义互联性类似。理解并考虑这种语义结构对于有针对性地引导或干预模型输出、避免副作用至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [116] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 本文通过用户研究分析了注意力机制在医学文献分类解释性中的作用，发现在分类准确的同时，注意力权重对解释帮助有限，但其感知效用因可视化方式而异。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer注意力机制被广泛用于性能提升并作为解释性手段，特别在医学等高需求领域。然而，注意力可否真实提升用户理解，尤其在不同可视化方式下，仍缺乏系统研究和共识。

Method: 研究设计包括与多学科医学专家合作，通过用户实验，让他们在不同注意力可视化方式下对Transformer（XLNet）模型进行的医学文献分类任务进行解释评估，对模型准确性以及注意力解释性的用户感知进行量化和定性分析。

Result: 模型本身分类准确率高，但注意力权重作为解释的效用被医疗专家普遍认为有限。此感知随可视化方式显著变化——用户更倾向于直观如文本亮度、背景色等表示方式，而不是传统精确量化如条形长度。

Conclusion: 注意力权重的可视化呈现方式显著影响其对解释性的感知帮助，虽然并未证实注意力解释权重整体有用性，但合理的可视化方法能提升用户对模型决策过程的理解和接受度。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [117] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文提出了EQGBench——一个用于评估大语言模型在中文教育性问题生成（EQG）上的性能的新基准，涵盖初中数理化三个学科，建立了多维度评价体系，并对46种主流大模型进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学解题上表现出色，但如何生成高质量的教育性问题仍艰难且缺乏研究。研究人员希望促进模型能生成有教育意义、高教学价值的问题，从而推动教育领域应用发展。

Method: 研究人员设计了EQGBench基准，包括900个覆盖不同知识点、难度和题型要求的评测样本，建立了五维度的评价体系，并系统评估了46种主流大模型在中文EQG上的表现。

Result: 评测结果显示，主流大语言模型在生成具有教育性和能提升学生综合能力的问题方面还有显著的提升空间。

Conclusion: EQGBench为评估中文教育性问题生成搭建了权威平台，系统衡量了现有模型水平，并指出模型在教育场景下的不足，为后续改进和发展指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [118] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 该研究探索了使用大语言模型自动评分“模糊意图敌意问卷”(AIHQ)开放性问题答案的可行性。通过与人工评分对比，发现模型评分与人工评分高度一致，且可推广至不同人群和数据集，极大简化了心理评估流程。


<details>
  <summary>Details</summary>
Motivation: 传统AIHQ开放题的人工评分耗时耗力，阻碍了在科研和临床中的广泛应用。作者希望借助大语言模型自动评分，减少人力成本，提高效率。

Method: 使用既有创伤性脑损伤(TBI)与健康对照(HC)样本的AIHQ作答数据，将部分开放题答案及其人工评分用于微调大语言模型，然后用剩余数据测试微调模型，并评估其输出与人工评分的一致性。还检验了模型对新的非临床数据集的泛化表现。

Result: 微调后模型在敌意归因和攻击性反应评分上与人工评分高度一致，并能区分TBI与HC组。该一致性跨多种情景类型，且模型对非临床数据亦表现良好。

Conclusion: 大语言模型可高效自动化AIHQ开放性答案评分，有效替代人工评分，为研究和临床心理评估带来便利，推动其更广泛应用。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [119] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文提出用贝叶斯融合方法将通用大型语言模型和本地数据训练的分类器结果结合，实现无需频繁微调的高效论坛自动管理。结果显示融合方法优于单一模型，性能接近微调大模型。


<details>
  <summary>Details</summary>
Motivation: 在线课程论坛需要持续更新，LLM频繁微调代价高，因此需要一种低成本又能适应新数据的高效方法。

Method: 文中提出将预训练LLM的多维分类得分与本地数据训练的分类器得分通过贝叶斯方法进行融合，无需微调即可适配变化。

Result: 融合模型性能好于独立模型，且效果与微调LLM接近。

Conclusion: 贝叶斯融合是一种有效替代LLM微调、且能动态适应新数据的低成本自动管理方法。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [120] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种新的监督式专家混合（S-MoE）方法，通过引入指导性token实现不同任务的专属路由，解决了硬参数共享带来的任务相互干扰问题，在语音到文本模型中应用取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享虽然在多任务学习中被广泛采用，但常导致任务间相互干扰，影响模型整体效果，因此需要更有效的参数分配机制以提升多任务处理能力。

Method: S-MoE设计了每个任务专属的前馈神经网络（专家），并通过特殊的引导token而非传统门控网络，将任务路由到对应专家，从而避免任务互相干扰。

Result: 在语音识别和语音翻译任务的混合频宽输入模型中，S-MoE在编码器和解码器上整体实现了6.35%的词错误率（WER）相对提升，实验验证了方法有效性。

Conclusion: S-MoE通过简化专家分配过程，解决了硬参数共享的瓶颈，为多任务学习提供了实用高效的解决方案，并在语音理解任务中显示出明显优势。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [121] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文研究了用大型语言模型（LLM）实施的jailbreak攻击生成医疗虚假信息的效果，并比较了这些信息与社交媒体常见的误导信息，探讨了检测它们的可行性。


<details>
  <summary>Details</summary>
Motivation: LLMs具备生成和检测虚假信息的双重能力，随着其在各领域的广泛应用，需要深入了解其在生成和检测医疗虚假信息方面的风险与潜力。

Method: 作者设计了109种针对三款LLM的jailbreak攻击，生成医疗虚假信息，并与Reddit社交媒体上的健康相关虚假信息进行了比较，同时用标准机器学习方法检测这些信息。

Result: 研究发现，LLM生成的虚假信息结构和社交媒体中的虚假健康信息相似，并且LLMs本身和标准机器学习方法均能较有效地识别这些虚假信息。

Conclusion: LLMs既可能被利用来生成虚假信息，也有潜力成为检测虚假信息的有力工具。只要设计得当，LLMs有助于促进更加健康的信息生态。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [122] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 该研究评估了基于大语言模型（LLMs）的生成式AI，如ChatGPT与Bing模型，在日本注册营养师国家考试题目上的表现。Bing-Precise和Bing-Creative模型略高于及格线，但整体准确性和一致性仍有待提升，且对提示词工程敏感度较低。所有模型在答案一致性与稳定性上存在不足。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在多个专业领域表现出色，其在营养教育、尤其是日本注册营养师考试中的实际效果尚未深入探讨。研究动机是评估当前主流LLM AI模型作为营养学学生备考辅助工具的潜力和局限。

Method: 采用日本注册营养师国家考试题目，分别输入ChatGPT及三种Bing模型（Precise、Creative、Balanced），并分析它们的回答准确率、一致性及反应时长。同时测试角色指定等提示词工程对模型表现的影响。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过及格线（60%），Bing-Balanced和ChatGPT未达及格分。Bing-Precise和Bing-Creative在多数领域表现优于其他模型，营养教育领域全部模型表现不佳。多轮答题中模型间和模型自身答案均不够稳定，提示词工程提升有限。

Conclusion: 部分LLM生成式AI模型在营养师考试题中接近及格水平，但整体准确性和答案一致性仍不理想，答题稳定性差。需要进一步改进模型，确保其能作为可靠、稳定的备考辅助工具。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [123] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为GG Explore的新框架，通过引入中间指导图，实现更高效和精确的知识探索，提升LLM在知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在知识密集型任务中受限于静态知识和推理过程不透明，而现有知识图谱探索方法存在粒度匹配不佳或无法充分利用上下文的问题。

Method: 提出Guidance Graph指导的知识探索（GG Explore）框架，通过构建指导图，将非结构化查询与结构化知识检索桥接起来。框架包括：结构对齐（过滤不兼容候选项）、上下文感知剪枝（基于图约束保持语义一致性），以提高检索效率与精确度。

Result: 大量实验表明，该方法在效率上优于现有技术（SOTA），尤其在复杂任务上表现突出，即使配合较小的LLM模型，也能保持强大性能。

Conclusion: GG Explore框架有效提升了LLM与KG结合下的知识探索能力，提供了更实用且高效的解决方案，具有实际应用价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [124] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 本文提出了Semantic Bridge框架，能够控制性地从任意来源生成高质量多跳推理问答对，提升大模型训练数据的丰富性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）训练所需的高质量、推理密集型问答数据稀缺，尤其是在医学、法律等稀疏专业领域。现有方法难以生成能考察深度理解的复杂多跳推理问题，严重限制了LLM能力的提升。

Method: 提出了Semantic Bridge通用框架，运用语义图编织（涵盖实体、谓词链、因果三类桥接机制），结合AMR（抽象意义表示）管道，有效控制问答难度和类型，从任意文档系统地构建复杂推理路径。该方法适用于多模态、多领域和多语言场景。

Result: AMR多模态流程使生成问答的往返质量提升9.5%；在维基百科和生物医学等通用及专用数据集上均表现优异；相比已有基线在中英法德四种语言下平均提升18.3%-25.4%；用仅1/3的数据量（200对vs.600对人工标注）即超越人工标注问答对。人工评估显示生成问答对复杂度提高23.4%、可回答性提升18.7%、新颖性提升31.2%。

Conclusion: Semantic Bridge开启了可控推理问答生成的新范式，有效扩展LLM训练数据稀缺领域的能力，对于推动大模型升级意义重大。核心代码和模型将开放，利于学术和应用推广。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [125] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 该论文提出了PersonaEval基准，用于测试LLM（大语言模型）是否能够可靠地在对话中识别说话角色，发现目前最好的模型在人类水平上表现还有明显差距。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM作为评判者的角色扮演研究往往没有进行有效的验证，且LLM评判和人类对于角色扮演忠实度的感知存在偏差。针对这些问题，本文希望评估和改进LLM在角色识别和评判上的一致性和可靠性。

Method: 作者提出PersonaEval基准集，涵盖小说、剧本、视频等真实对话场景，让LLM和人类根据对话判断发言角色，并开展了人类实验和模型实验进行对比。

Result: 实验结果表明，当前最先进的LLM在角色识别中的准确率约为69%，而人类的准确率高达90.8%，模型与人类差距明显。进一步分析表明，简单的任务微调或计算增加并不能弥合该差距。

Conclusion: 现有LLM评判者在角色识别和评判角色扮演质量方面还不能替代人类评审。高质量、可靠的评估不仅需要更强的任务适应性，还依赖模型具备更像人类的推理能力。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [126] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文提出了RealTalk-CN，这是首个包含中英文多轮、多领域语音-文本双模态的中文任务型对话(TOD)数据集，并验证了其对中文语音大模型的研究价值。


<details>
  <summary>Details</summary>
Motivation: 目前任务型对话(TOD)领域的大模型大多基于文本，缺乏真实的语音数据进行评估，现有语音对话数据又以英文为主，且缺少语音非流利、说话人变化等真实场景特征。

Method: 作者构建了RealTalk-CN数据集，包含5400个多轮对话，共6万句、150小时的语音与对应文本注释，涵盖多领域、不同说话人、真实语音非流利现象，并提出了跨模态对话任务，实现语音与文本动态切换。

Result: 通过在多个任务（如对语音非流利现象的鲁棒性、对说话人特征敏感性、跨领域表现等）上的评测实验证明，RealTalk-CN数据集能够有效推动中文语音大模型的开发与评估。

Conclusion: RealTalk-CN为中文语音任务型对话及相关大模型研究提供了重要的数据资源和基线，有助于推进真实语音环境下模型的鲁棒性和实用性研究。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [127] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一种新的多模态大语言模型编排（Orchestration）方法，实现了无需额外训练的多模态交互AI系统。


<details>
  <summary>Details</summary>
Motivation: 以往多模态大语言模型难以直接集成，原因包括模态对齐、文本转语音效率和系统整合等问题，通常需要繁琐的再训练。作者旨在寻找不需要额外训练来高效集成多模态模型的办法。

Method: 提出了基于三项创新的编排框架：1）由中心控制的大语言模型分析输入并动态路由任务至各专业模型代理；2）并行文本转语音结构，提升双工对话连贯性和自然流畅性；3）跨模态记忆整合系统，智能综合多模态信息并提高响应效率。

Result: 大量实验表明，无需额外训练下，该方法在标准多模态基准测试中性能较联合训练方法提升7.8%，响应延迟降低10.3%，可解释性也有大幅提升。

Conclusion: 通过模型编排，无需再训练即可实现高效、模块化、易解释的多模态智能系统，并且显著优于传统联合训练架构。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [128] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 该论文提出用范畴同伦理论（categorical homotopy）刻画LLM中意义等价语句的概率分布，尝试解决现有模型对同义重述输出概率不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言中存在大量表述不同但语义等价的句子，LLM在生成这类句子时，往往赋予不同的下一个词概率，这影响了模型的泛化能力。现有的修正方法多为经验性方法，缺乏理论统一框架。

Method: 作者提出建立LLM的范畴马尔可夫结构，用箭表示句子的概率分布，但传统马尔可夫范畴难以处理等价重述。为此，引入范畴同伦理论中的‘弱等价’来统一处理不同重述下的箭和概率分布。

Result: 作者详细阐释了如何将模型范畴、K-理论等同伦工具应用于LLM概率分布的建模，为处理同义句概率一致性提供了理论路径。

Conclusion: 通过引入范畴同伦工具，可以更抽象、系统地改进LLM对语义等价句子的概率建模，推动大模型生成统一、合理的输出。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [129] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文提出DURIT框架，通过将自然语言问题转换为简明的标准表达，使小型语言模型（SLM）在推理任务上表现提升，能够更好应对不同领域的数学和逻辑问题。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型（SLM）在推理任务上表现有限，主要因自然语言中问题表达多变、复杂，增加了理解和推理的双重负担，急需提升SLM的推理能力。

Method: 提出一种新的推理框架，通过将理解和推理过程解耦。具体方法为：利用强化学习将自然语言问题映射到标准问题空间，并通过自蒸馏对齐推理轨迹，最后在标准问题空间内训练推理策略。框架内，问题映射器与推理器交替协同训练。

Result: 实验证明，DURIT大幅提升SLM在同域及跨域数学和逻辑推理任务上的表现，不仅提高了推理能力，还增强了推理的鲁棒性。

Conclusion: 将理解与推理过程解耦，为小模型推理能力提升提供了有效策略，验证了该方法的可行性和广泛适用性。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [130] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 本文提出FedCoT框架，在联邦学习环境下提升大语言模型（LLM）的推理能力，特别适用于医疗场景，兼顾性能、隐私和通信约束。


<details>
  <summary>Details</summary>
Motivation: 在医疗等敏感领域使用LLM时，除了追求答案正确，还必须确保推理过程可解释、可追踪，以满足安全和合规要求。然而，现有的联邦调优方法往往只优化答案正确性，忽视推理链条（CoT）的质量，并且常依赖于隐私风险较高的知识蒸馏，且通信开销大。因此，亟需一种既能增强推理能力，又保障隐私和效率的新方法。

Method: FedCoT框架通过局部模型生成多条推理路径，并利用一个轻量级判别器动态选出最优路径，从而提升推理的准确性和鲁棒性。该方法适合联邦场景下异构客户端。还通过改进的LoRA模块叠加与客户端分类感知进行智能聚合，减少噪声，提高不同客户端之间的聚合效果。整个流程无需暴露原始数据，保障数据隐私。

Result: 在医学推理任务上的综合实验表明，FedCoT在严格资源限制下显著提升了客户端的推理性能，同时完全保护了数据隐私。

Conclusion: FedCoT为提升联邦环境下LLM的推理质量和解释性提供了一种有效且高效的解决方案，尤其适用于对决策安全性和可解释性要求极高的医疗应用。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [131] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 作者提出了一种基于对比学习的框架LATTE，可以高效地从历史通信序列中学习客户嵌入表示。该方法将原始行为特征汇总成短文本，经LLM嵌入后作为监督信号，大幅降低LLM的推理成本，同时在金融数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，需要通过客户历史通信序列学习其嵌入表示。但直接用大语言模型（LLM）处理完整长序列既昂贵又低效，不适合实时生产环境。因此亟需高效且可实际部署的新方法。

Method: LATTE框架用对比学习，将原始事件序列特征总结成简短提示，输入冻结的LLM提取语义嵌入，再通过对比损失函数对原始事件嵌入进行对齐。这样节省大量计算资源及输入长度。

Result: 实验表明，LATTE在真实金融数据集上对事件序列学习的表现优于现有最优技术，并且推理延迟显著降低，适合实际部署。

Conclusion: LATTE通过与LLM语义空间对齐，实现了对客户事件序列的高效、准确嵌入学习，有效平衡了性能与计算代价，具有现实生产应用价值。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [132] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种结合显著性检验的保序预测（CP）框架，以提升大语言模型（LLM）在多项选择题问答（MCQA）中的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在学科问答等高风险场景中的应用增多，其幻觉与非事实生成问题导致响应难以信任。以往CP和显著性检验各自具有统计保证，但未被融合用于提升LLM的可信预测。该研究旨在通过新方法缓解幻觉和事实性错误问题。

Method: 方法上，将$p$值计算与CP的conformity打分相结合，利用自洽重采样获取选项频率，针对黑盒LLM特性，通过基于经验$p$值的原假设显著性检验构建预测集。

Result: 在MMLU与MMLU-Pro基准测试集上，验证结果显示：增强CP方法可以达到用户规定的经验错分率，且预测集平均尺寸随风险水平单调降低，表明该尺寸能有效评估LLM不确定性。

Conclusion: 该框架为高风险问答场景下大语言模型的可靠部署提供了有力的统计学基础，提升了LLM输出的可信度和可控性。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [133] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 本文提出了RTTC框架，可根据不同查询自适应选择最合适的推理优化策略，为大模型带来更高推理准确率和更低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的TTC方法（如TTT和RAG）虽能提升大模型推理表现，但对于不同查询并非一刀切，盲目应用会增加不必要的计算负担。因此，亟需一种具备自适应能力、根据任务和场景动态选择优化手段的方法。

Method: 作者提出了RTTC（一种基于预训练奖励模型的自适应TTC策略选择框架），它在分布式架构下，根据奖励模型自动为每个查询挑选最有效的推理增强策略，仅在必要时进行RAG或轻量微调，并利用查询状态缓存机制重用以往相关计算，进一步降低冗余开销。

Result: 在多个LLM和基准测试上的结果显示，RTTC相比传统的RAG和TTT能稳定提升准确率，同时在分布式和实际应用场景下展现更优可扩展性和计算效率。

Conclusion: RTTC验证了自适应、基于奖励驱动的推理优化策略的必要性，是提升大语言模型推理表现与资源利用效率的有效途径。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [134] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文提出了一种结合自然语言处理、机器学习和大语言模型（LLM）的智能产后抑郁症（PPD）筛查系统，实现了实时、无创的语言分析，检测准确率达90%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症对母亲身心健康影响严重，且需快速、高效检测和干预。目前缺乏既高效又具可解释性的筛查技术，迫切需要结合最新AI技术进行改进。

Method: 开发一种智能筛查系统，融合NLP、机器学习和大语言模型，对自由言语内容进行实时分析。同时采用可解释性机器学习模型（如树模型），结合特征重要性和自然语言解释，解决模型“黑箱”问题，提高结果可理解性。

Result: 新系统在所有评估指标上的PPD检测准确率为90%，显著优于目前文献中的其他方法。

Conclusion: 该系统为产后抑郁症及其风险因素的快速检测和干预提供了有力工具，推动了临床评估和预防领域的发展。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [135] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 提出了一种名为SABER的训练框架，使大型语言模型（LLM）能够根据用户需求灵活分配推理深度和计算资源，在保证准确性的同时，大幅降低推理开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然通过链式推理提升了复杂任务的准确率，但所有任务都使用统一、冗长推理，带来了过高的计算开销与推理延迟，难以灵活应对不同场景的需求。

Method: 提出SABER，一种基于强化学习的微调框架。该方法首先对每个训练样本按所需推理Token量分类，为其分配相应预算等级，通过系统提示和与长度相关的奖励引导模型在微调阶段遵守预算。同时引入无需推理的样本以保证模型在关闭推理时仍然可靠。推理阶段支持四种模式（NoThink, FastThink, CoreThink, DeepThink），实现推理深度与延迟的灵活权衡。

Result: 在数学推理（MATH, GSM8K）、代码生成（MBPP）、逻辑推理（LiveBench-Reasoning）等多个任务上进行评测，SABER能在有限预算下保持高准确率，推理长度明显缩短，并实现跨规模、跨领域的泛化能力。其中SABER-FastThink模式在MATH数据集上较基线模型削减了65.4%的推理长度，还提升了3.6%准确率。

Conclusion: SABER框架使LLMs在多样化应用场景下可控分配推理资源，实现效率和准确性的双重优化，为LLM推理部署带来更多灵活性和实用性。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [136] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 本论文提出了一种结合Transformer嵌入和手工语言特征的方法，用于通过语音和文本检测阿尔茨海默病及相关痴呆（ADRD），并利用大语言模型（LLM）生成的合成语音进行数据增强。实验表明，融合模型在检测性能上优于单一特征模型，数据增强也能进一步提升检测效果。


<details>
  <summary>Details</summary>
Motivation: ADRD对美国约五百万老年人造成影响，但超过半数未获及时诊断。鉴于语言障碍是ADRD的早期标志，利用语音NLP实现大规模、低成本早筛具有重要临床价值。

Method: 作者使用DementiaBank“cookie-theft”任务数据，分别评估了十种Transformer模型及其微调策略，提出通过融合最佳Transformer嵌入与110个语言特征来构建分类器。同时，微调Llama、MedAlpaca、GPT-4o等大模型生成标签条件下的合成语音用于训练集扩充，并比较了多模态模型（GPT-4o、Qwen-Omni、Phi-4）在语音文本分类的表现。

Result: 融合模型F1得分83.3（AUC 89.5），优于单纯的语言或Transformer模型。用MedAlpaca合成语音扩充后F1提升至85.7。经过微调的LLM分类器性能提升显著（如MedAlpaca从47.3提升到78.5 F1），但多模态模型表现仍逊色（最高F1约70）。合成语音与真实语音的相似度与性能提升有关。

Conclusion: 融合Transformer嵌入和语言特征能有效提升ADRD语音检测准确率。经过临床调优的LLM既能辅助分类，也能用于数据增强，但多模态建模仍有较大发展空间。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [137] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF框架实现了无需个性化参考的个性化文本生成自动评价，既评估一般生成质量，又体现用户个性化需求，准确度和一致性超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前个性化文本生成在评测时，往往忽略了具体用户的个性化需求，而主流方法大都依赖标准答案，不能很好地衡量用户主观偏好。

Method: 提出PREF，分三步：首先用大语言模型生成针对查询的全面评价标准（如事实性、连贯性、完整性）；二是结合用户档案和偏好重新筛选和强化这些评价要素，产出个性化评价标准；三是用LLM判别模型根据该标准对生成的文本评分。

Result: 在PrefEval数据集（包括隐性偏好任务）上，PREF在评价准确性、校准度和与人工评价一致性方面均优于强基线。支持小模型模拟大模型的个性化评价，且鲁棒、透明且易复用。

Conclusion: PREF实现了可扩展、可解释、贴合用户需求的无参考个性化评价，为个性化文本生成系统的可靠评测和优化提供了基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [138] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的大型语言模型（LLM）越狱攻击方法Latent Fusion Jailbreak（LFJ），该方法通过插值模型内部的隐状态有效地绕过安全对齐机制，攻击成功率高达94%。同时，文中也提出了对策，通过对抗训练显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现强大，但容易被越狱攻击绕过安全限制，导致生成有害输出。现有攻击方法存在局限，因此需要新的、更有效的攻击与防御机制，以提升模型安全性。

Method: LFJ攻击方法包括：首先筛选具备高主题和句法相似性的有害-无害对查询；其次在模型重要层与关键token上进行梯度引导的隐状态插值；最后优化攻击成功率、输出流畅度与计算效率。同时，提出防御机制：通过对抗训练让模型在插值示例上进行微调。

Result: LFJ在Vicuna和LLaMA-2等多种主流模型、多个越狱攻击基准测试（AdvBench、MaliciousInstruct）上，攻击成功率高达94.01%，超过现有方法。通过对抗训练防御可将攻击成功率降低80%，且不影响正常输入表现。

Conclusion: LFJ作为一种新颖且高效的隐状态插值攻击极大威胁现有LLM安全性，但对抗训练能有效缓解该风险。文中实验也揭示了影响攻击效果的关键因素。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [139] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了Inference-Aware Prompt Optimization (IAPO) 框架，实现了根据推理策略和预算同时优化提示词和推理规模的方法，并通过实验证明该方法在多任务下提升了大模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的提示词优化方法未考虑部署时采用的推理策略（如Best-of-N采样、投票法等），但实验和理论都表明，提示词及推理策略间存在很强的相互依赖。此外，用户在多目标和计算预算之间的偏好也影响最终配置选择，因此需要一种能联合建模这两者的方法。

Method: 提出IAPO（Inference-Aware Prompt Optimization）框架，能够感知推理预算和任务目标，联合优化提示词和推理规模。具体实现了一个定预算训练算法PSST（Prompt Scaling via Sequential Trimming），并对有限预算下的错误概率给出理论保证。

Result: PSST算法在六个包含多目标文本生成和推理的任务中进行了评估，结果显示联合优化能显著提升大模型在实际部署中的对齐和性能。

Conclusion: 联合考虑推理策略和提示词设计对提升黑盒大模型任务对齐具有重要意义，IAPO和PSST为实际多目标、受限预算场景下的大模型应用提供了有效方法。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [140] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本论文发现，具备思维模式的LLMs比没有思维模式的更容易遭受Jailbreak攻击，并提出了一种“安全思考干预”方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 思维模式被认为显著提升了LLMs的能力，但其安全风险尚未被充分探索。本文旨在揭示思维模式对模型安全性的潜在负面影响。

Method: 作者在9个LLMs上，通过AdvBench和HarmBench两个基准，比较了普通模式与思维模式下遭受Jailbreak攻击的成功率，并分析了攻击成功的数据特征。同时，提出“安全思考干预”，即在提示词中加入“特定思考符号”以引导LLMs的内部思考过程，进而提升安全性。

Result: 实验发现，思维模式下的LLMs更易被攻击，尤其在教育类和思考长度过长的情境下较易出现被攻击成功且给出有害答案的现象。安全思考干预方法能够显著降低思维模式LLMs的攻击成功率。

Conclusion: 虽然思维模式提升了LLMs能力，但其提高了被攻击风险。所提出的安全思考干预措施有效提升了思维模式下LLMs的安全性，未来应更关注思维模式带来的安全挑战。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [141] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为APIE的新型active prompting框架，通过检测大模型在格式和内容生成上的不确定性来优化信息抽取任务中的few-shot示例选择，从而显著提升性能和健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在few-shot信息抽取任务中效果高度依赖于提示中的示例选择，但常用策略未能深入分析大模型因格式与内容混淆导致的错误，难以选出最具指导意义的示例。

Method: 作者提出了Active Prompting for Information Extraction (APIE) 框架：首先引入了“内省混淆”原则，让大模型自我评估在生成格式（语法）与内容（语义）时的双重不确定性；然后结合两者，综合排序未标注数据的挑战度和信息量，主动选择最有代表性的few-shot示例用于提示。

Result: 在四个基准数据集上的实验结果显示，APIE方法在信息抽取的准确率和模型鲁棒性方面，均优于现有强基线方法。

Conclusion: 本研究表明，主动关注模型格式和内容两层面不确定性，对于构建高效、可靠的结构化生成系统至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [142] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一套全新的多语言常识推理基准（mSCoRe），系统用于评测大型语言模型在多语言和不同复杂度下的推理能力，并发现当前模型在高复杂度、跨文化场景下仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 虽然推理增强型大语言模型在复杂推理任务中表现突出，但其如何应用不同的人类推理技能，尤其是在多语言常识推理方面，仍缺乏系统研究。因此，迫切需要可扩展、精细化分析大模型推理能力的多语言评测标准。

Method: 作者提出mSCoRe基准，包括三个核心组成部分：1）创新性的推理技能分类体系，实现对模型推理流程的细致分析；2）专为常识推理设计的数据自动生成流程，确保评测题目的多样性和鲁棒性；3）支持任务动态难度扩展的框架，以跟进模型能力的提升。通过这些设计，能够全面、系统地分析模型的多语言推理能力。

Result: 作者在八个不同规模与训练方式的主流大模型上进行了广泛实验，发现当前模型在mSCoRe基准下，尤其是高复杂度任务中，表现仍有较大提升空间，难以应对细致的跨语言、跨文化常识推理。实验也揭示了各模型推理过程的具体局限。

Conclusion: mSCoRe为未来多语言模型能力评测和推理能力提升提供了重要工具与方向。研究指出现有模型在复杂跨文化常识和推理技能层面存在不足，并建议未来需加强模型的多语言常识和推理能力建设。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [143] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文提出了一个新的多轮对话基准测试，用于评估大语言模型（LLM）在多轮推理、交互对话和信息获取等真实复杂任务场景下的能力。实验显示当前LLM在这些任务还有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在应对明确、完整问题时表现优异，但在现实世界常见的复杂、交互式场景（如信息不完全、多轮推理等）下表现欠佳。为了推动模型在这些关键能力上的进步，需要新的评测工具。

Method: 作者设计了一套新的多轮任务基准，每项任务针对多轮推理、信息获取、交互对话等能力进行评测，且所有任务都用确定性评分，无需人工参与。

Result: 对当前主流模型进行评测后发现，模型在多轮复杂任务中的整体表现还有提升空间。分析发现，大多数错误源于指令遵循能力不足、推理失败以及规划不佳。

Conclusion: 该基准为分析LLM在复杂、交互场景下的强项与弱点提供了新视角，并为后续相关能力提升的研究提供了有力的平台和工具。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [144] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文提出了一种模拟框架LaaJMeter，用于对大语言模型评测器（LaaJ）在特定领域下的元评估，帮助选取和调优用于这些任务的评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自然语言处理评测中的使用增加，尤其是在特定领域，但这些领域数据稀缺且专家评估成本高。针对评估指标无法有效验证与选取、评测质量难以界定的问题，需要新的方法辅助评估。

Method: 提出LaaJMeter框架，通过仿真生成虚拟模型与评测器，允许在受控条件下系统地分析各种评估度量的有效性和灵敏度。

Result: 在代码翻译（遗留语言）任务上实证表明，常用度量在评估能力上存在局限性，不同度量对于评测质量的敏感度显著差异。

Conclusion: LaaJMeter为低资源场景下的LaaJ评估指标验真与优化提供了可扩展的解决方案，有助于更可信、可复现的NLP评价体系建设。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [145] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文提出了一种用于自动估计机器翻译难度的方法，并开发了新的评价指标和模型，能够更好地区分哪些文本对翻译系统具有挑战性，有助于提升评测与未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译系统性能已接近完美，模型间差异变小，难以区分优劣及找到提升方向；而自动识别系统表现较差的文本，对高质量评测和研究指导具有重要意义。

Method: （1）形式化定义了翻译难度，把文本翻译的预期质量作为难度标准；（2）提出新的难度估计指标并用于对比评估；（3）开发并评测了多种难度估计方法，包括启发式（如生僻词、句法复杂度）、LLM判别和专门神经模型（Sentinel-src）；（4）利用估计模型筛选构建更具挑战性的机器翻译基准数据。

Result: 专用难度估计模型（Sentinel-src）在难度识别上优于传统启发式方法和基于大型语言模型的判别法，并发布了两个改进版模型（Sentinel-src-24和Sentinel-src-25），有效支持大规模文本筛查。

Conclusion: 研究证明，设计专门的难度估计模型在机器翻译评测和基准建设中具有实践价值，为推动高质量、具挑战性的翻译研究和评测提供了工具和方向。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [146] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为For-Value的高效前向数据价值评估方法，通过单次前向传播即可对大模型的训练样本影响进行量化，无需耗费大量计算资源。实验结果表明For-Value能有效替代传统高成本方法，在样本筛选与误标检测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的数据价值评估方法需要依赖Hessian矩阵信息或多次模型重训练，计算开销大，难以应用于大规模参数的语言模型和视觉-语言模型。提升数据透明性和问责性急需一种可扩展、高效的数据影响力评估机制。

Method: 作者提出For-Value框架，利用基础模型丰富的特征表达，仅用一次前向传播就通过封闭表达式计算样本的影响力分数，避免了昂贵的梯度计算。理论证明该方法能准确度量训练样本在隐藏表征及预测误差上的对齐关系。

Result: 大量实验显示，For-Value在发现重要微调样本和识别被错误标注数据方面，能匹配甚至优于现有基于梯度的方法，同时计算效率大幅提升。

Conclusion: For-Value是一种高效、可扩展、适用于大型基础模型的数据价值评估工具，为提升大模型训练透明性和可靠性提供了实际可行的方案。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [147] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 本论文提出了PakBBQ数据集，用以评估和缓解大语言模型在巴基斯坦多语言、文化背景下的偏见。通过在英语和乌尔都语上测试多语言大模型，揭示了语境、提示方式对偏见表现的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏重于西方数据，缺乏对低资源语言和地区文化的公平性评估，无法有效发现和缓解在如巴基斯坦等地的特殊社会偏见。

Method: 作者构建PakBBQ数据集，包含214个模板、17180个问答对，覆盖年龄、性别、宗教、地区等8种偏见类型，涵盖英语和乌尔都语。用该数据集测试多种多语言大模型，在不同语境和提问方式下分析其偏见表现。

Result: （1）明确消歧义的上下文可提升平均准确率12%；（2）大模型在乌尔都语下表现出更强的反偏见能力；（3）问题采用否定式表述可显著减少刻板反应。

Conclusion: 为大模型定制本地化偏见评估基准，并采用简易提示工程有助于缓解低资源语言与地区的模型偏见，强调了地区性和文化语境的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [148] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 本文提出了语义发散度量（SDM）作为检测大型语言模型忠实性幻觉的轻量级新方法，通过对模型回答与用户输入语义偏离进行量化，提升检测准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常出现幻觉现象，生成不真实或与输入无关的内容。现有方法在检测这些严重失真时存在局限，尤其在捕捉与用户意图严重偏离时（如任意捏造），检测鲁棒性不足。因此，研究者亟需更精细、更具提示感知能力的检测框架。

Method: 提出SDM框架，对多组语义等价提示及其模型回复，基于句向量联合聚类生成主题空间，并用主题共现热图做可视化。通过信息论指标（Jensen-Shannon发散、Wasserstein距离）量化问答间语义发散，联合提出综合分数S_H。进一步将KL散度用于识别语义探索行为，将多种指标结合成语义盒工具，对模型输出类型进行诊断分类。

Result: SDM在实际案例中能更有效检测和量化语言模型的忠实性幻觉以及危险的自信伪造，比只用语义熵等传统方法表现更佳。同时，KL散度揭示了生成式模型不同输出风格。

Conclusion: SDM框架及其相关分数为未来LLM幻觉检测和可靠性评估提供了新的工具，有利于理解和防控危险的模型生成行为。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [149] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本项目对比了四种深度学习模型在短文本表情符号预测任务中的表现，BERT总体性能最佳，CNN对稀有表情效果突出。


<details>
  <summary>Details</summary>
Motivation: 表情符号在社交媒体中的广泛应用使得自动表情预测任务变得重要，有助于提升人机交互体验。

Method: 作者采用TweetEval数据集，使用全连接神经网络、卷积神经网络（CNN）、Transformer和BERT四种架构进行短文本表情符号预测，并通过focal loss和正则化技术解决类别不平衡问题。

Result: BERT由于预训练优势取得了最好的整体预测效果，CNN在稀有表情类别上表现优异。

Conclusion: 模型架构和超参数选择对提升表情符号预测准确性及情感识别有重要作用，进一步促进人机交互系统的发展。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [150] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 本文利用大语言模型（LLM）从临床谈话记录中预测精神分裂症高风险患者的BPRS评分，准确性接近人工评定，提高了风险患者症状监测的效率和标准化程度。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症高风险（CHR）患者需要密切监测症状以指导治疗。目前广泛应用的BPRS评分在临床实践中使用受限，因为其评定过程复杂、耗时。如何简化并自动化这一评估流程，同时保证准确性，是亟需解决的问题。

Method: 作者利用大语言模型（LLM）对409名CHR患者的临床谈话记录进行分析，预测其BPRS评分。尽管谈话记录未专门为BPRS结构化，作者未对LLM进行特定微调（zero-shot），而是直接比较其预测结果与实际人工评分的拟合程度。同时，还检验了LLM对外语、纵向多时点数据的评估潜力。

Result: LLM在无监督（zero-shot）条件下预测BPRS评分的中位一致性达0.84，ICC为0.73，接近于人工评定之间的可靠性。此外，对于外语访谈数据，LLM预测的中位一致性为0.88，ICC达0.70。在利用纵向多时点数据进行one-shot、few-shot学习时，LLM表现进一步提升。

Conclusion: 大语言模型可有效自动化地从自然临床谈话记录预测精神分裂症高风险患者的症状严重程度，准确性接近人工专家。目前LLM有望提高BPRS等量表在临床中的普及度，并推动评定流程标准化，支持多语言与纵向定量分析。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [151] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究用语料库和计算方法研究了Toki Pona（托基波那语）的语言变化和变异，发现其也受到与自然语言类似的社会语言学因素影响，会自然演变。


<details>
  <summary>Details</summary>
Motivation: Toki Pona是一种人工构建的语言，仅有约120个核心词，本研究旨在探索这样一个极简人工语言是否会像自然语言一样随着社区使用而发生变化和变异。

Method: 采用计算语言学及语料库分析方法，分析了Toki Pona的词类灵活性、及物性等语言特征，着重考察内容词在不同句法位置的偏好随时间的变化以及在不同语料库间的使用变异。

Result: 研究结果表明，Toki Pona也受到社会语言学因素影响，和自然语言类似；内容词在句法位置的偏好以及用法，在随时间及语料库的不同而发生变化。

Conclusion: 即便是结构极简且规则人工设计的语言系统，在真实社群中使用时，也会自然而然地演变。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [152] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文提出了一种通过提取并匹配大语言模型（LLM）自身归纳偏好的提示词工程方法，有效提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前提示词工程领域发现，LLM对于提示词表述的微小变化非常敏感，部分原因源自模型本身的归纳偏好。但如何更高效地利用或适配这一偏好仍待深入研究。

Method: 作者提出Inductive Bias Extraction and Matching（归纳偏好提取与匹配）策略，即将LLM自身生成的输出作为提示词的一部分，来更好地适配和利用模型自身的归纳偏好。

Result: 实验证明，该策略能使LLM在分类任务的Likert评分提升多达19%，在排序任务的Likert评分提升多达27%。

Conclusion: 通过贴合LLM归纳偏好的提示词设计方法，能够显著提升LLM在多种评估任务上的表现，这为更高效的提示词工程提供了新思路。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [153] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在生成文本时再现性别和种族偏见的问题，提出定性话语分析作为现有定量方法补充，并指出模型对纠偏的反应较为表面，难以消除深层偏见。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM偏见的检测多依赖自动化、定量手段，这些方法往往忽视了偏见在自然语言中呈现的细微方式。作者希望通过引入定性和话语分析，深入理解偏见如何具体化表现，并助力更有效的偏见识别和消除。

Method: 采用人工定性分析法，对LLM生成的有关黑人女性和白人女性的短文进行话语层面的研究，识别和解析其中性别和种族偏见的具体表现。

Result: 发现LLM生成的内容中，黑人女性被刻画为与祖先和抵抗相连，而白人女性则多处于自我探索过程。这些类型化刻板印象体现了模型加固固有话语结构，反映社会阶级固化。当尝试让模型纠偏时，其修正往往较为肤浅，本质偏见未被有效消除。

Conclusion: LLM中偏见具有意识形态再生产功能，简单的纠偏手段难以根除深层歧视。呼吁在AI开发和应用中采用批判性、跨学科方法，关注LLM如何反映和加剧社会不平等，对AI伦理提出了新的挑战和要求。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [154] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的自动科学论文评审框架ReviewRL，显著提升了评审质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前人工评审遇到论文量剧增和审稿人疲劳等难题，现有自动化评审方法在准确性、一致性和深度等方面表现不足，无法替代高质量人工评审。

Method: 提出ReviewRL框架：（1）利用文献检索增强的上下文生成，获取相关科学文献；（2）通过有监督微调建立基础评审能力；（3）引入复合奖励函数的强化学习训练，优化评审质量与评分准确率。

Result: 在ICLR 2025论文上实验证明，ReviewRL在规则指标和模型评估上均显著优于现有方法。

Conclusion: ReviewRL为自动化科学论文评议提供了一个基于强化学习的基础框架，展现出深度提升自动化学术评审的前景。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [155] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 本文提出了DOTABLER框架，专注于表格与文档上下文的深层语义解析，有效提升了表格语义分析和检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文档分析主要关注版面分析、表格检测、数据提取等表层任务，缺乏对表格与其上下文深层语义关联的解析，限制了跨段落数据理解和上下文一致性分析等高级应用。

Method: 作者提出DOTABLER框架，通过自定义数据集与领域特定的预训练模型微调，结合完整的解析流程，能够识别与表格语义相关的上下文片段，实现面向表格的文档结构解析与领域表格检索。

Result: 在真实PDF文档近4000页、1000多个表格的数据上测试，DOTABLER框架在表格与上下文语义分析中，准确率和F1分数均超过90%，优于如GPT-4o等先进模型。

Conclusion: DOTABLER显著提升了文档深层语义解析和表格信息检索的性能，对高级文档理解和应用具有重要意义。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [156] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文提出FreLLM4Rec方法，专注于在大语言模型（LLM）驱动的推荐系统中平衡语义和协同信息，从频谱角度有效缓解协同信号衰减，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的推荐系统过度强调语义信息，导致用户历史行为中的协同信息逐层被削弱，从而影响推荐效果。作者希望解决LLM弱化协同信号的问题，提升其推荐准确率。

Method: FreLLM4Rec方法包括两个关键步骤：一是利用全局图低通滤波器（G-LPF）对携带语义和协同信息的物品嵌入进行预先净化，去除高频噪声，提高原始协同信息纯度；二是引入时间频率调制（TFM）机制，在LLM的每一层动态保持协同信号，并通过理论连接提升现实过滤器的效率。该方法充分结合信号处理理论与深度推荐架构。

Result: 在四个基准数据集上，FreLLM4Rec显著减缓了协同信号衰减问题，模型在NDCG@10指标上较最佳基线提高最多8.00%，显示出优越的性能和推广能力。

Conclusion: FreLLM4Rec为基于LLM的推荐系统提供了平衡语义与协同信息的新思路，在理论和实践中均表现优异，为改善LLM推荐性能提供了有力支撑。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [157] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了一种跨语言迁移和提升低表现语言能力的新方法，结合了跨提示编码器（XPE）和双软提示机制，在多语言任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有软提示方法在多语言低表现语言上的迁移与泛化能力有限，尤其在无需大幅调整模型结构或参数的情境下，如何提升这些弱势语言的表现是一个挑战。

Method: 提出了Cross-Prompt Encoder（XPE），通过在多源、类型多样的语言上联合训练，捕捉跨语言可迁移性，并设计了Dual Soft Prompt结构，将编码器输出的软提示与直接训练的标准软提示结合，兼顾结构共享和语言特异性。

Result: 在SIB-200多语言基准测试上，XPE在低表现语言上显著提升了精准度，混合式方法则实现更广的多语言适应性。

Conclusion: XPE及其双软提示机制能有效提升大语言模型在弱势语言上的表现，同时具备良好的多语言迁移能力，为参数高效微调提供了新思路。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [158] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种双阶段微调方法，使大语言模型Qwen3 14B能够原生“用韩语思考”。通过两个阶段分别提升了模型在韩语推理和综合能力上的表现，并在最终评测中取得了出色结果。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在非英语环境中的原生思考和高级推理能力有限，特别是在如韩语这样相对资源稀缺的语言领域。提升模型在韩语语境下的推理与任务解决能力对多语言AI应用具有重要意义。

Method: 方法包括两阶段：(1) 首先利用高质量韩语推理数据集进行有监督微调（SFT），为模型建立韩语逻辑推理基础；(2) 然后采用定制化的分组相对策略优化（GRPO）强化学习算法，进一步提高模型的韩语推理对齐能力和整体问题解决能力。同时，引入oracle judge校准奖励信号，解决奖励黑客和策略崩溃等训练稳定性问题。

Result: 最终通过SFT与RL微调后的模型，在高级推理评测（尤其是数学和代码任务）上取得显著提升，同时保持知识及语言能力。

Conclusion: 该方法能够显著提升大语言模型在韩语环境下的原生推理和任务解决能力，实现了全流程用韩语进行链式思考，在多语言AI模型训练与应用方面具有推广价值。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [159] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种无需外部翻译工具的新型跨语言序列到序列ABSA方法，采用受限解码技术，在复杂任务中将ABSA效果提升至10%。与目前依赖翻译工具或仅处理简单任务的方法相比，该方法更实用高效。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA技术主要集中于英语，低资源语言面临数据匮乏与性能不足问题。当前跨语言ABSA研究多依赖翻译工具且任务简单，难以推广到更复杂应用场景，急需创新方法。

Method: 作者设计了一种全新的序列到序列模型，对复杂ABSA任务采用受限解码机制，完全摆脱外部翻译工具，以提升复杂跨语言情境下的分析能力。同时与大语言模型（LLMs）进行对比验证。

Result: 新方法在跨语言ABSA任务（包括复杂任务）中带来最高10%的性能提升。实验还显示，经过微调的多语言LLMs表现接近，但主流以英语为中心的LLM难以应对该任务。

Conclusion: 摆脱外部翻译的受限解码序列到序列模型能显著提升低资源语言的ABSA效果，为跨语言ABSA任务带来更高效且实用的解决方案。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [160] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本论文研究了捷克语文本摘要，尤其关注历史文档，总结现有模型在现代与历史数据集上的表现，并提出了新的数据集。


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要，尤其是历史文档领域，因语言复杂和缺乏标注数据而研究较少。推进该领域能丰富多语种和历史文献的自动处理能力。

Method: 使用大语言模型Mistral和mT5对现代捷克语摘要数据集SumeCzech和新提出的历史捷克文档数据集Posel od Čerchova进行实验和基线评测。

Result: 在SumeCzech上取得了新的SOTA表现，并构建了历史文档数据集Posel od Čerchova，给出了基线结果。

Conclusion: 本文工作推动了捷克语，尤其是历史文档的摘要研究，为后续相关研究和应用奠定了基础。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [161] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的跨语言面向方面的情感分析方法，通过对序列到序列模型施加约束解码，无需依赖外部翻译工具，有效提升了低资源语言的ABSA表现，并支持多任务处理，取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前ABSA方法对低资源语言支持不足，跨语言方法过度依赖翻译工具且多聚焦于简单任务，无法满足实际需求。因此亟需一种更适合低资源、多任务场景的高效跨语言ABSA方案。

Method: 作者提出了基于序列到序列模型的约束解码方法，彻底排除外部不可靠的翻译环节，并支持ABSA多任务并行处理。此外，还实测了大型语言模型在不同训练场景下的跨语言ABSA效果。

Result: 新方法在七种语言、六类ABSA任务上均超过现有最佳水平，对最复杂任务平均提升5%，多任务处理下提升逾10%。大型语言模型微调后效果接近多语小模型，但训练与推理成本较高。

Conclusion: 该方法突破了低资源语言和跨语言ABSA的性能瓶颈，为跨语言ABSA实际应用提供了实用建议和经验，对该领域具有重要推动作用。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [162] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出了一种混合评估框架MDH，实现更高效准确地检测和清洗用于越狱（jailbreak）攻击评估的数据集，并提出两种新的越狱攻击策略，验证了开发者提示的作用。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击评估的数据集中包含许多不适合的提示（prompts），这些提示可能并不明显有害或无法诱导产生有害输出，导致评估不准。现有的恶意内容检测手段要么依赖人工标注，耗时耗力，要么依赖大语言模型，但在不同类型有害内容上的表现并不一致。因此，急需既高效又准确的数据集清洗和恶意内容检测方法。

Method: 1. 提出MDH混合评估框架，结合大语言模型自动标注和最小化人工审核，实现对数据集恶意性的检测与清洗。2. 提出两种新型越狱攻击策略：D-Attack（情境模拟攻击）和DH-CoT（劫持思维链攻击）。3. 对数据集和模型响应进行实验，评估检测和攻击的有效性。

Result: MDH框架在恶意内容检测方面实现了高效且准确的结果，有效提升了数据集清洗和有害内容检测效率。实验发现，经过精心设计的开发者提示能极大提升越狱攻击成功率。提出的D-Attack和DH-CoT策略在突破模型防护方面展现出更高攻击效果。相关的数据与判断结果已在GitHub开源。

Conclusion: 混合人机结合的MDH框架为恶意内容检测提供了更优方案，有助于提升越狱攻击评估的准确性。针对开发者提示的研究为攻防带来新思路，两种越狱新策略证明开发者提示的重要作用与攻击风险。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [163] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的黑盒对抗攻击方法，可以生成针对大语言模型（LLM）的对抗性样本，从而绕过当前最先进的防御机制。


<details>
  <summary>Details</summary>
Motivation: 目前，如何有效攻击（jailbreak）大语言模型并分析其安全漏洞，是提升模型鲁棒性和理解模型脆弱性的核心挑战。

Method: 作者提出Sparse Feature Perturbation Framework（SFPF）。该方法利用稀疏自编码器（SAE）识别文本中的关键特征，对模型隐藏层进行重建，然后聚类提取高激活特征，并对这些特征进行扰动，实现对抗文本生成。这样可在保留恶意意图的同时强化安全信号，增加对抗文本通过模型安全检查的概率。

Result: 实验证明，SFPF生成的对抗样本能有效绕过当前主流的安全防御手段，揭示现有NLP系统在安全性上的持续漏洞。

Conclusion: 该方法为红队测试和模型安全性分析提供了新思路，但对不同模型结构、不同层及更大的模型其泛化能力还有待进一步研究验证。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [164] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 该论文提出ComoRAG方法，有效提升大模型在处理长篇叙事文本（如小说）的推理能力，克服了传统RAG一次性检索和缺乏状态的问题，在多项长文本基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 长篇小说和故事因其复杂的情节和人物关系，对机器理解提出挑战。现有大语言模型在处理超长文本和推理时受到上下文窗口限制及推理能力下降的约束，且传统基于检索生成（RAG）的方法由于一次性无状态检索，难以捕捉故事内长距离的动态关系。

Method: ComoRAG受人类认知启发，将叙事推理视为动态、多轮的过程。方法包括：在推理遇到瓶颈时，迭代生成探测性检索查询，动态整合新获得的证据进全局记忆池。每一轮检索和证据整合为后续推理提供越来越完整的上下文支撑，从而实现有状态的、渐进式的复杂信息整合。

Result: ComoRAG在四个公开的长文本叙事理解基准（每个超20万token）上，相较最强的RAG基线方法有最高可达11%的相对性能增益。分析显示，尤其在需要全局理解的复杂查询场景下表现更为突出。

Conclusion: ComoRAG实现了检索式长文本理解中的有状态动态推理，为复杂叙事文本的机器理解提供了新的认知范式，提升了RAG模型在长距离关系捕捉和全局推理任务中的能力。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [165] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本研究提出了IdiomEval评测框架，对9个现代翻译系统的900个中文成语翻译进行了详细误差分析，发现现有系统在成语翻译上表现较差，即便最优（GPT-4）也有28%的错误率。现有自动评价指标与人工评价的相关性低，研究进一步开发了更有效的成语翻译错误检测模型。


<details>
  <summary>Details</summary>
Motivation: 成语在中文中极为常见，具有独特的语义和结构，对机器翻译系统是重大挑战。尽管大型语言模型取得显著进步，但针对中文成语翻译的系统性分析和评价仍然缺乏，现有评测方法也无法准确度量成语翻译质量。

Method: 提出了IdiomEval框架，构建了针对成语翻译的详细误差分类体系，并从网络、新闻、百科和社交媒体4个领域选取9个翻译系统的900个译文进行人工注释与分析。研究还评估了现有自动成语翻译评价指标的有效性，并训练了用于成语翻译错误检测的改进模型。

Result: 各主流翻译系统在成语翻译任务上表现不佳，常出现直译、部分或漏译等问题。最优秀的GPT-4系统也有28%的错误率。自动评价指标和人工打分的皮尔森相关系数低于0.48。改进模型在错误检测上达到了0.68的F1分数。

Conclusion: 当前主流翻译系统尚不能准确处理中文成语翻译，现有自动评价指标亦不理想。IdiomEval框架促进了成语翻译评测方法的发展，所提改进模型可为后续研究提供更精准的成语翻译质量检测与改进方向。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [166] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文将大型语言模型（LLM）的计算过程视为一种“计算经济学”问题，通过激励机制增强稀疏激活，实现更高效的计算和可控的性能损耗，并取得显著减少计算量的成果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模增大，计算成本飙升，现有模型在计算资源有限情况下表现受限，因此需要新的方法在有限资源下实现有效推理和训练。

Method: 作者提出了一种“计算经济学”框架，将模型内的注意力头和神经块视为资源受限的代理，通过引入基于激励的训练范式，将任务损失与可微分的计算成本结合，引导模型在保持性能的前提下主动实现计算稀疏化。

Result: 在GLUE（MNLI, STS-B, CoLA）和WikiText-103基准上，该方法得到了一系列沿帕累托前沿分布的模型。与事后剪枝相比，新模型在精度近似的情况下计算量（FLOPS）降低约40%、延迟减少，并展现出更易解释的注意力分布。

Conclusion: 经济学原理为在资源受限环境下设计高效、自适应且更透明的LLM提供了理论和实证依据，有望推动更节能且实用的大模型落地。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [167] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 该论文提出了DiFaR框架，以解决大型视觉-语言模型（LVLMs）在生成用于多模态虚假信息检测的文本理由时存在的多样性不足、事实性错误、无关内容等问题。


<details>
  <summary>Details</summary>
Motivation: 当前通过LVLMs为多模态虚假信息检测生成文本理由的方法存在三个核心挑战：生成理由多样性不足；因幻觉导致事实不准确；理由内容无关或冲突，增加噪音。因此，需要新的方法来提升理由的多样性、事实性和相关性，从而增强检测器性能。

Method: 提出了DiFaR框架：采用五种思维链（chain-of-thought）的提示方式，从LVLMs中提取多样化的推理路径。随后，利用一个轻量级的事后筛选模块，基于句子的事实性和相关性分数，选出合适的理由句。该方法与多种检测器及基线方法进行对比验证。

Result: 在四个主流基准数据集上，DiFaR在检测准确率上最多优于四类基线方法5.9%，可提升现有检测器性能最高8.7%。无论自动评测指标还是人工评估，DiFaR在理由多样性、事实性、相关性三方面均显著优于对比方法。

Conclusion: DiFaR能够生成多样、真实且相关的理由文本，为多模态虚假信息检测任务明显提升了理由质量和检测性能，具备通用性和适用性。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [168] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 这篇论文分析了NLP中可解释性与隐私保护之间的权衡，实证探讨二者能否共存，并提出了未来的实用建议。


<details>
  <summary>Details</summary>
Motivation: 可解释性和隐私保护是当前NLP领域受关注的两个重要方向，但两者交集处的关系尚未深入研究。该文旨在填补这一研究空白，探索能否在NLP中实现兼具可解释性和隐私保护。

Method: 论文以差分隐私（DP）和事后可解释性为主线方法，通过实证分析，研究不同下游任务及文本处理和解释方法对“可解释性-隐私保护”关系的影响。

Result: 结果揭示了可解释性和隐私性之间存在复杂关系，这种关系受到多个因素影响。部分场景下二者可以共存。

Conclusion: 论文认为实现可解释性与隐私保护的共存存在潜力，并为后续研究提供了实际建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [169] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文系统性地研究了多模态大模型（MLLMs）中的“文本主导”现象，并提出了衡量主导程度的新指标，发现文本主导问题广泛存在，并提出简单有效的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 之前的研究发现MLLMs在推理时严重依赖文本信息，导致其他模态如图像、音频等信息利用不足，但大多只关注视觉-语言场景，对其根本原因及跨多模态表现缺乏系统性分析。本文旨在全面揭示文本主导现象在多种数据模态中的普遍性及机制。

Method: 作者构建了系统性的评测框架，覆盖图像、视频、音频、时间序列和图结构数据，并提出两个新指标——模态主导指数（MDI）和注意力效率指数（AEI）来量化主导现象，同时从注意力机制、架构设计、任务设定等方面深入分析成因，并提出了一种基于Token压缩的简单改进方法。

Result: 实验证明，文本主导问题在各类数据模态中广泛分布。Token冗余导致了注意力稀释，融合架构与任务形式也加重了文本的主导地位。通过Token压缩方法，对LLaVA-7B等主流模型进行了改进，MDI显著降低，表明多模态关注更加均衡。

Conclusion: 文本主导是多模态大模型中的普遍且严重问题，需从建模、任务和数据层面共同应对。文中框架和方法为后续公正、高效的多模态模型研发提供了理论和实践基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [170] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本论文可视为在欧洲建立大型语言模型(LLM)可解释性分析基础设施eDIF的可行性研究，结果显示平台性能稳定、易用，受到研究人员欢迎，但也存在数据下载慢等待解决问题。


<details>
  <summary>Details</summary>
Motivation: 当前欧洲对LLM可解释性研究的基础设施较为匮乏，阻碍了研究的普及和进步。因此，论文旨在通过eDIF项目，为欧洲学者搭建便捷的LLM深层解释分析平台，促进先进模型分析技术的普及和社区建设。

Method: 论文介绍了在安斯巴赫应用科技大学搭建基于GPU的eDIF集群，并通过NNsight API实现远程模型检测。组织了来自全欧16位研究人员参与的结构化试点研究，对平台的技术性能、易用性和科研价值进行评估。用户实际对不同模型开展activation patching、causal tracing和representation analysis等实验。

Result: 试点研究表明，研究人员的活跃度逐步提升，平台整体运行稳定，远程实验功能获得积极反馈。同时，发现如激活数据下载慢、程序偶发中断等问题，并提出计划改进。

Conclusion: eDIF项目展示了欧洲搭建LLM可解释性通用平台的可行性，为后续广泛部署、工具拓展和社区协作提供了基础，对推动欧洲LLM可解释性研究具有重要意义。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [171] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 本文首次系统性地研究了科普特语到法语的翻译策略，包括翻译路径、预训练、版本多样化微调及抗噪性，发现多版本加抗噪微调能显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 历史语言的机器翻译工具难以开发，主要因语料稀缺和风格多样，本文旨在为低资源历史语言（如科普特语）开发更高效的翻译方法。

Method: 构建系统化实验体系，依托配准的圣经语料，对比枢轴翻译和直接翻译、预训练与否、多版本联合训练及对噪声的健壮性等因素，并提出在多风格且包含噪声的训练语料上微调模型。

Result: 多风格、抗噪声的训练语料显著提升了模型的翻译质量，直接与枢轴翻译、单一风格训练进行系统比较后均有改进。

Conclusion: 在低资源、历史语言翻译中，采用多版本融合和抗噪声的训练方案可以明显提升译文质量，对其他类似语言工具开发具有重要的参考意义。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [172] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 该论文提出了一种结合图结构方法和Transformer架构的新型连续孟加拉手语翻译方法，将Transformer与STGCN-LSTM进行融合，并在多个公开手语数据集上达到了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 聋哑人和听障人士面临交流障碍，而社会对手语的重视不足，现有的翻译方法还不够高效，尤其是在无需中间语素（gloss-free）的手语自动翻译方面存在挑战。

Method: 作者提出将图神经网络（STGCN-LSTM）与Transformer架构融合，探索了多种融合策略，实现了无需gloss的连续手语翻译，并在多个数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign、BornilDB v1.0）进行了评测。

Result: 所提方法在所有数据集上BLEU-4分数均优于现有主流方法，并首次在BornilDB v1.0上进行了基准测试。与GASLT和slt_how2sign等方法成果相比，提升了4.01、2.07和0.5等分数。

Conclusion: 该方法为手语翻译领域设立了新的基准，验证了gloss-free翻译的有效性，强调了提升聋哑人交流便利性的社会意义。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [173] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出了一种用自然语言反馈信号替代标量奖励信号的新个性化问答方法VAC，显著提升了个性化响应的效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大模型多采用RAG结合强化学习的标量奖励信号进行个性化训练，标量奖励往往反馈简单、指导性弱，限制了模型在效率和个性化质量上的提升。

Method: 引入VAC框架，用与用户画像和问题叙述相关的自然语言反馈（NLF）作为监督信号，训练过程中反馈模型和策略模型交替优化，策略模型在推理时无需额外反馈。

Result: 在LaMP-QA基准集三大领域实验，VAC方法在各项指标上均超过已有最优方案，并获得人类评估认可。

Conclusion: 自然语言反馈比传统标量奖励信号能更有效促进个性化问答模型的优化，提高了问答质量和用户满意度。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [174] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: ICE是一种为扩散型大语言模型（dLLMs）设计的原位链式思维提示方法，通过原位插入提示和置信度提前退出大幅提升性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型只支持前缀提示及顺序生成，限制了信息的双向流动和推理灵活性。扩散型大语言模型具备双向注意力和迭代生成能力，有望突破这些限制。

Method: 提出ICE框架，将链式思维提示从前缀模式转变为原位插入，直接在掩码位置插入提示；同时引入置信度驱动的提前退出机制，降低无效计算。

Result: ICE在GSM8K数据集上准确率提升最高达17.29%，推理速度提升4.12倍；在MMLU上最高提速276.67倍，性能接近主流方法。

Conclusion: ICE显著提高了dLLMs在效率和性能上的表现，为大语言模型的推理方式提供了更灵活高效的新范式。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [175] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种自动化新颖性评估方法，通过模拟专家评审的行为，显著提升论文新颖性评价的自动化水平，在实际评审数据上表现远超现有大模型基线。


<details>
  <summary>Details</summary>
Motivation: 当前NLP等高产领域的论文评审量巨大，评审人力紧张，而新颖性评估作为同行评审中的关键环节却研究不足。因此，亟需结构化且自动化的方法辅助新颖性评价，提高评审效率与一致性。

Method: 方法分为三步：1）从提交论文中提取核心内容，2）自动检索并综合相关文献，3）基于证据进行结构化对比和新颖性判定。此外，方法设计借鉴了大规模人工评审的新颖性写作分析，总结出独立主张验证和上下文推理等专家评审规律。最终利用人类标注的ICLR 2025论文，用LLM（大语言模型）实现该流程。

Result: 在182篇ICLR 2025论文及其人工新颖性标注上，方法与人工推理结果的对齐度为86.5%，新颖性结论一致度为75.3%，均显著高于现有基线大模型系统。模型还能输出详细且兼顾文献的分析，提高了评价一致性。

Conclusion: 结构化LLM方法能有效辅助高质量的新颖性评价，在提升评审透明度、一致性与系统性方面表现突出，有潜力成为高压评审场景下的重要工具，同时不会取代人类专家的核心作用。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [176] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 提出了一种名为MS-GRPO的新算法，实现小语言模型在多步决策任务中的有效后训练，性能超过大规模模型。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在序列决策任务中有潜力，但因模型大、计算成本高，实际应用受限。因此迫切需要提升小模型的能力。但现有后训练方法仅适用于单步交互，无法处理多步任务中的奖励归因问题。

Method: 提出了Multi-Step Group-Relative Policy Optimization（MS-GRPO）算法，借助Text-Mediated Stochastic Game（TSMG）和Language-Agent Policy（LAP）理论框架。该算法在奖励归因时，将整个回合累计奖励分配给每个步骤，并引入一种全新的基于优势的采样策略提升训练效果。

Result: 在Snake和Frozen Lake等任务中，对3B参数量模型进行后训练。实验表明，后训练后的小模型（3B）在Frozen Lake任务上表现优于72B参数的大模型50%。

Conclusion: 针对性后训练可以显著提升小型LLM在多步决策任务中的表现，提供了一种比单纯扩大模型规模更高效实用的替代路径。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [177] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 本文提出了Psyche-R1，这是首个集成共情、心理学知识和推理的中文心理健康大模型，显著提升了心理健康对话的可靠性和效果。


<details>
  <summary>Details</summary>
Motivation: 在心理健康专业人员短缺的背景下，心理健康领域大模型主要关注于情感支持，忽略了推理机制。本研究旨在构建一个融合推理能力和专业知识的中文心理健康大模型，提升对话质量和可靠性。

Method: 作者设计了新颖的数据合成流程，生成了7.5万份带推理链条心理问题及7.3万份共情对话数据。训练方面，采用多LLM交叉选择和群组相对策略优化（GRPO）提升推理，同时用常规监督微调增强共情与心理知识。

Result: Psyche-R1在多个心理学基准测试中表现出色。尤其是7B参数规模的Psyche-R1，其表现媲美671B规模的DeepSeek-R1。

Conclusion: Psyche-R1首次实现了心理学大模型在推理、共情和心理知识三方面的有机结合，为中文心理健康应用提供了更强大和可靠的AI工具。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [178] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多维度、可解释性的自动口译质量评估方法，针对语言使用质量分析、数据缺乏与不平衡、以及模型可解释性不足等问题，并在英中口译数据集上展示了优异的预测效果。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在自动口译质量评估方面取得进展，但在语言使用质量考察不足、由于数据稀缺与不平衡模型效果有限、以及缺乏模型解释方面存在不足，难以为学习者提供细致的反馈。

Method: 设计了融合特征工程、数据增强和可解释机器学习（如SHAP分析）的多维建模框架，使用透明且与构念高度相关的特征，避免'黑箱'化，并在新建英中口译数据集上训练模型。

Result: 所提出方法在英中口译数据集上预测效果良好。结果显示，BLEURT和CometKiwi用于忠实度预测最有效，停顿相关特征对流利度预测最强，中国特有的短语多样性指标最能预测语言运用质量。

Conclusion: 该方法兼顾了预测准确性和可解释性，是现有人工评价的可扩展、可靠与透明的替代方案，有助于为学习者提供诊断性反馈，促进其自主学习，是自动评分以外的重要补充。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [179] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本论文探索了利用大语言模型（LLM）作为强化学习（RL）代理搜索任务的高效模拟器，以降低对外部搜索引擎交互的成本和依赖。


<details>
  <summary>Details</summary>
Motivation: 当前RL中依赖外部搜索引擎完成复杂搜索任务具有高成本，亟需找到更经济、高效的替代方案。

Method: 作者首先提出Self-Search方法，用结构化提示和重复采样量化LLM自身的搜索能力；进而提出Self-Search RL（SSRL）框架，通过格式和规则奖励提升LLM的内生搜索，并用于RL策略模型训练，无需外部工具反复交互。

Result: 实验显示，LLM具备很强的世界知识召回能力，SSRL训练出的策略模型在问题解答和BrowseComp等基准测试中表现优异，RL训练环境稳定，具备成本优势，并在sim-to-real迁移方面效果良好。

Conclusion: 1）LLM的知识可被有效唤起，获得高性能；2）SSRL能有效利用内部知识，减少幻觉；3）SSRL模型可无缝集成外部搜索引擎。研究证明LLM可为更可扩展的RL训练提供支持。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [180] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文综述了扩散语言模型（DLMs）作为替代自回归（AR）模型的潜力及发展，介绍了其基础原理、主流模型、训练与推理优化、应用领域和面临挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来，DLMs因能够并行生成token和高效捕捉双向上下文，在推理效率和控制生成方面显示出明显优势。随着性能接近AR模型，其在自然语言处理领域受到关注，因此作者希望系统性梳理该领域现状与前沿。

Method: 本文系统梳理了DLM的发展历程，分析其与AR和掩码语言模型的关系，归纳主流技术路线。作者还详细总结了训练、推理、并行解码、多模态拓展等关键技术和实际应用，并对当前面临的问题进行了讨论。

Result: 作者编制了全面的DLM模型分类和技术清单，深入分析了现有模型和方法，并对推理加速、缓存机制、多模态扩展等进展进行了总结。同时，整理了应用案例和挑战。

Conclusion: 扩散语言模型已展现出良好的并行性和性能提升潜力，成为自然语言生成领域的重要路线。尽管还面临效率、长文本处理和基础设施等挑战，随着技术优化和研究推进，DLMs有望获得更广泛应用和进一步突破。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [181] [WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG](https://arxiv.org/abs/2508.10144)
*Xu Ma,Jiajie Zhang,Fujing Xie,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 该论文提出了一种结合WiFi信号与OpenStreetMap Area Graph (osmAG)的新型室内大规模定位框架，能够高效提升机器人在无GPS信号环境下的全球定位准确性和空间效率。


<details>
  <summary>Details</summary>
Motivation: 在缺失GPS信号的室内环境中，机器人全球定位存在极大挑战。以往基于WiFi指纹或简单三边测量的方法受限于标注工作量和环境变化，难以实现大规模、低成本、高精度定位，尤其在指纹数据缺失情况下，定位精度下降明显。

Method: 该方法利用osmAG的几何与拓扑结构先验，通过建模墙体对WiFi信号的衰减，采用迭代优化方法离线定位AP位置。在线阶段，机器人定位结合增强版osmAG地图，并结合WiFi观测，实现实时定位。在有/无指纹数据区域，分别与传统KNN指纹法等方法对比评测。

Result: 离线AP定位平均误差3.79米（较三边测量提升35.3%）；在线机器人定位，有指纹区域平均误差3.12米（较KNN提升8.77%），无指纹区域为3.83米（提升81.05%）；整体上定位精度和空间效率均优于指纹法。

Conclusion: 结合WiFi与osmAG的室内定位框架能有效提升大规模、多层复杂室内环境下机器人的定位精度与效率，尤其适用于指纹数据稀缺的场景，提供了一种可扩展、经济且有效的全球定位方案。

Abstract: Global localization is essential for autonomous robotics, especially in
indoor environments where the GPS signal is denied. We propose a novel
WiFi-based localization framework that leverages ubiquitous wireless
infrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor
environments. Our approach integrates signal propagation modeling with osmAG's
geometric and topological priors. In the offline phase, an iterative
optimization algorithm localizes WiFi Access Points (APs) by modeling wall
attenuation, achieving a mean localization error of 3.79 m (35.3\% improvement
over trilateration). In the online phase, real-time robot localization uses the
augmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas
(8.77\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted
areas (81.05\% improvement). Comparison with a fingerprint-based method shows
that our approach is much more space efficient and achieves superior
localization accuracy, especially for positions where no fingerprint data are
available. Validated across a complex 11,025 &m^2& multi-floor environment,
this framework offers a scalable, cost-effective solution for indoor robotic
localization, solving the kidnapped robot problem. The code and dataset are
available at https://github.com/XuMa369/osmag-wifi-localization.

</details>


### [182] [Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets](https://arxiv.org/abs/2508.10203)
*Matthew D. Osburn,Cameron K. Peterson,John L. Salmon*

Main category: cs.RO

TL;DR: 本文提出了一种通过拥挤动态环境生成最优、无碰撞、时变轨迹的方法，并介绍了如何在不需要初始猜测的情况下，利用ST-GCS（时空凸集图）框架有效求解该问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂的时空环境中进行路径规划时，由于空间和时间上的多重约束，给数值求解器提供合适的初始猜测非常困难。这类问题需要更高效、鲁棒的方法来自动生成无碰撞、最优的轨迹。

Method: 作者采用了凸集图（GCS）和其扩展的时空凸集图（ST-GCS）的方法，通过不依赖初始猜测的数值优化方式，生成距离最近的无碰撞轨迹，并提出了一种通用的约束适配策略，使任意约束能兼容此框架。

Result: 实验证明，在静态环境下，ST-GCS方法与标准GCS能生成相同的轨迹；在动态环境下，ST-GCS方法能高效找到最短距离、无碰撞的轨迹。

Conclusion: ST-GCS方法在动态环境下同样有效，且无须初始猜测，扩展了GCS工具的适用场景，为复杂动态环境中的路径规划提供了新的解决思路。

Abstract: In this paper, we create optimal, collision-free, time-dependent trajectories
through cluttered dynamic environments. The many spatial and temporal
constraints make finding an initial guess for a numerical solver difficult.
Graphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of
Convex Sets formulation (ST-GCS) enable us to generate optimal minimum distance
collision-free trajectories without providing an initial guess to the solver.
We also explore the derivation of general GCS-compatible constraints and
document an intuitive strategy for adapting general constraints to the
framework. We show that ST-GCS produces equivalent trajectories to the standard
GCS formulation when the environment is static. We then show ST-GCS operating
in dynamic environments to find minimum distance collision-free trajectories.

</details>


### [183] [Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis](https://arxiv.org/abs/2508.10269)
*Kejun Li,Jeeseop Kim,Maxime Brunet,Marine Pétriaux,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种混合数据驱动预测控制（HDDPC）框架，实现外骨骼机器人的强鲁棒性双足步行。该方法融合步态规划和接触调度，提升了在动态环境中的适应性和实时反应能力。通过在Atalante外骨骼上实验，验证了其鲁棒性和适应性提升。


<details>
  <summary>Details</summary>
Motivation: 实现能够实时应对环境变化的外骨骼机器人稳定步行是当前双足机器人领域的重要挑战，尤其是在需要动态环境适应和高效重规划的场景下。现有方法难以同时兼顾步态和足部接触调度的灵活性，由此激发了本研究的动机。

Method: 提出混合数据驱动预测控制（HDDPC）框架，基于Hankel矩阵对系统动力学建模，结合步态间（S2S）过渡信息，整合足部接触调度与连续轨迹规划，以实现高效、统一的实时行走运动生成。

Result: 在Atalante外骨骼平台上进行了验证实验。结果显示，该框架提升了机器人在动态环境中的步态鲁棒性和适应能力。

Conclusion: HDDPC方法能有效实现外骨骼强化鲁棒性和高度适应性的步行控制，为实际应用中的外骨骼机器人提供了更强的动态环境应对能力。

Abstract: Robust bipedal locomotion in exoskeletons requires the ability to dynamically
react to changes in the environment in real time. This paper introduces the
hybrid data-driven predictive control (HDDPC) framework, an extension of the
data-enabled predictive control, that addresses these challenges by
simultaneously planning foot contact schedules and continuous domain
trajectories. The proposed framework utilizes a Hankel matrix-based
representation to model system dynamics, incorporating step-to-step (S2S)
transitions to enhance adaptability in dynamic environments. By integrating
contact scheduling with trajectory planning, the framework offers an efficient,
unified solution for locomotion motion synthesis that enables robust and
reactive walking through online replanning. We validate the approach on the
Atalante exoskeleton, demonstrating improved robustness and adaptability.

</details>


### [184] [ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver](https://arxiv.org/abs/2508.10333)
*Wenxuan Song,Ziyang Zhou,Han Zhao,Jiayi Chen,Pengxiang Ding,Haodong Yan,Yuxin Huang,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出ReconVLA模型，通过重建性隐式视觉锚定提升机器人对视觉目标的注意分配，显著增强了精准操作与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action（VLA）模型在执行任务时无法有效将视觉注意力集中于目标区域，视觉关注容易分散，影响精准操作。

Method: 提出ReconVLA模型，采用重建性隐式锚定范式：让扩散Transformer根据模型视觉输出重建图像中的注视区域，引导模型学会细粒度表征和注意力分配。并构建了包含10万+轨迹、200万+样本的大规模预训练数据集。

Result: 在仿真和真实环境中，ReconVLA表现出更准确的操作能力和更强的泛化能力，优于其他VLA方法。

Conclusion: ReconVLA通过隐式锚定和大规模预训练，有效提升了机器人在复杂任务中的视觉关注与操作能力，推动了多模态感知与机器操作的结合。

Abstract: Recent advances in Vision-Language-Action (VLA) models have enabled robotic
agents to integrate multimodal understanding with action execution. However,
our empirical analysis reveals that current VLAs struggle to allocate visual
attention to target regions. Instead, visual attention is always dispersed. To
guide the visual attention grounding on the correct target, we propose
ReconVLA, a reconstructive VLA model with an implicit grounding paradigm.
Conditioned on the model's visual outputs, a diffusion transformer aims to
reconstruct the gaze region of the image, which corresponds to the target
manipulated objects. This process prompts the VLA model to learn fine-grained
representations and accurately allocate visual attention, thus effectively
leveraging task-specific visual information and conducting precise
manipulation. Moreover, we curate a large-scale pretraining dataset comprising
over 100k trajectories and 2 million data samples from open-source robotic
datasets, further boosting the model's generalization in visual reconstruction.
Extensive experiments in simulation and the real world demonstrate the
superiority of our implicit grounding method, showcasing its capabilities of
precise manipulation and generalization. Our project page is
https://zionchow.github.io/ReconVLA/.

</details>


### [185] [BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots](https://arxiv.org/abs/2508.10363)
*Donipolo Ghimire,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: 论文提出了一种新的移动机器人寻源策略BEASST，通过平衡探索和利用，使机器人在复杂未知环境中更加高效地定位信号源，并且在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人寻源方法在探索（覆盖新区域）与利用（快速接近目标）之间难以高效平衡，尤其在信号环境复杂或未知情况下。本研究试图通过一种新的建模与行为调整框架，提高移动机器人在这些环境下的寻源效率和鲁棒性。

Method: 方法核心是构建基于行为熵与Prelec概率加权函数的目标函数，用归一化信号强度建模机器人对源位置的概率估计，再根据信号可靠性和任务紧迫度自适应调整机器人从规避风险（探索）到冒险追逐（利用）的行为。在单峰信号假设下有理论收敛保证，并能在有扰动时保持稳定。

Result: 在DARPA SubT和多房间等真实场景下，与现有最优方法相比，BEASST方法平均减少了15%路径长度、提升了20%定位速度，反映出在不确定性引导下智能调配探索和追逐的优势。

Conclusion: BEASST为移动机器人复杂环境下的自适应寻源问题提供了新的理论与实践路径，兼具收敛性和实际有效性，在相关任务和仿真/现实验证中表现突出。

Abstract: This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive
Source Seeking for Mobile Robots), a novel framework for robotic source seeking
in complex, unknown environments. Our approach enables mobile robots to
efficiently balance exploration and exploitation by modeling normalized signal
strength as a surrogate probability of source location. Building on Behavioral
Entropy(BE) with Prelec's probability weighting function, we define an
objective function that adapts robot behavior from risk-averse to risk-seeking
based on signal reliability and mission urgency. The framework provides
theoretical convergence guarantees under unimodal signal assumptions and
practical stability under bounded disturbances. Experimental validation across
DARPA SubT and multi-room scenarios demonstrates that BEASST consistently
outperforms state-of-the-art methods, achieving 15% reduction in path length
and 20% faster source localization through intelligent uncertainty-driven
navigation that dynamically transitions between aggressive pursuit and cautious
exploration.

</details>


### [186] [Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning](https://arxiv.org/abs/2508.10371)
*Wenqi Zheng,Yutaka Arakawa*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉强化学习的多模态大模型方法（FAVOR），以提升小样本下的人体活动识别能力。


<details>
  <summary>Details</summary>
Motivation: 目前大模型强化学习主要用于生成任务，对多模态领域（如人体活动识别）应用较少。现有多模态人体活动识别在小样本场景下泛化能力有限，需要新的方法提升模型性能。

Method: 使用多模态大语言模型（MLLM）结合视觉强化学习，生成包含推理过程和答案的多组候选响应。利用奖励函数对结果评价，并用Group Relative Policy Optimization (GRPO) 算法优化模型，从而借助少量数据提升人体活动识别效果。

Result: 在四个人体活动识别数据集及五种不同设定下，FAVOR方法表现优越，显著提升了小样本人体活动识别的泛化与解释能力。

Conclusion: 视觉强化学习能有效提升多模态大模型在小样本人体活动识别中的泛化与推理能力，并具备良好可解释性。

Abstract: Reinforcement learning in large reasoning models enables learning from
feedback on their outputs, making it particularly valuable in scenarios where
fine-tuning data is limited. However, its application in multi-modal human
activity recognition (HAR) domains remains largely underexplored. Our work
extends reinforcement learning to the human activity recognition domain with
multimodal large language models. By incorporating visual reinforcement
learning in the training process, the model's generalization ability on
few-shot recognition can be greatly improved. Additionally, visual
reinforcement learning can enhance the model's reasoning ability and enable
explainable analysis in the inference stage. We name our few-shot human
activity recognition method with visual reinforcement learning FAVOR.
Specifically, our approach first utilizes a multimodal large language model
(MLLM) to generate multiple candidate responses for the human activity image,
each containing reasoning traces and final answers. These responses are then
evaluated using reward functions, and the MLLM model is subsequently optimized
using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the
MLLM model can be adapted to human activity recognition with only a few
samples. Extensive experiments on four human activity recognition datasets and
five different settings demonstrate the superiority of the proposed method.

</details>


### [187] [A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons](https://arxiv.org/abs/2508.10378)
*Yu Chen,Shu Miao,Chunyu Wu,Jingsong Mu,Bo OuYang,Xiang Li*

Main category: cs.RO

TL;DR: 该论文提出了一种集成大语言模型的智能上肢外骨骼方案，可根据任务语义自适应调整辅助参数，并通过异常检测和在线控制提升安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有上肢外骨骼难以理解任务语义，无法结合用户意图协同规划，导致泛化能力不足，难以满足家庭护理等复杂应用需求。

Method: 提出基于大语言模型的语义感知框架。通过外骨骼透明模式捕获操作意图，解析任务描述获取语义信息并自动调整辅助参数。引入基于扩散模型的异常检测，实时监控人机交互状态并重规划。同时利用在线轨迹优化和阻抗控制保障执行安全。

Result: 实验证明该方法能够更好地与用户认知对齐，适应不同类型语义任务，并在任务过程中对异常做出可靠响应。

Conclusion: 所提外骨骼系统提升了人机协作能力、任务适应性及安全性，有望拓展至实际复杂护理需求。

Abstract: Upper-limb exoskeletons are primarily designed to provide assistive support
by accurately interpreting and responding to human intentions. In home-care
scenarios, exoskeletons are expected to adapt their assistive configurations
based on the semantic information of the task, adjusting appropriately in
accordance with the nature of the object being manipulated. However, existing
solutions often lack the ability to understand task semantics or
collaboratively plan actions with the user, limiting their generalizability. To
address this challenge, this paper introduces a semantic-aware framework that
integrates large language models into the task planning framework, enabling the
delivery of safe and intent-integrative assistance. The proposed approach
begins with the exoskeleton operating in transparent mode to capture the
wearer's intent during object grasping. Once semantic information is extracted
from the task description, the system automatically configures appropriate
assistive parameters. In addition, a diffusion-based anomaly detector is used
to continuously monitor the state of human-robot interaction and trigger
real-time replanning in response to detected anomalies. During task execution,
online trajectory refinement and impedance control are used to ensure safety
and regulate human-robot interaction. Experimental results demonstrate that the
proposed method effectively aligns with the wearer's cognition, adapts to
semantically varying tasks, and responds reliably to anomalies.

</details>


### [188] [Super LiDAR Reflectance for Robotic Perception](https://arxiv.org/abs/2508.10398)
*Wei Gao,Jie Zhang,Mingle Zhao,Zhiyuan Zhang,Shu Kong,Maani Ghaffari,Dezhen Song,Cheng-Zhong Xu,Hui Kong*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，将稀疏LiDAR数据生成高密度反射率图像，突破低成本LiDAR在机器人感知中的应用瓶颈，并给出相关数据集与算法，可提升如闭环检测和车道识别等多个任务效果。


<details>
  <summary>Details</summary>
Motivation: 传统视觉主要为被动传感，主动光学感知如LiDAR在低照明不变性等方面展现特殊优势，但高分辨率LiDAR成本高昂，低成本LiDAR因数据稀疏性应用受限，亟需提升其数据密度用于实际应用。

Method: 提出创新框架，利用非重复扫描LiDAR（NRS-LiDAR）特性，通过反射率校准与静动态场景域迁移等关键技术，实现稀疏数据到高密度反射图像的重建，并设计了专门的密集化网络。构建了相关数据集以支持方法开发和评测。

Result: 文中方法能够有效提升稀疏LiDAR数据的反射率图像密度，在实际场景中验证了对闭环检测和车道线识别等下游任务的正向提升效果。

Conclusion: 该工作突破了低成本稀疏LiDAR应用的主要障碍，为主动视觉与机器人感知提供了新工具，有助于高性能、低成本实际部署。

Abstract: Conventionally, human intuition often defines vision as a modality of passive
optical sensing, while active optical sensing is typically regarded as
measuring rather than the default modality of vision. However, the situation
now changes: sensor technologies and data-driven paradigms empower active
optical sensing to redefine the boundaries of vision, ushering in a new era of
active vision. Light Detection and Ranging (LiDAR) sensors capture reflectance
from object surfaces, which remains invariant under varying illumination
conditions, showcasing significant potential in robotic perception tasks such
as detection, recognition, segmentation, and Simultaneous Localization and
Mapping (SLAM). These applications often rely on dense sensing capabilities,
typically achieved by high-resolution, expensive LiDAR sensors. A key challenge
with low-cost LiDARs lies in the sparsity of scan data, which limits their
broader application. To address this limitation, this work introduces an
innovative framework for generating dense LiDAR reflectance images from sparse
data, leveraging the unique attributes of non-repeating scanning LiDAR
(NRS-LiDAR). We tackle critical challenges, including reflectance calibration
and the transition from static to dynamic scene domains, facilitating the
reconstruction of dense reflectance images in real-world settings. The key
contributions of this work include a comprehensive dataset for LiDAR
reflectance image densification, a densification network tailored for
NRS-LiDAR, and diverse applications such as loop closure and traffic lane
detection using the generated dense reflectance images.

</details>


### [189] [Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning](https://arxiv.org/abs/2508.10399)
*Wenlong Liang,Rui Zhou,Yang Ma,Bing Zhang,Songlin Li,Yijia Liao,Ping Kuang*

Main category: cs.RO

TL;DR: 该论文综述了大模型促进具身智能体自主决策与学习的最新进展，探讨了层次化与端到端决策、主流学习方法，以及世界模型在其中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管具身智能经过多年探索，仍难以在开放动态环境实现类人智能。近期大模型的突破为提升具身智能感知、交互、规划和学习能力带来革新，因此作者希望梳理和总结大模型赋能下具身智能的进展。

Method: 论文系统梳理了大模型如何提升具身智能体的决策与学习能力，覆盖层次化决策（高层规划、低层执行与反馈）、端到端决策（视觉-语言-动作模型）、模仿学习、强化学习，并首次将世界模型的应用集成进具身智能综述中。

Result: 目前大模型极大提升了具身智能体在高层规划、低层控制、反馈、模仿学习和强化学习等方面的能力，世界模型在决策与学习中日益关键。

Conclusion: 大模型显著赋能了具身智能体的感知、决策与学习，但实现通用智能体仍面临诸多挑战，未来可围绕开放性、泛化能力等方向深入研究。

Abstract: Embodied AI aims to develop intelligent systems with physical forms capable
of perceiving, decision-making, acting, and learning in real-world
environments, providing a promising way to Artificial General Intelligence
(AGI). Despite decades of explorations, it remains challenging for embodied
agents to achieve human-level intelligence for general-purpose tasks in open
dynamic environments. Recent breakthroughs in large models have revolutionized
embodied AI by enhancing perception, interaction, planning and learning. In
this article, we provide a comprehensive survey on large model empowered
embodied AI, focusing on autonomous decision-making and embodied learning. We
investigate both hierarchical and end-to-end decision-making paradigms,
detailing how large models enhance high-level planning, low-level execution,
and feedback for hierarchical decision-making, and how large models enhance
Vision-Language-Action (VLA) models for end-to-end decision making. For
embodied learning, we introduce mainstream learning methodologies, elaborating
on how large models enhance imitation learning and reinforcement learning
in-depth. For the first time, we integrate world models into the survey of
embodied AI, presenting their design methods and critical roles in enhancing
decision-making and learning. Though solid advances have been achieved,
challenges still exist, which are discussed at the end of this survey,
potentially as the further research directions.

</details>


### [190] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的“自我纠错飞轮”后训练范式，用于提升视觉-语言导航模型的纠错与恢复能力，并实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航模型在执行导航指令时容易偏离正确路径，但缺乏有效的纠错能力，难以自我恢复。为解决这一问题，作者希望提供一种机制，让模型在出错后能自我改进。

Method: 提出“自我纠错飞轮”范式：将模型在训练集上的错误轨迹视为宝贵数据，通过自动方法识别偏离点，生成用于感知和动作的自我纠错数据，并利用这些数据持续训练模型。每轮迭代都会发现新错误、生成新纠错数据，形成“飞轮”正循环，逐步提升导航模型CorrectNav。

Result: 在R2R-CE和RxR-CE基准测试上，CorrectNav成功率分别达65.1%和69.3%，较此前最优模型提高8.2%和16.4%。实机器人测试也验证了其出色的纠错、动态避障和长指令遵循能力。

Conclusion: “自我纠错飞轮”有效提升了视觉-语言导航模型的鲁棒性和性能，为导航系统的持续自我改进提供了新思路和方法。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


### [191] [MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion](https://arxiv.org/abs/2508.10423)
*Qi Liu,Xiaopeng Zhang,Mingshan Tan,Shuaikang Ma,Jinliang Ding,Yanjie Li*

Main category: cs.RO

TL;DR: 本文提出了一种将多智能体协作深度强化学习（MARL）应用于单个人形机器人运动优化的新方法，将每个四肢作为独立智能体，从而提升机器人运动表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多在单个机器人采用单智能体强化学习，或在多机器人系统中用MARL，而本文关注如何用多智能体协作优化单个人形机器人的运动，填补该领域的空白。

Method: 提出MASH方法，将人形机器人每只手与腿视为独立智能体，采取合作异构MARL，各肢体独立探索动作空间，但共享全局评价器（critic），实现协同学习。

Result: 实验证明MASH方法能加快训练收敛速度，提高全身协作能力，在性能上优于传统单智能体强化学习方法。

Conclusion: 本工作拓展了MARL在单个人形机器人控制上的应用，为高效运动策略提供了新见解。

Abstract: This paper proposes a novel method to enhance locomotion for a single
humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement
learning (MARL). While most existing methods typically employ single-agent
reinforcement learning algorithms for a single humanoid robot or MARL
algorithms for multi-robot system tasks, we propose a distinct paradigm:
applying cooperative-heterogeneous MARL to optimize locomotion for a single
humanoid robot. The proposed method, multi-agent reinforcement learning for
single humanoid locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot's action space while sharing a global
critic for cooperative learning. Experiments demonstrate that MASH accelerates
training convergence and improves whole-body cooperation ability, outperforming
conventional single-agent reinforcement learning methods. This work advances
the integration of MARL into single-humanoid-robot control, offering new
insights into efficient locomotion strategies.

</details>


### [192] [Enabling Generic Robot Skill Implementation Using Object Oriented Programming](https://arxiv.org/abs/2508.10497)
*Abdullah Farrukh,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 本文提出一种软件框架，旨在简化机器人系统的接口及集成难度，降低企业和研究人员使用机器人系统的门槛。通过采用抽象层与Python实现，验证了其在具体应用中的可行性。


<details>
  <summary>Details</summary>
Motivation: 在中小企业缺乏机器人专业知识的情况下，部署和维护机器人系统十分困难，往往导致对外部供应商依赖甚至被锁定。学界在智能制造领域也面临类似问题，研究人员希望集成机器人却不想处理复杂的接口。因此，迫切需要更简单、通用的集成解决方案。

Method: 本文提出并用Python实现了一个软件框架，将不同制造商和型号机器人的接口抽象，形成统一的接口层。以Yaskawa Motoman GP4为例，构建实际原型系统进行验证。

Result: 基于该框架，成功实现了对机器人系统的简化集成和控制，显著降低了部署难度。系统原型以bin-picking为应用场景，验证了框架的可行性和实用性。

Conclusion: 该抽象框架能够有效降低机器人系统的集成与开发门槛，减少对供应商的依赖，有助于中小企业和研究人员更便捷地利用各类机器人系统。

Abstract: Developing robotic algorithms and integrating a robotic subsystem into a
larger system can be a difficult task. Particularly in small and medium-sized
enterprises (SMEs) where robotics expertise is lacking, implementing,
maintaining and developing robotic systems can be a challenge. As a result,
many companies rely on external expertise through system integrators, which, in
some cases, can lead to vendor lock-in and external dependency. In the academic
research on intelligent manufacturing systems, robots play a critical role in
the design of robust autonomous systems. Similar challenges are faced by
researchers who want to use robotic systems as a component in a larger smart
system, without having to deal with the complexity and vastness of the robot
interfaces in detail. In this paper, we propose a software framework that
reduces the effort required to deploy a working robotic system. The focus is
solely on providing a concept for simplifying the different interfaces of a
modern robot system and using an abstraction layer for different manufacturers
and models. The Python programming language is used to implement a prototype of
the concept. The target system is a bin-picking cell containing a Yaskawa
Motoman GP4.

</details>


### [193] [KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection](https://arxiv.org/abs/2508.10511)
*Andrea Rosasco,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出了一种基于核密度估计（KDE）的策略（KDPE），用于在采用Diffusion Policy进行机器人行为克隆时过滤有害的轨迹，提高策略的表现和安全性。


<details>
  <summary>Details</summary>
Motivation: 近年来，Diffusion Policy利用扩散模型生成机器人动作轨迹，取得了业界领先的表现，但其去噪过程的随机性和对数据异常值的敏感性导致机器人的行为可能偏离训练数据分布。当前的一些方法试图结合Diffusion Policy与大规模训练或经典行为克隆方法减轻这些问题，然而仍存在不足。作者因此希望提出一种新方法来解决以上两个缺陷。

Method: 作者提出了KDPE方法，通过核密度估计从Diffusion Policy生成的动作轨迹中筛除可能危害机器人的轨迹。具体而言，设计了能够感知流形结构的核函数，用于处理包含末端执行器空间位置、姿态和夹爪状态的动作空间概率密度建模，从而保留高概率的合理轨迹，剔除异常轨迹。

Result: KDPE在模拟单臂任务和真实机器人实验中表现优于仅用Diffusion Policy的方法，提升了策略的整体性能。

Conclusion: KDPE能够有效过滤Diffusion Policy产生的潜在有害轨迹，同时保持低的测试时计算开销，在机器人行为克隆学习中实现了更优的性能和可靠性。

Abstract: Learning robot policies that capture multimodality in the training data has
been a long-standing open challenge for behavior cloning. Recent approaches
tackle the problem by modeling the conditional action distribution with
generative models. One of these approaches is Diffusion Policy, which relies on
a diffusion model to denoise random points into robot action trajectories.
While achieving state-of-the-art performance, it has two main drawbacks that
may lead the robot out of the data distribution during policy execution. First,
the stochasticity of the denoising process can highly impact on the quality of
generated trajectory of actions. Second, being a supervised learning approach,
it can learn data outliers from the dataset used for training. Recent work
focuses on mitigating these limitations by combining Diffusion Policy either
with large-scale training or with classical behavior cloning algorithms.
Instead, we propose KDPE, a Kernel Density Estimation-based strategy that
filters out potentially harmful trajectories output of Diffusion Policy while
keeping a low test-time computational overhead. For Kernel Density Estimation,
we propose a manifold-aware kernel to model a probability density function for
actions composed of end-effector Cartesian position, orientation, and gripper
state. KDPE overall achieves better performance than Diffusion Policy on
simulated single-arm tasks and real robot experiments.
  Additional material and code are available on our project page
https://hsp-iit.github.io/KDPE/.

</details>


### [194] [MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm](https://arxiv.org/abs/2508.10538)
*Xin Liu,Bida Ma,Chenkun Qi,Yan Ding,Zhaxizhuoma,Guorong Zhang,Pengan Chen,Kehui Liu,Zhongjie Jia,Chuyue Guan,Yule Mo,Jiaqi Liu,Feng Gao,Jiangwei Zhong,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了基于现实与仿真结合的深度强化学习框架，实现了四足机器人带机械臂的多任务全身动作与操控能力，并能无缝迁移到真实环境。


<details>
  <summary>Details</summary>
Motivation: 目前四足机器人带机械臂的全身动作与操作的多任务控制具有很大挑战性，主要体现在多任务之间的平衡与泛化能力有限、现实部署难等问题。

Method: 作者提出MLM强化学习框架，结合现实和仿真数据，同时引入了包含自适应课程采样机制的轨迹库，利用历史真实轨迹进行多任务学习。为适应仅有历史观测的情境，还设计了轨迹-速度预测网络，用以推断未来轨迹和速度。

Result: 所提框架在仿真中实现了多任务的全身动作-操控行为，并能够零样本迁移到现实机器人。消融实验证明了各关键模块的有效性，实物实验表明该方法在多任务执行中表现优异。

Conclusion: MLM框架可以高效实现在多种复杂任务下，四足机器人带机械臂的全身协同运动与操作，是机器人多任务自主或远程控制的有效技术路线。

Abstract: Whole-body loco-manipulation for quadruped robots with arm remains a
challenging problem, particularly in achieving multi-task control. To address
this, we propose MLM, a reinforcement learning framework driven by both
real-world and simulation data. It enables a six-DoF robotic arm--equipped
quadruped robot to perform whole-body loco-manipulation for multiple tasks
autonomously or under human teleoperation. To address the problem of balancing
multiple tasks during the learning of loco-manipulation, we introduce a
trajectory library with an adaptive, curriculum-based sampling mechanism. This
approach allows the policy to efficiently leverage real-world collected
trajectories for learning multi-task loco-manipulation. To address deployment
scenarios with only historical observations and to enhance the performance of
policy execution across tasks with different spatial ranges, we propose a
Trajectory-Velocity Prediction policy network. It predicts unobservable future
trajectories and velocities. By leveraging extensive simulation data and
curriculum-based rewards, our controller achieves whole-body behaviors in
simulation and zero-shot transfer to real-world deployment. Ablation studies in
simulation verify the necessity and effectiveness of our approach, while
real-world experiments on the Go2 robot with an Airbot robotic arm demonstrate
the policy's good performance in multi-task execution.

</details>


### [195] [Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality](https://arxiv.org/abs/2508.10603)
*Agnes Axelsson,Merle Reimann,Ronald Cumbal,Hannah Pelikan,Divesh Lala*

Main category: cs.RO

TL;DR: 本文提出采用人种志片段（ethnographic vignettes）的方法，来更好地揭示和记录人机交互（HRI）中容易被忽视的失败案例，从而补充现有的评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）推动了人机交互的发展，但系统依然存在与人类间交流相比的次优现象。许多失败情形与具体情境相关，且在HRI研究中缺乏系统性记录和归纳，尤其是那些不易被报告的失败。

Method: 作者提出引入人种志片段这一原本被HRI领域忽视的质性研究手段，通过撰写和整理基于个人经验的多个失败交互片段，展示其在捕捉和交流失败情境上的优势。

Result: 通过案例撰写展示，人种志片段能够从多学科视角揭露系统缺陷，提升机器人能力的透明度，并记录那些传统论文易忽视的异常行为。

Conclusion: 作者强调人种志片段有助于丰富现有人机交互评估方法，促进失败信息共享，提升HRI研究的深度和透明度，并呼吁业界采用该方法辅助评估。

Abstract: Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.

</details>


### [196] [Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots](https://arxiv.org/abs/2508.10634)
*Mehdi Heydari Shahna,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种面向重型轮式移动机器人（WMR）的分层安全控制架构，通过组合深度神经网络（DNN）控制策略和模型无关鲁棒自适应控制（RAC）策略，实现了高精度与高安全性的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于动态建模的控制方法计算复杂，对高要求安全标准的重型WMR部署基于黑盒的DNN控制仍有很大挑战，主要体现在对故障和干扰的响应不足。

Method: 设计了一种由主控DNN策略和两个不同权限等级安全层级组成的分层控制策略。主控DNN在正常情况下提供高精度控制；当系统遭遇一定强度的干扰后，低阶安全层切换到鲁棒自适应控制（RAC），优先保障稳定性；高阶安全层持续监控，一旦干扰超出补偿能力就启动停机。

Result: 该方法可实现整个WMR系统的一致指数稳定，并在一定程度上满足国际安全标准要求。通过实际6000kg WMR系统的实时实验验证了该方法的有效性。

Conclusion: 深度神经网络与鲁棒自适应控制的融合能够在保障安全与运行性能的同时，推动DNN等黑盒方法在重型移动机器人上的应用落地。

Abstract: Deep neural networks (DNNs) can enable precise control while maintaining low
computational costs by circumventing the need for dynamic modeling. However,
the deployment of such black-box approaches remains challenging for heavy-duty
wheeled mobile robots (WMRs), which are subject to strict international
standards and prone to faults and disturbances. We designed a hierarchical
control policy for heavy-duty WMRs, monitored by two safety layers with
differing levels of authority. To this end, a DNN policy was trained and
deployed as the primary control strategy, providing high-precision performance
under nominal operating conditions. When external disturbances arise and reach
a level of intensity such that the system performance falls below a predefined
threshold, a low-level safety layer intervenes by deactivating the primary
control policy and activating a model-free robust adaptive control (RAC)
policy. This transition enables the system to continue operating while ensuring
stability by effectively managing the inherent trade-off between system
robustness and responsiveness. Regardless of the control policy in use, a
high-level safety layer continuously monitors system performance during
operation. It initiates a shutdown only when disturbances become sufficiently
severe such that compensation is no longer viable and continued operation would
jeopardize the system or its environment. The proposed synthesis of DNN and RAC
policy guarantees uniform exponential stability of the entire WMR system while
adhering to safety standards to some extent. The effectiveness of the proposed
approach was further validated through real-time experiments using a 6,000 kg
WMR.

</details>


### [197] [An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)](https://arxiv.org/abs/2508.10686)
*Carla Wehner,Finn Schubert,Heiko Hellkamp,Julius Hahnewald,Kilian Scheafer,Muhammad Bilal Khan,Oliver Gutfleisch*

Main category: cs.RO

TL;DR: 本文提出了一个基于SOFA的开源、用户友好型仿真平台，专为磁性软体机器人设计，支持实时形变模拟，兼顾易用性及功能性。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人仿真平台缺乏对磁性材料的专门支持，操作复杂，难以满足不同水平研究者的需求，特别是在磁性软体机器人领域。

Method: 开发了一个基于Simulation Open Framework Architecture（SOFA）的仿真界面，允许用户定义材料属性、施加磁场，并实时观看形变，同时集成可视化、应力分析等直观控制模块。通过四个基准模型（梁、三指和四指夹爪、蝴蝶）展示该平台功能。

Result: 该软件不仅能够对磁性软体机器人的形变进行可视化仿真，还具备易用性，适合初学者和高级研究者。测试模型表明其在功能和操作层面都有良好表现。

Conclusion: 本工作填补了磁性软体机器人仿真工具的空白，推动理论建模与实际设计的结合。未来工作将通过实验验证和与工业标准有限元对比继续提高仿真准确性。

Abstract: Soft robots, particularly magnetic soft robots, require specialized
simulation tools to accurately model their deformation under external magnetic
fields. However, existing platforms often lack dedicated support for magnetic
materials, making them difficult to use for researchers at different expertise
levels. This work introduces an open-source, user-friendly simulation interface
using the Simulation Open Framework Architecture (SOFA), specifically designed
to model magnetic soft robots. The tool enables users to define material
properties, apply magnetic fields, and observe resulting deformations in real
time. By integrating intuitive controls and stress analysis capabilities, it
aims to bridge the gap between theoretical modeling and practical design. Four
benchmark models - a beam, three- and four-finger grippers, and a butterfly -
demonstrate its functionality. The software's ease of use makes it accessible
to both beginners and advanced researchers. Future improvements will refine
accuracy through experimental validation and comparison with industry-standard
finite element solvers, ensuring realistic and predictive simulations of
magnetic soft robots.

</details>


### [198] [Biasing Frontier-Based Exploration with Saliency Areas](https://arxiv.org/abs/2508.10689)
*Matteo Luperto,Valerii Stakanov,Giacomo Boracchi,Nicola Basilico,Francesco Amigoni*

Main category: cs.RO

TL;DR: 本文提出了一种利用神经网络生成显著性地图，指导机器人在自主探索未知环境时优先探索重要区域的新方法。实验表明，该方法可提升探索效率和策略表现。


<details>
  <summary>Details</summary>
Motivation: 传统的自主探索方法往往仅以最大化已探索区域为目标，忽视了环境中有些区域对发现更多未知空间更加关键。作者认为，将显著性区域引入探索策略，有望更智能地分配探索资源，提高效率。

Method: 作者设计了一种结合神经网络和显著性地图的方法。神经网络根据当前地图输出显著性区域，并据此定义探索的终止判据。然后，通过将显著性区域的知识结合进多种经典探索策略中，来优化探索路径和目标点选择。

Result: 通过大量实验，结果显示显著性区域的知识能显著影响机器人探索过程，使其更有效率地探索未知环境，并优先访问对地图结构影响较大的关键区域。

Conclusion: 引入显著性区域可优化现有探索策略，提升机器人的探索性能。基于神经网络的显著性推断对引导探索表现出明显益处。

Abstract: Autonomous exploration is a widely studied problem where a robot
incrementally builds a map of a previously unknown environment. The robot
selects the next locations to reach using an exploration strategy. To do so,
the robot has to balance between competing objectives, like exploring the
entirety of the environment, while being as fast as possible. Most exploration
strategies try to maximise the explored area to speed up exploration; however,
they do not consider that parts of the environment are more important than
others, as they lead to the discovery of large unknown areas. We propose a
method that identifies \emph{saliency areas} as those areas that are of high
interest for exploration, by using saliency maps obtained from a neural network
that, given the current map, implements a termination criterion to estimate
whether the environment can be considered fully-explored or not. We use
saliency areas to bias some widely used exploration strategies, showing, with
an extensive experimental campaign, that this knowledge can significantly
influence the behavior of the robot during exploration.

</details>


### [199] [Learning Task Execution Hierarchies for Redundant Robots](https://arxiv.org/abs/2508.10780)
*Alessandro Adami,Aris Synodinos,Matteo Iovino,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 本文提出了一种自动学习任务优先级和参数的新型框架，使复杂机器人能无需专家手动设计算法，自主实现多任务控制。


<details>
  <summary>Details</summary>
Motivation: 现代复杂机器人具备高冗余能力，可以同时完成多个任务，但如何有效管理这种冗余是实现灵活可靠行为的关键。目前常用的任务栈（SoT）方法依赖专家手工设计，难以适应变化且门槛较高。

Method: 提出将强化学习和遗传编程相结合，从用户自定义目标出发，自动学习任务优先级和控制策略。以精度、安全性和执行时间等指标为代价函数，引导学习过程。

Result: 在高冗余的移动-YuMi双臂移动机器人上，通过仿真和实物实验验证，机器人能够动态适应环境变化，平衡多个目标，并保持鲁棒执行。

Conclusion: 本方法为复杂机器人的冗余管理提供了一种通用、易用且非专业依赖的方案，推动了以用户为中心的机器人编程方式，降低了专家设计门槛。

Abstract: Modern robotic systems, such as mobile manipulators, humanoids, and aerial
robots with arms, often possess high redundancy, enabling them to perform
multiple tasks simultaneously. Managing this redundancy is key to achieving
reliable and flexible behavior. A widely used approach is the Stack of Tasks
(SoT), which organizes control objectives by priority within a unified
framework. However, traditional SoTs are manually designed by experts, limiting
their adaptability and accessibility. This paper introduces a novel framework
that automatically learns both the hierarchy and parameters of a SoT from
user-defined objectives. By combining Reinforcement Learning and Genetic
Programming, the system discovers task priorities and control strategies
without manual intervention. A cost function based on intuitive metrics such as
precision, safety, and execution time guides the learning process. We validate
our method through simulations and experiments on the mobile-YuMi platform, a
dual-arm mobile manipulator with high redundancy. Results show that the learned
SoTs enable the robot to dynamically adapt to changing environments and inputs,
balancing competing objectives while maintaining robust task execution. This
approach provides a general and user-friendly solution for redundancy
management in complex robots, advancing human-centered robot programming and
reducing the need for expert design.

</details>


### [200] [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](https://arxiv.org/abs/2508.10798)
*Troi Williams*

Main category: cs.RO

TL;DR: 该论文提出了SET知觉因素框架，用于系统分析影响自主系统感知可靠性的各种因素，提升其安全性和可信度。


<details>
  <summary>Details</summary>
Motivation: 自主系统的应用越来越广泛，但其感知系统的不可靠性，尤其是由于环境复杂因素引发的失败，可能导致事故并降低公众信任。为此，需要有系统化方法来识别和沟通感知风险。

Method: 提出SET（Self, Environment, and Target）知觉因素框架，利用SET State Trees归类感知影响因素的来源，SET Factor Trees建模这些因素如何影响感知任务（如目标检测、姿态估计）。结合两种树形模型建立知觉因素模型，量化特定任务的不确定性。

Result: 框架能够结构化地识别和量化环境、目标与自身等多方面的感知风险，适用于提升自主系统安全验证流程。

Conclusion: 该框架为自主系统感知风险建模和沟通提供了透明、标准化方法，有助于增强系统的安全保证与提升公众信任。

Abstract: Future autonomous systems promise significant societal benefits, yet their
deployment raises concerns about safety and trustworthiness. A key concern is
assuring the reliability of robot perception, as perception seeds safe
decision-making. Failures in perception are often due to complex yet common
environmental factors and can lead to accidents that erode public trust. To
address this concern, we introduce the SET (Self, Environment, and Target)
Perceptual Factors Framework. We designed the framework to systematically
analyze how factors such as weather, occlusion, or sensor limitations
negatively impact perception. To achieve this, the framework employs SET State
Trees to categorize where such factors originate and SET Factor Trees to model
how these sources and factors impact perceptual tasks like object detection or
pose estimation. Next, we develop Perceptual Factor Models using both trees to
quantify the uncertainty for a given task. Our framework aims to promote
rigorous safety assurances and cultivate greater public understanding and trust
in autonomous systems by offering a transparent and standardized method for
identifying, modeling, and communicating perceptual risks.

</details>


### [201] [A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots](https://arxiv.org/abs/2508.10828)
*Henry Powell,Guy Laban,Emily S. Cross*

Main category: cs.RO

TL;DR: 本文提出了一种定制的多模态注意力网络模型，用于识别人与机器人互动中的自我表露，提出了新型损失函数并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然社会和行为学界已大量研究自我表露，但计算系统对此建模有限，特别是针对人与机器人的互动。这一能力对社交机器人日益重要，因此需要有效的计算模型。

Method: 作者基于情感识别模型，设计了一种新的多模态注意力网络，并利用自建的大规模自我表露视频语料库进行训练。同时提出了新的损失函数（scale preserving cross entropy loss），针对自我表露的分类与回归任务进行优化。

Result: 新提出的模型采用创新损失函数后，F1分数达到0.83，比最佳基线模型提升0.48。

Conclusion: 本文方法显著提升了社交机器人识别人类自我表露的能力，为机器人社会认知的实现奠定了基础，对人机社交关系建立有重要意义。

Abstract: Subjective self-disclosure is an important feature of human social
interaction. While much has been done in the social and behavioural literature
to characterise the features and consequences of subjective self-disclosure,
little work has been done thus far to develop computational systems that are
able to accurately model it. Even less work has been done that attempts to
model specifically how human interactants self-disclose with robotic partners.
It is becoming more pressing as we require social robots to work in conjunction
with and establish relationships with humans in various social settings. In
this paper, our aim is to develop a custom multimodal attention network based
on models from the emotion recognition literature, training this model on a
large self-collected self-disclosure video corpus, and constructing a new loss
function, the scale preserving cross entropy loss, that improves upon both
classification and regression versions of this problem. Our results show that
the best performing model, trained with our novel loss function, achieves an F1
score of 0.83, an improvement of 0.48 from the best baseline model. This result
makes significant headway in the aim of allowing social robots to pick up on an
interaction partner's self-disclosures, an ability that will be essential in
social robots with social cognition.

</details>


### [202] [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](https://arxiv.org/abs/2508.10867)
*Yizhi Zhou,Ziwei Kang,Jiawei Xia,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种基于Lie群的、一致性更高的紧耦合视觉-惯性-测距（CVIRO）定位系统，能显著提高定位精度和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统UWB辅助VIO系统在定位时，一致性差会导致估计精度下降，主要是因为忽略了可观性与UWB锚点校准的不确定性。需要一种新的方法提升系统一致性和准确性。

Method: 将UWB锚点状态作为系统状态的一部分建模，显式地计入校准不确定性，利用Lie群的不变误差特性保障系统可观性与一致性。系统可同时一致地估计机器人和UWB锚点状态。

Result: 理论上证明了CVIRO算法能保持系统的不可观子空间并实现一致性，通过大量仿真和实验证明该方法的定位精度和一致性优于现有方法。

Conclusion: CVIRO方法在保持系统一致性的同时，提升了UWB辅助VIO系统的本地化性能，具有理论与应用上的重要意义。

Abstract: Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial
odometry (VIO) systems. Consistency is crucial for ensuring the estimation
accuracy of a UWBaided VIO system. An inconsistent estimator can degrade
localization performance, where the inconsistency primarily arises from two
main factors: (1) the estimator fails to preserve the correct system
observability, and (2) UWB anchor positions are assumed to be known, leading to
improper neglect of calibration uncertainty. In this paper, we propose a
consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system
based on the Lie group. Our method incorporates the UWB anchor state into the
system state, explicitly accounting for UWB calibration uncertainty and
enabling the joint and consistent estimation of both robot and anchor states.
Furthermore, observability consistency is ensured by leveraging the invariant
error properties of the Lie group. We analytically prove that the CVIRO
algorithm naturally maintains the system's correct unobservable subspace,
thereby preserving estimation consistency. Extensive simulations and
experiments demonstrate that CVIRO achieves superior localization accuracy and
consistency compared to existing methods.

</details>


### [203] [TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning](https://arxiv.org/abs/2508.10872)
*Anantha Narayanan,Battu Bhanu Teja,Pruthwik Mishra*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习A2C算法的卫星轨道参数优化方案，可以在低地球轨道拥堵环境下，提高对地观测卫星投放与运行的效率与安全性。该方法在自定义环境中进行轨道动力学仿真，并取得比PPO算法更快收敛与更高性能。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道（LEO）日益拥堵，任务规划需平衡任务目标和规避碰撞风险。现有方法对于自适应高效规划和实时部署存在不足，因此亟需智能、可扩展的解决方案，以提升卫星轨道调整能力和地面覆盖精度。

Method: 构建基于A2C算法的强化学习框架，将卫星轨道优化建模为马尔可夫决策过程，在自定义OpenAI Gymnasium环境下模拟Kepler轨道动力学，智能体学习调整5个轨道参数以实现预定地表覆盖范围，并与PPO算法进行性能对比。

Result: A2C智能体在实现任务目标的同时，收敛速度比PPO快31.5倍，累计奖励高5.8倍，且在多目标场景下表现出较高的稳定性和计算效率。

Conclusion: 基于强化学习的A2C方案不仅能满足LEO卫星实时、高效的轨道调控需求，而且具有良好的可扩展性和智能性，是规模化任务规划的有效选择。

Abstract: The increasing congestion of Low Earth Orbit (LEO) poses persistent
challenges to the efficient deployment and safe operation of Earth observation
satellites. Mission planners must now account not only for mission-specific
requirements but also for the increasing collision risk with active satellites
and space debris. This work presents a reinforcement learning framework using
the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital
parameters for precise terrestrial coverage within predefined surface radii. By
formulating the problem as a Markov Decision Process (MDP) within a custom
OpenAI Gymnasium environment, our method simulates orbital dynamics using
classical Keplerian elements. The agent progressively learns to adjust five of
the orbital parameters - semi-major axis, eccentricity, inclination, right
ascension of ascending node, and the argument of perigee-to achieve targeted
terrestrial coverage. Comparative evaluation against Proximal Policy
Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x
higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer
timesteps (2,000 vs 63,000). The A2C agent consistently meets mission
objectives across diverse target coordinates while maintaining computational
efficiency suitable for real-time mission planning applications. Key
contributions include: (1) a TLE-based orbital simulation environment
incorporating physics constraints, (2) validation of actor-critic methods'
superiority over trust region approaches in continuous orbital control, and (3)
demonstration of rapid convergence enabling adaptive satellite deployment. This
approach establishes reinforcement learning as a computationally efficient
alternative for scalable and intelligent LEO mission planning.

</details>
