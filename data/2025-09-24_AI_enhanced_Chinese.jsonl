{"id": "2509.18282", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faPEEK\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u751f\u6210\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u952e\u4e2d\u95f4\u8868\u793a\uff0c\u5305\u62ec\u4efb\u52a1\u76f8\u5173\u7684\u52a8\u4f5c\u8def\u5f84\u548c\u5173\u6ce8\u533a\u57df\uff0c\u5927\u5927\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u96be\u4ee5\u540c\u65f6\u5b66\u597d\u5173\u6ce8\u3001\u52a8\u4f5c\u9009\u62e9\u548c\u6267\u884c\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f5c\u8005\u8ba4\u4e3a\u53ef\u5c06\u9ad8\u5c42\u7684\u201c\u5173\u6ce8\u54ea\u91cc\u3001\u505a\u4ec0\u4e48\u201d\u4efb\u52a1\u5916\u5305\u7ed9VLM\uff0c\u8ba9\u64cd\u4f5c\u7b56\u7565\u4e13\u6ce8\u4e8e\u201c\u5982\u4f55\u6267\u884c\u201d\u3002", "method": "\u63d0\u51faPEEK\u65b9\u6cd5\uff0c\u5fae\u8c03VLM\u8f93\u51fa\u7edf\u4e00\u7684\u70b9\u8868\u793a\uff0c\u5305\u62ec\u52a8\u4f5c\u8def\u5f84\uff08\u505a\u4ec0\u4e48\uff09\u548c\u611f\u5174\u8da3\u533a\u57df\uff08\u5173\u6ce8\u54ea\u91cc\uff09\uff0c\u5e76\u5c06\u6b64\u4f5c\u4e3a\u4e2d\u95f4\u8868\u5f81\u53e0\u52a0\u5728\u89c2\u6d4b\u4e0a\uff0c\u4f7f\u7b56\u7565\u4e0e\u6a21\u578b\u67b6\u6784\u65e0\u5173\u4e14\u6613\u4e8e\u8fc1\u79fb\u3002\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u81ea\u52a8\u6ce8\u91ca\u6d41\u6c34\u7ebf\uff0c\u4ece20\u591a\u4e2a\u673a\u5668\u4eba\u6570\u636e\u96c6\uff089\u7c7b\u786c\u4ef6\uff09\u751f\u6210\u6807\u6ce8\u3002", "result": "\u5728\u5b9e\u9645\u73af\u5883\u4e2d\uff0cPEEK\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f8b\u5982\u8ba9\u4ec5\u5728\u4eff\u771f\u8bad\u7ec3\u76843D\u7b56\u7565\u5728\u73b0\u5b9e\u4e2d\u63d0\u5347\u4e8641.4\u500d\uff0c\u540c\u65f6\u5bf9\u5927\u578bVLM\u548c\u5c0f\u578b\u64cd\u4f5c\u7b56\u7565\u90fd\u67092-3.5\u500d\u7684\u6536\u76ca\u3002", "conclusion": "\u901a\u8fc7\u8ba9VLM\u5904\u7406\u8bed\u4e49\u548c\u89c6\u89c9\u590d\u6742\u6027\uff0cPEEK\u4e3a\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u5173\u952e\u7ebf\u7d22\uff08\u5173\u6ce8\u54ea\u91cc\u3001\u505a\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u505a\uff09\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u67b6\u6784\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.18311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PRoP\uff0c\u4e00\u4e2a\u80fd\u5b9e\u73b0\u4e2a\u6027\u5316\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u673a\u5668\u4eba\u7b56\u7565\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u5bc6\u94a5\u52a0\u5bc6\u7b56\u7565\u53c2\u6570\uff0c\u4fdd\u969c\u4e2a\u4eba\u504f\u597d\u4e0d\u88ab\u6cc4\u9732\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u6027\u673a\u5668\u4eba\u7b56\u7565\u6613\u4e8e\u8fc1\u79fb\uff0c\u4f46\u7528\u6237\u5728\u4e2a\u6027\u5316\u8fc7\u7a0b\u4e2d\u6613\u6cc4\u9732\u4e2a\u4eba\u504f\u597d\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002\u5982\u4f55\u8ba9\u673a\u5668\u4eba\u5728\u4e2a\u6027\u5316\u524d\u63d0\u4e0b\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faPRoP\u6846\u67b6\uff0c\u6bcf\u4e2a\u7528\u6237\u6301\u6709\u552f\u4e00\u5bc6\u94a5\uff0c\u7528\u8be5\u5bc6\u94a5\u6570\u5b66\u53d8\u6362\u673a\u5668\u4eba\u7b56\u7565\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u53ea\u6709\u6301\u6709\u6b63\u786e\u5bc6\u94a5\u624d\u53ef\u6fc0\u6d3b\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u5426\u5219\u673a\u5668\u4eba\u4ec5\u8868\u73b0\u51fa\u901a\u7528\u7b56\u7565\u3002\u65b9\u6cd5\u4e3a\u6a21\u578b\u65e0\u5173\uff0c\u5e76\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u5206\u7c7b\u4efb\u52a1\u7b49\u591a\u79cd\u6a21\u578b\u4e0a\u901a\u7528\u3002", "result": "PRoP\u5728\u591a\u4e2a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u4e2a\u6027\u5316\u7b56\u7565\u9690\u79c1\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u539f\u6709\u7b56\u7565\u7684\u7f51\u7edc\u7ed3\u6784\u548c\u8868\u73b0\u884c\u4e3a\u3002", "conclusion": "PRoP\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u4e2a\u6027\u5316\u7b56\u7565\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u517c\u987e\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u5e94\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.18327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u5728\u534f\u4f5c\u642c\u8fd0\u7269\u4f53\u65f6\u901a\u8fc7\u7269\u4f53\u8fd0\u52a8\u5b9e\u73b0\u7684\u201c\u89e6\u89c9\u4ea4\u6d41\u201d\u65b9\u5f0f\uff0c\u5e76\u5c06\u4eba-\u4eba\u4e0e\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u53d1\u73b0\u4eba-\u4eba\u914d\u5408\u66f4\u6d41\u7545\u51c6\u786e\uff0c\u9f13\u52b1\u672a\u6765\u63d0\u5347\u673a\u5668\u4eba\u7684\u89e6\u89c9\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e2d\uff0c\u4eba\u4eec\u534f\u4f5c\u642c\u8fd0\u7269\u4f53\u9700\u8981\u6709\u6548\u6c9f\u901a\u5404\u81ea\u7684\u8fd0\u52a8\u8ba1\u5212\u3002\u4f5c\u8005\u5e0c\u671b\u5206\u6790\u8fd9\u79cd\u6c9f\u901a\uff08\u7279\u522b\u662f\u501f\u52a9\u7269\u4f53\u8fd0\u52a8\u7684\u201c\u89e6\u89c9\u4ea4\u6d41\u201d\uff09\u5728\u4eba-\u4eba\u548c\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4ece\u800c\u63a8\u52a8\u673a\u5668\u4eba\u534f\u4f5c\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "\u4f5c\u8005\u62db\u52df\u5b9e\u9a8c\u5bf9\u8c61\uff0c\u4ee5\u4e8c\u4eba\u4e00\u7ec4\u642c\u8fd0\u7269\u4f53\uff0c\u5176\u4e2d\u4e00\u4eba\u638c\u63e1\u8fd0\u52a8\u8ba1\u5212\uff0c\u53e6\u4e00\u4eba\u5e76\u4e0d\u77e5\u60c5\u3002\u968f\u540e\uff0c\u540c\u4e00\u7ec4\u4eba\u5458\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u642c\u8fd0\u540c\u4e00\u7269\u4f53\u3002\u901a\u8fc7\u4f4e\u6210\u672cIMU\u8ffd\u8e2a\u7269\u4f53\u8fd0\u52a8\uff0c\u5e76\u6536\u96c6\u95ee\u5377\u53cd\u9988\uff0c\u4ee5\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\u5bf9\u4e24\u79cd\u534f\u4f5c\u65b9\u5f0f\u8fdb\u884c\u5206\u6790\u548c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4eba-\u4eba\u534f\u4f5c\u4e0b\u88ab\u8bd5\u8005\u53cd\u9988\u534f\u4f5c\u6d41\u7545\u5ea6\u663e\u8457\u66f4\u9ad8\uff0c\u5e76\u4e14IMU\u6570\u636e\u4e5f\u8bc1\u5b9e\u4e86\u4e24\u79cd\u534f\u4f5c\u5728\u7269\u4f53\u8fd0\u52a8\u6a21\u5f0f\u4e0a\u7684\u5ba2\u89c2\u5dee\u5f02\u3002\u4eba-\u4eba\u534f\u4f5c\u7684\u7cbe\u786e\u6027\u548c\u6d41\u7545\u5ea6\u5747\u4f18\u4e8e\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u3002", "conclusion": "\u73b0\u6709\u673a\u5668\u4eba\u5728\u7269\u7406\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u89e6\u89c9\u4ea4\u6d41\u80fd\u529b\u76f8\u6bd4\u4eba\u7c7b\u660e\u663e\u4e0d\u8db3\uff0c\u63d0\u5347\u673a\u5668\u4eba\u53d1\u9001\u548c\u63a5\u6536\u7c7b\u4eba\u89e6\u89c9\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u662f\u672a\u6765\u6539\u8fdb\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u6548\u7387\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2509.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u706b\u661f\u63a2\u6d4b\u56682D\u548c3D\u56fe\u50cf\u53ca\u8f68\u9053\u6570\u636e\u7684\u4e92\u52a8\u4e09\u7ef4\u5730\u5f62\u53ef\u89c6\u5316\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u706b\u661f\u8868\u9762\u5bfc\u822a\u4e0e\u79d1\u7814\u5206\u6790\u4e2d\u591a\u6e90\u5730\u5f62\u6570\u636e\u6574\u5408\u4e0e\u53ef\u89c6\u5316\u96be\u9898\uff0c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u6548\u7387\u3002", "method": "\u81ea\u52a8\u5c06\u706b\u661f2020\u63a2\u6d4b\u8f66\u91c7\u96c6\u76842D\u548c3D\u56fe\u50cf\u3001\u8f68\u9053\u9ad8\u7a0b\u4e0e\u989c\u8272\u5730\u56fe\u6570\u636e\u8fdb\u884c\u878d\u5408\uff0c\u751f\u6210\u4ea4\u4e92\u5f0f\u4e09\u7ef4\u5730\u5f62\u7f51\u683c\uff0c\u5e76\u5728\u5730\u9762\u6570\u636e\u5904\u7406\u7cfb\u7edf\u4e2d\u4e3a\u6bcf\u4e2a\u7740\u9646\u4f4d\u7f6e\u751f\u6210\u5bf9\u5e94\u7684\u5730\u5f62\u6a21\u578b\u3002", "result": "\u6240\u6784\u5efa\u7684\u5730\u5f62\u7f51\u683c\u88ab\u96c6\u6210\u81f3ASTTRO\u5de5\u5177\u7528\u4e8e\u4efb\u52a1\u89c4\u5212\uff0c\u540c\u65f6\u90e8\u5206\u6570\u636e\u5f00\u653e\u81f3\u516c\u4f17\u7f51\u7ad9\u201cExplore with Perseverance\u201d\u3002", "conclusion": "\u8be5\u6280\u672f\u63d0\u5347\u4e86\u79d1\u5b66\u5bb6\u4efb\u52a1\u89c4\u5212\u7684\u6548\u7387\uff0c\u4e5f\u4fc3\u8fdb\u4e86\u516c\u4f17\u5bf9\u706b\u661f\u4efb\u52a1\u7684\u4e86\u89e3\u4e0e\u53c2\u4e0e\u3002"}}
{"id": "2509.18113", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982SPoT\uff09\u5728\u591a\u4efb\u52a1\u6cdb\u5316\u4e0a\u53d7\u9650\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u56fa\u5b9a\u6a21\u677f\uff0c\u96be\u4ee5\u9002\u5e94\u4efb\u52a1\u95f4\u8bed\u4e49\u5dee\u5f02\u548c\u8de8\u9886\u57df\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u52a8\u6001\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u63d0\u793a\u878d\u5408\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u52a8\u6001\u7684\u63d0\u793a\uff08prompt\uff09\u6c60\u548c\u4efb\u52a1\u611f\u77e5\u7684\u8c03\u5ea6\u7b56\u7565\u3002\u5177\u4f53\u5229\u7528\u4efb\u52a1\u5d4c\u5165\u548c\u95e8\u63a7\u673a\u5236\u52a8\u6001\u878d\u5408\u63d0\u793a\uff0c\u5b9e\u73b0\u4e0d\u540c\u4efb\u52a1\u63d0\u793a\u5185\u5bb9\u548c\u9700\u6c42\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u4f18\u5316\u76ee\u6807\u4e2d\u91c7\u7528\u81ea\u52a8\u5b66\u4e60\u7684\u6743\u91cd\u8c03\u5ea6\u7b56\u7565\uff0c\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u548c\u8d1f\u8fc1\u79fb\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u654f\u611f\u6027\u5b9e\u9a8c\uff08\u5982\u63d0\u793a\u6e29\u5ea6\u53c2\u6570\u3001\u4efb\u52a1\u6570\u91cf\u53d8\u5316\uff09\u9a8c\u8bc1\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u8a00\u7406\u89e3\u4e0e\u77e5\u8bc6\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u5728\u7edf\u4e00\u591a\u4efb\u52a1\u5efa\u6a21\u4e0e\u8de8\u9886\u57df\u81ea\u9002\u5e94\u65b9\u9762\u53ef\u6269\u5c55\u4e14\u6709\u6548\uff0c\u80fd\u591f\u514b\u670d\u4ee5\u5f80\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PolypSeg-GradCAM\uff0c\u4e00\u4e2a\u7ed3\u5408U-Net\u4e0eGrad-CAM\u7684\u53ef\u89e3\u91ca\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u80a0\u955c\u56fe\u50cf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u900f\u660e\u7684\u606f\u8089\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u7684\u9ad8\u53d1\u4e0e\u9ad8\u6b7b\u4ea1\u7387\u4ee4\u65e9\u671f\u53d1\u73b0\u606f\u8089\u975e\u5e38\u5173\u952e\uff0c\u800c\u73b0\u6709\u4eba\u5de5\u5206\u5272\u8017\u65f6\u4e14\u6613\u53d7\u4e3b\u89c2\u5f71\u54cd\uff0c\u81ea\u52a8\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u9700\u6c42\u8feb\u5207\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u5c06U-Net\u5206\u5272\u7f51\u7edc\u4e0eGrad-CAM\u70ed\u529b\u56fe\u7ed3\u5408\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e861000\u5f20Kvasir-SEG\u6570\u636e\u96c6\u4e2d\u6ce8\u91ca\u7684\u5185\u955c\u56fe\u50cf\uff0c\u5b9e\u73b0\u5bf9\u606f\u8089\u533a\u57df\u7684\u7cbe\u51c6\u3001\u53ef\u89e3\u91ca\u5206\u5272\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u5e73\u5747IoU\u4e3a0.9257\uff0c\u4ee5\u53ca\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684Dice\u7cfb\u6570\uff08F-score\uff09\u5747\u9ad8\u4e8e0.96\uff0c\u5e76\u901a\u8fc7Grad-CAM\u53ef\u89c6\u5316\u8bc1\u5b9e\u6a21\u578b\u5173\u6ce8\u7684\u533a\u57df\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "conclusion": "PolypSeg-GradCAM\u517c\u5177\u9ad8\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aAI\u8f85\u52a9\u80a0\u955c\u68c0\u67e5\u53ca\u7ed3\u76f4\u80a0\u764c\u65e9\u7b5b\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5176\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2509.18342", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8461\u8404\u56ed\u73af\u5883\u4e2d\u884c\u95f4\u91cd\u590d\u3001\u6613\u611f\u6df7\u6dc6\u95ee\u9898\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u7ed3\u5408\u8bed\u4e49\u68c0\u6d4b\u4e0e\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\uff08\u5982\u8461\u8404\u56ed\uff09\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u884c\u95f4\u91cd\u590d\u3001\u96be\u4ee5\u533a\u5206\u7279\u5f81\uff08\u611f\u77e5\u6df7\u6dc6\uff09\u95ee\u9898\uff0c\u7ecf\u5e38\u5bfc\u81f4\u5b9a\u4f4d\u5931\u8d25\u3002\u89c6\u89c9SLAM\u7b49\u65b9\u5f0f\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u4e5f\u4e0d\u591f\u53ef\u9760\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u9ad8\u9c81\u68d2\u6027\u5b9a\u4f4d\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u8be5\u65b9\u6cd5\u5c06\u85e4\u8513\u6811\u5e72\u548c\u652f\u6491\u6746\u7b49\u7a33\u5b9a\u8bed\u4e49\u5730\u6807\u7684\u76ee\u6807\u68c0\u6d4b\u7eb3\u5165\u5b9a\u4f4d\u7684\u4f3c\u7136\u8bc4\u4f30\u3002\u901a\u8fc7\u5c06\u68c0\u6d4b\u5230\u7684\u5730\u6807\u6295\u5f71\u5230\u9e1f\u77b0\u56fe\uff0c\u5e76\u4e0e\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u878d\u5408\uff0c\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u89c2\u6d4b\u3002\u521b\u65b0\u70b9\u5728\u4e8e\u63d0\u51fa\u201c\u8bed\u4e49\u5899\u201d\uff0c\u4ee5\u8fde\u63a5\u76f8\u90bb\u5730\u6807\u5f62\u6210\u4f2a\u7ed3\u6784\u7ea6\u675f\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u884c\u6df7\u6dc6\u95ee\u9898\u3002\u5bf9\u4e8e\u7aef\u5934\u5730\u533a\u8bed\u4e49\u7a00\u758f\u7684\u60c5\u51b5\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u566a\u58f0GPS\u5148\u9a8c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u8461\u8404\u56ed\u73af\u5883\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7ef4\u6301\u5728\u6b63\u786e\u7684\u8461\u8404\u884c\u5185\u7cbe\u786e\u5b9a\u4f4d\uff0c\u5728\u4f20\u7edfAMCL\u7b97\u6cd5\u5931\u6548\u65f6\u4f9d\u7136\u53ef\u6062\u590d\u504f\u79bb\uff0c\u5e76\u4f18\u4e8e\u5f53\u524d\u6d41\u884c\u7684\u89c6\u89c9SLAM\uff08\u5982RTAB-Map\uff09\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u76ee\u6807\u68c0\u6d4b\u548c\u7ed3\u6784\u6027\u4f2a\u7ea6\u675f\uff0c\u8bba\u6587\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u4e14\u9ad8\u5ea6\u91cd\u590d\u7ed3\u6784\u6237\u5916\uff08\u5982\u8461\u8404\u56ed\uff09\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u662f\u5bf9\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u4e0e\u89c6\u89c9SLAM\u4e0d\u8db3\u7684\u6709\u6548\u8865\u5145\u3002"}}
{"id": "2509.18122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "GAUSS\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u5341\u4e8c\u9879\u6838\u5fc3\u6280\u80fd\uff0c\u901a\u8fc7\u7ec6\u81f4\u7684\u5206\u7ef4\u5ea6\u6d4b\u8bd5\u63ed\u793a\u6a21\u578b\u4f18\u52a3\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b66\u80fd\u529b\u8bc4\u6d4b\u591a\u4e3a\u5355\u4e00\u7ef4\u5ea6\u6216\u6574\u4f53\u5f97\u5206\uff0c\u96be\u4ee5\u5c55\u73b0\u6a21\u578b\u4e0d\u540c\u6570\u5b66\u80fd\u529b\u5206\u5e03\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u548c\u89e3\u91ca\u6027\u3002", "method": "GAUSS\u57fa\u51c6\u6309\u77e5\u8bc6\u3001\u7406\u89e3\u3001\u95ee\u9898\u89e3\u51b3\u3001\u4ea4\u6d41\u3001\u5143\u6280\u80fd\u548c\u521b\u9020\u529b\u4e09\u5927\u7c7b\u5341\u4e8c\u9879\u80fd\u529b\u8bbe\u8ba1\u7ec6\u5206\u4efb\u52a1\uff0c\u5e76\u5c06\u95ee\u9898\u8fdb\u884c\u8ba4\u77e5\u6280\u80fd\u5206\u7c7b\uff0c\u4ece\u800c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u80fd\u529b\u753b\u50cf\u3002", "result": "\u4ee5GPT-5-thinking\u4e3a\u4f8b\uff0c\u751f\u6210\u5176\u6570\u5b66\u80fd\u529b\u591a\u7ef4\u6280\u80fd\u7ed3\u6784\u753b\u50cf\uff0c\u5e76\u4e0eo4-mini-high\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u660e\u786e\u5c55\u73b0\u5404\u81ea\u4f18\u52bf\u4e0e\u77ed\u677f\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u3001\u6280\u80fd\u7ec6\u5206\u7684\u6570\u5b66\u8bc4\u6d4b\u80fd\u66f4\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u6570\u5b66\u667a\u80fd\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u548c\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2509.18160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edfPerceptronCARE\uff0c\u53ef\u5728\u4e34\u5e8a\u53ca\u8fdc\u7a0b\u533b\u7597\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7b5b\u67e5\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u8303\u56f4\u5185\u6210\u5e74\u4eba\u81f4\u76f2\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u8d44\u6e90\u4e0d\u8db3\u7684\u5730\u533a\u3002\u4f20\u7edf\u7b5b\u67e5\u65b9\u5f0f\u96be\u4ee5\u6ee1\u8db3\u5927\u8303\u56f4\u3001\u65e9\u671f\u8bca\u65ad\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528ResNet-18\u3001EfficientNet-B0\u548cSqueezeNet\u7b49\u591a\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9488\u5bf9\u89c6\u7f51\u819c\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u4f18\u5316\uff0c\u6700\u7ec8\u9009\u62e9\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u6700\u4f18\u5e73\u8861\u7684\u6a21\u578b\u3002\u7cfb\u7edf\u8fd8\u96c6\u6210\u4e86\u4e91\u7aef\u6269\u5c55\u3001\u6570\u636e\u5b89\u5168\u7ba1\u7406\u548c\u591a\u7528\u6237\u652f\u6301\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5bf9\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523085.4%\uff0c\u53ef\u5b9e\u73b0\u5b9e\u65f6\u7b5b\u67e5\uff0c\u5e76\u9002\u7528\u4e8e\u4e34\u5e8a\u548c\u8fdc\u7a0b\u533b\u7597\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u8fdc\u7a0b\u533b\u7597\u5e94\u7528\u5982PerceptronCARE\uff0c\u6709\u52a9\u4e8e\u6269\u5927\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7b5b\u67e5\u7684\u8986\u76d6\u8303\u56f4\uff0c\u63d0\u5347\u65e9\u671f\u8bca\u65ad\u7387\uff0c\u6539\u5584\u533b\u60a3\u4e92\u52a8\uff0c\u5e76\u964d\u4f4e\u533b\u7597\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u504f\u8fdc\u53ca\u8d44\u6e90\u6709\u9650\u5730\u533a\u3002"}}
{"id": "2509.18384", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u3001\u5229\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u5b9e\u73b0\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u7684\u65b0\u65b9\u6cd5LAD-VF\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u5408\u89c4\u7684LLM\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u3002", "motivation": "\u76ee\u524dLLM\u5728\u5b9e\u9645\u7269\u7406\u4e16\u754c\u90e8\u7f72\uff0c\u6bd4\u5982\u673a\u5668\u4eba\u6216\u81ea\u52a8\u9a7e\u9a76\uff0c\u9700\u8981\u4e25\u683c\u9075\u5b88\u5b89\u5168\u548c\u6cd5\u89c4\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u6613\u51fa\u73b0\u5e7b\u89c9\u6216\u5bf9\u89c4\u8303\u5bf9\u9f50\u6027\u4e0d\u5f3a\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u7c7b\u6807\u6ce8\u6216\u5fae\u8c03\uff0c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u3002\u4e3a\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u3001\u7b26\u5408\u89c4\u8303\u7684LLM\u5e94\u7528\u65b9\u6848\u3002", "method": "\u63d0\u51faLAD-VF\u6846\u67b6\uff0c\u878d\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u4e0eLLM-AutoDiff\u673a\u5236\uff0c\u5c06\u9a8c\u8bc1\u53cd\u9988\u8f6c\u5316\u4e3a\u53ef\u5fae\u635f\u5931\uff0c\u76f4\u63a5\u7528\u4e8e\u81ea\u52a8\u6539\u8fdb\u63d0\u793a\uff08prompt\uff09\uff0c\u800c\u65e0\u9700\u5bf9\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u5408\u89c4\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e0e\u64cd\u4f5c\u5b9e\u9a8c\u4e2d\uff0cLAD-VF\u663e\u8457\u63d0\u5347\u4e86\u89c4\u8303\u9075\u5faa\u7387\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4ece60%\u63d0\u5347\u523090%\u4ee5\u4e0a\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "LAD-VF\u4e3a\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u5df2\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684LLM\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b0\u8def\u5f84\uff0c\u5177\u5907\u4e0d\u9700\u8981\u5fae\u8c03\u3001\u9ad8\u5ea6\u517c\u5bb9\u548c\u53ef\u5ba1\u8ba1\u6027\u7b49\u4f18\u52bf\u3002"}}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5408\u6210\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u4e2d\u901a\u8fc7\u751f\u6210\u201c\u5b6a\u751f\u4f53\u201d\u6765\u5b9e\u73b0\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\uff0c\u5176\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u96c6COPES-hard\u4e0a\u4f18\u4e8e\u5305\u62ecGPT-4\u5728\u5185\u7684\u5df2\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8bed\u8a00\u6a21\u5f0f\u548c\u591a\u8df3\u5173\u7cfb\u63a8\u7406\u7684\u4e8b\u4ef6\u56e0\u679c\u8bc6\u522b\u65b9\u6cd5\uff0c\u5bb9\u6613\u56e0\u975e\u6b63\u5f0f\u56e0\u679c\u8868\u8fbe\u548c\u56fe\u63a8\u7406\u9519\u8bef\u5bfc\u81f4\u8bef\u5224\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u56e0\u679c\u4e0e\u76f8\u5173\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u5c1d\u8bd5\u5f15\u5165\u66f4\u52a0\u4e25\u8c28\u7684\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u6765\u63d0\u5347\u56e0\u679c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u91c7\u7528Rubin\u56e0\u679c\u6a21\u578b\uff0c\u5c06\u4e8b\u4ef6\u5bf9\u4e2d\u7684\u9996\u4e8b\u4ef6\u89c6\u4e3a\u201c\u5904\u7406\u201d\uff0c\u540e\u4e8b\u4ef6\u4e3a\u201c\u7ed3\u679c\u201d\uff0c\u901a\u8fc7\u5bf9\u5904\u7406\u7684\u64cd\u63a7\u53ca\u7ed3\u679c\u6982\u7387\u4f30\u8ba1\u5224\u65ad\u56e0\u679c\u5173\u7cfb\u3002\u8003\u8651\u5230\u6587\u672c\u4e0d\u53ef\u76f4\u63a5\u64cd\u63a7\uff0c\u4f5c\u8005\u901a\u8fc7\u5728\u8bed\u6599\u5e93\u4e2d\u5bfb\u627e\u201c\u5b6a\u751f\u4f53\u201d\uff0c\u518d\u4ee5\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u4e0e\u9006\u5411\u6280\u672f\uff0c\u751f\u6210\u6ee1\u8db3\u6761\u4ef6\u7684\u5b6a\u751f\u4f53\uff0c\u4ece\u800c\u95f4\u63a5\u5b9e\u73b0\u5e72\u9884\u64cd\u4f5c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u5728\u516c\u5f00\u7684\u56e0\u679c\u6027\u57fa\u51c6\u96c6COPES-hard\u4e0a\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548cGPT-4\u66f4\u5f3a\u7684\u4e8b\u4ef6\u56e0\u679c\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u5229\u7528\u5408\u6210\u63a7\u5236\u4e0e\u6587\u672c\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u66f4\u7cbe\u786e\u3001\u7a33\u5065\u5730\u8bc6\u522b\u4e8b\u4ef6\u95f4\u56e0\u679c\u5173\u7cfb\uff0c\u4e3a\u4e8b\u4ef6\u56e0\u679c\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u3001\u66f4\u5177\u7406\u8bba\u4f9d\u636e\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2509.18165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6b63\u5219\u5316\u65b9\u6cd5Self Identity Mapping (SIM)\uff0c\u901a\u8fc7\u9006\u5411\u6620\u5c04\u673a\u5236\u6539\u5584\u8868\u5f81\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u3001\u591a\u9886\u57df\u4e0b\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u5176\u6539\u8fdb\u7248\u672c\u03c1SIM\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u4e0e\u73b0\u6709\u6b63\u5219\u5316\u6280\u672f\u53e0\u52a0\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\uff0c\u6548\u679c\u968f\u5e94\u7528\u73af\u5883\u53d8\u5316\u4e14\u4e0d\u7a33\u5b9a\u3002\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u66f4\u901a\u7528\u3001\u6709\u6548\u4e14\u4f4e\u6210\u672c\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u63d0\u5347\u6df1\u5ea6\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSIM\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f93\u51fa\u91cd\u65b0\u6620\u5c04\u56de\u8f93\u5165\u5b9e\u73b0\u81ea\u6211\u8eab\u4efd\u6620\u5c04\u4ee5\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u5e76\u652f\u6301\u5e73\u6ed1\u68af\u5ea6\u6d41\u3002\u4e3a\u63d0\u5347\u6548\u7387\uff0cSIM\u7684\u5177\u4f53\u5b9e\u73b0\u03c1SIM\u91c7\u7528\u4e86patch\u7ea7\u7279\u5f81\u91c7\u6837\u548c\u6295\u5f71\u91cd\u5efa\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002SIM\u4e3a\u4e0e\u6a21\u578b\u548c\u4efb\u52a1\u65e0\u5173\u7684\u63d2\u4ef6\u5f0f\u6a21\u5757\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5c11\u6837\u672c\u63d0\u793a\u5b66\u4e60\u3001\u9886\u57df\u6cdb\u5316\u7b49\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u03c1SIM\u8f83\u57fa\u7ebf\u6709\u660e\u663e\u63d0\u5347\u3002\u5b9e\u9a8c\u8fd8\u8868\u660e\u03c1SIM\u53ef\u4e0e\u73b0\u5b58\u6b63\u5219\u5316\u6280\u672f\u53e0\u52a0\u589e\u6548\uff0c\u5bf9\u8bed\u4e49\u5206\u5272\u3001\u56fe\u50cf\u7ffb\u8bd1\uff08\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff09\u53ca\u97f3\u9891\u5206\u7c7b\u3001\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\uff08\u975e\u89c6\u89c9\u4efb\u52a1\uff09\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u03c1SIM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u5e7f\u6cdb\u5e94\u7528\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u591a\u6837\u4efb\u52a1\u4e2d\u7684\u8868\u5f81\u80fd\u529b\u548c\u4e00\u822c\u5316\u6027\u80fd\uff0c\u5e76\u4e0e\u5176\u4ed6\u6b63\u5219\u5316\u7b56\u7565\u517c\u5bb9\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.18407", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u76d1\u7ba1\u8def\u53e3\uff08\u5982\u672a\u8bbe\u4ea4\u901a\u4fe1\u53f7\u706f\u6216\u6807\u5fd7\u7684\u8def\u53e3\uff09\u53f3\u4fa7\u4f18\u5148\u5224\u65ad\u7684\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u6982\u7387\u89c4\u5212\u7b97\u6cd5\u63d0\u5347\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e0\u4eba\u76d1\u7ba1\u8def\u53e3\u7531\u4e8e\u4f18\u5148\u901a\u884c\u89c4\u5219\u4e0d\u660e\u786e\u3001\u89c6\u7ebf\u906e\u6321\u4ee5\u53ca\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\uff0c\u662f\u4ea4\u901a\u4e8b\u6545\u9ad8\u53d1\u5730\u5e26\u3002\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u5927\u591a\u4fa7\u91cd\u4e8e\u5168\u81ea\u52a8\u8f66\u8f86\uff0c\u5bf9\u4e8e\u6539\u9020\u73b0\u5b9e\u4e2d\u7684\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4ee5\u63d0\u4f9b\u8f85\u52a9\u5bfc\u822a\u652f\u6301\u7684\u7cfb\u7edf\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u4f5c\u8005\u5c06\u65e0\u4eba\u76d1\u7ba1\u8def\u53e3\u7684\u9a7e\u9a76\u8f85\u52a9\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u5e76\u5728\u5e26\u6709\u968f\u673a\u4ea4\u901a\u53c2\u4e0e\u8005\u3001\u884c\u4eba\u3001\u89c6\u7ebf\u906e\u6321\u4e0e\u5bf9\u6297\u6027\u60c5\u666f\u7684\u5b9a\u5236\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u51b3\u7b56\u65b9\u6cd5\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u4e0e\u4e09\u79cd\u6982\u7387\u578b\u89c4\u5212\u5668\uff08QMDP\u3001POMCP\u3001DESPOT\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6982\u7387\u578b\u89c4\u5212\u5668\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u65e0\u78b0\u649e\u901a\u884c\u7387\u6700\u9ad8\u53ef\u8fbe97.5%\uff0c\u8fdc\u8d85\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002\u5176\u4e2d\uff0cPOMCP\u66f4\u52a0\u4fa7\u91cd\u5b89\u5168\u6027\uff0cDESPOT\u5219\u517c\u987e\u4e86\u901a\u884c\u6548\u7387\u548c\u5b9e\u65f6\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89c4\u5212\u5728\u63d0\u5347\u9a7e\u9a76\u8f85\u52a9\u5b89\u5168\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u878d\u5408\u591a\u4f20\u611f\u5668\u4e0e\u73af\u5883\u611f\u77e5\u6a21\u5757\uff0c\u5b9e\u73b0\u7cfb\u7edf\u5728\u5b9e\u9645\u4ea4\u901a\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u65b0\u65b9\u6cd5ZERA\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u51c6\u548c\u6743\u91cd\u5feb\u901f\u3001\u4f4e\u4ee3\u4ef7\u5730\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\uff0c\u5927\u5e45\u63d0\u5347\u5927\u6a21\u578b\u5728\u591a\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709APO\u65b9\u6cd5\u7f3a\u70b9\u660e\u663e\uff1a\u53ea\u5173\u6ce8\u7528\u6237\u63d0\u793a\u3001\u4f9d\u8d56\u65e0\u7ed3\u6784\u53cd\u9988\u3001\u6837\u672c\u548c\u8fed\u4ee3\u9700\u6c42\u5927\u3001\u6210\u672c\u9ad8\u4e14\u6613\u5931\u6548\u3002\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "ZERA\u65b9\u6cd5\u901a\u8fc7\u516b\u9879\u53ef\u6cdb\u5316\u7684\u6807\u51c6\uff0c\u5e76\u81ea\u52a8\u63a8\u65ad\u6807\u51c6\u6743\u91cd\uff0c\u7ed3\u5408\u5bf9\u7cfb\u7edf\u4e0e\u7528\u6237\u63d0\u793a\u7684\u8054\u5408\u7ed3\u6784\u5316\u4f18\u5316\uff0c\u5b9e\u73b0\u63d0\u793a\u5feb\u901f\u9ad8\u8d28\u91cf\u6536\u655b\uff0c\u6240\u9700\u793a\u4f8b\u91cf\u548c\u8fed\u4ee3\u5468\u671f\u5927\u5e45\u7f29\u77ed\u3002", "result": "ZERA\u5728\u4e94\u4e2a\u4e3b\u6d41LLM\u548c\u4e5d\u4e2a\u9886\u57df\u6570\u636e\u96c6\uff08\u63a8\u7406\u3001\u6458\u8981\u3001\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u7684\u5b9e\u9a8c\uff0c\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u6d88\u878d\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ZERA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u6548\u7387\u548c\u8d28\u91cf\uff0c\u9002\u7528\u6027\u5e7f\u6cdb\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u843d\u5730\u63d0\u4f9b\u4e86\u6613\u7528\u6027\u548c\u6027\u80fd\u65b0\u65b9\u6848\u3002"}}
{"id": "2509.18170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MAGIA\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u91cf\u81ea\u9002\u5e94\u6821\u6b63\u5b9e\u73b0\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\uff0c\u5728\u5355\u8f6e\u5e73\u5747\u68af\u5ea6\uff08SAG\uff09\u73af\u5883\u4e0b\u80fd\u9ad8\u6548\u91cd\u5efa\u591a\u5f20\u56fe\u7247\uff0c\u4e14\u65e0\u9700\u6807\u7b7e\u4fe1\u606f\u3002", "motivation": "\u5728SAG\u73af\u5883\u4e2d\uff0c\u68af\u5ea6\u4fe1\u606f\u9ad8\u5ea6\u6df7\u5408\uff0c\u5bfc\u81f4\u6bcf\u4e2a\u6837\u672c\u7684\u7279\u5f81\u4fe1\u53f7\u96be\u4ee5\u533a\u5206\uff0c\u73b0\u6709\u7684\u68af\u5ea6\u53cd\u6f14\u65b9\u6cd5\u5728\u5927\u6279\u91cf\u3001\u591a\u56fe\u7247\u91cd\u5efa\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMAGIA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u521b\u65b0\uff1a1\uff09\u5c01\u95ed\u5f0f\u7ec4\u5408\u91cd\u7f29\u653e\uff0c\u6536\u7d27\u4f18\u5316\u8fb9\u754c\uff1b2\uff09\u901a\u8fc7\u52a8\u91cf\u7b97\u6cd5\u6df7\u5408\u6574\u6279\u548c\u5b50\u96c6\u635f\u5931\uff0c\u63d0\u5347\u91cd\u5efa\u5065\u58ee\u6027\u3002\u540c\u65f6\u901a\u8fc7\u968f\u673a\u91c7\u6837\u5b50\u96c6\u63a2\u6d4b\u5355\u6837\u672c\u4fe1\u53f7\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cMAGIA\u5728\u5927\u6279\u91cf\u3001\u9ad8\u4fdd\u771f\u591a\u56fe\u7247\u91cd\u5efa\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u8fd0\u7b97\u6210\u672c\u4e0e\u5e38\u89c4\u65b9\u6cd5\u76f8\u5f53\uff0c\u65e0\u9700\u989d\u5916\u8f85\u52a9\u4fe1\u606f\u3002", "conclusion": "MAGIA\u65b9\u6cd5\u653b\u514b\u4e86SAG\u6761\u4ef6\u4e0b\u68af\u5ea6\u53cd\u6f14\u4e2d\u7684\u4e3b\u8981\u96be\u70b9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5065\u58ee\u4e14\u65e0\u6807\u7b7e\u63a8\u65ad\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u4e3a\u63d0\u5347\u9690\u79c1\u653b\u51fb\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.18428", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6LAWM\uff0c\u53ef\u901a\u8fc7\u89c6\u9891\u4e2d\u7684\u4e16\u754c\u5efa\u6a21\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u4ee5\u63d0\u5347\u7c7b\u4eba\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u52a8\u4f5c\u6570\u636e\uff0c\u4e14\u6a21\u578b\u5c0f\u5de7\uff0c\u6613\u4e8e\u5b9e\u9645\u90e8\u7f72\uff0c\u5e76\u5728LIBERO\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u867d\u7136\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u52a8\u4f5c\u6570\u636e\uff0c\u4e14\u6a21\u578b\u8f83\u5927\uff0c\u4e0d\u4fbf\u4e8e\u5b9e\u5730\u90e8\u7f72\u3002\u4e3a\u6b64\uff0c\u5b66\u754c\u5f00\u59cb\u63a2\u7d22\u65e0\u76d1\u7763\u5229\u7528\u89c6\u9891\u6570\u636e\u9884\u8bad\u7ec3\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u9650\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86LAWM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u65e0\u9700\u52a8\u4f5c\u6807\u6ce8\uff0c\u4ec5\u5229\u7528\u673a\u5668\u4eba\u6216\u4eba\u7c7b\u4e0e\u65e5\u5e38\u7269\u4f53\u4ea4\u4e92\u7684\u89c6\u9891\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\u6355\u6349\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u62bd\u8c61\u89c6\u89c9\u53d8\u5316\uff0c\u4ece\u800c\u5bf9\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u9884\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u5177\u5907\u6a21\u578b\u65e0\u5173\u6027\uff0c\u53ef\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u3001\u73af\u5883\u548c\u673a\u5668\u4eba\u5b9e\u73b0\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cLAWM\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u4f7f\u7528\u771f\u5b9e\u52a8\u4f5c\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8fd8\u4f18\u4e8e\u5f53\u524d\u7684\u76f8\u4f3c\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "LAWM\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u90e8\u7f72\u6210\u672c\u548c\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u5b9e\u73b0\u5728\u4e0d\u540c\u573a\u666f\u548c\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u6280\u80fd\u8fc1\u79fb\uff0c\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u9884\u8bad\u7ec3\u9886\u57df\u9ad8\u6548\u5b9e\u7528\u7684\u524d\u6cbf\u65b9\u6cd5\u3002"}}
{"id": "2509.18163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5916\u90e8\u4fe1\u606f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u65b0\u6570\u636e\u96c6SciAux\u6765\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u5bf9\u6709\u7528\u3001\u65e0\u5173\u6216\u8bef\u5bfc\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u53d1\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u9762\u5bf9\u8bef\u5bfc\u4fe1\u606f\u65f6\u8868\u73b0\u4e25\u91cd\u4e0b\u964d\uff0c\u2018\u601d\u8003\u2019\u53cd\u800c\u52a0\u5267\u9519\u8bef\u3002", "motivation": "\u5f53\u524dLLM\u5e38\u88ab\u7528\u4e8e\u9700\u8981\u63a8\u7406\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e14\u5e38\u5e38\u9700\u8981\u7ed3\u5408\u5916\u90e8\u4fe1\u606f\uff1b\u4f46\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u76ee\u524d\u5c1a\u7f3a\u4e4f\u5bf9\u5176\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u4eceScienceQA\u884d\u751f\u65b0\u7684SciAux\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u6a21\u578b\u2018\u663e\u5f0f\u9010\u6b65\u601d\u8003\u2019\u63a8\u7406\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5206\u6790\u6709\u7528\u3001\u65e0\u5173\u3001\u8bef\u5bfc\u7b49\u4e0d\u540c\u5916\u90e8\u4fe1\u606f\u5bf9LLM\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u8f85\u52a9\u6027\u6709\u7528\u4fe1\u606f\u80fd\u63d0\u5347LLM\u51c6\u786e\u7387\u3002\u4f46\u5f53\u5916\u90e8\u4fe1\u606f\u8bef\u5bfc\u65f6\uff0c\u6a21\u578b\u63a8\u7406\u6027\u80fd\u4f1a\u707e\u96be\u6027\u4e0b\u6ed1\uff0c\u2018\u9010\u6b65\u601d\u8003\u2019\u7b56\u7565\u4f1a\u8fdb\u4e00\u6b65\u653e\u5927\u9519\u8bef\u5f71\u54cd\u3002", "conclusion": "\u7b80\u5355\u63d0\u5347\u6a21\u578b\u2018\u601d\u8003\u529b\u2019\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u9c81\u68d2\u63a8\u7406\uff0c\u8fd8\u9700\u8ba9\u6a21\u578b\u5177\u5907\u6279\u5224\u6027\u8bc4\u4f30\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18174", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u963f\u62c9\u4f2f\u6587\u6863OCR\u4efb\u52a1\u8bbe\u8ba1\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578bBaseer\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u9ad8\u8d28\u91cf\u8bc4\u6d4b\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBaseer\u5728\u963f\u62c9\u4f2f\u6587\u6863OCR\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u53ca\u5546\u4e1a\u7cfb\u7edf\uff0c\u521b\u4e0b\u4e86\u6700\u65b0\u7684\u6027\u80fd\u7eaa\u5f55\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u6587\u6863\u7531\u4e8e\u5176\u8fde\u5199\u4f53\u3001\u591a\u53d8\u5b57\u4f53\u3001\u9644\u52a0\u7b26\u53f7\u4ee5\u53ca\u4ece\u53f3\u5230\u5de6\u7684\u4e66\u5199\u65b9\u5f0f\uff0c\u4f7f\u5f97OCR\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u76ee\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u6587\u6863\u7406\u89e3\u4e0a\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u5728\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u6709\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Baseer\u6a21\u578b\u2014\u2014\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u6587\u6863OCR\u7ec6\u81f4\u8c03\u4f18\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u6a21\u578b\u91c7\u7528decoder-only\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u6210\u4e0e\u771f\u5b9e\u5927\u89c4\u6a21\u6587\u6863\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4fdd\u6301\u901a\u7528\u89c6\u89c9\u7279\u5f81\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u5bf9\u963f\u62c9\u4f2f\u6587\u6863\u7684\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86Misraj-DocOCR\u57fa\u51c6\uff0c\u4f5c\u4e3a\u963f\u62c9\u4f2f\u6587OCR\u7cfb\u7edf\u8bc4\u6d4b\u6807\u51c6\u3002", "result": "Baseer\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u6587\u6863OCR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u8fbe\u52300.25\uff0c\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u4e0e\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u5237\u65b0\u4e86\u8be5\u9886\u57df\u7684\u6027\u80fd\u7eaa\u5f55\u3002", "conclusion": "\u4e13\u7528\u9886\u57df\u9002\u914d\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u63d0\u5347\u963f\u62c9\u4f2f\u7b49\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00OCR\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0cBaseer\u4e3a\u8be5\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u529b\u57fa\u7ebf\uff0c\u5e76\u63a8\u52a8\u4e86\u963f\u62c9\u4f2f\u6587\u6863\u667a\u80fd\u8bc6\u522b\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18447", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPrioriTouch\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u6839\u636e\u4e2a\u4f53\u63a5\u89e6\u504f\u597d\u5728\u591a\u70b9\u63a5\u89e6\u573a\u666f\u4e0b\u4f18\u5148\u8c03\u5ea6\u63a7\u5236\u4efb\u52a1\uff0c\u7ed3\u5408\u5b66\u4e60\u6392\u5e8f\u548c\u5206\u5c42\u63a7\u5236\uff0c\u63d0\u5347\u62a4\u7406\u7b49\u4efb\u52a1\u4e2d\u7684\u8212\u9002\u4e0e\u5b89\u5168\u3002", "motivation": "\u591a\u70b9\u4eba\u4e0e\u673a\u5668\u4eba\u7269\u7406\u63a5\u89e6\u65f6\uff0c\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u5bf9\u529b\u91cf\u548c\u4f4d\u7f6e\u6709\u4e0d\u540c\u9700\u6c42\uff0c\u8fd9\u4e9b\u9700\u6c42\u5f80\u5f80\u76f8\u4e92\u51b2\u7a81\uff0c\u5355\u4e00\u63a7\u5236\u7b56\u7565\u96be\u4ee5\u6ee1\u8db3\u5168\u90e8\u76ee\u6807\uff0c\u9700\u5b9e\u73b0\u504f\u597d\u4f18\u5148\u7ea7\u6392\u5e8f\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3001\u8212\u9002\u4e0e\u5b89\u5168\u3002", "method": "PrioriTouch\u6846\u67b6\u96c6\u5408\u901a\u7528\u63a7\u5236\u5668\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\uff08learning-to-rank\uff09\u4e0e\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff08hierarchical operational space control\uff09\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u56de\u8def\u7684\u6570\u636e\u9ad8\u6548\u5b89\u5168\u63a2\u7d22\uff0c\u6700\u7ec8\u5c06\u7528\u6237\u63a5\u89e6\u504f\u597d\uff08\u5982\u8212\u9002\u9608\u503c\uff09\u878d\u5165\u6574\u4f53\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u83b7\u5f97\u4e0d\u540c\u7528\u6237\u7684\u63a5\u89e6\u504f\u597d\u5e76\u636e\u6b64\u4e2a\u6027\u5316\u53c2\u6570\uff0c\u7cfb\u7edf\u7ecf\u4eff\u771f\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793aPrioriTouch\u80fd\u6839\u636e\u7528\u6237\u504f\u597d\u5728\u4efb\u52a1\u6267\u884c\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u7b49\u65b9\u9762\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u5e76\u5747\u8861\u8868\u73b0\u3002", "conclusion": "PrioriTouch\u6846\u67b6\u53ef\u7075\u6d3b\u6574\u5408\u591a\u79cd\u63a7\u5236\u5668\uff0c\u652f\u6301\u591a\u63a5\u89e6\u4efb\u52a1\u4e2d\u5bf9\u7528\u6237\u504f\u597d\u7684\u9002\u5e94\u4e0e\u4f18\u5148\u8c03\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u4e0e\u4efb\u52a1\u5b8c\u6210\u5ea6\uff0c\u6709\u671b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u62a4\u7406\u4e0e\u5176\u4ed6\u591a\u70b9\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2509.18167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u4efb\u52a1\u7684\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u52a0\u51b3\u7b56\u8005\u548c\u77e5\u8bc6\u7b5b\u9009\u8005\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\uff0c\u66f4\u597d\u5730\u534f\u8c03\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7684\u4e92\u52a8\uff0c\u4ece\u800c\u63d0\u5347\u95ee\u7b54\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u4e0e\u89e3\u91ca\u6027\u3002", "motivation": "RAG\u4f9d\u8d56\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7684\u534f\u540c\uff0c\u4f46\u73b0\u6709\u4e24\u8005\u72ec\u7acb\u5f00\u53d1\uff0c\u5bfc\u81f4\u68c0\u7d22\u5185\u5bb9\u53ef\u80fd\u65e0\u5173\u6216\u5197\u4f59\uff0c\u751f\u6210\u5668\u53c8\u65e0\u6cd5\u6709\u6548\u5229\u7528\u68c0\u7d22\u8bc1\u636e\uff0c\u9020\u6210\u6027\u80fd\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u51b3\u7b56\u8005\u548c\u77e5\u8bc6\u7b5b\u9009\u8005\u4e24\u4e2a\u667a\u80fd\u4f53\u7684\u6846\u67b6\u3002\u51b3\u7b56\u8005\u8d1f\u8d23\u63a7\u5236\u662f\u7ee7\u7eed\u68c0\u7d22\u8fd8\u662f\u8fdb\u5165\u7b54\u6848\u751f\u6210\u9636\u6bb5\uff0c\u77e5\u8bc6\u7b5b\u9009\u8005\u8fc7\u6ee4\u51fa\u6700\u6709\u7528\u7684\u68c0\u7d22\u6587\u6863\u3002\u7528LLM\u62c5\u4efb\u4ef2\u88c1\u8005\uff0c\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u7ed9\u4e88\u8fc7\u7a0b\u7ea7\u5956\u52b1\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u76d1\u7763\u548c\u66f4\u51c6\u786e\u7684\u5956\u52b1\u5206\u914d\uff1b\u91c7\u7528\u6811\u7ed3\u6784rollout\u53d1\u6398\u591a\u6837\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u7528PPO\u7b97\u6cd5\u7aef\u5230\u7aef\u8bad\u7ec3\u667a\u80fd\u4f53\u3002\u8be5\u6846\u67b6\u4e3a\u63d2\u62d4\u5f0f\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6709\u68c0\u7d22\u5668\u6216\u751f\u6210\u5668\u3002", "result": "\u5728\u5355\u8df3\u4e0e\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3001\u6536\u655b\u66f4\u7a33\u5b9a\uff0c\u5e76\u4e14\u63a8\u7406\u8fc7\u7a0b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u660e\u663e\u4f18\u4e8e\u6807\u51c6RAG\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u6d41\u7a0b\u76d1\u7763\u591a\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u95ee\u7b54\u80fd\u529b\u4e14\u53ef\u5b9e\u9645\u65e0\u7f1d\u96c6\u6210\u5e94\u7528\uff0c\u5bf9\u771f\u5b9e\u573a\u666f\u5177\u5907\u9ad8\u5ea6\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18176", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u7684InSAR\u5730\u9762\u5f62\u53d8\u65f6\u5e8f\u6570\u636e\u4e2d\u9884\u6d4b\u672a\u6765\u5730\u9762\u5f62\u53d8\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u66f4\u5177\u7a7a\u95f4\u8fde\u7eed\u6027\u7684\u9884\u6d4b\u3002", "motivation": "\u5730\u9762\u5f62\u53d8\u76d1\u6d4b\u5bf9\u4e8e\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u5b89\u5168\u548c\u5730\u8d28\u707e\u5bb3\u9632\u63a7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5229\u7528\u7a00\u758f\u7684InSAR\u6570\u636e\u5bf9\u672a\u6765\u5f62\u53d8\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u7a00\u758f\u7684\u70b9\u89c2\u6d4b\u6570\u636e\u8f6c\u5316\u4e3a\u7a20\u5bc6\u7684\u65f6\u7a7a\u5f20\u91cf\uff0c\u518d\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0e\u957f\u77ed\u65f6\u8bb0\u5fc6\uff08CNN-LSTM\uff09\u6df7\u5408\u6a21\u578b\uff0c\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u7279\u5f81\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u6a21\u578b\u4e0eLightGBM\u548cLASSO\u7b49\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u4f5c\u4e86\u5bf9\u6bd4\uff0c\u4f7f\u7528\u4e86\u7231\u5c14\u5170\u4e1c\u90e8\u7684Sentinel-1\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u4e14\u7a7a\u95f4\u4e0a\u66f4\u8fde\u8d2f\u7684\u5730\u9762\u5f62\u53d8\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\uff0c\u4f20\u7edf\u6a21\u578b\u591a\u53ea\u6355\u6349\u5230\u7b80\u5355\u7684\u6301\u7eed\u6027\u53d8\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u5730\u8868\u53d8\u5f62\u7684\u590d\u6742\u65f6\u7a7a\u52a8\u6001\u3002", "conclusion": "\u8bc1\u660e\u4e86\u65f6\u7a7a\u6df1\u5ea6\u5b66\u4e60\u5728\u5730\u9762\u5f62\u53d8\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\uff0c\u4e3a\u6b64\u9886\u57df\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u96c6\u6210\u65f6\u7a7a\u65b9\u6cd5\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6307\u7075\u5de7\u624b\u7684\u51e0\u4f55\u611f\u77e5\u63a8\u62c9\uff08GD2P\uff09\u65b9\u6cd5\uff0c\u5b9e\u73b0\u975e\u6293\u53d6\u5f0f\u64cd\u63a7\u3002\u901a\u8fc7\u91c7\u6837\u3001\u7269\u7406\u6a21\u62df\u548c\u6269\u6563\u6a21\u578b\uff0c\u4f18\u5316\u624b\u578b\u59ff\u6001\u4ee5\u9ad8\u6548\u64cd\u7eb5\u590d\u6742\u7269\u4f53\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002", "motivation": "\u4f20\u7edf\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u4f9d\u8d56\u5939\u722a\u6216\u7b80\u5355\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u7ed3\u6784\u590d\u6742\u6216\u96be\u4ee5\u6293\u53d6\u7684\u7269\u4f53\u3002\u591a\u6307\u7075\u5de7\u624b\u63a5\u89e6\u6a21\u5f0f\u4e30\u5bcc\uff0c\u66f4\u80fd\u9002\u5e94\u5404\u7c7b\u7269\u4f53\uff0c\u4f46\u5176\u5728\u975e\u6293\u53d6\u64cd\u4f5c\u4e2d\u6a21\u578b\u5efa\u7acb\u548c\u63a7\u5236\u96be\u5ea6\u5927\u3002\u4e3a\u5f25\u8865\u8fd9\u4e9b\u6311\u6218\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u7075\u5de7\u624b\u7684\u591a\u6837\u6027\u4e0e\u7075\u6d3b\u6027\uff0c\u63d0\u9ad8\u975e\u6293\u53d6\u64cd\u4f5c\u7684\u7a33\u5b9a\u4e0e\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faGD2P\u6846\u67b6\uff0c\u5c06\u63a8\u62c9\u64cd\u4f5c\u8f6c\u5316\u4e3a\u7075\u5de7\u624b\u7684\u9884\u63a5\u89e6\u59ff\u6001\u751f\u6210\u548c\u5b66\u4e60\u95ee\u9898\u3002\u9996\u5148\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u7684\u624b\u578b\uff0c\u518d\u7528\u7269\u7406\u4eff\u771f\u7b5b\u9009\u6709\u6548\u59ff\u6001\uff0c\u6700\u540e\u8bad\u7ec3\u6761\u4ef6\u5316\u6269\u6563\u6a21\u578b\u9884\u6d4b\u9002\u5408\u5f53\u524d\u7269\u4f53\u51e0\u4f55\u7684\u624b\u578b\u3002\u5728\u5b9e\u9645\u64cd\u4f5c\u65f6\uff0c\u7cfb\u7edf\u91c7\u6837\u624b\u578b\u5e76\u7528\u8fd0\u52a8\u89c4\u5212\u5668\u9009\u62e9\u548c\u6267\u884c\u5177\u4f53\u63a8\u62c9\u52a8\u4f5c\u3002", "result": "\u5728Allegro Hand\u5e73\u53f0\u8fdb\u884c\u4e86840\u6b21\u771f\u5b9e\u5b9e\u9a8c\uff0c\u5c06GD2P\u4e0e\u73b0\u6709\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u548c\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u5728LEAP Hand\u4e0a\u4e5f\u6210\u529f\u9a8c\u8bc1\uff0c\u8bc1\u660e\u672c\u65b9\u6cd5\u53ef\u9002\u5e94\u4e0d\u540c\u7684\u7075\u5de7\u624b\u786c\u4ef6\u3002", "conclusion": "GD2P\u4e3a\u7075\u5de7\u624b\u5b9e\u73b0\u9ad8\u6548\u3001\u901a\u7528\u7684\u975e\u6293\u53d6\u64cd\u63a7\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5f00\u653e\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.18175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u60c5\u611f\u8bc6\u522b\u4e0e\u9884\u6d4b\u67b6\u6784\uff08ERFC\uff09\uff0c\u80fd\u591f\u5728\u5bf9\u8bdd\u4e2d\u591a\u6a21\u6001\u5730\u8bc6\u522b\u5e76\u9884\u6d4b\u60c5\u611f\uff0c\u4e3a\u547c\u53eb\u4e2d\u5fc3\u7b49\u573a\u666f\u63d0\u5347\u5ba2\u6237\u4f53\u9a8c\u3002", "motivation": "\u5728\u547c\u53eb\u4e2d\u5fc3\u53ca\u4f17\u591a\u884c\u4e1a\u4e2d\uff0c\u80fd\u53ca\u65f6\u8bc6\u522b\u548c\u9884\u6d4b\u7528\u6237\u60c5\u7eea\u6709\u52a9\u4e8e\u63d0\u5347\u5ba2\u6237\u6ee1\u610f\u5ea6\u548c\u4e1a\u52a1\u4ef7\u503c\u3002\u5ba2\u670d\u4ee3\u8868\u9700\u501f\u52a9\u60c5\u611f\u6d1e\u5bdf\uff0c\u79ef\u6781\u5f15\u5bfc\u5ba2\u6237\u60c5\u7eea\uff0c\u4ece\u800c\u5316\u89e3\u8d1f\u9762\u60c5\u7eea\uff0c\u8fd9\u662f\u73b0\u4ee3\u5ba2\u6237\u670d\u52a1\u7684\u91cd\u8981\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86ERFC\u65b0\u67b6\u6784\uff0c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u7efc\u5408\u8003\u8651\u60c5\u7eea\u5c5e\u6027\u3001\u4e0a\u4e0b\u6587\u4ee5\u53ca\u8bf4\u8bdd\u4eba\u4e4b\u95f4\u7684\u60c5\u611f\u5173\u8054\uff0c\u9488\u5bf9\u6bcf\u8f6e\u5bf9\u8bdd\u8bed\u53e5\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u548c\u9884\u6d4b\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cERFC\u67b6\u6784\u5728\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "ERFC\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u5982\u547c\u53eb\u4e2d\u5fc3\u7b49\u573a\u666f\u4e2d\u7684\u5ba2\u6237\u4f53\u9a8c\uff0c\u5bf9\u9700\u8981\u60c5\u611f\u8bc6\u522b\u7684\u5546\u4e1a\u5e94\u7528\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2509.18177", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corr\u00eaa de Ara\u00fajo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Scrapbook\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u591a\u6837\u6027\u4e30\u5bcc\u7684AI\u6a21\u578b\u8bc4\u6d4b\u6570\u636e\u96c6\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u57fa\u7840\u6982\u5ff5\u5982\u7269\u4f53\u8bc6\u522b\u3001\u4f4d\u7f6e\u548c\u5c5e\u6027\u7b49\uff0c\u5e76\u7528\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f53\u524d\u6a21\u578b\u5728\u90e8\u5206\u57fa\u7840\u4efb\u52a1\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u5176\u5bf9\u57fa\u7840\u6982\u5ff5\uff08\u5982\u7269\u4f53\u8bc6\u522b\u3001\u4f4d\u7f6e\u548c\u5c5e\u6027\uff09\u7684\u771f\u6b63\u7406\u89e3\u3002\u73b0\u6709\u8bc4\u6d4b\u6570\u636e\u96c6\u8986\u76d6\u9762\u548c\u9488\u5bf9\u6027\u4e0d\u8db3\uff0c\u6709\u5fc5\u8981\u8bbe\u8ba1\u65b0\u7684\u65b9\u6cd5\u7cfb\u7edf\u6027\u63a2\u67e5AI\u5bf9\u8fd9\u4e9b\u57fa\u7840\u5143\u7d20\u7684\u638c\u63e1\u60c5\u51b5\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86Scrapbook\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5927\u91cf\u9488\u5bf9\u5355\u4e00\u57fa\u7840\u6982\u5ff5\u548c\u8bed\u8a00\u53d8\u4f53\u7684\u95ee\u9898\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8fd9\u4e9b\u591a\u6837\u7684\u6d4b\u95ee\uff0c\u5bf9AI\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u7b49\u57fa\u7840\u8bc6\u522b\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7edd\u5bf9/\u76f8\u5bf9\u4f4d\u7f6e\u7406\u89e3\u3001\u590d\u6742\u7ea6\u675f\u7b49\u95ee\u9898\u4e0a\u6613\u51fa\u9519\u3002\u4f8b\u5982MobileVLM-V2\u5728\u4f4d\u7f6e\u7c7b\u95ee\u9898\u4e0a\u7b54\u6848\u5206\u6b67\u5927\uff0c\u4e14\u5b58\u5728\u53ef\u4fe1\u4f46\u9519\u8bef\u56de\u7b54\uff1b\u5176\u5b83\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u5bf9\u80af\u5b9a\u7b54\u6848\u7684\u504f\u597d\u548c\u51e0\u4f55\u5f62\u72b6\u8bc6\u522b\u56f0\u96be\u3002", "conclusion": "Scrapbook\u6846\u67b6\u6709\u52a9\u4e8e\u4e30\u5bcc\u548c\u7ec6\u5316AI\u57fa\u7840\u80fd\u529b\u8bc4\u6d4b\uff0c\u6709\u5229\u4e8e\u7cfb\u7edf\u6027\u53d1\u73b0\u6a21\u578b\u4e0d\u8db3\u5e76\u6307\u5bfc\u5176\u6539\u8fdb\uff0c\u4e3a\u672a\u6765AI\u6a21\u578b\u6df1\u5165\u7406\u89e3\u4e0e\u4e00\u81f4\u6027\u63d0\u5347\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\uff08FDI\uff09\u6846\u67b6\uff0c\u65e0\u9700\u7269\u7406\u5197\u4f59\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u7cfb\u7edf\u7684\u611f\u77e5\u6a21\u5757\u5bf9\u4e0b\u6e38\u51b3\u7b56\u5f71\u54cd\u663e\u8457\uff0c\u611f\u77e5\u6545\u969c\u5177\u6709\u73af\u5883\u76f8\u5173\u6027\u4e14\u6613\u5728\u591a\u7ea7\u6d41\u7a0b\u4e2d\u4f20\u64ad\u3002\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u5b9e\u73b0\u7269\u7406\u5197\u4f59\uff0c\u6210\u672c\u9ad8\u4e14\u6709\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u5206\u6790\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5206\u6790\u5197\u4f59\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u628a\u53ef\u9760\u6027\u68c0\u6d4b\u89c6\u4e3a\u56e0\u679c\u7ed3\u679c\u3002\u901a\u8fc7\u5bf9\u5047\u8bbe\u6027\u6545\u969c\u573a\u666f\u4e0b\u7684\u6d4b\u8bd5\u7ed3\u679c\u8fdb\u884c\u63a8\u7406\uff0c\u52a8\u6001\u66f4\u65b0\u5bf9\u6545\u969c\u53d1\u751f\u7684\u4fe1\u5ff5\u3002\u5305\u542b\u88ab\u52a8\u548c\u4e3b\u52a8FDI\uff1a\u88ab\u52a8\u65b9\u6cd5\u901a\u8fc7\u4fe1\u5ff5\u66f4\u65b0\u5b9e\u73b0\uff0c\u4e3b\u52a8\u65b9\u6cd5\u5c06\u5176\u5efa\u6a21\u4e3a\u56e0\u679cbandit\u95ee\u9898\uff0c\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u7f6e\u4fe1\u4e0a\u754c\uff08UCB\uff09\u7b97\u6cd5\uff0c\u9009\u62e9\u6700\u80fd\u63d0\u5347\u68c0\u6d4b\u4e0e\u9694\u79bb\u6548\u80fd\uff08EI\uff09\u7684\u63a7\u5236\u8f93\u5165\u3002", "result": "\u5728\u7a7a\u95f4\u673a\u5668\u4eba\u89c6\u89c9\u5bfc\u822a\u573a\u666f\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u5b9e\u4e3b\u52a8\u8c03\u6574\u673a\u5668\u4eba\u59ff\u6001\u53ef\u63d0\u5347\u68c0\u6d4b\u4fe1\u606f\u91cf\uff08EI\uff09\uff0c\u5e76\u53ef\u6709\u6548\u9694\u79bb\u7531\u4f20\u611f\u5668\u635f\u574f\u3001\u52a8\u6001\u573a\u666f\u53ca\u611f\u77e5\u9000\u5316\u5f15\u53d1\u7684\u6545\u969c\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u65e0\u9700\u589e\u52a0\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.18293", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e868\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u68c0\u6d4b\u53cd\u72b9\u592a\u5185\u5bb9\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGuided-CoT\u7684\u65b0\u578b\u63d0\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u6240\u6709\u6a21\u578b\u7684\u68c0\u6d4b\u8868\u73b0\uff0c\u5c24\u5176Llama 3.1 70B\u8868\u73b0\u8d85\u8fc7\u4e86\u5fae\u8c03\u7248\u7684GPT-3.5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u4ec7\u6068\u5185\u5bb9\u5c42\u51fa\u4e0d\u7a77\uff0c\u81ea\u52a8\u68c0\u6d4b\u5de5\u5177\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u578b\u9700\u4e0d\u65ad\u9002\u5e94\u590d\u6742\u591a\u53d8\u7684\u6587\u672c\u73af\u5883\u3002\u7279\u522b\u662f\u9488\u5bf9\u53cd\u72b9\u592a\u5185\u5bb9\uff0c\u5f53\u524d\u65b9\u6cd5\u548c\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7b49\u5747\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u4e868\u4e2a\u5f00\u6e90LLM\uff0c\u4f7f\u7528\u4e0d\u540c\u63d0\u793a\u7b56\u7565\uff08\u7279\u522b\u662f\u57fa\u4e8e\u60c5\u5883\u5b9a\u4e49\u7684\u51c6\u5219\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u578b\u94fe\u5f0f\u601d\u8003\u63d0\u793a\uff08Guided-CoT\uff09\u3002\u540c\u65f6\u5206\u6790LLM\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u65b0\u6307\u6807\u91cf\u5316\u6a21\u578b\u751f\u6210\u7406\u7531\u7684\u8bed\u4e49\u504f\u5dee\u3002", "result": "Guided-CoT\u65b9\u6cd5\u5728\u5168\u90e8\u8bc4\u6d4b\u6a21\u578b\u4e2d\u90fd\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u65e0\u8bba\u63a8\u7406\u914d\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u8fd8\u662f\u63a8\u7406\u80fd\u529b\u3002Llama 3.1 70B\u6a21\u578b\u8868\u73b0\u751a\u81f3\u8d85\u8fc7\u4e86\u7ecf\u8fc7\u5fae\u8c03\u7684GPT-3.5\u3002", "conclusion": "\u5f15\u5165\u6307\u5bfc\u578bCoT\u63d0\u793a\u80fd\u6301\u7eed\u63d0\u5347LLM\u5728\u53cd\u72b9\u592a\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u5b9e\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u9700\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4f18\u5316\u6a21\u578b\u4e0e\u8bc4\u4f30\u65b9\u5f0f\u3002"}}
{"id": "2509.18179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u91cf\u5316\u4e86\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u591a\u6a21\u6001AI\u7cfb\u7edf\u4e2d\u4fe1\u606f\u635f\u5931\u7684\u7a0b\u5ea6\uff0c\u53d1\u73b0\u901a\u8fc7\u6587\u5b57\u4f5c\u4e3a\u4e2d\u4ecb\u4e25\u91cd\u635f\u5bb3\u4e86\u56fe\u50cf\u4fe1\u606f\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4e8e\u521b\u610f\u6d41\u7a0b\uff0c\u4f46\u89c6\u89c9\u5185\u5bb9\u7ecf\u8fc7\u6587\u672c\u4e2d\u4ecb\u540e\u6240\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\u5c1a\u7f3a\u4e4f\u91cf\u5316\u5206\u6790\u3002\u8bc4\u4f30\u8fd9\u79cd\u4fe1\u606f\u635f\u5931\u5bf9\u4e8e\u7406\u89e3\u7cfb\u7edf\u5c40\u9650\u6027\u5341\u5206\u5173\u952e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u201c\u63cf\u8ff0-\u518d\u751f\u6210\u201d\u6d41\u7a0b\uff0c\u5373\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u89c6\u89c9\u5185\u5bb9\u518d\u751f\u6210\u56fe\u50cf\uff1b\u751f\u6210150\u5bf9\u5b9e\u9a8c\u56fe\u7247\uff0c\u5e76\u91c7\u7528LPIPS\u3001SSIM\u548c\u8272\u5dee\u7b49\u6307\u6807\uff0c\u4ece\u611f\u77e5\u3001\u7ed3\u6784\u548c\u8272\u5f69\u7ef4\u5ea6\u8bc4\u4f30\u4fe1\u606f\u4fdd\u7559\u7a0b\u5ea6\u3002", "result": "\u7ed3\u679c\u663e\u793a99.3%\u7684\u6837\u672c\u5728\u611f\u77e5\u4e0a\u6709\u660e\u663e\u9000\u5316\uff0c91.5%\u7684\u6837\u672c\u7ed3\u6784\u4fe1\u606f\u6709\u663e\u8457\u635f\u5931\uff0c\u91cf\u5316\u4e86\u8fd9\u4e00\u73af\u8282\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\u4e25\u91cd\u548c\u666e\u904d\u3002", "conclusion": "\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\uff0c\u8868\u660e\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u4ee5\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u89c6\u89c9\u4e2d\u4ecb\u4f1a\u5e26\u6765\u660e\u663e\u7684\u4fe1\u606f\u635f\u5931\uff0c\u8fd9\u4e00\u201c\u63cf\u8ff0-\u518d\u751f\u6210\u201d\u74f6\u9888\u662f\u53ef\u6d4b\u4e14\u7a33\u5b9a\u7684\u5c40\u9650\u3002"}}
{"id": "2509.18463", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u5956\u52b1\u51fd\u6570\u6709\u610f\u8fdb\u884c\u6270\u52a8\uff0c\u53ef\u4ee5\u4f7f\u673a\u5668\u4eba\u4e60\u5f97\u591a\u6837\u5316\u7684\u64cd\u4f5c\u6280\u80fd\u3002\u901a\u8fc7\u6db2\u4f53\u5012\u5165\u7684\u6a21\u62df\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5f97\u5230\u65e2\u80fd\u5b8c\u6210\u4e3b\u8981\u4efb\u52a1\uff0c\u53c8\u80fd\u884d\u751f\u65b0\u6280\u80fd\u7684\u591a\u6837\u5316\u7b56\u7565\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e0c\u671b\u83b7\u5f97\u591a\u6837\u5316\u4e14\u53ef\u8fc1\u79fb\u7684\u6280\u80fd\uff0c\u800c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u53ea\u671d\u7740\u5355\u4e00\u6700\u4f18\u7b56\u7565\u6536\u655b\u3002\u5982\u4f55\u81ea\u52a8\u53d1\u6398\u66f4\u5e7f\u6cdb\u9ad8\u4ef7\u503c\u884c\u4e3a\uff0c\u5c24\u5176\u5728\u5956\u52b1\u51fd\u6570\u4e0d\u786e\u5b9a\u6216\u591a\u76ee\u6807\u573a\u666f\u4e0b\uff0c\u662f\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5bf9\u5956\u52b1\u51fd\u6570\u5404\u5206\u9879\u6743\u91cd\u65bd\u52a0\u9ad8\u65af\u566a\u58f0\uff08reward function mutation\uff09\uff0c\u4ee5\u6a21\u62df\u4e0d\u540c\u7684\u6210\u672c\u6548\u76ca\u6743\u8861\uff0c\u57fa\u4e8ePPO\u7b97\u6cd5\u5728NVIDIA Isaac Sim\u4eff\u771f\u73af\u5883\u548cFranka Emika Panda\u673a\u68b0\u81c2\u4e0a\u5f00\u5c55\u6db2\u4f53\u5012\u5165\u5b9e\u9a8c\uff0c\u5bf9\u5956\u52b1\u914d\u7f6e\u6270\u52a8\u540e\u7684\u7b56\u7565\u591a\u6837\u6027\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5bf9\u5956\u52b1\u51fd\u6570\u7684\u4e0d\u540c\u5e72\u6270\uff0c\u83b7\u5f97\u4e86\u591a\u6837\u5316\u7684\u5012\u6db2\u884c\u4e3a\uff0c\u5305\u62ec\u539f\u672c\u76ee\u6807\u4efb\u52a1\u7684\u591a\u79cd\u6267\u884c\u65b9\u5f0f\uff0c\u4ee5\u53ca\u80fd\u884d\u751f\u51fa\u8bf8\u5982\u6e05\u6d17\u5bb9\u5668\u8fb9\u7f18\u3001\u6405\u62cc\u6db2\u4f53\u4e0e\u6d47\u6c34\u7b49\u65b0\u9896\u6280\u80fd\u3002", "conclusion": "\u5956\u52b1\u51fd\u6570\u6270\u52a8\u4e3a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u4efb\u52a1\u591a\u6837\u5316\u4e0e\u6280\u80fd\u81ea\u4e3b\u63a2\u7d22\u7684\u65b0\u9014\u5f84\uff0c\u65e2\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u4e5f\u4e3a\u672a\u6765\u6280\u80fd\u7684\u81ea\u52a8\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5TEMPO\uff0c\u9488\u5bf9\u957f\u5e8f\u5217\u4e2dtoken\u7ea7\u5956\u52b1\u7a00\u758f\u4e14\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u524d\u7f00\u6811\u7684\u65e0\u53c2\u65b9\u6cd5\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684credit assignment\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982PPO\u3001GRPO\uff09\u7528\u4e8eLLM\u63a8\u7406\u65f6\uff0c\u5728token\u7ea7\u5956\u52b1\u5206\u914d\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u7a00\u758f\u4e14\u51b3\u7b56token\u6709\u9650\u7684\u9a8c\u8bc1\u578b\u4efb\u52a1\uff08\u5982\u6570\u5b66\u548c\u533b\u5b66\u95ee\u7b54\uff09\u4e0b\u3002PPO\u96be\u4ee5\u6cdb\u5316\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\uff0cGRPO\u5ffd\u7565\u4e86\u5206\u652f\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u66f4\u5f3a\u7684token\u7ea7credit assignment\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faPrefix-to-Tree (P2T)\u65b9\u6cd5\uff0c\u5c06\u591a\u7ec4\u54cd\u5e94\u6784\u6210\u524d\u7f00\u6811\uff0c\u5229\u7528\u8282\u70b9\u540e\u4ee3\u7ed3\u679c\u975e\u53c2\u6570\u5730\u8ba1\u7b97\u6bcf\u4e00\u6b65\u7684prefix value\u3002\u57fa\u4e8eP2T\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faTEMPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u652f\u8282\u70b9\u7684\u65f6\u5e8f\u5dee\u5206\u4fee\u6b63\uff0c\u5b9e\u73b0\u65e0\u9700\u8bc4\u4ef7\u7f51\u7edc\u7684\u7cbe\u786etoken\u7ea7\u5956\u52b1\u5206\u914d\uff0c\u517c\u5177GRPO\u7684\u9ad8\u6548\u548cPPO\u7684\u7cbe\u7ec6\u3002", "result": "\u5728Qwen3-1.7B/4B\u7b49\u5927\u6a21\u578b\u4e0a\uff0cTEMPO\u5728\u6570\u5b66\u3001\u533b\u5b66\u548c\u7efc\u5408\u63a8\u7406\u7c7b\u57fa\u51c6(in-domain: MATH, MedQA; out-of-domain: GSM-HARD\u7b49)\u5747\u4f18\u4e8ePPO\u548cGRPO\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u8bad\u7ec3\u65f6\u957f\u76f8\u5f53\u3002", "conclusion": "TEMPO \u5b9e\u73b0\u4e86\u65e0\u9700\u4ef7\u503c\u7f51\u7edc\u3001\u9002\u7528\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0b\u7684\u5927\u6a21\u578btoken\u7ea7credit assignment\uff0c\u5e76\u5728\u5e7f\u6cdb\u63a8\u7406\u4efb\u52a1\u4e0a\u6709\u66f4\u4f18\u8868\u73b0\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5229\u7528AI\u6280\u672f\u81ea\u52a8\u4ece\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u5f71\u50cf\u4e2d\u63a8\u65ad\u5c4b\u9876\u5c5e\u6027\uff0c\u5e76\u4ee5\u5723\u6587\u68ee\u7279\u548c\u683c\u6797\u7eb3\u4e01\u65af\u4e3a\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u89e3\u51b3\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u707e\u5bb3\u98ce\u9669\u8bc4\u4f30\u6570\u636e\u77ed\u7f3a\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u707e\u5bb3\u4e8b\u4ef6\u5982\u6c14\u65cb\u3001\u6d2a\u6c34\u548c\u6ed1\u5761\u8feb\u5207\u9700\u8981\u8be6\u7ec6\u7684\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\u4ee5\u8bc4\u4f30\u6f5c\u5728\u635f\u5bb3\uff0c\u4f46\u6c14\u5019\u6613\u53d7\u5f71\u54cd\u7684\u52a0\u52d2\u6bd4\u7b49\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6(SIDS)\u5f80\u5f80\u7f3a\u4e4f\u8fd9\u7c7b\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u79cdAI\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u5f71\u50cf\u81ea\u52a8\u63d0\u53d6\u5c4b\u9876\u5c5e\u6027\u3002\u6bd4\u8f83\u4e86\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u6d45\u5c42\u5206\u7c7b\u5668\u4e0e\u5fae\u8c03\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c4b\u9876\u5206\u7c7b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f15\u5165\u90bb\u8fd1SIDS\u989d\u5916\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f18\u6a21\u578b\u5728\u5c4b\u9876\u5761\u5ea6\u548c\u5c4b\u9876\u6750\u6599\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684F1\u5206\u6570\u5206\u522b\u8fbe\u52300.88\u548c0.83\u3002\u5f15\u5165\u6765\u81ea\u90bb\u8fd1\u56fd\u5bb6\u7684\u6570\u636e\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u672c\u5730\u80fd\u529b\u5efa\u8bbe\uff0c\u8be5\u5de5\u4f5c\u4e3a\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u5229\u7528AI\u548c\u5730\u7403\u89c2\u6d4b\u6570\u636e\u63d0\u5347\u57ce\u5e02\u6cbb\u7406\u548c\u5e94\u5bf9\u707e\u5bb3\u98ce\u9669\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u521b\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.18466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u63a7\u5236\u6846\u67b6\uff0c\u63d0\u5347\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfMPC\u5728\u53cc\u8db3\u673a\u5668\u4eba\u590d\u6742\u73af\u5883\u4e0b\uff08\u5982\u5d0e\u5c96\u548c\u6e7f\u6ed1\u5730\u5f62\uff09\u7684\u9002\u7528\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u96be\u4ee5\u7cbe\u786e\u5efa\u6a21\u5730\u5f62\u4ea4\u4e92\u3002\u800cRL\u5728\u591a\u6837\u5730\u5f62\u4e0b\u80fd\u8bad\u7ec3\u51fa\u9c81\u68d2\u7b56\u7565\uff0c\u4f46\u96be\u4ee5\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u4e14\u9700\u8981\u5927\u91cf\u5956\u52b1\u8bbe\u8ba1\u3002\u7ed3\u5408\u4e24\u8005\u6709\u671b\u517c\u53d6\u4f18\u70b9\uff0c\u56e0\u6b64\u63a2\u7d22\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u91c7\u7528RL\u589e\u5f3aMPC\uff0c\u5177\u4f53\u5bf9\u57fa\u4e8e\u5355\u521a\u4f53\u52a8\u529b\u5b66\u7684MPC\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u53c2\u6570\u5316\uff1a\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u817f\u63a7\u5236\u5668\u548c\u6b65\u6001\u9891\u7387\u3002\u901a\u8fc7\u5728NVIDIA IsaacLab\u4e2d\u5bf9\u591a\u79cd\u590d\u6742\u5730\u5f62\uff08\u697c\u68af\u3001\u8df3\u77f3\u3001\u4f4e\u6469\u64e6\u7b49\uff09\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u5bf9\u65b9\u6cd5\u7684\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684RL\u589e\u5f3aMPC\u6846\u67b6\u6bd4\u5355\u72ec\u7684MPC\u6216RL\u57fa\u7ebf\u65b9\u6cd5\u5177\u5907\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u5404\u7c7b\u590d\u6742\u5730\u5f62\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u878d\u5408\u4e86MPC\u7684\u7ea6\u675f\u4fdd\u8bc1\u4e0eRL\u7684\u9002\u5e94\u6027\u4f18\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u5d0e\u5c96\u6e7f\u6ed1\u7b49\u590d\u6742\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\u3002"}}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u6539\u5584\u8bca\u65ad\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u8def\u5f84\u5224\u522b\u80fd\u529b\u5bf9\u5b9e\u9645\u8bca\u65ad\u4efb\u52a1\u7684\u8fc1\u79fb\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u5f0f\u5bf9\u8def\u5f84\u5224\u522b\u6709\u6548\uff0c\u4f46\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u8fc1\u79fb\u6709\u9650\u3002", "motivation": "LLM\u5728\u533b\u5b66\u8bca\u65ad\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u524d\u666f\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u77e5\u8bc6\u652f\u6491\u3002\u77e5\u8bc6\u56fe\u8c31\u53ef\u4ee5\u63d0\u4f9b\u7ed3\u6784\u5316\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u4f20\u7edf\u96c6\u6210\u65b9\u5f0f\u672a\u53d1\u6325\u51faKG\u7684\u63a8\u7406\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u5f62\u5f0f\u8ba9LLM\u66f4\u597d\u5730\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u63d0\u5347\u533b\u5b66AI\u7cfb\u7edf\u5728\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u4ee5LLM\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u5bf9\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u5224\u522b\uff0c\u5224\u5b9a\u5176\u662f\u5426\u80fd\u6b63\u786e\u5bfc\u51fa\u533b\u751f\u8bca\u65ad\uff0c\u5e76\u5bf9\u4e94\u79cd\u4efb\u52a1\u8bbe\u5b9a\u548c\u516b\u79cd\u8bad\u7ec3\u8303\u5f0f\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u3002\u968f\u540e\uff0c\u68c0\u9a8c\u5224\u522b\u80fd\u529b\u5728\u5305\u62ec\u8bca\u65ad\u6458\u8981\u548c\u533b\u5b66\u95ee\u7b54\u7b49\u5b9e\u9645\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90\u6307\u4ee4\u5fae\u8c03LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u73b0\u51fa\uff1a\uff081\uff09\u9488\u5bf9\u8def\u5f84\u5224\u522b\uff0c\u7279\u5b9a\u7684\u5956\u52b1\u4f18\u5316\u548c\u77e5\u8bc6\u84b8\u998f\u80fd\u5e26\u6765\u8f83\u597d\u6548\u679c\uff1b\uff082\uff09\u4f46\u8be5\u5224\u522b\u80fd\u529b\u5bf9\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u7684\u8fc1\u79fb\u6027\u8f83\u5f31\uff0c\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "LLM\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u505a\u4e34\u5e8aKG\u63a8\u7406\u8def\u5f84\u5224\u522b\u6709\u4e00\u5b9a\u4f18\u52bf\uff0c\u4f46\u589e\u5f3a\u7684\u5224\u522b\u80fd\u529b\u96be\u4ee5\u76f4\u63a5\u8fc1\u79fb\u5230\u5b9e\u9645\u8bca\u65ad\u4efb\u52a1\u3002\u7814\u7a76\u4e3a\u533b\u7597GenAI\u7cfb\u7edf\u5229\u7528\u7ed3\u6784\u5316\u5956\u52b1\u76d1\u7763\u63d0\u4f9b\u4e86\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u3002"}}
{"id": "2509.18183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVLA-LPAF\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u591a\u89c6\u89d2\u73af\u5883\u4e0b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8bRoboFlamingo-LPAF\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u9700\u6839\u636e\u4e0d\u540c\u8bbe\u5907\uff08\u5982\u7b2c\u4e09\u4eba\u79f0\u548c\u624b\u8155\u6444\u50cf\u5934\uff09\u91c7\u96c6\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u4fe1\u606f\u6765\u5b8c\u6210\u4efb\u52a1\uff0c\u4f46\u7531\u4e8e\u4e0d\u540c\u89c6\u89d2\u5bfc\u81f4\u89c6\u89c9\u7279\u5f81\u5dee\u5f02\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u6025\u9700\u89e3\u51b3\u89c6\u89d2\u5f02\u8d28\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7VLA-LPAF\u6a21\u5757\uff0c\u57fa\u4e8e2D\u6570\u636e\u8fdb\u884c\u5355\u89c6\u89d2\u56fe\u50cf\u5fae\u8c03\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u878d\u5408\u591a\u89c6\u89d2\u89c2\u6d4b\uff0c\u6709\u6548\u7f13\u89e3\u56e0\u89c6\u89d2\u4e0d\u4e00\u81f4\u5e26\u6765\u7684\u7279\u5f81\u9e3f\u6c9f\u3002\u4ee5RoboFlamingo\u4e3a\u57fa\u7840\u5b9e\u73b0RoboFlamingo-LPAF\u7cfb\u7edf\u3002", "result": "RoboFlamingo-LPAF\u5728CALVIN\u3001LIBERO\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u4efb\u52a1\u6210\u529f\u7387\u5206\u522b\u63d0\u53478%\u300115%\uff0c\u5e76\u5728\u81ea\u5236\u4eff\u771f\u57fa\u51c6\u4e0a\u63d0\u534730%\u3002\u540c\u65f6\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u4e5f\u5c55\u793a\u51fa\u826f\u597d\u7684\u89c6\u89d2\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "VLA-LPAF\u6a21\u5757\u80fd\u9ad8\u6548\u63d0\u5347VLA\u6a21\u578b\u5bf9\u591a\u89c6\u89d2\u8f93\u5165\u7684\u9002\u5e94\u6027\uff0c\u4e3a\u901a\u7528\u3001\u591a\u73af\u5883\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u5efa\u6a21\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u5411\u5305\u7edc\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u53ef\u8ba9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u65e0\u9884\u5b9a\u53c2\u8003\u8f68\u8ff9\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u9a7e\u9a76\u5e76\u5e94\u5bf9\u5404\u79cd\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u4f18\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u53ef\u884c\u6027\u548c\u5b89\u5168\u7ea6\u675f\u7684\u7edf\u4e00\u89c4\u5212\u548c\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u4e0e\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u9ad8\u6027\u80fd\u81ea\u52a8\u9a7e\u9a76\u8981\u6c42\u8f66\u8f86\u5728\u52a8\u6001\u6781\u9650\u4e0b\u5b89\u5168\u884c\u9a76\uff0c\u4e14\u8bb8\u591a\u60c5\u5f62\u4e0b\u65e0\u6cd5\u4f9d\u8d56\u9884\u5148\u8bbe\u5b9a\u597d\u7684\u8f68\u8ff9\u3002\u7136\u800c\uff0c\u73b0\u6709\u5927\u591a\u6570\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\u4ee5\u53c2\u8003\u8f68\u8ff9\u4e3a\u57fa\u7840\uff0c\u96be\u4ee5\u5904\u7406\u65e0\u6cd5\u63d0\u524d\u5b9a\u4e49\u53c2\u8003\u7684\u590d\u6742\u60c5\u5883\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u6781\u9650\u5de5\u51b5\u4e0b\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u8f68\u8ff9\u3001\u5e76\u80fd\u81ea\u9002\u5e94\u591a\u573a\u666f\u7684\u9ad8\u6548\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u5305\u7edc\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u5305\u62ec\uff1a\uff081\uff09\u8bbe\u8ba1\u4f18\u5316\u63a7\u5236\u53cb\u597d\u7684\u9ad8\u6548\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\uff1b\uff082\uff09\u63d0\u51fa\u6301\u7eed\u53ef\u5fae\u7684\u9a7e\u9a76\u5305\u7edc\u6570\u5b66\u5efa\u6a21\uff0c\u5c06\u8f66\u8f86\u6240\u6709\u53ef\u884c\u9a76\u533a\u57df\u7cbe\u51c6\u63cf\u8ff0\uff1b\uff083\uff09\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u4f18\u5316\u65b9\u6cd5\u5b9e\u73b0\u5305\u7edc\u89c4\u5212\uff0c\u4ece\u800c\u65e0\u9700\u9884\u8bbe\u53c2\u8003\u8f68\u8ff9\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u60c5\u666f\u4e0b\u7684\u5b9e\u65f6\u51b3\u7b56\u4e0e\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u8def\u6d4b\uff0c\u9a8c\u8bc1\u8be5\u5305\u7edc\u63a7\u5236\u6846\u67b6\u5728\u8d5b\u8f66\u3001\u7d27\u6025\u907f\u969c\u3001\u8d8a\u91ce\u7b49\u591a\u79cd\u5178\u578b\u6781\u9650\u4efb\u52a1\u4e2d\u7684\u9ad8\u6027\u80fd\u8868\u73b0\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u590d\u6742\u73af\u5883\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u5bf9\u53c2\u8003\u8f68\u8ff9\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u573a\u666f\u4e0b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u52a8\u6001\u6781\u9650\u63a7\u5236\uff0c\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u6027\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5185\u5b58\u6709\u9650\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u90e8\u7f72\u96be\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubSpec\u7684\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u635f\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u9ad8\u5bf9\u9f50\u5ea6\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u66ff\u6362\u5c42\u8349\u7a3f\u6a21\u578b\uff0c\u5e76\u5171\u4eab\u90e8\u5206\u5c42\u4e0eKV\u7f13\u5b58\uff0c\u5b9e\u73b0\u4e86\u5927\u5e45\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u76ee\u524dLLM\u6a21\u578b\u4f53\u79ef\u5e9e\u5927\uff0c\u90e8\u7f72\u5728\u6d88\u8d39\u7ea7GPU\u65f6\u9762\u4e34\u663e\u5b58\u4e0d\u8db3\u6311\u6218\u3002\u5e38\u89c1\u7684\u538b\u7f29\u548c\u53c2\u6570\u5378\u8f7d\u65b9\u6cd5\u8981\u4e48\u635f\u5931\u6a21\u578b\u8d28\u91cf\uff0c\u8981\u4e48\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u3002\u6295\u673a\u89e3\u7801\u867d\u7136\u80fd\u63d0\u5347\u5378\u8f7d\u65f6\u901f\u5ea6\uff0c\u4f46\u73b0\u6709\u505a\u6cd5\u9700\u8981\u9644\u52a0\u8bad\u7ec3\uff0c\u4e14\u63d0\u901f\u6709\u9650\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u8d28\u91cf\u65e0\u635f\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51faSubSpec\u65b9\u6cd5\uff1a\u6839\u636e\u76ee\u6807LLM\u4e2d\u5378\u8f7d\u90e8\u5206\uff0c\u5229\u7528\u4f4e\u6bd4\u7279\u91cf\u5316\u751f\u6210\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\u5c42\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u90e8\u5206\u5c42\u4e0eKV-Cache\u7ee7\u7eed\u9a7b\u7559\u5728GPU\u4e0a\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u538b\u529b\u5e76\u63d0\u5347\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u5bf9\u9f50\u5ea6\u3002\u7528\u6237\u76f4\u63a5\u4f7f\u7528\u6b64\u65b9\u6cd5\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6709\u6a21\u578b\u6216\u9884\u8bad\u7ec3\u6743\u91cd\u3002", "result": "\u57288GB\u663e\u5b58\u9650\u5236\u4e0b\uff0cQwen2.5 7B\u6a21\u578b\u5728MT-Bench\u4e0a\u5e73\u5747\u63a8\u7406\u901f\u5ea6\u63d0\u53479.1\u500d\uff1b\u572824GB\u663e\u5b58\u9650\u5236\u4e0b\uff0cQwen2.5 32B\u6a21\u578b\u5728\u5e38\u7528\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u52a0\u901f\u8fbe\u523012.5\u500d\u3002", "conclusion": "SubSpec\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u635f\u8d28\u91cf\u4e14\u5373\u63d2\u5373\u7528\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u53ef\u5927\u5e45\u63d0\u5347LLM\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u5e26\u6765\u4e86\u73b0\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7f51\u7edc\uff08URNet\uff09\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728DSEC\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e8b\u4ef6\u76f8\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u666e\u904d\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8003\u8651\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86URNet\u7f51\u7edc\uff0c\u878d\u5408\u4e86\u7ec6\u7c92\u5ea6\u672c\u5730\u4fe1\u606f\u4e0e\u957f\u8ddd\u79bb\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u7ec6\u5316\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eKL\u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728DSEC\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cURNet\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "URNet\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e8b\u4ef6\u89c6\u89c9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2509.18576", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u6846\u67b6LCMF\uff0c\u6709\u6548\u878d\u5408\u5f02\u6784\u6570\u636e\u5e76\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u591a\u6a21\u6001\u667a\u80fd\u51b3\u7b56\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u5bf9\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u3001\u7406\u89e3\u6307\u4ee4\u548c\u667a\u80fd\u51b3\u7b56\u65f6\uff0c\u4f46\u9762\u4e34\u6570\u636e\u878d\u5408\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3\u7b49\u6280\u672f\u96be\u9898\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u5728Mamba\u6a21\u5757\u4e2d\u5f15\u5165\u591a\u5c42\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\uff0c\u7ed3\u5408Cross-Attention\u4e0e\u9009\u62e9\u6027\u53c2\u6570\u5171\u4eab\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5f02\u6784\u6a21\u6001\u878d\u5408\u548c\u8bed\u4e49\u4e92\u8865\u5bf9\u9f50\u3002", "result": "LCMF\u5728VQA\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u8fbe74.29%\uff0c\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u5728EQA\u89c6\u9891\u4efb\u52a1\u4e2d\u4e0eLLM Agents\u76f8\u6bd4\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u4e2d\u4e0a\u6e38\u8868\u73b0\u3002\u5176\u8f7b\u91cf\u5316\u8bbe\u8ba1\u76f8\u6bd4\u540c\u7c7b\u65b9\u6cd5FLOPs\u51cf\u5c114.35\u500d\uff0c\u53c2\u6570\u91cf\u663e\u8457\u8f83\u4f4e\u3002", "conclusion": "LCMF\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u6027\u7684\u591a\u6a21\u6001\u51b3\u7b56\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2509.18360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6587\u672c\u8f6c\u5f55\u5373\u53ef\u5bf9\u9f50\u5e73\u884c\u8bed\u97f3\u6587\u6863\u7684\u65b0\u65b9\u6cd5Speech Vecalign\uff0c\u5e76\u5728\u82f1\u5fb7\u8bed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u53ca\u66f4\u4f73\u7684\u8bed\u97f3\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6316\u6398\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u8f6c\u5f55\u6216\u5b58\u5728\u5bf9\u9f50\u7247\u6bb5\u77ed\u3001\u566a\u97f3\u5927\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u8bed\u97f3\u7ffb\u8bd1\u6570\u636e\u7684\u83b7\u53d6\u3002\u8be5\u5de5\u4f5c\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8f6c\u5f55\u3001\u5bf9\u9f50\u7247\u6bb5\u66f4\u957f\u3001\u566a\u97f3\u66f4\u5c0f\u7684\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u8bed\u97f3\u5bf9\u9f50\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u63d0\u51faSpeech Vecalign\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u8bed\u97f3\u5d4c\u5165\u7684\u5355\u8c03\u5bf9\u9f50\u5b9e\u73b0\u5e73\u884c\u8bed\u97f3\u6587\u6863\u914d\u5bf9\uff0c\u65e0\u9700\u6587\u672c\u8f6c\u5f55\uff1b\u4e0eGlobal Mining\u548cLocal Mining\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5728\u5927\u89c4\u6a21\u82f1\u5fb7\u8bed\u672a\u7ecf\u6807\u6ce8\u7684VoxPopuli\u6570\u636e\u96c6\u4e0a\u5e94\u7528\uff083000\u5c0f\u65f6\u539f\u59cb\u8bed\u6599\uff09\uff0c\u5e76\u7528\u5f97\u5230\u7684\u5bf9\u9f50\u6570\u636e\u8bad\u7ec3\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u3002", "result": "Speech Vecalign\u65b9\u6cd5\u83b7\u5f97\u4e861000\u5c0f\u65f6\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\u3002\u7ffb\u8bd1\u6a21\u578b\u5728\u82f1\u8bd1\u5fb7\u3001\u5fb7\u8bd1\u82f1\u4efb\u52a1\u4e0a\uff0cASR-BLEU\u5206\u6570\u76f8\u6bd4Global Mining\u63d0\u9ad8\u4e860.37\u548c0.18\uff0c\u5e76\u5728\u4ec5\u4f7f\u75281/8\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u7ffb\u8bd1\u6027\u80fd\u4e0eSpeechMatrix\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "Speech Vecalign\u65e0\u9700\u6587\u672c\u8f6c\u5f55\u5c31\u80fd\u9ad8\u6548\u83b7\u5f97\u5927\u89c4\u6a21\u3001\u957f\u7247\u6bb5\u3001\u4f4e\u566a\u97f3\u7684\u8bed\u97f3\u5bf9\u9f50\uff0c\u4e3a\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d28\u7684\u6570\u636e\u6765\u6e90\uff0c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u65e0\u76d1\u7763\u8bed\u97f3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "C\u00e9cile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVisionerves\u7684\u65b0\u578bAI\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86MRI\u6570\u636e\u4e2d\u5916\u5468\u795e\u7ecf\u7684\u6210\u50cf\u548c\u8bc6\u522b\u80fd\u529b\uff0c\u5bf9\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u76f8\u5173\u795e\u7ecf\u635f\u4f24\u7684\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u60a3\u8005\u5e38\u4f34\u6709\u6162\u6027\u76c6\u8154\u75db\u53ca\u795e\u7ecf\u635f\u4f24\uff0c\u4f46\u76ee\u524d\u57fa\u4e8e\u5f71\u50cf\u5b66\u7684\u5916\u5468\u795e\u7ecf\u6210\u50cf\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "method": "\u63d0\u51faVisionerves\u6df7\u5408AI\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u89e3\u5256\u7ed3\u6784\u7684\u81ea\u52a8\u5206\u5272\uff0c\u7ed3\u5408\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u8fdb\u884c\u795e\u7ecf\u675f\u8ffd\u8e2a\u4e0e\u8bc6\u522b\uff0c\u65e0\u9700\u624b\u52a8ROI\u9009\u62e9\uff0c\u57fa\u4e8e\u591a\u68af\u5ea6DWI\u548c\u5f62\u6001\u5b66MRI\u6570\u636e\u3002", "result": "\u572810\u4f8b\u786e\u8bca\u6216\u7591\u4f3c\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u5973\u6027\u7684\u8170\u9ab6\u4e1b\u9a8c\u8bc1\u4e2d\uff0cVisionerves\u5728Dice\u7cfb\u6570\u4e0a\u8f83\u6807\u51c6\u795e\u7ecf\u675f\u8ffd\u8e2a\u63d0\u5347\u6700\u9ad8\u8fbe25%\uff0c\u7a7a\u95f4\u8bef\u5dee\u964d\u81f35\u6beb\u7c73\u4ee5\u5185\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u4e14\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u53ef\u5b9e\u73b0\u8be6\u7ec6\u7684\u795e\u7ecf\u5206\u6790\uff0c\u4e3a\u65e0\u521b\u8bca\u65ad\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u76f8\u5173\u795e\u7ecf\u75c5\u53d8\u53ca\u5176\u5b83\u795e\u7ecf\u635f\u4f24\u6027\u75be\u75c5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18592", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u6846\u67b6VLN-Zero\uff0c\u80fd\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u96f6\u6837\u672c\u7684\u667a\u80fd\u51b3\u7b56\u4e0e\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u77e5\u73af\u5883\u65f6\u4f9d\u8d56\u5927\u91cf\u63a2\u7d22\u6216\u50f5\u5316\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u5b9e\u73b0\u81ea\u4e3b\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u4e14\u5177\u5907\u6cdb\u5316\u6027\u7684\u5bfc\u822a\u7b56\u7565\u3002", "method": "VLN-Zero\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\uff081\uff09\u63a2\u7d22\u9636\u6bb5\uff0c\u57fa\u4e8e\u7ed3\u6784\u5316\u63d0\u793a\u5f15\u5bfcVLM\u9ad8\u6548\u63a2\u7d22\u73af\u5883\uff0c\u6784\u5efa\u7d27\u51d1\u7684\u7b26\u53f7\u573a\u666f\u56fe\uff1b\uff082\uff09\u90e8\u7f72\u9636\u6bb5\uff0c\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u5229\u7528\u573a\u666f\u56fe\u4e0e\u73af\u5883\u89c2\u6d4b\u63a8\u7406\u51fa\u53ef\u6267\u884c\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u7f13\u5b58\u673a\u5236\u63d0\u9ad8\u8def\u5f84\u590d\u7528\u6548\u7387\uff0c\u52a0\u901f\u9002\u5e94\u3002", "result": "VLN-Zero\u5728\u591a\u79cd\u73af\u5883\u4e0b\u76f8\u8f83\u4e8e\u6700\u65b0\u96f6\u6837\u672c\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u53472\u500d\uff0c\u8d85\u8fc7\u591a\u6570\u5fae\u8c03\u57fa\u7ebf\uff0c\u62b5\u8fbe\u76ee\u6807\u6240\u9700\u65f6\u95f4\u51cf\u5c11\u4e00\u534a\uff0cVLM\u8c03\u7528\u6b21\u6570\u5e73\u5747\u51cf\u5c1155%\u3002", "conclusion": "\u878d\u5408\u5feb\u901f\u63a2\u7d22\u3001\u7b26\u53f7\u63a8\u7406\u53ca\u7f13\u5b58\u6267\u884c\uff0cVLN-Zero\u514b\u670d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u7684\u8ba1\u7b97\u4f4e\u6548\u4e0e\u6cdb\u5316\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u7c7b-\u7cfb\u7edf\u534f\u4f5c\u7684\u8bf4\u8bdd\u4eba\u5206\u79bb\u7ea0\u9519\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8ba9\u7528\u6237\u5b9e\u65f6\u7ea0\u6b63\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u9519\u8bef\uff0c\u4ece\u800c\u6709\u6548\u51cf\u5c11\u5206\u79bb\u9519\u8bef\u7387\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u81ea\u52a8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u4e0d\u652f\u6301\u7528\u6237\u53c2\u4e0e\u53cd\u9988\uff0c\u5bfc\u81f4\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u9519\u8bef\u65f6\u65e0\u6cd5\u5373\u65f6\u7ea0\u6b63\uff0c\u5f71\u54cd\u51c6\u786e\u7387\u3002\u5f15\u5165\u7528\u6237\u53cd\u9988\u53ef\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u4fbf\u6377\u7684\u7ea0\u9519\u6d41\u7a0b\u3002", "method": "1\uff09\u7cfb\u7edf\u5728\u7ebf\u6267\u884cASR\u548c\u8bf4\u8bdd\u4eba\u5206\u79bb\uff1b2\uff09\u5229\u7528LLM\u751f\u6210\u7b80\u8981\u5bf9\u8bdd\u6458\u8981\u5e76\u4f9b\u7528\u6237\u53c2\u8003\uff1b3\uff09\u7528\u6237\u7528\u7b80\u77ed\u8bed\u97f3\u53cd\u9988\u7ea0\u6b63\u5f52\u5c5e\u9519\u8bef\uff0c\u7cfb\u7edf\u7acb\u5373\u66f4\u65b0\u7ed3\u679c\u800c\u4e0d\u4e2d\u65ad\u4ea4\u4e92\uff1b4\uff09\u63d0\u51faSWM\u6280\u672f\uff0c\u80fd\u53d1\u73b0\u5e76\u62c6\u5206ASR\u8bef\u5f52\u4e3a\u5355\u4e00\u8bf4\u8bdd\u4eba\u7684\u591a\u8bf4\u8bdd\u4eba\u7247\u6bb5\uff1b5\uff09\u57fa\u4e8e\u7528\u6237\u7ea0\u6b63\u81ea\u52a8\u6536\u96c6\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\u4fe1\u606f\uff0c\u9010\u6b65\u51cf\u5c11\u672a\u6765\u5206\u79bb\u9519\u8bef\u3002", "result": "\u5728AMI\u6570\u636e\u96c6\u4e0a\u7684\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5c06\u8bf4\u8bdd\u4eba\u5206\u79bb\u9519\u8bef\u7387\uff08DER\uff09\u964d\u4f4e\u4e869.92%\uff0c\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u964d\u4f4e\u4e8644.23%\u3002\u5e76\u5206\u6790\u4e86\u6458\u8981\u663e\u793a\u4e0e\u5168\u6587\u672c\u663e\u793a\u3001\u5728\u7ebf\u6ce8\u518c\u6570\u4e0a\u9650\u3001\u53ca\u7ea0\u9519\u9891\u7387\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u5408LLM\u4e0e\u7528\u6237\u53cd\u9988\uff0c\u80fd\u6781\u5927\u63d0\u5347\u6d41\u5f0f\u8bed\u97f3\u5206\u79bb\u7684\u51c6\u786e\u6027\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u5b9e\u65f6\u9ad8\u6548\u548c\u7528\u6237\u53cb\u597d\u7684\u4f18\u70b9\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u4e0b\u7684\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u8303\u4f8b\u3002"}}
{"id": "2509.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86V-SenseDrive\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u5df4\u57fa\u65af\u5766\u9053\u8def\u73af\u5883\u4e0b\u6536\u96c6\u7684\u3001\u6ce8\u91cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u5229\u7528\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u548c\u524d\u5411\u6444\u50cf\u5934\u89c6\u9891\uff0c\u8bb0\u5f55\u4e86\u4e0d\u540c\u7c7b\u578b\u9053\u8def\u4e0a\u7684\u4e09\u79cd\u9a7e\u9a76\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u4e3b\u8981\u6765\u6e90\u4e8e\u53d1\u8fbe\u56fd\u5bb6\uff0c\u96be\u4ee5\u53cd\u6620\u65b0\u5174\u7ecf\u6d4e\u4f53\u591a\u6837\u5316\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4e14\u6d89\u53ca\u9a7e\u9a76\u8005\u9762\u90e8\u5f55\u5236\u5b58\u5728\u9690\u79c1\u95ee\u9898\u3002\u5df4\u57fa\u65af\u5766\u7b49\u56fd\u5bb6\u4ea4\u901a\u73af\u5883\u590d\u6742\uff0c\u6025\u9700\u9ad8\u9002\u7528\u6027\u6570\u636e\u4ee5\u63d0\u5347\u9053\u8def\u5b89\u5168\u53ca\u667a\u80fd\u4ea4\u901a\u6280\u672f\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u5b89\u5353App\uff0c\u91c7\u96c6\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u3001GPS\u53ca\u524d\u5411\u8def\u51b5\u89c6\u9891\uff0c\u5e76\u7cbe\u786e\u540c\u6b65\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6570\u636e\u83b7\u53d6\u3002\u7814\u7a76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6570\u636e\u91c7\u96c6\u6d41\u7a0b\u3001\u53c2\u4e0e\u8005\u9074\u9009\u3001\u9a7e\u9a76\u573a\u666f\u8bbe\u7f6e\u3001\u73af\u5883\u53ca\u6570\u636e\u540c\u6b65\u7b49\u5173\u952e\u73af\u8282\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u8986\u76d6\u57ce\u5e02\u4e3b\u5e72\u9053\u3001\u6b21\u8981\u9053\u8def\u548c\u9ad8\u901f\u516c\u8def\u4e09\u7c7b\u8def\u51b5\uff0c\u8bb0\u5f55\u6b63\u5e38\u3001\u6fc0\u70c8\u4e0e\u5371\u9669\u9a7e\u9a76\u884c\u4e3a\u7684\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u5206\u4e3a\u539f\u59cb\u3001\u5904\u7406\u3001\u8bed\u4e49\u4e09\u5c42\uff0c\u4fbf\u4e8e\u540e\u7eed\u591a\u79cd\u7814\u7a76\u5e94\u7528\u3002", "conclusion": "V-SenseDrive\u4e30\u5bcc\u4e86\u9a7e\u9a76\u884c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u8d44\u6e90\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u50cf\u5df4\u57fa\u65af\u5766\u8fd9\u6837\u7684\u65b0\u5174\u5e02\u573a\uff0c\u4e3a\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u3001\u672c\u5730\u4ea4\u901a\u5b89\u5168\u7814\u7a76\u53ca\u4e0a\u4e0b\u6e38\u76f8\u5173\u4ea7\u4e1a\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.18597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u673a\u5668\u4eba\u64cd\u4f5c\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ea0\u6b63\u4fe1\u606f\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u63d0\u5347\u4e86\u957f\u4efb\u52a1\u5e8f\u5217\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u539f\u8bed\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5bf9\u957f\u4efb\u52a1\u5e8f\u5217\u9002\u5e94\u6027\u5dee\uff1b\u7ea0\u9519\u77e5\u8bc6\u5b58\u50a8\u683c\u5f0f\u4e0d\u5f53\u5bfc\u81f4\u6cdb\u5316\u5f31\u548c\u9057\u5fd8\u4e25\u91cd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u6280\u80fd\u5b66\u4e60\u548c\u7ea0\u9519\u77e5\u8bc6\u7f16\u7801\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eba\u7c7b\u5728\u73af\u7684\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u5c06\u4fee\u6b63\u8fc7\u7684\u7ecf\u9a8c\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u5e76\u7ed3\u5408\u5916\u90e8\u8bb0\u5fc6\u548c\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u63d0\u793a\u673a\u5236\u652f\u6301\u52a8\u6001\u590d\u7528\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5728Ravens\u3001Franka Kitchen\u3001MetaWorld\u4ee5\u53ca\u771f\u5b9e\u573a\u666f\u4e0b\u5b9e\u9a8c\uff0c\u8be5\u6846\u67b6\u6210\u529f\u7387\u8fbe0.93\uff0c\u8f83\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe27%\uff0c\u7ea0\u9519\u8f6e\u6570\u6548\u7387\u63d0\u534742%\uff0c\u80fd\u591f\u7a33\u5b9a\u89e3\u51b3\u5305\u542b20\u591a\u4e2a\u539f\u8bed\u64cd\u4f5c\u7684\u8d85\u957f\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u9ad8\u6548\u3001\u7a33\u5065\u5730\u5904\u7406\u6781\u7aef\u957f\u4efb\u52a1\u5e8f\u5217\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u751f\u6210\uff0c\u63d0\u5347\u4e86\u7ea0\u9519\u3001\u6cdb\u5316\u548c\u6280\u80fd\u590d\u7528\u80fd\u529b\uff0c\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6848\u3002"}}
{"id": "2509.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86NormGenesis\uff0c\u4e00\u4e2a\u9762\u5411\u591a\u8bed\u8a00\uff08\u82f1\u6587\u3001\u4e2d\u6587\u3001\u97e9\u6587\uff09\u7684\u793e\u4f1a\u89c4\u8303\u5bf9\u8bdd\u751f\u6210\u548c\u6807\u6ce8\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u9896\u7684\u5bf9\u8bdd\u7c7b\u578b\u4e0e\u5927\u89c4\u6a21\u89c4\u8303\u6ce8\u91ca\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u901a\u5e38\u4ec5\u5173\u6ce8\u5185\u5bb9\u8fde\u8d2f\u6027\uff0c\u5ffd\u89c6\u4e86\u793e\u4f1a\u89c4\u8303\u548c\u6587\u5316\u5dee\u5f02\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5185\u5bb9\u7f3a\u4e4f\u793e\u4f1a\u53ef\u63a5\u53d7\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u3002\u56e0\u6b64\u9700\u8981\u8de8\u6587\u5316\u3001\u591a\u8bed\u8a00\u7684\u89c4\u8303\u5316\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0e\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u793e\u4ea4\u80fd\u529b\u548c\u4f26\u7406\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86NormGenesis\u6846\u67b6\uff0c\u6db5\u76d6\u4e09\u79cd\u8bed\u8a00\uff0c\u5e76\u521b\u9020\u6027\u5730\u63d0\u51fa\u4e86V2R\uff08Violation-to-Resolution\uff09\u5bf9\u8bdd\u7c7b\u578b\uff0c\u6a21\u62df\u793e\u4f1a\u89c4\u8303\u8fdd\u80cc\u540e\u7684\u8bc6\u522b\u548c\u4fee\u6b63\u8fc7\u7a0b\u3002\u4e3a\u5f31\u52bf\u8bed\u8a00\u5b9e\u65bd\u57fa\u4e8e\u8303\u4f8b\u7684\u8fed\u4ee3\u7ec6\u5316\uff0c\u63d0\u5347\u8bed\u7528\u4e00\u81f4\u6027\u3002\u6784\u5efa\u4e8610,800\u7ec4\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6807\u6ce8\u89c4\u8303\u9075\u5b88\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u60c5\u611f\u53cd\u5e94\uff0c\u5e76\u8fdb\u884c\u4e86\u4eba\u5de5\u4e0e\u5927\u6a21\u578b\u8bc4\u6d4b\u3002", "result": "NormGenesis\u5728\u5bf9\u8bdd\u81ea\u7136\u5ea6\u3001\u7cbe\u7ec6\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002V2R\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5904\u7406\u4f26\u7406\u654f\u611f\u73af\u5883\u4e2d\u7684\u8bed\u7528\u80fd\u529b\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "NormGenesis\u4e3a\u8de8\u6587\u5316\u3001\u8de8\u8bed\u8a00\u7684\u793e\u4f1a\u89c4\u8303\u5bf9\u8bdd\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u548c\u53ef\u6269\u5c55\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6587\u5316\u9002\u5e94\u6027\u548c\u4f26\u7406\u8bed\u7528\u80fd\u529b\u3002"}}
{"id": "2509.18189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Qianfan-VL\u7cfb\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u53c2\u6570\u89c4\u6a21\u4ece3B\u523070B\u4e0d\u7b49\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9886\u57df\u589e\u5f3a\u6280\u672f\u5b9e\u73b0\u4e86\u4e1a\u754c\u9886\u5148\u7684\u8868\u73b0\uff0c\u7279\u522b\u5728OCR\u3001\u6587\u6863\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u901a\u7528\u80fd\u529b\u548c\u7279\u5b9a\u9886\u57df\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u8bc6\u522b\u3001\u6587\u6863\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u7b49\u9886\u57df\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u63d0\u5347\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0a\u8ff0\u5173\u952e\u9886\u57df\u7684\u80fd\u529b\uff0c\u6ee1\u8db3\u4f01\u4e1a\u591a\u6837\u5316\u573a\u666f\u7684\u9700\u6c42\uff0c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u9ad8\u7cbe\u5ea6\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u5bf9\u7279\u5b9a\u9886\u57df\u80fd\u529b\u8fdb\u884c\u589e\u5f3a\uff0c\u517c\u987e\u6cdb\u5316\u80fd\u529b\u3002\u5f15\u5165\u94fe\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728Baidu\u81ea\u7814\u786c\u4ef6\u5927\u89c4\u6a21\u9ad8\u6548\u8bad\u7ec3\uff085000\u9897Kunlun P800\u82af\u7247\uff0c90%\u6269\u5c55\u6548\u7387\uff09\u3002", "result": "Qianfan-VL\u5728CCBench\u3001SEEDBench IMG\u3001ScienceQA\u3001MMStar\u7b49\u4e3b\u6d41\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6216\u4e0e\u4e3b\u6d41\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff1b\u5728OCRBench\u3001DocVQA\u7b49\u9886\u57df\u4efb\u52a1\u4e0a\u5927\u5e45\u9886\u5148\uff1b\u7279\u522b\u662f8B\u548c70B\u6a21\u578b\u5728\u6570\u7406\u63a8\u7406\uff08MathVista 78.6%\uff09\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002", "conclusion": "Qianfan-VL\u901a\u8fc7\u521b\u65b0\u6280\u672f\u5b9e\u73b0\u4e86\u901a\u7528\u4e0e\u9886\u57df\u80fd\u529b\u517c\u5177\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u4e3a\u9886\u57df\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u8def\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u5927\u89c4\u6a21\u9ad8\u6548AI\u57fa\u7840\u8bbe\u65bd\u5728\u4f01\u4e1a\u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2509.18608", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u80fd\u591f\u76f4\u63a5\u5c06\u539f\u59cb3D LiDAR\u6570\u636e\u6620\u5c04\u4e3a\u8fd0\u52a8\u63a7\u5236\u6307\u4ee4\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u5168\u9762\u6d4b\u8bd5\u3002", "motivation": "\u5728\u519c\u4e1a\u6797\u4e0b\u73af\u5883\u4e2d\uff0c\u7531\u4e8eGNSS\uff08\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff09\u4fe1\u53f7\u4e0d\u53ef\u9760\u3001\u73af\u5883\u6742\u4e71\u53ca\u5149\u7167\u6761\u4ef6\u591a\u53d8\uff0c\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u65e0\u4eba\u673a\u6216\u673a\u5668\u4eba\u5bfc\u822a\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u7b56\u7565\u7f51\u7edc\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb3D\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u3002\u4e3a\u63d0\u5347\u6548\u7387\uff0c\u91c7\u7528\u4f53\u7d20\u5316\u964d\u91c7\u6837\u7b56\u7565\uff0c\u5c06\u6fc0\u5149\u96f7\u8fbe\u8f93\u5165\u6570\u636e\u91cf\u51cf\u5c1195.83%\uff0c\u65e0\u9700\u6807\u7b7e\u6570\u636e\u96c6\u6216\u624b\u5de5\u8bbe\u8ba1\u63a7\u5236\u6a21\u5757\u3002", "result": "\u5728\u4eff\u771f\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u76f4\u7ebf\u578b\u519c\u4f5c\u7269\u884c\u95f4\u5bfc\u822a\u6210\u529f\u7387\u8fbe100%\u3002\u5f53\u519c\u4f5c\u7269\u884c\u7684\u66f2\u7387\u9010\u6e10\u589e\u52a0\u65f6\uff0c\u5bfc\u822a\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002\u6d4b\u8bd5\u6db5\u76d6\u4e86\u4e0d\u540c\u6b63\u5f26\u9891\u7387\u4e0e\u632f\u5e45\u7684\u884c\u95f4\u7ed3\u6784\u3002", "conclusion": "\u8be5\u5b66\u4e60\u578b\u5bfc\u822a\u7cfb\u7edf\u5728\u6807\u51c6\u76f4\u7ebf\u8def\u51b5\u4e0b\u8868\u73b0\u975e\u5e38\u7a33\u5b9a\uff0c\u5728\u66f2\u7ebf\u8def\u51b5\u4e0b\u6709\u4e00\u5b9a\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548cLiDAR\u611f\u77e5\u7684\u519c\u4e1a\u81ea\u52a8\u5bfc\u822a\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.18401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5177\u6709\u6ce2\u65af\u6587\u5316\u7279\u8272\u7684\u6587\u5b66\u6587\u672c\u65b9\u9762\u7684\u521b\u9020\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4ef7\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u5173\u4e8eLLM\u751f\u6210\u6587\u5b66\u6587\u672c\u7684\u7814\u7a76\u4e3b\u8981\u805a\u7126\u4e8e\u82f1\u8bed\uff0c\u8f83\u5c11\u6d89\u53ca\u975e\u82f1\u8bed\u6587\u5b66\u4f20\u7edf\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u3002\u672c\u8bba\u6587\u610f\u5728\u63a2\u7d22LLM\u5728\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6587\u672c\u65f6\u7684\u8868\u73b0\uff0c\u5f25\u8865\u76f8\u5173\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u8986\u76d620\u4e2a\u4e3b\u9898\u7684\u6ce2\u65af\u6587\u5b66\u7528\u6237\u751f\u6210\u6570\u636e\u96c6\uff0c\u4f9d\u636e\u6539\u7f16\u7684Torrance\u521b\u9020\u6027\u601d\u7ef4\u6d4b\u9a8c\uff0c\u4ece\u539f\u521b\u6027\u3001\u6d41\u7545\u6027\u3001\u7075\u6d3b\u6027\u548c\u62d3\u5c55\u6027\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4ef7\u6a21\u578b\u8f93\u51fa\u3002\u540c\u65f6\uff0c\u91c7\u7528LLM\u81ea\u52a8\u8bc4\u5206\uff0c\u5e76\u7528\u4eba\u7c7b\u8bc4\u5206\u8fdb\u884c\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u5206\u6790\u4e86LLM\u8fd0\u7528\u6bd4\u55bb\u3001\u9690\u55bb\u3001\u5938\u5f20\u548c\u5bf9\u7acb\u7b49\u6587\u5b66\u4fee\u8f9e\u624b\u6cd5\u7684\u80fd\u529b\u3002", "result": "LLM\u81ea\u52a8\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u5728\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u826f\u597d\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86LLM\u5728\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6587\u672c\u65f6\u65e2\u6709\u521b\u9020\u529b\u4e0a\u7684\u4f18\u70b9\uff0c\u4e5f\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\u3002", "conclusion": "LLM\u5728\u6ce2\u65af\u6587\u5b66\u6587\u672c\u751f\u6210\u5177\u6709\u4e00\u5b9a\u521b\u9020\u529b\uff0c\u4f46\u5728\u6587\u5316\u8868\u8fbe\u548c\u4fee\u8f9e\u8fd0\u7528\u7b49\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u8be5\u7814\u7a76\u4e3a\u975e\u82f1\u8bed\u6587\u5b66\u751f\u6210\u7684\u8bc4\u4ef7\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u65b9\u6cd5\u548c\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2509.18190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u53bb\u96fe\u65b0\u65b9\u6cd5HazeFlow\uff0c\u7ed3\u5408\u65b0\u7684\u4eff\u771f\u6570\u636e\u751f\u6210\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u771f\u5b9e\u73af\u5883\u4e0b\u590d\u6742\u96fe\u973e\u60c5\u51b5\u7684\u5904\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u53bb\u96fe\u65b9\u6cd5\u53d7\u9650\u4e8e\u771f\u5b9e\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u548c\u57df\u95f4\u843d\u5dee\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff1b\u800c\u7ecf\u5178\u5927\u6c14\u6563\u5c04\u6a21\u578b\uff08ASM\uff09\u53c8\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u7684\u591a\u6837\u5316\u96fe\u973e\u60c5\u51b5\u3002\u4f5c\u8005\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u548c\u6570\u636e\u6a21\u62df\u7684\u521b\u65b0\u601d\u8def\u3002", "method": "1\uff09\u5c06\u7ecf\u5178\u7684\u5927\u6c14\u6563\u5c04\u6a21\u578b\uff08ASM\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u4e2aODE\u95ee\u9898\uff0c\u501f\u9274Rectified Flow\uff08RF\uff09\u601d\u60f3\uff0c\u8bbe\u8ba1HazeFlow\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60ODE\u8f68\u8ff9\u5b9e\u73b0\u4ece\u6709\u96fe\u56fe\u50cf\u5230\u6e05\u6670\u56fe\u50cf\u7684\u6620\u5c04\uff0c\u4e14\u53ea\u9700\u5355\u6b65\u63a8\u7406\u30022\uff09\u63d0\u51fa\u57fa\u4e8e\u9a6c\u5c14\u79d1\u592b\u94fe\u5e03\u6717\u8fd0\u52a8\uff08MCBM\uff09\u7684\u975e\u5747\u5300\u96fe\u973e\u751f\u6210\u4eba\u5de5\u6570\u636e\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u548c\u73b0\u5b9e\u611f\u3002", "result": "HazeFlow\u65b9\u6cd5\u5728\u591a\u4e2a\u771f\u5b9e\u573a\u666f\u53bb\u96fe\u6570\u636e\u96c6\u4e0a\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u5c55\u73b0\u51fa\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u8fbe\u5230\u4e86\u4e1a\u754c\u6700\u65b0\u6c34\u5e73\uff08state-of-the-art\uff09\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u4e0e\u4eff\u771f\u6570\u636e\u521b\u65b0\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53bb\u96fe\u6a21\u578b\u73b0\u5b9e\u9002\u5e94\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u96be\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPIE\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u878d\u5408\u4e86\u591a\u6a21\u6001\u611f\u77e5\u3001\u63a8\u7406\u548c\u610f\u56fe\u5efa\u6a21\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u53ef\u4ee5\u7b80\u5316\u7e41\u7410\u7684\u7cfb\u7edf\u6d41\u7a0b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u7406\u89e3\u548c\u6709\u6548\u51b3\u7b56\u9884\u6d4b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u4f18\u7684\u611f\u77e5\u878d\u5408\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "PIE\u6846\u67b6\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u611f\u77e5\u3001\u63a8\u7406\u4e0e\u610f\u56fe\u5efa\u6a21\u80fd\u529b\uff0c\u91c7\u7528\u4e86\u53cc\u5411Mamba\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\u89e3\u51b3\u76f8\u673a\u4e0e\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u6570\u636e\u5728\u878d\u5408\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u57fa\u4e8eMamba\u548cMixture-of-Experts\u7684\u65b0\u578b\u63a8\u7406\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u573a\u666f\u4e00\u81f4\u7684\u951a\u70b9\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u4f18\u5316\u3002\u8fd8\u5305\u542b\u52a8\u4f5c-\u8fd0\u52a8\u4ea4\u4e92\u6a21\u5757\uff0c\u7528\u4e8e\u7ed3\u5408\u73af\u5883\u4e2d\u5176\u4ed6\u4ea4\u901a\u4f53\u7684\u72b6\u6001\u9884\u6d4b\u4f18\u5316\u81ea\u8eab\u51b3\u7b56\u3002", "result": "\u5728NAVSIM\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u4e0a\uff0cPIE\u672a\u91c7\u7528\u96c6\u6210\u6216\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0cPDM\u5f97\u520688.9\u3001EPDM\u5f97\u520685.6\uff0c\u5747\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "PIE\u6846\u67b6\u80fd\u591f\u7a33\u5b9a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\uff0c\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.18439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4f1a\u8bdd\u5bf9\u9f50(CA)\u5206\u6570\u81ea\u52a8\u5316\u5927\u89c4\u6a21\u6d4b\u91cf\u533b\u751f\u4e0e\u60a3\u8005\u5171\u51b3\u7b56(SDM)\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bc4\u4ef7SDM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u624b\u6bb5\u3002", "motivation": "\u5171\u51b3\u7b56(SDM)\u662f\u5b9e\u73b0\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u533b\u7597\u7684\u91cd\u8981\u65b9\u5f0f\uff0c\u76ee\u524d\u7f3a\u4e4f\u80fd\u591f\u81ea\u52a8\u5316\u3001\u89c4\u6a21\u5316\u6d4b\u91cfSDM\u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "method": "\u7814\u7a76\u6536\u96c6157\u4f8b\u5fc3\u623f\u98a4\u52a8\u60a3\u8005\u4e0e\u533b\u751f\u7684\u5bf9\u8bdd\u5f55\u50cf\u5e76\u8f6c\u5f55\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548cBERT\u6a21\u578b\u8fdb\u884c\u53e5\u5b50\u7ea7\u7684\u4e0a\u4e0b\u6587-\u5e94\u7b54\u9884\u6d4b(NSP)\uff0c\u6784\u5efa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdCA\u5206\u6570\u3002\u7136\u540e\u7ed3\u5408\u4e34\u5e8a\u8bc4\u5206\uff08DCS\u548cOPTION12\uff09\uff0c\u5206\u6790CA\u5206\u6570\u4e0eSDM\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u5fae\u8c03BERTbase\u53d6\u5f97\u6700\u9ad8recall@1\u4e3a0.640\uff0c\u4e0d\u7528stylebook\u7684DL\u6a21\u578brecall@1\u4e3a0.227\u3002DL\u65e0stylebook\u6a21\u578b\u7684AbsMax\u548cMax CA\u5206\u6570\u4e0eOPTION12\u76f8\u5173\uff1b\u5fae\u8c03BERTbase\u6a21\u578b\u751f\u6210\u7684Max CA\u5206\u6570\u4e0eDCS\u76f8\u5173\u3002BERT\u6a21\u578b\u5927\u5c0f\u4e0d\u5f71\u54cdCA\u5206\u6570\u4e0eSDM\u7684\u5173\u8054\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u533b\u751f-\u60a3\u8005\u5bf9\u8bddSDM\u6d4b\u91cf\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u89e3\u91ca\u7684CA\u5206\u6570\uff0c\u6709\u671b\u5b9e\u73b0SDM\u7b56\u7565\u5e7f\u6cdb\u8bc4\u4f30\u3002"}}
{"id": "2509.18193", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u519c\u4e1a\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u3001\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548cTensorRT\u52a0\u901f\uff0c\u5bf9EcoWeedNet\u6a21\u578b\u8fdb\u884c\u538b\u7f29\u4e0e\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7f29\u5c0f68.5%\u3001\u63a8\u65ad\u901f\u5ea6\u63d0\u5347\u81f3184 FPS\uff0c\u5e76\u5728\u68c9\u82b1\u7530\u6742\u8349\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41YOLO\u53d8\u4f53\uff0c\u8bc1\u660e\u5176\u9ad8\u6548\u3001\u51c6\u786e\uff0c\u9002\u4e8e\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u519c\u4e1a\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7b97\u529b\u548c\u5185\u5b58\u6709\u9650\uff0c\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u8f83\u5927\u6311\u6218\u3002\u73b0\u6709\u9ad8\u6027\u80fd\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\uff0c\u9700\u8981\u9488\u5bf9\u8bbe\u5907\u80fd\u529b\u8fdb\u884c\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u8bc1\u68c0\u6d4b\u7cbe\u5ea6\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u65b9\u6cd5\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u91cf\uff0c\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5229\u7528NVIDIA TensorRT\u5bf9Jetson Orin Nano\u8fdb\u884c\u90e8\u7f72\u548c\u63a8\u7406\u52a0\u901f\u3002\u9488\u5bf9\u6a21\u578b\u7ed3\u6784\u4e2d\u5b58\u5728\u7684\u6b8b\u5dee\u8fde\u63a5\u3001\u6ce8\u610f\u529b\u673a\u5236\u548cCSP\u5757\u7b49\u590d\u6742\u7ec4\u4ef6\uff0c\u8bbe\u8ba1\u76f8\u5e94\u7684\u526a\u679d\u548c\u4f18\u5316\u6280\u5de7\u3002", "result": "\u4f18\u5316\u540e\u7684EcoWeedNet\u6a21\u578b\u53c2\u6570\u91cf\u51cf\u5c1168.5%\uff0c\u8ba1\u7b97\u91cf\u964d\u4f4e3.2 GFLOPs\uff0c\u63a8\u65ad\u901f\u5ea6\u5728FP16\u7cbe\u5ea6\u4e0b\u8fbe\u5230184 FPS\uff0c\u8f83\u539f\u59cb\u57fa\u7ebf\u5feb28.7%\u3002\u5728CottonWeedDet12\u6570\u636e\u96c6\u4e0a\uff0c39.5%\u526a\u679d\u7387\u4e0b\u7684EcoWeedNet\u53d6\u5f9783.7% precision\u300177.5% recall\u548c85.9% mAP50\uff0c\u6307\u6807\u5747\u4f18\u4e8eYOLO11n\u548cYOLO12n\uff08\u5b83\u4eec\u53ea\u670920%\u526a\u679d\uff09\u3002", "conclusion": "\u538b\u7f29\u4e0e\u52a0\u901f\u540e\u7684EcoWeedNet\u6a21\u578b\u4e0d\u4ec5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\uff0c\u8fd8\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7cbe\u51c6\u519c\u4e1a\u9886\u57df\u7684\u4f18\u8d8a\u6027\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5SINGER\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\uff0c\u4ec5\u4f9d\u9760\u673a\u8f7d\u4f20\u611f\u4e0e\u8ba1\u7b97\uff0c\u7a81\u7834\u4e86\u5f00\u653e\u8bcd\u6c47\u65e0\u4eba\u673a\u5bfc\u822a\u9886\u57df\u7684\u74f6\u9888\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u673a\u68b0\u81c2\u9886\u57df\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u65e0\u4eba\u673a\u5bfc\u822a\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u3001\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u53ca\u7f3a\u4e4f\u5916\u90e8\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u4ecd\u672a\u53d6\u5f97\u7a81\u7834\u3002\u8fd9\u4e9f\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u65e0\u4eba\u673a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u590d\u6742\u81ea\u4e3b\u5bfc\u822a\u7684\u6311\u6218\u3002", "method": "SINGER\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u6210\uff1a1\uff09\u91c7\u7528\u9ad8\u4eff\u771f\u7684\u8bed\u8a00\u5d4c\u5165\u98de\u884c\u4eff\u771f\u5668\uff0c\u5229\u7528Gaussian Splatting\u9ad8\u6548\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff1b2\uff09\u57fa\u4e8e\u5feb\u901f\u968f\u673a\u6811\uff08RRT\uff09\u601d\u60f3\u7684\u591a\u8f68\u8ff9\u4e13\u5bb6\uff0c\u81ea\u52a8\u751f\u6210\u65e0\u78b0\u649e\u5bfc\u822a\u6f14\u793a\uff1b3\uff09\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u89c6\u89c9-\u52a8\u4f5c\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\u3002", "result": "\u5728700k-100\u4e07\u5bf9\u4ee5\u8bed\u8a00\u4e3a\u6761\u4ef6\u7684\u89c6\u89c9-\u52a8\u4f5c\u6570\u636e\u4e0a\u8bad\u7ec3SINGER\uff0c\u5e76\u90e8\u7f72\u5230\u771f\u5b9e\u65e0\u4eba\u673a\u786c\u4ef6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSINGER\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u4e0e\u65b0\u76ee\u6807\uff0c\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u7387\u9ad8\u51fa\u8bed\u4e49\u5f15\u5bfc\u57fa\u7ebf23.33%\uff0c\u76ee\u6807\u4fdd\u6301\u5728\u89c6\u91ce\u4e2d\u7684\u65f6\u95f4\u591a16.67%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c1110%\u3002", "conclusion": "SINGER\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u8bed\u8a00\u5f15\u5bfc\u81ea\u4e3b\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u6a21\u5757\uff0c\u4ec5\u7528\u673a\u8f7d\u4f20\u611f\u5668\uff0c\u5728\u73b0\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b89\u5168\u6027\u3002\u4e3a\u65e0\u4eba\u673a\u7684\u5f00\u653e\u8bcd\u6c47\u5bfc\u822a\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18458", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CogniLoad\uff0c\u4e00\u4e2a\u4ee5\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u4e3a\u57fa\u7840\u3001\u53ef\u8c03\u53c2\u6570\u7684\u5408\u6210\u57fa\u51c6\uff0c\u7528\u4e8e\u7cbe\u786e\u8bca\u65ad\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8bc4\u6d4b\u5f80\u5f80\u5c06\u4efb\u52a1\u672c\u8eab\u590d\u6742\u5ea6\u3001\u5e72\u6270\u9879\u6bd4\u4f8b\u53ca\u4efb\u52a1\u957f\u5ea6\u7b49\u5173\u952e\u56e0\u7d20\u6df7\u6dc6\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u5927\u6a21\u578b\u5728\u63a8\u7406\u4e0a\u7684\u5b9e\u9645\u8584\u5f31\u73af\u8282\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u7cbe\u7ec6\u53ef\u63a7\u7684\u57fa\u51c6\u63a8\u8fdb\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u6df1\u5ea6\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86CogniLoad\uff0c\u53ef\u6309\u7167\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u4e09\u5927\u7ef4\u5ea6\uff08\u5185\u5728\u96be\u5ea6\u3001\u5e72\u6270\u9879\u6bd4\u4f8b\u3001\u4efb\u52a1\u957f\u5ea6\uff09\u72ec\u7acb\u8c03\u53c2\uff0c\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u9898\u76ee\uff0c\u7cfb\u7edf\u8003\u5bdf\u6a21\u578b\u5728\u4e0d\u540c\u8d1f\u8377\u7ec4\u5408\u4e0b\u7684\u63a8\u7406\u8868\u73b0\u3002\u901a\u8fc7\u6b64\u5de5\u5177\uff0c\u8bc4\u6d4b\u4e8622\u4e2a\u5f53\u4e0b\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\uff1a\u4efb\u52a1\u957f\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u63a8\u7406\u8868\u73b0\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u5bb9\u5fcd\u5ea6\u4e0d\u540c\uff0c\u5e72\u6270\u9879\u6bd4\u4f8b\u7684\u5f71\u54cd\u5448\u73b0U\u578b\u53cd\u5e94\u66f2\u7ebf\u3002CogniLoad\u5e2e\u52a9\u91cf\u5316\u548c\u533a\u5206\u5404\u7c7b\u53c2\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\u3002", "conclusion": "CogniLoad\u5b9e\u73b0\u4e86\u8ba4\u77e5\u8d1f\u8377\u7ef4\u5ea6\u7684\u7cfb\u7edf\u53ef\u63a7\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8bca\u65ad\u3001\u5206\u6790\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u590d\u73b0\u6027\u5f3a\u3001\u53ef\u6269\u5c55\u3001\u5206\u6790\u4e30\u5bcc\u7684\u65b0\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u8bbe\u8ba1\u4e0e\u63d0\u5347\u3002"}}
{"id": "2509.18284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u6a21\u6001\u4e22\u5f03\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u5c24\u5176\u5728\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u5df2\u5728\u5927\u578b\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u6548\u679c\u3002", "motivation": "\u5b9e\u9645\u533b\u5b66\u8bca\u65ad\u4e2d\uff0c\u6570\u636e\u5e38\u5b58\u5728\u591a\u6a21\u6001\uff08\u5982\u56fe\u50cf\u548c\u8868\u683c\uff09\uff0c\u4f46\u6a21\u578b\u9700\u5e94\u5bf9\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u4e22\u5931\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u67d0\u4e9b\u6a21\u6001\u7f3a\u5931\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u63d0\u51fa\u65e2\u80fd\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u53c8\u80fd\u9002\u5e94\u6a21\u6001\u7f3a\u5931\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u5e26\u6709\u53ef\u5b66\u4e60\u6a21\u6001token\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u6a21\u6001\u7f3a\u5931\u7684\u5bb9\u9519\u6027\u548c\u878d\u5408\u80fd\u529b\uff1b2. \u7ed3\u5408\u589e\u5f3a\u7684\u6a21\u6001\u4e22\u5f03\u548c\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u65e2\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u5355\u4e00\u6a21\u6001\uff0c\u4e5f\u5b66\u4e60\u591a\u6a21\u6001\u878d\u5408\u8868\u793a\u30023. \u65b9\u6cd5\u9002\u914d\u89c6\u89c9\uff08\u5982CT\u56fe\u50cf\uff09\u4e0e\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\uff0c\u5e76\u9002\u914d\u4e3b\u6d41\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u75be\u75c5\u68c0\u6d4b\u548c\u9884\u6d4b\u7684\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u53ea\u63d0\u4f9b\u5355\u4e00\u6a21\u6001\u7684\u56f0\u96be\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff1b\u540c\u65f6\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9002\u914d\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u517c\u5177\u9ad8\u6548\u6027\u3001\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u4e3a\u73b0\u5b9e\u533b\u7597\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4f4e\u6210\u672c\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18626", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "\u672c\u6587\u5c06\u771f\u5b9e\u4e8b\u6545\u62a5\u544a\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u7684\u573a\u666f-\u52a8\u4f5c\u8868\u793a\uff0c\u5e76\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u68c0\u7d22\u548c\u5bf9\u6bd4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u8fb9\u754c\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u5b66\u4e60\u7cfb\u7edf\u5927\u591a\u57fa\u4e8e\u6ca1\u6709\u4e8b\u6545\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u7f3a\u5c11\u5173\u4e8e\u6781\u9650\u98ce\u9669\u573a\u666f\u7684\u6709\u6548\u6307\u5bfc\u3002\u73b0\u5b9e\u4e2d\u7684\u6c7d\u8f66\u4e8b\u6545\u62a5\u544a\u6b63\u597d\u80fd\u63d0\u4f9b\u6709\u5173\u5b89\u5168\u8fb9\u754c\u7684\u5bf9\u6bd4\u8bc1\u636e\uff0c\u4f46\u6587\u672c\u672c\u8eab\u6742\u4e71\u65e0\u7ae0\u3001\u4e0d\u6613\u4e0e\u4f20\u611f\u5668\u6570\u636e\u5173\u8054\uff0c\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u4f18\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u4e8b\u6545\u53d9\u8ff0\u6807\u51c6\u5316\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u7684\u8bed\u8a00\uff0c\u5e76\u5c06\u6b63\u5e38\u9a7e\u9a76\u65e5\u5fd7\u4e0e\u4e8b\u6545\u6848\u4f8b\u7edf\u4e00\u8f6c\u4e3a\u201c\u573a\u666f-\u52a8\u4f5c\u201d\u8868\u793a\u5f62\u5f0f\u4ee5\u4fbf\u68c0\u7d22\u3002\u51b3\u7b56\u65f6\uff0c\u7cfb\u7edf\u4ece\u7edf\u4e00\u573a\u666f\u7d22\u5f15\u4e2d\u68c0\u7d22\u76f8\u5173\u6848\u4f8b\u4f5c\u4e3a\u5148\u4f8b\uff0c\u8fd8\u53ef\u901a\u8fc7\u201c\u80fd\u52a8\u53cd\u4e8b\u5b9e\u201d\u63d0\u51fa\u53ef\u884c\u65b9\u6848\u5e76\u68c0\u7d22\u5404\u81ea\u540e\u679c\uff0c\u518d\u5bf9\u6bd4\u7efc\u5408\u8bc4\u4f30\u540e\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728nuScenes\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684\u68c0\u7d22\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u60c5\u666f\u4f18\u9009\u52a8\u4f5c\u7684\u53ec\u56de\u7387\uff0c\u4ece24%\u63d0\u9ad8\u523053%\uff1b\u80fd\u52a8\u53cd\u4e8b\u5b9e\u6269\u5c55\u5219\u5728\u4fdd\u7559\u4e0a\u8ff0\u63d0\u5347\u7684\u524d\u63d0\u4e0b\uff0c\u4f7f\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669\u8fb9\u7f18\u7684\u51b3\u7b56\u66f4\u52a0\u53ef\u9760\u548c\u6e05\u6670\u3002", "conclusion": "\u5c06\u771f\u5b9e\u4e8b\u6545\u4f5c\u4e3a\u68c0\u7d22\u5148\u4f8b\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u5728\u98ce\u9669\u573a\u666f\u4e0b\u7684\u6821\u51c6\u4e0e\u51b3\u7b56\u8868\u73b0\uff0c\u4e3a\u5b89\u5168\u654f\u611f\u7cfb\u7edf\u5f15\u5165\u4e86\u66f4\u5177\u5bf9\u6bd4\u6027\u548c\u6307\u793a\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u3002"}}
{"id": "2509.18467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5c06\u9884\u8bad\u7ec3transformer\u80fd\u529b\u8fc1\u79fb\u5230\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u7684\u65b0\u65b9\u6cd5LAWCAT\uff0c\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u4efb\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "transformer\u867d\u7136\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u5176\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\uff0c\u9650\u5236\u4e86\u957f\u5e8f\u5217\u548c\u4f4e\u5ef6\u8fdf\u573a\u666f\u7684\u5e94\u7528\uff1b\u800c\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u590d\u6742\u5ea6\u4f4e\uff0c\u4f46\u4ece\u5934\u8bad\u7ec3\u4ecd\u9700\u5927\u91cf\u8d44\u6e90\u3002\u4f5c\u8005\u5e0c\u671b\u7a81\u7834\u8fd9\u4e24\u4e2a\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86LAWCAT\u6846\u67b6\uff1a\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u52a0\u8de8\u65f6\u95f4\u5377\u79ef(Conv1D)\u589e\u5f3a\u5c40\u90e8\u4f9d\u8d56\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u5f52\u4e00\u5316\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u4ee5\u5f3a\u5316\u4e0d\u540c\u957f\u5ea6\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3transformer\u77e5\u8bc6\u84b8\u998f\u5230\u8be5\u7ebf\u6027\u6a21\u578b\uff0c\u63d0\u5347\u8868\u73b0\u3002", "result": "\u5728\u4f7f\u7528Mistral-7B\u3001Llama3.2-1B\u7b49\u84b8\u998f\u5b9e\u9a8c\u4e0a\uff0cLAWCAT\u7528\u6781\u5c11\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fd1\u4f3c\u539f\u6a21\u578b\uff0c\u5e76\u663e\u8457\u62d3\u5c55\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u540c\u65f6\u5728\u957f\u5e8f\u5217\u63a8\u7406\u901f\u5ea6\u8d85\u8fc7\u4e86\u5f53\u524d\u9ad8\u6548\u7b97\u6cd5\uff08\u5982FlashAttention-2\uff09\u3002", "conclusion": "LAWCAT\u4e3a\u5c06\u9ad8\u6027\u80fdtransformer\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u7ebf\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u957f\u5e8f\u5217\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\uff0c\u5177\u5907\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e869\u79cd\u5e38\u7528\u5206\u5272\u67b6\u6784\u7528\u4e8e\u80ba\u52a8\u8109\u6813\u585e\uff08PE\uff09\u7684CTPA\u5f71\u50cf\u5206\u5272\uff0c\u5728\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\uff0c\u53d1\u73b03D U-Net\uff08ResNet\u7f16\u7801\u5668\uff09\u4ecd\u4e3aPE\u5206\u5272\u7684\u6709\u6548\u65b9\u6848\uff0cCNN\u6574\u4f53\u4f18\u4e8eViT\uff0c\u540c\u65f6\u5206\u7c7b\u9884\u8bad\u7ec3\u4e0d\u5229\u4e8e\u5206\u5272\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u81ea\u52a8\u5206\u5272\u65b9\u6cd5\u8f83\u591a\uff0c\u4f46\u9488\u5bf9PE\u5206\u5272\u7684\u67b6\u6784\u9009\u62e9\u3001\u9884\u8bad\u7ec3\u65b9\u5f0f\u53ca3D/2D\u6a21\u578b\u8868\u73b0\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5bf9\u6bd4\uff0c\u5c24\u5176\u5728\u9ad8\u8d28\u91cf\u3001\u5bc6\u96c6\u6807\u6ce8\u4e0b\u3002\u7814\u7a76\u65e8\u5728\u660e\u786e\u6700\u4f18\u67b6\u6784\u5e76\u7406\u89e3\u4efb\u52a1\u96be\u70b9\u3002", "method": "\u6574\u7406\u5e76\u7cbe\u7ec6\u6807\u6ce8\u4e86490\u4f8bCTPA\u5f71\u50cf\uff0c\u7cfb\u7edf\u6027\u8bc4\u6d4bCNN\u548cViT\u4e24\u5927\u7c7b\u51719\u79cd\u5206\u5272\u67b6\u6784\uff0c\u5e76\u6bd4\u8f83\u9884\u8bad\u7ec3\u4e0e\u968f\u673a\u521d\u59cb\u5316\u6548\u679c\uff0c\u5728\u7edf\u4e00\u6d4b\u8bd5\u6846\u67b6\u4e0b\u8fdb\u884c\u6027\u80fd\u5ba1\u8ba1\u3002", "result": "3D U-Net (ResNet\u7f16\u7801\u5668)\u8868\u73b0\u6700\u4f18\uff0c3D\u6a21\u578b\u66f4\u9002\u5408 emboli \u5206\u5272\uff0cCNN\u4f18\u4e8eViT\uff0c\u5206\u7c7b\u9884\u8bad\u7ec3\u4f1a\u964d\u4f4e\u5206\u5272\u6027\u80fd\uff0c\u4e0d\u540c\u67b6\u6784\u5728\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e2d\u592e\u6216\u5927\u6813\u5b50\u5206\u5272\u7cbe\u5ea6\u9ad8\uff0c\u8fdc\u7aef\u6813\u5b50\u5206\u5272\u4ecd\u5177\u6311\u6218\u3002\u6700\u4f73\u6a21\u578bDice\u5206\u65700.7131\u3002", "conclusion": "3D CNN\u6a21\u578b\uff08\u5c24\u5176\u662f3D U-Net\uff09\u5728PE\u5206\u5272\u4efb\u52a1\u4e2d\u4ecd\u662f\u4f18\u9009\uff0c\u5206\u7c7b\u4efb\u52a1\u9884\u8bad\u7ec3\u53ef\u80fd\u4e0d\u5229\u4e8e\u5206\u5272\uff0c\u672a\u6765\u63d0\u5347\u8fdc\u7aefPE\u5206\u5272\u9700\u66f4\u9ad8\u8d28\u6570\u636e\u548c\u9488\u5bf9\u6027\u7814\u7a76\u3002"}}
{"id": "2509.18631", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f7f\u7528\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u7684\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u5b9e\u9645\u64cd\u4f5c\u6f14\u793a\u9700\u6c42\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5373\u53ef\u83b7\u5f97\u6cdb\u5316\u6027\u5f3a\u7684\u673a\u68b0\u81c2\u64cd\u4f5c\u7b56\u7565\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u6536\u96c6\u64cd\u63a7\u673a\u5668\u4eba\u7684\u6f14\u793a\u4ee3\u4ef7\u9ad8\u6602\uff0c\u7eaf\u4f9d\u9760\u4eff\u771f\u6570\u636e\u8bad\u7ec3\u4f1a\u53d7\u5230\u6a21\u62df\u4e0e\u771f\u5b9e\u9886\u57df\u5dee\u5f02\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u4e86\u517c\u987e\u8bad\u7ec3\u6548\u7387\u4e0e\u8868\u73b0\u6cdb\u5316\u6027\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u878d\u5408\u4eff\u771f\u548c\u73b0\u5b9e\u6f14\u793a\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdsim-and-real\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u91cd\u70b9\u662f\u5728\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u901a\u8fc7\u9886\u57df\u4e0d\u53d8\u4e14\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u5efa\u6a21\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u635f\u5931\u5bf9\u89c2\u6d4b-\u52a8\u4f5c\u8054\u5408\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\uff0c\u76f8\u6bd4\u5355\u72ec\u5bf9\u9f50\u89c2\u6d4b\u5206\u5e03\u4fe1\u53f7\u66f4\u4e30\u5bcc\u3002\u4e3a\u5e94\u5bf9\u4eff\u771f\u6570\u636e\u591a\u3001\u771f\u5b9e\u6570\u636e\u5c11\u7684\u95ee\u9898\uff0c\u8fd8\u5f15\u5165\u4e86\u4e0d\u5e73\u8861OT\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6311\u6218\u6027\u7684\u64cd\u4f5c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u80fd\u5145\u5206\u5229\u7528\u4eff\u771f\u6d77\u91cf\u6570\u636e\uff0c\u5c06\u73b0\u5b9e\u573a\u666f\u6210\u529f\u7387\u63d0\u5347\u4e86\u6700\u9ad830%\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4ec5\u89c1\u8fc7\u4eff\u771f\u4e14\u672a\u89c1\u8fc7\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5bf9\u9f50\u8de8\u57df\u7684\u89c2\u6d4b\u548c\u52a8\u4f5c\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u5229\u7528\u5927\u89c4\u6a21\u4eff\u771f\u63d0\u5347\u73b0\u5b9e\u8868\u73b0\u3001\u6781\u5927\u51cf\u5c0f\u5bf9\u771f\u5b9e\u6f14\u793a\u7684\u4f9d\u8d56\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u578b\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\u3001\u6a21\u578b\u89c4\u6a21\u4ee5\u53ca\u56fe\u7ed3\u6784\u7b49\u591a\u65b9\u9762\u56e0\u7d20\uff0c\u603b\u7ed3\u5176\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u5b9e\u9645\u5e94\u7528\u548c\u65b0\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6d89\u53ca\u6587\u672c\u548c\u56fe\u6570\u636e\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9LLMs\u4e0e\u56fe\u6570\u636e\u4ea4\u4e92\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e2e\u52a9\u4e1a\u754c\u548c\u5b66\u754c\u660e\u786e\u4e0d\u540cLLM-\u56fe\u4ea4\u4e92\u6a21\u5f0f\u7684\u9002\u7528\u6027\u53ca\u4f18\u5316\u65b9\u5411\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5168\u9762\u8986\u76d6\u591a\u79cd\u53d8\u91cf\uff0c\u5305\u62ecLLM-\u56fe\u6570\u636e\u4ea4\u4e92\u65b9\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u3001\u4ee3\u7801\u751f\u6210\uff09\u3001\u6570\u636e\u96c6\u9886\u57df\uff08\u5f15\u7528\u3001\u7f51\u7edc\u3001\u7535\u5546\u3001\u793e\u4ea4\uff09\u3001\u56fe\u7ed3\u6784\uff08\u540c\u8d28\u4e0e\u5f02\u8d28\uff09\u3001\u8282\u70b9\u7279\u5f81\u957f\u5ea6\uff0c\u4ee5\u53ca\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u3002\u5e76\u901a\u8fc7\u7279\u5f81\u622a\u65ad\u3001\u5220\u9664\u8fb9\u548c\u6807\u7b7e\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u4e0d\u540c\u8f93\u5165\u7684\u4f9d\u8d56\u6027\u3002", "result": "\uff081\uff09\u4ee5\u4ee3\u7801\u751f\u6210\u65b9\u5f0f\u4f7f\u7528LLM\u80fd\u5728\u56fe\u6570\u636e\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f73\u6574\u4f53\u8868\u73b0\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u6216\u9ad8\u8fde\u63a5\u5ea6\u56fe\u4e2d\u4f18\u52bf\u660e\u663e\u3002\uff082\uff09\u6240\u6709\u4ea4\u4e92\u7b56\u7565\u5728\u5f02\u8d28\u56fe\u4e2d\u4f9d\u65e7\u6709\u6548\uff0c\u6253\u7834\u4e86\u4f4e\u540c\u8d28\u6027\u4e0bLLM\u65b9\u6cd5\u5931\u6548\u7684\u5047\u8bbe\u3002\uff083\uff09\u4ee3\u7801\u751f\u6210\u65b9\u5f0f\u80fd\u591f\u7075\u6d3b\u8c03\u6574\u5bf9\u56fe\u7ed3\u6784\u3001\u8282\u70b9\u7279\u5f81\u6216\u6807\u7b7e\u7684\u4f9d\u8d56\uff0c\u4ee5\u5229\u7528\u6700\u6709\u4fe1\u606f\u91cf\u7684\u8f93\u5165\u3002", "conclusion": "\u8be5\u6587\u4e3a\u5f53\u524dLLM-\u56fe\u6570\u636e\u4ea4\u4e92\u6a21\u5f0f\u7684\u4f18\u7f3a\u70b9\u63d0\u4f9b\u4e86\u5168\u666f\u5f0f\u5256\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u884c\u6027\u5f3a\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5bf9\u4eca\u540e\u7684\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u5f88\u9ad8\u7684\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2509.18309", "categories": ["cs.CV", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u663e\u5f0f\u5efa\u6a21\u548c\u8bc6\u522b\u7f8e\u5f0f\u624b\u8bed\u4e2d\u624b\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u578b\u8bc6\u522b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u5f88\u5c11\u5bf9\u624b\u578b\u8fdb\u884c\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u8bc6\u522b\u7cbe\u5ea6\u6709\u9650\uff0c\u4e5f\u5f71\u54cd\u5bf9\u624b\u8bed\u7684\u8bed\u8a00\u5b66\u5206\u6790\u3002\u624b\u578b\u4f5c\u4e3a\u624b\u8bed\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u97f3\u4f4d\u5355\u5143\uff0c\u5bf9\u624b\u8bed\u8bc6\u522b\u548c\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u624b\u578b\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u5c06\u624b\u578b\u7684\u65f6\u5e8f\u52a8\u6001\u4e0e\u9759\u6001\u914d\u578b\u5206\u79bb\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u57fa\u4e8e\u89e3\u5256\u7ed3\u6784\u7684\u56fe\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u624b\u578b\u8bc6\u522b\u4e2d\u7c7b\u522b\u95f4\u7ec6\u5fae\u533a\u522b\u548c\u65f6\u5e8f\u53d8\u5316\u7b49\u96be\u9898\u3002", "result": "\u4f5c\u8005\u5efa\u7acb\u4e86\u9996\u4e2a\u7ed3\u6784\u5316\u624b\u578b\u8bc6\u522b\u57fa\u51c6\u4efb\u52a1\uff0c\u572837\u5206\u7c7b\u4e0a\u53d6\u5f97\u4e8646%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u4e8625%\u51c6\u786e\u7387\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u548c\u533a\u5206\u7f8e\u5f0f\u624b\u8bed\u4e2d\u7684\u5404\u7c7b\u624b\u578b\uff0c\u63a8\u52a8\u4e86\u624b\u578b\u81ea\u52a8\u8bc6\u522b\u548c\u624b\u8bed\u8bed\u8a00\u5b66\u5206\u6790\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u72ed\u7a84\u73af\u5883\u4e0b\u3001\u6570\u91cf\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u3002\u6838\u5fc3\u901a\u8fc7\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u4e0ePAAS\uff08Lloyd\u5747\u5300\u5212\u5206\u4e0e\u5308\u7259\u5229\u5206\u914d\uff09\u7b97\u6cd5\u5b9e\u73b0\u5b89\u5168\u95f4\u9694\u4e0e\u961f\u5f62\u5b8c\u6574\u6027\uff0c\u518d\u7ed3\u5408\u8f68\u8ff9\u751f\u6210\u4e0e\u4f18\u5316\uff0c\u5b9e\u73b0\u7f16\u961f\u6062\u590d\u4e0e\u73af\u5883\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u590d\u6742\u72ed\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u7f16\u961f\u7ecf\u5e38\u9700\u8981\u7075\u6d3b\u53d8\u5f62\u5e76\u5e94\u5bf9\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u961f\u5f62\u5feb\u901f\u6062\u590d\u4e0e\u73af\u5883\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u7f16\u961f\u89c4\u5212\u7b56\u7565\u3002", "method": "\u91c7\u7528DVS\u4f5c\u4e3a\u7f16\u961f\u57fa\u7840\uff0c\u7ed3\u5408Lloyd\u7b97\u6cd5\u5b9e\u73b0\u7a7a\u95f4\u5747\u5300\u5212\u5206\uff0c\u5308\u7259\u5229\u7b97\u6cd5\u5b8c\u6210\u65e0\u4eba\u673a\u4e0e\u76ee\u6807\u70b9\u7684\u6700\u4f18\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u539f\u59cb\u8f68\u8ff9\u641c\u7d22\u4e0e\u975e\u7ebf\u6027\u4f18\u5316\u751f\u6210\u6ee1\u8db3\u6570\u91cf\u53d8\u5316\u548c\u73af\u5883\u7ea6\u675f\u7684DVS\u65f6\u7a7a\u8f68\u8ff9\uff0c\u6bcf\u67b6\u65e0\u4eba\u673a\u4f9d\u636e\u5206\u914d\u81ea\u4e3b\u89c4\u5212\u8f68\u8ff9\uff0c\u878d\u5408\u907f\u969c\u548c\u52a8\u529b\u5b66\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u961f\u5f62\u6570\u91cf\u53ef\u52a8\u6001\u589e\u51cf15%\u4e14\u80fd\u5feb\u901f\u6062\u590d\u961f\u5f62\uff0c\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u590d\u73b0\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5b9e\u9645\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7f16\u961f\u6062\u590d\u901f\u5ea6\u4e0e\u9002\u5e94\u80fd\u529b\uff0c\u5bf9\u52a8\u6001\u7f16\u961f\u548c\u5b9e\u9645\u90e8\u7f72\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2509.18514", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528ByT5\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bd7\u6b4c\u4e2d\u63d2\u5165\u7b26\u5408\u7279\u5b9a\u97f5\u5f8b\u7684\u77ed\u8bed\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u8bad\u7ec3\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u97f5\u5f8b\u5bf9\u9f50\u4e14\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bd7\u6b4c\u5bf9\u97f5\u5f8b\u6709\u4e25\u683c\u8981\u6c42\uff0c\u624b\u52a8\u521b\u4f5c\u7e41\u7410\u4e14\u6280\u672f\u95e8\u69db\u9ad8\uff0c\u56e0\u6b64\u5e0c\u671b\u501f\u52a9AI\u534f\u52a9\u63d0\u5347\u521b\u4f5c\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u4f7f\u7528ByT5\u5b57\u8282\u7ea7\u591a\u8bed\u8a00Transformer\uff0c\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u5b57\u7d20\u5230\u97f3\u6b65\u8f6c\u5316\uff0c\u63d0\u53d6\u5b8c\u6574\u6807\u6ce8\u7684\u963f\u62c9\u4f2f\u6587\u97f5\u5f8b\u3002\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u76ee\u6807\u5bf9\u6a21\u578b\u5fae\u8c03\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5148\u7528\u901a\u7528\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u518d\u7528\u8bd7\u6b4c\u6587\u672c\u5fae\u8c03\uff0c\u5e76\u63a2\u7d22\u82f1\u5230\u963f\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u80fd\u5b9e\u73b0\u9ad8\u5ea6\u7684\u97f5\u5f8b\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u8bd7\u53e5\u8bed\u4e49\u4e0a\u7684\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u521b\u4f5c\u963f\u62c9\u4f2f\u8bd7\u6b4c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u53e4\u5178\u8bd7\u6b4c\u534f\u540c\u521b\u4f5c\u7b49\u9886\u57df\u3002"}}
{"id": "2509.18326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18326", "abs": "https://arxiv.org/abs/2509.18326", "authors": ["Chun Kit Wong", "Anders N. Christensen", "Cosmin I. Bercea", "Julia A. Schnabel", "Martin G. Tolsgaard", "Aasa Feragen"], "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "comment": "MICCAI 2025", "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u4e2d\uff0c\u4efb\u52a1\u9009\u62e9\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bOOD\uff08\u5206\u5e03\u5916\u6837\u672c\uff09\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff08\u5982\u80ce\u513f\u8d85\u58f0\uff09\u4e2d\u90e8\u7f72\u65f6\u9700\u4fdd\u8bc1\u80fd\u53ef\u9760\u68c0\u6d4b\u51fa\u5206\u5e03\u5916\u6837\u672c\uff08OOD\uff09\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u56fe\u50cf\u7279\u5f81\u4e0e\u4e34\u5e8a\u73af\u5883\u9ad8\u5ea6\u5f02\u8d28\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u672c\u6587\u5c1d\u8bd5\u63a2\u7a76\u5206\u7c7b\u4efb\u52a1\u672c\u8eab\u5bf9OOD\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e864\u4e2a\u4e0d\u540c\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u914d\u54088\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u4e0d\u540c\u4efb\u52a1\u4e0e\u65b9\u6cd5\u4e0b\u7684OOD\u68c0\u6d4b\u8868\u73b0\u3002\u8fd8\u8003\u5bdf\u4e86\u4e24\u7c7bOOD\u6807\u51c6\uff1a\u7531\u56fe\u50cf\u7279\u5f81\u6f02\u79fb\u6216\u89e3\u5256\u7ed3\u6784\u6f02\u79fb\u5bfc\u81f4\u7684OOD\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cOOD\u68c0\u6d4b\u7684\u8868\u73b0\u5bf9\u4e8e\u6240\u9009\u7684\u5206\u7c7b\u4efb\u52a1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4e0d\u540c\u7c7b\u578b\u7684ID-OOD\u5212\u5206\u4e0b\uff0c\u6700\u4f18\u4efb\u52a1\u5e76\u4e0d\u76f8\u540c\u3002\u6b64\u5916\uff0cOOD\u68c0\u6d4b\u6027\u80fd\u597d\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5f03\u6743\u9884\u6d4b\uff08\u62d2\u7edd\u4e0d\u786e\u5b9a\u6837\u672c\uff09\u6700\u4f18\u3002", "conclusion": "\u9700\u4f9d\u636e\u5177\u4f53\u4e34\u5e8a\u5e94\u7528\u573a\u666f\uff0c\u5408\u7406\u9009\u62e9\u5206\u7c7b\u4efb\u52a1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4ece\u800c\u534f\u8c03OOD\u68c0\u6d4b\u4e0e\u5f03\u6743\u9884\u6d4b\u7684\u76ee\u6807\uff0c\u63d0\u5347\u533b\u5b66\u5f71\u50cfAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18644", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f7f\u7528\u672c\u4f53\u72b6\u6001\uff08\u5982\u5173\u8282\u4f4d\u7f6e\u7b49\uff09\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff08State-free Policy\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u89c6\u89c9\u2014\u52a8\u529b\u5b66\u7b56\u7565\u5e38\u540c\u65f6\u4f9d\u8d56\u89c6\u89c9\u4e0e\u672c\u4f53\u611f\u77e5\u8f93\u5165\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u72b6\u6001\uff0c\u8bad\u7ec3\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5bfb\u6c42\u514b\u670d\u8fd9\u4e00\u5c40\u9650\uff0c\u63d0\u5347\u7b56\u7565\u7684\u6cdb\u5316\u4e0e\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86State-free Policy\uff0c\u4ec5\u4f9d\u8d56\u6765\u81ea\u53cc\u8155\u5e7f\u89d2\u6444\u50cf\u5934\u7684\u89c6\u89c9\u89c2\u6d4b\uff0c\u4e0d\u518d\u4f7f\u7528\u672c\u4f53\u72b6\u6001\u8f93\u5165\u3002\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u4e3a\u76f8\u5bf9\u672b\u7aef\u6267\u884c\u5668\u7a7a\u95f4\uff0c\u53ea\u7528\u89c6\u89c9\u4fe1\u606f\u9884\u6d4b\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6293\u53d6\u3001\u53e0\u8863\u3001\u5168\u8eab\u590d\u6742\u64cd\u4f5c\u7b49\u591a\u4efb\u52a1\u9a8c\u8bc1\uff0cState-free Policy\u5728\u9ad8\u5ea6\u6cdb\u5316\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u523085%\uff0c\u5728\u6c34\u5e73\u65b9\u5411\u6cdb\u5316\u4efb\u52a1\u6210\u529f\u7387\u4ece6%\u63d0\u5347\u523064%\u3002\u6b64\u5916\uff0c\u8be5\u7b56\u7565\u5728\u6570\u636e\u6548\u7387\u548c\u8de8\u673a\u5668\u4eba\u5f62\u6001\u9002\u5e94\u4e0a\u4e5f\u6709\u63d0\u5347\u3002", "conclusion": "\u79fb\u9664\u672c\u4f53\u72b6\u6001\uff0c\u4ec5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u89c6\u89c9\u8f93\u5165\u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u5176\u5b9e\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.18535", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u9c81\u68d2\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u5305\u62ec\u539f\u59cb\u6587\u672c\u548c\u7ecf\u540c\u4e49\u6539\u5199\uff08PSP\uff09\u540e\u7684\u6587\u672c\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8eChatGPT\u7b49AI\u6587\u672c\u751f\u6210\u5de5\u5177\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bfc\u81f4\u6ee5\u7528\u95ee\u9898\u9891\u53d1\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5bf9\u540c\u4e49\u6539\u5199\u4e0d\u9c81\u68d2\uff0c\u5b58\u5728\u6a21\u578b\u504f\u89c1\u3001\u5bf9\u53d8\u5f02\u6587\u672c\u51c6\u786e\u7387\u4e0b\u964d\u3001\u5bf9\u786c\u4ef6\u8d44\u6e90\u6709\u8f83\u9ad8\u9700\u6c42\u7b49\u7f3a\u9677\uff0c\u56e0\u6b64\u4e9f\u9700\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5185\u90e8\u7ed3\u6784\u800c\u975e\u5355\u4e2a\u8bcd\u5c42\u9762\u7684\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u63d0\u53d6\u7ecf\u8fc7\u5927\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7684\u53e5\u5b50\u5d4c\u5165\uff0c\u518d\u901a\u8fc7attention\u673a\u5236\u5efa\u6a21\u53e5\u5b50\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u4e3a\u51cf\u5c11\u751f\u6210\u6a21\u578b\u5e26\u6765\u7684\u5d4c\u5165\u504f\u5dee\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u56fe\u4e0e\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u5265\u79bb\u5185\u5bb9\u504f\u89c1\uff0c\u4ec5\u6355\u6349\u7ed3\u6784\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u4e13\u95e8\u6784\u5efa\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u8bba\u6587\u6458\u8981\u6bd4\u8f83\u4e0eFAQ\u95ee\u7b54\u6539\u5199\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4bAI\u751f\u6210\u4e0e\u540c\u4e49\u6539\u5199\u6587\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u7ed3\u6784\u5c42\u9762\u7684\u68c0\u6d4b\u6846\u67b6\u6bd4\u8bcd\u7ea7\u522b\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u88ab\u6539\u5199\u540e\u96be\u4ee5\u8fa8\u8ba4\u7684AI\u751f\u6210\u6587\u672c\uff0c\u5177\u6709\u8f83\u9ad8\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18350", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faOrthoLoC\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u65e0\u4eba\u673a\u822a\u62cd\u89c6\u89c9\u5b9a\u4f4d\uff0c\u9488\u5bf9\u4e0e\u6b63\u5c04\u5730\u7406\u6570\u636e\u7684\u914d\u51c6\u95ee\u9898\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u4e0e\u516c\u5f00\u8bc4\u6d4b\uff0c\u63d0\u51fa\u65b0\u7684\u7279\u5f81\u5339\u914d\u4f18\u5316\u65b9\u6cd5AdHoP\u3002", "motivation": "\u5f53\u524d\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u4f9d\u8d56\u5927\u578b\u56fe\u50cf\u6570\u636e\u5e93\u62163D\u6a21\u578b\uff0c\u53d7\u9650\u4e8e\u8d44\u6e90\u4e0e\u73af\u5883\uff08\u5982\u65e0GPS\uff09\uff0c\u800c\u6b63\u5c04\u5730\u7406\u6570\u636e\u8f7b\u91cf\u4e14\u65e5\u76ca\u5e73\u6c11\u5316\uff0c\u5374\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u4e3a\u89e3\u51b3\u6b64\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efa16,425\u5f20\u6765\u81ea\u5fb7\u56fd\u548c\u7f8e\u56fd\u7684\u65e0\u4eba\u673a\u591a\u6a21\u6001\u5f71\u50cf\u4e0e\u5176\u6b63\u5c04\u5730\u7406\u6570\u636e\u914d\u5bf9\u7684\u5927\u89c4\u6a21OrthoLoC\u6570\u636e\u96c6\u30022. \u9488\u5bf9\u65e0\u4eba\u673a\u5f71\u50cf\u4e0e\u5730\u7406\u6570\u636e\u95f4\u7684\u9886\u57df\u5dee\u5f02\u4e0e\u914d\u51c6\u95ee\u9898\uff0c\u8bbe\u8ba1\u652f\u6301\u72ec\u7acb\u8bc4\u6d4b\u7684\u914d\u5bf9\u7ed3\u6784\u30023. \u8bc4\u4f30\u9886\u57df\u5dee\u5f02\u3001\u5206\u8fa8\u7387\u3001\u5171\u89c6\u8303\u56f4\u5bf9\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u30024. \u63d0\u51fa\u7279\u5f81\u5339\u914d\u4f18\u5316\u6280\u672fAdHoP\uff0c\u53ef\u4e0e\u4efb\u610f\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u7ed3\u5408\uff0c\u63d0\u9ad8\u5339\u914d\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0cAdHoP\u80fd\u5c06\u7279\u5f81\u5339\u914d\u7684\u51c6\u786e\u6027\u63d0\u5347\u6700\u9ad895%\uff0c\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e\u6700\u9ad863%\u3002\u540c\u65f6\uff0c\u6570\u636e\u96c6\u4e3a\u5b66\u754c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "conclusion": "OrthoLoC\u6570\u636e\u96c6\u548cAdHoP\u65b9\u6cd5\u4e3a\u822a\u62cd\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u516c\u5f00\u7684\u6570\u636e\u652f\u6491\uff0c\u5e76\u63d0\u5347\u4e86\u5339\u914d\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u6709\u52a9\u4e8e\u62d3\u5c55\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u5e94\u7528\u3002"}}
{"id": "2509.18648", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 SPiDR\u2014\u2014\u4e00\u79cd\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5b89\u5168\u8fc1\u79fb\u7684\u53ef\u6269\u5c55\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u201c\u4eff\u771f\u5230\u73b0\u5b9e\u201d\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u6027\u80fd\u4f18\u826f\u3002", "motivation": "\u73b0\u5b9e\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\u65f6\uff0c\u5b89\u5168\u59cb\u7ec8\u662f\u4e3b\u8981\u6311\u6218\u3002\u867d\u7136\u4eff\u771f\u8bad\u7ec3\u5f88\u5b89\u5168\uff0c\u4f46\u201c\u4eff\u771f\u5230\u73b0\u5b9e\u201d\u7684\u5dee\u8ddd\u5bfc\u81f4\u73b0\u5b9e\u4e2d\u7684\u7b56\u7565\u53ef\u80fd\u4e0d\u5b89\u5168\uff0c\u73b0\u6709\u9c81\u68d2\u5b89\u5168\u65b9\u6cd5\u867d\u4e25\u8c28\u5374\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u517c\u5177\u5b89\u5168\u4e0e\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPiDR\u7b97\u6cd5\uff08Sim-to-real via Pessimistic Domain Randomization\uff09\uff0c\u5c06\u9886\u57df\u968f\u673a\u5316\u7ed3\u5408\u60b2\u89c2\u7ea6\u675f\uff0c\u628a\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\u4e2d\uff0c\u517c\u5bb9\u4e3b\u6d41\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u4eff\u771f-\u4eff\u771f\u57fa\u51c6\u548c\u4e24\u4e2a\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5927\u91cf\u5b9e\u9a8c\uff0cSPiDR\u90fd\u80fd\u6709\u6548\u4fdd\u969c\u201c\u4eff\u771f\u5230\u73b0\u5b9e\u201d\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5f3a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SPiDR\u7b97\u6cd5\u4e3a\u5728\u5b9e\u9645\u73af\u5883\u5b89\u5168\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u884c\u7684\u65b9\u6848\uff0c\u540c\u65f6\u5177\u6709\u7406\u8bba\u4fdd\u969c\u548c\u826f\u597d\u7684\u5de5\u7a0b\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.18536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u63a8\u7406\u65b9\u6cd5CCQA\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728\u6570\u5b66\u4e0e\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u56e0\u5148\u8fdb\u7684\u63a8\u7406\u7b56\u7565\u800c\u63d0\u5347\uff0c\u4f46\u8fd9\u4e9b\u7b56\u7565\u5bf9\u5c0f\u578b\u6a21\u578b\u7684\u6709\u6548\u6027\u8fd8\u4e0d\u660e\u786e\u3002\u5728\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u65e0\u52a9\u4e8e\u63d0\u5347\u5c0f\u6a21\u578b\u65f6\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u66f4\u9002\u5408\u5c0f\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5faa\u73af\u4e00\u81f4\u6027\u7684\u95ee\u7b54\u63a8\u7406\u65b9\u6cd5CCQA\u3002\u5177\u4f53\u505a\u6cd5\u4e3a\uff1a\u6cbf\u591a\u6761\u63a8\u7406\u8def\u5f84\u7ed9\u51fa\u7b54\u6848\u540e\uff0c\u4ece\u6bcf\u7ec4\u7b54\u6848\u548c\u63a8\u7406\u8def\u5f84\u751f\u6210\u4e00\u4e2a\u65b0\u95ee\u9898\uff0c\u5e76\u7528\u8fd9\u4e2a\u95ee\u9898\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u76f8\u4f3c\u5ea6\u8fdb\u884c\u8bc4\u4ef7\uff0c\u76f8\u4f3c\u5ea6\u6700\u9ad8\u8005\u88ab\u9009\u4e3a\u6700\u7ec8\u7b54\u6848\u3002\u4e3a\u5f25\u8865\u5c0f\u6a21\u578b\u751f\u6210\u95ee\u9898\u80fd\u529b\u4e0d\u8db3\uff0c\u4f7f\u7528Flan-T5\u505a\u95ee\u9898\u751f\u6210\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCCQA\u80fd\u57288\u4e2a\u4e3b\u6d41\u6a21\u578b\u548c\u6570\u5b66\u53ca\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u6301\u7eed\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "CCQA\u4e0d\u4ec5\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u7387\uff0c\u8fd8\u4e3a\u4e2d\u5c0f\u578b\u6a21\u578b\u9ad8\u6548\u63a8\u7406\u8bbe\u7acb\u4e86\u65b0\u7684\u5b9e\u7528\u57fa\u7ebf\u3002"}}
{"id": "2509.18354", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.18354", "abs": "https://arxiv.org/abs/2509.18354", "authors": ["Mehrdad Moradi", "Shengzhe Chen", "Hao Yan", "Kamran Paynabar"], "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data", "comment": "12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference", "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u4ec5\u5229\u7528\u5355\u5f20\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u65b0\u65b9\u6cd5SSDnet\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u53c2\u8003\u6837\u672c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u5e38\u53ea\u80fd\u83b7\u5f97\u5f85\u6d4b\u56fe\u50cf\u672c\u8eab\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\uff0c\u4e9f\u9700\u89e3\u51b3\u96f6\u6837\u672c\uff08zero-shot\uff09\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5148\u9a8c\u7684\u5355\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5SSDnet\uff0c\u53d7Deep Image Prior\u542f\u53d1\uff0c\u91c7\u7528\u57fa\u4e8epatch\u7684\u81ea\u91cd\u5efa\u8bad\u7ec3\u6846\u67b6\u3002\u76f4\u63a5\u5c06\u5f85\u68c0\u6d4b\u56fe\u50cf\u8f93\u5165\u7f51\u7edc\uff0c\u4e0d\u7528\u6620\u5c04\u566a\u58f0\uff0c\u800c\u662f\u91c7\u7528\u63a9\u7801\u3001patch\u6253\u4e71\u53ca\u9ad8\u65af\u566a\u58f0\u7b49\u624b\u6bb5\u9632\u6b62\u7f51\u7edc\u8bb0\u4f4f\u8f93\u5165\u3002\u521b\u65b0\u6027\u5730\u5f15\u5165\u57fa\u4e8e\u5185\u79ef\u76f8\u4f3c\u6027\u7684\u611f\u77e5\u635f\u5931\uff0c\u4ee5\u6355\u6349\u50cf\u7d20\u7ea7\u4e4b\u5916\u7684\u7ed3\u6784\u4fe1\u606f\u3002\u65b9\u6cd5\u65e0\u9700\u4efb\u4f55\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3001\u6807\u7b7e\u6216\u53c2\u8003\u6837\u672c\uff0c\u5bf9\u566a\u58f0\u548c\u7f3a\u5931\u50cf\u7d20\u8868\u73b0\u7a33\u5065\u3002", "result": "\u5728MVTec-AD\u548cfabric\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.99/0.98\u7684AUROC\u548c0.60/0.67\u7684AUPRC\uff0c\u4f18\u4e8e\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "SSDnet\u80fd\u591f\u5728\u65e0\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e14\u9c81\u68d2\u6027\u5f3a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18666", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5206\u5e03\u5f0f\u9c81\u68d2\u78b0\u649e\u89c4\u907f\u65b9\u6cd5\uff0c\u5e76\u5c06\u6b64\u65b9\u6cd5\u5e94\u7528\u4e8e\u81ea\u8f66\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u6700\u7ec8\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u78b0\u649e\u89c4\u907f\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u969c\u788d\u7269\u7684\u8fd0\u52a8\u5206\u5e03\u5df2\u77e5\u4e14\u56fa\u5b9a\uff0c\u7136\u800c\u5b9e\u9645\u73af\u5883\u4e2d\u969c\u788d\u7269\uff08\u5982\u5176\u4ed6\u8f66\u8f86\u3001\u884c\u4eba\u7b49\uff09\u7684\u8fd0\u52a8\u8f68\u8ff9\u5177\u6709\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u5e38\u4e0e\u81ea\u8f66\u884c\u4e3a\u76f8\u5173\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u8fc7\u53bb\u7684\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u83b7\u5f97\u66f4\u9c81\u68d2\u4e14\u5b89\u5168\u7684\u78b0\u649e\u89c4\u907f\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\uff08Conditional Kernel Mean Embedding\uff09\u5c06\u7ed9\u5b9a\u81ea\u8f66\u8fd0\u52a8\u6761\u4ef6\u4e0b\u969c\u788d\u7269\u672a\u6765\u8f68\u8ff9\u7684\u5206\u5e03\u5d4c\u5165RKHS\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5b9a\u4e49\u4e00\u4e2a\u5d4c\u5165\u6a21\u7cca\u96c6\uff0c\u542b\u6709\u6240\u6709\u4e0e\u7ecf\u9a8c\u6761\u4ef6\u5747\u503c\u5d4c\u5165\u8ddd\u79bb\u4e0d\u8d85\u8fc7\u67d0\u9608\u503c\u7684\u5206\u5e03\u3002\u4e4b\u540e\uff0c\u5c06\u5206\u5e03\u5f0f\u9c81\u68d2\u78b0\u649e\u89c4\u907f\u7ea6\u675f\u7eb3\u5165\u57fa\u4e8e\u6eda\u52a8\u65f6\u57df\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u5b9e\u73b0\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7684\u8fd0\u52a8\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u672a\u5f15\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u5206\u5e03\u9c81\u68d2\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u78b0\u649e\u89c4\u907f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u5f0f\u9c81\u68d2\u6027\u7684\u78b0\u649e\u89c4\u907f\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7b49\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u4e14\u8be5\u6846\u67b6\u5bf9\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u5177\u6709\u76f4\u63a5\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18577", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u9879\u5148\u9a8c\u6982\u7387\u7684\u9ad8\u6548\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u53ef\u5728\u5927\u5e45\u964d\u4f4e\u65f6\u95f4\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u7684\u57fa\u4e8e\u56f0\u60d1\u5ea6\uff08PPL\uff09\u8fc7\u6ee4\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u8017\u65f6\u4e14\u5bf9\u5f02\u5e38\u566a\u58f0\u6570\u636e\u4e0d\u591f\u9c81\u68d2\uff0c\u5f71\u54cdLLM\u9884\u8bad\u7ec3\u6570\u636e\u7684\u9009\u62e9\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u8bed\u6599\u4e2d\u7684\u8bcd\u9879\u9891\u7387\u7edf\u8ba1\uff0c\u57fa\u4e8e\u6bcf\u4e2atoken\u7684\u5148\u9a8c\u6982\u7387\uff08\u5747\u503c\u4e0e\u6807\u51c6\u5dee\uff09\uff0c\u5bf9\u6587\u6863\u8fdb\u884c\u5feb\u901f\u8fc7\u6ee4\uff0c\u65e0\u9700\u6a21\u578b\u63a8\u65ad\u8fc7\u7a0b\uff0c\u7406\u8bba\u57fa\u7840\u501f\u9274\u4e86\u8bcd\u6c47\u5bc6\u5ea6\u4e0e\u8bcd\u7c7b\u89d2\u8272\u7684\u8bed\u8a00\u5b66\u542f\u53d1\u3002", "result": "\u8be5\u65b9\u6cd5\u572820\u4e2a\u4e0b\u6e38\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6027\u80fd\uff0c\u540c\u65f6\u76f8\u6bd4PPL\u8fc7\u6ee4\u5b9e\u73b0\u4e861000\u500d\u4ee5\u4e0a\u7684\u63d0\u901f\u3002\u5728\u4ee3\u7801\u3001\u6570\u5b66\u7b49\u7b26\u53f7\u8bed\u8a00\u4ee5\u53ca\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bcd\u9879\u5148\u9a8c\u57fa\u8fc7\u6ee4\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5355\u6613\u884c\uff0c\u8fd8\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u5f0f\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u6599\u9884\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u3002"}}
{"id": "2509.18369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18369", "abs": "https://arxiv.org/abs/2509.18369", "authors": ["Riad Ahmed Anonto", "Sardar Md. Saffat Zabin", "M. Saifur Rahman"], "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning", "comment": null, "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u5b5f\u52a0\u62c9\u8bed\u89c6\u89c9\u63cf\u8ff0\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u591a\u635f\u5931\u76ee\u6807\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u7684\u591a\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u5e38\u51fa\u73b0\u63cf\u8ff0\u6d41\u7545\u4f46\u7269\u4f53\u8bc6\u522b\u9519\u8bef\u7684\u73b0\u8c61\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u53cc\u8bed\u6807\u6ce8\u6570\u636e\u3001\u7ffb\u8bd1\u8bef\u5dee\u4ee5\u53ca\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u76ee\u6807\u8bed\u8a00\u8bed\u4e49\u7684\u5ffd\u89c6\u3002\u672c\u6587\u65e8\u5728\u589e\u5f3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u89c6\u89c9\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\uff081\uff09\u6784\u5efaLaBSE\u9a8c\u8bc1\u7684\u82f1-\u5b5f\u53cc\u8bed\u914d\u5bf9\u53ca11\u4e07\u5e26\u53cc\u8bed\u63d0\u793a\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\u3002\uff082\uff09\u51bb\u7ed3MaxViT\u7528\u4e8e\u7a33\u5b9a\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u91c7\u7528\u4ee5\u5b5f\u52a0\u62c9\u8bed\u4e3a\u6bcd\u8bed\u7684mBART-50\u4f5c\u4e3a\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u6865\u63a5\u6a21\u5757\u8054\u901a\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u3002\uff083\uff09\u63d0\u51fa\u4e09\u91cd\u635f\u5931\uff08tri-loss\uff09\u76ee\u6807\uff1aPatch-Alignment Loss (PAL)\u7528\u4e8e\u901a\u8fc7\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cfPatch\u7279\u5f81\uff1bInfoNCE\u5168\u5c40\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210Patch\uff1bSinkhorn\u6700\u4f18\u4f20\u8f93\u635f\u5931\u4fc3\u4f7f\u7ec6\u7c92\u5ea6Patch\u95f4\u4e24\u4e24\u5bf9\u9f50\u3002", "result": "PAL+InfoNCE+OT\u4e09\u91cd\u635f\u5931\u534f\u540c\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u7269\u4f53\u5b9a\u4f4d\u4e0e\u63cf\u8ff0\u7684\u7cbe\u786e\u6027\uff0c\u964d\u4f4e\u9519\u8bef\u5339\u914d\uff0c\u5e76\u5728Flickr30k-1k\u4e0eMSCOCO-1k\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u5404\u9879\u4e3b\u6d41\u8bc4\u6d4b\u6307\u6807\uff08\u5982BLEU-4\u3001METEOR\u548cBERTScore\uff09\uff0c\u4e14\u663e\u8457\u7f29\u5c0f\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u7279\u5f81\u4e2d\u5fc3\u95f4\u7684\u8ddd\u79bb\uff0841%\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u96be\u9898\uff0c\u5229\u7528\u591a\u6e90\u6570\u636e\u548c\u591a\u91cd\u635f\u5931\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\u4e0e\u63cf\u8ff0\u8d28\u91cf\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bed\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u5177\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2509.18671", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "N2M\u6a21\u5757\u5e2e\u52a9\u79fb\u52a8\u64cd\u63a7\u673a\u5668\u4eba\u9009\u53d6\u66f4\u4f18\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5728\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0c\u64cd\u4f5c\u7b56\u7565\u5f80\u5f80\u5bf9\u521d\u59cb\u59ff\u6001\u6709\u8f83\u5f3a\u4f9d\u8d56\uff0c\u800c\u76ee\u524d\u5bfc\u822a\u6a21\u5757\u53ea\u5173\u6ce8\u5230\u8fbe\u4efb\u52a1\u533a\u57df\uff0c\u5ffd\u7565\u4e86\u540e\u7eed\u64cd\u4f5c\u7684\u6700\u4f18\u521d\u59cb\u4f4d\u7f6e\uff0c\u5bfc\u81f4\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdN2M\u4e2d\u8f6c\u6a21\u5757\uff0c\u673a\u5668\u4eba\u5230\u8fbe\u4efb\u52a1\u533a\u540e\uff0cN2M\u6839\u636e\u672c\u5730\u89c6\u89c9\u89c2\u6d4b\u5f15\u5bfc\u5176\u5230\u66f4\u4f18\u521d\u59cb\u4f4d\u7f6e\u3002\u8be5\u6a21\u5757\u5177\u5907\u65e0\u9700\u5168\u5c40/\u5386\u53f2\u4fe1\u606f\u3001\u53ef\u5b9e\u65f6\u9002\u5e94\u53d8\u5316\u3001\u9ad8\u89c6\u89d2\u9c81\u68d2\u6027\u3001\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\u4e94\u5927\u4f18\u70b9\u3002\u65b9\u6cd5\u901a\u8fc7\u6a21\u578b\u5b66\u4e60\u4e0e\u6570\u636e\u9a71\u52a8\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u4e2d\uff0cN2M\u5c06PnPCounterToCab\u4efb\u52a1\u7684\u6210\u529f\u7387\u4ece3%\u63d0\u9ad8\u523054%\uff1b\u5728Toybox Handover\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u4ec5\u752815\u4e2a\u6837\u672c\uff0c\u5728\u65b0\u73af\u5883\u4e0b\u4e5f\u80fd\u505a\u51fa\u53ef\u9760\u9884\u6d4b\uff0c\u663e\u793a\u51fa\u4f18\u5f02\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "N2M\u6709\u6548\u89e3\u51b3\u4e86\u5bfc\u822a\u548c\u64cd\u4f5c\u95f4\u7684\u521d\u59cb\u59ff\u6001\u5931\u914d\u95ee\u9898\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u673a\u5668\u4eba\u79fb\u52a8\u64cd\u63a7\u7684\u6210\u529f\u7387\u4e0e\u9002\u5e94\u6027\uff0c\u5bf9\u591a\u4efb\u52a1\u591a\u5e73\u53f0\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u8fc1\u79fb\u6f5c\u529b\u3002"}}
{"id": "2509.18585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.", "AI": {"tldr": "TsqLoRA\u7ed3\u5408\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u5c42\u654f\u611f\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6548\u7387\u4e14\u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u53c2\u6570\u91cf\u5e9e\u5927\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u4e14\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u4e0d\u540c\u5c42\u7684\u654f\u611f\u6027\u548c\u6570\u636e\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u66f4\u667a\u80fd\u3001\u7ec6\u81f4\u7684\u5fae\u8c03\u673a\u5236\u3002", "method": "TsqLoRA\u5305\u542b\u4e24\u5927\u521b\u65b0\u7ec4\u4ef6\uff1a\u4e00\u662f\u6839\u636e\u6570\u636e\u8d28\u91cf\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u8bad\u7ec3\u6837\u672c\uff1b\u4e8c\u662f\u6839\u636e\u6a21\u578b\u5404\u5c42\u5bf9\u53c2\u6570\u53d8\u5316\u7684\u654f\u611f\u5ea6\uff0c\u4e3a\u4e0d\u540c\u5c42\u52a8\u6001\u5206\u914d\u4f4e\u79e9\u77e9\u9635\u7684\u79e9\uff0c\u4ece\u800c\u63d0\u5347\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTsqLoRA\u5728\u591a\u9879\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u5fae\u8c03\u6548\u7387\uff0c\u540c\u65f6\u8fd8\u80fd\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "TsqLoRA\u6709\u6548\u5730\u517c\u987e\u4e86\u6570\u636e\u8d28\u91cf\u4e0e\u6a21\u578b\u5c42\u654f\u611f\u6027\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u89e3\u6cd5\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2509.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "AI": {"tldr": "TinyBEV\u662f\u4e00\u79cd\u4ec5\u7528\u76f8\u673a\u6570\u636e\u3001\u9762\u5411\u9e1f\u77b0\u56feBEV\u7684\u7d27\u51d1\u5168\u6808\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5728\u4ec528M\u53c2\u6570\u4e0b\u5b9e\u73b0\u7aef\u5230\u7aef\u611f\u77e5\u5230\u89c4\u5212\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff08\u5982UniAD\uff09\u53c2\u6570\u91cf\u5927\u3001\u4f9d\u8d56\u591a\u6a21\u6001\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u90e8\u7f72\uff0c\u800c\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u529f\u80fd\u4e0d\u5168\u3002\u4f5c\u8005\u5e0c\u671b\u5b9e\u73b0\u4e00\u4e2a\u652f\u6301\u5b8c\u6574\u81ea\u4e3b\u9a7e\u9a76\u4efb\u52a1\u3001\u4ec5\u4f9d\u8d56\u76f8\u673a\u3001\u53c2\u6570\u91cf\u6781\u4f4e\u5e76\u652f\u6301\u5b9e\u65f6\u5e94\u7528\u7684\u7edf\u4e00BEV\u6a21\u578b\u3002", "method": "\u63d0\u51faTinyBEV\uff0c\u5c06\u5927\u578b\u5168\u6808\u6a21\u578bUniAD\u7684\u6838\u5fc3\u80fd\u529b\u901a\u8fc7\u5206\u9636\u6bb5\uff08\u7279\u5f81\u7ea7\u3001\u8f93\u51fa\u7ea7\u548c\u81ea\u9002\u5e94\u533a\u57df\u76d1\u7763\uff09\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u6709\u6548\u8fc1\u79fb\u591a\u6a21\u6001\u77e5\u8bc6\u5230\u8f7b\u91cfBEV\u7ed3\u6784\uff0c\u5b9e\u73b0\u5168\u6808\u4efb\u52a1\uff083D\u68c0\u6d4b\u3001\u9ad8\u6e05\u5730\u56fe\u5206\u5272\u3001\u8fd0\u52a8\u9884\u4f30\u3001\u5360\u7528\u9884\u6d4b\u548c\u5b8f\u89c2\u89c4\u5212\uff09\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cTinyBEV\u4ec5\u7528\u76f8\u673a\u8f93\u5165\u53d6\u5f9739.0\u7684\u68c0\u6d4bmAP\uff0c1.08\u7684\u8fd0\u52a8\u9884\u4f30minADE\uff0c\u4ee5\u53ca0.32\u7684\u78b0\u649e\u7387\uff0c\u901f\u5ea6\u63d0\u53475\u500d\u8fbe\u523011 FPS\uff0c\u53c2\u6570\u91cf\u76f8\u6bd4UniAD\u51cf\u5c1178%\u3002", "conclusion": "TinyBEV\u5728\u663e\u8457\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8f93\u5165\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u4fdd\u6301\u5168\u9762\u7aef\u5230\u7aef\u9a7e\u9a76\u667a\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.18676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D Flow Diffusion Policy\uff083D FDP\uff09\u65b9\u6cd5\uff0c\u53ef\u901a\u8fc7\u573a\u666f\u7ea73D\u6d41\u573a\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u5728MetaWorld\u5e73\u53f0\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u64cd\u4f5c\u7b56\u7565\u5927\u591a\u5ffd\u7565\u4e86\u5c40\u90e8\u52a8\u6001\u4fe1\u606f\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5728\u590d\u6742\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u6709\u6548\u6355\u6349\u5c40\u90e8\u7cbe\u7ec6\u8fd0\u52a8\u4fe1\u606f\u5e76\u7528\u4e8e\u52a8\u4f5c\u51b3\u7b56\u7684\u6cdb\u5316\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u63d0\u51fa3D FDP\u6846\u67b6\uff0c\u9996\u5148\u9884\u6d4b\u573a\u666f\u4e2d\u91c7\u6837\u70b9\u7684\u65f6\u5e8f\u8f68\u8ff9\uff083D Flow\uff09\uff0c\u5c06\u5176\u4f5c\u4e3a\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\uff0c\u518d\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7ed3\u5408\u8fd9\u4e9b\u6d41\u573a\u4fe1\u606f\u751f\u6210\u52a8\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u5c40\u90e8\u4ea4\u4e92\u548c\u5168\u5c40\u573a\u666f\u63a8\u7406\u7684\u51b3\u7b56\u3002", "result": "3D FDP\u5728MetaWorld\u57fa\u51c650\u9879\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f53\u524d\u6700\u4f73\u8868\u73b0\uff0c\u5c24\u5176\u5728\u4e2d\u7b49\u548c\u56f0\u96be\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u57288\u9879\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u63a5\u89e6\u4e30\u5bcc\u4e0e\u975e\u6293\u53d6\u573a\u666f\u5747\u4f18\u4e8e\u4ee5\u5f80\u57fa\u7ebf\u3002", "conclusion": "3D\u6d41\u573a\u4f5c\u4e3a\u64cd\u4f5c\u7b56\u7565\u7ed3\u6784\u5148\u9a8c\uff0c\u6781\u5927\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\uff0c\u4e3a\u89c6\u89c9\u64cd\u4f5c\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5fc3\u7535\u56fe\uff08ECG\uff09\u7684\u7edf\u4e00\u6a21\u578bUniECG\uff0c\u80fd\u540c\u65f6\u5904\u7406ECG\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7a81\u7834\u4e86\u5f53\u524d\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u548c\u5fc3\u7535\u56fe\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5982GPT-5\u5df2\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u7406\u89e3\u548c\u751f\u6210ECG\u4fe1\u53f7\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u533b\u7597\u5b9e\u9645\u9700\u6c42\uff0c\u6240\u4ee5\u6709\u5fc5\u8981\u63a2\u7d22\u4e13\u95e8\u9488\u5bf9ECG\u7684\u7edf\u4e00\u751f\u6210\u4e0e\u7406\u89e3\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aUniECG\u7684\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u5f0f\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u5b66\u4e60\u57fa\u4e8e\u8bc1\u636e\u7684ECG\u89e3\u91ca\uff08ECG\u8f6c\u6587\u672c\uff09\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u5f15\u5165\u5fc3\u7535\u56fe\u751f\u6210\u80fd\u529b\uff08\u6587\u672c\u8f6cECG\uff09\uff0c\u5b9e\u73b0\u6a21\u578b\u540c\u65f6\u652f\u6301\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002", "result": "UniECG\u6a21\u578b\u80fd\u591f\u6839\u636e\u7528\u6237\u8f93\u5165\u9009\u62e9\u662f\u7406\u89e3\u8fd8\u662f\u751f\u6210ECG\uff0c\u5728ECG\u8bca\u65ad\u548c\u4fe1\u53f7\u751f\u6210\u4e24\u5927\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f53\u524dECG\u76f8\u5173\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "UniECG\u9996\u6b21\u5b9e\u73b0\u4e86ECG\u8bc1\u636e\u9a71\u52a8\u7684\u89e3\u91ca\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684ECG\u751f\u6210\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e86\u533b\u5b66AI\u7684\u5e94\u7528\u8303\u56f4\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5373\u5c06\u5f00\u6e90\uff0c\u6709\u671b\u4fc3\u8fdb\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f4\u7684\u7403\u7c7b\u68c0\u6d4b\u56f0\u96be\uff0c\u63d0\u51fa\u4ee5\u6a21\u7cca\u5e26\u4e2d\u5fc3\u4f5c\u4e3a\u6807\u6ce8\u3001\u5e76\u9644\u52a0\u6a21\u7cca\u5c5e\u6027\u7684\u65b0\u6807\u6ce8\u65b9\u6cd5\uff0c\u53d1\u5e03\u4e86\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86BlurBall\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u63d0\u5347\u3002", "motivation": "\u7403\u7c7b\u9ad8\u901f\u8fd0\u52a8\u4e0b\u6613\u4ea7\u751f\u6a21\u7cca\uff0c\u5bfc\u81f4\u68c0\u6d4b\u96be\u5ea6\u5927\u3002\u73b0\u6709\u6807\u6ce8\u65b9\u6cd5\u901a\u5e38\u6807\u6ce8\u6a21\u7cca\u5e26\u524d\u7f18\uff0c\u5ffd\u7565\u4e0e\u7403\u901f\u76f8\u5173\u7684\u91cd\u8981\u6a21\u7cca\u4fe1\u606f\u3002\u4e9f\u9700\u4f18\u5316\u6807\u6ce8\u65b9\u5f0f\uff0c\u4ee5\u63d0\u5347\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4ee5\u6a21\u7cca\u5e26\u4e2d\u5fc3\u4f5c\u4e3a\u7403\u4f4d\u7f6e\u7684\u65b0\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u660e\u786e\u8bb0\u5f55\u6a21\u7cca\u5c5e\u6027\u3002\u636e\u6b64\u6784\u5efa\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u63d0\u51faBlurBall\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5e27\u8f93\u5165\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982Squeeze-and-Excitation\uff09\uff0c\u8054\u5408\u4f30\u8ba1\u7403\u4f4d\u7f6e\u548c\u6a21\u7cca\u5c5e\u6027\u3002", "result": "\u65b0\u7684\u6807\u6ce8\u65b9\u5f0f\u5728\u591a\u79cd\u68c0\u6d4b\u6a21\u578b\u4e2d\u5747\u63d0\u5347\u4e86\u6027\u80fd\u3002BlurBall\u5728\u7403\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f18\u7ed3\u679c\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u7403\u7684\u8f68\u8ff9\u3002", "conclusion": "\u5408\u7406\u5229\u7528\u8fd0\u52a8\u6a21\u7cca\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7403\u7c7b\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\u53ca\u8f68\u8ff9\u63a8\u65ad\u80fd\u529b\uff0c\u5c06\u4e3a\u5b9e\u65f6\u4f53\u80b2\u5206\u6790\u7cfb\u7edf\u5e26\u6765\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18686", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuery-centric Diffusion Policy\uff08QDP\uff09\u7684\u5206\u5c42\u673a\u5668\u4eba\u88c5\u914d\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u63a5\u89e6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6\u4e0e\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5728\u88c5\u914d\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u90e8\u4ef6\u4ea4\u4e92\u590d\u6742\u4e14\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u96be\u4ee5\u5b9e\u73b0\u901a\u7528\u667a\u80fd\u548c\u9ad8\u6548\u63a7\u5236\u3002\u5e38\u89c4\u5206\u5c42\u7b56\u7565\u5728\u9ad8\u4f4e\u5c42\u4e4b\u95f4\u5b58\u5728\u6267\u884c\u5931\u914d\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86QDP\u67b6\u6784\uff0c\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u63a7\u5236\u901a\u8fc7\u4efb\u52a1\u76f8\u5173\u7684\u67e5\u8be2\uff08\u5305\u542b\u76ee\u6807\u5bf9\u8c61\u3001\u63a5\u89e6\u70b9\u548c\u6280\u80fd\u4fe1\u606f\uff09\u8854\u63a5\uff0c\u5e76\u57fa\u4e8e\u70b9\u4e91\u89c2\u6d4b\u4ee5\u589e\u5f3a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002\u8be5\u673a\u5236\u80fd\u7cbe\u51c6\u8bc6\u522b\u4efb\u52a1\u5173\u952e\u8981\u7d20\u5e76\u6307\u5bfc\u4f4e\u5c42\u63a7\u5236\u653f\u7b56\u3002", "result": "\u5728FurnitureBench\u5e73\u53f0\u7684\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0cQDP\u5728\u6280\u80fd\u7cbe\u5ea6\u548c\u957f\u65f6\u95f4\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u5c24\u5176\u662f\u63d2\u5165\u4e0e\u65cb\u62e7\u7b49\u9ad8\u96be\u5ea6\u4efb\u52a1\uff0cQDP\u6bd4\u65e0\u7ed3\u6784\u5316\u67e5\u8be2\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u6280\u80fd\u6210\u529f\u7387\u4e0a\u63d0\u5347\u8d8550%\u3002", "conclusion": "QDP\u4f5c\u4e3a\u4e00\u79cd\u5206\u5c42\u7ed3\u6784\uff0c\u52a0\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u88c5\u914d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u67e5\u8be2\u6865\u63a5\u9ad8\u4f4e\u5c42\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPlanorama\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u7528\u6237\u5728\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u8ba1\u5212\u65f6\u7684\u5b9e\u9645\u5e2e\u52a9\u60c5\u51b5\u548c\u504f\u597d\uff0c\u63ed\u793a\u4e86\u504f\u597d\u4e0e\u5b9e\u9645\u6709\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u7528\u6237\u504f\u597d\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5047\u8bbe\u7528\u6237\u559c\u6b22\u7684\u7b54\u6848\u5c31\u662f\u5b9e\u9645\u6709\u5e2e\u52a9\u7684\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u662f\u5426\u6210\u7acb\u672a\u88ab\u9a8c\u8bc1\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86Planorama\u754c\u9762\uff0c\u8ba9126\u540d\u7528\u6237\u5229\u7528LLM\u8ba1\u5212\u89e3\u51b3300\u4e2a\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u8bb0\u5f554388\u6b21\u8ba1\u5212\u6267\u884c\u548c5584\u6b21\u8ba1\u5212\u504f\u597d\u6bd4\u8f83\uff0c\u901a\u8fc7\u6bd4\u8f83\u7528\u6237\u6700\u7ec8\u4efb\u52a1\u6210\u529f\u7387\u548c\u4e3b\u89c2\u504f\u597d\uff0c\u5206\u6790\u504f\u597d\u4e0e\u5b9e\u9645\u5e2e\u52a9\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u7528\u6237/\u6a21\u578b\u7684\u504f\u597d\u4ee5\u53ca\u57fa\u4e8e\u504f\u597d\u7684\u4ee3\u7406\u6a21\u578b\u7684\u8868\u73b0\uff0c\u90fd\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u54ea\u4e9b\u8ba1\u5212\u771f\u6b63\u5e2e\u52a9\u4e86\u7528\u6237\uff1b\u7528\u6237\u559c\u6b22\u7684\u548c\u4e0d\u559c\u6b22\u7684\u8ba1\u5212\u5e26\u6765\u7684\u6210\u529f\u7387\u5dee\u522b\u4e0d\u5927\uff0c\u4e14\u7528\u6237\u504f\u597d\u53d7\u8868\u9762\u7279\u5f81\u5f71\u54cd\uff08\u5982\u7b80\u6d01\u6027\u3001\u4e0e\u95ee\u9898\u8868\u8ff0\u7684\u76f8\u4f3c\u6027\uff09\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u5e76\u4e0d\u80fd\u53cd\u6620\u5b9e\u9645\u7684\u6709\u7528\u6027\u3002", "conclusion": "\u4ec5\u7528\u7528\u6237\u504f\u597d\u8fdb\u884cLLM\u8bad\u7ec3\u548c\u8bc4\u4ef7\u53ef\u80fd\u5bfc\u81f4\u4e0e\u5b9e\u9645\u5e2e\u52a9\u8131\u94a9\uff0c\u547c\u5401\u7814\u7a76\u8005\u5e94\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u6548\u679c\u6536\u96c6\u53cd\u9988\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u7528\u7684LLM\u5bf9\u9f50\u3002"}}
{"id": "2509.18388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18388", "abs": "https://arxiv.org/abs/2509.18388", "authors": ["Binhua Huang", "Ni Wang", "Wendong Yao", "Soumyabrata Dev"], "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection", "comment": "5 pages, 1 figure", "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u987b\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u53ea\u9700\u5728\u56fa\u5b9a\u65f6\u95f4\u95f4\u9694\u7684\u5173\u952e\u5e27\u4e0a\u8fd0\u884c\u5927\u578b\u5f00\u96c6\u68c0\u6d4b\u5668\uff08\u5982OWLv2\uff09\uff0c\u5e76\u5229\u7528\u89c6\u9891\u538b\u7f29\u57df\u7684\u8fd0\u52a8\u5411\u91cf\uff08MV\uff09\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u64ad\u5230\u4e2d\u95f4\u5e27\uff0c\u4ece\u800c\u5927\u5e45\u964d\u4f4e\u4e86\u68c0\u6d4b\u6210\u672c\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u8f83\u9ad8\u7684\u68c0\u6d4b\u51c6\u786e\u5ea6\u3002", "motivation": "\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u76f4\u63a5\u5bf9\u6bcf\u4e00\u5e27\u90fd\u8fd0\u884c\u9ad8\u7cbe\u5ea6\u7684\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u6210\u672c\u6781\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6216\u9700\u8981\u5904\u7406\u5927\u91cf\u89c6\u9891\u65f6\u3002\u5982\u4f55\u964d\u4f4e\u68c0\u6d4b\u5668\u8c03\u7528\u6b21\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u96c6\u68c0\u6d4b\u80fd\u529b\u548c\u68c0\u6d4b\u6027\u80fd\uff0c\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u4ec5\u5728\u5173\u952e\u5e27\u4e0a\u8fd0\u884cOWLv2\u68c0\u6d4b\u5668\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u538b\u7f29\u57df\u4e2d\u7684\u8fd0\u52a8\u5411\u91cf\uff0c\u75283x3\u7f51\u683c\u7b80\u5355\u805a\u5408\uff0c\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u5728\u5e27\u95f4\u7684\u5e73\u79fb\u548c\u5c3a\u5ea6\u53d8\u5316\u4f20\u64ad\u3002\u8fd8\u52a0\u5165\u4e86\u9762\u79ef\u589e\u957f\u68c0\u67e5\u548c\u53ef\u9009\u7684\u5355\u7c7b\u5207\u6362\uff0c\u65e0\u9700\u6807\u7b7e\u3001\u5fae\u8c03\uff0c\u4e14\u9002\u7528\u4e8e\u6240\u6709\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u3002", "result": "\u5728ILSVRC2015-VID\u9a8c\u8bc1\u96c6\u4e0a\uff0cMVP\u65b9\u6cd5\u53d6\u5f97mAP@0.5=0.609\uff0cmAP@[0.5:0.95]=0.316\u3002\u5728\u8f83\u4f4eIoU\u65f6\uff080.2/0.3\uff09\uff0c\u6027\u80fd\u63a5\u8fd1\u9010\u5e27\u68c0\u6d4b\uff080.747/0.721 vs 0.784/0.780\uff09\u3002\u76f8\u6bd4\u4e8e\u4f20\u7edf\u591a\u76ee\u6807\u8ddf\u8e2a\u5668\uff08MOSSE\uff0cKCF\uff0cCSRT\uff09\u548c\u6709\u76d1\u7763\u65b9\u6cd5\uff08\u5982YOLOv12x\uff09\uff0cMVP\u5728\u65e0\u76d1\u7763\u548c\u5f00\u653e\u8bcd\u6c47\u4e0b\u5177\u6709\u66f4\u9ad8\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u538b\u7f29\u57df\u76ee\u6807\u4f20\u64ad\u4e3a\u964d\u4f4e\u5f00\u96c6\u68c0\u6d4b\u5668\u5728\u89c6\u9891\u4e2d\u8c03\u7528\u6b21\u6570\u63d0\u4f9b\u4e86\u7b80\u4fbf\u4e14\u9ad8\u6548\u7684\u6280\u672f\u8def\u7ebf\uff0c\u5bf9\u4fdd\u6301\u9ad8\u5e7f\u5ea6\u3001\u96f6\u6837\u672c\u7684\u68c0\u6d4b\u80fd\u529b\u6781\u5177\u5b9e\u9645\u610f\u4e49\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u7b7e\u548c\u5fae\u8c03\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.18734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u865a\u62df\u65e0\u4eba\u673a\u5728\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\uff0c\u91cd\u70b9\u89e3\u51b3\u8def\u5f84\u89c4\u5212\u4e0e\u907f\u969c\u96be\u9898\u3002", "motivation": "\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u5e94\u5bf9GPS\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u3001\u7a7a\u95f4\u72ed\u7a84\u548c\u52a8\u6001\u969c\u788d\u7269\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u907f\u969c\u548c\u8def\u5f84\u89c4\u5212\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5728\u865a\u62df\u73af\u5883\u4e2d\uff0c\u7ed3\u5408\u6df1\u5ea6\u6444\u50cf\u5934\u4f20\u611f\u4fe1\u606f\uff0c\u8fd0\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5bf9\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8fdb\u884c\u5bfc\u822a\u8bad\u7ec3\u3002", "result": "\u865a\u62df\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u589e\u5f3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u8fdb\u884c\u6709\u6548\u907f\u969c\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u548c\u907f\u969c\u80fd\u529b\uff0c\u5bf9\u81ea\u4e3b\u98de\u884c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.18655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAPE-KG\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u4e00\u81f4\u6027\u611f\u77e5\u7684\u53c2\u6570\u65e0\u635f\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u53c2\u6570\u65e0\u635f\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u77e5\u8bc6\u6c61\u67d3\u3001\u66f4\u65b0\u4e0d\u7a33\u5b9a\u548c\u68c0\u7d22\u884c\u4e3a\u65e0\u6cd5\u53cd\u6620\u9884\u671f\u7f16\u8f91\uff0c\u5f71\u54cd\u4e86\u65b9\u6cd5\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCAPE-KG\u6846\u67b6\uff1a\u5728\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u3001\u66f4\u65b0\u548c\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u786e\u4fdd\u6574\u4e2a\u7f16\u8f91\u6d41\u7a0b\u4e0e\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u8981\u6c42\u5bf9\u9f50\uff0c\u4fdd\u8bc1\u5bf9\u539f\u59cb\u4e0e\u7f16\u8f91\u540e\u77e5\u8bc6\u7684\u8fde\u8d2f\u63a8\u7406\u3002", "result": "\u5728MQuAKE\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aCAPE-KG\u80fd\u591f\u63d0\u5347\u53c2\u6570\u65e0\u635f\u77e5\u8bc6\u7f16\u8f91\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e00\u81f4\u6027\u673a\u5236\uff0cCAPE-KG\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u573a\u666f\u4e0b\u53c2\u6570\u65e0\u635f\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18390", "abs": "https://arxiv.org/abs/2509.18390", "authors": ["Zitian Zhang", "Joshua Urban Davis", "Jeanne Phuong Anh Vu", "Jiangtao Kuang", "Jean-Fran\u00e7ois Lalonde"], "title": "Improving the color accuracy of lighting estimation models", "comment": "Project page: https://lvsn.github.io/coloraccuracy", "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.", "AI": {"tldr": "\u672c\u8bba\u6587\u5173\u6ce8\u5355\u5f20\u56fe\u50cf\u4e0bHDR\u7167\u660e\u4f30\u8ba1\u4e2d\u989c\u8272\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u767d\u5e73\u8861\u9884\u5904\u7406\u80fd\u6709\u6548\u63d0\u5347\u7167\u660e\u989c\u8272\u51c6\u786e\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e3b\u6a21\u578b\u3002", "motivation": "\u5f53\u524dHDR\u7167\u660e\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u805a\u7126\u4e8e\u5f3a\u5ea6\u548c\u65b9\u5411\u7b49\u5c5e\u6027\uff0c\u989c\u8272\u9c81\u68d2\u6027\u5374\u88ab\u5ffd\u89c6\uff0c\u4f46\u771f\u5b9e\u611fAR\u6e32\u67d3\u5bf9\u8272\u5f69\u51c6\u786e\u8981\u6c42\u6781\u9ad8\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u4e13\u95e8\u63a2\u7a76\u73b0\u6709\u65b9\u6cd5\u7684\u989c\u8272\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u4e0d\u63d0\u51fa\u65b0\u7b97\u6cd5\uff0c\u800c\u662f\u901a\u8fc7\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u7167\u660e\u8272\u5f69\u7684HDR\u6570\u636e\u96c6\uff0c\u5bf9\u591a\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u3002\u91cd\u70b9\u6d4b\u8bd5\u4e86\u9884\u5148\u7528\u767d\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u8f93\u5165\u56fe\u7247\u8fd9\u79cd\u7b80\u5355\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u4e0e\u5176\u4ed6\u65b9\u6cd5\u505a\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u8f93\u5165\u56fe\u50cf\u5e94\u7528\u9884\u8bad\u7ec3\u7684\u767d\u5e73\u8861\u7f51\u7edc\u5728\u6240\u6709\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u989c\u8272\u9c81\u68d2\u6027\u63d0\u5347\u6700\u663e\u8457\uff0c\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6216\u91cd\u8bad\u7ec3\u539f\u7167\u660e\u4f30\u8ba1\u7b97\u6cd5\u3002", "conclusion": "\u7b80\u5355\u7684\u767d\u5e73\u8861\u9884\u5904\u7406\u6709\u6548\u63d0\u5347\u4e86\u73b0\u6709HDR\u7167\u660e\u4f30\u8ba1\u6a21\u578b\u7684\u989c\u8272\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u666e\u904d\u9002\u7528\u4e8e\u591a\u79cd\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e3a\u589e\u5f3a\u73b0\u5b9e\u6e32\u67d3\u63d0\u5347\u771f\u5b9e\u611f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18757", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MV-UMI\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7b2c\u4e09\u4eba\u79f0\u548c\u81ea\u6211\u89c6\u89d2\u76f8\u673a\uff0c\u7528\u4e8e\u63d0\u5347\u4fbf\u643a\u5f0f\u624b\u6301\u6293\u53d6\u5668\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u7684\u64cd\u63a7\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u9700\u8981\u573a\u666f\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u8d28\u591a\u6837\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u5e38\u7ed1\u5b9a\u7279\u5b9a\u673a\u5668\u4eba\u3002\u4fbf\u643a\u5f0f\u624b\u6301\u6293\u53d6\u5668\u867d\u76f4\u89c2\u3001\u53ef\u6269\u5c55\uff0c\u4f46\u56e0\u53ea\u7528\u8155\u90e8\u76f8\u673a\uff0c\u573a\u666f\u4fe1\u606f\u6709\u9650\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u548c\u6027\u80fd\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u6570\u636e\u91c7\u96c6\u8303\u56f4\u548c\u573a\u666f\u7406\u89e3\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MV-UMI\uff08\u591a\u89c6\u89d2\u901a\u7528\u64cd\u4f5c\u63a5\u53e3\uff09\uff0c\u5728\u539f\u6709\u624b\u6301\u6293\u53d6\u5668\u57fa\u7840\u4e0a\uff0c\u96c6\u6210\u4e86\u7b2c\u4e09\u4eba\u79f0\u4e0e\u81ea\u6211\u89c6\u89d2\u76f8\u673a\u91c7\u96c6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u7684\u73af\u5883\u611f\u77e5\u3002\u8be5\u65b9\u6cd5\u80fd\u7f13\u89e3\u4eba\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u90e8\u7f72\u95f4\u7684\u9886\u57df\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u673a\u5668\u4eba\u4f53\u7cfb\u7edf\u6570\u636e\u91c7\u96c6\u4f18\u52bf\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8868\u660eMV-UMI\u5728\u9700\u8981\u573a\u666f\u6574\u4f53\u7406\u89e3\u7684\u5b50\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u7ea647%\uff08\u6d89\u53ca3\u9879\u4efb\u52a1\uff09\u3002\u8fd9\u9a8c\u8bc1\u4e86\u591a\u89c6\u89d2\u76f8\u7ed3\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4fbf\u643a\u8bbe\u5907\u91c7\u96c6\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "MV-UMI\u6846\u67b6\u4e0d\u4ec5\u663e\u8457\u6269\u5c55\u4e86\u624b\u6301\u6293\u53d6\u5668\u53ef\u5b66\u4e60\u7684\u64cd\u4f5c\u4efb\u52a1\u8303\u56f4\uff0c\u4e5f\u4fdd\u7559\u4e86\u5176\u8de8\u8eab\u4f53\u7ed3\u6784\u7684\u6cdb\u5316\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8303\u5f0f\u3002"}}
{"id": "2509.18658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u7528\u4e8e\u5206\u6790LLM-as-a-judge\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\uff0c\u53ef\u4e3aLLM\u8bc4\u5206\u63d0\u4f9b\u7f6e\u4fe1\u533a\u95f4\uff0c\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8bc4\u4f30\uff08LLM-as-a-judge\uff09\uff0c\u4f46\u8bc4\u4f30\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u5f71\u54cd\u5176\u5b9e\u7528\u6027\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4fdd\u5e8f\u5f0f\u9884\u6d4b\uff08conformal prediction\uff09\u751f\u6210\u8fde\u7eed\u7684\u8bc4\u5206\u533a\u95f4\uff0c\u5e76\u9488\u5bf9\u79bb\u6563\u8bc4\u5206\u4efb\u52a1\u63d0\u51fa\u6709\u5e8f\u8fb9\u754c\u8c03\u6574\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u533a\u95f4\u4e2d\u70b9\u5f97\u5206\u4f5c\u4e3a\u66f4\u4f4e\u504f\u5dee\u7684\u66ff\u4ee3\u65b9\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u5206\u63d0\u793a\u4f18\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u5206\u6790\u8868\u660e\uff0c\u4fdd\u5e8f\u5f0f\u9884\u6d4b\u65b9\u6cd5\u53ef\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u6709\u6548\u7684\u533a\u95f4\u8986\u76d6\u4fdd\u8bc1\u3002\u533a\u95f4\u4e2d\u70b9\u548c\u591a\u6b21\u63d0\u793a\u5e73\u5747\u6709\u52a9\u4e8e\u63d0\u5347\u8bc4\u4ef7\u8d28\u91cf\u3002", "conclusion": "\u5f15\u5165\u7f6e\u4fe1\u533a\u95f4\u548c\u65b0\u8bc4\u5206\u65b9\u6cd5\u540e\uff0cLLM-as-a-judge\u7684\u8bc4\u4f30\u53d8\u5f97\u66f4\u53ef\u9760\uff0c\u5177\u5907\u63a8\u5e7f\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18405", "abs": "https://arxiv.org/abs/2509.18405", "authors": ["Sourav Halder", "Jinjun Tong", "Xinyu Wu"], "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models", "comment": "12 pages, 5 figures, 2 tables", "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u987b\u8bad\u7ec3\u7684\u652f\u7968\u5173\u952e\u5b57\u6bb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u652f\u7968\u91cd\u8981\u4fe1\u606f\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u6709\u6548\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u652f\u7968\u5728\u91d1\u878d\u9886\u57df\u4f9d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4e5f\u5e38\u56e0\u5173\u952e\u5b57\u6bb5\u88ab\u4f2a\u9020\u800c\u6210\u4e3a\u6b3a\u8bc8\u5bf9\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u548c\u5b9a\u4f4d\u652f\u7968\u4e0a\u7684\u5173\u952e\u5b57\u6bb5\uff08\u5982\u7b7e\u540d\u3001MICR\u7801\u3001\u91d1\u989d\u7b49\uff09\u3002\u7136\u800c\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u56e0\u9690\u79c1\u548c\u4e13\u6709\u6027\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7ed3\u5408\u7684\u65e0\u8bad\u7ec3\u5168\u65b0\u67b6\u6784\uff0c\u5b9e\u73b0\u652f\u7968\u5404\u5173\u952e\u5b57\u6bb5\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u80fd\u591f\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u9645\u90e8\u7f72\u95e8\u69db\u3002", "result": "\u5728110\u4efd\u6db5\u76d6\u591a\u79cd\u683c\u5f0f\u7684\u652f\u7968\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5f88\u5f3a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u91d1\u878d\u4e1a\u52a1\u573a\u666f\u4e0b\u5177\u6709\u6781\u5927\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e0d\u4ec5\u80fd\u591f\u72ec\u7acb\u8fdb\u884c\u5b57\u6bb5\u68c0\u6d4b\uff0c\u8fd8\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u52a0\u901f\u540e\u7eed\u66f4\u4e13\u4e1a\u5316\u6a21\u578b\u7684\u8fed\u4ee3\u5f00\u53d1\u3002"}}
{"id": "2509.18778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u65b0\u89c6\u89c9\u7f16\u7801\u7b56\u7565VGGT-DP\uff0c\u878d\u5408\u4e863D\u51e0\u4f55\u5148\u9a8c\u548c\u673a\u68b0\u81c2\u81ea\u8eab\u611f\u77e5\u4fe1\u606f\uff0c\u5728\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b56\u7565\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u7ed3\u6784\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u53d7\u9650\u3002\u56e0\u6b64\u6709\u5fc5\u8981\u501f\u9274\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\uff0c\u5c06\u89c6\u89c9\u4e0e\u81ea\u8eab\u611f\u77e5\u4fe1\u53f7\u7ed3\u5408\uff0c\u5b9e\u73b0\u66f4\u5065\u58ee\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51faVGGT-DP\u6846\u67b6\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u76843D\u611f\u77e5\u6a21\u578b\u63d0\u53d6\u7684\u51e0\u4f55\u5148\u9a8c\uff08\u4f7f\u7528VGGT\u89c6\u89c9\u7f16\u7801\u5668\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u8eab\u611f\u77e5\u4fe1\u53f7\u5f15\u5bfc\u89c6\u89c9\u5b66\u4e60\uff0c\u5b9e\u73b0\u611f\u77e5\u548c\u5185\u90e8\u72b6\u6001\u5bf9\u9f50\u3002\u8bbe\u8ba1\u4e86\u5e27\u7ea7token\u590d\u7528\u548c\u968f\u673atoken\u526a\u679d\u673a\u5236\u63d0\u5347\u6548\u7387\u53ca\u9c81\u68d2\u6027\u3002", "result": "\u5728MetaWorld\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cVGGT-DP\u5728\u7cbe\u5ea6\u8981\u6c42\u9ad8\u548c\u957f\u65f6\u95f4\u63a7\u5236\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8eDP\u3001DP3\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u878d\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u81ea\u8eab\u611f\u77e5\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7f16\u7801\u7ed3\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cbe\u5ea6\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u81ea\u4e3b\u5b66\u4e60\u4e0e\u63a7\u5236\u5f00\u8f9f\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.18713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMemOrb\u7684\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u5c42\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5ba2\u670d\u9886\u57df\u4e2d\u7684\u591a\u8f6e\u5bf9\u8bdd\u6210\u529f\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5ba2\u670d\u4ee3\u7406\u5b58\u5728\u9057\u5fd8\u3001\u91cd\u590d\u9519\u8bef\u3001\u7f3a\u4e4f\u6301\u7eed\u81ea\u6211\u63d0\u5347\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u9700\u8981\u7a33\u5b9a\u548c\u4e00\u81f4\u6027\u7684\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002", "method": "\u63d0\u51fa\u4e86MemOrb verbal reinforcement memory layer\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u603b\u7ed3\u4e3a\u7b56\u7565\u53cd\u601d\u5e76\u5b58\u50a8\u5728\u5171\u4eab\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u8c03\u7528\u65f6\u65e0\u9700\u518d\u5bf9\u6a21\u578b\u5fae\u8c03\uff0c\u901a\u8fc7\u68c0\u7d22\u53cd\u601d\u5185\u5bb9\u8f85\u52a9\u51b3\u7b56\u3002", "result": "MemOrb\u4f7f\u591a\u8f6e\u5bf9\u8bdd\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe63\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u91cd\u590d\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u673a\u5236\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5ba2\u670d\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6539\u5584\u6027\u80fd\u3002"}}
{"id": "2509.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18425", "abs": "https://arxiv.org/abs/2509.18425", "authors": ["Philip Wootaek Shin", "Jack Sampson", "Vijaykrishnan Narayanan", "Andres Marquez", "Mahantesh Halappanavar"], "title": "Losing the Plot: How VLM responses degrade on imperfect charts", "comment": null, "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.", "AI": {"tldr": "\u672c\u6587\u8bc4\u6d4b\u4e86\u9876\u5c16\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5e94\u5bf9\u5e26\u566a\u58f0\u3001\u906e\u6321\u4ee5\u53ca\u590d\u6742\u63a8\u7406\u9700\u6c42\u7684\u73b0\u5b9e\u56fe\u8868\u95ee\u9898\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u5e76\u516c\u5f00\u4e86\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u65b0\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\u8bc4\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7684\u8bc4\u6d4b\u57fa\u51c6\u5927\u591a\u57fa\u4e8e\u5e72\u51c0\u7684\u56fe\u8868\u548c\u7eaf\u4e8b\u5b9e\u7c7b\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e2d\u56fe\u8868\u5931\u771f\u3001\u906e\u6321\u548c\u590d\u6742\u63a8\u7406\u7684\u573a\u666f\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u65f6\u8868\u73b0\u4e0d\u53ef\u9760\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\u5f0f\u9519\u8bef\u3002", "method": "\u4f5c\u8005\u8bc4\u4f30\u4e86ChatGPT 4o\u3001Claude Sonnet 4\u3001Gemini 2.5 Pro\u5728\u5931\u771f\u3001\u906e\u6321\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faCHART NOISe\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u56fe\u8868\u6270\u52a8\u3001\u591a\u9879\u9009\u62e9\u9898\u4e0e\u9006\u5411\u4e00\u81f4\u6027\u6d4b\u8bd5\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u8d28\u91cf\u8fc7\u6ee4\u548c\u906e\u6321\u68c0\u6d4b\u7b49\u7f13\u89e3\u5bf9\u7b56\u3002", "result": "\u5728\u56fe\u8868\u635f\u574f\u6216\u906e\u6321\u540e\uff0c\u4e09\u5927\u6a21\u578b\u8868\u73b0\u5747\u5927\u5e45\u4e0b\u964d\uff0c\u5404\u7c7b\u5e7b\u89c9\uff08\u5982\u6570\u636e\u7f16\u9020\u3001\u8d8b\u52bf\u8bef\u89e3\u3001\u5bf9\u8c61\u6df7\u6dc6\uff09\u589e\u591a\u4e14\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u3002CHART NOISe\u6570\u636e\u96c6\u66f4\u80fd\u66b4\u9732\u51fa\u8fd9\u4e9b\u6a21\u578b\u5728\u73b0\u5b9e\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u6027\u8106\u5f31\u6027\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u56fe\u8868\u7406\u89e3\u5b58\u5728\u91cd\u5927\u8584\u5f31\u73af\u8282\uff0cCHART NOISe\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u65b0\u578b\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u52a0\u5065\u58ee\u4e0e\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u3002"}}
{"id": "2509.18786", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5GP-CA\uff0c\u6709\u6548\u5b9a\u91cf\u5e76\u89e3\u91ca\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u53d1\u73b0\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff08\u5982ICP\uff09\u5728\u9047\u5230\u4f20\u611f\u5668\u566a\u58f0\u3001\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u4ee5\u53ca\u906e\u6321\u5bfc\u81f4\u7684\u90e8\u5206\u91cd\u53e0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\u4e14\u96be\u4ee5\u89e3\u91ca\u51fa\u9519\u539f\u56e0\u3002\u56e0\u800c\u9700\u8981\u53d1\u5c55\u540c\u65f6\u53ef\u5b9a\u91cf\u4e0d\u786e\u5b9a\u6027\u5e76\u89e3\u91ca\u8bef\u5dee\u6765\u6e90\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u9ad8\u65af\u8fc7\u7a0b\u6982\u5ff5\u5f52\u56e0\uff08GP-CA\uff09\u65b9\u6cd5\uff0c\u80fd\u591f\u91cf\u5316\u70b9\u4e91\u914d\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u80fd\u89e3\u91ca\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u662f\u7531\u54ea\u4e9b\u5df2\u77e5\u8bef\u5dee\u6765\u6e90\u4ea7\u751f\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\uff0c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u901a\u8fc7\u67e5\u8be2\u4fe1\u606f\u91cf\u5927\u7684\u5b9e\u4f8b\u4e3b\u52a8\u53d1\u73b0\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cGP-CA\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u4e3b\u52a8\u5b66\u4e60\u4e0b\u7684\u6837\u672c\u6548\u7387\u4ee5\u53ca\u914d\u51c6\u7cbe\u5ea6\u65b9\u9762\u5747\u8d85\u8fc7\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u8bbe\u8ba1\u5408\u7406\u6027\u3002", "conclusion": "GP-CA\u4e0d\u4ec5\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\u7684\u6765\u6e90\u63d0\u4f9b\u4e86\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u7684\u9c81\u68d2\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u652f\u6301\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u884c\u4e3a\u3002"}}
{"id": "2509.18722", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86LOTUSDIS\uff0c\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6cf0\u8bed\u4f1a\u8bae\u8bed\u6599\u5e93\uff0c\u4e13\u4e3a\u63a8\u52a8\u8fdc\u573a\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u800c\u8bbe\u8ba1\u3002\u901a\u8fc7\u5bf9\u6bd4Whisper\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u8bc1\u5b9e\u8fdc\u8ddd\u79bb\u548c\u771f\u5b9e\u73af\u5883\u4e0b\u8bed\u97f3\u8bc6\u522b\u7684\u96be\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u8be5\u8bed\u6599\u5e93\u5fae\u8c03\u540e\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709ASR\u6a21\u578b\uff0c\u7279\u522b\u662f\u9762\u5411\u6cf0\u8bed\u548c\u8fdc\u573a\u73af\u5883\u7684\u516c\u5f00\u8bed\u6599\u8d44\u6e90\u7f3a\u4e4f\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u5728\u8fdc\u8ddd\u79bb\u3001\u771f\u5b9e\u573a\u666f\u4e0b\u6548\u679c\u5927\u5e45\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8be5\u9886\u57df\u6570\u636e\u7a7a\u767d\uff0c\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u6cf0\u8bed\u8fdc\u573aASR\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b114\u5c0f\u65f6\u81ea\u7136\u5bf9\u8bdd\u3001\u5927\u91cf\u91cd\u53e0\u7684\u6cf0\u8bed\u4f1a\u8bae\u6570\u636e\u96c6\uff0c\u8986\u76d6\u516d\u79cd\u7c7b\u578b\u3001\u4e0d\u540c\u8ddd\u79bb\u7684\u4e5d\u4e2a\u5f55\u97f3\u8bbe\u5907\uff0c\u4fdd\u7559\u4e86\u6df7\u54cd\u3001\u566a\u58f0\u7b49\u771f\u5b9e\u539f\u58f0\u7279\u5f81\u3002\u4f5c\u8005\u63d0\u4f9b\u89c4\u8303\u7684\u6570\u636e\u5212\u5206\u3001\u57fa\u7ebf\u6a21\u578b\u53ca\u516c\u5f00\u8bad\u7ec3\u8bc4\u6d4b\u811a\u672c\uff0c\u5e76\u7528Whisper\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6761\u4ef6\u4e0b\u8fdb\u884c\u6548\u679c\u6d4b\u8bd5\u3002", "result": "Whisper\u7b49\u73b0\u6210\u6a21\u578b\u5728\u8fdc\u573a\u6761\u4ef6\u4e0b\u8bc6\u522b\u6548\u679c\u660e\u663e\u4e0b\u964d\u3002\u901a\u8fc7\u5728LOTUSDIS\u8bed\u6599\u5e93\u4e0a\u5fae\u8c03\u540e\uff0cWhisper\u6a21\u578b\u6574\u4f53WER\u753164.3\u964d\u81f338.3\uff0c\u8fdc\u573aWER\u753181.6\u964d\u81f349.5\uff0c\u5c24\u5176\u5728\u6700\u8fdc\u8ddd\u79bb\u9ea6\u514b\u98ce\u4e0a\u8fdb\u6b65\u6700\u5927\u3002", "conclusion": "\u591a\u6837\u5316\u8ddd\u79bb\u548c\u8bbe\u5907\u7684\u771f\u5b9e\u8bed\u6599\u5bf9\u4e8e\u63d0\u5347ASR\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002LOTUSDIS\u8bed\u6599\u5e93\u4ee5\u53ca\u57fa\u7ebf\u7cfb\u7edf\u7684\u5f00\u653e\u53d1\u5e03\u5c06\u5927\u5927\u4fc3\u8fdb\u6cf0\u8bed\u4ee5\u53ca\u8fdc\u573aASR\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.18427", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.18427", "abs": "https://arxiv.org/abs/2509.18427", "authors": ["Xinyang Wu", "Muheng Li", "Xia Li", "Orso Pusterla", "Sairos Safai", "Philippe C. Cattin", "Antony J. Lomax", "Ye Zhang"], "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction", "comment": null, "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b4D-MRI\u91cd\u5efa\u65b9\u6cd5\uff0c\u5c06\u547c\u5438\u8fd0\u52a8\u89c6\u4e3a\u75311D\u4fe1\u53f7\u5f15\u5bfc\u7684\u8fde\u7eed\u5f62\u53d8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u67094D-MRI\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u6563\u5206\u7bb1\u6216\u6a21\u677f\uff0c\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u53d8\u5316\uff0c\u6d41\u7a0b\u590d\u6742\u4e14\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u653e\u7597\u9700\u6c42\u3002\u6025\u9700\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u6d41\u7a0b\u7b80\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u534f\u4f5c\u7f51\u7edc\uff1a\u7a7a\u95f4\u89e3\u5256\u7f51\u7edc\uff08SAN\uff09\u5b66\u4e60\u8fde\u7eed\u4e09\u7ef4\u89e3\u5256\u7ed3\u6784\uff0c\u65f6\u95f4\u8fd0\u52a8\u7f51\u7edc\uff08TMN\uff09\u7ed3\u5408Transformer\u63d0\u53d6\u7684\u547c\u5438\u4fe1\u53f7\uff0c\u751f\u6210\u65f6\u5e8f\u4e00\u81f4\u7684\u5f62\u53d8\u573a\uff0c\u5b9e\u73b0\u65e0\u6a21\u677f\u3001\u65e0\u76f8\u4f4d\u5206\u7bb1\u7684\u76f4\u63a54D-MRI\u91cd\u5efa\u3002", "result": "\u572819\u540d\u81ea\u7531\u547c\u5438\u5fd7\u613f\u8005\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u51c6\u786e\u6355\u6349\u5230\u4e86\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7684\u547c\u5438\u6a21\u5f0f\uff0c\u5e76\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u7684\u8840\u7ba1\u53ca\u652f\u6c14\u7ba1\u8fde\u7eed\u6027\u3002\u91cd\u5efa\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u603b\u5904\u7406\u65f6\u95f4\u4ece\u4f20\u7edf\u76845\u5c0f\u65f6\u51cf\u5c11\u81f315\u5206\u949f\u8bad\u7ec3\uff0c\u6bcf3D\u4f53\u79ef\u63a8\u65ad\u53ea\u97001\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u91cd\u5efa\u4efb\u610f\u547c\u5438\u72b6\u6001\u4e0b\u76843D\u56fe\u50cf\uff0c\u5e76\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u57284D\u653e\u7597\u8ba1\u5212\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u6cbb\u7597\u4e2d\u7684\u5de8\u5927\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18793", "categories": ["cs.RO", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u548cROS 2\u7684\u9700\u6c42\u9a71\u52a8\u578b\u5e94\u7528\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7ba1\u7406\u5927\u578b\u534f\u540c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08C-ITS\uff09\u4e2d\u7684\u5fae\u670d\u52a1\u5e94\u7528\uff0c\u5b9e\u73b0\u6309\u9700\u81ea\u52a8\u5316\u90e8\u7f72\u4e0e\u8d44\u6e90\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u548c\u4e92\u8054\u5316\u7684\u63d0\u5347\uff0cC-ITS\u7cfb\u7edf\u548c\u4e91\u7aef\u670d\u52a1\u5bf9\u5fae\u670d\u52a1\u4e0e\u5bb9\u5668\u7f16\u6392\u7b49\u4e91\u539f\u751f\u6280\u672f\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u4f46C-ITS\u5927\u89c4\u6a21\u73af\u5883\u4e0b\uff0c\u7531\u4e8e\u52a8\u6001\u7279\u6027\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u7684\u9700\u6c42\uff0c\u5982\u4f55\u6709\u6548\u7ba1\u7406\u548c\u8c03\u5ea6\u5e94\u7528\u7a0b\u5e8f\u6210\u4e3a\u96be\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u5229\u7528Kubernetes\u8fdb\u884c\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u81ea\u52a8\u5904\u7406\u5e94\u7528\u7684\u90e8\u7f72\u3001\u91cd\u6784\u3001\u5347\u7ea7\u548c\u4f38\u7f29\uff0c\u9488\u5bf9C-ITS\u4e2d\u7684\u591a\u5143\u5316\u9700\u6c42\uff0c\u52a8\u6001\u8c03\u548c\u65b0\u8001\u9700\u6c42\uff0c\u5229\u7528Kubernetes\u4e0eROS 2\u6784\u5efa\u7ba1\u7406\u6846\u67b6\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728C-ITS\u96c6\u4f53\u73af\u5883\u611f\u77e5\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u8fd0\u884c\u6548\u679c\uff0c\u5e76\u5c06\u539f\u578b\u4ee3\u7801\u5f00\u6e90\uff0c\u8bc1\u660e\u4e86\u6309\u9700\u7ba1\u7406\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u7f51\u7edc\u6d41\u91cf\u3002", "conclusion": "\u57fa\u4e8eKubernetes\u548cROS 2\u7684\u9700\u6c42\u9a71\u52a8\u578b\u5e94\u7528\u7ba1\u7406\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5e94\u5bf9C-ITS\u73af\u5883\u672c\u8eab\u7684\u52a8\u6001\u7279\u6027\uff0c\u4f18\u5316\u8d44\u6e90\u914d\u7f6e\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u4e0e\u4e92\u8054\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.18742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDyGRASP\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548c\u65f6\u5e8fGNN\uff0c\u6709\u6548\u5904\u7406\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\uff08DyTAG\uff09\u4e2d\u7684\u65f6\u5e8f\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u73b0\u6709GNN\u548cLLM\u65b9\u6cd5\u4e3b\u8981\u805a\u7126\u4e8e\u9759\u6001\u56fe\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u8282\u70b9\u6587\u672c\u7684\u8fd1\u671f\u548c\u5168\u5c40\u65f6\u5e8f\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u76f4\u63a5\u7528LLM\u5904\u7406\u52a8\u6001\u56fe\u7684\u5927\u91cf\u53d8\u52a8\u6587\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faDyGRASP\u65b9\u6cd5\uff0c\u521b\u65b0\u6027\u5730\u7ed3\u5408LLM\u4e0e\u65f6\u5e8fGNN\u3002\u5229\u7528\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u53ca\u8282\u70b9\u4e2d\u5fc3\u7684\u9690\u5f0f\u63a8\u7406\uff0c\u6355\u6349\u8282\u70b9\u7684\u8fd1\u671f\u8bed\u4e49\uff1b\u518d\u901a\u8fc7\u5b9a\u5236\u5316\u63d0\u793a\u548cRNN\u94fe\u5f0f\u7ed3\u6784\u663e\u5f0f\u5efa\u6a21\u8282\u70b9\u7684\u5168\u5c40\u8bed\u4e49\u6f14\u53d8\uff0c\u5e76\u5c06\u4e24\u7c7b\u65f6\u5e8f\u8bed\u4e49\u4e0e\u52a8\u6001\u56fe\u7ed3\u6784\u901a\u8fc7\u66f4\u65b0\u548c\u878d\u5408\u5c42\u7d27\u5bc6\u6574\u5408\u3002", "result": "\u5728\u591a\u4e2aDyTAG\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cDyGRASP\u5728\u76ee\u6807\u8282\u70b9\u68c0\u7d22\u4efb\u52a1\u7684Hit@10\u6307\u6807\u4e0a\u63d0\u5347\u6700\u591a\u8fbe34%\uff0c\u5c55\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002\u540c\u65f6DyGRASP\u5728\u4e0d\u540c\u7684\u65f6\u5e8fGNN\u548cLLM\u67b6\u6784\u4e0b\u5747\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DyGRASP\u80fd\u9ad8\u6548\u4e14\u6709\u6548\u5730\u878d\u5408\u8fd1\u671f\u4e0e\u5168\u5c40\u65f6\u5e8f\u8bed\u4e49\uff0c\u5b9e\u73b0\u5bf9\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5148\u8fdb\u63a8\u7406\uff0c\u6709\u671b\u63a8\u5e7f\u81f3\u591a\u7c7b\u73b0\u5b9e\u52a8\u6001\u56fe\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2509.18451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18451", "abs": "https://arxiv.org/abs/2509.18451", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony Maida"], "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects", "comment": null, "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u5148\u8fdb\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u9ad8\u901f\u5c0f\u7269\u4f53\uff08\u5982\u58c1\u7403\uff09\u8ddf\u8e2a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u8fd9\u79cd\u96be\u9898\u65f6\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff0c\u8bef\u5dee\u8f83\u5927\u3002", "motivation": "\u5728\u4f53\u80b2\u673a\u5668\u4eba\u7b49\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u7cbe\u786e\u4e14\u5feb\u901f\u5730\u8ddf\u8e2a\u9ad8\u901f\u5c0f\u578b\u7269\u4f53\uff08\u5982\u58c1\u7403\uff09\u5bf9\u63d0\u5347\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u89c4\u5212\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u4f46\u73b0\u6709\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u65b9\u6848\uff08\u5982\u57fa\u4e8eKalman\u6ee4\u6ce2\u7684\u65b9\u6cd5\uff09\u5728\u9762\u5bf9\u4e0d\u89c4\u5219\u8fd0\u52a8\u7684\u5feb\u901f\u5c0f\u7269\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u800c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u5176\u9002\u7528\u6027\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u4f5c\u8005\u81ea\u5efa\u4e86\u5305\u542b1\u4e07\u5e27\u6807\u6ce8\u58c1\u7403\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u4e94\u79cd\u4e3b\u6d41\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u8ddf\u8e2a\u7b97\u6cd5\uff08OCSORT\u3001DeepOCSORT\u3001ByteTrack\u3001BoTSORT\u3001StrongSORT\uff09\u5728\u63a8\u7406\u901f\u5ea6\u3001\u56fe\u50cf\u66f4\u65b0\u9891\u7387\u7b49\u6307\u6807\u4e0b\u8fdb\u884c\u591a\u573a\u666f\u5b9e\u9a8c\u8bc4\u6d4b\uff0c\u901a\u8fc7ADE\uff08\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\uff09\u7b49\u6307\u6807\u5206\u6790\u5176\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDeepOCSORT\u7684\u5e73\u5747ADE\u6700\u4f4e\u4e3a31.15\u50cf\u7d20\uff0cByteTrack\u901f\u5ea6\u6700\u5feb\uff0826.6ms/\u5e27\uff09\uff0c\u4f46\u603b\u4f53\u4e0a\u6240\u6709\u65b9\u6cd5\u5747\u5b58\u57283-11cm\u7684\u7a7a\u95f4\u8ddf\u8e2a\u6f02\u79fb\uff0cADE\u4e3a31-114\u50cf\u7d20\uff0c\u8bef\u5dee\u660e\u663e\u9ad8\u4e8e\u4e00\u822c\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u8f83\u5927\u7684\u5c40\u9650\u3002", "conclusion": "\u73b0\u6709\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u9ad8\u901f\u5c0f\u7269\u4f53\u7684\u4e0d\u89c4\u5219\u8fd0\u52a8\uff0c\u8bef\u5dee\u7387\u8fdc\u9ad8\u4e8e\u5e38\u89c4\u8ddf\u8e2a\u4efb\u52a1\uff0c\u4e9f\u9700\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u8be5\u7c7b\u5e94\u7528\u573a\u666f\u7684\u6539\u8fdb\u7b97\u6cd5\u3002"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u65b0\u578b\u67d4\u6027\u8d34\u5408\u7684\u76ae\u80a4\u72b6\u7535\u5bb9\u4f20\u611f\u5668\uff0c\u53ef\u4e3a\u673a\u5668\u624b\u6307\u8868\u9762\u63d0\u4f9b\u9ad8\u7075\u654f\u5ea6\u548c\u6613\u6821\u51c6\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e0b\u7684\u5b66\u4e60\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u76ae\u80a4\u80fd\u5728\u5927\u8303\u56f4\u5185\u9ad8\u6548\u5730\u611f\u77e5\u63a5\u89e6\uff0c\u4e3a\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u89e6\u89c9\u53cd\u9988\uff0c\u800c\u5982\u4f55\u8ba9\u673a\u5668\u4eba\u83b7\u5f97\u7c7b\u4f3c\u7684\u9ad8\u5206\u8fa8\u7387\u3001\u5168\u9762\u4e14\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u4e00\u76f4\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89e6\u89c9\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u5236\u9020\u4e86DexSkin\uff0c\u8fd9\u662f\u4e00\u79cd\u67d4\u8f6f\u3001\u53ef\u8d34\u5408\u5404\u79cd\u51e0\u4f55\u5f62\u72b6\u7684\u7535\u5bb9\u578b\u7535\u5b50\u76ae\u80a4\uff0c\u53ef\u5305\u8986\u5728\u5939\u722a\u624b\u6307\u8868\u9762\uff0c\u5b9e\u73b0\u8fd1\u5168\u8986\u76d6\u7684\u89e6\u89c9\u4f20\u611f\u3002\u5b9e\u9a8c\u91c7\u7528\u4e86\u5939\u722a\u673a\u5668\u4eba\uff0c\u901a\u8fc7DexSkin\u91c7\u96c6\u7684\u89e6\u89c9\u4fe1\u606f\uff0c\u7ed3\u5408\u4ece\u6f14\u793a\u5b66\u4e60\u53ca\u5f3a\u5316\u5b66\u4e60\uff0c\u5b8c\u6210\u5982\u7269\u4f53\u5728\u624b\u4e2d\u91cd\u5b9a\u4f4d\u3001\u6a61\u76ae\u7b4b\u7ed5\u76d2\u5b50\u7b49\u4efb\u52a1\uff0c\u5e76\u68c0\u9a8c\u4e86\u4f20\u611f\u5668\u7684\u6821\u51c6\u80fd\u529b\u53ca\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u3002", "result": "DexSkin\u80fd\u9ad8\u7075\u654f\u5730\u63a2\u6d4b\u5e76\u5b9a\u4f4d\u624b\u6307\u8868\u9762\u7684\u89e6\u89c9\u4e8b\u4ef6\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u4e0b\u7684\u64cd\u4f5c\u8868\u73b0\u3002\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u591a\u4f20\u611f\u5668\u5b9e\u4f8b\u4e4b\u95f4\u7684\u6a21\u578b\u8fc1\u79fb\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DexSkin\u5728\u590d\u6742\u64cd\u4f5c\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "DexSkin\u6781\u5927\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5939\u722a\u7684\u89e6\u89c9\u8986\u76d6\u4e0e\u5206\u8fa8\u80fd\u529b\uff0c\u4fbf\u4e8e\u6821\u51c6\u548c\u6a21\u578b\u8fc1\u79fb\uff0c\u9002\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\uff0c\u5bf9\u5b9e\u73b0\u9ad8\u96be\u5ea6\u3001\u63a5\u89e6\u5bc6\u96c6\u578b\u7684\u73b0\u5b9e\u64cd\u4f5c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.18750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5b50\u8bcd\u5206\u8bcd\u5668\u4e2d\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u5171\u4eabToken\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u8bed\u8a00\u95f4\u7684Token\u91cd\u53e0\u6709\u52a9\u4e8e\u63d0\u5347\u8de8\u8bed\u8a00\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u5728\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u4ea7\u751f\u91cd\u53e0Token\uff0c\u4f46\u8fd9\u4e9b\u91cd\u53e0\u5230\u5e95\u662f\u4fc3\u8fdb\u8fd8\u662f\u963b\u788d\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u76ee\u524d\u5b66\u672f\u754c\u5c1a\u65e0\u5b9a\u8bba\uff0c\u4e14\u65e9\u671f\u7814\u7a76\u56e0\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u540c\u7ed3\u8bba\u4e0d\u4e00\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\u6df1\u5165\u63a2\u7a76\u5176\u4f5c\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5728\u4e0d\u540c\u8bed\u8a00\u5bf9\u4e0a\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u8c03\u6574\u8bcd\u6c47\u91cd\u53e0\u5ea6\uff0c\u6bd4\u8f83\u6709\u65e0\u91cd\u53e0\u53ca\u4e0d\u540c\u91cd\u53e0\u7a0b\u5ea6\u4e0b\u6a21\u578b\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u8003\u5bdf\u4e86\u8bcd\u6c47\u91cd\u53e0\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u5bf9\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u9690\u8868\u793a\uff0c\u4ee5\u53ca\u5728XNLI\u548cXQuAD\u6570\u636e\u96c6\u4e0a\u7684\u8fc1\u79fb\u4efb\u52a1\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4efb\u4f55\u7a0b\u5ea6\u7684Token\u91cd\u53e0\u90fd\u80fd\u8ba9\u6a21\u578b\u5b66\u5230\u80fd\u6355\u6349\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u7cfb\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u65e0\u91cd\u53e0\u7684\u6a21\u578b\u5219\u8fd9\u79cd\u6548\u679c\u5f88\u5f31\u3002\u5b9e\u6d4b\u8868\u660e\uff0c\u6709\u91cd\u53e0\u7684\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u4efb\u52a1(XNLI/XQuAD)\u4e0a\u660e\u663e\u4f18\u4e8e\u65e0\u91cd\u53e0\u6a21\u578b\uff0c\u4e14\u91cd\u53e0\u7a0b\u5ea6\u8d8a\u9ad8\uff0c\u8fc1\u79fb\u6027\u80fd\u8d8a\u597d\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u662f\uff1a\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u8f83\u9ad8\u7684Token\u91cd\u53e0\u4e0e\u8f83\u5927\u7684\u5171\u4eab\u8bcd\u8868\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u4efb\u52a1\u6709\u663e\u8457\u5e2e\u52a9\u3002\u8fd9\u4e3a\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5efa\u8bae\u4f18\u5148\u8003\u8651\u5927\u8bcd\u8868\u548c\u9ad8\u91cd\u53e0\u65b9\u6848\u3002"}}
{"id": "2509.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop\u662f\u4e00\u79cd\u5229\u7528\u89c6\u9891\u538b\u7f29\u57df\u8fd0\u52a8\u77e2\u91cf\u5b9e\u73b0\u9ad8\u6548\u52a8\u4f5c\u8bc6\u522b\u7684\u8fd0\u52a8\u611f\u77e5\u81ea\u9002\u5e94\u88c1\u526a\u6a21\u5757\uff0c\u53ef\u4ee5\u63d0\u5347\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u6613\u4e8e\u96c6\u6210\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u5728\u538b\u7f29\u57df\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u51c6\u786e\u7387\u4e0e\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u6d01\u3001\u9ad8\u6548\u4e14\u6613\u7528\u7684\u88c1\u526a\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "MoCrop\u901a\u8fc7\u4f7f\u7528H.264\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u77e2\u91cf\uff0c\u81ea\u52a8\u68c0\u6d4b\u8fd0\u52a8\u5bc6\u96c6\u533a\u57df\uff0c\u7136\u540e\u5bf9\u6240\u6709I\u5e27\u505a\u7edf\u4e00\u88c1\u526a\uff0c\u8fc7\u7a0b\u5305\u542b\u53bb\u566a\u4e0e\u5408\u5e76\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u3001\u81ea\u9002\u5e94\u88c1\u526a\u7b49\u6b65\u9aa4\u3002\u4e0d\u5f15\u5165\u65b0\u53c2\u6570\u3001\u65e0\u9700\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u5404\u7c7b\u4e3b\u6d41\u7f51\u7edc\u4e2d\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\uff0cMoCrop\u914d\u5408ResNet-50\u4f7fTop-1\u51c6\u786e\u7387\u63d0\u53473.5%\u4e14\u8ba1\u7b97\u91cf\u4e0d\u53d8\uff0c\u6216\u5728\u51cf\u5c1126.5% FLOPs\u65f6\u63d0\u53472.4%\uff1b\u7528\u4e8eCoViAR\u6846\u67b6\u65f6\uff0c\u4ee5\u76f8\u540c\u8ba1\u7b97\u6210\u672c\u8fbe\u523089.2%\u51c6\u786e\u7387\uff0c\u964d\u81f38.5 GFLOPs\u65f6\u4e5f\u670988.5%\u3002\u5bf9MobileNet-V3\u3001EfficientNet-B1\u3001Swin-B\u7b49\u4e3b\u5e72\u7f51\u7edc\u5747\u6709\u663e\u8457\u589e\u76ca\u3002", "conclusion": "MoCrop\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u81ea\u9002\u5e94\u88c1\u526a\uff0c\u5728\u591a\u79cd\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u5e26\u6765\u51c6\u786e\u7387\u63d0\u5347\u5e76\u51cf\u5c11\u8ba1\u7b97\uff0c\u9002\u5408\u538b\u7f29\u57df\u7684\u5b9e\u65f6\u5e94\u7528\u4e14\u4f7f\u7528\u95e8\u69db\u4f4e\u3002"}}
{"id": "2509.18865", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u4fe1\u606f\u7684\u53cc\u5411\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff08Bi-VLA\uff09\uff0c\u80fd\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5904\u7406\u591a\u9879\u4efb\u52a1\uff0c\u5e76\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u5411\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u7cbe\u5bc6\u64cd\u4f5c\u4e0a\u867d\u7cbe\u786e\uff0c\u4f46\u6bcf\u9879\u4efb\u52a1\u9700\u5355\u72ec\u5efa\u6a21\uff0c\u6cdb\u5316\u6027\u548c\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u5f31\u3002", "method": "\u6574\u5408\u673a\u5668\u4eba\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u3001\u529b\u77e9\u7b49\u6570\u636e\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528SigLIP\u548c\u57fa\u4e8eFiLM\u7684\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u6280\u672f\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u673a\u5668\u4eba\u4f20\u611f\u5668\u6570\u636e\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u7406\u89e3\u4e0e\u64cd\u4f5c\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cBi-VLA\u6a21\u578b\u65e0\u8bba\u5728\u9700\u8981\u989d\u5916\u8bed\u8a00\u63d0\u793a\u8fd8\u662f\u4ec5\u9760\u89c6\u89c9\u5373\u53ef\u533a\u5206\u7684\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u878d\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "conclusion": "Bi-VLA\u7a81\u7834\u4e86\u4ee5\u5f80\u53ea\u80fd\u5355\u4efb\u52a1\u5904\u7406\u7684\u74f6\u9888\uff0c\u63d0\u4f9b\u4e86\u89c6\u89c9\u2014\u8bed\u8a00\u878d\u5408\u63d0\u5347\u4efb\u52a1\u591a\u6837\u5316\u4e0e\u9002\u5e94\u80fd\u529b\u7684\u5b9e\u8bc1\uff0c\u9a8c\u8bc1\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2509.18762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\uff08SFT\uff09\u9636\u6bb5\uff0c\u957f\u6587\u672c\u6570\u636e\u5bf9\u77ed\u6587\u672c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002\u4f5c\u8005\u53d1\u73b0\uff0c\u7528\u957f\u6587\u672c\u8fdb\u884cSFT\u53cd\u800c\u80fd\u63d0\u5347\u77ed\u6587\u672c\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u6ce8\u610f\u529b\u548c\u524d\u9988\u7f51\u7edc\u7684\u72ec\u7acb\u53ca\u534f\u540c\u4f5c\u7528\u3002\u6700\u540e\uff0c\u63d0\u51fa\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u4ee5\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u7684\u589e\u957f\uff0cLLM\u9700\u8981\u5904\u7406\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u76ee\u524d\u901a\u8fc7\u7ee7\u7eed\u9884\u8bad\u7ec3\u548c\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u9002\u5e94\u957f\u6587\u672c\u5df2\u6210\u4e3a\u5e38\u89c4\u505a\u6cd5\uff0c\u4f46SFT\u65f6\u6570\u636e\u957f\u5ea6\u5bf9\u6700\u7ec8\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u8fd8\u4e0d\u6e05\u695a\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u6df1\u5165\u7406\u89e3SFT\u6570\u636e\u957f\u5ea6\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u7814\u7a76SFT\u9636\u6bb5\u6587\u672c\u957f\u5ea6\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u77ed\u6587\u672c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u5206\u6790\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u548c\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u7684\u4f5c\u7528\uff0c\u8fdb\u4e00\u6b65\u7814\u7a76\u5b83\u4eec\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u77e5\u8bc6\u504f\u597d\u504f\u7f6e\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u957f\u6587\u672cSFT\u53ef\u63d0\u5347\u77ed\u6587\u672c\u4efb\u52a1\u8868\u73b0\uff0c\u4e0e\u957f\u6587\u672c\u7ee7\u7eed\u9884\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u7ed3\u679c\u76f8\u53cd\u3002MHA\u548cFFN\u90fd\u80fd\u72ec\u7acb\u53d7\u76ca\u4e8e\u957f\u6587\u672cSFT\u3002\u540c\u65f6\u6307\u51fa\u957f\u6587\u672cSFT\u66f4\u503e\u5411\u4e8e\u5b66\u4e60\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u77ed\u6587\u672cSFT\u504f\u5411\u53c2\u6570\u6027\u77e5\u8bc6\uff0c\u4ec5\u7528\u957f\u6587\u672cSFT\u5e76\u975e\u6700\u4f18\u3002", "conclusion": "\u957f\u6587\u672cSFT\u5bf9\u77ed\u6587\u672c\u4efb\u52a1\u6709\u610f\u60f3\u4e0d\u5230\u7684\u79ef\u6781\u4f5c\u7528\uff0c\u4f46\u5b58\u5728\u77e5\u8bc6\u504f\u597d\u504f\u7f6e\u3002\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u53ef\u4ee5\u7f13\u89e3\u8fd9\u79cd\u504f\u7f6e\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.18481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18481", "abs": "https://arxiv.org/abs/2509.18481", "authors": ["Xinyu Wang", "Zikun Zhou", "Yingjian Li", "Xin An", "Hongpeng Wang"], "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems", "comment": null, "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u7801\u672c\u7684\u81ea\u9002\u5e94\u7279\u5f81\u538b\u7f29\u6846\u67b6\uff08CAFC-SE\uff09\uff0c\u7528\u4e8e\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u4e3a\u673a\u5668\u5206\u6790\u9ad8\u6548\u7f16\u7801\u56fe\u50cf\uff0c\u80fd\u5728\u8fb9\u7f18-\u4e91\u534f\u540c\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u548c\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u7f16\u7801\u53ca\u5206\u6790\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u8868\u73b0\u8f83\u5dee\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4fdd\u7559\u4e86\u5197\u4f59\u7ec6\u8282\u6216\u5b66\u5230\u8fc7\u4e8e\u96c6\u4e2d\u7684\u7b26\u53f7\u5206\u5e03\u3002\u4e3a\u89e3\u51b3\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u7684\u5206\u6790\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u5728\u5145\u5206\u538b\u7f29\u7684\u540c\u65f6\u4ecd\u5177\u5907\u8f83\u5f3a\u5206\u6790\u80fd\u529b\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7801\u672c\u7684\u81ea\u9002\u5e94\u7279\u5f81\u538b\u7f29\u4e0e\u8bed\u4e49\u589e\u5f3a\u6846\u67b6\uff08CAFC-SE\uff09\u3002\u8be5\u6846\u67b6\u5229\u7528\u77e2\u91cf\u91cf\u5316\uff08VQ\uff09\u5c06\u8fde\u7eed\u7684\u89c6\u89c9\u7279\u5f81\u5728\u8fb9\u7f18\u4fa7\u6620\u5c04\u4e3a\u79bb\u6563\u7684\u7801\u672c\u7d22\u5f15\uff0c\u5e76\u6709\u9009\u62e9\u5730\u5c06\u8fd9\u4e9b\u7d22\u5f15\u4f20\u8f93\u5230\u4e91\u7aef\u3002VQ\u64cd\u4f5c\u628a\u7279\u5f81\u5411\u91cf\u6620\u5c04\u5230\u6700\u8fd1\u7684\u89c6\u89c9\u539f\u8bed\uff0c\u4f7f\u5f97\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u4f9d\u7136\u80fd\u4fdd\u7559\u66f4\u591a\u6709\u6548\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7801\u7387\u548c\u5206\u6790\u51c6\u786e\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5c24\u5176\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5206\u6790\u6027\u80fd\u3002", "conclusion": "CAFC-SE\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u7684\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u5c0f\u6570\u636e\u4f20\u8f93\u91cf\u548c\u66f4\u9ad8\u7684\u4e91\u7aef\u5206\u6790\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18-\u4e91\u534f\u540c\u7684\u9ad8\u6548\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2509.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u673a\u5668\u4eba\u624b\u90e8\u7ed3\u6784\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u52a1\u76f8\u5173\u673a\u5668\u624b\u5f62\u6001\u7684\u591a\u6837\u6027\u3001\u9488\u5bf9\u6027\u548c3D\u53ef\u6253\u5370\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u624b\u53d7\u9650\u4e8e\u4e13\u5bb6\u7ecf\u9a8c\u548c\u624b\u52a8\u8c03\u53c2\uff0c\u81ea\u52a8\u4f18\u5316\u65b9\u6cd5\u53c8\u4f9d\u8d56\u8ba1\u7b97\u548c\u4eff\u771f\uff0c\u96be\u4ee5\u9762\u5411\u9ad8\u7075\u5de7\u9700\u6c42\u3002\u5982\u4f55\u9ad8\u6548\u3001\u667a\u80fd\u5730\u4e3a\u5177\u4f53\u4efb\u52a1\u8bbe\u8ba1\u5408\u9002\u7684\u673a\u5668\u4eba\u624b\u6210\u4e3a\u96be\u9898\u3002", "method": "\u63d0\u51faLang2Morph\u7ba1\u9053\uff0c\u5206\u4e3a\u4e24\u6b65\uff1a1\uff09\u6839\u636e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\uff0cLLM\u751f\u6210\u8bed\u4e49\u6807\u7b7e\u3001\u7ed3\u6784\u6587\u6cd5\u53ca\u517c\u5bb9\u53c2\u6570\uff0c\u5f62\u6210\u521d\u6b65\u8bbe\u8ba1\uff1b2\uff09\u5bf9\u5019\u9009\u65b9\u6848\u8fdb\u884c\u8bed\u4e49\u548c\u5c3a\u5bf8\u7b5b\u9009\u3001\u5982\u6709\u5fc5\u8981\u518d\u7531LLM\u7ec6\u5316\u3002\u8bbe\u8ba1\u7ed3\u679c\u53ef\u5b9e\u73b03D\u6253\u5370\u3002", "result": "\u5b9e\u9a8c\u5c55\u793aLang2Morph\u80fd\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u751f\u6210\u591a\u6837\u4e14\u5951\u5408\u4efb\u52a1\u9700\u6c42\u7684\u673a\u5668\u4eba\u624b\u5f62\u6001\u3002\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u5907\u901a\u7528\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "conclusion": "Lang2Morph\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u9a71\u52a8\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u4e3a\u7075\u5de7\u673a\u5668\u4eba\u624b\u5b9a\u5236\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u517c\u5177\u521b\u65b0\u6027\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u752810-K\u8d22\u62a5\u6587\u4ef6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u7cfb\u7edf\u5316\u63d0\u53d6\u516c\u53f8\u95f4\u7684\u98ce\u9669\u5173\u8054\u5173\u7cfb\uff0c\u5e76\u53d6\u5f97\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u516c\u53f8\u95f4\u98ce\u9669\u5173\u8054\u7684\u8bc6\u522b\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\u548c\u4eba\u5de5\u5206\u6790\uff0c\u4e3b\u89c2\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u5df2\u65e0\u6cd5\u6ee1\u8db3\u6295\u8d44\u7ba1\u7406\u7b49\u5b9e\u9645\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5ba2\u89c2\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u4ee510-K\u8d22\u62a5\u4e3a\u6570\u636e\u6e90\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u65e0\u76d1\u7763\u5fae\u8c03\uff0c\u5bf9\u6587\u672c\u4e2d\u7684\u65f6\u95f4\u548c\u8bcd\u6c47\u6a21\u5f0f\u8fdb\u884c\u5efa\u6a21\uff0c\u8bad\u7ec3\u51fa\u91d1\u878d\u9886\u57df\u4e13\u7528\u7684\u7f16\u7801\u5668\u4ee5\u6df1\u5ea6\u6355\u6349\u516c\u53f8\u95f4\u9690\u542b\u7684\u98ce\u9669\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u53ef\u91cf\u5316\u7684\u98ce\u9669\u5173\u7cfb\u8bc4\u5206\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e10-K\u6587\u4ef6\u548cNLP\u7684\u4ea4\u53c9\u5e94\u7528\u4e0d\u4ec5\u63d0\u5347\u4e86\u516c\u53f8\u95f4\u98ce\u9669\u5173\u7cfb\u7684\u8bc6\u522b\u6548\u7387\u548c\u5ba2\u89c2\u6027\uff0c\u4e5f\u4e3a\u91d1\u878d\u7814\u7a76\u548c\u6295\u8d44\u51b3\u7b56\u5de5\u5177\u7684\u81ea\u52a8\u5316\u5347\u7ea7\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18493", "abs": "https://arxiv.org/abs/2509.18493", "authors": ["Md Mostafijur Rahman", "Radu Marculescu"], "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation", "comment": "11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD", "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86MK-UNet\uff0c\u4e00\u79cd\u8d85\u8f7b\u91cf\u591a\u6838U\u578b\u7f51\u7edc\uff08\u4ec50.316M\u53c2\u6570, 0.314G FLOPs\uff09\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u5206\u5272\u533b\u5b66\u56fe\u50cf\uff0c\u5e76\u5728\u516d\u4e2a\u6570\u636e\u96c6\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5f80\u5f80\u7cbe\u5ea6\u9ad8\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5982\u5e8a\u65c1\u8bbe\u5907\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u591a\u6838\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u6a21\u5757(MKDC)\uff0c\u7ed3\u5408\u901a\u9053\u3001\u7a7a\u95f4\u548c\u5206\u7ec4\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728U\u578b\u7ed3\u6784\u4e2d\u591a\u5206\u8fa8\u7387\u7279\u5f81\u7684\u9ad8\u6548\u63d0\u53d6\u3002\u6574\u4f53\u7ed3\u6784\u5927\u4e3a\u7cbe\u7b80\uff0c\u663e\u8457\u964d\u4f4e\u53c2\u6570\u4e0e\u8ba1\u7b97\u6d88\u8017\u3002", "result": "\u5728\u516d\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u51c6\u4e0a\uff0cMK-UNet\u660e\u663e\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5982TransUNet\uff08\u53c2\u6570\u3001FLOPs\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\uff09\u3001UNeXt\uff08DICE\u63d0\u53476.7%\uff0c\u53c2\u6570\u51cf\u5c114.7\u500d\uff09\uff0c\u540c\u65f6\u8d85\u8d8aMedT\u3001CMUNeXt\u7b49\u8f7b\u91cf\u6a21\u578b\u3002", "conclusion": "MK-UNet\u5b9e\u73b0\u4e86\u8d85\u8f7b\u91cf\u3001\u4f4e\u7b97\u529b\u6d88\u8017\u4e0e\u9ad8\u5206\u5272\u7cbe\u5ea6\u7684\u5b8c\u7f8e\u517c\u987e\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u65f6\u533b\u5b66\u8bca\u65ad\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2509.18953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Eva-VLA\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u5b9e\u9645\u7269\u7406\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u4e3b\u6d41VLA\u6a21\u578b\u5728\u591a\u79cd\u771f\u5b9e\u7269\u7406\u6270\u52a8\u4e0b\u6781\u6613\u5931\u6548\u3002", "motivation": "\u5c3d\u7ba1VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u4e14\u591a\u53d8\u7269\u7406\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u8fd8\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5b58\u5728\u6a21\u578b\u5b9e\u9a8c\u5ba4\u6709\u6548\u4f46\u90e8\u7f72\u4e0d\u7406\u60f3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Eva-VLA\u7edf\u4e00\u8bc4\u6d4b\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\uff08\u5982\u4e09\u7ef4\u53d8\u6362\u3001\u5149\u7167\u53d8\u5316\u3001\u5bf9\u6297\u6027\u8865\u4e01\uff09\u8f6c\u5316\u4e3a\u53ef\u8fde\u7eed\u4f18\u5316\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u7528\u9ed1\u76d2\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\u7cfb\u7edf\u6c42\u89e3\u6700\u574f\u60c5\u5f62\u3002\u6b64\u5916\uff0c\u6846\u67b6\u652f\u6301\u591a\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u52a8\u53d1\u73b0\u9ad8\u5931\u6548\u7387\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41VLA\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff1a\u4e09\u79cd\u7c7b\u578b\u7684\u7269\u7406\u53d8\u5316\u5747\u53ef\u5bfc\u81f4\u6a21\u578b\u5931\u6548\u7387\u8d85\u8fc760%\uff0c\u5176\u4e2d\u957f\u65f6\u4efb\u52a1\u4e2d\u5bf9\u8c61\u53d8\u6362\u5bfc\u81f4\u7684\u5931\u6548\u7387\u9ad8\u8fbe97.8%\u3002", "conclusion": "\u5f53\u524dVLA\u6a21\u578b\u5b58\u5728\u4ece\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u5411\u771f\u5b9e\u90e8\u7f72\u7684\u663e\u8457\u9c81\u68d2\u6027\u7f3a\u9677\u3002Eva-VLA\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4ef7\u4e0e\u6539\u8fdb\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u5bf9\u63d0\u5347\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.18776", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86AECBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5efa\u7b51\u3001\u5de5\u7a0b\u4e0e\u65bd\u5de5\uff08AEC\uff09\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5f53\u524dLLM\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u548c\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u5e94\u7528\u4e8eAEC\u9886\u57df\uff0c\u4f46\u8be5\u9886\u57df\u5bf9\u5b89\u5168\u548c\u4e13\u4e1a\u6027\u8981\u6c42\u6781\u9ad8\uff0c\u76ee\u524dLLMs\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u5c1a\u672a\u88ab\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b23\u9879\u5178\u578b\u4efb\u52a1\u7684\u4e94\u5c42\u6b21\u8ba4\u77e5\u8bc4\u4f30\u4f53\u7cfb\uff08\u6db5\u76d6\u77e5\u8bc6\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a8\u7406\u3001\u8ba1\u7b97\u4e0e\u5e94\u7528\uff09\uff0c\u5e76\u7531\u5de5\u7a0b\u5e08\u6839\u636e\u5b9e\u9645\u5de5\u4f5c\u5236\u4f5c\u4e864800\u9053\u591a\u7c7b\u578b\u9898\u76ee\uff0c\u901a\u8fc7\u53cc\u8f6e\u4e13\u5bb6\u8bc4\u5ba1\u4fdd\u969c\u6570\u636e\u8d28\u91cf\u3002\u5f15\u5165LLM\u81ea\u52a8\u88c1\u5224\u673a\u5236\uff0c\u4f7f\u7528\u4e13\u5bb6\u8bc4\u4ef7\u6807\u51c6\u5bf9\u590d\u6742\u7b54\u6848\u8fdb\u884c\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u8bc4\u5206\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e86\u4e5d\u79cd\u4e3b\u6d41LLM\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1aLLMs\u5728\u77e5\u8bc6\u8bb0\u5fc6\u548c\u7406\u89e3\u5c42\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c4\u8303\u8868\u683c\u4fe1\u606f\u89e3\u8bfb\u3001\u590d\u6742\u63a8\u7406\u4e0e\u8ba1\u7b97\u3001\u4e13\u4e1a\u6587\u6863\u751f\u6210\u7b49\u9ad8\u7ea7\u4efb\u52a1\u663e\u8457\u5931\u5206\u3002\u8fd9\u79cd\u80fd\u529b\u9010\u6b65\u9012\u51cf\u5728\u4e94\u4e2a\u8ba4\u77e5\u5c42\u6b21\u4e2d\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u5f53\u524dLLM\u5c1a\u672a\u80fd\u80dc\u4efbAEC\u9886\u57df\u9ad8\u9636\u8ba4\u77e5\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u573a\u666f\uff0c\u4f46AECBench\u4e3a\u672a\u6765\u6a21\u578b\u80fd\u529b\u63d0\u5347\u548c\u53ef\u9760\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u57fa\u7840\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.18501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u79d1\u5bfc\u822a\u65b9\u6cd5BridgeSplat\uff0c\u901a\u8fc7\u7ed3\u5408\u672f\u4e2d3D\u91cd\u5efa\u548c\u672f\u524dCT\u6570\u636e\uff0c\u5b9e\u73b0\u624b\u672f\u89c6\u9891\u4e0e\u4f53\u79ef\u60a3\u8005\u6570\u636e\u4e4b\u95f4\u7684\u6865\u63a5\u3002\u8be5\u65b9\u6cd5\u5229\u75283D\u9ad8\u65af\u5206\u5e03\u5bf9CT\u7f51\u683c\u8fdb\u884c\u914d\u51c6\u548c\u8054\u5408\u4f18\u5316\uff0c\u5b9e\u73b0CT\u7684\u5408\u7406\u53d8\u5f62\u4ee5\u9002\u5e94\u5b9e\u9645\u624b\u672f\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u732a\u7684\u5185\u810f\u624b\u672f\u548c\u4eba\u7c7b\u809d\u810f\u6a21\u62df\u6570\u636e\u4e0a\u80fd\u591f\u6709\u6548\u5730\u7528\u5355\u76eeRGB\u89c6\u9891\u6307\u5bfc\u672f\u524dCT\u53d8\u5f62\u3002", "motivation": "\u5916\u79d1\u624b\u672f\u5e38\u5e38\u9700\u8981\u5c06\u672f\u4e2d\u5b9e\u65f6\u5f71\u50cf\u4e0e\u672f\u524d\u7684CT\u7b49\u4f53\u79ef\u6570\u636e\u5bf9\u9f50\uff0c\u4f46\u7531\u4e8e\u7ec4\u7ec7\u53d8\u5f62\u7b49\u56e0\u7d20\uff0c\u4e8c\u8005\u96be\u4ee5\u76f4\u63a5\u5bf9\u5e94\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u548c\u8fd8\u539f\u624b\u672f\u8fc7\u7a0b\u4e2d\u5b9e\u9645\u53d1\u751f\u7684\u5f62\u53d8\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u548c\u914d\u51c6\u624b\u672f\u8fc7\u7a0b\u4e2d\u76843D\u91cd\u5efa\u548c\u672f\u524dCT\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u5916\u79d1\u5bfc\u822a\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "BridgeSplat\u65b9\u6cd5\u901a\u8fc7\u5c063D\u9ad8\u65af\u5206\u5e03\u4e0eCT\u7f51\u683c\u8282\u70b9\u7ed1\u5b9a\uff0c\u5e76\u5bf9\u9ad8\u65af\u53c2\u6570\u548c\u7f51\u683c\u53d8\u5f62\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u6bcf\u4e2a\u9ad8\u65af\u5206\u5e03\u90fd\u6839\u636e\u5176\u7236\u7f51\u683c\u4e09\u89d2\u5f62\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u5149\u5ea6\u76d1\u7763\u4fdd\u8bc1\u9ad8\u65af\u5206\u5e03\u4e0e\u7f51\u683c\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u80fd\u5c06\u624b\u672f\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5f62\u6709\u6548\u5730\u4f20\u56de\u66f4\u65b0CT\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u5355\u76eeRGB\u89c6\u9891\u76d1\u7763\u4e0b\u76843D\u7ed3\u6784\u53d8\u5f62\u3002", "result": "\u4f5c\u8005\u5728\u732a\u7684\u5185\u810f\u624b\u672f\u6570\u636e\u4ee5\u53ca\u4eba\u7c7b\u809d\u810f\u7684\u6a21\u62df\u6570\u636e\u4e0a\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5355\u76eeRGB\u6570\u636e\u5b9e\u73b0\u672f\u524dCT\u7684\u5408\u7406\u53d8\u5f62\uff0c\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u7b26\u5408\u5b9e\u9645\u751f\u7406\u7ed3\u6784\u53d8\u5316\u7684\u6548\u679c\u3002", "conclusion": "BridgeSplat\u65b9\u6cd5\u6709\u6548\u586b\u8865\u4e86\u672f\u4e2d\u89c6\u9891\u5f71\u50cf\u4e0e\u672f\u524dCT\u6570\u636e\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u4e86\u5f62\u53d8\u611f\u77e5\u7684\u5b9e\u65f6\u5916\u79d1\u5bfc\u822a\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u7684\u51c6\u786e\u5ea6\u548c\u4e34\u5e8a\u9002\u5e94\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u4eca\u540e\u624b\u672f\u5bfc\u822a\u6280\u672f\u53d1\u5c55\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18954", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5728\u65e0\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4bICP\u914d\u51c6\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u6fc0\u5149\u96f7\u8fbe\u5b9a\u4f4d\u548cSLAM\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709ICP\u7b97\u6cd5\u5728\u7279\u5f81\u7a00\u758f\u6216\u52a8\u6001\u573a\u666f\u4e0b\u6613\u51fa\u9519\uff0c\u4e14\u914d\u51c6\u8bef\u5dee\u7684\u4e0d\u786e\u5b9a\u6027\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u4eba\u5de5\u6a21\u578b\u6216\u8fc7\u4e8e\u7b80\u5355\u7684\u5047\u8bbe\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5c40\u90e8\u53ef\u5b9a\u4f4d\u6027\u4f30\u8ba1\u65b9\u6cd5\u9650\u4e8e\u5730\u56fe\u4f9d\u8d56\u6216\u53ea\u505a\u4e8c\u5206\u7c7b\uff0c\u65e0\u6cd5\u7ec6\u81f4\u523b\u753b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u5730\u56fe\u6761\u4ef6\u4e0b\u5bf9ICP\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\u7684\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u4e3a\u6bcf\u5e27\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u9884\u6d4b\u53ef\u9760\u76846\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u5e76\u4fbf\u4e8e\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u3001\u51c6\u786e\u9884\u6d4bICP\u914d\u51c6\u534f\u65b9\u5dee\uff0c\u5e76\u5728\u6709\u5730\u56fe\u5b9a\u4f4d\u6216SLAM\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u5b9a\u4f4d\u8bef\u5dee\u3001\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u5148\u9a8c\u5730\u56fe\uff0c\u53ef\u4e3aICP\u63d0\u4f9b\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6fc0\u5149\u96f7\u8fbe\u5b9a\u4f4d\u548cSLAM\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u673a\u68b0\u89e3\u91ca\u6027\u65b9\u6cd5\u201c\u6a21\u578b\u5dee\u5206(model diffing)\u201d\u5206\u6790\u4e86\u7cbe\u8c03\u8fc7\u7a0b\u4e2d\u5927\u6a21\u578b\u80fd\u529b\u7684\u5177\u4f53\u53d8\u5316\uff0c\u53d1\u73b0SimPO\u589e\u5f3a\u4e3b\u8981\u63d0\u5347\u4e86\u5b89\u5168\u3001\u591a\u8bed\u79cd\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u800c\u81ea\u6307\u548c\u5e7b\u89c9\u7ba1\u7406\u80fd\u529b\u6709\u6240\u51cf\u5f31\u3002\u6a21\u578b\u5dee\u5206\u6cd5\u63ed\u793a\u4e86\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u4f53\u73b0\u7684\u80fd\u529b\u5c42\u9762\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u5fae\u8c03(fine-tuning)\u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u4e3b\u8981\u624b\u6bb5\uff0c\u4f46\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u96be\u4ee5\u89e3\u91ca\u6a21\u578b\u95f4\u5dee\u5f02\u7684\u5177\u4f53\u539f\u56e0\uff0c\u6545\u9700\u91c7\u7528\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\u7406\u89e3\u8fd9\u4e9b\u53d8\u5316\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e86\u673a\u68b0\u89e3\u91ca\u65b9\u6cd5\u4e2d\u7684\u6a21\u578b\u5dee\u5206(model diffing)\u7b56\u7565\uff0c\u5e76\u5229\u7528crosscoders\u5b9a\u4f4d\u548c\u5f52\u7eb3\u4e86\u4e24\u4e2a\u6a21\u578b\uff08Gemma-2-9b-it\u4e0eSimPO\u589e\u5f3a\u7248\uff09\u5728\u6f5c\u5728\u8868\u5f81\u4e0a\u7684\u533a\u522b\u3002", "result": "SimPO\u8bad\u7ec3\u4e3b\u8981\u589e\u5f3a\u4e86\u5b89\u5168(+32.8%)\u3001\u591a\u8bed(+43.8%)\u548c\u6307\u4ee4\u7406\u89e3(+151.7%)\u7b49\u6f5c\u5728\u6982\u5ff5\u80fd\u529b\uff0c\u4f46\u524a\u5f31\u4e86\u6a21\u578b\u81ea\u6307(-44.1%)\u548c\u5e7b\u89c9\u7ba1\u7406(-68.5%)\u7b49\u80fd\u529b\u3002", "conclusion": "\u6a21\u578b\u5dee\u5206\u65b9\u6cd5\u80fd\u63d0\u4f9b\u6bd4\u6392\u884c\u699c\u6307\u6807\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u80fd\u529b\u5256\u6790\uff0c\u6709\u5229\u4e8e\u660e\u786e\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u7684\u673a\u5236\u6765\u6e90\uff0c\u4e3a\u5bf9\u6bd4\u4e0e\u4f18\u5316LLMs\u63d0\u4f9b\u900f\u660e\u548c\u6709\u9488\u5bf9\u6027\u7684\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2509.18502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u6846\u67b6\u2014\u2014\u6269\u6563\u5f15\u5bfc\u6807\u7b7e\u589e\u5f3a\uff08DGLE\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6e90\u4e0d\u53ef\u8fbe\u6761\u4ef6\u4e0b\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u9886\u57df\u81ea\u9002\u5e94\u6027\u80fd\u3002\u65b9\u6cd5\u4ece\u5c11\u91cf\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u51fa\u53d1\uff0c\u501f\u52a9\u6269\u6563\u6a21\u578b\u5c06\u5176\u4f20\u64ad\u81f3\u5b8c\u6574\u6807\u7b7e\u96c6\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u751f\u6210\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u9886\u57df\u81ea\u9002\u5e94\u7814\u7a76\u4e2d\uff0c\u6e90\u57df\u6570\u636e\u4e0d\u53ef\u5f97\uff08\u5373\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\uff0cSFDA\uff09\u7684\u5b9e\u9645\u573a\u666f\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u6709\u9650\u3002\u5e38\u7528\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u4f46\u76f4\u63a5\u4f18\u5316\u6574\u4e2a\u4f2a\u6807\u7b7e\u96c6\u5b58\u5728\u5927\u91cf\u566a\u58f0\uff0c\u964d\u4f4e\u4e86\u57df\u81ea\u9002\u5e94\u7684\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51faDGLE\u6846\u67b6\u3002\u9996\u5148\uff0c\u901a\u8fc7\u4fe1\u5fc3\u8fc7\u6ee4\u548c\u8d85\u5206\u8fa8\u589e\u5f3a\u8fdb\u884c\u4f2a\u6807\u7b7e\u878d\u5408\uff0c\u63d0\u53d6\u5c11\u91cf\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u79cd\u5b50\u3002\u63a5\u7740\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u53bb\u566a\u548c\u5206\u5e03\u5efa\u6a21\u80fd\u529b\uff0c\u5c06\u4e0d\u5b8c\u6574\u3001\u5206\u5e03\u4e0d\u89c4\u5219\u7684\u4f2a\u6807\u7b7e\u79cd\u5b50\u4f20\u64ad\u4e3a\u5b8c\u6574\u3001\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u96c6\uff0c\u4ece\u800c\u907f\u514d\u76f4\u63a5\u4f18\u5316\u542b\u566a\u58f0\u7684\u5168\u90e8\u4f2a\u6807\u7b7e\u3002", "result": "DGLE\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u57df\u4e0a\u7684\u5206\u5272\u6027\u80fd\u4f18\u4e8e\u4ee5\u5f80SFDA\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f2a\u6807\u7b7e\u566a\u58f0\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "DGLE\u4e3a\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u4e0e\u4f18\u5316\u65b0\u601d\u8def\uff0c\u4e3a\u5b9e\u9645\u9065\u611f\u56fe\u50cf\u9886\u57df\u81ea\u9002\u5e94\u4efb\u52a1\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5e94\u7528\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.18979", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7269\u4f53\u5f62\u72b6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u7684\u5feb\u901f\u5c40\u90e8\u6c42\u89e3\u5668\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u7c7b\u522b\u7ea7\u5bf9\u8c61\u5148\u9a8c\uff0c\u5e76\u53ef\u9ad8\u6548\u5224\u5b9a\u5168\u5c40\u6700\u4f18\u6027\u3002\u524d\u7aef\u901a\u8fc7\u5b66\u4e60\u7684\u65b9\u6cd5\u68c0\u6d4b\u5bf9\u8c61\u7684\u7a00\u758f\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u7ed3\u5408\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\uff0c\u5c06\u5f62\u72b6\u548c\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u5927\u540e\u9a8c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u81ea\u6d3d\u573a\u8fed\u4ee3\u9ad8\u6548\u6c42\u89e3\u3002\u5355\u6b21\u8fed\u4ee3\u7ea6 100 \u5fae\u79d2\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u6709\u6548\u6027\u4e0e\u901a\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u4f53\u5f62\u72b6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u5e38\u5e38\u4f9d\u8d56\u4e8e\u8f83\u591a\u5148\u9a8c\u4fe1\u606f\u6216\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u4f30\u8ba1\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u8868\u73b0\uff08\u5982\u64cd\u4f5c\u3001\u5bfc\u822a\u7b49\uff09\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u7c7b\u522b\u7ea7\u522b\u4fe1\u606f\u65f6\u3002", "method": "1. \u4f7f\u7528\u5b66\u4e60\u65b9\u6cd5\u4ece RGB-D \u56fe\u50cf\u4e2d\u68c0\u6d4b\u5bf9\u8c61\u7a00\u758f\u8bed\u4e49\u5173\u952e\u70b9\u30022. \u7528\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u76ee\u6807\u7269\u4f53\u672a\u77e5\u5f62\u72b6\u30023. \u8868\u8ff0\u4e3a\u5355\u4f4d\u56db\u5143\u6570\u4e0b\u7684\u4f4d\u7f6e\u3001\u671d\u5411\u548c\u5f62\u72b6\u8054\u5408\u6700\u5927\u540e\u9a8c\u4f18\u5316\u30024. \u5229\u7528\u81ea\u6d3d\u573a\u8fed\u4ee3\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u8fed\u4ee3\u6c42\u6700\u5c0f\u7279\u5f81\u503c-\u5411\u91cf\u5bf9\u7684\u5c0f\u578b\uff084x4\uff09\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u901f\u5ea6\u30025. \u901a\u8fc7\u89e3\u7ebf\u6027\u65b9\u7a0b\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u6027\u7684\u9ad8\u6548\u8bc1\u4e66\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6bcf\u6b21\u8fed\u4ee3\u53ea\u9700\u7ea6 100 \u5fae\u79d2\uff0c\u6781\u5927\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u5f02\u5e38\u70b9\u6392\u9664\u3002\u5728\u5408\u6210\u6570\u636e\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08\u5305\u62ec\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5168\u5c40\u6700\u4f18\u6027\u53ef\u5224\u5b9a\u7684\u5f62\u72b6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4e3a\u57fa\u4e8eRGB-D\u7684\u771f\u5b9e\u573a\u666f\u611f\u77e5\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6MAPEX\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65b9\u6cd5\u4ee5\u9002\u5e94\u4e0d\u540c\u957f\u5ea6\u6587\u672c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u4e00\u63a8\u7406\u6d41\u7a0b\uff0c\u4e0d\u8003\u8651\u6587\u6863\u957f\u5ea6\u53ca\u6a21\u578b\u5dee\u5f02\uff0c\u672a\u80fd\u5145\u5206\u6316\u6398\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u5b9e\u9645\u573a\u666f\u3002", "method": "\u63d0\u51faMAPEX\u6846\u67b6\uff0c\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5305\u62ec\u4e13\u5bb6\u53ec\u96c6\u3001\u5019\u9009\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u4e3b\u9898\u5f15\u5bfc\u3001\u77e5\u8bc6\u589e\u5f3a\u548c\u540e\u5904\u7406\u7b49\u6a21\u5757\u3002\u4f7f\u7528\u53cc\u8def\u5f84\u7b56\u7565\uff1a\u77ed\u6587\u672c\u91c7\u7528\u77e5\u8bc6\u9a71\u52a8\u63d0\u53d6\uff0c\u957f\u6587\u672c\u91c7\u7528\u4e3b\u9898\u5f15\u5bfc\u63d0\u53d6\uff0c\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u6587\u672c\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u79cd\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cMAPEX\u5728F1@5\u6307\u6807\u4e0a\u5e73\u5747\u8d85\u8d8a\u6700\u4f18\u65e0\u76d1\u7763\u65b9\u6cd52.44%\uff0c\u4f18\u4e8e\u6807\u51c6\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf4.01%\u3002", "conclusion": "MAPEX\u6846\u67b6\u80fd\u5145\u5206\u53d1\u6325\u5927\u8bed\u8a00\u6a21\u578b\u591a\u573a\u666f\u5173\u952e\u8bcd\u63d0\u53d6\u80fd\u529b\uff0c\u5177\u5907\u4f18\u826f\u7684\u6cdb\u5316\u6027\u548c\u901a\u7528\u6027\uff0c\u63a8\u52a8\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u6cd5\u7684\u8fdb\u6b65\u3002"}}
{"id": "2509.18504", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18504", "abs": "https://arxiv.org/abs/2509.18504", "authors": ["Jiaxin Dai", "Xiang Xiang"], "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning", "comment": null, "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5c42\u6b21\u5316\u6570\u636e\u7684\u8868\u793a\u80fd\u529b\uff0c\u63d0\u51fa\u5728Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL) \u4efb\u52a1\u4e2d\uff0c\u5c06\u7279\u5f81\u63d0\u53d6\u5d4c\u5165\u5230\u53cc\u66f2\u7a7a\u95f4\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5897\u91cf\u5b66\u4e60\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709C2FSCIL\u65b9\u6cd5\u5927\u591a\u5728\u6b27\u5f0f\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8868\u5f81\uff0c\u4f46\u6b27\u5f0f\u7a7a\u95f4\u5bf9\u5c42\u6b21\u5316\u7ed3\u6784\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u800c\u53cc\u66f2\u7a7a\u95f4\u5728\u5c42\u6b21\u7ed3\u6784\u8868\u793a\u4e0a\u5177\u6709\u5929\u7136\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5728C2FSCIL\u4e2d\u5f15\u5165\u53cc\u66f2\u7a7a\u95f4\u63d0\u5347\u6a21\u578b\u8868\u73b0\u53ca\u7406\u89e3\u201c\u7531\u7c97\u5230\u7ec6\u201d\u7684\u5b66\u4e60\u8303\u5f0f\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u5c06Knowe\u65b9\u6cd5\u6539\u8fdb\u4e3a\u5728Poincar\u00e9\u7403\u578b\u53cc\u66f2\u7a7a\u95f4\u5b9e\u73b0\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u66f2\u5bf9\u6bd4\u635f\u5931\u548c\u53cc\u66f2\u5168\u8fde\u63a5\u5c42\u6765\u9002\u914d\u53cc\u66f2\u7a7a\u95f4\u4f18\u5316\u4e0e\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u91c7\u7528\u6700\u5927\u71b5\u5206\u5e03\u5728\u53cc\u66f2\u7a7a\u95f4\u4f30\u7b97\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7279\u5f81\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e\u7279\u5f81\u589e\u5f3a\uff0c\u7f13\u89e3\u5c11\u6837\u672c\u8fc7\u62df\u5408\u3002", "result": "\u5728C2FSCIL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u53cc\u66f2\u7a7a\u95f4\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u5c06\u7279\u5f81\u63d0\u53d6\u53ca\u5206\u7c7b\u5c42\u5d4c\u5165\u5230\u53cc\u66f2\u7a7a\u95f4\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u7279\u5f81\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u5bf9C2FSCIL\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u53cc\u66f2\u7a7a\u95f4\u5728\u5c42\u6b21\u5316\u3001\u5c11\u6837\u672c\u589e\u91cf\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.19012", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u65b0\u8303\u5f0f\uff0c\u5bf9\u73b0\u6709\u6280\u672f\u4e0e\u672a\u6765\u8d70\u5411\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\u548c\u5f52\u7eb3\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u4efb\u52a1\u590d\u6742\u6027\u63d0\u5347\uff0c\u4f20\u7edf\u7b56\u7565\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u548c\u52a8\u6001\u73af\u5883\uff0cVLA\u6a21\u578b\u4f9d\u6258\u4e8e\u591a\u6a21\u6001\u80fd\u529b\uff0c\u6709\u671b\u5b9e\u73b0\u901a\u7528\u578b\u673a\u5668\u4eba\u667a\u80fd\u3002", "method": "\u6587\u7ae0\u5bf9VLA\u6a21\u578b\u7684\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff1a\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u53ca\u4e13\u7528\u65b9\u6cd5\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u5176\u6838\u5fc3\u7b56\u7565\u53ca\u5b9e\u9645\u5b9e\u73b0\uff0c\u540c\u65f6\u4ecb\u7ecd\u5173\u952e\u6570\u636e\u96c6\u3001\u8bc4\u6d4b\u57fa\u51c6\u548c\u4eff\u771f\u5e73\u53f0\u3002", "result": "\u672c\u7efc\u8ff0\u6574\u7406\u4e86\u4e09\u767e\u4f59\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u603b\u7ed3\u4e86VLA\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u5e76\u7ed8\u5236\u4e86\u5f53\u524d\u9886\u57df\u7684\u65b9\u6cd5\u7248\u56fe\u3002\u4f5c\u8005\u5206\u6790\u4e86\u5404\u7c7b\u8303\u5f0f\u7684\u4f18\u52a3\u3001\u6311\u6218\u53ca\u9002\u7528\u573a\u666f\u3002", "conclusion": "\u4f5c\u8005\u6307\u51faVLA\u6a21\u578b\u5728\u53ef\u6269\u5c55\u3001\u901a\u7528\u673a\u5668\u4eba\u667a\u80fd\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u7684\u6311\u6218\u548c\u6f5c\u529b\u65b9\u5411\u3002"}}
{"id": "2509.18843", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u4e86\u5f00\u6e90\u5927\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\uff08BioASQ 13B\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u5c01\u95ed\u6e90\u9876\u5c16\u6a21\u578b\uff08\u5982GPT-4o\u7b49\uff09\u5bf9\u6bd4\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u6548\u679c\u53ef\u4e0e\u5c01\u95ed\u6e90\u6a21\u578b\u5ab2\u7f8e\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u597d\u3002", "motivation": "\u8fd1\u5e74\u6765\u5f00\u653e\u6743\u91cd\u5927\u6a21\u578b\u8fdb\u6b65\u663e\u8457\uff0c\u4f46\u5176\u6027\u80fd\u80fd\u5426\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u751f\u7269\u533b\u5b66\u95ee\u7b54\uff09\u4e0a\u53d6\u4ee3\u66f4\u5927\u3001\u66f4\u5f3a\u7684\u5c01\u95ed\u6e90\u6a21\u578b\u5c1a\u672a\u660e\u4e86\u3002\u4f5c\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u8bc4\u6d4b\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u53c2\u4e0e\u4e86BioASQ 13B\u6311\u6218\uff0c\u6bd4\u8f83\u4e86\u591a\u4e2a\u5f00\u6e90\u5927\u6a21\u578b\u4e0e\u4e3b\u6d41\u5c01\u95ed\u6e90\u6a21\u578b\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u5305\u62ec\u68c0\u7d22\u6700\u76f8\u5173\u6587\u672c\u7247\u6bb5\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u53ca\u6a21\u578b\u96c6\u6210\u7b49\u65b9\u6cd5\u6765\u63d0\u5347\u95ee\u7b54\u6548\u679c\u3002\u90e8\u5206\u5b9e\u9a8c\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u6a21\u578b\u7684\u7b54\u6848\u8fdb\u4e00\u6b65\u63d0\u5347\u8868\u73b0\u3002", "result": "\u5f00\u6e90\u5927\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4e0e\u9876\u7ea7\u7684\u5c01\u95ed\u6e90\u5927\u6a21\u578b\u76f8\u5f53\u3002\u5728\u90e8\u5206\u4efb\u52a1\u548c\u6a21\u578b\u96c6\u6210\u60c5\u5883\u4e0b\uff0c\u5f00\u6e90\u6a21\u578b\u751a\u81f3\u8d85\u8d8a\u4e86\u95ed\u6e90\u5bf9\u624b\u3002", "conclusion": "\u7ecf\u8fc7\u591a\u79cd\u63d0\u5347\u7b56\u7565\u548c\u7cfb\u7edf\u6027\u5bf9\u6bd4\uff0c\u5f00\u6e90\u6743\u91cd\u5927\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e0a\u5df2\u5177\u6709\u4e0e\u5c01\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u751a\u81f3\u8d85\u8d8a\u7684\u80fd\u529b\uff0c\u8fd9\u4e3a\u5f00\u6e90\u6a21\u578b\u5728\u5b9e\u9645\u533b\u5b66AI\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\u3002"}}
{"id": "2509.18538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18538", "abs": "https://arxiv.org/abs/2509.18538", "authors": ["Zixin Zhu", "Haoxiang Li", "Xuelu Feng", "He Wu", "Chunming Qiao", "Junsong Yuan"], "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts", "comment": "Accepted as Spotlight at NeurIPS 2025", "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5b9e\u73b0\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5bf9\u8c61\u53ca\u5176\u56e0\u679c\u89c6\u89c9\u4f2a\u5f71\u7684\u9ad8\u8d28\u91cf\u53bb\u9664\uff0c\u5e76\u5728\u4e3b\u6d41\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u56fe\u50cf\u5916\u89c2\uff0c\u4e0d\u80fd\u6709\u6548\u53bb\u9664\u5982\u9634\u5f71\u3001\u53cd\u5c04\u7b49\u672a\u88ab\u660e\u786e\u63a9\u6a21\u5316\u7684\u56e0\u679c\u89c6\u89c9\u4f2a\u5f71\uff0c\u6216\u8005\u63a9\u6a21\u8303\u56f4\u4e0d\u7cbe\u51c6\u5bfc\u81f4\u53ef\u63a7\u6027\u5dee\u3001\u8bef\u5220\u5176\u4ed6\u5185\u5bb9\u3002\u8fd9\u662f\u56e0\u4e3a\u5ffd\u89c6\u4e86\u5bf9\u8c61\u51e0\u4f55\u5b58\u5728\u4e0e\u89c6\u89c9\u6548\u679c\u4e4b\u95f4\u7684\u56e0\u679c\u8054\u7cfb\u3002", "method": "\u4f5c\u8005\u5c06\u5bf9\u8c61\u53bb\u9664\u8fc7\u7a0b\u62c6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u51e0\u4f55\u4fe1\u606f\uff08\u5982\u6df1\u5ea6\uff09\u4e25\u683c\u6309\u63a9\u6a21\u76d1\u7763\u76f4\u63a5\u53bb\u9664\u76ee\u6807\u5bf9\u8c61\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u7684\u9ad8\u7cbe\u5ea6\u7f16\u8f91\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u66f4\u65b0\u540e\u7684\u51e0\u4f55\u6570\u636e\u6e32\u67d3\u51fa\u771f\u5b9e\u611fRGB\u56fe\u50cf\uff0c\u56e0\u679c\u4f2a\u5f71\u901a\u8fc73D\u51e0\u4f55\u7684\u53d8\u5316\u88ab\u9690\u5f0f\u5904\u7406\u3002\u4e3a\u51e0\u4f55\u79fb\u9664\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u6b63\u8d1f\u6837\u672c\u5bf9\u7684\u504f\u597d\u9a71\u52a8\u76ee\u6807\uff0c\u786e\u4fdd\u79fb\u9664\u5bf9\u8c61\u53ca\u5176\u56e0\u679c\u4f2a\u5f71\uff0c\u540c\u65f6\u907f\u514d\u5f15\u5165\u65b0\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u5bf9\u8c61\u548c\u56e0\u679c\u4f2a\u5f71\u53bb\u9664\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u611f\u77e5\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u5bf9\u8c61\u53ca\u4f2a\u5f71\u53bb\u9664\u4efb\u52a1\u7684\u53ef\u63a7\u6027\u4e0e\u771f\u5b9e\u611f\uff0c\u8d85\u8d8a\u73b0\u6709\u5916\u89c2\u9a71\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2509.19023", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08ROM-GRL\uff09\uff0c\u7528\u4e8e\u65e0\u9700\u52a8\u4f5c\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u7684\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u5b66\u4e60\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6b65\u6001\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4eba\u7c7b\u793a\u8303\uff08\u5982\u52a8\u4f5c\u6355\u6349\uff09\u6216\u7cbe\u7ec6\u8bbe\u8ba1\u5956\u52b1\uff0c\u4f46\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u6709\u5c40\u9650\u6027\uff1a\u524d\u8005\u6570\u636e\u83b7\u53d6\u6602\u8d35\uff0c\u540e\u8005\u96be\u4ee5\u83b7\u5f97\u81ea\u7136\u7a33\u5b9a\u7684\u884c\u4e3a\u3002\u4f5c\u8005\u65e8\u5728\u65e0\u9700\u5916\u90e8\u4eba\u7c7b\u6570\u636e\uff0c\u4ec5\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u9ad8\u6548\u3001\u81ea\u7136\u7684\u4eba\u5f62\u884c\u8d70\u7b56\u7565\u3002", "method": "ROM-GRL\u5206\u4e24\u6b65\uff1a\u7b2c\u4e00\u6b65\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3\u4e00\u4e2a4\u81ea\u7531\u5ea6\u7684\u7b80\u5316\u6a21\u578b\uff08ROM\uff09\uff0c\u83b7\u5f97\u80fd\u91cf\u9ad8\u6548\u7684\u6b65\u6001\u6a21\u677f\uff1b\u7b2c\u4e8c\u6b65\uff0c\u901a\u8fc7SAC\u7b97\u6cd5\u548c\u5bf9\u6297\u5224\u522b\u5668\u8bad\u7ec3\u5168\u8eab\u9ad8\u7ef4\u7b56\u7565\uff0c\u4f7f\u5176\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0eROM\u4e00\u81f4\uff0c\u5b9e\u73b0\u4ece\u4f4e\u7ef4\u6a21\u677f\u5230\u9ad8\u7ef4\u7b56\u7565\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u57281\u7c73/\u79d2\u548c4\u7c73/\u79d2\u4e24\u4e2a\u901f\u5ea6\u4e0b\uff0cROM-GRL\u8bad\u7ec3\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u7a33\u5b9a\u3001\u5bf9\u79f0\uff0c\u8fdc\u4f18\u4e8e\u5355\u7eaf\u5956\u52b1\u9a71\u52a8\uff08\u65e0ROM\u5f15\u5bfc\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "ROM-GRL\u6210\u529f\u5730\u5728\u65e0\u9700\u4eba\u5de5\u793a\u8303\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u7cbe\u7b80\u6a21\u578b\u7684\u4f18\u8d28\u6b65\u6001\u8fc1\u79fb\u5230\u9ad8\u7ef4\u5168\u8eab\u63a7\u5236\u4e2d\uff0c\u517c\u5177\u5956\u52b1\u9a71\u52a8\u548c\u4eff\u771f\u6a21\u4eff\u6cd5\u4f18\u52bf\uff0c\u80fd\u591f\u751f\u6210\u4eba\u6027\u5316\u3001\u7075\u6d3b\u7684\u6b65\u884c\u884c\u4e3a\u3002"}}
{"id": "2509.18862", "categories": ["cs.CL", "I.2.7; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u591a\u7279\u5f81\u96c6\u6210\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u5b9e\u9645\u6548\u76ca\uff0c\u53d1\u73b0\u4e0e\u5355\u4e00\u6a21\u578b\u76f8\u6bd4\uff0c\u591a\u7279\u5f81\u65b9\u6cd5\u4ec5\u5e26\u6765\u6781\u5c0f\u7cbe\u5ea6\u63d0\u5347\uff0c\u5374\u6709\u5de8\u5927\u8ba1\u7b97\u4ee3\u4ef7\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u8fdb\u6b65\u8fc5\u901f\uff0c\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u591a\u4ee5\u5355\u4e00\u795e\u7ecf\u6a21\u578b\u4e3a\u4e3b\uff0c\u4f46\u76f4\u89c9\u8ba4\u4e3a\u8bed\u4e49\u3001\u53e5\u6cd5\u3001\u7edf\u8ba1\u7b49\u591a\u79cd\u7279\u5f81\u7ed3\u5408\u5e94\u80fd\u4e92\u8865\uff0c\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u4e25\u683c\u9a8c\u8bc1\u591a\u5c42\u6b21\u7279\u5f81\u96c6\u6210\u7684\u6709\u6548\u6027\u4e0e\u6027\u4ef7\u6bd4\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86MHFD\uff08\u591a\u5c42\u6b21\u7279\u5f81\u68c0\u6d4b\uff09\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u57fa\u4e8eDeBERTa\u7684\u8bed\u4e49\u5206\u6790\u3001\u53e5\u6cd5\u89e3\u6790\u548c\u7edf\u8ba1\u6982\u7387\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728AI\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u591a\u7279\u5f81\u65b9\u6cd5\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u4ec5\u67090.4-0.5%\u7684\u589e\u76ca\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5374\u9ad8\u8fbe4.2\u500d\u3002\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u96c6\u4e0a\u7684\u6700\u4f73\u8868\u73b0\u4e3a\u4e1a\u5185\u9886\u5148\u768489.7%\uff08\u57df\u5185\uff09\u548c84.2%\uff08\u8de8\u57df\uff09\uff0c\u4f46\u6539\u8fdb\u5e45\u5ea6\u4ec5\u4e3a0.4-2.6%\u3002", "conclusion": "\u73b0\u4ee3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5df2\u80fd\u6709\u6548\u6355\u83b7AI\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u5927\u90e8\u5206\u5173\u952e\u4fe1\u53f7\uff0c\u591a\u7279\u5f81\u878d\u5408\u5bf9\u68c0\u6d4b\u63d0\u5347\u6709\u9650\uff0c\u6027\u4ef7\u6bd4\u6781\u4f4e\uff0c\u672a\u6765\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u5e94\u66f4\u591a\u5173\u6ce8\u6a21\u578b\u81ea\u8eab\u7ed3\u6784\u4f18\u5316\uff0c\u800c\u975e\u5355\u7eaf\u7279\u5f81\u5806\u53e0\u3002"}}
{"id": "2509.18546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18546", "abs": "https://arxiv.org/abs/2509.18546", "authors": ["Yujia Liu", "Dingquan Li", "Tiejun Huang"], "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models", "comment": null, "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u6a21\u578b\u7684\u9ad8\u8fc1\u79fb\u6027\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5 SEGA\uff0c\u5e76\u9a8c\u8bc1\u5176\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5bb9\u6613\u7a81\u7834\u672a\u77e5\u6a21\u578b\u3002", "motivation": "\u4ee5\u5f80\u9488\u5bf9NR-IQA\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u591a\u4e3a\u767d\u76d2\u573a\u666f\uff0c\u9ed1\u76d2\u73af\u5883\u4e0b\u653b\u51fb\u8fc1\u79fb\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u578b\u8106\u5f31\u6027\u5206\u6790\u4e0e\u9c81\u68d2\u6027\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86SEGA\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u5e73\u6ed1\u5e94\u7528\u4e8e\u6e90\u6a21\u578b\u68af\u5ea6\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u5e73\u6ed1\u68af\u5ea6\u6765\u8fd1\u4f3c\u76ee\u6807\u6a21\u578b\u68af\u5ea6\uff0c\u540c\u65f6\u52a0\u4e0a\u4e13\u95e8\u7684\u6270\u52a8\u6ee4\u6ce2\u906e\u7f69\uff0c\u4ee5\u786e\u4fdd\u6270\u52a8\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "result": "\u5728CLIVE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEGA\u5728\u9ed1\u76d2\u653b\u51fb\u4e2d\u7684\u8fc1\u79fb\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u6709\u6548\u5730\u653b\u51fb\u672a\u77e5NR-IQA\u6a21\u578b\u3002", "conclusion": "SEGA\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u5728NR-IQA\u6a21\u578b\u4e2d\u7684\u9ed1\u76d2\u8fc1\u79fb\u6027\uff0c\u6709\u52a9\u4e8e\u68c0\u6d4b\u6a21\u578b\u8106\u5f31\u6027\u5e76\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2509.19037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TacEva\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u89c6\u89c9\u578b\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u7684\u6027\u80fd\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u7684\u95ee\u9898\u3002", "motivation": "\u4e0d\u540c\u7684\u89c6\u89c9\u578b\u89e6\u89c9\u4f20\u611f\u5668\u5728\u611f\u77e5\u673a\u5236\u3001\u7ed3\u6784\u5c3a\u5bf8\u7b49\u65b9\u9762\u6709\u8f83\u5927\u5dee\u5f02\uff0c\u5bfc\u81f4\u5176\u6027\u80fd\u8868\u73b0\u53c2\u5dee\u4e0d\u9f50\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u963b\u788d\u4e86\u5176\u4f18\u5316\u548c\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u5957\u5168\u9762\u3001\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5ba2\u89c2\u6bd4\u8f83\u548c\u9009\u7528\u4e0d\u540c\u7684VBTS\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86TacEva\u8bc4\u4f30\u6846\u67b6\uff0c\u5177\u4f53\u5305\u62ec\u4e3a\u5e38\u89c1\u7684\u5e94\u7528\u573a\u666f\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u6027\u80fd\u6307\u6807\uff0c\u5e76\u9488\u5bf9\u6bcf\u4e2a\u6307\u6807\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u4ee5\u786e\u4fdd\u6d4b\u8bd5\u6570\u636e\u7684\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002\u540c\u65f6\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u611f\u77e5\u673a\u5236\u7684VBTS\uff0c\u8fdb\u884c\u6a2a\u5411\u5bf9\u6bd4\u3002", "result": "TacEva\u6846\u67b6\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u5404\u79cd\u7c7b\u578bVBTS\u7684\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u5176\u5404\u4e2a\u6027\u80fd\u7ef4\u5ea6\u63d0\u4f9b\u5b9a\u91cf\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9884\u9009\u5408\u9002\u7684VBTS\uff0c\u4e5f\u4e3aVBTS\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6491\u548c\u65b9\u5411\u6307\u5f15\u3002", "conclusion": "TacEva\u4e3aVBTS\u9886\u57df\u5e26\u6765\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8bbe\u5907\u5bf9\u6bd4\u3001\u9009\u578b\u548c\u8bbe\u8ba1\u4f18\u5316\u3002\u672a\u6765\uff0c\u968f\u7740\u8be5\u6846\u67b6\u548c\u76f8\u5173\u8d44\u6e90\u7684\u4e0d\u65ad\u5b8c\u5584\uff0cVBTS\u7684\u7814\u53d1\u6548\u7387\u548c\u5b9e\u7528\u6027\u5c06\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2509.18880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5DivEye\uff0c\u5229\u7528\u6587\u672c\u4e2d\u610f\u5916\u6027\u6ce2\u52a8\u7684\u7edf\u8ba1\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u5728\u6559\u80b2\u3001\u5a92\u4f53\u7b49\u9886\u57df\u7684\u6ee5\u7528\u98ce\u9669\u589e\u52a0\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u5bf9\u6297\u9ad8\u8d28\u91cfAI\u751f\u6210\u6587\u672c\u3002", "method": "\u5f15\u5165DivEye\u68c0\u6d4b\u6846\u67b6\uff0c\u57fa\u4e8esurprisal\uff08\u610f\u5916\u6027\uff09\u7edf\u8ba1\u7279\u5f81\uff0c\u6355\u6349\u4eba\u7c7b\u4e0eAI\u6587\u672c\u5728\u8bcd\u6c47\u3001\u7ed3\u6784\u4e0a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u5dee\u5f02\u3002\u65b9\u6cd5\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u53ef\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u63d0\u5347\u5176\u4ed6\u68c0\u6d4b\u5668\u6027\u80fd\u3002", "result": "DivEye\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u9886\u5148\u96f6\u6837\u672c\u68c0\u6d4b\u566833.2%\uff0c\u5bf9\u4e8e\u88ab\u7be1\u6539\u6216\u8de8\u9886\u57df\u7684\u6587\u672c\u540c\u6837\u9c81\u68d2\uff0c\u5e76\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u53ef\u63d0\u5347\u73b0\u6709\u68c0\u6d4b\u566818.7%\u7684\u6027\u80fd\u3002", "conclusion": "DivEye\u4e0d\u4ec5\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\uff0c\u8fd8\u5177\u5907\u8f83\u5f3a\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u8bc1\u660e\u8282\u594f\u6027\u610f\u5916\u6027\u662f\u8bc6\u522bAI\u6587\u672c\u7684\u91cd\u8981\u7279\u5f81\uff0c\u6709\u671b\u5b8c\u5584\u5f53\u524dLLM\u8f93\u51fa\u68c0\u6d4b\u6280\u672f\u3002"}}
{"id": "2509.18550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18550", "abs": "https://arxiv.org/abs/2509.18550", "authors": ["Mohammad Junayed Hasan", "Nabeel Mohammed", "Shafin Rahman", "Philipp Koehn"], "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025.\n  Final version to appear in the conference proceedings", "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5fae\u7b11\u8868\u60c5\u8bc6\u522b\u6846\u67b6HadaSmileNet\uff0c\u901a\u8fc7\u76f4\u63a5\u878d\u5408\u57fa\u4e8eTransformer\u7684\u7279\u5f81\u4e0e\u7269\u7406\u751f\u7406\u6807\u8bb0\uff08D-Marker\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5237\u65b0\u4e86\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u5982\u793e\u4f1a\u79d1\u5b66\u3001\u533b\u7597\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\uff0c\u51c6\u786e\u533a\u5206\u771f\u60c5\u4e0e\u4f2a\u88c5\u60c5\u611f\u6781\u4e3a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u624b\u5de5\u7279\u5f81\u65f6\u5e38\u9762\u4e34\u8ba1\u7b97\u4f4e\u6548\u548c\u8bad\u7ec3\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86HadaSmileNet\u6846\u67b6\uff0c\u91c7\u7528\u53c2\u6570\u65e0\u5173\u7684Hadamard\u4e58\u6cd5\u878d\u5408Transformer\u7279\u5f81\u4e0eD-Marker\u7279\u5f81\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e8615\u79cd\u878d\u5408\u7b56\u7565\uff0c\u6700\u7ec8\u8bc1\u5b9e\u73b0\u4e58\u6cd5\u65b9\u6cd5\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5747\u4f18\u3002", "result": "HadaSmileNet\u5728\u56db\u4e2a\u6743\u5a01\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff08\u5982UvA-NEMO 88.7%\u3001BBC 100%\uff09\uff0c\u53c2\u6570\u91cf\u964d\u4f4e26%\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u7b80\u5355\uff0c\u7279\u5f81\u53ef\u89c6\u5316\u663e\u793a\u5176\u5224\u522b\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u878d\u5408\u6df1\u5ea6\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u7387\u4e0e\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u975e\u5e38\u9002\u5408\u5bf9\u5b9e\u65f6\u8868\u60c5\u8bc6\u522b\u6709\u8981\u6c42\u7684\u591a\u5a92\u4f53\u6570\u636e\u6316\u6398\u548c\u60c5\u611f\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2509.19047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u624b\u6301\u5f0f\u7cfb\u7edfManipForce\uff0c\u53ef\u5728\u81ea\u7136\u6f14\u793a\u4e2d\u91c7\u96c6\u9ad8\u9891\u529b-\u77e9\uff08F/T\uff09\u4e0eRGB\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u9891\u7387\u611f\u77e5\u591a\u6a21\u6001Transformer\uff08FMT\uff09\u5b9e\u73b0\u5bf9\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u7684\u9ad8\u7cbe\u5ea6\u6a21\u4eff\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u5b9e\u9645\u4efb\u52a1\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6570\u636e\uff0c\u5bf9\u4e8e\u9700\u8981\u7cbe\u7ec6\u529b\u63a7\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u88c5\u914d\uff09\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5f15\u5165\u9ad8\u9891F/T\u6570\u636e\uff0c\u63d0\u5347\u5bf9\u64cd\u4f5c\u529b\u7684\u5efa\u6a21\u548c\u6267\u884c\u7cbe\u5ea6\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86ManipForce\u7cfb\u7edf\u91c7\u96c6\u9ad8\u9891\u7387\u7684F/T\u4e0eRGB\u4fe1\u53f7\uff0c\u63d0\u51faFMT\u6a21\u578b\u91c7\u7528\u9891\u7387\u548c\u6a21\u6001\u8bc6\u522b\u7684\u5d4c\u5165\u65b9\u5f0f\uff0c\u4ee5\u53ca\u5728Transformer\u6269\u6563\u7b56\u7565\u4e2d\u5229\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u516d\u9879\u771f\u5b9e\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cFMT\u5728ManipForce\u6f14\u793a\u4e0b\u5e73\u5747\u6210\u529f\u7387\u8fbe83%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u7528\u89c6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u548c\u91c7\u6837\u9891\u7387\u5206\u6790\u4e5f\u9a8c\u8bc1\u4e86F/T\u6570\u636e\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "\u878d\u5408\u9ad8\u9891F/T\u6570\u636e\u4e0e\u89c6\u89c9\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u6a21\u6001Transformer\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u7a33\u5b9a\u6027\u63a5\u89e6\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u4e0e\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2509.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovi\u010d", "Michael F\u00e4rber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u7684\u67b6\u6784\uff08JEDI\uff09\uff0c\u80fd\u591f\u5728\u65e0\u9700\u751f\u6210\u5f0f\u5927\u6a21\u578b\u53c2\u4e0e\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5e76\u663e\u8457\u63d0\u5347NLI\u4efb\u52a1\u7684\u5065\u58ee\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4e2d\u7279\u522b\u662f\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7b49\u4efb\u52a1\uff0c\u4e3a\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u5065\u58ee\u6027\u901a\u5e38\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u751f\u6210\u5f0f\u5927\u6a21\u578b\u5b9e\u73b0\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u66f4\u8f7b\u91cf\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86JEDI\uff0c\u4e00\u79cd\u4ec5\u7528\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u80fd\u8054\u5408\u8fdb\u884c\u62bd\u53d6\u5f0f\u7684\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u53ca\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u6784\u9020\u8de8\u591aNLI\u57fa\u51c6\u7684\u5927\u89c4\u6a21\u5408\u6210\u63a8\u7406\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u4e0e\u4f9d\u8d56\u751f\u6210\u5f0f\u6a21\u578b\u7684\u65b9\u6848\u4e0d\u540c\uff0c\u5728\u63a8\u7406\u9636\u6bb5JEDI\u65e0\u9700\u751f\u6210\u5f0f\u6a21\u578b\u53c2\u4e0e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJEDI\u5728\u539f\u59cb\u5206\u5e03\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u73b0\u6709\u65b9\u6848\u76f8\u5f53\uff0c\u4f46\u5728\u5206\u5e03\u5916\u53ca\u5bf9\u6297\u6027\u6d4b\u8bd5\u4e2d\uff0c\u5176\u5065\u58ee\u6027\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u4f9d\u8d56\u62bd\u53d6\u5f0f\u63a8\u7406\u76d1\u7763\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5229\u7528\u7f16\u7801\u5668\u67b6\u6784\u914d\u5408\u5408\u6210\u63a8\u7406\u6570\u636e\uff0c\u65e0\u9700\u751f\u6210\u5f0f\u6a21\u578b\u4ea6\u80fd\u5b9e\u73b0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u5f3a\u5065\u6cdb\u5316\u529b\uff0c\u5bf9NLI\u9886\u57df\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.18566", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u4e0e3D\u9ad8\u65af\u70b9\u4e91\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5355\u76ee\u89c6\u9891\u4e0b\u52a8\u6001\u4eba\u7269\u4e0e\u9759\u6001\u573a\u666f\u7684\u8054\u5408\u91cd\u5efa\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u62cd\u6444\u7684\u5355\u76ee\u89c6\u9891\u5728\u9ad8\u901f\u8fd0\u52a8\u65f6\u5e38\u5e26\u6709\u8fd0\u52a8\u6a21\u7cca\uff0c\u5bfc\u81f4\u52a8\u6001\u4eba\u7269\u7684\u4e09\u7ef4\u91cd\u5efa\u96be\u5ea6\u5927\u3002\u4e8b\u4ef6\u76f8\u673a\u80fd\u4ee5\u5fae\u79d2\u7ea7\u5206\u8fa8\u7387\u6355\u6349\u53d8\u5316\uff0c\u5176\u4f18\u52bf\u5c1a\u672a\u88ab\u5145\u5206\u7c7b\u7528\u4e8e\u52a8\u6001\u4eba\u7269\u4e0e\u573a\u666f\u7684\u8054\u5408\u5efa\u6a21\u3002", "method": "\u91c7\u7528\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\uff0c\u5229\u75283D\u9ad8\u65af\u70b9\u4e91\u7edf\u4e00\u8868\u793a\u4eba\u7269\u4e0e\u573a\u666f\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\u533a\u5206\u9ad8\u65af\u70b9\u662f\u5426\u5c5e\u4e8e\u4eba\u7269\uff0c\u4ec5\u5bf9\u4eba\u7269\u70b9\u8fdb\u884c\u59ff\u6001\u53d8\u5f62\u3002\u5f15\u5165\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\u51fd\u6570\uff0c\u5c06\u8fde\u7eed\u4e24\u5e27\u7684\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41\u5bf9\u9f50\uff0c\u63d0\u5347\u9ad8\u901f\u533a\u57df\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u4f53\u5206\u5272\u63a9\u7801\uff0c\u7b80\u5316\u4e86\u70b9\u4e91\u7ba1\u7406\u6d41\u7a0b\u3002", "result": "\u5728ZJU-MoCap-Blur\u548cMMHPSD-Blur\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728PSNR/SSIM\u548cLPIPS\u7b49\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u901f\u8fd0\u52a8\u4eba\u7269\u7684\u91cd\u5efa\u4efb\u52a1\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u7ed3\u54083D\u9ad8\u65af\u70b9\u4e91\u662f\u4e00\u79cd\u9ad8\u6548\u52a8\u6001\u4eba\u4f53\u4e0e\u9759\u6001\u573a\u666f\u8054\u5408\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u8fd0\u52a8\u6a21\u7cca\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u5404\u79cd\u9ad8\u901f\u8fd0\u52a8\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u8868\u73b0\u3002"}}
{"id": "2509.19076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SlicerROS2\u6a21\u5757\u7684\u65b0\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e863D Slicer\u4e0eROS\u7684\u6df1\u5ea6\u96c6\u6210\uff0c\u5e76\u7ed9\u51fa\u5728\u533b\u5b66\u673a\u5668\u4eba\u56fe\u50cf\u5f15\u5bfc\u4e0b\u7684\u56db\u4e2a\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "motivation": "\u533b\u5b66\u673a\u5668\u4eba\u624b\u672f\u5bf9\u5f71\u50cf\u5f15\u5bfc\u4e0e\u5b9e\u65f6\u6570\u636e\u5904\u7406\u9700\u6c42\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u3001\u6807\u51c6\u5316\u7684\u8f6f\u4ef6\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7SlicerROS2\u5b9e\u73b0\u533b\u5b66\u5f71\u50cf\u8f6f\u4ef6\uff083D Slicer\uff09\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff08ROS\uff09\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u4fc3\u8fdb\u533b\u5b66\u673a\u5668\u4eba\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5f00\u53d1\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u91cd\u5199\u548c\u91cd\u65b0\u8bbe\u8ba1SlicerROS2\u6a21\u5757\uff0c\u63d0\u5347\u5176\u6a21\u5757\u5316\u7a0b\u5ea6\uff0c\u5e76\u6dfb\u52a0\u5bf9\u4f4e\u5c42\u529f\u80fd\u30013D Slicer\u7684Python API\u7684\u652f\u6301\u53ca\u4f18\u5316\u6570\u636e\u4f20\u8f93\u534f\u8bae\uff0c\u540c\u65f6\u63d0\u51fa\u5e76\u6f14\u793a\u56db\u4e2a\u57fa\u4e8eSlicerROS2\u6838\u5fc3\u529f\u80fd\u7684\u5b9e\u9645\u533b\u5b66\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u3002", "result": "\u65b0\u4e00\u7248SlicerROS2\u652f\u6301\u66f4\u5f3a\u7684\u6570\u636e\u4ea4\u4e92\u548c\u529f\u80fd\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u52a0\u9ad8\u6548\u548c\u7075\u6d3b\u5730\u6574\u5408\u533b\u5b66\u5f71\u50cf\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u6ee1\u8db3\u590d\u6742\u3001\u73b0\u5b9e\u7684\u533b\u5b66\u5f71\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "\u65b0\u7248SlicerROS2\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u3001\u6a21\u5757\u5316\u7684\u96c6\u6210\u6846\u67b6\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u80fd\u529b\uff0c\u6709\u671b\u63a8\u52a8\u533b\u5b66\u673a\u5668\u4eba\u5728\u4e34\u5e8a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.18987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u5bf9\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u7684\u8868\u73b0\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\uff08E2E-ST\uff09\u65b9\u6cd5\u4e2d\uff0c\u8bed\u97f3\u548c\u6587\u672c\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u662f\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bcd\u6216token\u7ea7\u522b\u7684\u5bf9\u9f50\u5de5\u5177\uff0c\u4f46\u6b64\u7c7b\u5de5\u5177\u5e76\u975e\u5bf9\u6240\u6709\u8bed\u8a00\u90fd\u53ef\u7528\uff0c\u73b0\u6709\u65e0\u9700\u5bf9\u9f50\u5de5\u5177\u7684\u6700\u8fd1\u90bb\u65b9\u6cd5\u5bf9\u9f50\u7cbe\u5ea6\u4e0d\u9ad8\u3002", "method": "\u4f5c\u8005\u5c06\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u65b9\u6cd5\u5e94\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u7684\u5d4c\u5165\u8868\u793a\uff0c\u65e0\u9700\u4e13\u7528\u5bf9\u9f50\u5de5\u5177\uff0c\u4e5f\u907f\u514d\u6700\u8fd1\u90bb\u65b9\u6cd5\u5e26\u6765\u7684\u5bf9\u9f50\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u5bf9\u9f50\u8bed\u97f3\u4e0e\u6587\u672c\u5d4c\u5165\uff0c\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\u8fbe\u5230\u7c7b\u4f3cE2E-ST\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002\u57285/6\u4e2a\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u5411\u4e0a\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8eDTW\u7684\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u7f29\u5c0f\u4e86\u8bed\u97f3\u548c\u6587\u672c\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u5bf9\u63d0\u5347\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u8868\u73b0\u3001\u52a0\u901f\u8bad\u7ec3\u53ca\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5e94\u7528\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2509.18571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLive-E2T\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u6d41\u5a01\u80c1\u884c\u4e3a\u7684\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u89e3\u91ca\uff0c\u517c\u987e\u4e86\u5b9e\u65f6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u9700\u6c42\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5a01\u80c1\u76d1\u63a7\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u89e3\u91ca\u6027\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u6d41\u4e2d\u9ad8\u6548\u8bc6\u522b\u5a01\u80c1\u884c\u4e3a\u5e76\u8fdb\u884c\u5408\u7406\u89e3\u91ca\u4ecd\u662f\u6311\u6218\u3002", "method": "\u63d0\u51faLive-E2T\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1) \u5c06\u89c6\u9891\u5e27\u5206\u89e3\u4e3a\u4eba-\u7269-\u4ea4\u4e92-\u5730\u70b9\u7684\u7ed3\u6784\u5316\u8bed\u4e49\u5143\u7ec4\uff0c\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u5e76\u89c4\u907f\u4fe1\u606f\u538b\u7f29\u4e22\u5931\uff1b2) \u8bbe\u8ba1\u9ad8\u6548\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u4e0e\u66f4\u65b0\u673a\u5236\uff0c\u6d88\u9664\u65f6\u7a7a\u5197\u4f59\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6027\u80fd\uff1b3) \u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e76\u5f15\u5165Chain-of-Thought\u63a8\u7406\uff0c\u4f7f\u5176\u5bf9\u4e8b\u4ef6\u5e8f\u5217\u8fdb\u884c\u900f\u660e\u3001\u5408\u903b\u8f91\u7684\u5a01\u80c1\u89e3\u91ca\u3002", "result": "\u5728XD-Violence\u3001UCF-Crime\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLive-E2T\u5728\u5a01\u80c1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5b9e\u65f6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u5927\u5e45\u8d85\u8d8a\u4e86\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Live-E2T\u6846\u67b6\u6709\u6548\u878d\u5408\u4e86\u89c6\u9891\u5a01\u80c1\u68c0\u6d4b\u7684\u5b9e\u65f6\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u6027\u80fd\u4e0e\u89e3\u91ca\u80fd\u529b\u96be\u4ee5\u517c\u5f97\u7684\u95ee\u9898\u3002"}}
{"id": "2509.19080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86World4RL\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4e3a\u4e16\u754c\u5efa\u6a21\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u9ad8\u4fdd\u771f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u4eff\u771f\u73af\u5883\u4e0e\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u73b0\u5b9e\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u4ef7\u9ad8\u3001\u98ce\u9669\u5927\uff0c\u800c\u6a21\u4eff\u5b66\u4e60\u53c8\u53d7\u9650\u4e8e\u4e13\u5bb6\u6570\u636e\u7a00\u7f3a\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u65b9\u6cd5\u5728\u6ca1\u6709\u771f\u5b9e\u4ea4\u4e92\u7684\u524d\u63d0\u4e0b\u4f18\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u63d0\u51faWorld4RL\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u591a\u4efb\u52a1\u6570\u636e\u96c6\u5bf9\u6269\u6563\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u83b7\u5f97\u80fd\u53cd\u6620\u591a\u6837\u52a8\u6001\u7684\u865a\u62df\u73af\u5883\u3002\u7136\u540e\u5728\u51bb\u7ed3\u7684\u865a\u62df\u73af\u5883\u4e2d\uff0c\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u8fc7\u7a0b\u4e2d\u65e0\u9700\u771f\u5b9e\u73af\u5883\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u9488\u5bf9\u64cd\u4f5c\u4efb\u52a1\u7684two-hot\u52a8\u4f5c\u7f16\u7801\u548c\u57fa\u4e8ediffusion\u7684\u9aa8\u5e72\u7f51\u7edc\u4ee5\u63d0\u5347\u6a21\u62df\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWorld4RL\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5747\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u73af\u5883\u5efa\u6a21\uff0c\u5e76\u80fd\u591f\u6301\u7eed\u4f18\u5316\u7b56\u7565\uff0c\u6700\u7ec8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u663e\u8457\u8d85\u8fc7\u6a21\u4eff\u5b66\u4e60\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8ediffusion\u7684\u4e16\u754c\u6a21\u578b\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u98ce\u9669\u7684\u653f\u7b56\u4f18\u5316\uff0c\u5728\u65e0\u9700\u771f\u5b9e\u73af\u5883\u4ea4\u4e92\u7684\u524d\u63d0\u4e0b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6539\u8fdb\u7684\u74f6\u9888\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.19020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u673a\u5668\u7ffb\u8bd1\u4e2d\u63a8\u7406\u65f6\u6269\u5c55\u8ba1\u7b97\u91cf\uff08Test-Time Scaling, TTS\uff09\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u6709\u6548\u3002\u5c0f\u6a21\u578b\u7ed3\u5408\u5927N\u53ef\u4ee5\u5ab2\u7f8e\u5927\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u6216\u56fa\u5b9a\u7b97\u529b\u4e0b\u7ed3\u679c\u53d7\u9650\u3002", "motivation": "\u5f53\u524d\u901a\u8fc7\u6269\u5927\u6a21\u578b\u53c2\u6570\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u8ba1\u7b97\u6d88\u8017\u6781\u5927\u3002TTS\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u751f\u6210\u66f4\u591a\u5019\u9009\u5e76\u7b5b\u9009\u6700\u4f18\uff0c\u5df2\u5728\u6570\u5b66\u63a8\u7406\u7b49\u9886\u57df\u663e\u793a\u51fa\u6548\u679c\uff0c\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u673a\u5668\u7ffb\u8bd1\u7684best-of-N TTS\u65b9\u6848\uff0c\u5728WMT24\u57fa\u51c6\u4e0a\uff0c\u8986\u76d66\u4e2a\u9ad8\u8d44\u6e90\u30011\u4e2a\u4f4e\u8d44\u6e90\u8bed\u5bf9\uff0c5\u79cd\u6a21\u578b\u89c4\u6a21\uff083B-72B\uff09\uff0c\u4e0d\u540cTTS\u8ba1\u7b97\u9884\u7b97\uff08N\u6700\u5927\u81f31024\uff09\uff0c\u5e76\u7528\u591a\u79cd\u81ea\u52a8\u8bc4\u6d4b\u548c\u4eba\u5de5\u8bc4\u6d4b\u8861\u91cf\u6548\u679c\u3002", "result": "\uff08a\uff09\u9ad8\u8d44\u6e90\u8bed\u8a00TTS\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5f97\u5230\u591a\u9879\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u6d4b\u786e\u8ba4\uff1b\uff08b\uff09\u5c0f\u6a21\u578b\u914d\u5927N\u65f6\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5927\u6a21\u578bN=1\u7684\u6548\u679c\uff0c\u4f46\u8017\u8d39\u66f4\u591a\u7b97\u529b\uff1b\uff08c\uff09\u5728\u7b97\u529b\u56fa\u5b9a\u4e0b\uff0c\u5927\u6a21\u578b\u901a\u5e38\u6548\u7387\u66f4\u9ad8\uff0c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0bTTS\u53cd\u800c\u4f1a\u56e0\u8bc4\u4ef7\u76f2\u533a\u800c\u635f\u5bb3\u8d28\u91cf\u3002", "conclusion": "TTS\u5728\u9ad8\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e0b\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u4f46\u5176\u6536\u76ca\u4f9d\u8d56\u8d44\u6e90\u6761\u4ef6\u548c\u7b97\u529b\u5206\u914d\uff0c\u5728\u4f4e\u8d44\u6e90\u6216\u7b97\u529b\u53d7\u9650\u4e0b\u9700\u8c28\u614e\u5e94\u7528\u3002"}}
{"id": "2509.18582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18582", "abs": "https://arxiv.org/abs/2509.18582", "authors": ["Daiqing Qi", "Handong Zhao", "Jing Shi", "Simon Jenni", "Yifei Fan", "Franck Dernoncourt", "Scott Cohen", "Sheng Li"], "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers", "comment": null, "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u89c6\u89c9\u7f8e\u5b66\u7406\u89e3\u7684\u5b8c\u6574\u65b9\u6848\uff0c\u5305\u62ec\u65b0\u6570\u636e\u96c6\u3001\u65b0\u6a21\u578b\u53ca\u65b0\u57fa\u51c6\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u56fe\u50cf\u7406\u89e3\u65f6\uff0c\u6613\u5c40\u9650\u4e8e\u57fa\u7840\u7684\u5143\u7d20\u68c0\u6d4b\u548c\u5e38\u8bc6\u6027\u7f8e\u5b66\u5224\u65ad\uff0c\u96be\u4ee5\u6355\u6349\u5982\u4e13\u4e1a\u6444\u5f71\u5e08\u5bf9\u7f8e\u5b66\u7ec6\u8282\u3001\u6784\u56fe\u548c\u8272\u5f69\u7b49\u6df1\u5c42\u5ba1\u7f8e\u4fe1\u606f\u3002\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u4e13\u4e1a\u6570\u636e\u96c6\u548c\u9488\u5bf9\u6027\u4f18\u5316\u65b9\u6cd5\u5236\u7ea6\u5176\u7f8e\u5b66\u7406\u89e3\u80fd\u529b\u3002", "method": "(1) \u6784\u5efa\u4e13\u4e1a\u4e30\u5bcc\u3001\u5177\u591a\u6837\u6027\u7684\u65b0\u578b\u6570\u636e\u96c6PhotoCritique\uff0c\u57fa\u4e8e\u6444\u5f71\u5e08\u548c\u7231\u597d\u8005\u8ba8\u8bba\u91c7\u96c6\u3002(2) \u63d0\u51faPhotoEye\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u89c6\u89c9\u4fe1\u606f\u878d\u5408\u548c\u8bed\u8a00\u5f15\u5bfc\uff0c\u5b9e\u73b0\u66f4\u7ec6\u81f4\u7684\u7f8e\u5b66\u5206\u6790\u3002(3) \u53d1\u5e03\u4e13\u4e1a\u6743\u5a01\u7f8e\u5b66\u7406\u89e3\u8bc4\u6d4b\u96c6PhotoBench\uff0c\u63a8\u52a8\u7cfb\u7edf\u6027\u8bc4\u6d4b\u3002", "result": "PhotoEye\u6a21\u578b\u5728\u73b0\u6709\u8bc4\u6d4b\u96c6\u548c\u65b0\u5efa\u7684PhotoBench\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\uff0c\u63d0\u5347\u4e86MLLM\u56fe\u50cf\u7f8e\u5b66\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u4f5c\u8005\u901a\u8fc7\u6784\u5efa\u4e13\u4e1a\u6570\u636e\u96c6\u4e0e\u521b\u65b0\u6a21\u578b\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5ba1\u7f8e\u7406\u89e3\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u4eca\u540e\u7684\u89c6\u89c9\u7f8e\u5b66\u667a\u80fd\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.19102", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u65f6\u5e8f\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u7531\u52a8\u4f5c\u3001\u52a8\u8bcd\u548c\u5bf9\u8c61\u5b9a\u4e49\u7684\u52a8\u4f5c\u7247\u6bb5\uff0c\u5b9e\u73b0\u4e86\u653f\u7b56\u7684\u7ec4\u6210\u6027\u548c\u590d\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u529f\u80fd\u89c4\u8303\u5316\u548c\u6269\u6563\u653f\u7b56\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4eff\u771f\u4e0e\u5b9e\u9645\u73af\u5883\u5747\u5177\u6709\u4f18\u79c0\u7684\u6cdb\u5316\u4e0e\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u901a\u5e38\u53ea\u80fd\u83b7\u5f97\u4efb\u52a1\u7279\u5b9a\u7684\u7b56\u7565\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u6216\u4e0d\u540c\u5bf9\u8c61\u3002\u4e3a\u89e3\u51b3\u7b56\u7565\u6cdb\u5316\u548c\u590d\u7528\uff0c\u5b9e\u73b0\u590d\u6742\u64cd\u4f5c\u9886\u57df\u7684\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u9488\u5bf9\u52a8\u4f5c\u4e0e\u5bf9\u8c61\u7684\u66f4\u4e00\u822c\u6027\u8868\u8fbe\u548c\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "1. \u5c06\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u5206\u89e3\u6210\u2018\u52a8\u4f5c-\u52a8\u8bcd-\u5bf9\u8c61\u2019\u4e09\u5143\u7ec4\u7684\u52a8\u4f5c\u7247\u6bb5\uff0c\u805a\u7126\u4e8e\u52a8\u4f5c\u672c\u8eab\u4fbf\u4e8e\u7ec4\u5408\u4e0e\u590d\u7528\uff1b2. \u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4f9b\u6027\u63d0\u793a\uff0c\u5bf9\u5bf9\u8c61\u8fdb\u884c\u529f\u80fd\u89c4\u8303\u5316\uff0c\u5c06\u4e0d\u540c\u5b9e\u4f8b\u6620\u5c04\u5230\u5171\u4eab\u7684\u529f\u80fd\u5750\u6807\u7cfb\uff0c\u5b9e\u73b0\u529f\u80fd\u5bf9\u9f50\u548c\u8f68\u8ff9\u81ea\u52a8\u8fc1\u79fb\uff1b3. \u8bbe\u8ba1\u4e86\u5bf9\u8c61\u4e0e\u52a8\u4f5c\u4e2d\u5fc3\u7684\u6269\u6563\u7b56\u7565\u6a21\u578bFuncDiffuser\uff0c\u5728\u5bf9\u9f50\u540e\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u81ea\u52a8\u6355\u6349\u5bf9\u8c61\u59ff\u6001\u4e0e\u53ef\u4f9b\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\uff08\u542b\u8de8\u7c7b\u522b\u3001\u8de8\u4efb\u52a1\u53casim2real\uff09\u5b9e\u9a8c\u4e2d\uff0cFunCanon\u6846\u67b6\u5c55\u73b0\u4e86\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5b9e\u73b0\u7c7b\u522b\u7ea7\u6cdb\u5316\u3001\u4efb\u52a1\u8fc1\u79fb\u548c\u7a33\u5b9a\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "\u529f\u80fd\u89c4\u8303\u5316\uff08Functional Canonicalization\uff09\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5e26\u6765\u5f3a\u6709\u529b\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u590d\u6742\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u653f\u7b56\u6cdb\u5316\u548c\u590d\u7528\u80fd\u529b\uff0c\u662f\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u6a21\u4eff\u5b66\u4e60\u7684\u6709\u6548\u65b9\u5411\u3002"}}
{"id": "2509.19033", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e862014\u81f32024\u5e74CLiC-it\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4f1a\u8bae\u7684\u8bba\u6587\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8d8b\u52bf\u4e0e\u53d1\u5c55\u3002", "motivation": "\u8fd1\u5341\u5e74\u968f\u7740Transformer\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0cCL\u548cNLP\u9886\u57df\u7814\u7a76\u91cd\u70b9\u53d1\u751f\u5de8\u5927\u8f6c\u53d8\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u672c\u5730\u9876\u7ea7\u4f1a\u8bae\u8bba\u6587\uff0c\u603b\u7ed3\u9886\u57df\u6f14\u8fdb\u548c\u65b0\u8d8b\u52bf\uff0c\u4e3a\u5b66\u754c\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f5c\u8005\u5c06CLiC-it\u4f1a\u8bae\u524d\u5341\u5c4a\u8bba\u6587\u53ca\u5143\u6570\u636e\uff08\u4f5c\u8005\u6765\u6e90\u3001\u6027\u522b\u3001\u673a\u6784\u7b49\uff09\u6784\u5efa\u6210CLiC-it\u8bed\u6599\u5e93\uff0c\u5bf9\u8bba\u6587\u5185\u5bb9\u548c\u5143\u6570\u636e\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u6316\u6398\u4e3b\u8981\u8bdd\u9898\u548c\u8d8b\u52bf\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u610f\u5927\u5229CL\u548cNLP\u793e\u533a\u7684\u6f14\u53d8\u8f68\u8ff9\uff0c\u5305\u62ec\u7814\u7a76\u91cd\u70b9\u7684\u8f6c\u79fb\u548c\u91cd\u8981\u8bdd\u9898\u7684\u5174\u8d77\uff0c\u8be6\u7ec6\u523b\u753b\u4e86\u4f5c\u8005\u7ed3\u6784\u4e0e\u7814\u7a76\u5185\u5bb9\u7684\u53d8\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u610f\u5927\u5229\u53ca\u56fd\u9645CL/NLP\u5b66\u8005\u63d0\u4f9b\u4e86\u8be6\u5b9e\u7684\u8d8b\u52bf\u6d1e\u5bdf\uff0c\u6709\u52a9\u4e8e\u5236\u5b9a\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u653f\u7b56\u51b3\u7b56\u3002"}}
{"id": "2509.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5b9e\u65f6MRI\u5f15\u5bfc\u653e\u5c04\u6cbb\u7597\u7684\u80bf\u7624\u5206\u5272\u65b9\u6cd5\uff0c\u57fa\u4e8eXMem\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u80fd\u591f\u5728\u957f\u5e8f\u5217cine-MRI\u4e2d\u8ffd\u8e2a\u4e0e\u5206\u5272\u80bf\u7624\u3002", "motivation": "\u5728MRI\u5f15\u5bfc\u7684\u653e\u5c04\u6cbb\u7597\u4e2d\uff0c\u80bf\u7624\u4f4d\u7f6e\u52a8\u6001\u53d8\u5316\u3001\u6570\u636e\u6807\u6ce8\u6709\u9650\uff0c\u5b9e\u65f6\u3001\u7cbe\u786e\u7684\u80bf\u7624\u5206\u5272\u4e0e\u8ffd\u8e2a\u6210\u4e3a\u96be\u9898\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4ee5\u4e0a\u6311\u6218\uff0c\u63d0\u5347\u80bf\u7624\u8ffd\u8e2a\u7684\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u4e86XMem\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u52a0\u5165\u8bb0\u5fc6\u673a\u5236\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u5e8f\u5217\u4fe1\u606f\u7684\u8bb0\u5fc6\u548c\u5229\u7528\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u957f\u65f6\u95f4\u5e8f\u5217\u7684\u653e\u5c04\u6cbb\u7597\u8fc7\u7a0b\u4e2d\u7cbe\u51c6\u6355\u6293\u80bf\u7624\u52a8\u6001\u3002\u7cfb\u7edf\u4e13\u4e3a\u5b9e\u65f6\u5e94\u7528\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u9700\u6c42\u3002", "result": "\u5c3d\u7ba1\u8be6\u7ec6\u5b9e\u9a8c\u8bb0\u5f55\u4e22\u5931\uff0c\u65e0\u6cd5\u7ed9\u51fa\u5177\u4f53\u7684\u5b9a\u91cf\u7ed3\u679c\uff0c\u4f46\u5728\u5f00\u53d1\u9636\u6bb5\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u8f83\u597d\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e76\u6ee1\u8db3\u4e86\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "conclusion": "\u91c7\u7528XMem\u8bb0\u5fc6\u589e\u5f3a\u6a21\u578b\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347MRI\u5f15\u5bfc\u4e0b\u653e\u5c04\u6cbb\u7597\u4e2d\u80bf\u7624\u8ffd\u8e2a\u7684\u7cbe\u5ea6\u4e0e\u5b89\u5168\u6027\uff0c\u5bf9\u764c\u75c7\u6cbb\u7597\u7684\u51c6\u786e\u5ea6\u548c\u5b89\u5168\u6027\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2509.19105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u795e\u7ecf\u7f51\u7edc\u6a21\u578bRS-Net\uff0c\u53ef\u4ec5\u901a\u8fc7RGB\u56fe\u50cf\u9884\u6d4b\u5730\u8868\u5149\u8c31\u7279\u5f81\uff0c\u4ece\u800c\u63a8\u65ad\u5730\u9762\u5c5e\u6027\u5e76\u63d0\u5347\u673a\u5668\u4eba\u6237\u5916\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u673a\u5668\u4eba\u5224\u65ad\u53ef\u901a\u884c\u5730\u5f62\u591a\u4f9d\u8d56\u51e0\u4f55\u6216\u8bed\u4e49\u6807\u7b7e\uff0c\u96be\u4ee5\u533a\u5206\u5916\u89c2\u7c7b\u4f3c\u4f46\u6750\u8d28\u4e0d\u540c\u7684\u5730\u8868\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u7684\u51c6\u786e\u6027\u3002\u800c\u76f4\u63a5\u7528\u5149\u8c31\u4f20\u611f\u5668\u867d\u80fd\u89e3\u51b3\uff0c\u4f46\u6210\u672c\u9ad8\u3001\u9700\u5b9a\u5236\u786c\u4ef6\u5e76\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51faRS-Net\u795e\u7ecf\u7f51\u7edc\uff0c\u8f93\u5165RGB\u56fe\u50cf\u5757\uff0c\u9884\u6d4b\u8be5\u533a\u57df\u7684\u5149\u8c31\u7279\u5f81\uff0c\u518d\u636e\u6b64\u63a8\u65ad\u5730\u9762\u7c7b\u578b\u548c\u6469\u64e6\u7cfb\u6570\u3002\u9884\u6d4b\u7ed3\u679c\u5206\u522b\u96c6\u6210\u5165\u8f6e\u5f0f\u673a\u5668\u4eba\u7684\u91c7\u6837\u89c4\u5212\u5668\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684MPC\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u5bfc\u822a\u65f6\u9759\u9760RGB\u5373\u53ef\u63a8\u7406\u5730\u9762\u7684\u771f\u5b9e\u7269\u7406\u5c5e\u6027\u3002", "result": "RS-Net\u80fd\u591f\u8f83\u51c6\u786e\u5730\u901a\u8fc7\u666e\u901aRGB\u6570\u636e\u91cd\u5efa\u5730\u8868\u5149\u8c31\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5730\u8868\u5206\u7c7b\u548c\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u3002\u90e8\u7f72\u4e8e\u4e24\u7c7b\u673a\u5668\u4eba\u540e\uff0c\u5747\u53ef\u5728\u590d\u6742\u6237\u5916\u5730\u5f62\u4e0b\u63d0\u9ad8\u79fb\u52a8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u5c06\u6df1\u5ea6\u5b66\u4e60\u548cRGB\u611f\u77e5\u7ed3\u5408\uff0c\u7a81\u7834\u4e86\u9ad8\u6210\u672c\u5149\u8c31\u4f20\u611f\u786c\u4ef6\u7684\u9650\u5236\uff0c\u8ba9\u673a\u5668\u4eba\u5728\u4f4e\u6210\u672c\u901a\u7528\u786c\u4ef6\u4e0b\u5373\u53ef\u83b7\u53d6\u5730\u8868\u7269\u7406\u5c5e\u6027\uff0c\u6709\u671b\u52a0\u901f\u5176\u5728\u5404\u7c7b\u6237\u5916\u73af\u5883\u7684\u5e94\u7528\u666e\u53ca\u3002"}}
{"id": "2509.19094", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u95ee\u7b54\u65b9\u6cd5Pathways of Thoughts\uff08PoT\uff09\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u610f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u7279\u5b9a\u5fae\u8c03\uff0c\u5e76\u5728LaMP-QA\u57fa\u51c6\u4e0a\u5927\u5e45\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u4e2a\u6027\u5316\u95ee\u7b54\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u7b54\u6848\u51c6\u786e\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4f46\u7531\u4e8e\u7528\u6237\u504f\u597d\u96be\u4ee5\u4ece\u590d\u6742\u3001\u542b\u566a\u6216\u9690\u542b\u7684\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\uff0c\u4ee5\u53ca\u96be\u4ee5\u751f\u6210\u65e2\u6b63\u786e\u53c8\u8d34\u5408\u7528\u6237\u9884\u671f\u7684\u56de\u590d\uff0c\u4f7f\u5176\u7814\u7a76\u53d7\u9650\u3002", "method": "\u63d0\u51faPoT\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\uff0c\u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u591a\u6b65\u51b3\u7b56\uff0c\u52a8\u6001\u9009\u62e9\u63a8\u7406\u3001\u4fee\u8ba2\u3001\u4e2a\u6027\u5316\u548c\u6f84\u6e05\u7b49\u8ba4\u77e5\u64cd\u4f5c\uff0c\u63a2\u7d22\u591a\u6761\u63a8\u7406\u8def\u5f84\uff0c\u751f\u6210\u591a\u6837\u5019\u9009\u7b54\u6848\uff0c\u5e76\u6839\u636e\u63a8\u65ad\u7684\u7528\u6237\u504f\u597d\u52a0\u6743\u805a\u5408\u51fa\u6700\u7ec8\u4e2a\u6027\u5316\u56de\u590d\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u5728LaMP-QA\u4e2a\u6027\u5316\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0cPoT\u65b9\u6cd5\u53d6\u5f97\u4e86\u9ad8\u8fbe13.1%\u7684\u76f8\u5bf9\u63d0\u5347\u3002\u4eba\u5de5\u8bc4\u6d4b\u4e2d\uff0c66%\u7684\u6848\u4f8b\u4e2d\u8bc4\u5ba1\u66f4\u504f\u597dPoT\u751f\u6210\u5185\u5bb9\uff0c\u4ec515%\u8bc4\u4e3a\u6301\u5e73\u3002", "conclusion": "PoT\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u80fd\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u5bf9\u6bd4\u78c1\u5171\u632f\u6210\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08SSCM\uff09\uff0c\u901a\u8fc7\u7a7a\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u9891\u57df\u4fe1\u606f\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u91cd\u5efa\u6548\u679c\uff0c\u53c2\u6570\u66f4\u5c11\u3001\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u76ee\u524d\u7684MC-MRI SR\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u7a7a\u95f4\u4e0e\u8bed\u4e49\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u7ecf\u5e38\u5bf9\u9ad8\u9891\u7ec6\u8282\u8fd8\u539f\u4e0d\u8db3\uff0c\u9020\u6210\u91cd\u5efa\u89e3\u5256\u7ed3\u6784\u7684\u5bf9\u9f50\u4e0d\u826f\u4e0e\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u578b\uff08SSCM\uff09\uff0c\u5305\u542b\u52a8\u6001\u7a7a\u95f4\u53d8\u5f62\u6a21\u5757\uff08\u7a7a\u95f4\u5bf9\u9f50\uff09\u3001\u8bed\u4e49\u611f\u77e5\u805a\u5408\u6a21\u5757\uff08\u957f\u8ddd\u79bb\u8bed\u4e49\u4e00\u81f4\u6027\uff09\u548c\u7a7a\u95f4-\u9891\u7387\u878d\u5408\u6a21\u5757\uff08\u7ec6\u8282\u6062\u590d\uff09\uff0c\u5171\u540c\u63d0\u5347\u8d85\u5206\u8fa8\u91cd\u5efa\u8d28\u91cf\u3002", "result": "SSCM\u5728\u591a\u4e2a\u516c\u5f00\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u65e2\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u4e5f\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SSCM\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u9ad8\u9891\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728MC-MRI SR\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002"}}
{"id": "2509.19142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBiGraspFormer\u7684\u7aef\u5230\u7aef\u53cc\u81c2\u6293\u53d6\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u534f\u8c03\u6027\u548c\u6548\u7387\u4e0a\u5747\u6709\u660e\u663e\u63d0\u5347\u3002", "motivation": "\u76ee\u524d\u673a\u5668\u4eba\u53cc\u81c2\u534f\u8c03\u6293\u53d6\u5b58\u5728\u78b0\u649e\u98ce\u9669\u548c\u529b\u5206\u914d\u4e0d\u5747\u7b49\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u53ea\u5173\u6ce8\u5355\u81c2\u6293\u53d6\uff0c\u8981\u4e48\u5206\u9636\u6bb5\u5904\u7406\uff0c\u7f3a\u4e4f\u6574\u4f53\u534f\u8c03\u3002", "method": "\u4f5c\u8005\u63d0\u51faBiGraspFormer\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u3002\u5176\u6838\u5fc3\u662fSingle-Guided Bimanual\uff08SGB\uff09\u7b56\u7565\uff0c\u5148\u901a\u8fc7\u89e3\u7801\u5668\u751f\u6210\u591a\u6837\u5355\u81c2\u6293\u53d6\u5019\u9009\uff08\u542b\u7279\u5f81\uff09\uff0c\u518d\u901a\u8fc7\u4e13\u95e8\u7684\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u9884\u6d4b\u53cc\u81c2\u6293\u53d6\u4f4d\u59ff\u53ca\u8d28\u91cf\u5206\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u641c\u7d22\u4e0e\u534f\u8c03\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cBiGraspFormer\u5728\u534f\u8c03\u6027\u3001\u51c6\u786e\u5ea6\u548c\u63a8\u7406\u6548\u7387\uff08<0.05\u79d2\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "BiGraspFormer\u4e3a\u673a\u5668\u4eba\u9ad8\u6548\u534f\u8c03\u5730\u6267\u884c\u53cc\u81c2\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.19108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4f53\u88c1\u7684\u5927\u578b\u8bed\u6599\u5e93\uff0c\u8c03\u67e5\u4e86\u8bed\u8a00\u5b66\u4e2d\u5173\u4e8e\u5927\u591a\u6570\u53e5\u5b50\u90fd\u662f\u9996\u6b21\u51fa\u73b0\u7684\u8bf4\u6cd5\uff0c\u53d1\u73b0\u53e5\u5b50\u72ec\u7279\u6027\u53d7\u5230\u4f53\u88c1\u5f71\u54cd\u3002", "motivation": "\u8bb8\u591a\u8bed\u8a00\u5b66\u5bb6\u8ba4\u4e3a\uff0c\u7edd\u5927\u591a\u6570\u8bed\u8a00\u8868\u8fbe\u90fd\u662f\u72ec\u4e00\u65e0\u4e8c\u3001\u524d\u6240\u672a\u6709\u7684\uff0c\u4f46\u8fd9\u79cd\u8bf4\u6cd5\u4ee5\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u5b9e\u8bc1\u652f\u6301\u3002\u968f\u7740\u5927\u8bed\u6599\u5e93\u7684\u5f00\u653e\uff0c\u53ef\u4ee5\u7528\u6570\u636e\u5177\u4f53\u68c0\u9a8c\u8be5\u89c2\u70b9\u3002", "method": "\u5229\u7528NLTK Python\u5de5\u5177\uff0c\u5bf9\u4e0d\u540c\u4f53\u88c1\u7684\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u8fdb\u884c\u53e5\u5b50\u89e3\u6790\uff0c\u7edf\u8ba1\u5b8c\u5168\u91cd\u590d\uff08exact string match\uff09\u548c\u552f\u4e00\uff08unique\uff09\u7684\u53e5\u5b50\u51fa\u73b0\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u867d\u7136\u5728\u8bb8\u591a\u8bed\u6599\u5e93\u4e2d\u552f\u4e00\uff08\u4e0d\u91cd\u590d\uff09\u53e5\u5b50\u5360\u591a\u6570\uff0c\u4f46\u8fd9\u4e00\u6bd4\u4f8b\u53d7\u6587\u672c\u4f53\u88c1\u5f3a\u70c8\u5f71\u54cd\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8bed\u6599\u5e93\u4e2d\u91cd\u590d\u53e5\u5b50\u7684\u6570\u91cf\u90fd\u4e0d\u5bb9\u5ffd\u89c6\u3002", "conclusion": "\u8bed\u8a00\u8868\u8fbe\u7684\u72ec\u7279\u6027\u5e76\u975e\u7edd\u5bf9\uff0c\u6587\u672c\u4f53\u88c1\u5bf9\u53e5\u5b50\u91cd\u590d\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002\u91cd\u590d\u53e5\u5b50\u5728\u771f\u5b9e\u8bed\u6599\u4e2d\u5177\u6709\u5b9e\u9645\u5206\u91cf\uff0c\u8fd9\u5bf9\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u7684\u751f\u6210\u548c\u7406\u89e3\u673a\u5236\u6709\u542f\u793a\u4f5c\u7528\u3002"}}
{"id": "2509.18600", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728CheXpert Plus\u6570\u636e\u96c6\u4e0a\u5237\u65b0\u4e86\u6700\u65b0\u6210\u7ee9\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5927\u578b\u6a21\u578b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u7279\u522b\u5728\u6570\u636e\u6709\u9650\u6216\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u96be\u4ee5\u5e94\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51faOraPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06RL\u4e2d\u5931\u8d25\u63a2\u7d22\u8f6c\u4e3aoracle\u8f7b\u91cf\u76d1\u7763\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5\u3001\u4ec5\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u52a0\u5165FactScore\u5956\u52b1\u673a\u5236\uff0c\u7528\u4e34\u5e8a\u4e8b\u5b9e\u4f5c\u4e3a\u57fa\u7840\uff0c\u4f9d\u636e\u8bca\u65ad\u8bc1\u636e\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u7528\u5f88\u5c11\u6570\u636e\u548c\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728CheXpert Plus\u6570\u636e\u96c6\u4e0a\u53d6\u5f97F1\u5206\u65700.341\uff0c\u8fdc\u8d85\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "OraPO\u7ed3\u5408oracle\u6307\u5bfc\u548c\u4e8b\u5b9e\u9a71\u52a8\u5956\u52b1\uff0c\u6781\u5927\u63d0\u5347\u4e86\u533b\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8be5\u9886\u57df\u843d\u5730\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u524d\u89c6\uff08receding-horizon\uff09\u89c4\u5212\u5668\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\uff0c\u589e\u5f3a\u4e86\u89e3\u7a7a\u95f4\u63a2\u7d22\u80fd\u529b\u548c\u5bf9\u5c40\u90e8\u6700\u4f18\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u673a\u5668\u4eba\u907f\u78b0\u4e0e\u56e2\u961f\u534f\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u5728\u9762\u5bf9\u590d\u6742\u73af\u5883\u6216\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u5c40\u90e8\u6700\u4f18\u548c\u6b7b\u9501\u7b49\u95ee\u9898\u65f6\uff0c\u5bb9\u6613\u5931\u8d25\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\u6765\u589e\u52a0\u7cfb\u7edf\u7684\u63a2\u7d22\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u89e3\u51b3\u7387\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u524d\u89c6\u89c4\u5212\u548c\u4ea4\u53c9\u71b5\u4f18\u5316\uff0c\u5728\u7edf\u4e00\u4ee3\u4ef7\u51fd\u6570\u4e0b\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5141\u8bb8\u7b56\u7565\u5171\u4eab\uff0c\u907f\u514d\u6b7b\u9501\uff0c\u65e0\u9700\u4e2d\u5fc3\u5316\u9ad8\u590d\u6742\u5ea6\u4f18\u5316\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8bc1\u660e\u5728\u9677\u9631\u73af\u5883\u548c\u591a\u673a\u5668\u4eba\u907f\u78b0\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u89c4\u5212\u80fd\u5927\u5e45\u63d0\u5347\u6210\u529f\u7387\u3002\u786c\u4ef6\u5b9e\u9a8c\u5219\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u65f6\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u591a\u6a21\u6001\u3001\u57fa\u4e8e\u91c7\u6837\u7684\u524d\u89c6\u89c4\u5212\u80fd\u63d0\u5347\u89c4\u5212\u5b89\u5168\u4e0e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u573a\u666f\uff0c\u540c\u65f6\u907f\u514d\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002"}}
{"id": "2509.19109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86KyrgyzNER\uff0c\u8fd9\u662f\u9996\u4e2a\u4e3a\u67ef\u5c14\u514b\u5b5c\u8bed\u4eba\u5de5\u6807\u6ce8\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdNER\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u67ef\u5c14\u514b\u5b5c\u8bed\u5c5e\u4e8e\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\uff0c\u5c1a\u65e0\u516c\u5f00\u9ad8\u8d28\u91cf\u7684NER\u6570\u636e\u96c6\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5efa\u7acb\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u8be5\u8bed\u8a00NLP\u53d1\u5c55\uff0c\u652f\u6301\u540e\u7eed\u6a21\u578b\u4e0e\u5e94\u7528\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e861499\u7bc7\u65b0\u95fb\u62a5\u9053\uff0c\u624b\u5de5\u6807\u6ce8\u4e8610900\u4e2a\u53e5\u5b50\u300139075\u4e2a\u4eba\u540d\u5b9e\u4f53\uff0c\u6db5\u76d627\u7c7b\u5b9e\u4f53\u7c7b\u578b\u3002\u4ecb\u7ecd\u4e86\u6807\u6ce8\u65b9\u6848\u3001\u9047\u5230\u7684\u96be\u9898\u53ca\u6570\u636e\u7edf\u8ba1\u3002\u4e4b\u540e\uff0c\u4f5c\u8005\u5206\u522b\u7528\u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\uff08\u6761\u4ef6\u968f\u673a\u573a\uff09\u548c\u591a\u8bed\u8a00Transformer\uff08\u5982multilingual RoBERTa\uff09\u7b49\u6a21\u578b\uff0c\u5728\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u7a00\u6709\u5b9e\u4f53\u7c7b\u522b\u4e0a\u8868\u73b0\u4e00\u822c\uff0c\u4f46\u591a\u8bed\u8a00\u7248\u672c\u7684RoBERTa\u6a21\u578b\u5728\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u8fbe\u5230\u4e86\u8f83\u597d\u5e73\u8861\u3002\u5176\u4ed6\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e5f\u76f8\u8fd1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u8f83\u597d\u5904\u7406\u4f4e\u8d44\u6e90\u67ef\u5c14\u514b\u5b5c\u8bedNER\u4efb\u52a1\uff0c\u4f46\u7a00\u6709\u7c7b\u522b\u4ecd\u5177\u6311\u6218\u3002\u672a\u6765\u5982\u679c\u91c7\u7528\u66f4\u7ec6\u7c92\u5ea6\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u6216\u8bb8\u80fd\u8fdb\u4e00\u6b65\u6539\u5584\u67ef\u5c14\u514b\u5b5c\u8bedNLP\u7ba1\u9053\u7684\u8bc4\u4f30\u4e0e\u6548\u679c\u3002"}}
{"id": "2509.18602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.", "AI": {"tldr": "AMSF\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u591a\u53c2\u8003\u98ce\u683c\u878d\u5408\u7684\u6269\u6563\u6a21\u578b\u65b0\u65b9\u6cd5\uff0c\u80fd\u5e73\u8861\u591a\u98ce\u683c\u5f71\u54cd\u3001\u7075\u6d3b\u878d\u5408\u591a\u79cd\u98ce\u683c\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684\u98ce\u683c\u878d\u5408\u65b9\u6cd5\u4ec5\u80fd\u63a5\u53d7\u5355\u4e00\u98ce\u683c\u8f93\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u591a\u98ce\u683c\u878d\u5408\u4e14\u7f3a\u4e4f\u8c03\u63a7\u4e0d\u540c\u98ce\u683c\u5f71\u54cd\u7684\u673a\u5236\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "AMSF\u5c06\u591a\u4e2a\u98ce\u683c\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u7f16\u7801\u4e3a\u8bed\u4e49token\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u9002\u914d\u6ce8\u5165\u5230\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u6bcf\u4e00\u5c42cross-attention\u4e2d\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u76f8\u4f3c\u5ea6\u611f\u77e5\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u5728\u6269\u6563\u53bb\u566a\u7684\u6bcf\u4e00\u6b65\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u98ce\u683c\u7684\u5173\u6ce8\uff0c\u7528\u6237\u53ef\u63a7\u3001\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u5747\u8868\u660e\uff0cAMSF\u5728\u591a\u98ce\u683c\u878d\u5408\u4efb\u52a1\u4e0a\u6548\u679c\u660e\u663e\u4f18\u4e8e\u76ee\u524d\u6700\u65b0\u65b9\u6cd5\uff0c\u4e14\u80fd\u8f7b\u677e\u6269\u5c55\u5230\u878d\u5408\u4e24\u79cd\u53ca\u4ee5\u4e0a\u98ce\u683c\u3002", "conclusion": "AMSF\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u591a\u98ce\u683c\u8868\u8fbe\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u98ce\u683c\u751f\u6210\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.19169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86MagiClaw\uff0c\u4e00\u79cd\u80fd\u7528\u4e8e\u6570\u636e\u91c7\u96c6\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53cc\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u786c\u4ef6\u548c\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\uff0c\u7f29\u5c0f\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u95f4\u64cd\u4f5c\u6280\u80fd\u8fc1\u79fb\u7684\u9e3f\u6c9f\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u793a\u8303\u5230\u673a\u5668\u4eba\u6267\u884c\u65f6\uff0c\u56e0\u4e3a\u611f\u77e5\u548c\u5f62\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u4f7f\u64cd\u4f5c\u6280\u80fd\u8fc1\u79fb\u96be\u4ee5\u5b9e\u73b0\u6216\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86MagiClaw\u4f5c\u4e3a\u624b\u6301\u5de5\u5177\u548c\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\uff0c\u96c6\u6210\u4e86\u5185\u5d4c\u6444\u50cf\u5934\u7684\u8f6f\u591a\u9762\u4f53\u7f51\u7edc(SPN)\u7528\u4e8e6\u81ea\u7531\u5ea6\u529b\u548c\u63a5\u89e6\u5f62\u53d8\u4f30\u8ba1\uff1b\u7ed3\u5408iPhone\u5b9e\u73b0\u76846D\u4f4d\u59ff\u3001RGB\u89c6\u9891\u53caLiDAR\u6df1\u5ea6\u611f\u77e5\uff1b\u901a\u8fc7\u5b9a\u5236iOS\u5e94\u7528\u5b9e\u73b0\u591a\u6a21\u6001\u5b9e\u65f6\u6570\u636e\u540c\u6b65\u548c\u591a\u79cd\u63a7\u5236\u6a21\u5f0f\u3002", "result": "MagiClaw\u80fd\u591f\u9ad8\u8d28\u91cf\u5730\u91c7\u96c6\u542b\u4e30\u5bcc\u63a5\u89e6\u4fe1\u606f\u7684\u6570\u636e\uff0c\u652f\u6301\u5b9e\u65f6\u9065\u64cd\u4f5c\u3001\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u548c\u6df7\u5408\u73b0\u5b9e\u4ea4\u4e92\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u80fd\u964d\u4f4e\u9ad8\u4fdd\u771f\u6570\u636e\u6536\u96c6\u96be\u5ea6\uff0c\u52a0\u5feb\u901a\u7528\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u3002", "conclusion": "MagiClaw\u4e3a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u786c\u4ef6\u4e00\u81f4\u7684\u6570\u636e\u91c7\u96c6\u4e0e\u64cd\u4f5c\u7b56\u7565\u90e8\u7f72\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4eba\u673a\u6280\u80fd\u8fc1\u79fb\u7684\u9886\u57df\u9e3f\u6c9f\uff0c\u5bf9\u63a8\u52a8\u673a\u5668\u4eba\u6cdb\u5316\u64cd\u4f5c\u80fd\u529b\u53d1\u5c55\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.19125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u6a21\u578b\u5f15\u5bfc\u7684\u591a\u7ef4\u7279\u5f81\u7f16\u7801\u4e0e\u52a8\u6001\u805a\u7c7b\u7684\u5206\u5c42\u5b66\u672f\u8bba\u6587\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u9ad8\u8d28\u91cf\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u6587\u732e\u5206\u7c7b\u65b9\u6cd5\uff08\u5982\u65e0\u76d1\u7763\u805a\u7c7b\u6216\u76f4\u63a5\u5927\u6a21\u578b\u63d0\u793a\uff09\u5f80\u5f80\u7f3a\u4e4f\u5c42\u6b21\u6027\u3001\u4e00\u81f4\u6027\u548c\u7ec6\u7c92\u5ea6\uff0c\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u6587\u732e\u7ec4\u7ec7\u548c\u603b\u7ed3\u7684\u9700\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc6\u522b\u6bcf\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8981\u7d20\uff08\u5982\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u7b49\uff09\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u8981\u7d20\u751f\u6210\u7279\u5b9a\u7ef4\u5ea6\u7684\u6458\u8981\u6587\u672c\u3002\u968f\u540e\uff0c\u5c06\u6bcf\u4e2a\u65b9\u9762\u7684\u6458\u8981\u8fdb\u884c\u7f16\u7801\u5e76\u52a8\u6001\u805a\u7c7b\uff0c\u4ece\u800c\u6cbf\u4e0d\u540c\u8981\u7d20\u6784\u5efa\u51fa\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u7ec6\u81f4\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u4f5c\u8005\u6253\u9020\u4e86\u7531156\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u5206\u7c7b\u4f53\u7cfb\uff08\u6db5\u76d61.16\u4e07\u7bc7\u8bba\u6587\uff09\u4f5c\u4e3a\u8bc4\u6d4b\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5206\u7c7b\u7684\u4e00\u81f4\u6027\u3001\u5c42\u6b21\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u6587\u4e2d\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5927\u6a21\u578b\u7406\u89e3\u548c\u591a\u7ef4\u805a\u7c7b\u663e\u8457\u63d0\u5347\u4e86\u6587\u732e\u5206\u7c7b\u7684\u8d28\u91cf\u548c\u7cbe\u7ec6\u5ea6\uff0c\u4e3a\u5b66\u672f\u77e5\u8bc6\u7684\u7ec4\u7ec7\u548c\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2509.18613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18613", "abs": "https://arxiv.org/abs/2509.18613", "authors": ["Yuzhi Wu", "Li Xiao", "Jun Liu", "Guangfeng Jiang", "XiangGen Xia"], "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving", "comment": null, "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b214D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e\u6444\u50cf\u5934\u878d\u5408\u7684\u65b0\u578b\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5MLF-4DRCNet\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u96f7\u8fbe\u57283D\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u63a5\u8fd1LiDAR\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u56e0\u5176\u6210\u672c\u4f4e\u548c\u9c81\u68d2\u6027\u5f3a\uff0c\u9010\u6e10\u88ab\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3002\u4f46\u5176\u70b9\u4e91\u7a00\u758f\u4e14\u566a\u58f0\u5927\uff0c\u96be\u4ee5\u5355\u72ec\u80dc\u4efb\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u3002\u73b0\u6709\u96f7\u8fbe-\u6444\u50cf\u5934\u878d\u5408\u591a\u91c7\u7528\u6e90\u4e8eLiDAR\u7684\u878d\u5408\u7b56\u7565\uff0c\u672a\u9488\u5bf94D\u96f7\u8fbe\u7684\u5f31\u70b9\u4f18\u5316\uff0c\u878d\u5408\u7c92\u5ea6\u8f83\u7c97\uff0c\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86MLF-4DRCNet\u591a\u5c42\u878d\u5408\u6846\u67b6\uff0c\u5305\u542b\u589e\u5f3a\u96f7\u8fbe\u70b9\u7f16\u7801\uff08ERPE\uff09\u3001\u5206\u5c42\u573a\u666f\u878d\u5408\u6c60\u5316\uff08HSFP\uff09\u3001\u63d0\u6848\u7ea7\u878d\u5408\u589e\u5f3a\uff08PLFE\uff09\u4e09\u5927\u6a21\u5757\u3002ERPE\u5728\u70b9\u7ea7\u5bf9\u96f7\u8fbe\u70b9\u4e91\u8fdb\u884c\u5bc6\u96c6\u5316\u548c\u7f16\u7801\uff0cHSFP\u5229\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u5c06\u4f53\u7d20\u4e0e\u56fe\u50cf\u7279\u5f81\u5728\u591a\u5c3a\u5ea6\u573a\u666f\u5c42\u7ea7\u8fdb\u884c\u52a8\u6001\u878d\u5408\uff0c\u5e76\u6c60\u5316\u7279\u5f81\uff0cPLFE\u5728\u63d0\u6848\u533a\u57df\u8fdb\u4e00\u6b65\u878d\u5408\u4e0e\u4f18\u5316\u533a\u57df\u7279\u5f81\uff0c\u83b7\u5f97\u66f4\u52a0\u7cbe\u7ec6\u7684\u4e09\u7ef4\u68c0\u6d4b\u7ed3\u679c\u3002", "result": "\u5728View-of-Delft (VoD)\u548cTJ4DRadSet\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMLF-4DRCNet\u57283D\u76ee\u6807\u68c0\u6d4b\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7ed3\u679c\uff0c\u4e14\u5728VoD\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u63a5\u8fd1\u4e8e\u57fa\u4e8eLiDAR\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "MLF-4DRCNet\u901a\u8fc7\u591a\u5c42\u6b21\u9ad8\u6548\u878d\u54084D\u96f7\u8fbe\u4e0e\u6444\u50cf\u5934\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u96f7\u8fbe3D\u68c0\u6d4b\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u591a\u4f20\u611f\u5668\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e94\u5bf9\u673a\u5668\u4eba\u96c6\u7fa4\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u5206\u5e03\u5f0f\u5171\u8bc6\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\uff0c\u5e76\u5728\u7f16\u961f\u63a7\u5236\u573a\u666f\u4e0b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u96c6\u7fa4\u5bb9\u9519\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6c38\u4e45\u6027\u6545\u969c\uff0c\u800c\u5bf9\u95f4\u6b47\u6027\u6545\u969c\u5173\u6ce8\u8f83\u5c11\uff0c\u539f\u56e0\u5728\u4e8e\u96c6\u7fa4\u7f51\u7edc\u7ed3\u6784\u52a8\u6001\u4e14\u96be\u4ee5\u68c0\u6d4b\u95f4\u6b47\u6027\u6545\u969c\u3002\u5168\u65b0SoNS\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u6301\u4e45\u5316\u7f51\u7edc\u7ed3\u6784\uff0c\u4e3a\u68c0\u6d4b\u95f4\u6b47\u6027\u6545\u969c\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u96c6\u7fa4\u6301\u4e45\u5316\u7f51\u7edc\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u524d\u77bb-\u53cd\u5e94\u7ed3\u5408\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\u3002\u524d\u77bb\u5730\uff0c\u81ea\u7ec4\u7ec7\u52a8\u6001\u5907\u4efd\u8def\u5f84\uff0c\u9002\u5e94\u4e3b\u7f51\u7edc\u62d3\u6251\u548c\u673a\u5668\u4eba\u4f4d\u7f6e\u53d8\u5316\u3002\u53cd\u5e94\u5730\uff0c\u901a\u8fc7\u4e00\u6b21\u6027\u4f3c\u7136\u6bd4\u68c0\u9a8c\u5bf9\u591a\u8def\u7f51\u7edc\u8def\u5f84\u4e0a\u7684\u4fe1\u606f\u8fdb\u884c\u6bd4\u5bf9\uff0c\u65e9\u671f\u68c0\u6d4b\u6545\u969c\u3002\u68c0\u6d4b\u5230\u6545\u969c\u540e\uff0c\u901a\u4fe1\u81ea\u7ec4\u7ec7\u5730\u4e34\u65f6\u5207\u6362\u8def\u5f84\uff0c\u76f4\u81f3\u6545\u969c\u6d88\u5931\u3002", "result": "\u5728\u7f16\u961f\u63a7\u5236\u4e2d\u6a21\u62df\u4f4d\u7f6e\u6570\u636e\u95f4\u6b47\u6027\u6545\u969c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u907f\u514d\u6545\u969c\u5f71\u54cd\u96c6\u7fa4\u6536\u655b\uff0c\u5177\u5907\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u81ea\u7ec4\u7ec7\u7684\u5907\u4efd\u548c\u591a\u8def\u7f51\u7edc\u5171\u8bc6\u673a\u5236\u4f7f\u673a\u5668\u4eba\u96c6\u7fa4\u5bf9\u95f4\u6b47\u6027\u6545\u969c\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765\u6301\u4e45\u5316\u7ed3\u6784\u7684\u673a\u5668\u4eba\u96c6\u7fa4\u5e94\u7528\u3002"}}
{"id": "2509.19143", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201canecdoctoring\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u5728\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u73af\u5883\u4e0b\u751f\u6210\u9488\u5bf9\u751f\u6210\u5f0fAI\u7684\u5bf9\u6297\u6027\u653b\u51fb\u6570\u636e\uff0c\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u7f13\u89e3AI\u751f\u6210\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\uff0c\u5229\u7528AI\u5236\u9020\u865a\u5047\u4fe1\u606f\uff08\u5373\u4fe1\u606f\u8bef\u5bfc\uff09\u5df2\u6210\u4e3a\u6700\u7a81\u51fa\u7684\u98ce\u9669\u4e4b\u4e00\u3002\u76ee\u524d\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6d4b\u8bd5\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u7f8e\u56fd\u548c\u82f1\u8bed\u73af\u5883\uff0c\u7f3a\u4e4f\u5168\u7403\u5316\u3001\u8de8\u6587\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u8986\u76d6\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u73af\u5883\u7684\u7cfb\u7edf\u6027\u653b\u51fb\u6784\u5efa\u65b0\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e86\u6765\u81ea\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u5370\u5730\u8bed\uff08\u7f8e\u56fd\u548c\u5370\u5ea6\u4e24\u4e2a\u5730\u533a\uff09\u7684\u865a\u5047\u4fe1\u606f\u4e3b\u5f20\uff1b\u901a\u8fc7\u805a\u7c7b\u751f\u6210\u66f4\u5e7f\u6cdb\u7684\u53d9\u4e8b\u7c7b\u522b\uff0c\u5e76\u7528\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u63cf\u8ff0\uff0c\u518d\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u589e\u5f3a\u653b\u51fb\u7528\u5927\u6a21\u578b\uff0c\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u653b\u51fb\u63d0\u793a\u3002", "result": "\u6240\u63d0\u51fa\u7684anecdoctoring\u65b9\u6cd5\uff0c\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u653b\u51fb\u89e3\u91ca\u6027\uff08\u76f8\u8f83\u4e8efew-shot\u65b9\u6cd5\uff09\u65b9\u9762\u90fd\u6709\u4f18\u52bf\uff0c\u80fd\u66f4\u597d\u5730\u8bc4\u4f30AI\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u3001\u8bed\u8a00\u4e0b\u88ab\u8bef\u7528\u5236\u9020\u5047\u4fe1\u606f\u7684\u98ce\u9669\u3002", "conclusion": "\u5e94\u5bf9\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u5168\u7403\u8303\u56f4\u865a\u5047\u4fe1\u606f\u98ce\u9669\uff0c\u9700\u8981\u5efa\u7acb\u771f\u5b9e\u4e16\u754c\u3001\u5b9e\u7528\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u5bf9\u6297\u6d4b\u8bd5\u673a\u5236\u3002anecdoctoring\u4e3a\u5168\u7403AI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u5957\u6709\u6548\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5PDLS\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5c06\u635f\u574f\u56fe\u50cf\u9006\u5411\u6620\u5c04\u5230\u6269\u6563\u6a21\u578b\u7684\u6f5c\u7a7a\u95f4\uff0c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u4e0a\u90fd\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u9006\u5411\u5230\u6f5c\u7a7a\u95f4\u7684\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u7528\u4e00\u4e2a\u6f5c\u5411\u91cf\u8868\u793a\u539f\u56fe\uff0c\u9020\u6210\u91cd\u5efa\u65f6\u5728\u7ec6\u8282\u548c\u8bed\u4e49\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5f80\u5f80\u51fa\u73b0\u7ec6\u8282\u6a21\u7cca\u3001\u8bed\u4e49\u504f\u79bb\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u53ef\u4ee5\u540c\u65f6\u517c\u987e\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8bed\u4e49\u51c6\u786e\u6027\u7684\u9006\u5411\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86Prompt-Guided Dual Latent Steering\uff08PDLS\uff09\uff0c\u57fa\u4e8eRectified Flow\u6a21\u578b\uff0c\u5c06\u9006\u5411\u8fc7\u7a0b\u5206\u4e3a\u7ed3\u6784\u8def\u5f84\uff08\u4fdd\u8bc1\u539f\u56fe\u7ed3\u6784\uff09\u548c\u8bed\u4e49\u8def\u5f84\uff08\u901a\u8fc7prompt\u5f15\u5bfc\uff09\u3002\u8fd9\u4e24\u6761\u8def\u5f84\u901a\u8fc7\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5f62\u5f0f\u5316\uff0c\u5e76\u7528\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u63a8\u5bfc\u51fa\u89e3\u6790\u89e3\uff0c\u65e0\u9700\u4e3a\u6bcf\u5f20\u56fe\u5355\u72ec\u4f18\u5316\uff0c\u52a8\u6001\u5f15\u5bfc\u6269\u6563\u9006\u5411\u8fc7\u7a0b\u3002", "result": "\u5728FFHQ-1K\u548cImageNet-1K\u4e0a\u8fdb\u884c\u4e86\u5305\u62ec\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u81ea\u7531\u4fee\u8865\u7b49\u591a\u79cd\u9006\u5411\u4efb\u52a1\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0cPDLS\u5728\u7ed3\u6784\u4fdd\u771f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u90fd\u660e\u663e\u4f18\u4e8e\u5355\u6f5c\u53d8\u91cf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PDLS\u80fd\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u635f\u574f\u56fe\u50cf\u9006\u5411\u5230\u6269\u6563\u6a21\u578b\u6f5c\u7a7a\u95f4\u7684\u8d28\u91cf\uff0c\u5b9e\u73b0\u66f4\u52a0\u7ed3\u6784\u4fdd\u771f\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u91cd\u5efa\u3002"}}
{"id": "2509.19261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u793a\u8303\u4e0e\u9ad8\u6548\u6293\u53d6\u8fc7\u6e21\u7684\u65b0\u578b\u53cc\u81c2\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8fc7\u6e21\u7b56\u7565\u548c\u8fd0\u52a8\u6027\u80fd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e0b\u6293\u53d6\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\u4e0e\u7075\u5de7\u5ea6\u3002", "motivation": "\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u64cd\u4f5c\u5e38\u5e38\u9700\u8981\u5728\u4e0d\u540c\u6293\u53d6\u65b9\u5f0f\u95f4\u81ea\u5982\u5207\u6362\u4ee5\u4fdd\u6301\u7a33\u5b9a\u548c\u9ad8\u6548\u3002\u4f46\u5f53\u524d\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u5916\u90e8\u529b\u53d8\u5316\uff0c\u6293\u53d6\u5207\u6362\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u517c\u987e\u8fd0\u52a8\u6027\u80fd\u4e0e\u5b9e\u65f6\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4eff\u771f\u5f15\u5bfc\u7684\u53cc\u81c2\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u3002\u91c7\u7528\u65b0\u7684\u6293\u53d6\u6d41\u5f62\u7a33\u5b9a\u4ea4\u96c6\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u5355\u81c2\u6293\u53d6\u548c\u53cc\u81c2\u6293\u53d6\u4e4b\u95f4\u7684\u65e0\u7f1d\u8fc7\u6e21\uff0c\u540c\u65f6\u5f15\u5165\u5206\u5c42\u53cc\u9636\u6bb5\u8fd0\u52a8\u67b6\u6784\uff1a\u5229\u7528\u4eff\u771f\u5b66\u4e60\u7684\u5168\u5c40\u8def\u5f84\u751f\u6210\u548c\u4e8c\u6b21\u89c4\u5212\u9a71\u52a8\u7684\u5c40\u90e8\u89c4\u5212\uff0c\u786e\u4fdd\u8fd0\u52a8\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3001\u907f\u969c\u548c\u64cd\u4f5c\u6027\u80fd\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u9ad8\u529b\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u5207\u6362\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u53d7\u529b\u590d\u6742\u548c\u8fd0\u52a8\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u7075\u5de7\u5ea6\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u589e\u5f3a\u4e86\u5b9e\u9645\u5e94\u7528\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.19163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\"slop\"\uff08\u610f\u6307\u4f4e\u8d28\u91cfAI\u6587\u672c\uff09\u7684\u5b9a\u4e49\u4e0e\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u5efa\u7acb\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u53d1\u73b0\"slop\"\u5224\u65ad\u5177\u6709\u4e3b\u89c2\u6027\u4f46\u4e0e\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u7b49\u5185\u5728\u7ef4\u5ea6\u76f8\u5173\u3002\u63d0\u51fa\u7684\u6846\u67b6\u6709\u52a9\u4e8eAI\u6587\u672c\u68c0\u6d4b\u548c\u504f\u597d\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u7684\u4f4e\u8d28\u91cf\u6587\u672c\u73b0\u8c61\u666e\u904d\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bfc\u81f4\u96be\u4ee5\u7cfb\u7edf\u5224\u5b9a\u548c\u6539\u8fdbAI\u6587\u672c\u8d28\u91cf\u3002\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8be5\u9886\u57df\u65b9\u6cd5\u8bba\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u4e0eNLP\u3001\u5199\u4f5c\u548c\u54f2\u5b66\u9886\u57df\u4e13\u5bb6\u8bbf\u8c08\uff0c\u5efa\u7acbAI\u6587\u672c\"slop\"\u5206\u7c7b\u6cd5\uff1b\u518d\u57fa\u4e8e\u7ef4\u5ea6\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u91cf\u8868\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u7247\u6bb5\u6807\u6ce8\u5b9e\u8bc1\u6570\u636e\uff0c\u8bc4\u4f30\u4e3b\u89c2\u6027\u4e0e\u76f8\u5173\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\"slop\"\u7684\u4e8c\u5143\u5224\u5b9a\u5b58\u5728\u4e00\u5b9a\u4e3b\u89c2\u6027\uff0c\u4f46\u4e0e\u8fde\u8d2f\u6027\u3001\u76f8\u5173\u6027\u7b49\u7ef4\u5ea6\u5b58\u6b63\u76f8\u5173\u3002\u6240\u5efa\u8bc4\u4f30\u6846\u67b6\u53ef\u7528\u4e8eAI\u6587\u672c\u68c0\u6d4b\u4e0e\u504f\u597d\u4e8c\u9009\u4e00\u7b49\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\"slop\"\u5b9a\u4e49\u548c\u8bc4\u4f30\u4f53\u7cfb\u4e3aAI\u6587\u672c\u8d28\u91cf\u5224\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5f71\u54cd\u8d28\u91cf\u7684\u8bed\u8a00\u4e0e\u98ce\u683c\u56e0\u7d20\uff0c\u63a8\u52a8\u751f\u6210\u6587\u672c\u8d28\u91cf\u63d0\u5347\u3002"}}
{"id": "2509.18638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18638", "abs": "https://arxiv.org/abs/2509.18638", "authors": ["Yiwei Lyu", "Samir Harake", "Asadur Chowdury", "Soumyanil Banerjee", "Rachel Gologorsky", "Shixuan Liu", "Anna-Katharina Meissner", "Akshay Rao", "Chenhui Zhao", "Akhil Kondepudi", "Cheng Jiang", "Xinhai Hou", "Rushikesh S. Joshi", "Volker Neuschmelting", "Ashok Srinivasan", "Dawn Kleindorfer", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Learning neuroimaging models from health system-scale data", "comment": null, "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u5f71\u50cfAI\u57fa\u7840\u6a21\u578bPrima\uff0c\u53ef\u5bf9\u5b9e\u9645MRI\u4e34\u5e8a\u6570\u636e\u8fdb\u884c\u591a\u75c5\u79cd\u8bca\u65ad\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u5907\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\uff0c\u80fd\u591f\u63d0\u5347\u8bca\u65ad\u6548\u7387\u3001\u51cf\u8f7b\u7cfb\u7edf\u8d1f\u62c5\u5e76\u51cf\u5c11\u5065\u5eb7\u8d44\u6e90\u4e0d\u5747\u3002", "motivation": "\u7531\u4e8eMRI\u68c0\u67e5\u9700\u6c42\u6fc0\u589e\uff0c\u7ed9\u5168\u7403\u533b\u7597\u7cfb\u7edf\u5e26\u6765\u5f88\u5927\u538b\u529b\uff0c\u5c24\u5176\u52a0\u5267\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u60a3\u8005\u7684\u4e0d\u5e73\u7b49\u4e0e\u8bca\u65ad\u5ef6\u8fdf\u3002\u73b0\u6709AI\u5de5\u5177\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u771f\u5b9e\u3001\u5927\u89c4\u6a21\u4e34\u5e8aMRI\u6570\u636e\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u4ee5\u4e00\u4e2a\u5927\u578b\u5b66\u672f\u533b\u7597\u7cfb\u7edf\u4e3a\u6570\u636e\u5f15\u64ce\uff0c\u6784\u5efa\u5e76\u8bad\u7ec3\u4e86\u57fa\u4e8e\u5206\u5c42\u89c6\u89c9\u67b6\u6784\u7684Vision Language Model\uff08VLM\uff09\u2014\u2014Prima\uff0c\u6a21\u578b\u4ee522\u4e07\u4f59\u4efdMRI\u7814\u7a76\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u57281\u5e74\u5185\u5f00\u5c55\u5305\u542b3\u4e07\u4f8bMRI\u7684\u591a\u75c5\u79cd\u4e34\u5e8a\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4e3b\u6d41AI\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "Prima\u6a21\u578b\u5bf9\u4e8e\u6d89\u53ca52\u79cd\u4e3b\u8981\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684MRI\u68c0\u67e5\uff0c\u5e73\u5747ROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u8fbe\u523092.0\uff0c\u5168\u9762\u4f18\u4e8e\u5176\u4ed6\u901a\u7528\u53ca\u533b\u5b66\u7c7bSOTA AI\u6a21\u578b\uff1b\u652f\u6301\u75be\u75c5\u9274\u522b\u3001\u4f18\u5148\u7ea7\u6392\u5e8f\u4e0e\u8f6c\u8bca\u5efa\u8bae\uff1b\u4e0d\u540c\u7fa4\u4f53\u95f4\u4fdd\u6301\u516c\u5e73\u6027\uff0c\u65e0\u660e\u663e\u504f\u89c1\u6216\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "Prima\u5c55\u793a\u4e86\u5728\u5065\u5eb7\u5927\u7cfb\u7edf\u573a\u666f\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e34\u5e8a\u5de5\u4f5c\u6548\u7387\uff0c\u8fd8\u5177\u6709\u964d\u4f4e\u7cfb\u7edf\u6027\u4e0d\u5e73\u7b49\u7684\u80fd\u529b\uff0c\u662fAI\u63a8\u52a8\u795e\u7ecf\u5f71\u50cf\u533b\u7597\u8fdb\u6b65\u7684\u91cd\u8981\u91cc\u7a0b\u7891\u3002"}}
{"id": "2509.19292", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSOE\uff08Self-Improvement via On-Manifold Exploration\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u63a2\u7d22\u80fd\u529b\u3002SOE\u901a\u8fc7\u5b66\u4e60\u7d27\u51d1\u7684\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ec5\u5728\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\u63a2\u7d22\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u591a\u6837\u5316\u7684\u7b56\u7565\u6539\u8fdb\uff0c\u5e76\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u7684\u63a2\u7d22\u80fd\u529b\u6709\u9650\uff0c\u5e38\u56e0\u52a8\u4f5c\u6a21\u5f0f\u574d\u7f29\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u4e3b\u6d41\u7684\u63a2\u7d22\u589e\u5f3a\u6cd5\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u6270\u52a8\uff0c\u5b58\u5728\u5b89\u5168\u548c\u7a33\u5b9a\u6027\u9690\u60a3\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u63a2\u7d22\u65b9\u6cd5\u6765\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\u3002", "method": "SOE\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u53ea\u5728\u5bf9\u5e94\u7684\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\u8fdb\u884c\u63a2\u7d22\u3002\u8fd9\u79cd\u63a2\u7d22\u673a\u5236\u4fdd\u969c\u4e86\u63a2\u7d22\u7684\u5b89\u5168\u6027\u3001\u7b56\u7565\u6267\u884c\u7684\u591a\u6837\u6027\u4e0e\u6709\u6548\u6027\u3002SOE\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u96c6\u6210\u4e8e\u5404\u7c7b\u7b56\u7565\u6a21\u578b\uff0c\u4e14\u4e0d\u4f1a\u5f71\u54cd\u539f\u6709\u7b56\u7565\u8868\u73b0\u3002\u540c\u65f6\uff0c\u5176\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u8fd8\u652f\u6301\u4eba\u7c7b\u5f15\u5bfc\u63a2\u7d22\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u53ef\u63a7\u6027\u3002", "result": "\u5728\u5927\u91cf\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cSOE\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u63a2\u7d22\u7684\u5e73\u6ed1\u6027\u4e0e\u5b89\u5168\u6027\u3001\u6837\u672c\u5229\u7528\u7387\u7b49\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SOE\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u52a0\u5b89\u5168\u548c\u9ad8\u6548\u7684\u7b56\u7565\u63a2\u7d22\u4e0e\u81ea\u6211\u6539\u8fdb\uff0c\u786e\u7acb\u4e86\u57fa\u4e8e\u6d41\u5f62\u63a2\u7d22\u7684\u7b56\u7565\u4f18\u5316\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.19170", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f4\u63a5\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u94fe\uff08CoT\uff09\u9636\u6bb5\u4f7f\u7528\u8fde\u7eedToken\u7684\u65b9\u6cd5\uff0c\u65b9\u6cd5\u53ef\u4f38\u7f29\u4e14\u65e0\u9700\u79bb\u6563Token\u8f68\u8ff9\u7684\u84b8\u998f\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7b49\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u79bb\u6563Token\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u7528\u8fde\u7eedToken\u80fd\u7406\u8bba\u4e0a\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u56e0\u8bad\u7ec3\u96be\u5ea6\u4e0e\u9ad8\u8ba1\u7b97\u6210\u672c\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u9ad8\u6548\u53ef\u6269\u5c55\u5730\u8bad\u7ec3\u8fde\u7eedToken\uff0c\u4ece\u800c\u53d1\u6325\u5176\u7406\u8bba\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u8f93\u5165\u7aef\u5f15\u5165\u5e26\u566a\u58f0\u7684\u201c\u8f6f\u201dToken\uff08Token\u7684\u8fde\u7eed\u6df7\u5408\uff09\uff0c\u540c\u65f6\u907f\u514d\u4ece\u79bb\u6563Token\u84b8\u998f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002\u65b9\u6cd5\u53ef\u5904\u7406\u4e0a\u767eToken\u957f\u5ea6\u7684\u8fde\u7eed\u63a8\u7406\u94fe\u3002", "result": "\u5728Llama\u548cQwen\u7b49\u6a21\u578b\u4e0a\uff0c\u8bad\u7ec3\u5f97\u5230\u7684\u8fde\u7eedCoT\u6a21\u578b\u5728pass@1\u6027\u80fd\u4e0e\u79bb\u6563\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728pass@32\uff08\u591a\u6837\u6027\uff09\u4e0a\u8d85\u8d8a\u79bb\u6563\u65b9\u6cd5\u3002\u8bad\u7ec3\u4e2d\u7528\u8fde\u7eedToken\uff0c\u63a8\u7406\u65f6\u7528\u79bb\u6563Token\u8868\u73b0\u6700\u597d\u3002\u540c\u65f6\uff0c\u8fde\u7eedCoT\u7684RL\u8bad\u7ec3\u80fd\u66f4\u597d\u4fdd\u6301\u539f\u6a21\u578b\u5728\u8de8\u9886\u57df\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7RL\u5927\u89c4\u6a21\u5b66\u4e60\u8fde\u7eedCoT\u7684\u65b9\u6cd5\uff0c\u4e0d\u4f46\u63d0\u5347\u4e86\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e5f\u80fd\u65e0\u7f1d\u90e8\u7f72\u6807\u51c6\u79bb\u6563Token\u63a8\u7406\uff0c\u4e3aLLM\u63a8\u7406\u94fe\u8bad\u7ec3\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u6a21\u578b\u63a8\u7406\u6846\u67b6\u2014\u2014\u7406\u89e3\u4e2d\u751f\u6210\uff08UiG\uff09\uff0c\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a0\u5165\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u65b9\u6cd5\u5c06\u7406\u89e3\u4e0e\u751f\u6210\u8fc7\u7a0b\u5272\u88c2\uff0c\u5bfc\u81f4\u7edf\u4e00\u6a21\u578b\u5728\u8865\u8db3\u751f\u6210\u80fd\u529b\u4e0d\u8db3\u65f6\u65e0\u6cd5\u8fdb\u884c\u6709\u6548\u63a8\u7406\u5f15\u5bfc\uff0c\u9650\u5236\u4e86\u5176\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u7406\u89e3\u4e2d\u751f\u6210\uff08UiG\uff09\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u7f16\u8f91\u4f5c\u4e3a\u6865\u6881\uff0c\u5c06\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u878d\u5408\u5230\u751f\u6210\u6d41\u7a0b\u3002\u5728\u751f\u6210\u521d\u6b65\u56fe\u50cf\u540e\uff0c\u7528\u7edf\u4e00\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u5236\u5b9a\u7f16\u8f91\u6307\u4ee4\uff0c\u5206\u6b65\u8fed\u4ee3\u7f16\u8f91\u4f18\u5316\u56fe\u50cf\uff0c\u5b9e\u73b0\u7406\u89e3\u4e0e\u751f\u6210\u7684\u6df1\u5ea6\u7ed3\u5408\u3002", "result": "UiG\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5982\u5728TIIF\u57fa\u51c6\u7684\u957f\u6587\u672c\u8bbe\u5b9a\u4e0b\u63d0\u5347\u4e863.92%\u3002", "conclusion": "UiG\u6846\u67b6\u901a\u8fc7\u5c06\u7406\u89e3\u80fd\u529b\u9010\u6b65\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u5f3a\u5316\u4e86\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u5728\u6587\u56fe\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u3001\u6709\u524d\u666f\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.19301", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6b8b\u5dee\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u865a\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002", "motivation": "\u73b0\u6709BC\u867d\u7136\u80fd\u5b9e\u73b0\u51fa\u8272\u7684\u89c6\u89c9\u63a7\u5236\uff0c\u4f46\u53d7\u9650\u4e8e\u6f14\u793a\u6570\u636e\u7684\u8d28\u91cf\u548c\u91c7\u96c6\u6210\u672c\uff0c\u4e14\u5355\u7eaf\u589e\u52a0\u6570\u636e\u56de\u62a5\u6709\u9650\u3002\u800c\u76f4\u63a5\u7528RL\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8bad\u7ec3\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u5b89\u5168\u98ce\u9669\u9ad8\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u81ea\u7531\u5ea6\u548c\u7a00\u758f\u5956\u52b1\u573a\u666f\u3002", "method": "\u91c7\u7528\u4ee5BC\u4e3a\u9ed1\u76d2\u57fa\u7ebf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u79bb\u7ebfRL\u5bf9\u6bcf\u4e00\u6b65\u51b3\u7b56\u505a\u8f7b\u91cf\u7ea7\u7684\u6b8b\u5dee\u6821\u6b63\u3002\u6574\u4e2a\u6846\u67b6\u53ea\u9700\u7a00\u758f\u4e8c\u503c\u5956\u52b1\u4fe1\u53f7\u5373\u53ef\u9a71\u52a8\u5b66\u4e60\uff0c\u65e0\u9700\u5bc6\u96c6\u4eba\u5de5\u53cd\u9988\u3002", "result": "\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u9ad8\u81ea\u7531\u5ea6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5c55\u793a\u4e86\u9996\u4e2a\u57fa\u4e8eRL\u7684\u7075\u5de7\u624b\u7c7b\u4eba\u673a\u5668\u4eba\u5b9e\u673a\u8bad\u7ec3\u7ed3\u679c\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u79cd\u7ed3\u5408BC\u548cRL\u3001\u4ee5\u6b8b\u5dee\u65b9\u5f0f\u4fee\u6b63\u7684\u7b56\u7565\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u4e2d\u7684\u843d\u5730\u5e94\u7528\u5f00\u8f9f\u4e86\u5b9e\u9645\u3001\u6709\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.19199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u65b0\u65b9\u6cd5\u2014\u2014\u5728\u7ebf\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\uff08OPRL\uff09\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u957f\u671f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4fe1\u7528\u5206\u914d\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u957f\u671f\u63a8\u7406\u548c\u51b3\u7b56\u65f6\uff0c\u9762\u4e34\u5956\u52b1\u7a00\u758f\u3001\u4e0d\u6613\u9a8c\u8bc1\u4ee5\u53ca\u4fe1\u7528\u5206\u914d\u56f0\u96be\u7b49\u6311\u6218\u3002\u540c\u65f6\uff0c\u5229\u7528\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u65f6\u53c8\u4f1a\u9047\u5230\u6807\u6ce8\u504f\u5dee\u3001\u5956\u52b1\u6f0f\u6d1e\u3001\u4fe1\u53f7\u65b9\u5dee\u5927\u6216\u72b6\u6001\u7f55\u89c1\u5bfc\u81f4\u5931\u8d25\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u76d1\u7763\u7684\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\u3002", "method": "OPRL\u65b9\u6cd5\u5c06\u8f68\u8ff9\u504f\u597d\u901a\u8fc7\u57fa\u4e8e\u8f68\u8ff9\u7684DPO\u76ee\u6807\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u9aa4\u5956\u52b1\uff0c\u4e0d\u4f9d\u8d56\u989d\u5916rollouts\u6216\u663e\u5f0f\u6b65\u9aa4\u6807\u7b7e\u3002\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e0e\u4ee3\u7406\u7b56\u7565\uff0c\u5c06\u6b65\u9aa4\u5956\u52b1\u4e0e\u6574\u4f53\u56de\u62a5\u7ed3\u5408\uff0c\u7528\u4e8e\u7b56\u7565\u66f4\u65b0\uff0c\u5f62\u6210\u6b63\u53cd\u9988\u95ed\u73af\uff0c\u5e76\u7406\u8bba\u4fdd\u8bc1\u5956\u52b1\u4e0e\u8f68\u8ff9\u504f\u597d\u4e00\u81f4\u3001\u68af\u5ea6\u6709\u754c\u3001\u8bad\u7ec3\u7a33\u5b9a\u3002", "result": "OPRL\u5728WebShop\u3001VisualSokoban\u53caSOTOPIA\uff08\u542b\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f00\u653e\u5f0f\u793e\u4f1a\u4ea4\u4e92\uff09\u7b49\u57fa\u51c6\u4e0a\uff0c\u5747\u4f18\u4e8e\u4e3b\u6d41\u5927\u6a21\u578b\u548c\u5f3a\u52b2\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u53d6\u5f97\u66f4\u4f18\u8bad\u7ec3\u6548\u7387\u3001\u4f4e\u65b9\u5dee\u548c\u66f4\u9ad8\u63a2\u7d22\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "OPRL\u65b9\u6cd5\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5927\u6a21\u578b\u4ee3\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\uff0c\u5177\u5907\u5728\u73b0\u5b9e\u573a\u666f\u63a8\u5e7f\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18642", "abs": "https://arxiv.org/abs/2509.18642", "authors": ["Nicolas Toussaint", "Emanuele Colleoni", "Ricardo Sanchez-Matilla", "Joshua Sutcliffe", "Vanessa Thompson", "Muhammad Asad", "Imanol Luengo", "Danail Stoyanov"], "title": "Zero-shot Monocular Metric Depth for Endoscopic Images", "comment": "Accepted at MICCAI 2025 DEMI Workshop", "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.", "AI": {"tldr": "\u672c\u6587\u805a\u7126\u4e8e\u5355\u76ee\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u7684\u76f8\u5bf9\u548c\u7edd\u5bf9\u6df1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9886\u57df\u6a21\u578b\u5728\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\uff08\u7279\u522b\u662f\u57fa\u4e8etransformer\u7684\u5927\u6a21\u578b\uff09\u5728\u81ea\u7136\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u65f6\uff0c\u7f3a\u4e4f\u6709\u4ee3\u8868\u6027\u7684\u8bc4\u6d4b\u57fa\u51c6\u548c\u516c\u5f00\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u76f8\u5173\u6280\u672f\u7684\u771f\u5b9e\u843d\u5730\u3002", "method": "1. \u7cfb\u7edf\u8bc4\u6d4b\u4e3b\u6d41\uff08\u76f8\u5bf9/\u7edd\u5bf9\uff09\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u771f\u5b9e\u3001\u672a\u89c1\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u7684\u6548\u679c\uff0c\u5206\u6790\u5176\u6cdb\u5316\u80fd\u529b\uff1b2. \u6784\u5efa\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b\u5185\u7aa5\u955c\u624b\u672f\u5668\u68b0\u3001\u5177\u5907\u50cf\u7d20\u7ea7\u771f\u5b9e\u503c\u6807\u6ce8\uff08\u6df1\u5ea6\u548c\u5206\u5272\u63a9\u819c\uff09\u7684\u5168\u65b0\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6 EndoSynth\uff1b3. \u7528EndoSynth\u5bf9\u57fa\u7840\u6df1\u5ea6\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5bf9\u672a\u89c1\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5728EndoSynth\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u591a\u6570\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\u5728\u672a\u89c1\u7684\u771f\u5b9e\u5185\u7aa5\u955c\u6570\u636e\u4e0a\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002\u8be5\u57fa\u51c6\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u53ef\u76f4\u63a5\u652f\u6301\u672a\u6765\u76f8\u5173\u7814\u7a76\u3002", "conclusion": "\u672c\u5de5\u4f5c\u7cfb\u7edf\u5730\u89e3\u51b3\u4e86\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u8bc4\u6d4b\u548c\u6570\u636e\u74f6\u9888\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u5408\u6210\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u548c\u5b9e\u9a8c\u57fa\u7840\u3002"}}
{"id": "2509.19212", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u611f\u77e5\u89e3\u7801\u6846\u67b6SafeCoDe\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u590d\u6742\u4e0a\u4e0b\u6587\u4e0b\u7684\u5b89\u5168\u62d2\u7edd\u80fd\u529b\uff0c\u5e76\u517c\u987e\u6a21\u578b\u53ef\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5bb9\u6613\u51fa\u73b0\u5b89\u5168\u51b3\u7b56\u5931\u8bef\uff0c\u8981\u4e48\u8fc7\u5ea6\u654f\u611f\u62d2\u7edd\u6b63\u5e38\u8bf7\u6c42\uff0c\u8981\u4e48\u5bf9\u5e26\u6709\u89c6\u89c9\u98ce\u9669\u7684\u8bf7\u6c42\u68c0\u6d4b\u4e0d\u8db3\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u505a\u5230\u5b89\u5168\u5bf9\u9f50\u3002", "method": "\u63d0\u51faSafety-aware Contrastive Decoding\uff08SafeCoDe\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u4e3a\u8f7b\u91cf\u7ea7\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u65b9\u5f0f\uff0c\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u5229\u7528\u5bf9\u6bd4\u89e3\u7801\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u4e0e\u566a\u58f0\u56fe\u7247\uff0c\u7a81\u51fa\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\u654f\u611f\u7684token\uff1b2\uff09\u63d0\u51fa\u5168\u5c40\u611f\u77e5\u7684token\u8c03\u8282\u7b56\u7565\uff0c\u5c06\u573a\u666f\u7ea7\u63a8\u7406\u4e0etoken\u8c03\u8282\u7ed3\u5408\uff0c\u6839\u636e\u9884\u6d4b\u5b89\u5168\u6027\u52a8\u6001\u8c03\u6574\u62d2\u7edd\u3002", "result": "\u5728\u591a\u79cdMLLM\u6a21\u578b\u548c\u591a\u7c7b\u5b89\u5168\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cSafeCoDe\u660e\u663e\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4e8e\u4e0a\u4e0b\u6587\u5b89\u5168\u62d2\u7edd\u7684\u8868\u73b0\uff0c\u51cf\u5c11\u4e86\u8bef\u62d2\u548c\u6f0f\u68c0\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6709\u7528\u6027\u3002", "conclusion": "SafeCoDe\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6539\u5584\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\u3002\u5176\u6a21\u578b\u65e0\u5173\u3001\u90e8\u7f72\u7b80\u4fbf\uff0c\u4e3a\u5b9e\u9645\u591a\u6a21\u6001\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18683", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18683", "abs": "https://arxiv.org/abs/2509.18683", "authors": ["Lanhu Wu", "Zilin Gao", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection", "comment": "Accepted to ACM MM 2025", "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LEAF-Mamba\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347RGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0cCNN\u53d7\u9650\u4e8e\u611f\u53d7\u91ce\u5c40\u90e8\u6027\uff0cVision Transformer\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u8fd1\u671f\u7684Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u867d\u7136\u6709\u6548\uff0c\u4f46\u662f\u76f4\u63a5\u5e94\u7528\u5728RGB-D\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u90e8\u8bed\u4e49\u4e0d\u8db3\u548c\u591a\u6a21\u6001\u878d\u5408\u4e0d\u5145\u5206\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86LEAF-Mamba\uff0c\u5305\u62ec\u4e24\u4e2a\u521b\u65b0\u6a21\u5757\uff1a\u4e00\u662f\u5c40\u90e8\u5f3a\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08LE-SSM\uff09\uff0c\u7528\u4e8e\u5728RGB\u548cDepth\u4e24\u4e2a\u6a21\u6001\u4e2d\u6355\u83b7\u591a\u5c3a\u5ea6\u7684\u5c40\u90e8\u4f9d\u8d56\uff1b\u4e8c\u662f\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff08AFM\uff09\uff0c\u5b9e\u73b0\u4e92\u8865\u6027\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u878d\u5408\u3002", "result": "LEAF-Mamba\u572816\u4e2a\u4e3b\u6d41RGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u6b64\u5916\uff0c\u5728RGB-T\u4efb\u52a1\u4e0a\u540c\u6837\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LEAF-Mamba\u80fd\u6709\u6548\u5efa\u6a21\u591a\u5c3a\u5ea6\u5c40\u90e8\u4e0e\u957f\u8ddd\u79bb\u5168\u5c40\u4f9d\u8d56\uff0c\u540c\u65f6\u5728\u591a\u6a21\u6001\u878d\u5408\u4e0e\u4ea4\u4e92\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u4f18\u5f02\u7684\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2509.19224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\uff08Bert Base, BioBert, Bio+Clinical Bert, RoBerta, Clinical Longformer\uff09\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u5176\u5728\u836f\u7269\u76f8\u5173\u4e8b\u4ef6\u62bd\u53d6\u53ca\u4e0a\u4e0b\u6587\u5206\u7c7b\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u6ce8\u610f\u529b\u673a\u5236\u6a21\u578b\u5728\u533b\u5b66NLP\u7279\u522b\u662f\u4e34\u5e8a\u7b14\u8bb0\u5904\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5177\u4f53EHR\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u4f18\u52a3\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u548c\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u4e86Harvard Medical School 2022\u5e74n2c2\u6311\u6218\u7684Track 1\u4efb\u52a1\u548cCMED\u6570\u636e\u96c6\uff0c\u5bf9\u5404\u9884\u8bad\u7ec3\u6a21\u578b\u5206\u522b\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u5c55\u836f\u7269\u62bd\u53d6\u3001\u533b\u5b66\u4e8b\u4ef6\u68c0\u6d4b\u548c\u591a\u7ef4\u5ea6\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u3002\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86EHR\u5206\u5272\u4e0e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u51c6\u7684\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u4e0eF1\u5206\u6570\u8fdb\u884c\u7efc\u5408\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1a\u5728\u836f\u7269\u548c\u836f\u7269\u4e8b\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u57fa\u4e8e\u4e34\u5e8a\u6216\u751f\u7269\u533b\u5b66\u9884\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u578b\uff08\u5982BioBert\u3001Bio+Clinical Bert\u7b49\uff09\u66f4\u4e3a\u6709\u6548\uff1b\u800c\u5728\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u7528\u9886\u57df\u9884\u8bad\u7ec3\u7684Bert Base\u6a21\u578b\u53cd\u800c\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9488\u5bf9EHR\u7684\u591a\u9879\u76f8\u5173NLP\u4efb\u52a1\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7ec6\u5206\u5b50\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u4e34\u5e8a\u3001\u533b\u5b66\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5408\u836f\u7269\u4e8b\u4ef6\u62bd\u53d6\uff0c\u4f46\u901a\u7528\u6a21\u578b\u5728\u590d\u6742\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\u4e2d\u4ea6\u6709\u4f18\u52bf\uff0c\u9700\u7ed3\u5408\u5b9e\u9645\u9700\u6c42\u9009\u7528\u5408\u9002\u6a21\u578b\u3002"}}
{"id": "2509.18692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18692", "abs": "https://arxiv.org/abs/2509.18692", "authors": ["Xinle Gao", "Linghui Ye", "Zhiyong Xiao"], "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification", "comment": null, "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u98df\u7269\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u968f\u7740\u793e\u4f1a\u548c\u79d1\u6280\u7684\u8fdb\u6b65\uff0c\u98df\u54c1\u5de5\u4e1a\u5bf9\u751f\u4ea7\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u8981\u6c42\u8d8a\u6765\u8d8a\u9ad8\u3002\u98df\u7269\u56fe\u50cf\u5206\u7c7b\u5728\u81ea\u52a8\u5316\u8d28\u68c0\u3001\u98df\u54c1\u5b89\u5168\u76d1\u7ba1\u53ca\u667a\u80fd\u519c\u4e1a\u751f\u4ea7\u7b49\u9886\u57df\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u4e14\u80fd\u517c\u987e\u9ad8\u6548\u4e0e\u9ad8\u7cbe\u5ea6\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u65b9\u6cd5\u7ed3\u5408\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08WMHAM\uff09\u4e0e\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\u3002WMHAM\u901a\u8fc7\u9ad8\u6548\u7684\u7a97\u53e3\u5212\u5206\uff0c\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff1bSAM\u5219\u81ea\u9002\u5e94\u5730\u5173\u6ce8\u5173\u952e\u7a7a\u95f4\u533a\u57df\uff0c\u63d0\u5347\u7279\u5f81\u533a\u5206\u80fd\u529b\u3002", "result": "\u5728Food-101\u548cVireo Food-172\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5206\u522b\u53d6\u5f97\u4e8695.24%\u548c94.33%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u53c2\u6570\u91cf\u548cFLOPs\u4e0a\u663e\u8457\u4f4e\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u6548\u5e73\u8861\uff0c\u7279\u522b\u9002\u5408\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.19228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompLLM\u7684\u8f6f\u6027\u4e0a\u4e0b\u6587\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u5206\u5272\uff0c\u518d\u5206\u522b\u72ec\u7acb\u538b\u7f29\u5404\u6bb5\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u3001\u7f13\u5b58\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u539f\u59cb\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9762\u4e34\u8ba1\u7b97\u74f6\u9888\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff0c\u96be\u4ee5\u9ad8\u6548\u5e94\u5bf9\u6709\u91cd\u53e0\u4e0a\u4e0b\u6587\u7684\u591a\u8f6e\u67e5\u8be2\u3002", "method": "CompLLM\u5c06\u8f93\u5165\u6587\u672c\u5212\u5206\u4e3a\u591a\u4e2a\u7247\u6bb5\uff0c\u5bf9\u6bcf\u4e2a\u7247\u6bb5\u72ec\u7acb\u538b\u7f29\uff0c\u5e76\u652f\u6301\u538b\u7f29\u7247\u6bb5\u7684\u7f13\u5b58\u4e0e\u590d\u7528\u3002\u76f8\u6bd4\u6574\u4f53\u538b\u7f29\uff0cCompLLM\u7684\u538b\u7f29\u8fc7\u7a0b\u5bf9\u4e0a\u4e0b\u6587\u957f\u5ea6\u5448\u7ebf\u6027\u6269\u5c55\uff0c\u5e76\u53ef\u5927\u5e45\u8282\u7701KV\u7f13\u5b58\u548c\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57282\u500d\u538b\u7f29\u7387\u4e0b\uff0cCompLLM\u5728\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u65f6\u5c06\u201c\u9996\u6b21\u8f93\u51fa\u5ef6\u8fdf\u201d\u7f29\u77ed\u81f3\u539f\u6765\u76841/4\uff0c\u51cf\u5c11KV\u7f13\u5b58\u533a50%\uff1b\u65e0\u635f\u6027\u80fd\uff0c\u751a\u81f3\u5728\u6781\u957f\u5e8f\u5217\u4e0a\u8d85\u8d8a\u672a\u538b\u7f29\u6a21\u578b\u3002", "conclusion": "CompLLM\u80fd\u591f\u5927\u5e45\u63d0\u5347LLM\u5904\u7406\u957f\u6587\u672c\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u4e0e\u53ef\u590d\u7528\u6027\uff0c\u5177\u5907\u843d\u5730\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2509.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18693", "abs": "https://arxiv.org/abs/2509.18693", "authors": ["Siyi Chen", "Kai Wang", "Weicong Pang", "Ruiming Yang", "Ziru Chen", "Renjun Gao", "Alexis Kai Hon Lau", "Dasa Gu", "Chenchen Zhang", "Cheng Li"], "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery", "comment": "Project is available at\n  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md", "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u5f00\u653e\u96c6\u5730\u8868\u8986\u76d6\u7269\u5206\u6790\u6846\u67b6OSDA\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9065\u611f\u5f71\u50cf\u4e2d\u65b0\u5f02\u7c7b\u522b\u7684\u81ea\u52a8\u53d1\u73b0\u3001\u5206\u5272\u4e0e\u8bed\u4e49\u63cf\u8ff0\u3002", "motivation": "\u9065\u611f\u9886\u57df\u5730\u8868\u8986\u76d6\u5206\u6790\u9762\u4e34\u7740\u7269\u4f53\u7c7b\u522b\u591a\u6837\u4e14\u53d8\u5316\u8fc5\u901f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u7c7b\u522b\u7684\u68c0\u6d4b\u4e0e\u89e3\u91ca\uff0c\u4e9f\u9700\u65e0\u9700\u6807\u6ce8\u3001\u80fd\u5904\u7406\u672a\u77e5\u7c7b\u522b\u5e76\u5177\u5907\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "OSDA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u5229\u7528\u53ef\u63d0\u793a\u7684\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u8fdb\u884c\u7cbe\u786e\u7684\u76ee\u6807\u53d1\u73b0\u4e0e\u63a9\u6a21\u63d0\u53d6\uff1b2\uff09\u901a\u8fc7\u4e8c\u9636\u6bb5\u5fae\u8c03\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e3a\u53d1\u73b0\u533a\u57df\u8d4b\u4e88\u8bed\u4e49\u4e0e\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b3\uff09\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u8bc4\u4ef7\u63a5\u5408\u8fdb\u884c\u6700\u7ec8\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u67b6\u6784\u65e0\u5173\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u3002", "result": "OSDA\u5728\u4e0d\u4f9d\u8d56\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u79cd\u536b\u661f\u9065\u611f\u5f71\u50cf\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u50cf\u7d20\u7ea7\u76ee\u6807\u5206\u5272\u4e0e\u5f00\u653e\u5f0f\u8bed\u4e49\u63cf\u8ff0\uff0c\u8bc4\u4ef7\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u60c5\u5883\u4e0b\u6548\u679c\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u5730\u8868\u76d1\u6d4b\u3001\u81ea\u52a8\u5236\u56fe\u66f4\u65b0\u548c\u5927\u89c4\u6a21\u5730\u7403\u89c2\u6d4b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8868\u660e\u5728\u9065\u611f\u5f71\u50cf\u5f00\u653e\u96c6\u5206\u6790\u4e2d\u5177\u6709\u5f88\u5927\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19249", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u65b9\u6cd5RLPT\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u4e3b\u63a2\u7d22\u6570\u636e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7b97\u529b\u589e\u957f\u8fdc\u5feb\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u79ef\u7d2f\uff0c\u4f20\u7edf\u4f9d\u8d56\u66f4\u591a\u6570\u636e\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u9047\u5230\u4e86\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u65b9\u5f0f\u8fdb\u4e00\u6b65\u91ca\u653eLLM\u6f5c\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51faRLPT\uff0c\u7528\u5f3a\u5316\u5b66\u4e60(\u800c\u975e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60)\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u3002\u5177\u4f53\u505a\u6cd5\u662f\u901a\u8fc7\u201c\u4e0b\u4e00\u6bb5\u63a8\u7406\u201d\u76ee\u6807\uff0c\u6839\u636e\u6a21\u578b\u9884\u6d4b\u540e\u7eed\u6587\u672c\u7684\u51c6\u786e\u6027\u6765\u76f4\u63a5\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002\u8fd9\u79cd\u65b9\u5f0f\u4fc3\u8fdb\u6a21\u578b\u63a2\u7d22\u66f4\u591a\u53ef\u80fd\u8def\u5f84\uff0c\u63d0\u5347\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u901a\u7528\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cRLPT\u5e26\u6765\u4e86\u5927\u5e45\u6027\u80fd\u63d0\u5347\u3002\u4f8b\u5982\uff0cQwen3-4B-Base\u6a21\u578b\u5728MMLU\u3001GPQA-Diamond\u3001AIME\u7b49\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u63d0\u53478.1\u5206\uff0c\u4e14\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "RLPT\u7a81\u7834\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u9650\u5236\uff0c\u4e3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u540c\u65f6\u8fd8\u80fd\u8fdb\u4e00\u6b65\u589e\u5f3a\u5982RLVR\u7b49\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u6548\u679c\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18697", "abs": "https://arxiv.org/abs/2509.18697", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2021: cross-domain plant identification", "comment": "15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation\n  Forum, September 21 to 24, 2021, Bucharest, Romania", "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86LifeCLEF 2021\u690d\u7269\u8bc6\u522b\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u6807\u672c\u9986\u6570\u5b57\u5316\u8d44\u6e90\u548c\u91ce\u5916\u7167\u7247\uff0c\u63d0\u5347\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\u4f46\u6570\u636e\u7a00\u7f3a\u5730\u533a\u7684\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u80fd\u529b\uff0c\u6d4b\u8bd5\u6570\u636e\u4e3b\u8981\u805a\u7126\u4e8e\u5357\u7f8e\u572d\u4e9a\u90a3\u9ad8\u5730\u3002\u8bba\u6587\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u8d44\u6e90\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u53c2\u4e0e\u65b9\u6848\u548c\u4e3b\u8981\u7ed3\u679c\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6781\u5927\u63d0\u5347\u4e86\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u80fd\u529b\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u4e30\u5bcc\u7684\u5317\u7f8e\u548c\u897f\u6b27\uff0c\u751f\u7269\u591a\u6837\u6027\u9ad8\u4f46\u6570\u636e\u7a00\u7f3a\u7684\u70ed\u5e26\u5730\u533a\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u6570\u4e16\u7eaa\u6765\u79ef\u7d2f\u7684\u5927\u91cf\u70ed\u5e26\u5730\u533a\u690d\u7269\u6807\u672c\uff0c\u7531\u4e8e\u6570\u5b57\u5316\u8fdb\u5c55\uff0c\u53ef\u4f5c\u4e3a\u8f85\u52a9\u8bad\u7ec3\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u5982\u4f55\u501f\u52a9\u6807\u672c\u9986\u6570\u636e\u6539\u5584\u8fd9\u4e9b\u5730\u533a\u81ea\u52a8\u8bc6\u522b\u6210\u4e3a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u4ee52021\u5e74LifeCLEF\u690d\u7269\u8bc6\u522b\u6311\u6218\u4e3a\u5e73\u53f0\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u7ea61000\u4e2a\u5357\u7f8e\u572d\u4e9a\u90a3\u9ad8\u5730\u690d\u7269\u79cd\u7684\u5927\u578b\u6570\u636e\u96c6\u3002\u8bad\u7ec3\u96c6\u7531\u6570\u5341\u4e07\u5f20\u6807\u672c\u9986\u6570\u5b57\u56fe\u50cf\u548c\u90e8\u5206\u91ce\u5916\u7167\u7247\u7ec4\u6210\uff0c\u6db5\u76d6\u591a\u9879\u7269\u79cd\u5f62\u6001\u548c\u529f\u80fd\u6027\u72b6\uff0c\u91c7\u7528\u8de8\u9886\u57df\u5206\u7c7b\u4efb\u52a1\u5f62\u5f0f\uff08\u6807\u672c\u4e0e\u73b0\u573a\u7167\u7247\uff09\uff0c\u4ee5\u4fc3\u8fdb\u4e24\u79cd\u6570\u636e\u57df\u7684\u76f8\u4e92\u5b66\u4e60\u3002\u6d4b\u8bd5\u96c6\u5168\u90e8\u4e3a\u5b9e\u5730\u62cd\u6444\u7167\u7247\u3002", "result": "\u8bba\u6587\u5bf9\u53c2\u8d5b\u56e2\u961f\u4f7f\u7528\u7684\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6848\u53ca\u5176\u5728\u6311\u6218\u8d5b\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u591a\u6570\u65b9\u6848\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u6807\u672c\u548c\u73b0\u573a\u56fe\u7247\u7279\u5f81\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u8bc6\u522b\u6548\u679c\u3002\u6574\u4f53\u8bc4\u6d4b\u663e\u793a\uff0c\u5f15\u7528\u6807\u672c\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6807\u672c\u8d2b\u4e4f\u5730\u533a\u690d\u7269\u8bc6\u522b\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u70ed\u5e26\u5730\u533a\u7684\u6807\u672c\u9986\u8d44\u6e90\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u662f\u6539\u8fdb\u751f\u7269\u591a\u6837\u6027\u70ed\u70b9\u533a\u57df\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u7684\u6709\u6548\u9014\u5f84\u3002\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5145\u6570\u636e\u7c7b\u578b\u548c\u7269\u79cd\u8303\u56f4\uff0c\u63a8\u8fdb\u5168\u7403\u690d\u7269\u6570\u5b57\u5316\u8bc6\u522b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.19269", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u63d0\u53d6\u80fd\u8868\u8fbe\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u6982\u5ff5\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u63cf\u8ff0\u7684\u5d4c\u5165\uff0c\u5b9e\u73b0\u5bf9\u7279\u5f81\u7ef4\u5ea6\u7684\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5bf9\u9f50\u7a7a\u95f4\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5c3d\u7ba1\u6982\u5ff5\u7a7a\u95f4\u5728\u8ba4\u77e5\u79d1\u5b66\u548c\u53ef\u89e3\u91caAI\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u96be\u4ee5\u88ab\u6709\u6548\u5b66\u4e60\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4eceLLM\u4e2d\u76f4\u63a5\u63d0\u53d6\u5177\u5907\u8ba4\u77e5\u542b\u4e49\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7528\u539f\u578b\u63cf\u8ff0\uff08\u5982\u63cf\u8ff0\u975e\u5e38\u751c\u7684\u98df\u7269\uff09\u4f5c\u4e3a\u7279\u5f81\u7684\u4ee3\u8868\uff0c\u5c06\u5176\u5d4c\u5165\u4f5c\u4e3a\u7279\u5f81\u7ef4\u5ea6\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03LLM\uff0c\u4f7f\u539f\u578b\u5d4c\u5165\u4e0e\u9884\u8bbe\u6982\u5ff5\u7a7a\u95f4\u5bf9\u5e94\u7ef4\u5ea6\u5bf9\u9f50\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u8be5\u57fa\u4e8e\u539f\u578b\u63cf\u8ff0\u7684\u7a7a\u95f4\u63d0\u53d6\u4e0e\u5bf9\u9f50\u65b9\u6cd5\u6548\u679c\u4f18\u5f02\uff0c\u53ef\u4ee5\u6709\u6548\u83b7\u5f97\u5177\u5907\u53ef\u89e3\u91ca\u6027\u7684\u6982\u5ff5\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u5bf9LLM\u8fdb\u884c\u7ec6\u81f4\u5fae\u8c03\uff0c\u7ed3\u5408\u539f\u578b\u63cf\u8ff0\u5d4c\u5165\uff0c\u53ef\u4ee5\u9ad8\u6548\u3001\u6709\u6548\u5730\u4eceLLM\u4e2d\u63d0\u53d6\u5177\u8ba4\u77e5\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6982\u5ff5\u7a7a\u95f4\uff0c\u63a8\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u8fdb\u7a0b\u3002"}}
{"id": "2509.18699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18699", "abs": "https://arxiv.org/abs/2509.18699", "authors": ["Zedong Zhang", "Ying Tai", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping", "comment": null, "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u7c7b\u522b\u76ee\u6807\u878d\u5408\u65b9\u6cd5AGSwap\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u8de8\u7c7b\u522b\u878d\u5408\u6570\u636e\u96c6COF\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u56fe\u50cf\u4e2d\u591a\u7c7b\u522b\u76ee\u6807\u7684\u878d\u5408\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u65b9\u6cd5\u5728\u5c06\u4e0d\u540c\u7c7b\u522b\u7684\u7269\u4f53\u878d\u5408\u6210\u4e00\u4e2a\u65b0\u7269\u4f53\u65f6\uff0c\u5e38\u5e38\u9762\u4e34\u878d\u5408\u5931\u771f\u3001\u8bed\u4e49\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u5e76\u4e14\u7f3a\u4e4f\u76f8\u5173\u7684\u8bc4\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5236\u7ea6\u4e86\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86AGSwap\u65b9\u6cd5\uff0c\u5305\u62ec\u7ec4\u5185\u7279\u5f81\u4ea4\u6362\uff08Group-wise Embedding Swapping\uff09\u4ee5\u878d\u5408\u4e0d\u540c\u7c7b\u522b\u7684\u8bed\u4e49\u7279\u5f81\u3001\u4ee5\u53ca\u81ea\u9002\u5e94\u7ec4\u66f4\u65b0\uff08Adaptive Group Updating\uff09\u4f9d\u636e\u52a8\u6001\u5e73\u8861\u8bc4\u4f30\u5206\u6570\u4f18\u5316\u878d\u5408\u8fc7\u7a0b\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86COF\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u57fa\u4e8eImageNet-1K\u548cWordNet\uff0c\u5171\u5177\u670995\u4e2a\u8d85\u7ea7\u7c7b\uff0c\u6bcf\u7c7b10\u4e2a\u5b50\u7c7b\uff0c\u53ef\u751f\u621045\u4e07+\u72ec\u7279\u878d\u5408\u5bf9\u3002", "result": "AGSwap\u65b9\u6cd5\u5728\u7b80\u5355\u4e0e\u590d\u6742\u63d0\u793a\u8bcd\u4e0b\u5747\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u7ec4\u5408\u578bT2I\u65b9\u6cd5\uff08\u5982GPT-Image-1\uff09\uff0c\u4e14\u5728\u4f5c\u8005\u81ea\u5efaCOF\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "AGSwap\u4e0d\u4ec5\u65b9\u6cd5\u6709\u6548\uff0cCOF\u6570\u636e\u96c6\u4e5f\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\u548c\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u8de8\u7c7b\u522b\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.19270", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Bo\u017e\u00edk", "Marek \u0160uppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SloPalSpeech\uff0c\u4e00\u4e2a\u5305\u542b2806\u5c0f\u65f6\u65af\u6d1b\u4f10\u514b\u8bed\u8bae\u4f1a\u5f55\u97f3\u7684\u5927\u89c4\u6a21ASR\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u6b64\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86Whisper\u6a21\u578b\u7684\u65af\u6d1b\u4f10\u514b\u8bed\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u65af\u6d1b\u4f10\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u53d1\u5c55\u53d7\u9650\u3002\u8be5\u7814\u7a76\u65e8\u5728\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u65af\u6d1b\u4f10\u514b\u8bedASR\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u5e76\u5904\u7406\u4e86\u5927\u89c4\u6a21\u8bae\u4f1a\u5f55\u97f3\uff0c\u5f00\u53d1\u51fa\u4e00\u5957\u5065\u58ee\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u5c06\u957f\u97f3\u9891\u5207\u5206\u4e3a\u5e72\u51c0\u3001\u9002\u7528\u4e8e\u8bad\u7ec3\u768430\u79d2\u97f3\u9891-\u8f6c\u5f55\u5bf9\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6570\u636e\u5fae\u8c03\u4e86\u591a\u79cdOpenAI Whisper\u6a21\u578b\u3002", "result": "\u5229\u7528SloPalSpeech\u5fae\u8c03\u7684Whisper-small\u6a21\u578b\u5728\u65af\u6d1b\u4f10\u514b\u8bedCommon Voice\u548cFLEURS\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\uff0c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u6700\u9ad8\u4e0b\u964d70%\uff0c\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u7684\u57fa\u7ebf\u8868\u73b0\u3002", "conclusion": "SloPalSpeech\u6781\u5927\u63d0\u5347\u4e86\u65af\u6d1b\u4f10\u514b\u8bedASR\u6a21\u578b\u7684\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6a21\u578b\u95e8\u69db\uff0c\u76f8\u5173\u6570\u636e\u4e0e\u5fae\u8c03\u6a21\u578b\u5747\u5df2\u516c\u5f00\uff0c\u4fc3\u8fdb\u4e86\u4f4e\u8d44\u6e90\u8bed\u79cd\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18705", "abs": "https://arxiv.org/abs/2509.18705", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries", "comment": "13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2019, Lugano, Switzerland", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86LifeCLEF 2019\u690d\u7269\u8bc6\u522b\u6311\u6218\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u690d\u7269\u8bc6\u522b\uff0c\u805a\u7126\u4e8e\u7f3a\u4e4f\u6570\u636e\u7684\u690d\u7269\u591a\u6837\u6027\u533a\u57df\uff0c\u5e76\u5bf9\u6bd4\u4e86\u7cfb\u7edf\u4e0e\u4e13\u5bb6\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u4e86\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u7684\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u4ec5\u8986\u76d6\u5168\u7403\u5f88\u5c0f\u90e8\u5206\u7269\u79cd\uff0c\u7edd\u5927\u591a\u6570\u690d\u7269\u7269\u79cd\u4ecd\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u3002\u5c24\u5176\u662f\u4e9a\u9a6c\u900a\u5317\u90e8\u548c\u572d\u4e9a\u90a3\u5730\u533a\uff0c\u7269\u79cd\u591a\u6837\u4e14\u6570\u636e\u7a00\u7f3a\uff0c\u4e9f\u9700\u5efa\u7acb\u6709\u6548\u7684\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u3002\u8be5\u6311\u6218\u610f\u5728\u8bc4\u4f30\u73b0\u6709\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u5728\u6570\u636e\u7a00\u7f3a\u533a\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u5305\u542b\u7ea61\u4e07\u79cd\u4e3b\u8981\u6765\u81ea\u572d\u4e9a\u90a3\u53ca\u5317\u4e9a\u9a6c\u900a\u70ed\u5e26\u96e8\u6797\u7684\u690d\u7269\u6570\u636e\u96c6\uff0c\u7ec4\u7ec7\u4e86\u793e\u533a\u6311\u6218\uff0c\u9080\u8bf7\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u548c\u63d0\u4ea4\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7b49\u65b9\u6cd5\u7684\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u5c06\u8fd9\u4e9b\u7cfb\u7edf\u7684\u8868\u73b0\u4e0e\u9876\u7ea7\u70ed\u5e26\u690d\u7269\u4e13\u5bb6\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53c2\u4e0e\u56e2\u961f\u91c7\u7528\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u4e4f\u6570\u636e\u5730\u533a\u7684\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u80fd\u529b\u3002\u7cfb\u7edf\u7684\u8bc6\u522b\u51c6\u786e\u7387\u5df2\u5f00\u59cb\u63a5\u8fd1\u751a\u81f3\u5728\u90e8\u5206\u573a\u666f\u4e2d\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u7cfb\u7edf\u5728\u6570\u636e\u7a00\u7f3a\u7684\u9ad8\u751f\u7269\u591a\u6837\u6027\u533a\u57df\u8868\u73b0\u5df2\u5f97\u5230\u589e\u5f3a\uff0c\u90e8\u5206\u9876\u5c16\u7cfb\u7edf\u8868\u73b0\u4f18\u8d8a\uff0c\u663e\u793a\u6df1\u5ea6\u5b66\u4e60\u53ca\u65b0\u6570\u636e\u96c6\u5bf9\u63a8\u52a8\u751f\u6001\u5b66\u4e0e\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.19271", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Fr\u00e9d\u00e9ric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u516c\u5f00\u4e86WolBanking77\u2014\u2014\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u94f6\u884c\u9886\u57df\u7684\u6c83\u6d1b\u592b\u8bed\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6587\u672c\u4e0e\u8bed\u97f3\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u7ebf\u5b9e\u9a8c\u3002", "motivation": "\u4ee5\u5f80\u610f\u56fe\u5206\u7c7b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\uff0c\u5bfc\u81f4\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\uff08\u5982\u6c83\u6d1b\u592b\u8bed\uff09\u548c\u6587\u76f2\u7387\u9ad8\u5730\u533a\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u6c83\u6d1b\u592b\u8bed\u5728\u585e\u5185\u52a0\u5c14\u53ca\u897f\u975e\u533a\u57df\u5e7f\u6cdb\u4f7f\u7528\uff0c\u56e0\u6b64\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u5df2\u6210\u4e3a\u6280\u672f\u5e94\u7528\u7684\u969c\u788d\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u5e76\u6574\u7406\u4e86\u5305\u542b9,791\u6761\u6587\u672c\u548c4\u5c0f\u65f6\u8bed\u97f3\u7684\u94f6\u884c\u4e1a\u52a1\u76f8\u5173\u6c83\u6d1b\u592b\u8bed\u53e5\u5b50\uff0c\u6784\u5efaWolBanking77\u6570\u636e\u96c6\uff0c\u5bf9\u591a\u79cd\u6587\u672c\u4e0e\u8bed\u97f3\u610f\u56fe\u5206\u7c7b\u6a21\u578b\uff08\u5305\u62ecSOTA\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u62a5\u544aNLP\u548cASR\u6a21\u578b\u4e0a\u7684\u57fa\u7ebff1\u503c\u3001\u8bcd\u9519\u8bef\u7387\u7b49\u6307\u6807\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u5bf9\u6bd4\u5206\u6790\u3002", "result": "WolBanking77\u4e0a\u5404\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0cNLP\u4e0eASR\u6a21\u578b\u7684\u6027\u80fd\u6307\u6807\u5df2\u8be6\u7ec6\u516c\u5f00\uff0c\u5e76\u901a\u8fc7\u5bf9\u6570\u636e\u5185\u5bb9\u8fdb\u884c\u5206\u6790\u9a8c\u8bc1\u4e86\u6570\u636e\u7684\u4ee3\u8868\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "WolBanking77\u6570\u636e\u96c6\u4e3a\u6c83\u6d1b\u592b\u8bed\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u5728\u591a\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\u3002\u4f5c\u8005\u5c06\u6301\u7eed\u7ef4\u62a4\u548c\u66f4\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u516c\u5f00\u6e90\u4ee3\u7801\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2509.18711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u6307\u4ee3\uff08RSVG\uff09\u6846\u67b6RSVG-ZeroOV\uff0c\u65e0\u9700\u9488\u5bf9\u4efb\u52a1\u4e13\u95e8\u8bad\u7ec3\uff0c\u5373\u53ef\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u3001\u96f6\u6837\u672c\u7684\u76ee\u6807\u5b9a\u4f4d\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RSVG\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c01\u95ed\u8bcd\u8868\uff0c\u4e14\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u65f6\u95f4\u6d88\u8017\u578b\u5fae\u8c03\u4f9d\u8d56\u8fc7\u91cd\uff0c\u96be\u4ee5\u6269\u5c55\u81f3\u5f00\u653e\u573a\u666f\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u9700\u8981\u80fd\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u901a\u7528\u5927\u6a21\u578b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e09\u6b65\uff1a1\uff09\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u83b7\u5f97\u6587\u672c\u4e0e\u89c6\u89c9\u533a\u57df\u7684\u6ce8\u610f\u529b\u76f8\u5173\u6027\u56fe\uff1b2\uff09\u5f15\u5165\u6269\u6563\u6a21\u578b\uff08DM\uff09\u8865\u8db3VLM\u5ffd\u7565\u7684\u7ed3\u6784\u548c\u5f62\u72b6\u7ec6\u8282\uff1b3\uff09\u63d0\u51fa\u6ce8\u610f\u529b\u6f14\u5316\u6a21\u5757\u6291\u5236\u65e0\u5173\u6fc0\u6d3b\uff0c\u4ea7\u51fa\u7eaf\u51c0\u7684\u76ee\u6807\u5206\u5272\u63a9\u7801\u3002\u5168\u6d41\u7a0b\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5f31\u76d1\u7763\u548c\u96f6\u6837\u672cRSVG\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "RSVG-ZeroOV\u5145\u5206\u5229\u7528\u51bb\u7ed3\u7684\u5927\u6a21\u578b\uff0c\u5728\u6ca1\u6709\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u9886\u5148\u7684\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u56fe\u50cf\u76ee\u6807\u5b9a\u4f4d\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2509.19274", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDRISHTIKON\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u5370\u5ea6\u6587\u5316\u7684\u591a\u6a21\u6001\u591a\u8bed\u8a00AI\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5370\u5ea6\u672c\u571f\u5185\u5bb9\u53ca\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u504f\u5411\u5168\u7403\u6027\u6216\u6cdb\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5370\u5ea6\u4e30\u5bcc\u3001\u591a\u6837\u7684\u672c\u5730\u6587\u5316\u7ec6\u8282\u7684\u6df1\u5165\u8986\u76d6\uff0c\u5bfc\u81f4AI\u7cfb\u7edf\u5728\u5730\u533a\u6587\u5316\u7406\u89e3\u548c\u5305\u5bb9\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u77ed\u677f\u3002\u4e3a\u63a8\u52a8\u6587\u5316\u654f\u611f\u4e0e\u5305\u5bb9\u7684AI\u7814\u7a76\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u8986\u76d6\u5370\u5ea615\u79cd\u8bed\u8a00\u3001\u6240\u6709\u5730\u533a\u7684\u5927\u89c4\u6a21\u5370\u5ea6\u6587\u5316\u591a\u6a21\u6001\u8bc4\u6d4b\u96c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b64000+\u6587\u672c-\u56fe\u50cf\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u4e2a\u5730\u533a\u3001\u8bed\u8a00\u53ca\u6587\u5316\u4e3b\u9898\uff08\u5982\u8282\u65e5\u3001\u670d\u9970\u3001\u9910\u996e\u3001\u827a\u672f\u7b49\uff09\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u4e0d\u540c\u7c7b\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\uff0c\u5305\u62ec\u5f00\u6e90\u5c0f\u578b\u548c\u5927\u578b\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u3001\u63a8\u7406\u5f3a\u5316\u6a21\u578b\u53ca\u5370\u5ea6\u672c\u5730\u5316\u6a21\u578b\uff0c\u6db5\u76d6\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u8bc4\u6d4b\u8bbe\u5b9a\u3002", "result": "\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u8bba\u5f00\u6e90\u8fd8\u662f\u5546\u4e1a\u6a21\u578b\uff0c\u76ee\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u672c\u571f\u6587\u5316\u76f8\u5173\u591a\u6a21\u6001\u5185\u5bb9\u4e0a\u5747\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u4f53\u73b0\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e0d\u5e38\u89c1\u6587\u5316\u4f20\u7edf\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u4e0a\u3002", "conclusion": "DRISHTIKON\u4e3a\u6587\u5316\u5305\u5bb9\u6027\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5177\u6709\u66f4\u5f3a\u5370\u5ea6\u672c\u571f\u6587\u5316\u611f\u77e5\u80fd\u529b\u3001\u591a\u6a21\u6001\u878d\u5408\u4e0e\u591a\u8bed\u8a00\u9002\u5e94\u6027\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18715", "abs": "https://arxiv.org/abs/2509.18715", "authors": ["Yingquan Wang", "Pingping Zhang", "Chong Sun", "Dong Wang", "Huchuan Lu"], "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification", "comment": "Accepted by TCSVT2025", "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5c5e\u6027\u63d0\u793a\u7ec4\u5408\uff08APC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5c5e\u6027\u751f\u6210\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u63d0\u5347\u76ee\u6807\u91cd\u8bc6\u522b\uff08ReID\uff09\u7684\u5224\u522b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u9879\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709ReID\u6a21\u578b\u591a\u88ab\u9650\u5236\u5728\u5355\u57df\u6216\u8de8\u57df\u573a\u666f\uff1a\u5355\u57df\u6a21\u578b\u6613\u8fc7\u62df\u5408\u3001\u8de8\u57df\u6a21\u578b\u56e0\u89c4\u8303\u5316\u7b56\u7565\u590d\u6742\u53ef\u80fd\u6291\u5236\u8eab\u4efd\u7279\u5f81\uff0c\u56e0\u6b64\u96be\u4ee5\u540c\u65f6\u517c\u987e\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u573a\u666f\u6709\u9650\u3002", "method": "\u63d0\u51faAPC\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u5c5e\u6027\u63d0\u793a\u751f\u6210\u5668\uff08APG\uff09\uff1a\u7531\u8bed\u4e49\u5c5e\u6027\u8bcd\u5178\uff08SAD\uff09\u548c\u63d0\u793a\u7ec4\u5408\u6a21\u5757\uff08PCM\uff09\u7ec4\u6210\uff0c\u524d\u8005\u4f5c\u4e3a\u4e30\u5bcc\u5c5e\u6027\u63cf\u8ff0\u7684\u8bcd\u5178\uff0c\u540e\u8005\u81ea\u9002\u5e94\u7ec4\u5408\u751f\u6210\u6709\u5224\u522b\u529b\u7684\u5c5e\u6027\u7279\u5f81\u3002\u53e6\u5916\uff0c\u91c7\u7528\u5feb\u6162\u8bad\u7ec3\u7b56\u7565\uff08FSTS\uff09\uff0c\u7ed3\u5408\u5feb\u901f\u66f4\u65b0\u6d41\uff08FUS\uff09\u83b7\u53d6\u5224\u522b\u77e5\u8bc6\u4e0e\u6162\u66f4\u65b0\u6d41\uff08SUS\uff09\u4fdd\u7559\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u5224\u522b\u4e0e\u6cdb\u5316\u7684\u52a8\u6001\u5e73\u8861\u3002", "result": "\u5728\u4f20\u7edf\u548c\u57df\u6cdb\u5316ReID\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\uff0c\u5b9e\u9a8c\u6548\u679c\u7a81\u51fa\u3002", "conclusion": "APC\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3001\u5c5e\u6027\u8bed\u4e49\u548c\u521b\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86ReID\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u591a\u57df\u76ee\u6807\u91cd\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18717", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18717", "abs": "https://arxiv.org/abs/2509.18717", "authors": ["Tong Zhang", "Kuofeng Gao", "Jiawang Bai", "Leo Yu Zhang", "Xin Yin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment", "comment": null, "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9CLIP\u6a21\u578b\u5728\u5229\u7528\u5927\u89c4\u6a21\u4e92\u8054\u7f51\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u6613\u53d7\u6570\u636e\u6295\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff08Optimal Transport, OT\uff09\u7684\u56fe\u50cf-\u6587\u672c\u914d\u5bf9\u91cd\u5efa\u6846\u67b6OTCCLIP\uff0c\u80fd\u6709\u6548\u63d0\u5347\u9632\u5fa1\u80fd\u529b\u5e76\u7f13\u89e3\u539f\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "CLIP\u6a21\u578b\u56e0\u4f9d\u8d56\u4ece\u4e92\u8054\u7f51\u722c\u53d6\u7684\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u5bb9\u6613\u53d7\u5230\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u6295\u6bd2\u548c\u540e\u95e8\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5168\u5c40\u7279\u5f81\u8fdb\u884c\u56fe\u50cf-\u6587\u672c\u91cd\u65b0\u914d\u5bf9\uff0c\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u5b58\u5728\u5f15\u5165\u914d\u5bf9\u9519\u8bef\u7684\u98ce\u9669\uff0c\u5f71\u54cd\u6a21\u578b\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u8868\u73b0\u3002", "method": "\u63d0\u51faOTCCLIP\u6846\u67b6\uff0c\u65e0\u9700\u4ec5\u4f9d\u8d56\u5168\u5c40\u7279\u5f81\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u8ddd\u79bb\u5ea6\u91cf\u56fe\u50cf\u548c\u6587\u672c\u7ec6\u7c92\u5ea6\u7279\u5f81\u96c6\u5408\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u57fa\u4e8e\u6b64\u5ea6\u91cf\u4e3a\u6bcf\u4e2a\u56fe\u50cf\u91cd\u65b0\u5206\u914d\u6700\u4f18\u6587\u672c\u63cf\u8ff0\u3002\u5e76\u5f15\u5165\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f3a\u5316\u6a21\u6001\u5185\u5916\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u914d\u5bf9\u9519\u8bef\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOTCCLIP\u5728\u9632\u5fa1\u6570\u636e\u6295\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u65f6\u80fd\u6709\u6548\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u5728\u4e2d\u6bd2\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u7684zero-shot\u4e0elinear probing\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OTCCLIP\u80fd\u591f\u5728\u65e0\u987b\u989d\u5916\u5f3a\u76d1\u7763\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6700\u4f18\u4f20\u8f93\u914d\u5bf9\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347CLIP\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u9c81\u68d2\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u9884\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2509.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18733", "abs": "https://arxiv.org/abs/2509.18733", "authors": ["Yilin Gao", "Kangyi Chen", "Zhongxing Peng", "Hengjie Lu", "Shugong Xu"], "title": "Knowledge Transfer from Interaction Learning", "comment": "Accepted by ICCV2025", "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4ea4\u4e92\u4e3a\u6838\u5fc3\u7684\u8ba4\u77e5\u542f\u53d1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fc1\u79fb\u77e5\u8bc6\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u4e24\u8005\u5728\u5efa\u6a21\u65b9\u5f0f\u4e0a\u7684\u4ee3\u8868\u6027\u5dee\u5f02\uff1aVLMs\u91cd\u89c6\u8de8\u6a21\u6001\u4e92\u52a8\u8fc7\u7a0b\uff0cVFMs\u591a\u91c7\u7528\u7ed3\u679c\u9a71\u52a8\u8303\u5f0f\uff0c\u5ffd\u7565\u4e86\u7269\u4f53\u95f4\u590d\u6742\u7684\u4ea4\u4e92\u3002\u8fd9\u5bfc\u81f4\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u53d7\u9650\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u591a\u6837\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u7684\u201c\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u201d\uff08LFI\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u5927\u521b\u65b0\uff1a1\uff09\u4ea4\u4e92\u67e5\u8be2\uff08Interaction Queries\uff09\uff0c\u5728\u7f51\u7edc\u4e0d\u540c\u5c42\u4e2d\u7ef4\u6301\u6301\u4e45\u5316\u7684\u5173\u7cfb\u7ed3\u6784\uff1b2\uff09\u57fa\u4e8e\u4ea4\u4e92\u7684\u76d1\u7763\u673a\u5236\uff0c\u4eceVLM\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e2d\u83b7\u5f97\u66f4\u771f\u5b9e\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u901a\u8fc7\u8fd9\u4e24\u8005\uff0c\u5c06VLM\u4e2d\u7f16\u7801\u7684\u52a8\u6001\u4ea4\u4e92\u8fc1\u79fb\u5230VFMs\u4e2d\u3002", "result": "\u5728TinyImageNet\u5206\u7c7b\u548cCOCO\u68c0\u6d4b\u3001\u5206\u5272\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u7edd\u5bf9\u63d0\u53473.3\u548c1.6mAP/2.4AP\uff0c\u540c\u65f6\u53c2\u6570\u5f00\u9500\u5c0f\u3001\u6536\u655b\u66f4\u5feb\u3002\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u6cdb\u5316\u4efb\u52a1\uff08PACS\u3001VLCS\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5b9e\u73b0\u4e862.4\u548c9.3\u7684zero-shot\u63d0\u5347\u3002\u4eba\u7c7b\u8bc4\u4ef7\u4e5f\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd52.7\u500d\u3002", "conclusion": "LFI\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4ee5\u4ea4\u4e92\u4e3a\u4e2d\u5fc3\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u89e3\u51b3\u4e86VFM\u4e0eVLM\u4ee3\u8868\u6027\u5dee\u5f02\u6240\u5e26\u6765\u7684\u77e5\u8bc6\u8fc1\u79fb\u969c\u788d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u9886\u57df\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18847", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18847", "abs": "https://arxiv.org/abs/2509.18847", "authors": ["Junhao Su", "Yuanliang Wan", "Junwei Yang", "Hengyu Shi", "Tianyang Han", "Junfeng Luo", "Yurui Qiu"], "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "comment": "9pages", "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u201cstructured reflection\u201d\uff0c\u901a\u8fc7\u663e\u5f0f\u53cd\u601d\u673a\u5236\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u7ea0\u9519\u548c\u5b66\u4e60\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u7ea0\u9519\u7387\u548c\u8c03\u7528\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u589e\u5f3aLLM\u5de5\u5177\u8c03\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u505a\u6d45\u663e\u6a21\u4eff\u6216\u7c97\u7565\u5956\u52b1\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u6211\u9519\u8bef\u8bca\u65ad\u548c\u4fee\u6b63\u673a\u5236\uff0c\u591a\u8f6e\u8c03\u7528\u4e2d\u6613\u91cd\u590d\u51fa\u9519\u3002", "method": "\u5f15\u5165structured reflection\uff0c\u8ba9\u6a21\u578b\u5728\u51fa\u9519\u540e\u663e\u5f0f\u4ea7\u51fa\u53cd\u601d\uff1a\u5148\u57fa\u4e8e\u4e0a\u4e00\u6b65\u8bc1\u636e\u8bca\u65ad\u5931\u8d25\uff0c\u518d\u63d0\u51fa\u53ef\u6267\u884c\u7684\u4fee\u6b63\u64cd\u4f5c\u3002\u8bad\u7ec3\u4e2d\u7ed3\u5408DAPO\u548cGSPO\u76ee\u6807\uff0c\u5e76\u4e13\u95e8\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u4ee5\u5f3a\u5316\u6bcf\u4e00\u6b65\u53cd\u601d\u2014\u8c03\u7528\u2014\u7ec8\u7ed3\u6d41\u7a0b\u3002\u540c\u65f6\u63d0\u51fa\u65b0\u57fa\u51c6Tool-Reflection-Bench\u5bf9\u7ed3\u6784\u3001\u6267\u884c\u3001\u53c2\u6570\u548c\u7ed3\u679c\u4e00\u81f4\u6027\u591a\u65b9\u9762\u68c0\u9a8c\u3002", "result": "\u5728BFCL v3\u548cTool-Reflection-Bench\u4e24\u9879\u5b9e\u9a8c\u4e2d\uff0cstructured reflection\u65b9\u6cd5\u6781\u5927\u63d0\u5347\u4e86\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u7684\u6210\u529f\u7387\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u5197\u4f59\u8c03\u7528\u3002", "conclusion": "\u5c06\u53cd\u601d\u6d41\u7a0b\u7ed3\u6784\u5316\u548c\u7aef\u5230\u7aef\u4f18\u5316\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u81ea\u4e3b\u7ea0\u9519\u548c\u5de5\u5177\u4ea4\u4e92\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u6a21\u578b\u4ece\u5931\u8d25\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u7b56\u7565\u8def\u5f84\u3002"}}
{"id": "2509.18738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u201cSegment Anything Model (SAM)\u201d\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff08HyPSAM\uff09\uff0c\u63d0\u5347\u4e86RGB-\u70ed\u611f\uff08RGB-T\uff09\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RGB-T\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b faces\u9762\u4e34\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u4e0e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5bfc\u81f4\u76ee\u6807\u8fb9\u754c\u548c\u5b8c\u6574\u6027\u96be\u4ee5\u7cbe\u786e\u68c0\u6d4b\u3002", "method": "1. \u63d0\u51fa\u52a8\u6001\u878d\u5408\u7f51\u7edc\uff08DFNet\uff09\uff0c\u5229\u7528\u52a8\u6001\u5377\u79ef\u4e0e\u591a\u5206\u652f\u89e3\u7801\uff0c\u5b9e\u73b0RGB-\u70ed\u611f\u4e4b\u95f4\u81ea\u9002\u5e94\u7279\u5f81\u4e92\u8865\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u89c6\u89c9\u63d0\u793a\u30022. \u8bbe\u8ba1\u53ef\u63d2\u62d4\u4f18\u5316\u7f51\u7edc\uff08P2RNet\uff09\uff0c\u7ed3\u5408\u6587\u672c\u3001\u63a9\u7801\u548c\u6846\u7b49\u591a\u6a21\u6001\u63d0\u793a\uff0c\u6709\u6548\u5f15\u5bfcSAM\u4f18\u5316\u663e\u8457\u6027\u5206\u5272\u7ed3\u679c\u3002\u6574\u4e2a\u6846\u67b6\u53ef\u4e0e\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0SOTA\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e14\u5728\u4e0e\u5176\u4ed6RGB-T SOD\u65b9\u6cd5\u7ed3\u5408\u65f6\u5747\u5e26\u6765\u663e\u8457\u63d0\u5347\uff0c\u5c55\u73b0\u51fa\u7b97\u6cd5\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "HyPSAM\u6709\u6548\u89e3\u51b3\u4e86RGB-T\u663e\u8457\u6027\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u878d\u5408\u4e0e\u6cdb\u5316\u96be\u9898\uff0c\u63d0\u793a\u5de5\u7a0b\u548cSAM\u7684\u7ed3\u5408\u4e3a\u591a\u6a21\u6001\u5206\u5272\u9886\u57df\u5e26\u6765\u65b0\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.19002", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19002", "abs": "https://arxiv.org/abs/2509.19002", "authors": ["Hao Wang", "Eiki Murata", "Lingfang Zhang", "Ayako Sato", "So Fukuda", "Ziqi Yin", "Wentao Hu", "Keisuke Nakao", "Yusuke Nakamura", "Sebastian Zwirner", "Yi-Chia Chen", "Hiroyuki Otomo", "Hiroki Ouchi", "Daisuke Kawahara"], "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VIR-Bench\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u957f\u8ddd\u79bb\u65c5\u884c\u89c6\u9891\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u6d4b\u8bd5\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\uff0c\u5e76\u6210\u529f\u7528VIR-Bench\u4fc3\u8fdb\u65c5\u884c\u63a8\u8350\u4ee3\u7406\u7684\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u5ba4\u5185\u6216\u77ed\u8ddd\u79bb\u573a\u666f\uff0c\u5bf9\u4e8e\u957f\u8ddd\u79bb\u65c5\u884c\u76f8\u5173\u7684\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7f3a\u4e4f\u6d4b\u8bd5\uff0c\u7136\u800c\u8fd9\u5bf9\u4e8e\u4e0b\u4ee3\u5927\u6a21\u578b\u3001\u5982\u667a\u80fd\u4f53\u5bfc\u822a\u4e0e\u89c4\u5212\u7b49\u5b9e\u9645\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86VIR-Bench\u57fa\u51c6\uff0c\u6536\u96c6\u4e86200\u4e2a\u65c5\u884c\u89c6\u9891\uff0c\u5c06\u884c\u7a0b\u91cd\u5efa\u4f5c\u4e3a\u8bc4\u4f30\u4efb\u52a1\uff0c\u6d4b\u8bd5\u591a\u6a21\u6001\u5927\u6a21\u578b\u5bf9\u5730\u7406-\u65f6\u5e8f\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\u3002\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5229\u7528VIR-Bench\u63d0\u5347\u7684\u65c5\u884c\u63a8\u8350\u667a\u80fd\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5305\u62ec\u5546\u7528\u6a21\u578b\uff09\u5728\u8be5\u57fa\u51c6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5e94\u5bf9\u957f\u65f6\u95f4\u3001\u957f\u8ddd\u79bb\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002\u4f7f\u7528VIR-Bench\u8bad\u7ec3\u7684\u65c5\u884c\u63a8\u8350\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u63a8\u8350\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VIR-Bench\u663e\u8457\u586b\u8865\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5927\u5c3a\u5ea6\u65f6\u7a7a\u89c6\u9891\u7406\u89e3\u7684\u8bc4\u6d4b\u7a7a\u767d\uff0c\u5e76\u80fd\u6709\u6548\u5f15\u5bfc\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2509.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriFusion-AE\u7684\u591a\u6a21\u6001\u8de8\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u6587\u672c\u3001\u6df1\u5ea6\u56fe\u548c\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4fe1\u606f\uff0c\u5b9e\u73b0\u9c81\u68d2\u70b9\u4e91\u91cd\u5efa\uff0c\u5c24\u5176\u5728\u5f3a\u566a\u58f0\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u7eaf\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u5bf9\u4e8e\u566a\u58f0\u3001\u906e\u6321\u548c\u5bf9\u6297\u6027\u6270\u52a8\u975e\u5e38\u654f\u611f\uff0c\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u5728\u771f\u5b9e\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u6709\u9650\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u63d0\u5347\u70b9\u4e91\u611f\u77e5\u7684\u9c81\u68d2\u6027\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTriFusion-AE\u591a\u6a21\u6001\u8de8\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\uff0c\u5c06\u6587\u672c\u5148\u9a8c\u3001\u7531\u56fe\u50cf\u751f\u6210\u7684\u5355\u76ee\u6df1\u5ea6\u56fe\u4e0e\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u7efc\u5408\u8bed\u4e49\u3001\u51e0\u4f55\u4e0e\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u66f4\u5f3a\u9c81\u68d2\u8868\u5f81\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u5728nuScenes-mini\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793aTriFusion-AE\u5728\u9762\u5bf9\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u548c\u91cd\u566a\u58f0\u65f6\uff0c\u91cd\u5efa\u6548\u679c\u8fdc\u4f18\u4e8eCNN\u81ea\u7f16\u7801\u5668\u6a21\u578b\uff0c\u800c\u5728\u8f7b\u5ea6\u6270\u52a8\u4e0b\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u80fd\u663e\u8457\u63d0\u5347\u81ea\u7f16\u7801\u5668\u5bf9\u70b9\u4e91\u6570\u636e\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u6240\u63d0\u65b9\u6cd5\u6a21\u5757\u5316\u5f3a\uff0c\u53ef\u5e7f\u6cdb\u9002\u914d\u73b0\u6709CNN\u70b9\u4e91\u81ea\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u8054\u5408\u7279\u5f81\u5b66\u4e60\u3002"}}
{"id": "2509.19070", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19070", "abs": "https://arxiv.org/abs/2509.19070", "authors": ["Zijian Ling", "Han Zhang", "Yazhuo Zhou", "Jiahao Cui"], "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?", "comment": "Accepted at the Open Science for Foundation Models (SCI-FM) Workshop\n  at ICLR 2025", "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86ColorBlindnessEval\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u8272\u76f2\u7c7b\u89c6\u89c9\u5bf9\u6297\u573a\u666f\u4e0b\u8bc6\u522b\u6570\u5b57\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u3001\u5177\u5907\u89c6\u89c9\u5e72\u6270\uff08\u5982\u8272\u76f2\u6d4b\u8bd5\u56fe\u50cf\uff09\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u8f83\u5f31\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u9488\u5bf9\u6027\u8bc4\u6d4b\u5de5\u5177\u3002\u8be5\u7814\u7a76\u5e0c\u671b\u91cf\u5316\u5e76\u63ed\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u7c7b\u4f3c\u8272\u76f2\u8868\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4e3a\u4eca\u540e\u6a21\u578b\u7684\u6539\u8fdb\u63d0\u4f9b\u57fa\u51c6\u4e0e\u65b9\u5411\u3002", "method": "\u5efa\u7acb\u4e86ColorBlindnessEval\u57fa\u51c6\uff0c\u5305\u542b500\u5f20\u6a21\u62df\u8272\u76f2\u6d4b\u8bd5\u7684\u56fe\u50cf\uff0c\u56fe\u7247\u4e2d\u5d4c\u67090-99\u4e4b\u95f4\u7684\u6570\u5b57\u4e14\u91c7\u7528\u591a\u79cd\u989c\u8272\u7ec4\u5408\u3002\u5206\u522b\u4f7f\u7528\u5168\u5c40\u9009\u62e9\uff08Yes/No\uff09\u548c\u5f00\u653e\u5f0f\u95ee\u9898\u6d4b\u8bd59\u4e2a\u4e3b\u6d41VLMs\u5e76\u4e0e\u4eba\u7c7b\u53d7\u8bd5\u8005\u6bd4\u8f83\u7ed3\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6240\u6709\u88ab\u8bc4\u4f30\u7684VLMs\u5728\u8be5\u5bf9\u6297\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u5d4c\u5165\u6570\u5b57\uff0c\u4e14\u666e\u904d\u5b58\u5728\u5e7b\u89c9\u8f93\u51fa\u95ee\u9898\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u8272\u76f2\u7c7b\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u4e0d\u7406\u60f3\uff0cColorBlindnessEval\u80fd\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u53ef\u9760\u6027\u53ca\u771f\u5b9e\u73af\u5883\u9002\u7528\u6027\u7684\u6709\u529b\u8bc4\u6d4b\u5de5\u5177\u3002"}}
{"id": "2509.18754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18754", "abs": "https://arxiv.org/abs/2509.18754", "authors": ["Yuyang Liu", "Xinyuan Shi", "Bang Yang", "Peilin Zhou", "Jiahua Dong", "Long Chen", "Ian Reid", "Xiaondan Liang"], "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage", "comment": "16 pages", "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u89c6\u9891\u5927\u6a21\u578b\u65b9\u6cd5COLT\uff0c\u80fd\u5728\u5de5\u5177\u6570\u636e\u6301\u7eed\u53d8\u5316\u7684\u73af\u5883\u4e0b\uff0c\u81ea\u52a8\u5b66\u4e60\u65b0\u5de5\u5177\u7684\u4f7f\u7528\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u9057\u5fd8\u5df2\u5b66\u5de5\u5177\uff0c\u5b9e\u73b0\u5bf9\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u591a\u5de5\u5177\u8fde\u7eed\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u6a21\u578b\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7684\u5de5\u5177\u5e93\uff0c\u4e0d\u9002\u5e94\u73b0\u5b9e\u73af\u5883\u4e2d\u5de5\u5177\u6570\u636e\u6301\u7eed\u65b0\u589e\u3001\u6f14\u5316\u7684\u60c5\u5f62\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u4e0d\u8db3\u548c\u9057\u5fd8\u4ee5\u5f80\u77e5\u8bc6\u3002", "method": "\u63d0\u51faCOLT\u65b9\u6cd5\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u4ee3\u7801\u672c\uff08codebook\uff09\u4f5c\u4e3a\u5de5\u5177\u4e13\u5c5e\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u4e0e\u4ee3\u7801\u672c\u4e2d\u5de5\u5177\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u52a8\u6001\u9009\u62e9\u76f8\u5173\u5de5\u5177\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u89c6\u9891\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6VideoToolBench\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5bf9\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5b66\u4e60\u3002", "result": "\u5728\u73b0\u6709\u89c6\u9891\u5927\u6a21\u578b\u57fa\u51c6\u4e0e\u65b0\u5efa\u7acb\u7684\u5de5\u5177\u4f7f\u7528\u6570\u636e\u96c6VideoToolBench\u4e0a\uff0cCOLT\u65b9\u6cd5\u5728\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "conclusion": "COLT\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5927\u6a21\u578b\u5728\u5de5\u5177\u6301\u7eed\u5b66\u4e60\u4e2d\u9057\u5fd8\u65e7\u5de5\u5177\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6a21\u578b\u57fa\u7840\u3002"}}
{"id": "2509.19090", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19090", "abs": "https://arxiv.org/abs/2509.19090", "authors": ["Guoxin Wang", "Jun Zhao", "Xinyi Liu", "Yanbo Liu", "Xuyang Cao", "Chao Li", "Zhuoyun Liu", "Qintian Sun", "Fangru Zhou", "Haoqiang Xing", "Zhenhong Yang"], "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning", "comment": null, "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Citrus-V\uff0c\u4e00\u4e2a\u878d\u5408\u56fe\u50cf\u5206\u6790\u4e0e\u6587\u672c\u63a8\u7406\u7684\u591a\u6a21\u6001\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u68c0\u6d4b\u3001\u5206\u5272\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u8bca\u65ad\u4e00\u4f53\u5316\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u6a21\u578b\u529f\u80fd\u5355\u4e00\u4e14\u666e\u904d\u4f9d\u8d56\u591a\u4e2a\u4e13\u95e8\u7f51\u7edc\uff0c\u96be\u4ee5\u63a8\u5e7f\u548c\u6cdb\u5316\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u6ee1\u8db3\u4e34\u5e8a\u5bf9\u7cbe\u51c6\u53ef\u89e3\u91ca\u3001\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "Citrus-V\u901a\u8fc7\u521b\u65b0\u6027\u7684\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u68c0\u6d4b\u3001\u5206\u5272\u548c\u94fe\u5f0f\u63a8\u7406\u6574\u5408\u5230\u5355\u4e00\u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u50cf\u7d20\u7ea7\u75c5\u7076\u5b9a\u4f4d\u3001\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u53ca\u7c7b\u533b\u751f\u7684\u8bca\u65ad\u63a8\u65ad\u3002\u4f5c\u8005\u8fd8\u5f00\u653e\u4e86\u6db5\u76d6\u63a8\u7406\u3001\u68c0\u6d4b\u3001\u5206\u5272\u3001\u6587\u6863\u7406\u89e3\u4efb\u52a1\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "result": "Citrus-V\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u533b\u5b66\u6a21\u578b\u548c\u4e13\u5bb6\u7ea7\u5f71\u50cf\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u89c6\u89c9\u5b9a\u4f4d\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u5b8c\u6574\u7edf\u4e00\u6d41\u7a0b\u3002", "conclusion": "Citrus-V\u80fd\u591f\u9ad8\u6548\u7cbe\u786e\u5730\u5b9e\u73b0\u75c5\u7076\u5b9a\u91cf\u3001\u81ea\u52a8\u62a5\u544a\u751f\u6210\u53ca\u53ef\u9760\u7684\u8f85\u52a9\u8bca\u65ad\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5c55\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFixingGS\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u63d0\u53473D\u9ad8\u65af\u6295\u5f71\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5728\u53bb\u9664\u4f2a\u5f71\u548c\u8865\u5168\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6295\u5f71\uff083DGS\uff09\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u91cd\u5efa\u4e09\u7ef4\u573a\u666f\u65f6\uff0c\u56e0\u89c6\u89c9\u4fe1\u606f\u4e0d\u8db3\u5e38\u4f1a\u751f\u6210\u4e25\u91cd\u4f2a\u5f71\u3002\u867d\u7136\u5df2\u6709\u5229\u7528\u751f\u6210\u5148\u9a8c\u7684\u65b9\u6cd5\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u4e86\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u7ed3\u6784\u6a21\u7cca\u548c\u7ec6\u8282\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51fa\u4e86FixingGS\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u57fa\u4e8e\u73b0\u6709\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\u589e\u5f3a\u3002\u6838\u5fc3\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u84b8\u998f\u65b9\u6cd5\uff0c\u4ece\u6269\u6563\u5148\u9a8c\u4e2d\u63d0\u53d6\u66f4\u51c6\u786e\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u4fe1\u606f\uff0c\u6709\u5229\u4e8e\u4f2a\u5f71\u53bb\u9664\u4e0e\u5185\u5bb9\u8865\u5168\uff0c\u540c\u65f6\u5f15\u5165\u81ea\u9002\u5e94\u7684\u9010\u6b65\u589e\u5f3a\u673a\u5236\uff0c\u5bf9\u53d7\u9650\u533a\u57df\u8fdb\u884c\u8fdb\u4e00\u6b65\u7ec6\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFixingGS\u5728\u89c6\u89c9\u8d28\u91cf\u53ca\u91cd\u5efa\u8868\u73b0\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u7a00\u758f\u89c6\u89d2\u4e0b\u4e09\u7ef4\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\u548c\u7f3a\u5931\u5185\u5bb9\u95ee\u9898\u3002", "conclusion": "FixingGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u3001\u8bad\u7ec3\u81ea\u7531\u7684\u4e09\u7ef4\u91cd\u5efa\u589e\u5f3a\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u89d2\u4e0b\u76843DGS\u8868\u73b0\uff0c\u5e76\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u6280\u672f\u53d1\u5c55\u3002\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2509.18763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18763", "abs": "https://arxiv.org/abs/2509.18763", "authors": ["Xijun Wang", "Junyun Huang", "Rayyan Abdalla", "Chengyuan Zhang", "Ruiqi Xian", "Dinesh Manocha"], "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models", "comment": null, "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBi-VLM\u7684\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u975e\u5747\u5300\u7684\u9ad8\u65af\u5206\u4f4d\u6570\u5bf9\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u5206\u7ec4\u548c\u91cf\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d85\u4f4e\u6bd4\u7279\uff08\u4e8c\u8fdb\u5236\u6216\u4e8c\u8fdb\u5236\u4ee5\u4e0b\uff09\u6743\u91cd\u91cf\u5316\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8ba1\u7b97\u91cf\u548c\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u786c\u4ef6\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5728\u6781\u4f4e\u6bd4\u7279\u4e0b\u9ad8\u6548\u91cf\u5316\u6743\u91cd\u3001\u964d\u4f4e\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u5206\u4f4d\u6570\u7684\u975e\u5747\u5300\u6743\u91cd\u91cf\u5316\uff0c\u5c06\u6743\u91cd\u6309\u663e\u8457\u6027\u5212\u5206\u4e3a\u201c\u79bb\u7fa4\u503c\uff08\u663e\u8457\uff09\u201d\u4e0e\u201c\u975e\u79bb\u7fa4\u503c\uff08\u4e0d\u663e\u8457\uff09\u201d\u591a\u4e2a\u5b50\u96c6\u3002\u5f15\u5165\u663e\u8457\u6027\u611f\u77e5\u7684\u6df7\u5408\u91cf\u5316\u7b97\u6cd5\uff0c\u9488\u5bf9\u4e0d\u540c\u663e\u8457\u6027\u548c\u538b\u7f29\u76ee\u6807\uff0c\u5bf9\u6bd4\u4f8b\u7f29\u653e\u77e9\u9635\u4e0e\u4e8c\u503c\u77e9\u9635\u65bd\u52a0\u4e0d\u540c\u7ea6\u675f\u3002\u540c\u65f6\u8fd8\u5728\u91cf\u5316\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86token\u526a\u679d\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u591a\u79cdVLM\u53ca\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0cBi-VLM\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u79cd\u4e0d\u540c\u6a21\u578b\u4e0b\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u63d0\u53473%-47%\uff1b\u6574\u4f53VLM\u63d0\u53474%-45%\uff1b\u5bf9\u91cf\u5316\u6a21\u578b\u8fdb\u884ctoken\u526a\u679d\u53d1\u73b090%-99%\u7684\u89c6\u89c9token\u5b58\u5728\u5197\u4f59\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "conclusion": "Bi-VLM\u80fd\u591f\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u5927\u5e45\u63d0\u5347\u6a21\u578b\u6548\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9token\u526a\u679d\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8ba1\u7b97\u4e0e\u5b58\u50a8\u6548\u7387\uff0c\u4e3aVLM\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.18765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18765", "abs": "https://arxiv.org/abs/2509.18765", "authors": ["Azad Singh", "Deepak Mishra"], "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u6846\u67b6DiSSECT\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\uff0c\u4e3a\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u63d0\u4f9b\u7ea6\u675f\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8de8\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u4f9d\u8d56\u590d\u6742\u7ed3\u6784\u3001\u7279\u5b9a\u5148\u9a8c\u6216\u9700\u8981\u5927\u91cf\u8c03\u53c2\uff0c\u4e14\u5bb9\u6613\u51fa\u73b0\u201c\u6377\u5f84\u5b66\u4e60\u201d\u95ee\u9898\uff0c\u96be\u4ee5\u6cdb\u5316\uff0c\u5c24\u5176\u5728\u80f8\u7247\u7b49\u6a21\u6001\u4e0b\u95ee\u9898\u7a81\u51fa\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u7a00\u7f3a\u65f6\u3002", "method": "\u63d0\u51faDiSSECT\u6846\u67b6\uff0c\u5c06\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\uff08Vector Quantization\uff09\u5f15\u5165\u81ea\u76d1\u7763\u6d41\u7a0b\uff0c\u5efa\u7acb\u79bb\u6563\u8868\u5f81\u74f6\u9888\uff0c\u6291\u5236\u65e0\u7528\u548c\u89c6\u89d2\u76f8\u5173\u7279\u5f81\uff0c\u4fc3\u8fdb\u7ed3\u6784\u5173\u8054\u7279\u5f81\u7684\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347\u8de8\u4efb\u52a1\u548c\u8de8\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cDiSSECT\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u4e14\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u5927\u5e45\u964d\u4f4e\uff0c\u5728\u6709\u9650\u6807\u6ce8\u60c5\u51b5\u4e0b\u4f9d\u7136\u6548\u679c\u7a81\u51fa\u3002\u8f83\u5c11\u751a\u81f3\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u83b7\u5f97\u8f83\u597d\u7ed3\u679c\u3002", "conclusion": "DiSSECT\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u8f83\u73b0\u6709\u6700\u5148\u8fdb\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5177\u5907\u66f4\u9ad8\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u6807\u7b7e\u6548\u7387\uff0c\u9002\u5408\u533b\u5b66\u5f71\u50cf\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.18779", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18779", "abs": "https://arxiv.org/abs/2509.18779", "authors": ["Hemanth Puppala", "Wayne Sarasua", "Srinivas Biyaguda", "Farhad Farzinpour", "Mashrur Chowdhury"], "title": "Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning", "comment": "Preprint under review in TRR, 20 pages, 9 figures, 4 tables", "summary": "Deer-vehicle collisions represent a critical safety challenge in the United\nStates, causing nearly 2.1 million incidents annually and resulting in\napproximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic\ndamages. These collisions also contribute significantly to declining deer\npopulations. This paper presents a real-time detection and driver warning\nsystem that integrates thermal imaging, deep learning, and\nvehicle-to-everything communication to help mitigate deer-vehicle collisions.\nOur system was trained and validated on a custom dataset of over 12,000 thermal\ndeer images collected in Mars Hill, North Carolina. Experimental evaluation\ndemonstrates exceptional performance with 98.84 percent mean average precision,\n95.44 percent precision, and 95.96 percent recall. The system was field tested\nduring a follow-up visit to Mars Hill and readily sensed deer providing the\ndriver with advanced warning. Field testing validates robust operation across\ndiverse weather conditions, with thermal imaging maintaining between 88 and 92\npercent detection accuracy in challenging scenarios where conventional visible\nlight based cameras achieve less than 60 percent effectiveness. When a high\nprobability threshold is reached sensor data sharing messages are broadcast to\nsurrounding vehicles and roadside units via cellular vehicle to everything\n(CV2X) communication devices. Overall, our system achieves end to end latency\nconsistently under 100 milliseconds from detection to driver alert. This\nresearch establishes a viable technological pathway for reducing deer-vehicle\ncollisions through thermal imaging and connected vehicles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u70ed\u6210\u50cf\u3001\u6df1\u5ea6\u5b66\u4e60\u4ee5\u53ca\u8f66\u8054\u7f51\u901a\u4fe1\u7684\u5b9e\u65f6\u9e7f\u68c0\u6d4b\u4e0e\u9a7e\u9a76\u5458\u9884\u8b66\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u9e7f\u4e0e\u8f66\u8f86\u7684\u78b0\u649e\u3002\u7cfb\u7edf\u5728\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6444\u50cf\u5934\uff0c\u5e76\u80fd\u5b9e\u65f6\u5c06\u9884\u8b66\u4fe1\u606f\u5e7f\u64ad\u81f3\u5468\u8fb9\u8f66\u8f86\u3002", "motivation": "\u7f8e\u56fd\u6bcf\u5e74\u6709210\u4e07\u8d77\u9e7f\u4e0e\u8f66\u8f86\u78b0\u649e\u4e8b\u6545\uff0c\u9020\u6210\u5927\u91cf\u4eba\u5458\u4f24\u4ea1\u548c\u5de8\u989d\u8d22\u4ea7\u635f\u5931\uff0c\u5e76\u5bfc\u81f4\u9e7f\u6570\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u7684\u53ef\u89c1\u5149\u6444\u50cf\u5934\u68c0\u6d4b\u6548\u679c\u6b20\u4f73\uff0c\u4e9f\u9700\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684\u6280\u672f\u51cf\u5c11\u4e8b\u6545\u53d1\u751f\u3002", "method": "\u5229\u7528\u70ed\u6210\u50cf\u6444\u50cf\u5934\u91c7\u96c612000\u591a\u5f20\u9e7f\u7684\u56fe\u50cf\u6570\u636e\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u7cfb\u7edf\u901a\u8fc7CV2X\u6280\u672f\u5728\u9ad8\u6982\u7387\u68c0\u6d4b\u65f6\u5b9e\u65f6\u5c06\u8b66\u544a\u4fe1\u606f\u5206\u4eab\u7ed9\u5468\u8fb9\u8f66\u8f86\u53ca\u8def\u4fa7\u8bbe\u5907\u3002\u7cfb\u7edf\u5728\u5b9e\u5730\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u7387\u53ca\u7aef\u5230\u7aef\u5ef6\u8fdf\u8868\u73b0\u3002", "result": "\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e8698.84%\u7684mAP\uff0c95.44%\u7684\u7cbe\u786e\u7387\u548c95.96%\u7684\u53ec\u56de\u7387\u3002\u5b9e\u5730\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u70ed\u6210\u50cf\u4fdd\u630188-92%\u7684\u51c6\u786e\u7387\uff0c\u660e\u663e\u4f18\u4e8e\u53ef\u89c1\u5149\u6444\u50cf\u5934\uff08<60%\uff09\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7a33\u5b9a\u5728100\u6beb\u79d2\u4ee5\u5185\u3002", "conclusion": "\u6240\u63d0\u7cfb\u7edf\u80fd\u5927\u5e45\u63d0\u5347\u9e7f-\u8f66\u78b0\u649e\u68c0\u6d4b\u548c\u9884\u8b66\u80fd\u529b\uff0c\u901a\u8fc7\u70ed\u6210\u50cf\u548c\u8f66\u8054\u7f51\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3001\u4f4e\u5ef6\u8fdf\u9884\u8b66\uff0c\u4e3a\u51cf\u5c11\u6b64\u7c7b\u4ea4\u901a\u4e8b\u6545\u63d0\u4f9b\u4e86\u53ef\u884c\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.18796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18796", "abs": "https://arxiv.org/abs/2509.18796", "authors": ["Danush Kumar Venkatesh", "Stefanie Speidel"], "title": "Towards Application Aligned Synthetic Surgical Image Synthesis", "comment": null, "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6846\u67b6\uff08SAADi\uff09\uff0c\u53ef\u4ee5\u751f\u6210\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u5229\u7684\u5916\u79d1\u624b\u672f\u56fe\u50cf\uff0c\u4ee5\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u624b\u672f\u5f71\u50cf\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u80fd\u591f\u5408\u6210\u903c\u771f\u7684\u56fe\u50cf\uff0c\u4f46\u5e38\u89c4\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u8bb0\u5fc6\u548c\u6837\u672c\u591a\u6837\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u3002", "method": "\u63d0\u51faSurgical Application-Aligned Diffusion\uff08SAADi\uff09\u6846\u67b6\uff1a\u901a\u8fc7\u6784\u5efa\u4e0b\u6e38\u6a21\u578b\u504f\u597d\u7684\u201cpreferred\u201d\u4e0e\u201c\u4e0d\u504f\u597d\u201d\u7684\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u5bf9\u6269\u6563\u6a21\u578b\u5fae\u8c03\uff0c\u4f7f\u751f\u6210\u8fc7\u7a0b\u4e0e\u5177\u4f53\u4e0b\u6e38\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u91c7\u7528\u8fed\u4ee3\u7684\u65b9\u5f0f\u8fdb\u4e00\u6b65\u7cbe\u70bc\u5408\u6210\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u624b\u672f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSAADi\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u63d0\u53477%-9%\uff0c\u5206\u5272\u4efb\u52a1\u63d0\u53472%-10%\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u91cf\u7a00\u7f3a\u7c7b\u522b\u4e0a\u6539\u8fdb\u660e\u663e\u3002\u901a\u8fc7\u591a\u8f6e\u751f\u6210\u4e0e\u5fae\u8c03\uff0c\u8fd8\u53ef\u5e26\u67654%-10%\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "SAADi\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6837\u672c\u9000\u5316\u4e0e\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u4efb\u52a1\u611f\u77e5\u7684\u6570\u636e\u751f\u6210\u65b9\u5f0f\u6210\u4e3a\u7f13\u89e3\u6807\u6ce8\u7a00\u7f3a\u3001\u63d0\u5347\u533b\u7597\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2509.18801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18801", "abs": "https://arxiv.org/abs/2509.18801", "authors": ["Kuang Xiaodong", "Li Bingxuan", "Li Yuan", "Rao Fan", "Ma Gege", "Xie Qingguo", "Mok Greta S P", "Liu Huafeng", "Zhu Wentao"], "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising", "comment": null, "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff08KMDS-Net\uff09\uff0c\u7528\u4e8e\u52a8\u6001PET\uff08\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf\uff09\u56fe\u50cf\u53bb\u566a\uff0c\u5728\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u6210\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001PET\u56fe\u50cf\u7531\u4e8e\u6bcf\u5e27\u91c7\u96c6\u7684\u7edf\u8ba1\u91cf\u6709\u9650\uff0c\u5c24\u5176\u5728\u77ed\u65f6\u95f4\u5e27\u4e0b\u56fe\u50cf\u8d28\u91cf\u5dee\u3002\u73b0\u6709\u7684\u53bb\u566a\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u6709\u6548\u63d0\u5347\u52a8\u6001\u56fe\u50cf\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5e27\u95f4\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u5e27\u5185\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u6838\u7a7a\u95f4\u591a\u7ef4\u7a00\u758f\u6a21\u578b\uff08KMDS\uff09\uff0c\u5e76\u4ee5\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u53c2\u6570\u4f30\u8ba1\uff0c\u4f7f\u5176\u53c2\u6570\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u6a21\u578b\uff08KMDS-Net\uff09\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u795e\u7ecfKMDS-Net\u5728\u52a8\u6001PET\u56fe\u50cf\u53bb\u566a\u65b9\u9762\u4f18\u4e8e\u4ee5\u5f80\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u53bb\u566a\u80fd\u529b\u3002", "conclusion": "\u795e\u7ecfKMDS-Net\u80fd\u591f\u6709\u6548\u63d0\u5347\u52a8\u6001PET\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u6210\u50cf\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u52a8\u6001PET\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u9ad8\u6548\u53bb\u566a\u5de5\u5177\u3002"}}
{"id": "2509.18802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18802", "abs": "https://arxiv.org/abs/2509.18802", "authors": ["Garam Kim", "Tae Kyeong Jeong", "Juyoun Park"], "title": "Surgical Video Understanding with Label Interpolation", "comment": "8 pages, 10 figures", "summary": "Robot-assisted surgery (RAS) has become a critical paradigm in modern\nsurgery, promoting patient recovery and reducing the burden on surgeons through\nminimally invasive approaches. To fully realize its potential, however, a\nprecise understanding of the visual data generated during surgical procedures\nis essential. Previous studies have predominantly focused on single-task\napproaches, but real surgical scenes involve complex temporal dynamics and\ndiverse instrument interactions that limit comprehensive understanding.\nMoreover, the effective application of multi-task learning (MTL) requires\nsufficient pixel-level segmentation data, which are difficult to obtain due to\nthe high cost and expertise required for annotation. In particular, long-term\nannotations such as phases and steps are available for every frame, whereas\nshort-term annotations such as surgical instrument segmentation and action\ndetection are provided only for key frames, resulting in a significant\ntemporal-spatial imbalance. To address these challenges, we propose a novel\nframework that combines optical flow-based segmentation label interpolation\nwith multi-task learning. optical flow estimated from annotated key frames is\nused to propagate labels to adjacent unlabeled frames, thereby enriching sparse\nspatial supervision and balancing temporal and spatial information for\ntraining. This integration improves both the accuracy and efficiency of\nsurgical scene understanding and, in turn, enhances the utility of RAS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u5149\u6d41\u63d2\u503c\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u4e00\u4efb\u52a1\uff0c\u96be\u4ee5\u5168\u9762\u7406\u89e3\u624b\u672f\u573a\u666f\u4e2d\u7684\u590d\u6742\u65f6\u5e8f\u52a8\u6001\u53ca\u5668\u68b0\u4ea4\u4e92\uff0c\u800c\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u6570\u636e\u83b7\u5f97\u6210\u672c\u9ad8\u4e14\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u65f6\u7a7a\u4e0a\u5206\u5e03\u6781\u4e0d\u5747\u8861\u3002", "method": "\u63d0\u51fa\u7528\u5149\u6d41\u5bf9\u6807\u6ce8\u5173\u952e\u5e27\u7684\u5206\u5272\u6807\u7b7e\u8fdb\u884c\u63d2\u503c\uff0c\u5c06\u6807\u7b7e\u4f20\u64ad\u5230\u76f8\u90bb\u672a\u6807\u6ce8\u5e27\uff0c\u5b9e\u73b0\u7a00\u758f\u7a7a\u95f4\u76d1\u7763\u7684\u8865\u5145\uff0c\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\uff0c\u5171\u540c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u56e0\u6807\u6ce8\u7a00\u7f3a\u5bfc\u81f4\u7684\u8bad\u7ec3\u74f6\u9888\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u5149\u6d41\u63d2\u503c\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u5145\u5206\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.18824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18824", "abs": "https://arxiv.org/abs/2509.18824", "authors": ["Yanzuo Lu", "Xin Xia", "Manlin Zhang", "Huafeng Kuang", "Jianbin Zheng", "Yuxi Ren", "Xuefeng Xiao"], "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation", "comment": "Technical Report", "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Hyper-Bagel\u6846\u67b6\uff0c\u5927\u5e45\u52a0\u901f\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u5b9e\u73b0\u591a\u500d\u63d0\u901f\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u7edf\u4e00\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5176\u5728\u7406\u89e3\u548c\u751f\u6210\u591a\u79cd\u5185\u5bb9\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u6269\u6563\u53bb\u566a\u548c\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u4e0e\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u52a0\u901f\u6846\u67b6\u2014\u2014Hyper-Bagel\uff0c\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\uff1a\u5229\u7528speculative decoding\u52a0\u901f\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u52a0\u901f\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u3002\u9488\u5bf9\u751f\u6210\u4efb\u52a1\uff0c\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e86\u5bf9\u6297\u84b8\u998f\u548c\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\uff0c\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u52a0\u901f\u500d\u6570\u8d85\u8fc72\u500d\uff1b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0clossless 6-NFE\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0c\u5206\u522b\u83b7\u5f97\u4e8616.67\u500d\u548c22\u500d\u7684\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\u3002\u9ad8\u6548\u76841-NFE\u6a21\u578b\u53ef\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u7f16\u8f91\u548c\u751f\u6210\u3002", "conclusion": "Hyper-Bagel\u6846\u67b6\u6781\u5927\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u54cd\u5e94\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6027\u4ef7\u6bd4\u548c\u65e0\u635f\u9ad8\u8d28\u91cf\u8f93\u51fa\uff0c\u4e3a\u591a\u6a21\u6001\u7edf\u4e00\u6a21\u578b\u5e26\u6765\u4e86\u5373\u65f6\u4e14\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.18839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18839", "abs": "https://arxiv.org/abs/2509.18839", "authors": ["Gianmarco Spinaci", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography", "comment": "11 pages, 2 figures", "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u57fa\u7763\u6559\u56fe\u50cf\u5355\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u90e8\u5206\u901a\u7528\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u793a\u5176\u5728\u6587\u5316\u9057\u4ea7\u5143\u6570\u636e\u5f52\u6863\u5177\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u7814\u7a76\u5e0c\u671b\u63a2\u7d22\u8fd9\u4e9b\u6a21\u578b\u80fd\u5426\u80dc\u4efb\u4f20\u7edf\u76d1\u7763\u5206\u7c7b\u5668\u5728\u590d\u6742\u6587\u5316\u9898\u6750\u56fe\u50cf\uff08\u5982\u57fa\u7763\u6559\u5723\u4eba\u56fe\u50cf\uff09\u4e0a\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e30\u5bcc\u77e5\u8bc6\u6ce8\u5165\u548c\u6837\u4f8b\u63d0\u793a\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528ArtDL\u3001ICONCLASS\u4e0eWikidata\u4e09\u4e2a\u652f\u6301Iconclass\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u9009\u62e9\u5341\u4e2a\u6700\u5e38\u89c1\u7c7b\u522b\uff0c\u5bf9\u6bd4\u6d4b\u8bd5CLIP\u3001SigLIP\u3001GPT-4o\u3001Gemini 2.5\u4e0eResNet50\uff08\u5fae\u8c03\uff09\u5728\u4e09\u79cd\u60c5\u5883\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\uff1a\u4ec5\u7528\u5206\u7c7b\u6807\u7b7e\u3001\u7528Iconclass\u63cf\u8ff0\u3001\u4ee5\u53ca\u4e94\u4e2a\u793a\u4f8b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "Gemini-2.5 Pro\u4e0eGPT-4o\u5728\u591a\u6570\u60c5\u5883\u4e0b\u51c6\u786e\u7387\u8d85\u8fc7ResNet50\uff0c\u5c24\u5176\u4e30\u5bcc\u7c7b\u63cf\u8ff0\u65f6\u63d0\u5347\u663e\u8457\u3002Wikidata\u96c6\u8868\u73b0\u4e0b\u6ed1\uff0cSigLIP\u5f97\u5206\u6700\u9ad8\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u56fe\u7247\u5c3a\u5bf8\u53ca\u5143\u6570\u636e\u654f\u611f\u3002\u5c11\u6837\u672c\u5b66\u4e60\u6210\u6548\u6709\u9650\uff0c\u5076\u6709\u5c0f\u5e45\u63d0\u5347\u3002", "conclusion": "\u901a\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u80fd\u80dc\u4efb\u590d\u6742\u6587\u5316\u9057\u4ea7\u56fe\u50cf\u5206\u7c7b\uff0c\u53ef\u7528\u4e8e\u6570\u5b57\u4eba\u6587\u5b66\u79d1\u4e2d\u7684\u5143\u6570\u636e\u5f52\u6863\u5de5\u5177\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u4f18\u5316\u63d0\u793a\u8bcd\u5e76\u6269\u5c55\u5230\u66f4\u591a\u5206\u7c7b\u65b9\u6cd5\u4e0e\u6a21\u578b\u7684\u7814\u7a76\u3002"}}
{"id": "2509.18840", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18840", "abs": "https://arxiv.org/abs/2509.18840", "authors": ["Ismael Elsharkawi", "Hossam Sharara", "Ahmed Rafea"], "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction", "comment": "Under Review", "summary": "Image Representation Learning is an important problem in Computer Vision.\nTraditionally, images were processed as grids, using Convolutional Neural\nNetworks or as a sequence of visual tokens, using Vision Transformers.\nRecently, Vision Graph Neural Networks (ViG) have proposed the treatment of\nimages as a graph of nodes; which provides a more intuitive image\nrepresentation. The challenge is to construct a graph of nodes in each layer\nthat best represents the relations between nodes and does not need a\nhyper-parameter search. ViG models in the literature depend on\nnon-parameterized and non-learnable statistical methods that operate on the\nlatent features of nodes to create a graph. This might not select the best\nneighborhood for each node. Starting from k-NN graph construction to HyperGraph\nConstruction and Similarity-Thresholded graph construction, these methods lack\nthe ability to provide a learnable hyper-parameter-free graph construction\nmethod. To overcome those challenges, we present the Learnable Reparameterized\nGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies\nkey-query attention between every pair of nodes; then uses soft-threshold\nreparameterization for edge selection, which allows the use of a differentiable\nmathematical model for training. Using learnable parameters to select the\nneighborhood removes the bias that is induced by any clustering or thresholding\nmethods previously introduced in the literature. In addition, LRGC allows\ntuning the threshold in each layer to the training data since the thresholds\nare learnable through training and are not provided as hyper-parameters to the\nmodel. We demonstrate that the proposed ViG-LRGC approach outperforms\nstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u56fe\u6784\u5efa\u65b9\u6cd5LRGC\uff08Learnable Reparameterized Graph Construction\uff09\uff0c\u7528\u4e8e\u89c6\u89c9\u56fe\u795e\u7ecf\u7f51\u7edc\uff08ViG\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53ef\u5fae\u5206\u65b9\u5f0f\u81ea\u52a8\u9009\u53d6\u90bb\u5c45\u8282\u70b9\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u624b\u5de5\u8d85\u53c2\u6570\u7684\u4f9d\u8d56\uff0c\u5e76\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u56fe\u795e\u7ecf\u7f51\u7edc\u591a\u4f9d\u8d56\u4e8e\u975e\u53c2\u6570\u5316\u3001\u4e0d\u53ef\u5b66\u4e60\u7684\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5982k-NN\u3001\u9608\u503c\u6cd5\u7b49\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u8bbe\u5b9a\u624b\u5de5\u8d85\u53c2\u6570\uff0c\u4e14\u672a\u5145\u5206\u6316\u6398\u8282\u70b9\u95f4\u5173\u7cfb\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b50\u6700\u4f18\u7684\u56fe\u7ed3\u6784\u3002\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u53ef\u5b66\u4e60\u4e14\u65e0\u9700\u4eba\u5de5\u8d85\u53c2\u6570\u8bbe\u5b9a\u7684\u56fe\u6784\u5efa\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86LRGC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165key-query\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u8282\u70b9\u5bf9\u95f4\u7684\u76f8\u5173\u6027\uff0c\u518d\u91c7\u7528\u8f6f\u9608\u503c\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\u9009\u62e9\u8fb9\u3002\u9608\u503c\u8bbe\u7f6e\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u81ea\u52a8\u8c03\u6574\uff0c\u5b9e\u73b0\u5c42\u7ea7\u81ea\u9002\u5e94\u90bb\u57df\u9009\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528LRGC\u7684ViG\u6a21\u578b\uff08ViG-LRGC\uff09\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u540c\u89c4\u6a21\u4e0b\u7684\u5176\u4ed6\u4e3b\u6d41\u89c6\u89c9\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "conclusion": "LRGC\u4e3a\u89c6\u89c9\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u8d85\u53c2\u6570\u548c\u66f4\u52a0\u81ea\u9002\u5e94\u7684\u56fe\u6784\u5efa\u65b9\u5f0f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u62d3\u5bbd\u4e86ViG\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18891", "abs": "https://arxiv.org/abs/2509.18891", "authors": ["Xueyu Liu", "Xiaoyi Zhang", "Guangze Shi", "Meilin Liu", "Yexin Lai", "Yongfei Wu", "Mingqiang Wei"], "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model", "comment": null, "summary": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoint Prompt Defender\u7684\u5bf9\u6297\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4f18\u5316\u70b9\u63d0\u793a\u63d0\u5347SAM\u5206\u5272\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709SAM\u5206\u5272\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u6216\u624b\u5de5\u6784\u5efa\u7684\u63d0\u793a\uff08prompts\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u5c06\u56fe\u50cf\u8865\u4e01\u8868\u793a\u4e3a\u53cc\u7a7a\u95f4\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u7ed3\u5408\u7269\u7406\u4e0e\u8bed\u4e49\u8ddd\u79bb\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4efb\u52a1\u65e0\u5173\u7684\u70b9\u63d0\u793a\u73af\u5883\u3002\u5728\u8be5\u73af\u5883\u4e2d\uff0c\u653b\u51fb\u8005\u667a\u80fd\u4f53\u5b66\u4e60\u6fc0\u6d3b\u6700\u5177\u5e72\u6270\u6027\u7684\u63d0\u793a\u4ee5\u964d\u4f4eSAM\u5206\u5272\u8868\u73b0\uff0c\u9632\u5fa1\u8005\u5219\u5b66\u4e60\u6291\u5236\u8fd9\u4e9b\u63d0\u793a\u5e76\u6062\u590d\u51c6\u786e\u6027\u3002\u4e24\u8005\u5747\u5229\u7528\u6df1\u5ea6Q\u7f51\u7edc\u8bad\u7ec3\uff0c\u5956\u52b1\u4fe1\u53f7\u57fa\u4e8e\u5206\u5272\u8d28\u91cf\u53d8\u5316\u3002\u5728\u63a8\u7406\u65f6\u4ec5\u90e8\u7f72\u9632\u5fa1\u8005\uff0c\u5bf9\u4efb\u610f\u7c97\u7cd9\u63d0\u793a\u96c6\u8fdb\u884c\u4f18\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86SAM\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "Point Prompt Defender\u662f\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u65b0\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0b\u7684\u8868\u73b0\uff0c\u662f\u63a8\u52a8\u8be5\u9886\u57df\u81ea\u52a8\u5316\u4e0e\u6cdb\u5316\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2509.18894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18894", "abs": "https://arxiv.org/abs/2509.18894", "authors": ["Jenna Kline", "Anirudh Potlapally", "Bharath Pillai", "Tanishka Wani", "Rugved Katole", "Vedant Patil", "Penelope Covey", "Hari Subramoni", "Tanya Berger-Wolf", "Christopher Stewart"], "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset", "comment": "8 pages", "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SmartWilds\uff0c\u8fd9\u662f\u9996\u4e2a\u540c\u6b65\u91c7\u96c6\u65e0\u4eba\u673a\u5f71\u50cf\u3001\u76f8\u673a\u9677\u9631\u7167\u7247\u4e0e\u89c6\u9891\u3001\u751f\u7269\u58f0\u5b66\u5f55\u97f3\u7684\u591a\u6a21\u6001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u6570\u636e\u96c6\u3002\u6570\u636e\u91c7\u96c6\u4e8e2025\u5e74\u590f\u5929\uff0c\u5730\u70b9\u4e3a\u4fc4\u4ea5\u4fc4\u5ddeThe Wilds\u91ce\u751f\u52a8\u7269\u56ed\u3002", "motivation": "\u968f\u7740\u73af\u5883\u53d8\u5316\u4e0e\u7269\u79cd\u706d\u7edd\u5a01\u80c1\u589e\u52a0\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u9ad8\u6548\u7684\u751f\u6001\u76d1\u6d4b\u624b\u6bb5\u3002\u73b0\u6709\u5355\u4e00\u4f20\u611f\u5668\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7684\u751f\u6001\u7814\u7a76\u4e0e\u4fdd\u62a4\u9700\u6c42\uff0c\u6025\u9700\u96c6\u6210\u591a\u79cd\u6570\u636e\u6e90\u7684\u516c\u5f00\u6570\u636e\u96c6\u652f\u6301\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u3002", "method": "\u5728220\u82f1\u4ea9\u7684\u7267\u573a\u5185\uff0c\u9488\u5bf9\u591a\u79cd\u4fdd\u62a4\u53ca\u672c\u5730\u7269\u79cd\uff08\u5927\u536b\u9e7f\u3001\u56db\u5ddd\u7f9a\u725b\u3001\u666e\u6c0f\u91ce\u9a6c\u7b49\uff09\u5f00\u5c55\u4e3a\u671f\u56db\u5929\u7684\u540c\u6b65\u591a\u6a21\u6001\u76d1\u6d4b\uff0c\u91c7\u96c6\u65e0\u4eba\u673a\u3001\u76f8\u673a\u3001\u58f0\u97f3\u7b49\u591a\u6e90\u6570\u636e\uff0c\u5e76\u6bd4\u8f83\u5404\u611f\u77e5\u65b9\u5f0f\u5728\u5730\u8c8c\u3001\u7269\u79cd\u68c0\u6d4b\u3001\u884c\u4e3a\u5206\u6790\u3001\u6816\u606f\u5730\u76d1\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5206\u6790\u8868\u660e\u4e0d\u540c\u4f20\u611f\u5668\u5728\u751f\u6001\u76d1\u6d4b\u573a\u666f\u4e2d\u5404\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u76f8\u4e92\u8865\u5145\u3002\u7814\u7a76\u5236\u5b9a\u4e86\u53ef\u91cd\u590d\u7684\u591a\u6a21\u6001\u76d1\u6d4b\u6d41\u7a0b\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u5411\u793e\u4f1a\u516c\u5f00\uff0c\u4fc3\u8fdb\u4fdd\u62a4\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u53d1\u5c55\u3002", "conclusion": "SmartWilds\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u6811\u7acb\u4e86\u65b0\u8303\u4f8b\uff0c\u540c\u65f6\u89c4\u5212\u672a\u6765\u6269\u5c55\uff0c\u5305\u62ec\u5f15\u5165GPS\u8ffd\u8e2a\u3001\u516c\u4f17\u79d1\u5b66\u6570\u636e\u53ca\u591a\u5b63\u8282\u91c7\u6837\uff0c\u4e3a\u751f\u6001\u4fdd\u62a4\u548c\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.18897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18897", "abs": "https://arxiv.org/abs/2509.18897", "authors": ["Jiayu Wang", "Ruizhi Wang", "Jie Song", "Haofei Zhang", "Mingli Song", "Zunlei Feng", "Li Sun"], "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing", "comment": "26 pages, 4 figures", "summary": "In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRS3DBench\u7684\u5168\u65b0\u57fa\u51c6\uff0c\u9488\u5bf9\u9065\u611f\u56fe\u50cf\u76843D\u89c6\u89c9\u7406\u89e3\uff0c\u5305\u542b\u5bf9\u9f50\u7684\u6df1\u5ea6\u56fe\u4e0e\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u7684\u524d\u6cbf\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u3002\u6570\u636e\u96c6\u53ca\u4ee3\u7801\u53ef\u516c\u5f00\u83b7\u53d6\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u6570\u636e\u96c6\u5f80\u5f80\u7f3a\u5c11\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u4fe1\u606f\u6216\u6df1\u5ea6\u4e0e\u56fe\u50cf\u5bf9\u9f50\u4e0d\u7cbe\u51c6\uff0c\u5236\u7ea6\u4e86\u5927\u89c4\u6a213D\u89c6\u89c9\u6a21\u578b\u5728\u9065\u611f\u573a\u666f\u7684\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b5\u4e07\u591a\u5bf9\u9065\u611f\u56fe\u50cf\u4e0e\u50cf\u7d20\u7ea7\u6df1\u5ea6\u56fe\u7684\u6570\u636e\u96c6\uff0c\u5e76\u914d\u4ee5\u6587\u672c\u63cf\u8ff0\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u7a33\u5b9a\u6269\u6563\u7684\u9065\u611f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u53d1\u6325\u591a\u6a21\u6001\u7279\u6027\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684RS3DBench\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u9065\u611f3D\u89c6\u89c9\u57fa\u51c6\uff0c\u57fa\u4e8e\u65b0\u57fa\u51c6\u5f00\u53d1\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002", "conclusion": "RS3DBench\u4e3a\u9065\u611f\u573a\u666f3D\u89c6\u89c9\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\uff0c\u63a8\u52a8\u4e86\u5730\u7406\u667a\u80fd\u4e0e3D\u89c6\u89c9\u611f\u77e5\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u65e0\u9700\u7ed3\u6784\u5149\u6d41\uff08SfM\uff09\u76843D\u9ad8\u65af\u55b7\u6e85\u53bb\u6a21\u7cca\u65b9\u6cd5\u2014\u2014DeblurSplat\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u4fe1\u606f\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u65e0\u9700\u76f8\u673a\u4f4d\u59ff\u4f30\u7b97\uff0c\u5373\u53ef\u63d0\u5347\u4e09\u7ef4\u573a\u666f\u53bb\u6a21\u7cca\u4e0e\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u76843D\u53bb\u6a21\u7cca\u91cd\u5efa\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5149\u6d41\uff08SfM\uff09\uff0c\u4f46\u8fd9\u4e00\u6d41\u7a0b\u7684\u76f8\u673a\u4f4d\u59ff\u63a8\u65ad\u6613\u5f15\u5165\u8bef\u5dee\uff0c\u5f71\u54cd\u70b9\u4e91\u751f\u6210\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u5feb\u901f\u8fd0\u52a8\u573a\u666f\u4e0b\u666e\u901a\u76f8\u673a\u6355\u6349\u80fd\u529b\u6709\u9650\u3002\u5982\u4f55\u63d0\u5347\u52a8\u6001\u6a21\u7cca\u573a\u666f\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\uff0c\u662f\u8be5\u5de5\u4f5c\u51fa\u53d1\u70b9\u3002", "method": "\u4f5c\u8005\u5229\u7528\u9884\u8bad\u7ec3\u7684DUSt3R\u5bc6\u96c6\u7acb\u4f53\u6a21\u5757\u76f4\u63a5\u4ece\u6a21\u7cca\u56fe\u50cf\u751f\u6210\u9ad8\u7cbe\u5ea6\u521d\u59cb\u70b9\u4e91\uff0c\u7701\u7565\u4e86\u76f8\u673a\u4f4d\u59ff\u7684\u8ba1\u7b97\uff1b\u540c\u65f6\u5f15\u5165\u9ad8\u52a8\u6001\u54cd\u5e94\u7684\u4e8b\u4ef6\u76f8\u673a\u4fe1\u606f\uff0c\u7ed3\u5408\u4e8b\u4ef6\u6d41\u548c\u6a21\u7cca\u56fe\u50cf\u89e3\u7801\u6e05\u6670\u56fe\uff0c\u4ece\u800c\u4e3a\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u4e0d\u4ec5\u751f\u6210\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\uff0c\u76f8\u6bd4\u73b0\u6709\u9876\u5c16\u65b9\u6cd5\uff08SOTA\uff09\u57283D\u53bb\u6a21\u7cca\u9ad8\u65af\u55b7\u6e85\u4efb\u52a1\u4e0a\u4e5f\u5177\u6709\u66f4\u9ad8\u7684\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "DeblurSplat\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u7531\u4e8e\u6a21\u7cca\u548c\u76f8\u673a\u59ff\u6001\u8bef\u5dee\u5bfc\u81f4\u7684\u4e09\u7ef4\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u7684\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u76f8\u51733D\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.18910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18910", "abs": "https://arxiv.org/abs/2509.18910", "authors": ["Shuwei Guo", "Simin Luan", "Yan Ke", "Zeyd Boukhers", "John See", "Cong Yang"], "title": "Moir\u00e9Net: A Compact Dual-Domain Network for Image Demoir\u00e9ing", "comment": null, "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u53bb\u9664\u7167\u7247\u83ab\u5c14\u6761\u7eb9\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edcMoir\u00e9Net\uff0c\u80fd\u517c\u987e\u9ad8\u8d28\u91cf\u4e0e\u8f7b\u91cf\u5316\uff0c\u9002\u5408\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u83ab\u5c14\u6761\u7eb9\u4e25\u91cd\u5f71\u54cd\u6570\u5b57\u56fe\u50cf\u8d28\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u5728\u9ad8\u6548\u4e0e\u6548\u679c\u95f4\u517c\u987e\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u89e3\u51b3\u591a\u5c3a\u5ea6\u3001\u5404\u5411\u5f02\u6027\u7b49\u95ee\u9898\u3002", "method": "\u57fa\u4e8eU-Net\u7ed3\u6784\uff0c\u7ed3\u5408\u9891\u7387\u4e0e\u7a7a\u95f4\u57df\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u65b9\u5411\u6027\u9891\u7387-\u7a7a\u95f4\u7f16\u7801\u5668(DFSE)\u8bc6\u522b\u83ab\u5c14\u6761\u7eb9\u65b9\u5411\uff0c\u4ee5\u53ca\u9891\u7387-\u7a7a\u95f4\u81ea\u9002\u5e94\u9009\u62e9\u5668(FSAS)\u81ea\u9002\u5e94\u6291\u5236\u6761\u7eb9\uff0c\u5b9e\u73b0\u7cbe\u51c6\u53bb\u9664\u3002", "result": "\u5728\u591a\u9879\u516c\u5f00\u548c\u6d3b\u8dc3\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f18\u6548\u679c\uff0c\u540c\u65f6\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u67095.513M\uff0c\u76f8\u8f83\u4e3b\u6d41\u65b9\u6cd5\u51cf\u5c11\u4e8648%\u3002", "conclusion": "Moir\u00e9Net\u7ed3\u5408\u4f18\u5f02\u7684\u4fee\u590d\u80fd\u529b\u4e0e\u4f4e\u53c2\u6570\u91cf\uff0c\u6781\u9002\u5408\u624b\u673a\u6444\u5f71\u3001\u5de5\u4e1a\u89c6\u89c9\u3001\u589e\u5f3a\u73b0\u5b9e\u7b49\u5bf9\u8d44\u6e90\u654f\u611f\u7684\u573a\u666f\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18912", "abs": "https://arxiv.org/abs/2509.18912", "authors": ["Yunzhe Shen", "Kai Peng", "Leiye Liu", "Wei Ji", "Jingjing Li", "Miao Zhang", "Yongri Piao", "Huchuan Lu"], "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation", "comment": null, "summary": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u97f3\u89c6\u9891\u5206\u5272\u6846\u67b6\uff0c\u91cd\u70b9\u5728\u4e8e\u4ece\u9891\u57df\u89d2\u5ea6\u5efa\u7b51\u4e0e\u878d\u5408\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u4ee5\u63d0\u5347\u5206\u5272\u6548\u679c\uff0c\u8fbe\u6210\u4e86\u4e1a\u754c\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u97f3\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5ffd\u89c6\u4e86\u97f3\u9891\u548c\u89c6\u89c9\u5728\u9ad8\u9891\u4fe1\u53f7\u4e2d\u7684\u56fa\u6709\u77db\u76fe\uff0c\u5bfc\u81f4\u97f3\u9891\u9ad8\u9891\u901a\u5e38\u5305\u542b\u5e72\u6270\u566a\u58f0\uff0c\u800c\u89c6\u89c9\u9ad8\u9891\u5219\u8574\u542b\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\uff0c\u8fd9\u4e00\u5dee\u5f02\u4e00\u65e6\u5ffd\u89c6\u4f1a\u8ba9\u5206\u5272\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f5c\u8005\u4ece\u9891\u57df\u89c6\u89d2\u91cd\u65b0\u5b9a\u4e49\u4e86\u97f3\u89c6\u9891\u5206\u5272\u4efb\u52a1\uff0c\u5c06\u5176\u89c6\u4f5c\u9891\u57df\u5206\u89e3\u4e0e\u91cd\u6784\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86FAVS\u6846\u67b6\uff0c\u5305\u62ec\u201c\u9891\u57df\u589e\u5f3a\u5206\u89e3\u5668\uff08FDED\uff09\u201d\u548c\u201c\u534f\u540c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff08SCMC\uff09\u201d\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\u3002FDED\u6a21\u5757\u901a\u8fc7\u6b8b\u5dee\u8fed\u4ee3\u9891\u57df\u5206\u89e3\uff0c\u533a\u5206\u4e0d\u540c\u6a21\u6001\u7684\u8bed\u4e49\u4e0e\u7ed3\u6784\u7279\u5f81\uff1bSCMC\u6a21\u5757\u501f\u52a9\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u8def\u7531\uff0c\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u4fdd\u6301\u6a21\u6001\u7279\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0FAVS\u6846\u67b6\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7ed3\u679c\u3002\u5927\u91cf\u53ef\u89c6\u5316\u5206\u6790\u4e5f\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u4e24\u5927\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u9891\u57df\u4e0a\u878d\u5408\u548c\u4fdd\u6301\u97f3\u89c6\u9891\u5404\u81ea\u4f18\u52bf\uff0c\u63d0\u51fa\u7684\u4e24\u4e2a\u6a21\u5757\u53ef\u663e\u8457\u63d0\u5347\u97f3\u89c6\u9891\u5206\u5272\u4efb\u52a1\u8868\u73b0\uff0c\u4e3a\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u65b9\u5411\u5e26\u6765\u65b0\u7684\u89c1\u89e3\u548c\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.18913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18913", "abs": "https://arxiv.org/abs/2509.18913", "authors": ["Nguyen Van Tu", "Pham Nguyen Hai Long", "Vo Hoai Viet"], "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision", "comment": null, "summary": "Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f53\u524d\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08xAI\uff09\u5728\u56fe\u50cf\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5404\u81ea\u673a\u7406\u3001\u4f18\u52a3\u548c\u8bc4\u4ef7\u6807\u51c6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u6a21\u578b\u96be\u4ee5\u89e3\u91ca\uff0c\u53ef\u9760\u6027\u53d7\u8d28\u7591\uff0c\u4e9f\u9700\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63d0\u5347\u5bf9AI\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "method": "\u7cfb\u7edf\u68b3\u7406\u5e76\u5206\u6790\u4e86\u56db\u79cd\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e0b\u7684xAI\u65b9\u6cd5\uff1a\u663e\u8457\u6027\u56fe\u3001\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u3001\u539f\u578b\u65b9\u6cd5\u53ca\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u5b83\u4eec\u7684\u5e95\u5c42\u673a\u5236\u548c\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u5bf9\u56db\u7c7bxAI\u65b9\u6cd5\u7684\u4f18\u70b9\u3001\u5c40\u9650\u53ca\u8bc4\u4f30\u65b9\u5f0f\u8fdb\u884c\u4e86\u5f52\u7eb3\u603b\u7ed3\uff0c\u5c55\u793a\u4e86\u5404\u81ea\u9002\u7528\u573a\u666f\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u7efc\u8ff0\u4e3axAI\u9886\u57df\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u6307\u51fa\u672a\u6765\u5728\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\u63d0\u5347\u65b9\u9762\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.18917", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18917", "abs": "https://arxiv.org/abs/2509.18917", "authors": ["Amirhesam Aghanouri", "Cristina Olaverri-Monreal"], "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models", "comment": null, "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210LiDAR\u70b9\u4e91\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u4e8e3D\u89c6\u89c9\u4e0eLiDAR\u4f20\u611f\u5668\uff0c\u4f46\u771f\u5b9eLiDAR\u6570\u636e\u91c7\u96c6\u8d39\u65f6\u4e14\u6613\u53d7\u566a\u97f3\u3001\u7a00\u758f\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u6570\u636e\u91cf\u548c\u6a21\u578b\u6548\u679c\u3002\u4e3a\u7a81\u7834\u6570\u636e\u74f6\u9888\uff0c\u9700\u5f00\u53d1\u9ad8\u8d28\u91cf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\uff0c\u5e76\u5f15\u5165\u521b\u65b0\u7684\u566a\u58f0\u8c03\u5ea6\u53ca\u65f6\u95f4\u6b65\u5d4c\u5165\u6280\u672f\uff0c\u751f\u6210\u66f4\u52a0\u771f\u5b9e\u548c\u7ed3\u6784\u4e30\u5bcc\u7684\u70b9\u4e91\u6570\u636e\u3002\u65b9\u6cd5\u5728IAMCV\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\uff0c\u5229\u7528\u56db\u9879\u6307\u6807\u5bf9\u6bd4SOTA\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u566a\u58f0\u548c\u7a00\u758f\u6570\u636e\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u53ef\u751f\u6210\u7ed3\u6784\u7ec6\u8282\u4e30\u5bcc\u7684\u70b9\u4e91\u6570\u636e\uff0c\u8986\u76d6\u591a\u79cd\u7a7a\u95f4\u5173\u7cfb\u3002", "conclusion": "\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u62dfLiDAR\u70b9\u4e91\u8d28\u91cf\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u4efb\u52a1\u589e\u5f3a\u63d0\u4f9b\u6570\u636e\u652f\u6491\uff0c\u663e\u8457\u6539\u5584\u5bf9\u566a\u58f0\u548c\u7a00\u758f\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18919", "abs": "https://arxiv.org/abs/2509.18919", "authors": ["Chuni Liu", "Hongjie Li", "Jiaqi Du", "Yangyang Hou", "Qian Sun", "Lei Jin", "Ke Xu"], "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset", "comment": null, "summary": "The pretraining-finetuning paradigm is a crucial strategy in metallic surface\ndefect detection for mitigating the challenges posed by data scarcity. However,\nits implementation presents a critical dilemma. Pretraining on natural image\ndatasets such as ImageNet, faces a significant domain gap. Meanwhile, naive\nself-supervised pretraining on in-domain industrial data is often ineffective\ndue to the inability of existing learning objectives to distinguish subtle\ndefect patterns from complex background noise and textures. To resolve this, we\nintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm\nthat explicitly guides representation learning through anomaly priors. AGSSP\nemploys a two-stage framework: (1) it first pretrains the model's backbone by\ndistilling knowledge from anomaly maps, encouraging the network to capture\ndefect-salient features; (2) it then pretrains the detector using pseudo-defect\nboxes derived from these maps, aligning it with localization tasks. To enable\nthis, we develop a knowledge-enhanced method to generate high-quality anomaly\nmaps and collect a large-scale industrial dataset of 120,000 images.\nAdditionally, we present two small-scale, pixel-level labeled metallic surface\ndefect datasets for validation. Extensive experiments demonstrate that AGSSP\nconsistently enhances performance across various settings, achieving up to a\n10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to\nImageNet-based models. All code, pretrained models, and datasets are publicly\navailable at https://clovermini.github.io/AGSSP-Dev/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u5e38\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08AGSSP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u91d1\u5c5e\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\uff0c\u89e3\u51b3\u9886\u57df\u5dee\u5f02\u548c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7f3a\u9677\u95ee\u9898\u3002\u901a\u8fc7\u5f02\u5e38\u5148\u9a8c\u5f15\u5bfc\u7279\u5f81\u5b66\u4e60\uff0c\u5b9e\u9a8c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfImageNet\u9884\u8bad\u7ec3\u3002", "motivation": "\u9762\u5bf9\u91d1\u5c5e\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u9886\u57df\u6570\u636e\u7a00\u7f3a\uff0c\u5e38\u7528\u9884\u8bad\u7ec3\u5fae\u8c03\u7b56\u7565\u5b58\u5728\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u53ca\u81ea\u76d1\u7763\u65b9\u5f0f\u96be\u4ee5\u6709\u6548\u533a\u5206\u7f3a\u9677\u548c\u590d\u6742\u80cc\u666f\u3002\u4e3a\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u66f4\u9002\u5408\u5de5\u4e1a\u6570\u636e\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "AGSSP\u91c7\u7528\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u5f02\u5e38\u56fe\u77e5\u8bc6\u84b8\u998f\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5bfc\u5176\u5173\u6ce8\u7f3a\u9677\u663e\u8457\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u4f2a\u7f3a\u9677\u6846\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\uff0c\u4f7f\u5176\u4e0e\u5b9a\u4f4d\u4efb\u52a1\u5bf9\u9f50\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u5f02\u5e38\u56fe\u751f\u6210\u65b9\u6cd5\u5e76\u6784\u5efa12\u4e07\u5f20\u5de5\u4e1a\u5927\u6570\u636e\u96c6\uff0c\u8fd8\u5236\u4f5c2\u4e2a\u5c0f\u89c4\u6a21\u50cf\u7d20\u7ea7\u91d1\u5c5e\u7f3a\u9677\u96c6\u4ee5\u9a8c\u8bc1\u6548\u679c\u3002", "result": "AGSSP\u5728\u4e0d\u540c\u8bbe\u5b9a\u4e0b\u5747\u660e\u663e\u63d0\u5347\u8868\u73b0\uff0c\u5bf9\u6bd4ImageNet\u9884\u8bad\u7ec3\u6a21\u578b\uff0cmAP@0.5\u63d0\u5347\u6700\u591a10%\uff0cmAP@0.5:0.95\u63d0\u5347\u8fbe11.4%\u3002", "conclusion": "AGSSP\u6709\u6548\u7f13\u89e3\u4e86\u9886\u57df\u9002\u5e94\u548c\u81ea\u76d1\u7763\u7279\u5f81\u74f6\u9888\uff0c\u5207\u5b9e\u63d0\u5347\u5de5\u4e1a\u91d1\u5c5e\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\uff0c\u65b9\u6cd5\u53ca\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u53ef\u4e3a\u76f8\u5173\u7814\u7a76\u4e0e\u4ea7\u4e1a\u5e94\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.18924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18924", "abs": "https://arxiv.org/abs/2509.18924", "authors": ["Kartik Teotia", "Helge Rhodin", "Mohit Mendiratta", "Hyeongwoo Kim", "Marc Habermann", "Christian Theobalt"], "title": "Audio-Driven Universal Gaussian Head Avatars", "comment": "(SIGGRAPH Asia 2025) Project page:\n  https://kartik-teotia.github.io/UniGAHA/", "summary": "We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u97f3\u9891\u9a71\u52a8\u7684\u901a\u7528\u62df\u771f\u5934\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u901a\u7528\u5934\u90e8\u5934\u50cf\u5148\u9a8c\uff08UHAP\uff09\u548c\u65e0\u5173\u8eab\u4efd\u7684\u8bed\u97f3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u7ec6\u81f4\u7684\u9762\u90e8\u8868\u60c5\u548c\u5916\u89c2\u91cd\u5efa\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u6a21\u578b\u591a\u53ea\u5173\u6ce8\u4e8e\u51e0\u4f55\u53d8\u5f62\uff0c\u5ffd\u7565\u4e86\u97f3\u9891\u76f8\u5173\u7684\u5916\u89c2\u7ec6\u8282\uff0c\u4e14\u7f3a\u4e4f\u53ef\u4ee5\u901a\u7528\u4e14\u9ad8\u4fdd\u771f\u7684\u5934\u50cf\u5408\u6210\u65b9\u6848\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u51c6\u786e\u8868\u8fbe\u4e2a\u4f53\u7279\u5f81\uff0c\u53c8\u80fd\u6a21\u62df\u7ec6\u81f4\u5916\u89c2\u53d8\u5316\u7684\u901a\u7528\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86\u4ee5\u591a\u8eab\u4efd\u6570\u636e\u8bad\u7ec3\u5f97\u5230\u7684Universal Head Avatar Prior (UHAP)\uff0c\u4ee5\u53ca\u80fd\u5c06\u97f3\u9891\u76f4\u63a5\u6620\u5c04\u5230\u8be5\u8868\u8fbe\u7a7a\u95f4\u7684\u901a\u7528\u8bed\u97f3\u6a21\u578b\u3002\u4e2a\u4eba\u5316\u6b65\u9aa4\u91c7\u7528\u5355\u76ee\u7f16\u7801\u5668\uff0c\u4ee5\u9ad8\u6548\u56de\u5f52\u89c6\u9891\u5e27\u95f4\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u5fae\u8c03\u5206\u79bb\u8868\u60c5\u4e0e\u5916\u89c2\u8d21\u732e\u3002\u6b64\u5916\uff0cUHAP\u7684\u8868\u8fbe\u7a7a\u95f4\u5305\u542b\u4e86\u51e0\u4f55\u548c\u5916\u89c2\u4fe1\u606f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u9ad8\u4fdd\u771f\u5408\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u62df\u771f\u3001\u7cbe\u51c6\u5507\u5f62\u540c\u6b65\u4e14\u5bcc\u542b\u7ec6\u8282\uff08\u5982\u7709\u6bdb\u79fb\u52a8\u3001\u51dd\u89c6\u3001\u5634\u90e8\u5185\u4fa7\u52a8\u6001\u7b49\uff09\u7684\u5934\u50cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5176\u5728\u5507\u52a8\u540c\u6b65\u51c6\u786e\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u611f\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u51e0\u4f55\u9a71\u52a8\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u4e0d\u540c\u8eab\u4efd\u7684\u97f3\u9891\u9a71\u52a8\u62df\u771f\u5934\u50cf\u901a\u7528\u6a21\u578b\uff0c\u7a81\u7834\u4e86\u4ec5\u57fa\u4e8e\u51e0\u4f55\u4fe1\u606f\u7684\u5c40\u9650\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5916\u89c2\u7ec6\u8282\u590d\u73b0\u80fd\u529b\u548c\u5934\u50cf\u751f\u6210\u7684\u901a\u7528\u6027\u4e0e\u771f\u5b9e\u611f\u3002"}}
{"id": "2509.18926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18926", "abs": "https://arxiv.org/abs/2509.18926", "authors": ["Pamela Osuna-Vargas", "Altug Kamacioglu", "Dominik F. Aschauer", "Petros E. Vlachos", "Sercan Alipek", "Jochen Triesch", "Simon Rumpel", "Matthias Kaschube"], "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines", "comment": null, "summary": "Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u5206\u6790\u7ba1\u9053\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u8ffd\u8e2a\u548c\u63d0\u53d6\u6811\u7a81\u68d8\u57283D+\u65f6\u95f4\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u91cd\u8981\u7ed3\u6784\u7279\u5f81\uff0c\u5927\u5927\u7b80\u5316\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u5206\u6790\u6d41\u7a0b\u3002", "motivation": "\u6811\u7a81\u68d8\u7684\u7ed3\u6784\u5bf9\u4e8e\u7814\u7a76\u5927\u8111\u7a81\u89e6\u53ef\u5851\u6027\u3001\u5b66\u4e60\u548c\u8bb0\u5fc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u3001\u957f\u65f6\u95f4\u76843D\u6811\u7a81\u68d8\u52a8\u6001\u5206\u6790\u56e0\u6570\u636e\u5904\u7406\u590d\u6742\u800c\u6781\u5177\u6311\u6218\u3002\u4e9f\u9700\u81ea\u52a8\u5316\u5de5\u5177\u4ee5\u52a0\u901f\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5305\u62ec\uff081\uff09\u57fa\u4e8etransformer\u7684\u6811\u7a81\u68d8\u68c0\u6d4b\u6a21\u5757\uff1b\uff082\uff09\u878d\u5408\u7a7a\u95f4\u7279\u5f81\u7684\u6df1\u5ea6\u8ffd\u8e2a\u7ec4\u4ef6\uff1b\uff083\uff09\u5229\u7528\u7a7a\u95f4\u4e00\u81f4\u6027\u5b9e\u73b0\u7684\u65f6\u95f4\u8ffd\u8e2a\u6a21\u5757\uff1b\uff084\uff09\u5b9a\u91cf\u63d0\u53d6\u751f\u7269\u5b66\u76f8\u5173\u68d8\u7279\u5f81\u7684\u5355\u5143\u3002\u65b9\u6cd5\u5728\u5f00\u6e90\u6570\u636e\u548c\u4f5c\u8005\u65b0\u53d1\u5e03\u7684\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u9a8c\u8bc1\u663e\u793a\u65b0\u7ba1\u9053\u80fd\u591f\u6709\u6548\u5730\u81ea\u52a8\u68c0\u6d4b\u3001\u8ffd\u8e2a\u4ee5\u53ca\u91cf\u5316\u6811\u7a81\u68d8\u57283D+time\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7ba1\u9053\u5b9e\u73b0\u4e86\u6811\u7a81\u68d8\u7ed3\u6784\u52a8\u6001\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9ad8\u6548\u5206\u6790\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u6570\u636e\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\u57fa\u7840\uff0c\u6709\u671b\u6781\u5927\u4fc3\u8fdb\u8111\u79d1\u5b66\u53ca\u76f8\u5173\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.18938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18938", "abs": "https://arxiv.org/abs/2509.18938", "authors": ["Matheus Vin\u00edcius Todescato", "Joel Lu\u00eds Carbonera"], "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning", "comment": "This paper was accepted at International Conference on Tools with\n  Artificial Intelligence (ICTAI) 2025", "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u548c\u81ea\u5b66\u4e60\u5faa\u73af\uff0c\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u5b9e\u9645\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002\u4e3a\u4e86\u89e3\u51b3\u65e0\u6807\u6ce8\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u7ed3\u5408\u5df2\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u96f6\u6837\u672c\u5206\u7c7b\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86VLM\u4e0e\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff1a1\uff09\u4ec5\u9700\u7c7b\u522b\u540d\u79f0\u4f5c\u4e3a\u4fe1\u606f\uff0c\u4e0d\u7528\u4efb\u4f55\u5df2\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff1b2\uff09\u4f7f\u7528VLM\u5728\u6d4b\u8bd5\u96c6\u9009\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u6837\u672c\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6837\u672c\u7684\u7279\u5f81\u8fdb\u884c\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff1b3\uff09\u901a\u8fc7\u81ea\u5b66\u4e60\u5faa\u73af\uff0c\u7528\u589e\u5f3a\u7279\u5f81\u53cd\u590d\u8fed\u4ee3\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u52a8\u6001\u9002\u5e94\u4efb\u52a1\uff1b4\uff09\u4e0d\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u89c6\u89c9\u6a21\u578b\u964d\u4f4e\u5bf9\u8bed\u4e49\u8868\u5f81\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u793a\u7b56\u7565\u5177\u6709\u8f83\u597d\u7684\u63a8\u5e7f\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u7ed3\u5408VLM\u4e0e\u89c6\u89c9\u6a21\u578b\u5e76\u91c7\u7528\u4f2a\u6807\u7b7e\u81ea\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u65e0\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u4e0b\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u53ef\u884c\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18956", "abs": "https://arxiv.org/abs/2509.18956", "authors": ["Zijing Guo", "Yunyang Zhao", "Lin Wang"], "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting", "comment": null, "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86MirrorScene3D\u6570\u636e\u96c6\u5e76\u57fa\u4e8e\u53cd\u5c04\u4fe1\u606f\u63d0\u51fa\u4e86\u63d0\u5347\u955c\u9762\u4e30\u5bcc\u73af\u5883\u4e0b\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5ReflectiveGS\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u4e0e\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff08\u5982NeRF\u30013DGS\uff09\u5728\u5305\u542b\u955c\u9762\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u53cd\u5c04\u5e26\u6765\u7684\u89c6\u89d2\u4f9d\u8d56\u6027\u5931\u771f\u4e0e\u4e0d\u4e00\u81f4\u6027\uff1b\u800c\u4ee5\u5f80\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u955c\u9762\u5bf9\u79f0\uff0c\u5374\u5ffd\u7565\u955c\u50cf\u89c6\u89d2\u4e2d\u8574\u542b\u7684\u4e30\u5bcc\u91cd\u5efa\u4fe1\u606f\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86MirrorScene3D\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u5ba4\u5185\u573a\u666f\u30011256\u5f20\u9ad8\u8d28\u91cf\u56fe\u7247\u53ca\u955c\u9762\u6807\u6ce8\uff0c\u4f5c\u4e3a\u955c\u9762\u73af\u5883\u4e09\u7ef4\u91cd\u5efa\u7684\u57fa\u51c6\u3002\u5e76\u63d0\u51faReflectiveGS\uff0c\u5c06\u955c\u9762\u53cd\u5c04\u4f5c\u4e3a\u8865\u5145\u89c6\u89d2\uff0c\u5229\u75283D Gaussian Splatting\u65b9\u6cd5\u63d0\u5347\u573a\u666f\u51e0\u4f55\u548c\u7ec6\u8282\u8fd8\u539f\u80fd\u529b\u3002", "result": "ReflectiveGS\u5728MirrorScene3D\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SSIM\u3001PSNR\u3001LPIPS\u7b49\u6307\u6807\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u955c\u9762\u53cd\u5c04\u89c6\u89d2\u80fd\u591f\u663e\u8457\u63d0\u5347\u5e26\u955c\u573a\u666f\u7684\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\uff0c\u6240\u63d0\u65b9\u6cd5\u6811\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u6709\u8f83\u5927\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2509.18958", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18958", "abs": "https://arxiv.org/abs/2509.18958", "authors": ["Cristina Iacono", "Mariarosaria Meola", "Federica Conte", "Laura Mecozzi", "Umberto Bracale", "Pietro Falco", "Fanny Ficuciello"], "title": "Generative data augmentation for biliary tract detection on intraoperative images", "comment": null, "summary": "Cholecystectomy is one of the most frequently performed procedures in\ngastrointestinal surgery, and the laparoscopic approach is the gold standard\nfor symptomatic cholecystolithiasis and acute cholecystitis. In addition to the\nadvantages of a significantly faster recovery and better cosmetic results, the\nlaparoscopic approach bears a higher risk of bile duct injury, which has a\nsignificant impact on quality of life and survival. To avoid bile duct injury,\nit is essential to improve the intraoperative visualization of the bile duct.\nThis work aims to address this problem by leveraging a deep-learning approach\nfor the localization of the biliary tract from white-light images acquired\nduring the surgical procedures. To this end, the construction and annotation of\nan image database to train the Yolo detection algorithm has been employed.\nBesides classical data augmentation techniques, the paper proposes Generative\nAdversarial Network (GAN) for the generation of a synthetic portion of the\ntraining dataset. Experimental results have been discussed along with ethical\nconsiderations.", "AI": {"tldr": "\u672c\u8bba\u6587\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u767d\u5149\u56fe\u50cf\u81ea\u52a8\u5b9a\u4f4d\u80c6\u9053\uff0c\u4ee5\u964d\u4f4e\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u7684\u80c6\u9053\u635f\u4f24\u98ce\u9669\u3002", "motivation": "\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u867d\u7136\u6062\u590d\u5feb\u3001\u5916\u89c2\u597d\uff0c\u4f46\u80c6\u9053\u635f\u4f24\u98ce\u9669\u8f83\u9ad8\uff0c\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\uff0c\u63d0\u5347\u672f\u4e2d\u80c6\u9053\u53ef\u89c6\u5316\u80fd\u529b\u8feb\u5728\u7709\u776b\u3002", "method": "\u6784\u5efa\u5e76\u6807\u6ce8\u80c6\u9053\u767d\u5149\u56fe\u50cf\u6570\u636e\u5e93\uff0c\u91c7\u7528Yolo\u68c0\u6d4b\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u5e94\u7528\u7ecf\u5178\u6570\u636e\u589e\u5f3a\u548cGAN\u751f\u6210\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\uff0c\u4ee5\u589e\u5f3a\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5bf9\u672f\u4e2d\u80c6\u9053\u5b9a\u4f4d\u6709\u6548\uff0c\u76f8\u5173\u5b9e\u9a8c\u7ed3\u679c\u548c\u4f26\u7406\u8003\u91cf\u4e5f\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u591f\u8f85\u52a9\u63d0\u9ad8\u672f\u4e2d\u80c6\u9053\u8bc6\u522b\u7387\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u533b\u7597\u98ce\u9669\uff0c\u540c\u65f6\u5408\u6210\u56fe\u50cf\u7684\u4f7f\u7528\u9700\u8003\u8651\u4f26\u7406\u56e0\u7d20\u3002"}}
{"id": "2509.18973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18973", "abs": "https://arxiv.org/abs/2509.18973", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images", "comment": "MICCAI2025", "summary": "Domain adaptive segmentation (DAS) of numerous organelle instances from\nlarge-scale electron microscopy (EM) is a promising way to enable\nannotation-efficient learning. Inspired by SAM, we propose a promptable\nmultitask framework, namely Prompt-DAS, which is flexible enough to utilize any\nnumber of point prompts during the adaptation training stage and testing stage.\nThus, with varying prompt configurations, Prompt-DAS can perform unsupervised\ndomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well\nas interactive segmentation during testing. Unlike the foundation model SAM,\nwhich necessitates a prompt for each individual object instance, Prompt-DAS is\nonly trained on a small dataset and can utilize full points on all instances,\nsparse points on partial instances, or even no points at all, facilitated by\nthe incorporation of an auxiliary center-point detection task. Moreover, a\nnovel prompt-guided contrastive learning is proposed to enhance discriminative\nfeature learning. Comprehensive experiments conducted on challenging benchmarks\ndemonstrate the effectiveness of the proposed approach over existing UDA, WDA,\nand SAM-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\u6846\u67b6Prompt-DAS\uff0c\u53ef\u4ee5\u652f\u6301\u591a\u79cd\u70b9\u63d0\u793a\u914d\u7f6e\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u3001\u5f31\u76d1\u7763\u4ee5\u53ca\u4ea4\u4e92\u5f0f\u5206\u5272\u4efb\u52a1\uff0c\u76f8\u8f83\u4e3b\u6d41\u65b9\u6cd5\u5728\u7535\u5b50\u663e\u5fae\u955c\u4e0b\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5927\u89c4\u6a21\u7535\u5b50\u663e\u5fae\u955c\u4e0b\u7ec6\u80de\u5668\u5b9e\u4f8b\u7684\u5206\u5272\u9700\u8981\u5927\u91cf\u6807\u6ce8\uff0c\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\u6280\u672f\u80fd\u591f\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982SAM\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u90fd\u9700\u63d0\u793a\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u66f4\u7075\u6d3b\u7684\u63d0\u793a\u65b9\u5f0f\u548c\u66f4\u5c11\u6807\u6ce8\u8fdb\u884c\u81ea\u9002\u5e94\u5206\u5272\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u63d0\u51faPrompt-DAS\uff0c\u4e00\u4e2a\u53ef\u63a5\u6536\u4efb\u610f\u70b9\u63d0\u793a\u6570\u91cf\u7684\u591a\u4efb\u52a1\u5206\u5272\u6846\u67b6\uff0c\u652f\u6301\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u591a\u79cd\u63d0\u793a\u914d\u7f6e\u3002\u901a\u8fc7\u8f85\u52a9\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u4efb\u52a1\u5b9e\u73b0\u65e0\u63d0\u793a\u60c5\u51b5\u4e0b\u7684\u5206\u5272\uff0c\u5e76\u5f15\u5165\u63d0\u793a\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u589e\u5f3a\u7279\u5f81\u5224\u522b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u53ef\u5728\u5168\u63d0\u793a\u3001\u90e8\u5206\u63d0\u793a\u3001\u751a\u81f3\u65e0\u63d0\u793a\u60c5\u5883\u4e0b\u81ea\u9002\u5e94\u8fdb\u884c\u65e0\u76d1\u7763/\u5f31\u76d1\u7763\u5206\u5272\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aPrompt-DAS\u5728\u65e0\u76d1\u7763\u3001\u5f31\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\u548c\u57fa\u4e8eSAM\u7684\u65b9\u6cd5\u4e0a\u5747\u53d6\u5f97\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "Prompt-DAS\u5177\u5907\u66f4\u9ad8\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u6807\u6ce8\u548c\u591a\u6837\u7684\u63d0\u793a\u65b9\u5f0f\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2509.19003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19003", "abs": "https://arxiv.org/abs/2509.19003", "authors": ["Honghao Chen", "Xingzhou Lou", "Xiaokun Feng", "Kaiqi Huang", "Xinlong Wang"], "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "comment": "Accepted by NeurIPS 2025", "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5e94\u7528step-level\uff08\u6b65\u9aa4\u7ea7\uff09\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5956\u52b1\u548c\u65b0\u9896\u7684\u5956\u52b1\u8bc4\u4f30\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728chain of thought\u63a8\u7406\u4e0a\u83b7\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5c06\u5176\u8fc1\u79fb\u5230\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4f9d\u7136\u9762\u4e34\u96be\u4ee5\u7ec6\u7c92\u5ea6\u63a8\u7406\u53ca\u4e2d\u95f4\u8fc7\u7a0b\u8d28\u91cf\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u521b\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fachain of step\u63a8\u7406\u8303\u5f0f\uff0c\u5f15\u5165step-level\u63a8\u7406\u6570\u636e\u3001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u63a8\u7406\u6b65\u9aa4\u7ea7\u522b\u7684\u5956\u52b1\u8bc4\u4f30\u4e0e\u8bad\u7ec3\uff0c\u4ece\u800c\u7ec6\u81f4\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u591a\u4e2a\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u5145\u5206\u7684\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u4e86\u5404\u7ec4\u6210\u6a21\u5757\u7684\u4f5c\u7528\u53ca\u63a8\u7406\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u5c5e\u6027\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u6811\u7acb\u4e86\u4e00\u4e2a\u5f3a\u57fa\u7ebf\uff0c\u5bf9\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u66f4\u6df1\u5165\u53d1\u5c55\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u548c\u89c1\u89e3\uff0c\u76f8\u5173\u6570\u636e\u96c6\u3001\u6a21\u578b\u53ca\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2509.19028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19028", "abs": "https://arxiv.org/abs/2509.19028", "authors": ["Ioannis Sarafis", "Alexandros Papadopoulos", "Anastasios Delopoulos"], "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model", "comment": "Submitted to the 20th International Workshop on Semantic and Social\n  Media Adaptation & Personalization", "summary": "In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Segment Anything Model\uff08SAM\uff09\u4e0eVision Transformers\uff08ViT\uff09\u76f8\u7ed3\u5408\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u98df\u54c1\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u6548\u679c\u8f83\u597d\u3002", "motivation": "\u98df\u54c1\u56fe\u50cf\u5206\u5272\u901a\u5e38\u9700\u8981\u5927\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u975e\u5e38\u8017\u65f6\u3002\u89e3\u51b3\u8be5\u95ee\u9898\u53ef\u4ee5\u52a0\u901f\u76f8\u5173\u7684\u6ce8\u91ca\u5de5\u4f5c\uff0c\u5e76\u6709\u5229\u4e8e\u98df\u54c1\u548c\u8425\u517b\u8ddf\u8e2a\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u4ec5\u6709\u56fe\u50cf\u7ea7\u6ce8\u91ca\u8bad\u7ec3\u7684Swin Transformer\uff08ViT\uff09\u63d0\u53d6\u7c7b\u6fc0\u6d3b\u56fe\uff08CAMs\uff09\u751f\u6210SAM\u7684\u63d0\u793a\uff0c\u518d\u901a\u8fc7SAM\u4ea7\u751f\u5206\u5272\u63a9\u7801\uff1b\u540c\u65f6\u7ed3\u5408\u56fe\u50cf\u9884\u5904\u7406\u53ca\u5355/\u591a\u63a9\u7801\u7b56\u7565\u63d0\u5347\u63a9\u7801\u8d28\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728FoodSeg103\u6570\u636e\u96c6\u4e0a\uff0c\u6bcf\u5f20\u56fe\u5e73\u5747\u751f\u62102.4\u4e2a\u63a9\u7801\uff08\u4e0d\u542b\u80cc\u666f\uff09\uff0c\u591a\u63a9\u7801\u573a\u666f\u4e0bmIoU\u8fbe0.54\u3002", "conclusion": "\u6240\u63d0\u5f31\u76d1\u7763\u65b9\u6cd5\u65e0\u9700\u50cf\u7d20\u7ea7\u4fe1\u6807\u6ce8\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u98df\u54c1\u5206\u5272\uff0c\u80fd\u591f\u52a0\u901f\u98df\u54c1\u56fe\u50cf\u6ce8\u91ca\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u98df\u54c1\u4e0e\u8425\u517b\u8ddf\u8e2a\u573a\u666f\u3002"}}
{"id": "2509.19052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19052", "abs": "https://arxiv.org/abs/2509.19052", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation", "comment": null, "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDyL-UNet\u7684\u65b0\u578b\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u517c\u987e\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u58f0\u5fc3\u52a8\u56fe\u81ea\u52a8\u5206\u5272\u7684\u7a33\u5b9a\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u5728\u5fc3\u8840\u7ba1\u8bca\u65ad\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5176\u5206\u5272\u6613\u53d7\u5230\u56fe\u50cf\u566a\u58f0\u548c\u5f62\u53d8\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5e27\u95f4\u5206\u5272\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u529f\u80fd\u8bc4\u4f30\u548c\u4e34\u5e8a\u89e3\u91ca\u3002\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u5206\u5272\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51faDyL-UNet\u67b6\u6784\uff0c\u5229\u7528\u52a8\u6001\u5b66\u4e60\u6784\u5efa\u56de\u58f0\u52a8\u529b\u5b66\u56fe\uff08EDG\uff09\uff0c\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u52a8\u6001\u4fe1\u606f\u3002\u540c\u65f6\uff0c\u6a21\u578b\u91c7\u7528\u57fa\u4e8eSwin-Transformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u652f\u5904\u7406\u5355\u5e27\u56fe\u50cf\uff0c\u5e76\u5728\u8df3\u8dc3\u8fde\u63a5\u5904\u5f15\u5165\u5fc3\u52a8\u5468\u671f-\u52a8\u529b\u5b66\u6ce8\u610f\u529b\uff08CPDA\uff09\uff0c\u7ed3\u5408EDG\u52a8\u6001\u7279\u5f81\u4e0e\u5fc3\u52a8\u5468\u671f\u4fe1\u606f\uff0c\u5b9e\u73b0\u65f6\u5e8f\u4e00\u81f4\u5206\u5272\u3002", "result": "\u5728CAMUS\u548cEchoNet-Dynamic\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDyL-UNet\u5206\u5272\u51c6\u786e\u7387\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u65f6\u5e8f\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "DyL-UNet\u4e3a\u81ea\u52a8\u5316\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u51c6\u786e\u6027\u3001\u9ad8\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4e34\u5e8a\u8d85\u58f0\u5fc3\u52a8\u56fe\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.19073", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u91cd\u5efa\u65b9\u6cd5WaveletGaussian\uff0c\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u5bf9\u4f4e\u9891\u5b50\u5e26\u8fdb\u884c\u6269\u6563\u5efa\u6a21\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u76843D Gaussian Splatting\u6280\u672f\u5728\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002\u867d\u7136\u5229\u7528\u6269\u6563\u6a21\u578b\u4fee\u590d\u6e32\u67d3\u56fe\u50cf\u80fd\u6539\u5584\u6548\u679c\uff0c\u4f46\u5e26\u6765\u4e86\u9ad8\u6602\u7684\u8ba1\u7b97\u4e0e\u4f18\u5316\u6210\u672c\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u4f46\u6548\u679c\u4e0d\u900a\u8272\u7684\u65b0\u65b9\u6cd5\u3002", "method": "WaveletGaussian\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5c06\u6269\u6563\u6a21\u578b\u548c\u5c0f\u6ce2\u53d8\u6362\u7ed3\u5408\uff1a\u5148\u5c06\u6e32\u67d3\u56fe\u50cf\u5206\u89e3\u6210\u4f4e\u9891\u4e0e\u9ad8\u9891\u5b50\u5e26\uff0c\u4ec5\u5bf9\u4f4e\u5206\u8fa8\u7387\u7684LL\u5b50\u5e26\u5e94\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u91cd\u5efa\uff0c\u800c\u9ad8\u9891\u90e8\u5206\u5219\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u7ec6\u5316\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u968f\u673a\u63a9\u7801\u91c7\u6837\u7b56\u7565\uff0c\u7528\u4e8e\u6784\u9020\u6269\u6563\u5fae\u8c03\u7684\u8bad\u7ec3\u5bf9\uff0c\u53d6\u4ee3\u4f4e\u6548\u7684leave-one-out\u7b56\u7565\u3002", "result": "\u5728Mip-NeRF 360\u548cOmniObject3D\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cWaveletGaussian\u5728\u6e32\u67d3\u8d28\u91cf\u4e0e\u4e3b\u6d41\u6269\u6563\u4fee\u590d\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "WaveletGaussian\u65b9\u6cd5\u6709\u6548\u517c\u987e\u4e86\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u4e0e\u8bad\u7ec3\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5c0f\u6ce2\u57df\u5206\u6b65\u6269\u6563\u548c\u968f\u673a\u63a9\u7801\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e3a\u540e\u7eed\u9ad8\u6548\u76843D\u91cd\u5efa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.19082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19082", "abs": "https://arxiv.org/abs/2509.19082", "authors": ["Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference", "comment": null, "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i", "AI": {"tldr": "Sa2VA-i\u5bf9Sa2VA\u6a21\u578b\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u901a\u8fc7\u4fee\u6b63\u8bad\u7ec3\u4e0e\u63a8\u7406\u65f6\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5237\u65b0\u4e86\u6700\u4f73\u6210\u7ee9\u3002", "motivation": "\u867d\u7136Sa2VA\u5728\u591a\u9879\u5206\u5272\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u5176\u5728\u89c6\u9891\u76ee\u6807\u5206\u5272\u4efb\u52a1\u4e0a\u672a\u80fd\u5145\u5206\u53d1\u6325\u6f5c\u529b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002\u56e0\u6b64\uff0c\u63a8\u52a8\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u6548\u679c\u7684\u63d0\u5347\u662f\u672c\u5de5\u4f5c\u7684\u4e3b\u8981\u52a8\u529b\u3002", "method": "\u4f5c\u8005\u9488\u5bf9Sa2VA\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6539\u8fdb\u7248\u672cSa2VA-i\u3002\u8be5\u65b9\u6cd5\u4fee\u6b63\u4e86\u6d41\u7a0b\u4e2d\u7684\u5177\u4f53\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u4e0e\u8bad\u7ec3\u9636\u6bb5\u66f4\u52a0\u4e00\u81f4\u3002", "result": "Sa2VA-i\u5728\u591a\u4e2a\u89c6\u9891\u5206\u5272\u57fa\u51c6\u4e0a\u5237\u65b0\u4e86\u6700\u4f73\u6210\u7ee9\u3002\u5177\u4f53\u5305\u62ec\u5728MeViS\u4e0a\u63d0\u5347\u4e8611.6 J&F\uff0c\u5728Ref-YT-VOS\u4e0a\u63d0\u5347\u4e861.4\uff0c\u5728Ref-DAVIS\u4e0a\u63d0\u5347\u4e863.3\uff0c\u5728ReVOS\u4e0a\u63d0\u5347\u4e864.1\uff0c\u5e76\u4e14\u5229\u7528\u540c\u6837\u7684Sa2VA\u6743\u91cd\u3002Sa2VA-i-1B\u6a21\u578b\u4ec5\u7528\u539f\u7248\u7ea61/26\u53c2\u6570\u91cf\u4fbf\u53ef\u8fbe\u5230\u548c26B\u6807\u51c6\u6a21\u578b\u5728MeViS\u4e0a\u7684\u540c\u7b49\u8868\u73b0\u3002", "conclusion": "\u7ec6\u8282\u95ee\u9898\u53ef\u80fd\u6781\u5927\u5f71\u54cd\u5206\u5272\u6a21\u578b\u7684\u5b9e\u9645\u8868\u73b0\u3002\u6b64\u6b21\u4fee\u6b63\u63d0\u5347\u663e\u8457\uff0c\u4e3a\u540e\u7eed\u89c6\u9891\u6307\u4ee3\u5206\u5272\u9886\u57df\u6a21\u578b\u8bbe\u8ba1\u548c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u501f\u9274\u3002"}}
{"id": "2509.19087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19087", "abs": "https://arxiv.org/abs/2509.19087", "authors": ["Ganesh Mallya", "Yotam Gigi", "Dahun Kim", "Maxim Neumann", "Genady Beryozkin", "Tomer Shekel", "Anelia Angelova"], "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications", "comment": null, "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f7f\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982Gemini2.5\uff09\u80fd\u591f\u76f4\u63a5\u5728Zero-Shot\u60c5\u5883\u4e0b\u5904\u7406\u591a\u5149\u8c31\u9065\u611f\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u571f\u5730\u8986\u76d6\u548c\u5229\u7528\u5206\u7c7b\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u591a\u5149\u8c31\u9065\u611f\u5f71\u50cf\u4e3b\u8981\u4f9d\u8d56\u4e13\u95e8\u5b9a\u5236\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8bad\u7ec3\u4e0e\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\uff0c\u800c\u901a\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982Gemini2.5\uff09\u867d\u80fd\u529b\u5f3a\u5927\uff0c\u5374\u65e0\u6cd5\u7406\u89e3\u4e13\u4e1a\u7684\u591a\u5149\u8c31\u4fe1\u606f\uff0c\u5236\u7ea6\u4e86\u5176\u5728\u9065\u611f\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u591a\u5149\u8c31\u6570\u636e\u4ee5Zero-Shot\u6a21\u5f0f\u4f5c\u4e3a\u8f93\u5165\uff0c\u5229\u7528\u6307\u4ee4\u5c06\u4e13\u4e1a\u9886\u57df\u7684\u4fe1\u606f\u7f16\u7801\u6ce8\u5165\u5927\u6a21\u578b\uff0c\u5728\u65e0\u9700\u9488\u5bf9\u591a\u5149\u8c31\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u8ba9\u4ec5\u63a5\u53d7RGB\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u5904\u7406\u4e13\u4e1a\u9065\u611f\u4efb\u52a1\u3002\u4ee5Gemini2.5\u4e3a\u4f8b\uff0c\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Gemini2.5\u5728\u571f\u5730\u8986\u76d6\u4e0e\u5229\u7528\u5206\u7c7b\u7b49\u4e3b\u6d41\u9065\u611f\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u517c\u5bb9\u591a\u5149\u8c31\u8f93\u5165\u7684\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u672c\u65b9\u6cd5\uff0c\u5730\u7406\u7a7a\u95f4\u9886\u57df\u7684\u4e13\u4e1a\u4eba\u58eb\u53ef\u4ee5\u66f4\u65b9\u4fbf\u5730\u5229\u7528\u901a\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982Gemini2.5\uff09\u6765\u5904\u7406\u975e\u6807\u51c6\u3001\u4e13\u4e1a\u7684\u591a\u5149\u8c31\u8f93\u5165\uff0c\u63d0\u5347\u5de5\u4f5c\u6548\u7387\u5e76\u6316\u6398\u66f4\u591a\u9065\u611f\u4fe1\u606f\u4ef7\u503c\u3002"}}
{"id": "2509.19096", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19096", "abs": "https://arxiv.org/abs/2509.19096", "authors": ["Ilhan Skender", "Kailin Tong", "Selim Solmaz", "Daniel Watzenig"], "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models", "comment": "Accepted for presentation at the 2025 IEEE International Automated\n  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE\n  Xplore", "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u65e0\u9700\u5148\u524d\u6807\u6ce8\u6570\u636e\u4e0b\uff0c\u5229\u7528\u57fa\u7840\u8bbe\u65bd\u76f8\u673a\u56fe\u50cf\u5b9e\u73b0\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u4e0e\u63cf\u8ff0\u7684\u80fd\u529b\u3002\u5f15\u5165YOLO\u3001Deep SORT\u548cSegment Anything\u4e09\u5927\u89c6\u89c9\u5206\u6790\u7ec4\u4ef6\uff0c\u52a0\u5f3a\u6a21\u578b\u8868\u73b0\u4e0e\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8bc1\u660ePixtral\u5728F1\u548c\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cGemini\u7cfb\u5217\u7cbe\u51c6\u5ea6\u9ad8\u4f46\u5e73\u8861\u6027\u8f83\u5dee\uff0cGemma 3\u8868\u73b0\u6700\u5747\u8861\u3002", "motivation": "\u7531\u4e8e\u5b9e\u65f6\u51c6\u786e\u7684\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u5bf9\u5feb\u901f\u5e94\u6025\u548c\u98ce\u9669\u964d\u4f4e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u5927\u89c4\u6a21\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u96be\u5ea6\u5927\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684MLLMs\uff0c\u7ed3\u5408\u89c6\u89c9\u5206\u6790\u65b9\u6cd5\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u4e8b\u6545\u68c0\u6d4b\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f5c\u8005\u5229\u7528CARLA\u4eff\u771f\u5f97\u5230\u7684DeepAccident\u6570\u636e\u96c6\uff0c\u5206\u522b\u6d4b\u8bd5\u4e86Gemini 1.5/2.0\u3001Gemma 3\u3001Pixtral\u7b49MLLMs\u7684\u96f6\u6837\u672c\u4e8b\u6545\u68c0\u6d4b\u6027\u80fd\u3002\u4e3a\u63d0\u5347\u8868\u73b0\uff0c\u8fd8\u6574\u5408\u4e86YOLO\uff08\u76ee\u6807\u68c0\u6d4b\uff09\u3001Deep SORT\uff08\u591a\u76ee\u6807\u8ddf\u8e2a\uff09\u3001SAM\uff08\u5b9e\u4f8b\u5206\u5272\uff09\u7684\u89c6\u89c9\u5206\u6790\u7ed3\u679c\u8fdb\u6a21\u578b\u63d0\u793a\u8bcd\uff0c\u5f3a\u5316\u6a21\u578b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u4e0d\u540cMLLMs\u4e0a\uff0cPixtral\u53d6\u5f97F1\u5206\u65700.71\u3001\u53ec\u56de\u738783%\u7684\u6700\u4f73\u7efc\u5408\u8868\u73b0\u3002Gemini\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u7cbe\u51c6\u7387\u8fbe90%\uff0c\u4f46F1\u4e0e\u53ec\u56de\u4e0b\u964d\u660e\u663e\u3002Gemma 3\u5728\u5404\u9879\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4e3a\u5747\u8861\uff0c\u6ce2\u52a8\u8f83\u5c0f\u3002", "conclusion": "\u5c06MLLMs\u4e0e\u5148\u8fdb\u89c6\u89c9\u5206\u6790\u6280\u672f\u8054\u5408\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u4e8b\u6545\u81ea\u52a8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u667a\u6167\u4ea4\u901a\u9886\u57df\u81ea\u52a8\u76d1\u6d4b\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u529b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.19115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19115", "abs": "https://arxiv.org/abs/2509.19115", "authors": ["G\u00f6rkay Aydemir", "Weidi Xie", "Fatma G\u00fcney"], "title": "Track-On2: Enhancing Online Point Tracking with Memory", "comment": null, "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faTrack-On2\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u6548\u5728\u7ebf\u70b9\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f18\u6548\u679c\uff0c\u53ef\u5b9e\u65f6\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u3002", "motivation": "\u957f\u671f\u70b9\u8ffd\u8e2a\u5728\u6781\u7aef\u5916\u89c2\u53d8\u5316\u3001\u8fd0\u52a8\u548c\u906e\u6321\u4e0b\u4ecd\u9700\u4fdd\u8bc1\u7a33\u5b9a\u8bc6\u522b\uff0c\u4e14\u9700\u6ee1\u8db3\u5728\u7ebf\u3001\u5b9e\u65f6\u573a\u666f\u7684\u9700\u6c42\u3002\u5f53\u524d\u65b9\u6cd5\u6027\u80fd\u6709\u9650\uff0c\u4e14\u5927\u591a\u9700\u79bb\u7ebf\u5904\u7406\u6216\u5168\u5e8f\u5217\u8bbf\u95ee\uff0c\u65e0\u6cd5\u5f88\u597d\u5e94\u7528\u4e8e\u6d41\u5a92\u4f53\u7b49\u5728\u7ebf\u60c5\u5883\u3002Motivation\u5728\u4e8e\u63d0\u5347\u5728\u7ebf\u8ffd\u8e2a\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8eTransformer\u67b6\u6784\uff0cTrack-On2\u56e0\u7ed3\u6784\u7cbe\u7b80\u548c\u5185\u5b58\u673a\u5236\u521b\u65b0\uff0c\u80fd\u591f\u56e0\u679c\uff08\u5355\u5411\uff09\u5904\u7406\u5e8f\u5217\u5e27\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u5386\u53f2\u5e27\u4fe1\u606f\u5b9e\u73b0\u8bb0\u5fc6\u548c\u6f02\u79fb\u8865\u507f\u3002\u65b9\u6cd5\u5305\u542b\u7c97\u7c92\u5ea6\u8865\u4e01\u5206\u7c7b\u53ca\u540e\u7eed\u7cbe\u7ec6\u5316\u3002\u63d0\u51fa\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u65b0\u9896\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u957f\u671f\u65f6\u5e8f\u7684\u9002\u5e94\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u5408\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\uff0cTrack-On2\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u7684\u5b9e\u65f6\u5728\u7ebf\u8ffd\u8e2a\u5668\uff0c\u751a\u81f3\u4f18\u4e8e\u90e8\u5206\u5177\u5907\u53cc\u5411\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u79bb\u7ebf\u65b9\u6848\u3002\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u8bb0\u5fc6\u548c\u56e0\u679c\u63a8\u7406\u7684Transformer\u67b6\u6784\u7ed3\u5408\u7eaf\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u662f\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u957f\u671f\u70b9\u8ffd\u8e2a\u65b9\u6848\uff0c\u5728\u5b9e\u8df5\u4e2d\u5bf9\u5728\u7ebf\u3001\u5b9e\u65f6\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.19129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19129", "abs": "https://arxiv.org/abs/2509.19129", "authors": ["Adam Romlein", "Benjamin X. Hou", "Yuval Boss", "Cynthia L. Christman", "Stacie Koslovsky", "Erin E. Moreland", "Jason Parham", "Anthony Hoogs"], "title": "KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments", "comment": "Accepted to the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025)", "summary": "We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral\nsynchronization and real-time detection of seals and polar bears. Utilized in\naerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort\nseas around Alaska, KAMERA provides up to an 80% reduction in dataset\nprocessing time over previous methods. Our rigorous calibration and hardware\nsynchronization enable using multiple spectra for object detection. All\ncollected data are annotated with metadata so they can be easily referenced\nlater. All imagery and animal detections from a survey are mapped onto a world\nplane for accurate surveyed area estimates and quick assessment of survey\nresults. We hope KAMERA will inspire other mapping and detection efforts in the\nscientific community, with all software, models, and schematics fully\nopen-sourced.", "AI": {"tldr": "KAMERA\u7cfb\u7edf\u96c6\u6210\u591a\u6444\u50cf\u5934\u548c\u591a\u5149\u8c31\u540c\u6b65\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6d77\u8c79\u548c\u5317\u6781\u718a\u7684\u5b9e\u65f6\u68c0\u6d4b\uff0c\u6781\u5927\u63d0\u5347\u4e86\u6781\u5730\u822a\u7a7a\u8c03\u67e5\u6548\u7387\u3002", "motivation": "\u6781\u5730\u5730\u533a\u7684\u51b0\u751f\u52a8\u7269\uff08\u5982\u6d77\u8c79\u548c\u5317\u6781\u718a\uff09\u76d1\u6d4b\u9700\u8981\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u6570\u636e\u91c7\u96c6\u4e0e\u5206\u6790\uff0c\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u79d1\u5b66\u7814\u7a76\u548c\u4fdd\u62a4\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6444\u50cf\u5934\u3001\u591a\u5149\u8c31\u540c\u6b65\u7684\u786c\u4ef6\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e25\u683c\u6821\u51c6\u4e0e\u5927\u91cf\u5143\u6570\u636e\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\uff0c\u6240\u6709\u6570\u636e\u53ef\u6620\u5c04\u81f3\u5730\u7406\u5e73\u9762\u4ee5\u63d0\u5347\u6570\u636e\u5229\u7528\u7387\u3002", "result": "KAMERA\u7cfb\u7edf\u5728\u963f\u62c9\u65af\u52a0\u5730\u533a\u7684\u822a\u7a7a\u8c03\u67e5\u4e2d\uff0c\u6570\u636e\u5904\u7406\u6548\u7387\u6bd4\u65e7\u65b9\u6cd5\u63d0\u5347\u4e8680%\uff0c\u4e14\u52a8\u7269\u68c0\u6d4b\u548c\u6570\u636e\u5b9a\u4f4d\u66f4\u52a0\u51c6\u786e\u3002", "conclusion": "KAMERA\u663e\u8457\u63d0\u5347\u4e86\u6781\u5730\u52a8\u7269\u8c03\u67e5\u7684\u81ea\u52a8\u5316\u548c\u6548\u7387\uff0c\u5168\u90e8\u8f6f\u786c\u4ef6\u548c\u6a21\u578b\u5f00\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u79d1\u5b66\u68c0\u6d4b\u548c\u5730\u56fe\u5236\u4f5c\u5de5\u4f5c\u3002"}}
{"id": "2509.19156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19156", "abs": "https://arxiv.org/abs/2509.19156", "authors": ["Maurf Hassan", "Steven Davy", "Muhammad Zawish", "Owais Bin Zuber", "Nouman Ashraf"], "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit", "comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore", "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeuCODEX\uff0c\u4e00\u79cd\u9488\u5bf9\u7c7b\u8111\uff08\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0cSNN\uff09\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u7684\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u538b\u7f29\u4e0e\u52a8\u6001\u63a8\u7406\u673a\u5236\u663e\u8457\u964d\u4f4e\u6570\u636e\u4f20\u8f93\u3001\u80fd\u8017\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7cbe\u5ea6\u4e0b\u7684\u9ad8\u6548SNN\u63a8\u7406\u3002", "motivation": "\u5c3d\u7ba1SNN\u56e0\u80fd\u6548\u9ad8\u88ab\u8ba4\u4e3a\u9002\u5408\u5728\u8fb9\u7f18\u7aef\u90e8\u7f72\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u80fd\u8017\u5927\u7b49\u95ee\u9898\u3002\u540c\u65f6\uff0c\u73b0\u6709\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u7cfb\u7edf\u53c8\u53d7\u9650\u4e8e\u9ad8\u5ef6\u8fdf\u548c\u7279\u5f81\u4f20\u8f93\u4ee3\u4ef7\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u4f18\u5316\u7a7a\u95f4\u4e0e\u65f6\u95f4\u5197\u4f59\u3001\u964d\u4f4e\u6574\u4f53\u6210\u672c\u7684\u534f\u540c\u63a8\u7406\u67b6\u6784\u3002", "method": "\u63d0\u51faNeuCODEX\u67b6\u6784\uff0c\u4e3b\u8981\u5305\u62ec\uff1a1\uff09\u5f15\u5165\u57fa\u4e8e\u5b66\u4e60\u7684\u8109\u51b2\u9a71\u52a8\u538b\u7f29\u6a21\u5757\uff0c\u964d\u4f4e\u6570\u636e\u4f20\u8f93\u91cf\uff1b2\uff09\u8bbe\u8ba1\u52a8\u6001\u65e9\u9000\u673a\u5236\uff0c\u6839\u636e\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u63d0\u524d\u7ec8\u6b62\u63a8\u7406\uff1b3\uff09\u5728ResNet-18\u548cVGG-16\u4e3b\u5e72\u7f51\u7edc\u53ca\u5b9e\u9645\u8fb9\u7f18-\u4e91\u6d4b\u8bd5\u73af\u5883\u4e0b\u5b9e\u73b0\u539f\u578b\uff0c\u5e76\u5728\u9759\u6001\u56fe\u50cf\u548c\u4e8b\u4ef6\u6d41\u6570\u636e\u96c6\uff08\u5982CIFAR10, Caltech, CIFAR10-DVS, N-Caltech\uff09\u4e0a\u8bc4\u4f30\u3002", "result": "NeuCODEX\u5927\u5e45\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff1a\u6570\u636e\u4f20\u8f93\u964d\u4f4e\u9ad8\u8fbe2048\u500d\uff0c\u8fb9\u7f18\u7aef\u80fd\u8017\u964d\u4f4e90%\u4ee5\u4e0a\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e3\u500d\u4ee5\u5185\uff1b\u4e0e\u6b64\u540c\u65f6\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e2%\u3002", "conclusion": "NeuCODEX\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6027\u80fdSNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u534f\u540c\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u517c\u987e\u4e86\u7cbe\u51c6\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u3002"}}
{"id": "2509.19165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19165", "abs": "https://arxiv.org/abs/2509.19165", "authors": ["Yun Wang", "Junjie Hu", "Junhui Hou", "Chenghao Zhang", "Renwei Yang", "Dapeng Oliver Wu"], "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions", "comment": null, "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5065\u7684\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u6709\u6548\u5e94\u5bf9\u6076\u52a3\u5929\u6c14\uff08\u5982\u591c\u95f4\u3001\u96e8\u3001\u96fe\uff09\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u4e0e\u573a\u666f\u5bf9\u5e94\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u53ca\u76d1\u7763\u4fe1\u53f7\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5408\u6210\u6076\u52a3\u5929\u6c14\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u7279\u5f81\u63d0\u53d6\u5bf9\u566a\u58f0\u548c\u80fd\u89c1\u5ea6\u964d\u4f4e\u654f\u611f\uff0c\u4ee5\u53ca\u5149\u5ea6\u4e00\u81f4\u6027\u5047\u8bbe\u5931\u6548\uff0c\u4e9f\u9700\u63d0\u5347\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u6b64\u7c7b\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\uff081\uff09\u5f15\u5165\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u9c81\u68d2\u5148\u9a8c\uff0c\u6539\u5584CNN\u7279\u5f81\u63d0\u53d6\u5bf9\u6076\u52a3\u5929\u6c14\u7684\u9002\u5e94\u6027\uff1b\uff082\uff09\u8bbe\u8ba1\u573a\u666f\u5bf9\u5e94\u5148\u9a8c\uff0c\u901a\u8fc7\u6784\u5efa\u542b\u6076\u52a3\u5929\u6c14\u9000\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff08\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u4f46\u5916\u89c2\u6076\u52a3\uff09\uff0c\u4ee5\u63d0\u5347\u81ea\u76d1\u7763\u4fe1\u53f7\u7684\u6709\u6548\u6027\uff1b\uff083\uff09\u63d0\u51fa\u5305\u542b\u573a\u666f\u5bf9\u9f50\u5b66\u4e60\u548c\u6076\u52a3\u5929\u6c14\u84b8\u998f\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u6a21\u578b\u80fd\u5bf9\u6e05\u6670\u548c\u6076\u52a3\u56fe\u50cf\u90fd\u80fd\u51c6\u786e\u4f30\u7b97\u89c6\u5dee\u3002", "result": "\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6076\u52a3\u5929\u6c14\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u5148\u9a8c\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u6ce8\u5165\u9c81\u68d2\u5148\u9a8c\u548c\u6784\u5efa\u573a\u666f\u5bf9\u5e94\u76d1\u7763\u4fe1\u53f7\uff0c\u672c\u65b9\u6cd5\u6781\u5927\u589e\u5f3a\u4e86\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u573a\u666f\u667a\u80fd\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2509.19166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19166", "abs": "https://arxiv.org/abs/2509.19166", "authors": ["Siddharth Gupta", "Jitin Singla"], "title": "YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives", "comment": null, "summary": "Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal\nmucosal cell proliferation called polyps in the inner wall of the colon. When\nleft undetected, polyps can become malignant tumors. Colonoscopy is the\nstandard procedure for detecting polyps, as it enables direct visualization and\nremoval of suspicious lesions. Manual detection by colonoscopy can be\ninconsistent and is subject to oversight. Therefore, object detection based on\ndeep learning offers a better solution for a more accurate and real-time\ndiagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based\npolyp detection pipeline, trained using M2IoU loss, versatile data\naugmentations and negative data to replicate real clinical situations. Our\npipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp\ndatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12\nand mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg\ndataset. The significant increase is achieved in mAP$_{50:95}$ score, showing\nthe precision of polyp detection. We show robustness based on polyp size and\nprecise location detection, making it clinically relevant in AI-assisted\ncolorectal screening.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLO\u7684\u606f\u8089\u68c0\u6d4b\u65b9\u6cd5YOLO-LAN\uff0c\u901a\u8fc7\u6539\u826f\u635f\u5931\u51fd\u6570\u3001\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u548c\u5f15\u5165\u8d1f\u6837\u672c\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u8d77\u59cb\u4e8e\u606f\u8089\uff0c\u82e5\u672a\u53ca\u65f6\u53d1\u73b0\uff0c\u53ef\u80fd\u6076\u53d8\u4e3a\u80bf\u7624\u3002\u73b0\u6709\u4eba\u5de5\u80a0\u955c\u68c0\u6d4b\u606f\u8089\u5b58\u5728\u9057\u6f0f\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u81ea\u52a8\u68c0\u6d4b\u5de5\u5177\u8f85\u52a9\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4ee5YOLO\u4e3a\u57fa\u7840\u7684YOLO-LAN\u606f\u8089\u68c0\u6d4b\u6d41\u7a0b\uff0c\u7ed3\u5408M2IoU\u635f\u5931\u51fd\u6570\u3001\u591a\u6837\u5316\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u8d1f\u6837\u672c\uff0c\u6a21\u62df\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728Kvasir-seg\u548cBKAI-IGH NeoPolyp\u6570\u636e\u96c6\u4e0a\uff0cYOLO-LAN\u53d6\u5f97\u4e86mAP$_{50}$\u6700\u9ad80.9619\u3001mAP$_{50:95}$\u6700\u9ad80.8599\u7b49\u4f18\u5f02\u6307\u6807\uff0cmAP$_{50:95}$\u5206\u6570\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u606f\u8089\u5c3a\u5bf8\u548c\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "YOLO-LAN\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u5177\u6709\u826f\u597d\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u80a0\u955c\u68c0\u67e5\u7684\u81ea\u52a8\u5316\u548c\u8bca\u65ad\u8d28\u91cf\u3002"}}
{"id": "2509.19183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19183", "abs": "https://arxiv.org/abs/2509.19183", "authors": ["Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han"], "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC", "comment": null, "summary": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which\ntargets complex semi-supervised video object segmentation. By analysing and\nadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its\nlong-term memory and concept-aware memory, showing that long-term memory\npreserves temporal continuity under occlusion and reappearance, while\nconcept-aware memory supplies semantic priors that suppress distractors;\ntogether, these traits directly benefit several MOSEv2's core challenges. Our\nsolution achieves a JF score of 39.89% on the test set, ranking 1st in the\nMOSEv2 track of the LSVOS Challenge.", "AI": {"tldr": "\u672c\u62a5\u544a\u9488\u5bf9LSVOS Challenge\u4e2d\u7684MOSEv2\u8d5b\u9053\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u534a\u76d1\u7763\u89c6\u9891\u76ee\u6807\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u957f\u671f\u8bb0\u5fc6\u4e0e\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u5e76\u83b7\u5f97\u699c\u9996\u6210\u7ee9\u3002", "motivation": "MOSEv2\u8d5b\u9053\u805a\u7126\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u7684\u89c6\u9891\u76ee\u6807\u5206\u5272\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u906e\u6321\u3001\u76ee\u6807\u91cd\u73b0\u53ca\u5e72\u6270\u7269\u5f71\u54cd\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u80fd\u63d0\u5347\u5206\u5272\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8be5\u6587\u5206\u6790\u5e76\u6539\u8fdb\u4e86SeC\u67b6\u6784\uff08SAM-2\uff09\u7684\u957f\u671f\u8bb0\u5fc6\u53ca\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\u6a21\u5757\u3002\u957f\u671f\u8bb0\u5fc6\u7528\u4e8e\u4fdd\u6301\u76ee\u6807\u65f6\u5e8f\u8fde\u7eed\u6027\uff0c\u5373\u4fbf\u51fa\u73b0\u906e\u6321\u540e\u518d\u73b0\uff0c\u4ecd\u80fd\u51c6\u786e\u8bc6\u522b\uff1b\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\u4e3a\u6a21\u578b\u63d0\u4f9b\u8bed\u4e49\u5148\u9a8c\uff0c\u589e\u5f3a\u6297\u5e72\u6270\u80fd\u529b\u3002\u4e24\u4e2a\u8bb0\u5fc6\u6a21\u5757\u7ed3\u5408\u5e94\u7528\u4e8eMOSEv2\u8d5b\u9053\u4efb\u52a1\u3002", "result": "\u5728LSVOS Challenge\u7684MOSEv2\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u672c\u65b9\u6cd5\u53d6\u5f97\u4e8639.89%\u7684JF\u5206\u6570\uff0c\u5e76\u83b7\u5f97\u8be5\u8d5b\u9053\u7b2c\u4e00\u540d\u3002", "conclusion": "\u7ed3\u5408\u957f\u671f\u8bb0\u5fc6\u4e0e\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u906e\u6321\u3001\u76ee\u6807\u91cd\u73b0\u53ca\u5e72\u6270\u7b49\u590d\u6742\u573a\u666f\uff0c\u63d0\u5347\u89c6\u9891\u76ee\u6807\u5206\u5272\u8868\u73b0\u3002\u6240\u63d0\u65b9\u6cd5\u5728MOSEv2\u8d5b\u9053\u53d6\u5f97\u663e\u8457\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u4e0e\u521b\u65b0\u6027\u3002"}}
{"id": "2509.19191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19191", "abs": "https://arxiv.org/abs/2509.19191", "authors": ["Yueyan Li", "Chenggong Zhao", "Zeyuan Zang", "Caixia Yuan", "Xiaojie Wang"], "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u89c6\u89c9\u5904\u7406\u673a\u5236\uff0c\u5e76\u9488\u5bf9\u5bf9\u8c61\u8bc6\u522b\u548c\u7a7a\u95f4\u611f\u77e5\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u91ca\u548c\u6539\u8fdb\u65b9\u6cd5\u3002\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u63d0\u5347VLM\u6548\u7387\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u65b0\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709VLMs\u7684\u89c6\u89c9\u5904\u7406\u65b9\u5f0f\uff08\u56fe\u50cf\u5e8f\u5217\u5316\uff09\u4e0e\u4eba\u7c7b\u5e76\u884c\u89c6\u89c9\u5904\u7406\u65b9\u5f0f\u663e\u8457\u4e0d\u540c\uff0c\u4e14\u5176\u5185\u90e8\u673a\u5236\u4e0d\u900f\u660e\uff0c\u963b\u788d\u4e86\u6df1\u5165\u7406\u89e3\u4e0e\u521b\u65b0\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4eba\u7c7b\u89c6\u89c9\u7684\u53cc\u901a\u8def\u5047\u8bf4\uff0c\u6df1\u5165\u7406\u89e3\u5e76\u6539\u8fdbVLMs\u7684\u89c6\u89c9\u5904\u7406\u3002", "method": "\u5c06VLM\u89c6\u89c9\u5904\u7406\u5206\u89e3\u4e3a\u5bf9\u8c61\u8bc6\u522b\u4e0e\u7a7a\u95f4\u611f\u77e5\u4e24\u90e8\u5206\uff1a\u5bf9\u8c61\u8bc6\u522b\u901a\u8fc7\u5c06\u56fe\u7247\u8f6c\u4e3a\u6587\u672ctoken map\uff0c\u5206\u6790\u6a21\u578b\u4ece\u6d45\u5c42\u5c5e\u6027\u8bc6\u522b\u5230\u6df1\u5c42\u8bed\u4e49\u6d88\u6b67\u7684\u4e24\u9636\u6bb5\u611f\u77e5\u8fc7\u7a0b\uff1b\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\uff0c\u5219\u7406\u8bba\u63a8\u5bfc\u5e76\u5b9e\u8bc1\u9a8c\u8bc1VLM\u4e2d\u4f4d\u7f6e\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u514d\u6307\u4ee4token\u538b\u7f29\u7b97\u6cd5\uff08\u501f\u52a9\u53ef\u63d2\u62d4\u89c6\u89c9\u89e3\u7801\u5668\u63d0\u5347\u89e3\u7801\u6548\u7387\uff09\u4e0eRoPE\u7f29\u653e\u6280\u672f\uff08\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u63d0\u51fa\u7684\u7406\u8bba\u5206\u6790\u548c\u6539\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002token\u538b\u7f29\u4e0eRoPE\u7f29\u653e\u5747\u5e26\u6765\u6a21\u578b\u6548\u7387\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86VLM\u89c6\u89c9\u5904\u7406\u7684\u5185\u5728\u673a\u5236\uff0c\u4e3a\u540e\u7eedVLM\u67b6\u6784\u8bbe\u8ba1\u4e0e\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u540c\u65f6\u901a\u8fc7\u65b0\u7b97\u6cd5\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.19203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19203", "abs": "https://arxiv.org/abs/2509.19203", "authors": ["Ioanna Ntinou", "Alexandros Xenos", "Yassine Ouali", "Adrian Bulat", "Georgios Tzimiropoulos"], "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions", "comment": "Accepted at EMNLP 2025", "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6587\u672c-\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u7ed3\u6784\u5316\u56fe\u50cf\u63cf\u8ff0\u66ff\u4ee3\u4f20\u7edf\u56fe\u7247\u8f93\u5165\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8fd8\u66f4\u6ce8\u91cd\u9690\u79c1\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff08\u5982CLIP\uff09\u867d\u80fd\u5b66\u4e60\u5224\u522b\u6027\u5f3a\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u4f46\u5b58\u5728\u8bed\u8a00\u7406\u89e3\u6d45\u663e\u3001\u4f9d\u8d56\u5927\u91cf\u7f51\u7edc\u6570\u636e\u3001\u9690\u79c1\u98ce\u9669\u9ad8\u7b49\u95ee\u9898\uff0c\u4e14\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\u52a0\u5267\u4e86\u6a21\u6001\u5dee\u5f02\uff08modality gap\uff09\u3002", "method": "\u521b\u65b0\u6027\u63d0\u51fa\u4e86vision-free\u5355\u7f16\u7801\u5668\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u56fe\u50cf\u63cf\u8ff0\uff0c\u5c06\u539f\u6709\u7684\u6587\u672c-\u56fe\u7247\u68c0\u7d22\u8f6c\u4e3a\u6587\u672c-\u6587\u672c\u68c0\u7d22\uff0c\u65e0\u9700\u539f\u59cb\u56fe\u50cf\u53c2\u4e0e\u3002\u65b0\u6784\u5efasubFlickr\u4e0esubCOCO\u57fa\u51c6\uff0c\u4e13\u6ce8\u77ed\u6587\u672c\u7ec4\u5408\u8868\u8fbe\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u6a21\u6001\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u590d\u6742\u7ec4\u5408\u7406\u89e3\u529b\uff0c\u4ee5\u53ca\u77ed\u957f\u63cf\u8ff0\u7684\u68c0\u7d22\u8868\u73b0\u3002\u6a21\u578b\u53c2\u6570\u91cf\u4f4e\u81f30.3B\u5373\u53ef\uff0c\u5728\u591a\u4e2a\u68c0\u7d22\u548c\u7ec4\u5408\u6027\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u96f6\u6837\u672c\uff08zero-shot\uff09\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5bf9\u4e8e\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\uff0c\u5b8c\u5168\u89c6\u89c9\u81ea\u7531\u7684\u6587\u672c-\u6587\u672c\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u548c\u9690\u79c1\u6210\u672c\uff0c\u8fd8\u5177\u5907\u66f4\u4f18\u6027\u80fd\u4e0e\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u6a21\u6001\u68c0\u7d22\u6280\u672f\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.19207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19207", "abs": "https://arxiv.org/abs/2509.19207", "authors": ["Israfel Salazar", "Desmond Elliott", "Yova Kementchedjhieva"], "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs", "comment": null, "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5bf9\u957f\u800c\u590d\u6742\u63cf\u8ff0\uff08\u957f\u6807\u9898\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u51fa\u7ec4\u5408\u6027\u7406\u89e3\u80fd\u529b\u4e0e\u957f\u6807\u9898\u7406\u89e3\u76f8\u4e92\u4fc3\u8fdb\uff0c\u5e76\u7ed9\u51fa\u63d0\u5347\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1VLM\u5728\u89c6\u89c9-\u6587\u672c\u7ed1\u5b9a\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5bf9\u957f\u4e14\u4fe1\u606f\u5bc6\u96c6\u7684\u63cf\u8ff0\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\u3002\u4f5c\u8005\u8ba4\u4e3a\u63d0\u5347\u6a21\u578b\u7684\u7ec4\u5408\u6027\u63a8\u7406\u80fd\u529b\u80fd\u6709\u6548\u589e\u5f3a\u5bf9\u590d\u6742\u63cf\u8ff0\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u63a2\u7d22\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u5206\u522b\u9488\u5bf9\u7ec4\u5408\u6027\u63a8\u7406\u548c\u957f\u63cf\u8ff0\u7406\u89e3\u80fd\u529b\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8003\u5bdf\u5404\u81ea\u53ca\u8054\u5408\u8bad\u7ec3\u4e0b\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7ec4\u5408\u6027\u8bad\u7ec3\u80fd\u63d0\u5347\u6a21\u578b\u5bf9\u957f\u63cf\u8ff0\u7684\u68c0\u7d22\u548c\u7406\u89e3\u80fd\u529b\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u8fd9\u79cd\u63d0\u5347\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u826f\u597d\u6a21\u578b\u8bbe\u8ba1\uff1b\u4e0d\u826f\u7684\u6570\u636e\u7ed3\u6784\u6216\u90e8\u5206\u51bb\u7ed3\u6a21\u578b\u53c2\u6570\u5e76\u4e0d\u80fd\u5e26\u6765\u6cdb\u5316\u589e\u5f3a\u3002", "conclusion": "\u7ec4\u5408\u6027\u7406\u89e3\u4e0e\u957f\u63cf\u8ff0\u7406\u89e3\u80fd\u529b\u7d27\u5bc6\u76f8\u5173\uff0c\u53ef\u901a\u8fc7\u8054\u5408\u9ad8\u8d28\u91cf\u8bad\u7ec3\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u53ef\u4e3aVLM\u6cdb\u5316\u548c\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u65b9\u6cd5\u6307\u5bfc\u3002"}}
{"id": "2509.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19208", "abs": "https://arxiv.org/abs/2509.19208", "authors": ["Earl Ranario", "Ismael Mayanja", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data", "comment": null, "summary": "Accurate plant segmentation in thermal imagery remains a significant\nchallenge for high throughput field phenotyping, particularly in outdoor\nenvironments where low contrast between plants and weeds and frequent\nocclusions hinder performance. To address this, we present a framework that\nleverages synthetic RGB imagery, a limited set of real annotations, and\nGAN-based cross-modality alignment to enhance semantic segmentation in thermal\nimages. We trained models on 1,128 synthetic images containing complex mixtures\nof crop and weed plants in order to generate image segmentation masks for crop\nand weed plants. We additionally evaluated the benefit of integrating as few as\nfive real, manually segmented field images within the training process using\nvarious sampling strategies. When combining all the synthetic images with a few\nlabeled real images, we observed a maximum relative improvement of 22% for the\nweed class and 17% for the plant class compared to the full real-data baseline.\nCross-modal alignment was enabled by translating RGB to thermal using\nCycleGAN-turbo, allowing robust template matching without calibration. Results\ndemonstrated that combining synthetic data with limited manual annotations and\ncross-domain translation via generative models can significantly boost\nsegmentation performance in complex field environments for multi-model imagery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u6210RGB\u56fe\u50cf\u3001\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u548c\u57fa\u4e8eGAN\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u70ed\u6210\u50cf\u690d\u7269\u8bed\u4e49\u5206\u5272\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6742\u8349\u548c\u4f5c\u7269\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5728\u6237\u5916\u73af\u5883\u4e0b\uff0c\u9ad8\u901a\u91cf\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u7684\u70ed\u6210\u50cf\u5206\u5272\u9762\u4e34\u7740\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u906e\u6321\u9891\u7e41\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u5206\u5272\uff0c\u6025\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u70ed\u6210\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u5408\u6210RGB\u56fe\u50cf\u548c\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\uff0c\u901a\u8fc7CycleGAN-turbo\u5b9e\u73b0RGB\u4e0e\u70ed\u6210\u50cf\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u5e76\u5c1d\u8bd5\u591a\u79cd\u6807\u6ce8\u6837\u672c\u878d\u5408\u7b56\u7565\u3002", "result": "\u5c06\u6240\u6709\u5408\u6210\u56fe\u548c\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u7ed3\u5408\u8bad\u7ec3\uff0c\u76f8\u6bd4\u5b8c\u5168\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\uff0c\u6742\u8349\u7c7b\u7684\u5206\u5272\u6027\u80fd\u63d0\u5347\u4e8622%\uff0c\u4f5c\u7269\u7c7b\u63d0\u5347\u4e8617%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u7ed3\u5408\u5408\u6210\u6570\u636e\u3001\u6709\u9650\u4eba\u5de5\u6807\u6ce8\u548c\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8de8\u57df\u5bf9\u9f50\uff0c\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u91ce\u5916\u73af\u5883\u4e0b\u7684\u70ed\u6210\u50cf\u690d\u7269\u5206\u5272\u8868\u73b0\u3002"}}
{"id": "2509.19218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19218", "abs": "https://arxiv.org/abs/2509.19218", "authors": ["Yunzhi Xu", "Yushuang Ding", "Hu Sun", "Hongxi Zhang", "Li Zhao"], "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus", "comment": "10 pages, 7 figures", "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HyKid\uff0c\u4e00\u4e2a\u5305\u542b48\u4f8b\u513f\u79d1\u8111\u79ef\u6c34\u60a3\u8005\u7684\u5f00\u6e90\u9ad8\u5206\u8fa8\u7387MRI\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\uff0c\u5305\u62ec\u8109\u7edc\u4e1b\u5206\u5272\uff0c\u65e8\u5728\u4fc3\u8fdb\u8111\u79ef\u6c34\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u513f\u7ae5\u8111\u79ef\u6c34\u7684\u7814\u7a76\u53d7\u9650\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u3001\u4e13\u5bb6\u6807\u6ce8\u4e14\u5305\u542b\u8109\u7edc\u4e1b\u5206\u5272\u7684\u6570\u636e\u96c6\uff0c\u5f71\u54cd\u4e86\u76f8\u5173\u5f71\u50cf\u7b97\u6cd5\u548c\u751f\u7269\u6807\u5fd7\u7269\u5f00\u53d1\u3002", "method": "\u4f5c\u8005\u91c7\u96c648\u540d\u8111\u79ef\u6c34\u60a3\u513f\u7684\u4f8b\u884c\u4f4e\u5206\u8fa8\u7387MRI\u56fe\u50cf\uff0c\u901a\u8fc7slice-to-volume\u7b97\u6cd5\u91cd\u5efa\u4e3a1mm\u7b49\u4f53\u7d203D\u9ad8\u5206\u8fa8\u7387MRI\uff0c\u518d\u7531\u795e\u7ecf\u79d1\u4e13\u5bb6\u624b\u52a8\u4fee\u6b63\u5206\u5272\u767d\u8d28\u3001\u7070\u8d28\u3001\u4fa7\u8111\u5ba4\u3001\u5916\u90e8\u8111\u810a\u6db2\u548c\u8109\u7edc\u4e1b\u3002\u540c\u65f6\uff0c\u5e94\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u5206\u6790\u53d1\u73b0\u8109\u7edc\u4e1b\u4f53\u79ef\u4e0e\u603b\u8111\u810a\u6db2\u4f53\u79ef\u9ad8\u5ea6\u76f8\u5173\uff0c\u53ef\u4f5c\u4e3a\u8111\u79ef\u6c34\u8bc4\u4f30\u751f\u7269\u6807\u5fd7\u7269\u3002\u5728\u9884\u6d4b\u6a21\u578b\u4e2d\u8868\u73b0\u7a81\u51fa\uff08AUC=0.87\uff09\u3002", "conclusion": "HyKid\u6570\u636e\u96c6\u4e3a\u513f\u7ae5\u8111\u79ef\u6c34\u795e\u7ecf\u5f71\u50cf\u7b97\u6cd5\u7814\u53d1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u7684\u57fa\u51c6\u6570\u636e\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u8109\u7edc\u4e1b\u5728\u8111\u79ef\u6c34\u8bc4\u4f30\u4e2d\u4f5c\u7528\u7684\u8ba4\u8bc6\uff1b\u6570\u636e\u5df2\u516c\u5f00\u53ef\u4f9b\u79d1\u7814\u4f7f\u7528\u3002"}}
{"id": "2509.19227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19227", "abs": "https://arxiv.org/abs/2509.19227", "authors": ["Tongshuai Wu", "Chao Lu", "Ze Song", "Yunlong Lin", "Sizhe Fan", "Xuemei Chen"], "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation", "comment": null, "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7279\u5f81\u4ea4\u4e92\u7f51\u7edc\uff08MsFIN\uff09\u7528\u4e8e\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u89d2\u4e0b\u7684\u4ea4\u901a\u4e8b\u6545\u65e9\u671f\u9884\u6d4b\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u884c\u8f66\u8bb0\u5f55\u4eea\u4e8b\u6545\u9884\u6d4b\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u4ea4\u901a\u53c2\u4e0e\u8005\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u7279\u5f81\u7ea7\u4ea4\u4e92\uff0c\u4e8c\u662f\u96be\u4ee5\u6355\u6349\u5bfc\u81f4\u4e8b\u6545\u53d1\u751f\u7684\u590d\u6742\u3001\u5f02\u6b65\u3001\u591a\u65f6\u5e8f\u884c\u4e3a\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86MsFIN\u7f51\u7edc\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u3001\u65f6\u5e8f\u7279\u5f81\u5904\u7406\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u540e\u878d\u5408\u4e09\u5927\u6a21\u5757\u3002\u901a\u8fc7\u81ea\u7814\u7684\u591a\u5c3a\u5ea6\u6a21\u5757\u63d0\u53d6\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\uff08\u77ed\u671f\u3001\u4e2d\u671f\u3001\u957f\u671f\uff09\u7684\u573a\u666f\u7279\u5f81\uff0c\u5e76\u5229\u7528Transformer\u5b9e\u73b0\u4e30\u5bcc\u7684\u7279\u5f81\u4ea4\u4e92\u3002\u65f6\u5e8f\u7279\u5f81\u5904\u7406\u6a21\u5757\u5728\u56e0\u679c\u7ea6\u675f\u4e0b\u63d0\u53d6\u573a\u666f\u4e0e\u76ee\u6807\u7684\u65f6\u5e8f\u6f14\u5316\u7279\u5f81\uff0c\u878d\u5408\u9636\u6bb5\u5219\u6574\u5408\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u573a\u666f\u548c\u76ee\u6807\u7279\u5f81\uff0c\u5b9e\u73b0\u5168\u5c40\u98ce\u9669\u8868\u5f81\u3002", "result": "\u5728DAD\u548cDADA\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMsFIN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u63d0\u524d\u91cf\u4e0a\u5747\u5927\u5e45\u8d85\u8fc7\u73b0\u6709\u5355\u5c3a\u5ea6\u7279\u5f81\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5404\u6a21\u5757\u5728\u63d0\u5347\u6574\u4f53\u6027\u80fd\u65b9\u9762\u7684\u8d21\u732e\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4e0e\u4e0a\u4e0b\u6587\u4ea4\u4e92\u80fd\u663e\u8457\u63d0\u5347\u4ea4\u901a\u4e8b\u6545\u65e9\u671f\u9884\u6d4b\u6a21\u578b\u7684\u7efc\u5408\u6027\u80fd\uff0cMsFIN\u4e3a\u57fa\u4e8e\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u89d2\u7684\u4e3b\u52a8\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e94\u5bf9\u4e0d\u65ad\u6f14\u5316\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u6709\u6548\u9002\u5e94\u65b0\u578b\u4f2a\u9020\u65b9\u5f0f\u5e76\u9632\u6b62\u5df2\u5b66\u77e5\u8bc6\u9057\u5fd8\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u771f\u5b9e\u611f\u6570\u5b57\u4eba\u8138\u751f\u6210\u4e0e\u64cd\u63a7\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f2a\u9020\u624b\u6bb5\u591a\u6837\u4e14\u8fc5\u901f\u53d8\u5316\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u7ecf\u5e38\u88ab\u65b0\u578b\u4f2a\u9020\u65b9\u5f0f\u7a81\u7834\u3002\u6536\u96c6\u5168\u90e8\u4f2a\u9020\u6837\u672c\u5e76\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u4e0d\u65ad\u9002\u5e94\u65b0\u578b\u4f2a\u9020\u4e14\u4e0d\u4f1a\u9057\u5fd8\u539f\u6709\u77e5\u8bc6\u3002", "method": "\u4f5c\u8005\u5c06\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u5efa\u6a21\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u201cDevelopmental Mixture of Experts (MoE)\u201d\u67b6\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u662f\u4e00\u4e2aLoRA\u6a21\u578b\u3002\u4e13\u5bb6\u5206\u4e3aReal-LoRA\uff08\u5b66\u4e60\u771f\u5b9e\u4eba\u8138\u7279\u5f81\uff09\u548c\u591a\u4e2aFake-LoRA\uff08\u5b66\u4e60\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\uff09\u3002\u901a\u8fc7\u7ea6\u675fFake-LoRA\u7684\u5b66\u4e60\u65b9\u5411\u4e0e\u5df2\u77e5\u5b50\u7a7a\u95f4\u6b63\u4ea4\uff0c\u5e76\u5c06\u6b63\u4ea4\u68af\u5ea6\u878d\u5165\u6b63\u4ea4\u635f\u5931\uff0c\u6709\u6548\u907f\u514d\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u5e72\u6270\u548c\u9057\u5fd8\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u4e0d\u65ad\u589e\u52a0\u7684\u4f2a\u9020\u7c7b\u578b\u534f\u8bae\u4e0b\u8fdb\u884c\u8bc4\u6d4b\uff0c\u7ed3\u679c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u65b0\u7684\u4f2a\u9020\u7c7b\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u4f2a\u9020\u77e5\u8bc6\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u548c\u6297\u9057\u5fd8\u80fd\u529b\u3002", "conclusion": "\u672c\u65b9\u6cd5\u80fd\u9002\u5e94\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u6f14\u5316\uff0c\u5728\u6709\u9650\u6837\u672c\u548c\u8ba1\u7b97\u6761\u4ef6\u4e0b\u6301\u7eed\u63d0\u5347\u4f2a\u9020\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u6709\u6548\u9632\u6b62\u9057\u5fd8\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4e0d\u65ad\u6f14\u5316\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u573a\u666f\u3002"}}
{"id": "2509.19244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19244", "abs": "https://arxiv.org/abs/2509.19244", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation", "comment": "32 pages, 15 figures", "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.", "AI": {"tldr": "Lavida-O\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001Masked Diffusion Model\uff0c\u517c\u5177\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u591a\u9879\u80fd\u529b\uff0c\u5305\u62ec\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u3001\u56fe\u50cf\u7f16\u8f91\u7b49\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4e0a\u53d7\u9650\uff08\u5982\u4ec5\u80fd\u505a\u7b80\u5355\u7684\u7406\u89e3\u4efb\u52a1\u548c\u4f4e\u5206\u8fa8\u7387\u751f\u6210\uff09\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7edf\u4e00\u7684\u6a21\u578b\u6765\u63d0\u5347\u7406\u89e3\u3001\u751f\u6210\u4ee5\u53ca\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Lavida-O\u6a21\u578b\uff0c\u91c7\u7528Elastic Mixture-of-Transformer\u67b6\u6784\u3001\u901a\u7528\u6587\u672c\u8c03\u63a7\u548c\u5206\u5c42\u91c7\u6837\u7b49\u521b\u65b0\u6280\u672f\uff0c\u5b9e\u73b0\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7edf\u4e00\uff0c\u80fd\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\uff081024px\uff09\u5408\u6210\u3002\u540c\u65f6\uff0c\u6a21\u578b\u53ef\u901a\u8fc7\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u8fed\u4ee3\uff0c\u63d0\u5347\u751f\u6210\u4e0e\u7f16\u8f91\u8d28\u91cf\u3002", "result": "Lavida-O\u5728RefCOCO\u7269\u4f53\u5b9a\u4f4d\u3001GenEval\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001ImgEdit\u56fe\u50cf\u7f16\u8f91\u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u4f18\u4e8eQwen2.5-VL\u548cFluxKontext-dev\u7b49\u4e3b\u6d41\u81ea\u56de\u5f52\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "Lavida-O\u6269\u5c55\u4e86\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u9996\u6b21\u5c06\u7406\u89e3\u80fd\u529b\u7528\u4e8e\u63d0\u5347\u56fe\u50cf\u751f\u6210/\u7f16\u8f91\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5237\u65b0\u4e86\u6700\u65b0\u6c34\u5e73\uff0c\u5177\u5907\u66f4\u5f3a\u5b9e\u7528\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.19245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19245", "abs": "https://arxiv.org/abs/2509.19245", "authors": ["Benedetta Liberatori", "Alessandro Conti", "Lorenzo Vaquero", "Yiming Wang", "Elisa Ricci", "Paolo Rota"], "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts", "comment": "Accepted to NeurIPS 2025", "summary": "What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u6982\u5ff5\u7684\u89c6\u9891\u76f8\u4f3c\u6027\u4f30\u8ba1\u4efb\u52a1\uff08ConViS\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5e94\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08ConViS-Bench\uff09\uff0c\u8bc4\u6d4b\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5177\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u7684\u89c6\u9891\u76f8\u4f3c\u6027\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u76f8\u4f3c\u6027\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u5168\u5c40\u76f8\u4f3c\u6027\u5206\u6570\uff0c\u96be\u4ee5\u7ec6\u81f4\u533a\u5206\u4e0d\u540c\u8bed\u4e49\u5c42\u9762\uff08\u5982\u52a8\u4f5c\u3001\u573a\u666f\u7b49\uff09\u7684\u76f8\u4f3c\u6027\uff0c\u4e0e\u4eba\u7c7b\u590d\u6742\u7684\u6bd4\u8f83\u65b9\u5f0f\u5b58\u5728\u5dee\u8ddd\u3002\u7f3a\u4e4f\u53ef\u89e3\u91ca\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u624b\u6bb5\u9650\u5236\u4e86\u6a21\u578b\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faConViS\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5728\u9884\u8bbe\u7684\u591a\u4e2a\u8bed\u4e49\u6982\u5ff5\u7ef4\u5ea6\u4e0a\uff0c\u5bf9\u89c6\u9891\u5bf9\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u76f8\u4f3c\u6027\u6253\u5206\uff0c\u4ece\u800c\u6a21\u62df\u4eba\u7c7b\u7684\u591a\u7ef4\u5ea6\u6bd4\u8f83\u3002\u4e3a\u652f\u6301\u8be5\u4efb\u52a1\uff0c\u6784\u5efa\u4e86ConViS-Bench\u57fa\u51c6\u96c6\uff0c\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u7684\u591a\u9886\u57df\u89c6\u9891\u5bf9\uff0c\u6bcf\u5bf9\u89c6\u9891\u914d\u6709\u6982\u5ff5\u5c42\u9762\u7684\u76f8\u4f3c\u6027\u5206\u6570\u53ca\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u4f5c\u8005\u5728ConViS-Bench\u4e0a\u8bc4\u6d4b\u4e86\u591a\u4e2a\u4e3b\u6d41\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u90e8\u5206\u8bed\u4e49\u6982\u5ff5\u5bf9\u4e8e\u5927\u6a21\u578b\u6765\u8bf4\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "ConViS\u53ca\u5176\u6570\u636e\u96c6\u80fd\u591f\u66f4\u597d\u6a21\u62df\u4eba\u7c7b\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63a8\u7406\u4e0e\u89c6\u9891\u76f8\u4f3c\u6027\u6bd4\u8f83\uff0c\u52a9\u529b\u76f8\u5173\u9886\u57df\u53d1\u5c55\uff0c\u4fc3\u8fdb\u8bed\u8a00\u9a71\u52a8\u7684\u89c6\u9891\u7406\u89e3\u7814\u7a76\u3002"}}
{"id": "2509.19252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19252", "abs": "https://arxiv.org/abs/2509.19252", "authors": ["Gabriel Maldonado", "Narges Rashvand", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps", "comment": null, "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5bf9\u6297\u6027\u7cbe\u70bcVQ-GAN\u6846\u67b6\uff0c\u5bf9\u4eba\u7c7b\u52a8\u4f5c\u7684\u65f6\u7a7a\u7279\u5f81\u70ed\u56fe\u8fdb\u884c\u9ad8\u6548\u538b\u7f29\u4e0e\u8868\u8fbe\uff0c\u5728\u4fdd\u6301\u8fd0\u52a8\u7ec6\u8282\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u538b\u7f29\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4eba\u7c7b\u8fde\u7eed\u52a8\u4f5c\u5177\u6709\u9ad8\u7ef4\u548c\u5197\u4f59\u6027\uff0c\u7406\u89e3\u548c\u5206\u6790\u4f9d\u8d56\u4e8e\u9ad8\u6548\u7684\u538b\u7f29\u4e0e\u8868\u8fbe\u65b9\u5f0f\uff1b\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u538b\u7f29\u7387\u548c\u52a8\u4f5c\u7ec6\u8282\u4fdd\u7559\uff0c\u5e38\u51fa\u73b0\u91cd\u5efa\u4f2a\u5f71\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u5bc6\u96c6\u52a8\u4f5c\u5206\u8bcd\uff08dense motion tokenization\uff09\u7ed3\u5408\u5bf9\u6297\u6027\u7cbe\u70bc\u7684VQ-GAN\u6a21\u578b\uff0c\u5bf9\u65f6\u7a7a\u70ed\u56fe\u8fdb\u884c\u9ad8\u6548\u5206\u8bcd\u548c\u538b\u7f29\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u6d88\u9664\u91cd\u5efa\u4f2a\u5f71\u4e0e\u65f6\u95f4\u9519\u4f4d\u3002", "result": "\u5728CMU Panoptic\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41dVAE\u57fa\u7ebf\uff0cSSIM\u63d0\u53479.31%\uff0c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u964d\u4f4e37.1%\uff1b\u9a8c\u8bc1\u4e862D\u52a8\u4f5c\u538b\u7f29\u53ef\u7528128-token\u8bcd\u8868\uff0c3D\u52a8\u4f5c\u5219\u97001024-token\u91cd\u5efa\u3002", "conclusion": "\u672c\u65b9\u6cd5\u5728\u52a8\u4f5c\u538b\u7f29\u4e0e\u91cd\u5efa\u4e2d\u6548\u679c\u663e\u8457\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u5206\u6790\u573a\u666f\uff0c\u5e76\u5f00\u653e\u4ee3\u7801\u5e93\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2509.19258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19258", "abs": "https://arxiv.org/abs/2509.19258", "authors": ["Dheerendranath Battalapalli", "Apoorva Safai", "Maria Jaramillo", "Hyemin Um", "Gustavo Adalfo Pineda Ortiz", "Ulas Bagci", "Manmeet Singh Ahluwalia", "Marwa Ismail", "Pallavi Tiwari"], "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies", "comment": "Under Review: npj Digital Medicine", "summary": "A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u653e\u5c04\u7ec4\u5b66\uff08Graph-Radiomic Learning, GrRAiL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u533a\u5206\u5e38\u89c4\u5f71\u50cf\u4e2d\u6076\u6027\u80bf\u7624\u4e0e\u6df7\u6dc6\u75c5\u53d8\uff0c\u5e76\u5728\u591a\u4e2d\u5fc3\u4e34\u5e8a\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5e38\u89c4\u5f71\u50cf\u96be\u4ee5\u51c6\u786e\u533a\u5206\u6076\u6027\u80bf\u7624\u4e0e\u76f8\u4f3c\u7684\u5176\u4ed6\u75c5\u7406\u6539\u53d8\u3002\u73b0\u6709\u653e\u5c04\u7ec4\u5b66\u7b97\u6cd5\u591a\u805a\u5408\u533a\u57df\u6027\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u75c5\u53d8\u5185\u90e8\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\u3002\u8fd9\u5bfc\u81f4\u5728\u75be\u75c5\u9274\u522b\u8bca\u65ad\u548c\u98ce\u9669\u5206\u5c42\u4e0a\u7684\u51c6\u786e\u7387\u53d7\u9650\u3002", "method": "GrRAiL\u65b9\u6cd5\u9996\u5148\u57fa\u4e8e\u6bcf\u4e2a\u4f53\u7d20\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5bf9\u80bf\u7624\u533a\u57df\u5206\u7c07\uff0c\u7136\u540e\u57fa\u4e8e\u805a\u7c7b\u7ed3\u679c\u5efa\u7acb\u52a0\u6743\u56fe\uff0c\u91cf\u5316\u4e0d\u540c\u4e9a\u533a\u95f4\u7684\u7a7a\u95f4\u8054\u7cfb\u3002\u8be5\u65b9\u6cd5\u5206\u522b\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u3001\u8111\u8f6c\u79fb\u548c\u80f0\u817aIPMN\u9ad8\u4f4e\u98ce\u9669\u5206\u5c42\u4e09\u7c7b\u5b9e\u9645\u4e34\u5e8a\u95ee\u9898\uff08\u5171947\u4f8b\u60a3\u8005\uff09\u4e2d\u8fdb\u884c\u591a\u673a\u6784\u5b9e\u9a8c\uff0c\u5e76\u4e0eGNN\u3001\u4f20\u7edf\u7eb9\u7406\u653e\u5c04\u7ec4\u5b66\u3001\u5f3a\u5ea6-\u56fe\u5206\u6790\u7b49\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GrRAiL\u5728\u4e09\u9879\u4efb\u52a1\u4e2d\u7684\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u5747\u663e\u8457\u9ad8\u4e8e\u6240\u6709\u5bf9\u6bd4\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u590d\u53d1\u9274\u522b\u4e2d\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u8fbe78%\uff0c\u8f83\u57fa\u7ebf\u63d0\u9ad810%\u4ee5\u4e0a\uff1b\u5176\u4ed6\u4efb\u52a1\u4e2d\u63d0\u5347\u5e45\u5ea6\u4e5f\u572810-13%\u4ee5\u4e0a\u3002", "conclusion": "GrRAiL\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u6355\u6349\u548c\u91cf\u5316\u80bf\u7624\u5185\u90e8\u5f02\u8d28\u6027\uff0c\u5e76\u5b9e\u73b0\u66f4\u4f18\u826f\u7684\u4e34\u5e8a\u8bca\u65ad\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u590d\u6742\u5b9e\u4f53\u7624\u7684\u7cbe\u786e\u9274\u522b\u53ca\u8f85\u52a9\u51b3\u7b56\uff0c\u5177\u6709\u8f83\u9ad8\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19259", "abs": "https://arxiv.org/abs/2509.19259", "authors": ["Markos Diomataris", "Berat Mert Albaba", "Giorgio Becherini", "Partha Ghosh", "Omid Taheri", "Michael J. Black"], "title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation", "comment": null, "summary": "The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLOPS\u7cfb\u7edf\uff0c\u4f7f\u865a\u62df\u4eba\u7c7b\u89d2\u8272\u4ec5\u4f9d\u9760\u7b2c\u4e00\u89c6\u89d2\u89c6\u89c9\u8fdb\u884c\u73af\u5883\u611f\u77e5\u548c\u5bfc\u822a\uff0c\u5b9e\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5ffd\u7565\u4e86\u4eba\u7c7b\u611f\u77e5\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\uff0c\u5f80\u5f80\u91c7\u7528\u4e0e\u4eba\u7c7b\u4e0d\u540c\u7684\u611f\u77e5\u65b9\u5f0f\uff0c\u5bfc\u81f4\u865a\u62df\u89d2\u8272\u884c\u4e3a\u7f3a\u4e4f\u4eba\u6027\u5316\u3002\u8be5\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u4eba\u7c7b\u5f0f\u611f\u77e5\u63d0\u5347\u865a\u62df\u5316\u8eab\u7684\u884c\u4e3a\u5408\u7406\u6027\u3002", "method": "\u63d0\u51faCLOPS\u7cfb\u7edf\uff0c\u5148\u7528\u5927\u89c4\u6a21\u52a8\u6355\u6570\u636e\u8bad\u7ec3\u4f4e\u5c42\u6b21\u52a8\u4f5c\u5148\u9a8c\u6a21\u578b\uff0c\u518d\u7528Q-learning\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u7b2c\u4e00\u89c6\u89d2\u89c6\u89c9\u8f93\u5165\u6620\u5c04\u4e3a\u52a8\u4f5c\u63a7\u5236\u6307\u4ee4\uff0c\u4ece\u800c\u9a71\u52a8\u89d2\u8272\u8fd0\u52a8\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u7b2c\u4e00\u89c6\u89d2\u89c6\u89c9\u7684CLOPS\u865a\u62df\u4eba\u5177\u6709\u66f4\u7c7b\u4eba\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u6bd4\u5982\u80fd\u591f\u81ea\u4e3b\u907f\u5f00\u89c6\u89c9\u4e2d\u53ef\u89c1\u7684\u969c\u788d\u7269\u3002", "conclusion": "\u914d\u5907\u4eba\u7c7b\u7c7b\u4f3c\u7684\u611f\u77e5\u624b\u6bb5\uff08\u5982\u7b2c\u4e00\u89c6\u89d2\u89c6\u89c9\uff09\u6709\u52a9\u4e8e\u8bad\u7ec3\u51fa\u66f4\u5177\u4eba\u7c7b\u884c\u4e3a\u7279\u5f81\u7684\u865a\u62df\u5316\u8eab\uff0c\u5bf9\u63d0\u5347\u865a\u62df\u89d2\u8272\u7684\u81ea\u7136\u6027\u548c\u9002\u5e94\u6027\u5177\u6709\u610f\u4e49\u3002"}}
{"id": "2509.19282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19282", "abs": "https://arxiv.org/abs/2509.19282", "authors": ["Bingnan Li", "Chen-Yu Wang", "Haiyang Xu", "Xiang Zhang", "Ethan Armand", "Divyansh Srivastava", "Xiaojun Shan", "Zeyuan Chen", "Jianwen Xie", "Zhuowen Tu"], "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps", "comment": "Accepted to NeurIPS 2025 Dataset&Benchmark Track", "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8861\u91cflayout-to-image\u751f\u6210\u4e2d\u91cd\u53e0\u590d\u6742\u5ea6\u7684\u6307\u6807OverLayScore\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6db5\u76d6\u66f4\u591a\u91cd\u53e0\u573a\u666f\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6OverLayBench\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u63d0\u5347\u590d\u6742\u91cd\u53e0\u751f\u6210\u8d28\u91cf\u7684\u6a21\u578bCreatiLayout-AM\u3002", "motivation": "\u5f53\u524dlayout-to-image\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u5927\u91cf\u6216\u8bed\u4e49\u65e0\u533a\u5206\u91cd\u53e0\u7684\u5e03\u5c40\u65f6\u6548\u679c\u5dee\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u504f\u5411\u7b80\u5355\u573a\u666f\uff0c\u65e0\u6cd5\u7cfb\u7edf\u8861\u91cf\u548c\u4f18\u5316\u7b97\u6cd5\u5728\u590d\u6742\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "method": "1\uff09\u63d0\u51faOverLayScore\u6307\u6807\uff0c\u91cf\u5316\u5e03\u5c40\u4e2dbbox\u91cd\u53e0\u7684\u590d\u6742\u6027\uff1b2\uff09\u6784\u5efaOverLayBench\u6570\u636e\u96c6\uff0c\u4e30\u5bcc\u9ad8\u91cd\u53e0\u573a\u666f\uff0c\u6db5\u76d6\u4e0d\u540cOverLayScore\u5206\u5e03\uff1b3\uff09\u63d0\u51faCreatiLayout-AM\u6a21\u578b\uff0c\u57fa\u4e8e\u7cbe\u7ec6\u5316amodal mask\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u4e13\u6ce8\u63d0\u5347\u590d\u6742\u91cd\u53e0\u60c5\u5f62\u4e0b\u7684\u751f\u6210\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cOverLayScore\u80fd\u6709\u6548\u533a\u5206\u91cd\u53e0\u590d\u6742\u5ea6\uff0c\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u91cd\u53e0\u590d\u6742\u5ea6\u504f\u4f4e\u7684\u95ee\u9898\u3002CreatiLayout-AM\u6a21\u578b\u5728\u65b0\u7684\u9ad8\u590d\u6742\u5ea6\u91cd\u53e0\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5b8c\u5584\u4e86layout-to-image\u751f\u6210\u4e2d\u5bf9\u91cd\u53e0\u590d\u6742\u5ea6\u7684\u8bc4\u6d4b\u6807\u51c6\u548c\u6570\u636e\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u63d0\u5347\u590d\u6742\u91cd\u53e0\u573a\u666f\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.19296", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u987b\u771f\u5b9e\u591a\u89c6\u89d2\u6570\u636e\u5373\u53ef\u751f\u6210\u865a\u62df3D\u73af\u5883\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u6846\u67b6\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9690\u5f0f3D\u77e5\u8bc6\u63d0\u53d6\u5e76\u8f6c\u6362\u4e3a\u663e\u5f0f3D\u8868\u8ff0\uff0c\u5b9e\u73b0\u9ad8\u65483D\u573a\u666f\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5b66\u4e60\u76843D\u91cd\u5efa\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u771f\u5b9e\u7269\u7406\u73af\u5883\u7684\u591a\u89c6\u89d2\u91c7\u96c6\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u5e38\u5e38\u96be\u4ee5\u83b7\u53d6\uff0c\u9650\u5236\u4e86\u865a\u62df\u73af\u5883\u751f\u6210\u6280\u672f\u5728\u8bf8\u5982\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9690\u5f0f3D\u77e5\u8bc6\u8fc1\u79fb\u4e3a3D\u9ad8\u65af\u6591\u70b9\uff083DGS\uff09\u663e\u5f0f\u8868\u5f81\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5728\u4f20\u7edfRGB\u89e3\u7801\u5668\u57fa\u7840\u4e0a\u52a0\u51653DGS\u89e3\u7801\u5668\uff0c\u5229\u7528RGB\u89e3\u7801\u5668\u7ed3\u679c\u5bf9\u5176\u76d1\u7763\uff0c\u4ec5\u9700\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u3002\u6700\u7ec8\u7cfb\u7edf\u652f\u6301\u6839\u636e\u6587\u672c\u63d0\u793a\u6216\u5355\u5f20\u56fe\u7247\u5b9e\u65f6\u751f\u62103D\u573a\u666f\uff0c\u4ee5\u53ca\u5b9e\u73b0\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u52a8\u60013D\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u7a81\u7834\u4e86\u5bf9\u591a\u89c6\u89d2\u771f\u5b9e\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u865a\u62df\u73af\u5883\uff0c\u4e3a\u865a\u62df\u4eff\u771f\u3001\u673a\u5668\u4eba\u7b49\u5e94\u7528\u5960\u5b9a\u4e86\u66f4\u4fbf\u6377\u6709\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2509.19297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19297", "abs": "https://arxiv.org/abs/2509.19297", "authors": ["Weijie Wang", "Yeqing Chen", "Zeyu Zhang", "Hengyu Liu", "Haoxiao Wang", "Zhiyuan Feng", "Wenkang Qin", "Zheng Zhu", "Donny Y. Chen", "Bohan Zhuang"], "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction", "comment": "Project Page: https://lhmd.top/volsplat, Code:\n  https://github.com/ziplab/VolSplat", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VolSplat\uff0c\u4e00\u79cd\u57fa\u4e8e\u4f53\u7d20\u5bf9\u9f50\u9ad8\u65af\u70b9\u7684\u65b0\u578b\u524d\u99883D\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u50cf\u7d20\u5bf9\u9f50\u9ad8\u65af\u65b9\u6cd5\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u524d\u99883D\u9ad8\u65af\u70b9\u6e85\u5c04\uff083DGS\uff09\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u50cf\u7d20\u5bf9\u9f50\u7b56\u7565\uff0c\u5373\u6bcf\u4e2a2D\u50cf\u7d20\u5bf9\u5e94\u4e00\u4e2a3D\u9ad8\u65af\u70b9\uff0c\u4f46\u8fd9\u79cd\u505a\u6cd5\u4f9d\u8d56\u8f93\u5165\u89c6\u89d2\u6570\u76ee\uff0c\u6613\u5bfc\u81f4\u89c6\u89d2\u504f\u5dee\u3001\u5bf9\u906e\u6321\u53ca\u4f4e\u7eb9\u7406\u533a\u57df\u654f\u611f\uff0c\u5bb9\u6613\u51fa\u73b0\u5bf9\u9f50\u8bef\u5dee\uff0c\u5f71\u54cd\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51faVolSplat\uff0c\u4e00\u79cd\u5c06\u9ad8\u65af\u70b9\u76f4\u63a5\u6620\u5c04\u57283D\u4f53\u7d20\u7f51\u683c\u4e0a\u7684\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u662f\u653e\u5f03\u57fa\u4e8e2D\u50cf\u7d20\u7279\u5f81\u5339\u914d\u7684\u9ad8\u65af\u70b9\u751f\u6210\uff0c\u901a\u8fc7\u75313D\u4f53\u7d20\u7f51\u683c\u76f4\u63a5\u9884\u6d4b\u9ad8\u65af\u70b9\uff0c\u4ee5\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u591f\u6839\u636e\u573a\u666f\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u9ad8\u65af\u70b9\u5bc6\u5ea6\u3002", "result": "\u5728RealEstate10K\u548cScanNet\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cVolSplat\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f73\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u9ad8\u65af\u70b9\u4e91\u51e0\u4f55\u4e00\u81f4\u6027\u66f4\u597d\uff0c\u6e32\u67d3\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e14\u65b9\u6cd5\u5177\u5907\u66f4\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "VolSplat\u4e0d\u4ec5\u6709\u6548\u63d0\u5347\u4e863D\u9ad8\u65af\u91cd\u5efa\u7684\u6548\u679c\uff0c\u8fd8\u4e3a\u66f4\u9ad8\u5bc6\u5ea6\u3001\u66f4\u5065\u58ee\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u7ebf\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7814\u7a76\u5e26\u6765\u65b0\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.19300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19300", "abs": "https://arxiv.org/abs/2509.19300", "authors": ["Chen Chen", "Pengsheng Guo", "Liangchen Song", "Jiasen Lu", "Rui Qian", "Xinze Wang", "Tsu-Jui Fu", "Wei Liu", "Yinfei Yang", "Alex Schwing"], "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching", "comment": null, "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.", "AI": {"tldr": "CAR-Flow\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u611f\u77e5\u7684\u91cd\u53c2\u6570\u5316\u7f29\u77ed\u4e86\u5b66\u4e60\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u5982\u6269\u6563\u6a21\u578b\u548c\u6d41\u6a21\u578b\u9700\u8981\u6a21\u578b\u540c\u65f6\u5b66\u4e60\u566a\u58f0\u7684\u8d28\u91cf\u4f20\u9012\u548c\u6761\u4ef6\u6ce8\u5165\uff0c\u6a21\u578b\u8d1f\u62c5\u8f83\u91cd\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u6548\u679c\u53d7\u9650\u3002", "method": "CAR-Flow\u65b9\u6cd5\u63d0\u51fa\u5728\u6e90\u5206\u5e03\u3001\u76ee\u6807\u5206\u5e03\u6216\u4e8c\u8005\u90fd\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u5b66\u4e60\u7684\u6761\u4ef6\u79fb\u4f4d\uff08shift\uff09\uff0c\u901a\u8fc7\u6761\u4ef6\u611f\u77e5\u7684\u91cd\u53c2\u6570\u5316\u6765\u8c03\u6574\u5206\u5e03\u4f4d\u7f6e\uff0c\u4ece\u800c\u51cf\u5c0f\u6a21\u578b\u9700\u8981\u5b66\u4e60\u7684\u6982\u7387\u8def\u5f84\u957f\u5ea6\u3002", "result": "\u5728\u4f4e\u7ef4\u5408\u6210\u6570\u636e\u4e0a\u53ef\u89c6\u5316\u4e86CAR-Flow\u7684\u6548\u679c\u3002\u5728\u9ad8\u7ef4\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6ImageNet-256\u4e0a\uff0c\u5c06CAR-Flow\u5e94\u7528\u4e8eSiT-XL/2\u6a21\u578b\uff0c\u5c06FID\u4ece2.07\u964d\u4f4e\u52301.68\uff0c\u4e14\u989d\u5916\u53c2\u6570\u91cf\u4e0d\u52300.6%\u3002", "conclusion": "CAR-Flow\u80fd\u6709\u6548\u51cf\u8f7b\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u5b66\u4e60\u8d1f\u62c5\uff0c\u4ee4\u6a21\u578b\u8bad\u7ec3\u66f4\u5feb\u3001\u6548\u679c\u66f4\u4f18\uff0c\u53c2\u6570\u589e\u52a0\u6781\u5c11\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
