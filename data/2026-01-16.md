<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.CL](#cs.CL) [Total: 78]
- [cs.RO](#cs.RO) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 本文提出了一个端到端流程，能够生成、优化并评估用于攻击人脸识别系统的对抗性贴纸，结合多种方法提升伪装效果并进行检测分析。


<details>
  <summary>Details</summary>
Motivation: 人脸生物识别系统虽已广泛应用于安全和取证领域，但其易受对抗性攻击，提升了攻击检测和防护需求。因此亟需开发能够生成、优化以及识别对抗性补丁的完整方法链路，既满足攻防对抗、又助力取证分析。

Method: 采用FGSM生成针对身份分类器的对抗噪声，再用扩散模型结合反扩散、Gaussian平滑与亮度自适应进行优化，使补丁更难被察觉。贴纸应用于面部图片后，对识别系统进行测试，并利用ViT-GPT2为对抗图像生成语义描述，有助取证评估。还利用感知哈希和分割方法对贴纸进行检测和分析。

Result: 所提出的对抗贴纸可以有效地逃避现有的人脸识别系统，并保持自然外观。通过感知哈希和分割检测方法，在对抗补丁检测和分析中取得很高的准确率，SSIM指标高达0.95。

Conclusion: 该流程不仅可为安全测试和取证分析提供强有力的对抗样本生成及评估工具，也揭示了现有识别系统在对抗性攻击下的潜在脆弱性，对人脸识别安全具有重要参考价值。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 该论文提出了LCF3D，一种创新性的传感器融合框架，将RGB图像上的2D目标检测与LiDAR点云上的3D目标检测结合，用于自动驾驶场景下的3D目标定位。该方法通过后期融合和级联融合两种策略，有效减少了误检并提升了检测准确率，尤其对行人、自行车等难检测目标有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶领域，准确检测和定位各类3D目标（如行人、骑手、车辆）对于安全至关重要。尽管车辆通常配备摄像头和激光雷达辅助感知，但如何高效融合两种传感器的数据一直是技术难题。LiDAR提供了丰富的空间信息，但存在误检和漏检的问题，单独使用RGB对应的2D检测也有限。因此，作者致力于开发一种新颖且有效的多模态融合方案，提高检测性能并增强模型跨域泛化能力。

Method: LCF3D采用了两大创新融合策略：（1）'后期融合'，即将LiDAR 3D检测结果与RGB 2D检测结果进行匹配，仅保留匹配成功的结果，过滤掉不匹配的LiDAR误检；（2）'级联融合'，针对仅在RGB检测中出现但LiDAR中遗漏的对象，生成新的三维提案以尝试补全漏检。整体框架结合两种传感器各自强项，既降低了误检率，又补足了漏检。

Result: 实验表明，LCF3D框架在KITTI和nuScenes两个主流自动驾驶数据集上，尤其是在检测难度较大类别如行人、自行车等方面，相较于传统的LiDAR单一方案取得了显著提升。此外，该方法对不同组装与域间切换的鲁棒性也表现优秀。

Conclusion: 综合来看，LCF3D作为一种创新型多模态融合方案，有效弥补了单一传感器的缺点，增强了3D目标检测的准确率与泛化能力，特别适用于自动驾驶中对弱目标和跨域场景的检测需求。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 本研究比较了两种主流卷积神经网络（DenseNet121 和 EfficientNet-B0）在自动检测儿童肺炎方面的表现，结果显示 EfficientNet-B0 效果更佳，并通过可解释性方法增强了模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎是全球儿童发病与死亡的重要原因，急需高效准确的辅助诊断工具。深度学习特别是在医学影像分析领域（如胸片判读）显示出较大潜力，因此探索不同网络架构对儿童肺炎自动诊断的性能有重要价值。

Method: 研究使用5863张公开的儿童胸片，通过图像标准化、重采样、数据增强进行预处理，分别以预训练的 ImageNet 权重微调 DenseNet121 与 EfficientNet-B0，采用统一训练策略。评估指标包括准确率、F1 分数、MCC 和召回率，并用 Grad-CAM 与 LIME 等方法对模型预测区域进行可解释性分析。

Result: EfficientNet-B0 在准确率（84.6%）、F1分数（0.8899）、MCC（0.6849）上均优于 DenseNet121（准确率79.7%，F1分数0.8597，MCC 0.5852），两者召回率均高于0.99。可解释性可视化显示，模型关注区域与临床相关肺部区域一致。

Conclusion: EfficientNet-B0 兼具更优的性能与计算效率，是临床部署的有力候选模型。引入可解释性技术有助于提升 AI 助力儿童肺炎诊断的透明度与可信度。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 本文提出了NanoSD系列模型，通过全面优化Stable Diffusion 1.5的模型架构，使其在保证生成性能的同时，极大降低了计算资源消耗，适用于边缘设备的实时视觉生成与恢复。


<details>
  <summary>Details</summary>
Motivation: 当前潜在扩散模型（如Stable Diffusion 1.5）虽有强大生成能力，但完整流程在边缘设备的计算成本过高，现有轻量化变种多关注U-Net压缩或缩短扩散过程，导致泛化能力有限，且难以跨任务应用。

Method: 作者提出通过“网络外科手术（network surgery）”、特征层级生成式蒸馏（feature-wise generative distillation）、结构化架构缩放（structured architectural scaling），协同优化U-Net和VAE编码器-解码器，实现整个管线的联合轻量化。

Result: NanoSD全家族在130M-315M参数量区间，实现移动端NPU上20ms级实时推理。分析显示，仅减小参数量无法充分提升硬件效率，模型架构平衡、特征流和潜空间保持对延迟影响更大。NanoSD作为骨干网络，在超分、去模糊、人脸修复、单目深度估计任务上皆优于现有轻量扩散模型。

Conclusion: NanoSD为通用型的扩散基础模型家族，在边缘设备上实现了高质量、可实用的实时视觉生成与恢复，为扩散模型轻量化部署树立了新标准。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: 本文提出了一种统一的哈希检索框架UniHash，兼顾已见和未见类别，实现平衡且优越的图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现代图像检索系统需要在已见类别（已知类别、精确识别）和未见类别（新类别、泛化能力）上都表现良好。但现有深度哈希方法多局限单一训练范式，难以兼顾两者。

Method: 提出了UniHash双分支哈希框架：一支基于中心（pointwise），一支基于对（pairwise）。引入创新的哈希码学习方式，通过双向知识转移、互学习损失、以及SM-MoH模块实现分支间特征互通和协同优化。

Result: 大量实验（CIFAR-10、MSCOCO、ImageNet）显示，UniHash在已见和未见类别检索场景下均取得了行业领先的效果，理论分析也证实了其有效性。

Conclusion: UniHash兼收pointwise与pairwise范式优点，实现了在现实应用中更实际、更全面的图像检索能力，为后续相关研究提供了新方向。

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 该论文提出用于多模态视频摘要的信息损失指标ViSIL，用以跨模态评估摘要信息覆盖率，并验证其与人类和模型表现显著相关。


<details>
  <summary>Details</summary>
Motivation: 传统的视频摘要评价指标如BLEU、ROUGE难以衡量不同模态（文本与关键帧序列）间的信息覆盖，缺乏对多模态摘要全面性和有效性的量化手段，因而需要新的统一评价标准。

Method: 作者提出ViSIL（Video Summary Information Loss）分数，基于信息理论和视觉-语言模型推断，通过对比视频原始信息与摘要信息，量化摘要未包含的视频内容，实现跨结构多模态摘要间的直接比较。

Result: 实验证明，ViSIL与人类及视觉-语言模型在视频问答任务的表现存在显著统计相关性，并且利用ViSIL优化摘要选取可实现信息损失与处理速度的帕累托最优，比传统文本摘要在不增加计算负担前提下提升VQA准确率7%。

Conclusion: ViSIL为多模态视频摘要提供了有效的信息覆盖评估工具，可助力高效筛选和生成更具信息性的视频摘要，为多模态检索和生成任务带来性能提升。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 作者提出了TuneCLIP框架，通过自监督方式对开源CLIP模型进行后微调，实现了在多个下游任务上的泛化性能提升，且无需大规模从头训练。


<details>
  <summary>Details</summary>
Motivation: 当前提升CLIP多模态模型性能，通常需耗费巨大的算力和数据进行重新训练，而仅利用已有自监督数据提升模型通用能力，尚未被很好地解决，并且常规微调往往导致性能下降。

Method: 提出了TuneCLIP自监督微调框架，包含两阶段：(1)优化统计量预热阶段，缓解冷启动偏差；(2)通过新的对比损失进行微调，降低误判负样本对性能的影响。

Result: TuneCLIP在不同架构和规模的CLIP模型上均带来一致的性能提升。以SigLIP (ViT-B/16)为例，在ImageNet及其OOD基准上提升至多2.5%，在DataComp基准上提升1.2%。

Conclusion: TuneCLIP为开源CLIP模型提供了高效且有效的后训练自适应方式，无需标签或大规模训练，成为通用下游任务适配的新强基线。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: 该论文提出了VibrantSR方法，可用10米分辨率的Sentinel-2卫星影像生成0.5米分辨率的树冠高度模型（CHM），实现了大范围、频繁的森林监测，且精度优于现有卫星方法，接近航拍方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于航拍的树冠高度监测受限于昂贵且时效不高的获取方式，现有的卫星方法精度有限，难以满足大范围森林长期动态监测与碳核算需求。

Method: VibrantSR是一种生成式超分辨率框架，将10米分辨率的Sentinel-2影像转换为0.5米分辨率的CHMs。利用季节性合成影像，结合先进的训练方法，在美国西部22个生态区跨空间验证表现。

Result: VibrantSR在树冠高≥2m时，平均绝对误差为4.39米，优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）等现有卫星方法。虽略逊于航拍法VibrantVS（2.71米），但精度大幅提升。

Conclusion: VibrantSR可实现不依赖航拍的、广域、频繁、高精度森林监测与碳核算，大幅降低监测成本并提高数据时效性，具有广泛应用前景。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 该论文提出了MedVL-SAM2，一种统一的3D医学多模态模型，能够同时进行报告生成、视觉问答（VQA）和多种分割任务，取得了多个任务的最新性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉-语言模型（VLMs）在图像级文本任务（如报告生成、VQA）表现良好，但在3D医学影像中实现更细粒度的视觉定位和体积空间推理仍具有挑战，尤其是想在同一框架下统一这些能力。

Method: 提出了MedVL-SAM2，将图像级推理与像素级感知相结合，并集成基于SAM2的体积分割模块，支持更精确的空间推理。模型分阶段训练：先利用大规模3D CT影像-文本对进行预训练，实现影像特征与放射学语言嵌入对齐；再在3D CT分割数据集上进行联合优化，兼顾语言理解与分割目标。支持通过文本、点或框交互，实现高阶推理与精确定位统一。

Result: MedVL-SAM2在报告生成、VQA和多种3D分割任务上取得了最新的性能。分析表明，该模型能实现可靠的3D视觉定位、可控的交互式分割，和强大的跨模态推理能力。

Conclusion: 论文证明了高层语义推理与精确的3D定位能够在统一的3D医学VLM中共同实现，具有较强的通用性和实用价值。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: TMD是一种新的视频扩散模型蒸馏框架，将多步扩散高效转化为少步高质生成，实现视频模型推理提速，且保证画质。


<details>
  <summary>Details</summary>
Motivation: 当前高清视频扩散和流模型生成质量高，但推断慢，难以实时应用。需要一种蒸馏方法，实现高质量且高效的视频生成。

Method: 提出Transition Matching Distillation（TMD）框架，将扩散模型的多步去噪轨迹配对为少步骤的条件流，模型分主骨干（提取语义表示）和流头（多步内流更新），利用条件流图做分布匹配蒸馏。

Result: 在Wan2.1 1.3B和14B大文本生成视频模型上实验，TMD在生成速度与视觉质量之间实现良好平衡，推理开销相近时超过现有方法，画质和文本契合度更高。

Conclusion: TMD加速了视频扩散模型的推理，实现了实时与高质量的兼得，为视频生成的实际应用提供了新方案。

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

</details>


### [11] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于最优传输（Optimal Transport, OT）的多模态融合方法OT-Drive，用于提升自动驾驶中在非结构化环境下的可通行区域分割鲁棒性。通过融合RGB和表面法线信息，并引入场景锚生成器（SAG）和最优传输融合模块（OT Fusion），实现了强大的分割效果，尤其在分布外（OOD）场景下表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶中，非结构化环境下的可通行区域分割依赖数据驱动方法，但这些方法在遇到训练数据分布外的新场景时性能显著下降，影响实际应用的规划和决策能力。为了解决分布外泛化问题，亟需提升模型对新场景的鲁棒性。

Method: 作者提出将RGB与表面法线两种模态的信息融合，视为分布传输问题。引入SAG将场景信息分解成天气、时间、路类型的联合分布，作为语义锚点；再用OT Fusion模块将模态特征映射到由语义锚定义的流形上，实现对分布外场景的鲁棒分割。

Result: 在ORFD分布外场景上，方法取得95.16%的mIoU，比之前最好方法高6.35%；在跨数据集任务上取得89.79%的mIoU，超过基线13.99%。表明该方法仅用有限训练样本即可实现出色的分布外泛化能力。

Conclusion: OT-Drive方法显著提升了自动驾驶可通行区域分割在分布外场景的泛化能力，同时节省训练数据，增强了实际部署的可行性与高效性。

Abstract: Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [12] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

TL;DR: 本文指出现有视觉-语言模型（VLMs）对于空间关系的捕捉能力不足，提出通过改进编码器结构来提升空间推理能力。实验表明，使用不同训练目标和2D位置编码可以有效提升模型在空间推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的VLM（如CLIP）往往将图像展平成一维patch序列，丢失了图像的二维空间结构，因此表现出空间理解能力的缺陷，这严重影响了模型在实际空间定位需求场景（如机器人、具身智能AI等）的应用价值。

Method: 作者探索了两种改进方法：（1）采用不同于传统对比学习的训练目标来训练图像编码器；（2）在模型中引入二维空间位置信息编码，以保留和利用图像的空间结构。

Result: 实验结果表明，这些结构和训练上的改动能够明显提升VLM在多个空间推理基准上的表现，证明了空间结构对模型理解能力的重要性。

Conclusion: 论文强调了现有VLM空间感知能力的短板，并通过结构创新提升了其空间推理效果，为后续相关应用和模型设计提供了新的思路。

Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [13] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了首个用于事件相机水下光流估计的合成基准数据集，填补了该领域数据匮乏的空白，并提供了基准测试。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临光衰减、强散射、浑浊模糊和不均匀照明等问题，标准相机难以获取真实运动的地面真值。事件相机虽有高时间分辨率和动态范围，但缺少配对的真实水下数据集阻碍了研究进展。

Method: 作者基于物理光线追踪合成RGBD水下视频，用最新的video-to-event流水线生成真实感强的事件数据流及其密集光流、深度与相机运动的真值。同时对主流学习型和模型型光流方法进行了基准评测。

Result: 获得包含密集光流、深度和相机运动真值的合成事件数据集，并用它评估现有主流的水下事件光流算法表现，分析了水下光学特性对事件数据形成和运动估计准确度的影响。

Conclusion: 该数据集为水下事件感知算法的发展与评价提供了新基准，有助于推动该领域的研究，相关代码和数据集已公开发布。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [14] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

TL;DR: 论文提出了DR$^2$Seg框架，通过自激励策略改善了推理分割任务的推理效率与分割精度，并通过自监督和双阶段方法有效抑制了冗余思考。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在推理分割任务中容易因推理链条过长（过度思考）而降低分割精度，缺乏有效抑制冗余推理的机制。

Method: 提出DR$^2$Seg自激励框架，将推理分割分为两个阶段：先生成目标对象的自洽描述，然后用该描述替代复杂原查询进行验证，并引入两种自激励机制促进目标导向的推理并抑制冗余。

Result: 在多种规模的多模态大模型和分割模型上实验，DR$^2$Seg稳定提升了推理效率和分割性能。

Conclusion: DR$^2$Seg可以在无需额外监督下，提升推理效率且增强分割精度，为推理分割任务提供了更有效的解决思路。

Abstract: Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [15] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

TL;DR: 提出了一种新的开放词汇3D场景图生成方法RAG-3DSG，能提高3D场景中物体识别的准确率和速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图生成方法在物体识别准确率和效率方面存在不足，主要由于视角受限、遮挡和冗余表面导致聚合噪声大。需要更高效、准确地从多视角图片中重建场景语义结构，以更好地支持机器人操控和导航任务。

Method: 提出RAG-3DSG方法，通过重拍引导的不确定性估计减少聚合噪声，并利用低不确定性对象支持物体级的检索增强生成。同时，引入动态降采样映射策略，自适应调整跨图像物体聚合的粒度，以提升聚合速度。

Result: 在Replica数据集上实验表明，RAG-3DSG生成的3D场景图节点描述准确率显著优于现有方法，且映射时间减少约三分之二。

Conclusion: RAG-3DSG有效提升了开放词汇3D场景图生成的准确性和效率，为机器人任务提供更优质的结构化场景理解。

Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [16] [DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2601.10001)
*Chengjia Liang,Zhenjiong Wang,Chao Chen,Ruizhi Zhang,Songxi Liang,Hai Xie,Haijun Lei,Zhongwei Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种动态加权双图注意力网络（DW-DGAT）模型，有效融合多指标多结构数据，通过兼顾脑区和样本关系提取特征，并采用类别权重与损失函数缓解类别不平衡问题，实现了对帕金森病和阿尔茨海默病的早期精准诊断。


<details>
  <summary>Details</summary>
Motivation: 多指标、高维、结构多样的数据、神经影像和表型数据的异质性以及类别不平衡，使得现有神经退行性疾病（如帕金森病和阿尔茨海默病）早期诊断面临巨大挑战。

Method: 作者提出了DW-DGAT方法：(1) 融合三种结构形式的多指标数据；(2) 构建基于脑区和样本关系的双图注意力网络提取细粒度与全局特征；(3) 结合类别权重生成机制与两种有效损失函数缓解类别不平衡。

Result: 在PPMI和ADNI数据集上进行的大量实验显示，该方法在神经退行性疾病早期诊断任务中达到了最先进的性能。

Conclusion: DW-DGAT模型能够有效融合多类型神经影像和表型数据，解决高维、异质性和类别不平衡问题，极大提升了帕金森病和阿尔茨海默病的早期诊断效果。

Abstract: Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.

</details>


### [17] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 本文提出了用于评估视频大语言模型（VideoLLMs）事件关系幻觉的新基准VERHallu，并提出了Key-Frame Propagating（KFP）策略以缓解该现象。


<details>
  <summary>Details</summary>
Motivation: 当前VideoLLMs容易产生事件关系层面的幻觉，现有研究大多只关注对象和事件本身的出现，而忽略了事件间因果、时间等关系的幻觉问题。为了解决这一评测空白，作者提出新基准。

Method: 设计了VERHallu基准，涵盖事件关系分类、问答和反事实问答三类任务，并引入反直觉视频和人工标注，覆盖视觉-语言和纯语言偏差。提出Key-Frame Propagating（KFP）方法，在模型中更好分配帧级注意力以理解多事件关系。

Result: 实验表明当前主流VideoLLMs在复杂事件关系推理上表现有限，易依赖先验，忽视子事件。KFP方法能有效减缓事件关系幻觉，同时不影响推理速度。

Conclusion: VERHallu基准系统性揭示了VideoLLMs在事件关系理解上的不足，KFP方案有效提升了对多事件关系的推理能力，为后续研究提供了有益的工具与思路。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [18] [See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection](https://arxiv.org/abs/2601.10707)
*Amir Mallak,Erfan Aasi,Shiva Sreeram,Tsun-Hsuan Wang,Daniela Rus,Alaa Maalouf*

Main category: cs.CV

TL;DR: 本文提出了一种名为随机补丁选择（SPS）的新方法，通过在每帧随机屏蔽一部分特征补丁，显著提升端到端自动驾驶在分布外（OOD）环境中的泛化与鲁棒性，且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法使用基础模型的补丁特征，在分布外场景下有更好表现，但这些特征存在高度冗余，容易导致策略模型过拟合非本质相关性，影响泛化能力。

Method: 通过PCA和跨补丁相似度量化冗余性，发现主成分很少但补丁间高相关。提出SPS方法：每帧随机屏蔽一部分补丁特征，只输入剩余补丁，并保证空间布局。这样，策略模型每次基于随机但完整的视图作决策，从而学习到对特定补丁不敏感的鲁棒特征。

Result: SPS方法在所有OOD场景下均优于现有方法，平均提升6.2%，最高提升20.4%，并提升2.4倍推理速度。消融实验和不同参数组合验证了方法的有效性。

Conclusion: SPS极大增强了自动驾驶模型的泛化能力与效率，不仅提升仿真表现，也能直接迁移到真实车辆，无需额外适应。

Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.

</details>


### [19] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像行人重识别（TIReID）方法DiCo，通过分层解耦的跨模态对齐提升检索性能并解释性。


<details>
  <summary>Details</summary>
Motivation: TIReID任务因图像与文本模态之间存在巨大鸿沟，并且行人间细粒度属性（如服饰颜色、纹理、款式等）区分难度大，致使检索效果受到限制。因此需要创新性方法来有效对齐和解耦多模态表征，实现更精细的人体特征匹配。

Method: 作者提出了Disentangled Concept Representation（DiCo）框架，采用共享的slot-based表示。在这种表示下，每个slot作为跨模态的局部锚点，并进一步细分为多个concept block。该设计可解耦（如颜色、纹理、形状等）互补属性，同时保持图像与文本在局部层面的对应一致性。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid等数据集上的大量实验结果显示，DiCo框架不仅达到了与当前最先进方法相当的检索性能，还通过显式的slot和block层级表示提升了模型的可解释性和细粒度检索效果。

Conclusion: 该工作证明了分层解耦的跨模态对齐机制能有效提升TIReID任务性能，同时提供了更强的可解释性，为后续多模态行人检索方法的发展提供了新思路。

Abstract: Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [20] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了一种将链式帧推理(Chain-of-Frame, CoF)引入文本到图像(T2I)生成的方法，通过逐步视觉细化提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 虽然视频生成模型中的链式帧推理已被证明能实现复杂视觉任务，但其在文本到图像生成领域的应用受限，主要因为缺乏清晰的推理起点和可解释的中间状态。本文旨在通过引入显式的推理步骤，提升T2I生成的可控性和可解释性。

Method: 提出CoF-T2I模型，将CoF推理结构用于T2I生成过程中，每一帧代表一次显式的推理步骤，最终帧输出为生成结果。作者还设计了CoF-Evol-Instruct数据集，涵盖从语义到美学的生成过程。为提升质量与消除运动伪影，对每帧采用独立的编码操作。

Result: CoF-T2I在多个基准上显著优于基础视频模型，在GenEval和Imagine-Bench上分别达到0.86和7.468的分数。

Conclusion: 链式帧推理赋能的视频模型在推动高质量文本到图像生成方面具备巨大潜力。

Abstract: Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [21] [ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology](https://arxiv.org/abs/2601.10073)
*Hyun Do Jung,Jungwon Choi,Hwiyoung Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种用于全切片病理图像的多实例学习方法ReaMIL，通过新增轻量级选择头，提升模型在保留充分证据下的高效推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多实例学习方法虽然性能强，但缺乏对推理证据的可控选择与高效利用，影响模型解释性及诊断效率。

Method: ReaMIL在强大的MIL骨干网络上增加了一个软门控选择头，利用预算充分目标，通过铰链损失和稀疏性约束，仅用有限的关键tile保证分类概率≥τ。

Result: 在多个公开病理数据集上，ReaMIL在不损失AUC的前提下，显著减少了关键tile数量（如NSCLC子集平均仅需8.2块tile即可获得高置信度AUC 0.983），并提供了定量证据-效率评估。

Conclusion: ReaMIL无需额外标注，能无缝集成现有MIL框架，有效提升证据利用率与推理可解释性，并为WSI自动诊断带来更严谨的评价手段。

Abstract: We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.

</details>


### [22] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D高斯点云风格化方法，通过2D绘画中的流场引导，在无网格前提下实现几何结构的艺术化变形。其核心在几何而非表面纹理层面再现后印象派风格，并引入新的主观性评测标准。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移多把几何当作纹理投影的载体，无法真实再现后印象派强调结构表现与简化细节的艺术风格。艺术需要超越表观细节，突出结构本质，从几何本身出发进行表达。

Method: 1. 提取2D绘画中的定向流场信息，并反投影回3D空间，用以引导3D高斯点云的结构变形，无需网格先验。
2. 采用亮度-结构分离策略，将几何变形与颜色优化过程分开，避免在大幅结构抽象时产生伪影。
3. 提出以大模型为评判者（VLM-as-a-Judge）的新评价框架，通过审美判断而非像素级指标评价效果，更贴合艺术主观性。

Result: 该方法能实现更具表现力的3D风格迁移，生成符合后印象派风格、结构变形丰富且伪影较少的3D点云美术作品。VLM评价框架显示主观艺术真实性显著提升。

Conclusion: 论文首次在无网格3D点云风格迁移中实现以结构变形为主导的表现力提升，同时提出的亮度-结构解耦和主观审美评价框架为3D艺术生成树立了新范式。

Abstract: In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [23] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种难度引导采样（DGS）方法，通过考虑下游任务的具体需求，提升数据集蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集蒸馏方法忽略了与下游任务紧密相关的任务特定信息，导致蒸馏目标与下游任务之间存在目标偏差，影响了蒸馏数据集的有效性。

Method: 作者提出在蒸馏数据集的采样过程中引入任务难度信息，即利用已生成的图像池，依据预设的难度分布采样最终蒸馏数据。同时，提出难度感知指导（DAG）机制，用于在数据生成阶段探索难度对数据效果的影响。DGS方法为现有数据蒸馏流程的插件模块，可直接搭配主流方法。

Result: 通过在多种实验设置下的广泛实验，证明了DGS和DAG能够有效提升数据集蒸馏的表现，改善下游任务（如图像分类）的性能。难度信息的引入也展现了对多种下游任务的广泛适用潜力。

Conclusion: 引入难度指导的采样和生成机制能够有效缩小数据蒸馏目标与实际下游任务之间的差距，提升蒸馏数据集的实用性和下游表现，为后续任务定制蒸馏提供了新的思路。

Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [24] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

TL;DR: V-Zero提出了一种无需人工标注数据、利用无标签图像自我提升多模态视觉-语言模型性能的新框架。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型的推理能力依赖大量人工标注数据，获取成本高，限制了模型进一步发展。

Method: 设计了V-Zero后训练框架，引入Questioner和Solver两个角色，通过协同进化对话形式，Questioner生成高质量难题、Solver用自身生成的伪标签学习，两者在Group Relative Policy Optimization（GRPO）机制下迭代优化。

Result: 在Qwen2.5-VL-7B-Instruct上提升视觉数学推理+1.7分、通用视觉性任务+2.6分，在没有人工标注的情况下取得一致性能提升。

Conclusion: V-Zero证实了多模态系统可通过自我改进实现性能提升，为取消人工标注提供可行方案。

Abstract: Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [25] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本论文提出了InfoSculpt框架，从信息论角度出发，通过最小化双重条件互信息目标，有效实现已知与新类别的发现和分类。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法依赖伪标签或分两步聚类，难以有效区分类别信号和实例噪声，缺乏理论支撑，现实应用存在局限。

Method: 作者将GCD问题基于信息瓶颈（IB）原则重新建模，提出InfoSculpt框架。具体方法为：对有标签数据采用类别级条件互信息（CMI）优化，学习已知类别的紧致判别表达；对所有数据采用实例级CMI优化，压缩增强带来的噪声，提取类别无关的稳健特征，实现表征空间的解耦。两者协同作用形成鲁棒、区分性强的潜在表示。

Result: 在8个主流基准数据集上，InfoSculpt表现优异，实验验证了方法的有效性。

Conclusion: InfoSculpt通过信息论方法有效区分类别和实例特征，提升了GCD的表现，为开放世界类别发现提供了理论和实践新方案。

Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [26] [FlowAct-R1: Towards Interactive Humanoid Video Generation](https://arxiv.org/abs/2601.10103)
*Lizhen Wang,Yongming Zhu,Zhipeng Ge,Youwei Zheng,Longhao Zhang,Tianshu Hu,Shiyang Qin,Mingshuang Luo,Jiaxu Zhang,Xin Chen,Yulong Wang,Zerong Zheng,Jianwen Jiang,Chao Liang,Weifeng Chen,Xing Wang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: FlowAct-R1是一种针对实时交互式人形体视频生成的新框架，实现了高保真、低延迟的视频合成，支持流畅全身细粒度控制和长时间连续交互。


<details>
  <summary>Details</summary>
Motivation: 当前的视频合成方法难以兼顾高保真和交互实时性的需求，严重影响人形体虚拟代理在实时场景下的体验。

Method: 提出FlowAct-R1框架，基于MMDiT架构，引入chunkwise diffusion forcing与自监督变体来减缓误差累积和保证视频连贯性，并通过高效蒸馏和系统优化提升运行速率和响应速度。

Result: FlowAct-R1可实现480p分辨率下25fps的实时合成，TTFF仅约1.5秒，并支持长时连续、行为细腻流畅的全身控制，在不同角色样式下表现良好。

Conclusion: FlowAct-R1有效提升了实时交互式人形体视频合成的质量和效率，有助于推动交互视频代理的实际应用发展。

Abstract: Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

</details>


### [27] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

TL;DR: 本文提出了MathDoc，这是首个基于真实高中数学试卷的结构化问题提取基准数据集，并评估了模型在有噪音文档下的表现与拒绝不完整输入的能力。


<details>
  <summary>Details</summary>
Motivation: 当前结构化数学试题自动提取对智能教育意义重大，但现实中的试卷存在大量视觉噪音，导致信息提取极具挑战。以往基准数据集主要关注干净文档或一般布局分析，忽视了数学题目的结构完整性和模型主动拒绝不完整输入的能力。

Method: 作者构建了MathDoc数据集，包括3609条经过人工筛选的真实性问题，其中含有不可识别样本以测试模型拒绝能力，并提出了包括题干准确性、视觉相似度及拒绝能力在内的多维评估框架。同时，对包括Qwen3-VL、Gemini-2.5-Pro等最新多模态大模型进行了实验。

Result: 实验发现，尽管端到端模型在结构化题目抽取上表现优异，但对于无法识别的输入几乎不能给出拒绝，反而自信输出无效结果，表明当前模型在文档退化环境下的可靠性存在明显不足。

Conclusion: MathDoc数据集揭示并弥补了现有基准的空白，为后续模型在现实条件下的鲁棒性与可靠性评估提供了重要平台和方法。

Abstract: The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [28] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的多分支融合框架MULTI-VQGAN，以提升视觉In-Context Learning（VICL）对多样上下文信息的利用能力。通过三路多组合分支，与多任务融合，有效提升泛化与表现。


<details>
  <summary>Details</summary>
Motivation: 现有VICL方法多采用“单最佳提示”策略，或简单地将多个提示融合为一，导致丰富的上下文信息丢失，进而限制了模型的推理能力。作者认为需要更充分且协同的多路上下文信息利用机制。

Method: 作者提出了一种新的多组合协同融合方法，不再仅将多个最佳提示简单叠加，而是生成三个分别由不同高质量提示组合融合而成的上下文分支，引导输入到新设计的MULTI-VQGAN架构中，实现多源协同推理与信息利用。

Result: 在前景分割、单物体检测、图像上色等多种任务上进行了大量实验，结果显示该方法具备很强的跨任务泛化能力、上下文融合效率高，并显著优于现有方法，预测更鲁棒、更准确。

Conclusion: 多组合协同融合和MULTI-VQGAN架构极大提升了VICL模型对复杂上下文的利用，推动了视觉推理能力及多任务表现，具有较高的实用价值和推广潜力。

Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [29] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的端到端VICL框架，通过自适应融合模块和布局特定MLP，有效提升了视觉预训练模型对多样任务的适应性，实验证明在多个任务上有更优泛化和表现。


<details>
  <summary>Details</summary>
Motivation: 现有VICL方法在选择提示时只用最相似的，丢弃了其他高质量提示的补充信息，同时也未利用不同提示排列蕴含的结构化信息。这些局限限制了模型的泛化能力和表现。

Method: 提出了两个关键方法：1) 自适应融合模块从多条提示中聚合关键信息，生成更精准的上下文提示；2) 使用布局特定的轻量级MLP，将预设布局偏好与主模型分离，只对主模型做极小改动。此外，引入了双向微调机制，通过交换query与prompt角色促进融合模块与修复模型之间的协作。

Result: 在前景分割、单目标检测、图像上色等多项任务上，该方法均取得了优于现有方法的结果，并展现出很强的跨任务泛化能力。

Conclusion: 该VICL框架针对性解决了以往方法的信息丢失和结构利用不足的问题，通过多提示融合和结构解耦显著提升了模型适应性和多任务表现。

Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [30] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种用于半监督医学图像分割的新方法VQ-Seg，通过向量量化替代传统dropout来实现特征扰动，从而克服了dropout需要手动调参的问题，并通过一系列新模块有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前特征扰动大多依赖dropout，但其参数难以优化、正则化效果有限，限制了半监督分割性能。需要一种可控且高效的新特征扰动方式。

Method: 提出VQ-Seg方法，在特征空间应用向量量化并构建了可控的量化扰动模块（QPM），利用扰动离散化特征实现正则化。同时通过双分支架构同时支持重建和分割，减少信息损失，并设计后量化特征适配器（PFA），结合大模型语义补充。在大规模肺癌CT数据集及公开数据集上进行实验和验证。

Result: 在自建和公开医学数据集上，VQ-Seg在分割精度等多项指标均优于现有最优方法。

Conclusion: VQ-Seg通过创新性地采用向量量化和一系列配套机制，实现了比传统dropout更优的半监督医学图像分割效果，为相关任务带来了新的特征扰动和正则化策略。

Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [31] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: 该论文提出LaViT框架，通过对齐模型的潜在视觉思维和注意力轨迹，从而增强多模态模型在视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法主要依赖外部监督信号，忽视了模型内部的视觉注意力动态，导致学生模型容易仅模仿教师输出，而非真正理解视觉信息。

Method: 论文提出LaViT，通过让学生模型自回归地重建教师的视觉语义和注意力轨迹，采用课程化感知门控机制，抑制捷径学习，实现视觉思维对齐。

Result: 实验表明，LaViT大幅提升了视觉基础能力，在复杂推理任务上性能提升达16.9%，3B体积模型优于更大规模的开源和GPT-4o等专有模型。

Conclusion: LaViT框架有效弥补了视觉感知差距，推动多模态推理更精确、更高效，为小模型赋能，具有广泛应用前景。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [32] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文引入了视频异常推理（VAR）新任务和大型数据集，并提出了相应的MLLM模型，大幅提升了多模态大模型在视频异常推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视频理解任务上取得显著进展，但针对视频异常检测与理解（VAD&U）的研究局限于异常定位或后验描述，缺乏显式推理、风险感知和决策解释。因此，亟需推动异常分析从描述性向结构化、多阶段推理演进。

Method: 作者提出了视频异常推理（VAR）任务和近9000条视频的新数据集，数据标注依据感知-认知-行动链式思维（PerCoAct-CoT）结构，涵盖多层次推理问题，并设计Anomaly-Aware Group Relative Policy Optimization算法。基于此，研发了Vad-R1-Plus端到端MLLM模型，实现分层推理及风险感知决策。

Result: 所提出的基准和Vad-R1-Plus模型在VAR任务上表现优越，推理能力显著超越现有主流开源及专有多模态大模型。

Conclusion: 引入的VAR任务、数据集和方法有效推动了视频异常推理领域的理论与实际能力，为多模态模型在高阶结构化推理任务中奠定了坚实基础。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [33] [From Physical Degradation Models to Task-Aware All-in-One Image Restoration](https://arxiv.org/abs/2601.10192)
*Hu Gao,Xiaoning Lei,Xichen Xu,Xingjian Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种高效的全能型图像复原方法OPIR，通过预测任务自适应的逆退化算子，实现对多种图像复原任务的统一处理，兼具效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多任务图像复原方法通过引入提示信息或使用大型模型提高性能，但导致系统复杂、难以实时应用，因此需要一种高效、统一且效果优异的全能图像复原方案。

Method: 提出了一种两阶段的框架：第一阶段，利用预测的逆退化算子生成初步复原结果及不确定性感知图，突出难以还原的区域以提升可靠性；第二阶段，在该不确定性感知图的引导下对图像进一步精细复原。两阶段均采用同一逆算子预测网络，配合任务自适应的参数调节，并进一步优化逆算子的卷积速度以提升整体效率。

Result: 通过广泛实验，所提出的OPIR不仅在多任务图像复原（all-in-one restoration）上表现优异，同时在针对性任务（task-aligned restoration）上也具备极强的竞争力。

Conclusion: OPIR在保证复原性能的同时，大幅降低了方法复杂度与运算开销，为高效、实用的全能图像复原提供了新思路。

Abstract: All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.

</details>


### [34] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出ELITE方法，通过结合3D Mesh与2D生成先验，实现了从单目视频高效合成高保真可动画头部头像，兼具泛化能力和合成速度。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频头像合成方法多数依赖单一的3D先验，难以泛化到真实场景，或者依赖2D生成先验，计算开销大且易发生身份错觉。本文希望结合两者优点，实现高效且泛化性强的头像生成。

Method: 提出Mesh2Gaussian先验模型（MGPM），可快速初始化3D高斯头像。测试时引入生成自适应阶段，通过真实与合成图像联合监督，并采用基于渲染的单步扩散增强策略，有效恢复视觉细节，提升头像的质量与表现力。

Result: ELITE在头像外观、可动画性和泛化能力上均优于已有方法，特别在复杂表情情况下效果更佳；且相较主流2D生成先验方法合成速度提升60倍。

Conclusion: ELITE方法高效结合3D和2D先验，不仅生成效果优异还大幅提升合成速度，为单目视频头像合成提供了新的解决方案，适用于更广泛的真实场景。

Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [35] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为DepthDirector的视频重渲染框架，实现了在保持视频内容一致性的同时对摄像机轨迹进行精确控制。通过引入深度视频作为摄像机控制引导，并结合创新性的双流条件机制，显著提升了摄像头可控性和视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频生成时对摄像机轨迹的精确操控依赖3D表示扭曲，但忽略了视频扩散模型的3D先验，常常导致内容一致性变差和画质下降。为解决这一问题，作者希望提升摄像机控制精度的同时维持内容稳定。

Method: 方法上，作者提出DepthDirector框架，利用从显式3D表示获取的深度视频作为摄像机控制引导，并设计双流机制，将源视频和在目标视角下扭曲得到的深度序列共同输入预训练的视频生成模型。引入LoRA微调适配器以精炼训练过程，并自行构建了大规模多摄像机同步数据集（MultiCam-WarpData）。

Result: 实验表明，DepthDirector在摄像机可控性和视觉质量上均显著优于已有方法。

Conclusion: DepthDirector创新性地结合几何深度引导和视频扩散模型先验，实现了高质量、精确可控的视频内容重渲染，对条件视频生成领域具有重要推动作用。代码和数据集将公开发布。

Abstract: Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [36] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一套多阶段视频问答处理框架，通过预处理、模型微调、链式推理和后处理，有效提升了MLLMs在HD-EPIC VQA等复杂视频问答基准测试上的表现。系统取得了41.6%的准确率，展现了整体流程优化的重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在面对如HD-EPIC VQA这类复杂视频问答任务时，受限于问题/选项歧义、时序推理困难以及输出不标准，导致表现不佳。因此，亟需一种能提升理解和推理的系统性方案。

Method: 方法包括四个关键部分：1）对问题和选项进行预处理以减少歧义；2）基于领域知识对Qwen2.5-VL模型进行微调；3）引入创新性Temporal Chain-of-Thought（T-CoT）提示，强化多步时序推理能力；4）采用稳健的后处理方式标准化输出。

Result: 提出的系统在HD-EPIC VQA基准上实现了41.6%的准确率，明显优于基础MLLMs；相关代码和微调模型已开源。

Conclusion: 面对复杂视频问答任务，单独依靠通用MLLMs不够，需通过系统性优化全流程（包括预处理、模型微调、推理方式和后处理）来显著提升实际表现。

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [37] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

TL;DR: 本论文提出了一种方法，根据演讲者的讲述自动识别并高亮幻灯片上最相关的区域，提升听众在内容丰富演讲中对关键信息的获取效率。


<details>
  <summary>Details</summary>
Motivation: 在面对内容密集的演讲（如会议报告）时，听众往往难以同时跟上演讲内容和视觉材料，对重要信息的提取感到吃力。论文旨在解决演讲内容与视觉材料不同步带来的理解困难。

Method: 作者设计了一种自动方法，通过分析演讲者的语音内容，并将其与幻灯片上的文本或图形元素进行匹配，从而自动高亮出与当前陈述相关的幻灯片区域。论文还探讨了多种解决此类问题的方法，并评估了各自的优点与限制。

Result: 新方法提高了听众对关键细节的关注效率，缓解了视觉材料与口头交流之间的脱节，从而提升了内容理解力和吸收效果。文中还讨论了该方法在成功和失败场景下的表现。

Conclusion: 自动关联和高亮幻灯片区域能够有效降低听众的认知负担，改善对内容丰富视频（如教学和会议演讲）的理解和吸收，对多媒体文档分析研究具有现实意义和应用前景。

Abstract: Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [38] [DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset](https://arxiv.org/abs/2601.10305)
*Hengyu Shen,Tiancheng Gu,Bin Qin,Lan Wu,Yuling Wu,Shuo Tan,Zelong Sun,Jun Wang,Nan Wu,Xiang An,Weidong Cai,Ziyong Feng,Kaicheng Yang*

Main category: cs.CV

TL;DR: 本论文提出了高质量中文图文数据集DanQing，并通过实验验证其在多项中文视觉-语言任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 受限于缺乏高质量中文图文数据，中文视觉-语言预训练领域进展缓慢，显著落后于英文。为推动该领域发展，需构建更优质、更大规模的中文跨模态数据集。

Method: 作者搭建了一条严格筛选流程的数据构建管线，从Common Crawl采集并精挑细选出1亿对图文数据，形成DanQing数据集。与现有数据集相比，其主要由2024-2025年最新网络数据组成，更准确反映当前语义趋势。随后，作者通过对SigLIP2模型的持续预训练，全面评测了DanQing的实际效果。

Result: 实验显示，使用DanQing数据集预训练后，模型在零样本分类、跨模态检索和基于多模态大模型的评测等多个中文下游任务上均取得了显著优于其他数据集的效果。

Conclusion: DanQing推动了中文视觉-语言预训练领域的发展，并计划以知识共享CC-BY 4.0协议开源此数据集，为后续研究提供资源。

Abstract: Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.

</details>


### [39] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种适用于视觉-语言预训练模型（VLP）的通用多模态对抗攻击方法——分层精炼攻击（HRA），能高效制造图像与文本的通用扰动，并在多任务和多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对VLP模型的对抗攻击大多针对单个样本，导致在大规模数据或新场景下计算开销大，难以泛化。因此，需要一种高效、通用的多模态攻击方法，应对复杂实际应用。

Method: 作者提出了HRA框架，从样本层级和优化层级优化通用扰动。图像方面，将对抗样本分离为干净图像与扰动，利用ScMix数据增强提升全局与局部攻击能力，并通过历史与未来梯度的分层优化提高扰动学习稳定性。文本方面，通过句内和句间重要性度量选择影响力大的词语作为通用扰动。

Result: 针对不同下游任务、模型和数据集进行大量实验，结果表明HRA的通用多模态攻击明显优于现有攻击方法，无论在攻击强度还是泛化能力上都有突破性提升。

Conclusion: HRA实现了高效、鲁棒且泛化性强的多模态对抗攻击，对推动VLP模型安全研究具有重要意义。

Abstract: Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [40] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

TL;DR: ROMA是一种统一的全模态流式语音、视觉和文本处理助手，在实时多模态流输入下表现突出，尤其在主动响应任务中达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 目前的全模态大型语言模型在流式音视频理解上存在支持不全或缺乏主动监控的问题，难以统一和高效处理连续的多模态数据流，限制了实际应用场景中的智能交互能力。

Method: ROMA通过将连续输入处理为同步的多模态单元，采用对齐技术解决音频与视频粒度不匹配问题；引入轻量级说话头模块，实现响应启动与内容生成的解耦，从而准确触发响应并避免冲突。模型基于精心构建的流式多模态数据集进行两阶段课程训练，分别优化流式适应和主动响应能力，还对评测基准进行了重组和统一。

Result: 在12项基准的广泛实验中，ROMA在主动任务（如警报、解说）上取得了当前最佳表现，在反应式任务（如问答）上也表现出竞争力，显示其流式全模态理解的鲁棒性和优越性。

Conclusion: ROMA实现了兼具主动与反应能力的统一流式全模态理解和交互，为多模态持续监控和智能助手应用提供了强大支持。

Abstract: Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [41] [SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition](https://arxiv.org/abs/2601.10324)
*Yiming Zhang,Weibo Qin,Yuntian Liu,Feng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对合成孔径雷达(SAR)自动目标识别系统的全新对抗攻击方法Space-Reweighted Adversarial Warping (SRAW)，能够以空间变形实现隐蔽而有效的对抗样本生成。实验表明SRAW在保隐性和攻击能力上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度神经网络的SAR自动目标识别系统虽然应用广泛，但对抗鲁棒性较差，容易受到攻击样本干扰，并且现有攻击方法往往需要较明显的扰动，难以兼顾隐蔽性和攻击效果。因此，亟需一种能够平衡两者的新型对抗攻击方式。

Method: 提出SRAW方法，通过对前景与背景区域设置不同的空间扰动预算，优化空间变形，从而生成更具隐蔽性的对抗样本。相比传统方法，SRAW更关注扰动的空间分布与视觉不可辨性。

Result: 大量实验表明，SRAW能显著降低多种先进SAR-ATR模型的识别性能，且其生成的对抗样本无论在不可察觉性还是迁移攻击能力上，都优于现有同类方法。

Conclusion: SRAW为攻击SAR图像目标识别系统提供了一种强大而隐蔽的手段，验证了空间重加权变形在提高对抗样本质量和攻击效果上的有效性。

Abstract: Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.

</details>


### [42] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文提出了think-then-generate (T2G)范式，使文本-图像扩散模型不仅依赖LLM编码文本，还能发挥LLM推理能力，对原始用户提示进行推理和重写，从而提升生成图像的语义一致性和真实感。


<details>
  <summary>Details</summary>
Motivation: 当前T2I扩散模型大多仅将LLM作为文本编码器，未有效利用其推理能力，导致对复杂或隐含语义的文本理解和图像生成有限。为解决这一不足，作者尝试结合LLM的推理能力，实现对用户提示的理解和重写。

Method: 方法包括：1）通过轻量有监督微调，激发LLM的“先思考后重写”能力；2）LLM编码器与扩散主干协同优化，具体采用Dual-GRPO机制，其中编码器用图像奖励加强推理能力，扩散主干提升语义一致性和图像质量。

Result: 实验证明，该方法在推理类图像生成与编辑基准上，事实一致性、语义对齐和视觉逼真度均有大幅提升，WISE分数达0.79，接近GPT-4水平。

Conclusion: T2G范式有效推动T2I模型从字面映射向具备推理、表达和展示能力的统一新一代模型迈进。

Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [43] [An analytic theory of convolutional neural network inverse problems solvers](https://arxiv.org/abs/2601.10334)
*Minh Hai Nguyen,Quoc Bao Do,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: 本论文通过MMSE（最小均方误差）视角，理论分析了带有卷积归纳偏置（平移等变性和局部性）的神经网络，并推导出LE-MMSE（局部等变MMSE）解析表达式，理论结果与多个视觉反问题的神经网络实验输出高度吻合。


<details>
  <summary>Details</summary>
Motivation: 尽管卷积神经网络（CNN）在成像反问题上表现优异，但其内部机制和理论分析仍不充分。现有方法更多当作黑箱处理，缺少对其归纳偏置和性能关系的理论认识，因此亟需建立可解释性强的理论框架。

Method: 提出仿照CNN的两大归纳偏置（平移等变性和有限感受野），将MMSE估计器加上功能性约束，推导出一种局部-等变MMSE（LE-MMSE）理论表达式。在多个逆问题任务、数据集和CNN架构下，通过实验证实该理论与网络输出吻合，量化度量为PSNR。

Result: LE-MMSE理论与实际神经网络（如U-Net、ResNet、PatchMLP）在图像去噪、修补、反卷积等任务上的输出结果（PSNR≥25dB）高度吻合。同时，分析了物理感知与非感知估计器的差异、高密度训练分布、数据集大小和patch大小等因素的影响。

Conclusion: 本研究为CNN在逆问题中的表现提供了可解释性强的理论基础，揭示了网络性能与其归纳偏置的内在联系，为理解和设计高性能视觉网络提供了理论工具。

Abstract: Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).

</details>


### [44] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了HPE-Bench基准数据集和一个新的多模态大模型评测框架，用于更细致、全面地评估文本引导的人体姿态编辑的真实性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的人体姿态编辑生成方法会出现结构异常、伪影等问题，而现有评估指标无法细粒度地评估姿态编辑的真实性和质量，这限制了实际应用和模型改进。

Method: 作者构建了包含1,700个样本和多维质量标注的HPE-Bench基准，涵盖17个最新姿态编辑模型。同时提出了基于分层选择和LoRA微调、多模态大语言模型的统一框架，并引入新的层敏感性分析机制，自动寻找最优特征层用于评测。

Result: 所提出的方法在真实性检测和多维质量回归上都取得了更好的效果，提高了评测的准确性和细致度。

Conclusion: 新基准和评测框架能更好地弥合伪造检测与质量评估的鸿沟，为姿态编辑模型的开发与评估提供了更科学有效的工具。

Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [45] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffCR的扩散模型图像压缩新框架，通过一致性先验精炼和频率感知模块，实现了低码率下高保真、高效率的图像重建，并大幅提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像生成压缩方法，在超低码率下虽表现较好，但存在采样速度较慢、比特分配效率低等问题，主要因训练流程割裂。

Method: DiffCR引入了频率感知跳跃估计（FaSE）模块，通过频率解耦注意力（FDA）精炼扩散模型的先验，并对不同时间步的特征进行对齐。同时，设计轻量级的一致性估计器，依靠保留扩散采样的语义轨迹，实现高效的两步解码。无需更新主干扩散模型。

Result: DiffCR在不更改主干扩散模型参数的前提下，相比最新扩散压缩方法，码率大幅下降（LPIPS指标下BD-rate降低27.2%，PSNR指标下BD-rate降低65.1%），解码速度超过10倍提升。

Conclusion: DiffCR方法显著提升了基于扩散模型的图像压缩，在确保高视觉质量的同时实现了高效率和快速解码，对实际低码率场景应用更具有实用价值。

Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [46] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的变换器模型VIST2，通过将输入文本分块转换为可视化图像，实现视觉-文本融合压缩，从而大幅减少推理时Token数量，在长文本生成与内存效率上优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然能通过视觉编码降低编码阶段的Token数量，但推理时仍需逐Token处理，内存与算力压力大。因此迫切需要能在推理期间也压缩Token的方法，以提升效率。

Method: 提出VIST2模型，将文本分块后渲染为素描图像，将视觉Token与文本交替输入模型，并仅用视觉Token预读预测下一个文本Token。模型训练分阶段进行，包括课程学习预训练和模态交错微调。实验探索了不同规模（0.6B~8B）模型与相关超参。

Result: VIST2在4倍压缩比下，于长文本处理基准上优于现有基线，首Token生成速度快3倍，内存消耗减少77%，FLOPS降低74%。

Conclusion: VIST2有效实现了视觉-文本深度压缩，显著提升长文推理速率和资源利用率，适应大规模生成任务。代码与数据将开放，有助于后续研究。

Abstract: Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [47] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 本论文提出了一种针对不可切除II-III期非小细胞肺癌（NSCLC）生存预测的多模态深度学习框架，能够有效整合断层扫描（CT）、全切片病理图像和结构化临床变量，即使数据缺失也能保证模型稳定性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有NSCLC生存预测需整合多模态异质信息，但由于数据集规模小和模态缺失，传统方法受限，常需剔除不全数据或采用激进的数据补全，影响模型泛化和临床应用价值。因此，急需设计能适应自然模态缺失的多模态学习框架。

Method: 本文提出一种基于基础模型（Foundation Model）提取单模态特征、结合缺失感知编码策略的多模态中间融合神经网络，对CT、全切片病理图像和结构化临床数据进行集成学习，不需在模态缺失时丢弃患者或强制补全数据。

Result: 实验结果显示，中间融合方法在生存预测C指数上显著优于单模态、早期融合和晚期融合基线，WSI与临床数据联合最佳（C-index 73.30）。分析表明模型可自适应下调信息较少的模态（如CT）的权重。

Conclusion: 该方法能够充分利用不完整的多模态临床数据，提高生存预测的泛化能力和适用性，为异质医学数据的集成分析和癌症预后建模提供了新思路。

Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [48] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

TL;DR: 本文提出了一种将多时序的荧光显微图像融合为单张高质量图像的新方法，能显著提升生物学信号的可视化与数据质量。


<details>
  <summary>Details</summary>
Motivation: 荧光显微成像常用于活体生物样本分析，但其记录易受噪声、时间变异和信号变化影响，导致结果不一致，影响后续分析。因此，亟需一种方法能整合动态信息，提升图像质量。

Method: 作者提出了一个创新性的计算框架，将多帧时序显微视频信息融合为一张高质量图像。该方法结合了来自不同计算机视觉领域的可解释性技术，并在复杂的心肌细胞2D单层数据集上测试了111种配置。

Result: 实验显示该融合方法显著提升了单帧图像的信息量和质量。相较于既有方法，细胞计数提升了44%。

Conclusion: 该方法不仅改善了荧光显微图像的质量和生物学信息保留，还适用于需要多时序影像融合的其它成像领域，有助于更好的标注和分割等后续任务。

Abstract: Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [49] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法（Lunar-G2R），可以直接从月球数字高程模型（DEM）预测空间变化的BRDF参数，无需多视图影像或专用硬件，实现更真实的月表渲染。实验证明该方法在Tycho环形山区域比现有方法降低了38%的光度误差，并且在PSNR、SSIM及感知相似性上都有提升。


<details>
  <summary>Details</summary>
Motivation: 高保真渲染和基于视觉的导航对复杂月球表面真实反射率估计要求很高，但现有渲染流程只能假设简化或空间一致的BRDF模型，难以精确捕捉局部反射变化，导致仿真和实际差距较大。

Method: 提出Lunar-G2R框架，利用U-Net神经网络，结合可微渲染，把地形几何映射为BRDF参数。在训练中，通过最小化物理渲染图像与真实轨道图像的光度差异，无需多视角、可控照明或专用反射采集硬件。

Result: 在Tycho环形山地理独立测试区，与先进基线方法相比，光度误差降低了38%，PSNR和SSIM指标更高，感知相似性提升，能够捕捉到空间一致模型遗漏的细粒度反射变化。

Conclusion: Lunar-G2R首次证明了可直接借助地形几何推断空间变化的反射率模型，为月面真实渲染及视觉导航提供了新方法，提升了物理真实性和局部细节表现。

Abstract: We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [50] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型推理的社会语义分割方法，并发布了全新数据集SocioSeg，实现了从卫星图像中对社会定义类别（如学校、公园）高效分割，显著领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市表面包含大量实体，社会定义类别（如学校和公园）的遥感分割一直是难题，限制了众多应用场景。现有方法主要针对物理属性明显的类别，难以处理依赖于社会语义理解的类别。

Method: 提出了SocioSeg数据集，包含卫星图像、数字地图及像素级社会语义实体标签，类别具备层级结构。方法上，创新性地提出了SocioReasoner框架，利用视觉-语言模型进行跨模态、多阶段推理，同时引入强化学习优化非可微过程，提升对社会语义实体的识别与标注能力。

Result: 实验结果表明，所提方法在社会语义分割任务上的表现优于当前最先进的模型，尤其在零样本泛化能力上表现突出。

Conclusion: 通过融合视觉-语言推理和强化学习，显著提升了卫星图像中社会语义类别的分割能力，为相关领域提供了数据和方法新工具。

Abstract: As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>


### [51] [mergetune: Continued fine-tuning of vision-language models](https://arxiv.org/abs/2601.10497)
*Wenqing Wang,Da Li,Xiatian Zhu,Josef Kittler*

Main category: cs.CV

TL;DR: 该论文提出了一种称为“持续微调(continued fine-tuning, CFT)”的新范式，以及名为MERGETUNE的策略，用于在视觉-语言模型(如CLIP)微调后恢复其预训练知识，从而缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 微调视觉-语言模型时，通常会发生灾难性遗忘，即模型适应新任务时丢失了原有的预训练知识。现有方法主要试图在适应期间减缓遗忘，但仍难以完全避免。为此，作者希望提出一种在模型已被微调之后追回丢失知识的方法。

Method: 作者提出MERGETUNE，这是一种模型无关的持续微调(CFT)策略。方法本质上利用线性模式连通性(linear mode connectivity, LMC)理论：在模型已经经过微调后，进一步微调其可训练参数，如soft prompt或线性头，使其在损失空间上可以与零样本预训练模型和微调模型连通（即两个方向均为低损失路径）。为避免需要原始预训练数据，作者用二阶近似替代了LMC的原始数据重放要求。该方法无需结构改动，可后置地用于任意已有微调模型。

Result: MERGETUNE在标准基-新（base-novel）泛化任务上使CoOp模型的调和均值提升了5.6%，且无需增加参数。在强鲁棒性微调实验中，该方法优于集成基线且推理开销更低，与零样本模型集成时能进一步获得SOTA性能。

Conclusion: MERGETUNE作为一种后处理手段，有效缓解了视觉-语言模型微调后的灾难性遗忘，并在多个基准上取得显著改进。

Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.

</details>


### [52] [SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction](https://arxiv.org/abs/2601.10512)
*Kanak Mazumder,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 本论文提出SatMap，一种融合卫星地图和多视角摄像头观测的在线高精地图构建方法，在nuScenes数据集上显著提升了地图预测性能，尤其在远距离和恶劣天气下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中，摄像头基于的高精地图构建手段存在视深感知受限、遮挡导致精度下降等问题，因此亟需提升地图构建的准确性和鲁棒性。

Method: 利用卫星图像提供鸟瞰视角的全局先验，将其与多视角车载摄像头的数据融合，通过神经网络直接预测矢量化的高精地图。

Result: 在nuScenes数据集上，SatMap方案相比仅用摄像头的方法mAP提升34.8%，相比摄像头与LiDAR融合的方法提升8.5%。同时在远距离和恶劣天气场景下也展现出优势。

Conclusion: SatMap方法有效利用卫星图像的全局信息，提升了摄像头主导的高精地图构建的准确性和鲁棒性，对自动驾驶系统具备实际应用价值。

Abstract: Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.

</details>


### [53] [BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition](https://arxiv.org/abs/2601.10521)
*Max A. Buettner,Kanak Mazumder,Luca Koecher,Mario Finkbeiner,Sebastian Niebler,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 本文提出了FUSE-Bike平台和BikeActions多模态数据集，用于从骑行者视角提升对弱势道路使用者（VRU）意图的预测，为相关自动驾驶研究提供首个高精度基准。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶和移动机器人领域，主要关注行人过街行为，且多从车辆视角出发。面对在高密度共享空间中VRU行为预测的研究空白，亟需基于骑行者视角的高精度数据和基准推动研究进展。

Method: 作者提出了FUSE-Bike开放平台，集成两台LiDAR、摄像头和GNSS，实现从骑行者一侧的近距离高精度数据采集；并据此采集和标注了BikeActions多模态数据集（852个样本，5类动作）。文中还用图卷积与transformer模型对数据集进行基准性能测试。

Result: 构建了主流图卷积和transformer模型在BikeActions数据集上的性能基准，公开完整数据集、硬件设计和基准代码，使后续研究者能够复现和深入开展VRU行为理解。

Conclusion: FUSE-Bike和BikeActions推动了自动驾驶领域对高密度共享空间中VRU行为预测的研究，为未来相关算法和系统开发打下了坚实数据和工具基础。

Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.

</details>


### [54] [SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery](https://arxiv.org/abs/2601.10535)
*Chong Liu,Luxuan Fu,Yang Jia,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 该论文提出SVII-3D，一种面向基础设施数字化的统一框架，实现基于稀疏图像的智能资产创建和精准定位，用于智慧城市建设和设施全生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 智慧城市和设施管理需要高精度、自动化的资产数字化方案，但现有基于稀疏图像方法受鲁棒性不足、定位不准及状态感知粗糙等局限影响，无法满足实际应用需求。

Method: SVII-3D框架包含三大核心模块：1）结合LoRA微调开放集目标检测器与空间注意力匹配网络，实现多视角下资产观测的鲁棒关联；2）引入几何引导的精化机制，解决结构性误差，提升3D定位精度至分米级；3）集成视觉-语言模型Agent，利用多模态提示实现资产状态的自动化细粒度诊断。

Result: 实验表明，SVII-3D在资产识别准确率及定位误差控制方面，较现有方法有显著提升。

Conclusion: SVII-3D为基础设施的高保真数字化提供了一种可扩展、经济高效的技术路径，推动稀疏感知与自动化智能运维的结合，助力智慧城市建设。

Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.

</details>


### [55] [Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning](https://arxiv.org/abs/2601.10537)
*Oscar H. Ramírez-Agudelo,Akshay N. Shewatkar,Edoardo Milana,Roland C. Aydin,Kai Franke*

Main category: cs.CV

TL;DR: 本文提出利用深度学习方法提升烟雾和雾气环境下仪表读数图像的可辨识度，通过生成合成数据集和对比不同网络结构，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在烟雾和雾气环境中，传统的图像监控与仪表读数会因可见性降低而变得极其困难，影响紧急救援和基础设施监控。提高这些图像的自动可读性对于应急响应有实际价值。

Method: 作者生成了一个基于Unreal Engine的合成仪表图像数据集，包括轻度到重度烟雾和雾气，并采用FFA-Net和AECR-Net深度学习网络对图像进行去雾、去烟处理。训练集、验证集和测试集的比例分别为80%、10%、10%。主要通过SSIM和PSNR评估模型表现。

Result: 在合成雾气数据集上，模型取得了约0.98的SSIM和43dB的PSNR，达到了先进水平。在去烟任务上，由于烟雾的非均匀性和高密度，表现略差，但整体效果仍有提升，其中AECR-Net优于FFA-Net。增强后的图像能够更好地被自动系统读取。

Conclusion: 深度学习方法能显著提升烟雾和雾气场景下仪表图像的质量，增强后的图像支持自动仪表读取，对应急场景下的基础设施监控具备良好应用前景。

Abstract: Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\% train, 10\% validation, and 10\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges

</details>


### [56] [Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure](https://arxiv.org/abs/2601.10551)
*Luxuan Fu,Chong Liu,Bisheng Yang,Zhen Dong*

Main category: cs.CV

TL;DR: 本文提出了一种将大规模视觉语言模型（VLMs）转化为智能基础设施分析专用代理的框架，通过精细调优和检索增强机制，提升了对城市道路基础设施的感知和属性识别能力。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型对城市道路基础设施的细粒度属性和领域规范把握有限，难以满足智能城市管理的精确需求。因此，需要构建专门适用于该领域且能够依从工程标准的感知模型。

Method: 提出了结合数据高效微调与知识推理机制的领域适应框架。具体做法包括：在Grounding DINO上采用开放词汇微调以实现多样化资产的精准定位，然后在Qwen-VL上基于LoRA方法进行深层语义属性推理，并通过双模态检索增强生成（RAG）模块，在推理阶段动态引入行业标准和视觉范例，降低幻觉并提升专业合规性。

Result: 在新的城市道路场景数据集上，框架检测性能达到58.9 mAP，属性识别准确率为95.5%，展示了该方法的有效性和实用性。

Conclusion: 本文方法能有效提升智能基础设施监控中对细粒度属性的识别准确性与合规性，为智能城市管理提供了稳健、高效的解决方案。

Abstract: Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.

</details>


### [57] [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553)
*Jianhao Yuan,Xiaofeng Zhang,Felix Friedrich,Nicolas Beltran-Velez,Melissa Hall,Reyhane Askari-Hemmat,Xiaochuang Han,Nicolas Ballas,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理方法WMReward，用于提升视频生成模型的物理合理性，并在主流基准上取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视频生成模型经常生成与物理常识不符的内容，这被认为不仅是预训练物理知识不足的问题，也与推理策略不佳有关。因此需要一种有效方法提升生成视频的物理可信度。

Method: 作者提出将视频生成的物理合理性作为推理阶段的对齐问题，设计了WMReward方法，利用具有物理先验的潜在世界模型(VJEPA-2)作为奖励，通过搜索和引导多种去噪轨迹来提升物理真实性，从而在测试时通过增加计算量带来更佳生成性能。

Result: 该方法在图像条件、多帧条件和文本条件下的视频生成任务中显著提高了物理合理性，并得到人工偏好实验的验证。在ICCV 2025 Perception Test PhysicsIQ Challenge中以62.64%的分数获得第一，超过前SOTA 7.42%。

Conclusion: 利用具有强物理先验的潜在世界模型可以有效提升视频生成模型的物理合理性，这一理念不限于本文实例和具体模型，具有广泛适用性。

Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.

</details>


### [58] [DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery](https://arxiv.org/abs/2601.10554)
*Constantin Selzer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 该论文提出了DeepUrban，一个专为密集城市交通交互设计的无人机轨迹数据集，显著提升了自动驾驶在复杂城市交通场景中的预测与规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶基准数据集在密集交通场景下样本有限，缺乏对复杂交通参与者交互的深入建模，限制了自动驾驶系统的性能提升。

Method: 作者与工业合作伙伴DeepScenario合作，通过无人机在约100米高空对城市交叉口进行高分辨率成像，并提取3D交通物体信息，形成包含丰富地图与场景信息的新数据集DeepUrban，再用来评估和实验最新的轨迹预测与规划方法。

Result: 通过在现有nuScenes数据集基础上加入DeepUrban，模型的轨迹预测与规划准确率有显著提升，ADE和FDE指标上最高分别提升了44.1%和44.3%。

Conclusion: DeepUrban极大丰富了密集城市交通场景的数据资源，能够更有效地支持自动驾驶系统在复杂环境下的预测与规划能力，推动了领域内方法的进步。

Abstract: The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban

</details>


### [59] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: 该论文提出了一种基于拓扑学（Jordan曲线定理）的全新图像分割评价标准，用于评价分割结果的结构和连通性，从而弥补传统指标在捕捉全局形状与拓扑一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分割评价指标在处理细微错误（如边界断裂、孔洞、分割碎片）时可能依然给出高分，但却无法反映分割结果的整体结构和连通性，特别是在医学图像等实际场景下，这一不足更加明显。论文希望基于拓扑学原理，引入能更好捕捉分割结构完整性的评价标准。

Method: 作者提出了'Jordan-segmentatable mask'的概念，即基于数字拓扑和同调理论的分割评价方法：首先从分割掩码中提取数字4-曲线候选，结合Betti数（β0，β1）来验证分割的拓扑有效性，当满足β0=β1=1，或等价地其补集恰好分为两个8连通分支时，该掩码被认为结构一致。

Result: 该方法无需监督，能严谨地评价掩码的结构连贯性和拓扑正确性。作者证明该方法能比传统像素、区域、边界指标更好评估分割的全球结构性，尤其适用于需要拓扑连续性的实际应用场景。

Conclusion: 基于数字Jordan理论与同调不变量的评价框架为图像分割结构性评价提供了新方向，对保障医学、遥感等任务中的分割拓扑正确性具有重要意义，是传统指标的重要补充。

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


### [60] [Adversarial Evasion Attacks on Computer Vision using SHAP Values](https://arxiv.org/abs/2601.10587)
*Frank Mollard,Marcus Becker,Florian Roehrbein*

Main category: cs.CV

TL;DR: 本论文提出了一种基于SHAP值的白盒攻击方法，能有效欺骗计算机视觉模型且难被人眼察觉。同时，该方法在特定场景下比常见的对抗攻击方法表现更强。


<details>
  <summary>Details</summary>
Motivation: 深度学习视觉模型容易受到对抗性攻击，但现有攻击方式如FGSM在某些情况下受限（如梯度隐藏）。作者希望利用SHAP值更精确地识别出对模型输出影响最大的输入部分，从而增强攻击效果。

Method: 作者提出用SHAP值来度量输入对模型输出的贡献，再基于这些高贡献输入对图像进行最小幅度但最有效的扰动，通过这种方式实施对抗样本攻击，并与Fast Gradient Sign Method（FGSM）进行对比实验。

Result: 实验表明，SHAP攻击能更有效地产生误分类，特别是在梯度隐藏等对FGSM不利的场景下表现更稳健。

Conclusion: 基于SHAP方法的对抗攻击能更精准定位关键输入特征，提高了攻击的有效性和鲁棒性，对现有防御机制提出了新的挑战。

Abstract: The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.

</details>


### [61] [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592)
*Delong Chen,Tejaswi Kasarla,Yejin Bang,Mustafa Shukor,Willy Chung,Jade Yu,Allen Bolourchi,Theo Moutakanni,Pascale Fung*

Main category: cs.CV

TL;DR: 本文介绍了Action100M，这是一个大规模开放词汇的视频动作数据集，包含了来自120万网络教学视频的约1亿个动作片段及详尽标注，为视频理解和物理世界的推理研究提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别数据集在规模、领域覆盖和标签丰富度上都存在局限，难以满足机器智能在物理世界中对动作推理与理解的需求，尤其缺乏开放词汇、多样场景及丰富语义描述的大数据集。

Method: 作者提出了全自动构建流程：(1) 利用V-JEPA 2嵌入做层次化时序分割；(2) 对各层级片段和帧生成结构化的'树状字幕'（Tree-of-Captions）；(3) 通过大型推理模型（GPT-OSS-120B）与多轮自我优化（Self-Refine）聚合证据，生成动作、行为者、详细描述等结构化标注。

Result: 采用Action100M训练VL-JEPA模型，在多项动作识别基准测试中取得了显著的数据规模提升收益，并展现了强大的零样本泛化能力。

Conclusion: Action100M为大规模视频理解和世界建模奠定了新基础，有助于推动机器智能在物理行为推理等方面的研究。

Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.

</details>


### [62] [RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation](https://arxiv.org/abs/2601.10606)
*Peng Chen,Xiaobao Wei,Yi Yang,Naiming Yao,Hui Chen,Feng Tian*

Main category: cs.CV

TL;DR: RSATalker提出了一种结合3D Gaussian Splatting与社交关系建模的方法，实现了高真实感、低计算成本且具社会意识的对话人物生成，适合多轮交流场景。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实社交对多轮高质量交互式头像生成需求增加，现有方法要么缺乏真实性、要么计算成本高，且很少关注头像间的社会关系。

Method: RSATalker首先利用语音驱动基于mesh的3D面部动作，然后将3D高斯分布绑定到mesh面以渲染高保真2D头像。同时引入社交关系感知模块，通过可学习查询机制把包括血缘/非血缘、平等/不平等等社会关系编码为高层次特征，并采用三阶段训练和新构建的RSATalker数据集进行优化。

Result: 实验显示RSATalker在头像真实感和社会关系感知上均达到当前最优表现。

Conclusion: RSATalker有效结合3DGS与社交关系建模，显著推动虚拟社交中多轮交互头像生成技术的发展，相关代码和数据集将开源。

Abstract: Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.

</details>


### [63] [Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding](https://arxiv.org/abs/2601.10611)
*Christopher Clark,Jieyu Zhang,Zixian Ma,Jae Sung Park,Mohammadreza Salehi,Rohun Tripathi,Sangho Lee,Zhongzheng Ren,Chris Dongjoo Kim,Yinuo Yang,Vincent Shao,Yue Yang,Weikai Huang,Ziqi Gao,Taira Anderson,Jianrui Zhang,Jitesh Jain,George Stoica,Winson Han,Ali Farhadi,Ranjay Krishna*

Main category: cs.CV

TL;DR: Molmo2是顶尖的开源视频-语言模型，具备先进的像素级指向和跟踪能力，并在多个任务上超越现有开源和一些专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前最强的视频-语言模型（VLMs）大多为专有产品，开源模型在数据和训练方法公开性上存在不足，这限制了社区对模型能力的进一步提升。同时，许多实际应用需要模型具备在像素层面进行定位和跟踪的能力，而即便是专有模型往往也做不到。

Method: 作者推出了Molmo2模型，发布了7个新的视频数据集和2个多图像数据集，这些数据集全部独立采集，并未依赖闭源模型。同时，提出了高效数据打包及消息树编码方案，创新性地引入了视觉token的双向注意力和“token权重”机制。

Result: Molmo2的8B规模模型在短视频理解、计数和描述等任务上优于当前所有开源模型，并在长视频任务中表现有竞争力。特别是在视频定位和追踪等下游任务中大幅超越开源模型（如Qwen3-VL），并在一些任务上超越了Gemini 3 Pro等专有模型。

Conclusion: Molmo2为开源社区带来了具备领先性能的视频-语言模型和全套训练数据，首次实现了高效、灵活的像素级指向和跟踪能力，有助于推动相关下游任务的发展。

Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).

</details>


### [64] [CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos](https://arxiv.org/abs/2601.10632)
*Chengfeng Zhao,Jiazhi Shu,Yubo Zhao,Tianyu Huang,Jiahao Lu,Zekai Gu,Chengwei Ren,Zhiyang Dou,Qing Shuai,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的协同生成框架CoMoVi，将3D人体动作生成和2D视频生成结合在单一扩散去噪过程内同步实现，显著提升了动作和视频生成的表现。


<details>
  <summary>Details</summary>
Motivation: 3D动作生成和2D视频生成本质上是相关联的：3D动作为视频生成提供结构先验，预训练视频模型则有助于提升动作生成的泛化能力。以往方法二者分离，无法发挥它们的互补优势，因此需要耦合生成过程以提升整体效果。

Method: 首先提出了一种可以利用预训练视频扩散模型先验的2D人体动作表达；其次设计了一个双分支的扩散模型，通过特征交互及3D-2D跨注意力机制，将动作与视频生成过程协同耦合；此外还构建了大规模、包含文本与动作标签的真实视频数据集CoMoVi Dataset。所有生成阶段在单一的扩散去噪循环中同步进行。

Result: 大量实验表明，该方法在3D人体动作生成和人类视频生成两项任务上均表现优越。

Conclusion: 将3D动作和2D视频生成过程协同耦合，能共同提升生成质量。新提出的CoMoVi框架展现出较强的适应性和生成效果，有助于推进人体动作与视频生成相关应用的发展。

Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.

</details>


### [65] [CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning](https://arxiv.org/abs/2601.10649)
*Darshan Singh,Arsha Nagrani,Kawshik Manikantan,Harman Singh,Dinesh Tewari,Tobias Weyand,Cordelia Schmid,Anelia Angelova,Shachi Dave*

Main category: cs.CV

TL;DR: 本文提出了CURVE数据集，用于评估多文化、多语言环境下的视频理解能力，解决现有评价偏西方及英语的问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流视频理解基准数据主要集中在西方文化和英语，评测结果存在严重文化和语言偏见，无法全面反映模型的全球适应性。

Method: 作者构建了CURVE数据集，涵盖全球18个地区、不同文化的视频，全部由人类注释，包含复杂的问题、答案和多步推理。与自动翻译不同，所有内容均为本地语言原生生成。借助推理流程，构建基于证据的图，并提出一种新型迭代方法用以定位推理细粒度错误。

Result: 在CURVE测试中，现有最先进的视频大模型（SoTA Video-LLMs）表现远低于人类准确率，主要错误集中在对文化视觉元素的识别。

Conclusion: CURVE为多文化视频理解提供更公平和具有挑战性的基准；当前AI模型在此类任务中表现较差，需要提升对多元文化语境的感知和推理能力。

Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural

</details>


### [66] [A continental-scale dataset of ground beetles with high-resolution images and validated morphological trait measurements](https://arxiv.org/abs/2601.10687)
*S M Rayeed,Mridul Khurana,Alyson East,Isadora E. Fluck,Elizabeth G. Campolongo,Samuel Stevens,Iuliia Zarubiieva,Scott C. Lowe,Michael W. Denslow,Evan D. Donoso,Jiaman Wu,Michelle Ramirez,Benjamin Baiser,Charles V. Stewart,Paula Mabee,Tanya Berger-Wolf,Anuj Karpatne,Hilmar Lapp,Robert P. Guralnick,Graham W. Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本文通过高分辨率数字化和测量，构建了超过13,200只地甲虫（Carabidae）的多模态数据集，显著提升了无脊椎动物形态性状数据库的数据量和可用性，推动了生态和AI分析在生物多样性保护中的应用。


<details>
  <summary>Details</summary>
Motivation: 无脊椎动物，尤其是地甲虫，在生态系统中占有重要地位，但现有全球形态性状数据库过于偏向脊椎动物和植物，导致高多样性无脊椎类群的生态分析受限。物理标本难以实现大规模共享和分析，因此亟需高效、标准化的数字化数据资源。

Method: 利用美国国家生态观测网（NEON）采集的地甲虫标本，通过高分辨率成像进行数字化，并为每只标本测量数字化鞘翅长度和宽度，将这些数据与人工测量结果进行比对验证其准确性。

Result: 构建了覆盖美国本土和夏威夷30个站点、超过13,200个标本的地甲虫高分辨率数字化数据集，自动性状提取与人工测量达到亚毫米精度，验证了方法的可靠性。

Conclusion: 本研究填补了无脊椎动物在全球性状数据库中的空白，为AI驱动的自动物种识别及性状研究奠定了坚实基础，有助于提升生物多样性监测与保护研究的效率和广度。

Abstract: Despite the ecological significance of invertebrates, global trait databases remain heavily biased toward vertebrates and plants, limiting comprehensive ecological analyses of high-diversity groups like ground beetles. Ground beetles (Coleoptera: Carabidae) serve as critical bioindicators of ecosystem health, providing valuable insights into biodiversity shifts driven by environmental changes. While the National Ecological Observatory Network (NEON) maintains an extensive collection of carabid specimens from across the United States, these primarily exist as physical collections, restricting widespread research access and large-scale analysis. To address these gaps, we present a multimodal dataset digitizing over 13,200 NEON carabids from 30 sites spanning the continental US and Hawaii through high-resolution imaging, enabling broader access and computational analysis. The dataset includes digitally measured elytra length and width of each specimen, establishing a foundation for automated trait extraction using AI. Validated against manual measurements, our digital trait extraction achieves sub-millimeter precision, ensuring reliability for ecological and computational studies. By addressing invertebrate under-representation in trait databases, this work supports AI-driven tools for automated species identification and trait-based research, fostering advancements in biodiversity monitoring and conservation.

</details>


### [67] [From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion](https://arxiv.org/abs/2601.10710)
*Cheng Chen,Yuyu Guo,Pengpeng Zeng,Jingkuan Song,Peng Di,Hang Yu,Lianli Gao*

Main category: cs.CV

TL;DR: 现有视觉-语言模型（VLMs）由于仅采用视觉编码器输出与大语言模型（LLM）输入之间的单一静态连接，导致视觉特征瓶颈，进而限制其深层融合和推理能力。本文提出跨层注入（CLI）框架，能为两种模态建立动态多对多联系，大幅提升多模态理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs模型对视觉和语言的结合过于粗糙，无法充分对齐层次化的视觉知识，尤其难以兼顾局部细节与全局语义，降低了多模态推理与表达能力。因此，亟需新方法以实现视觉特征的高效对接与全面融合。

Method: 提出Cross-Layer Injection（CLI）框架。CLI包含自适应多投影（AMP）模块，可融合不同视觉层特征，以及自适应门控融合（AGF）机制，根据LLM解码时的动态上下文，选择性地将最相关的视觉信息注入LLM。CLI结构轻量，参数效率高。

Result: 将CLI集成到LLaVA-OneVision和LLaVA-1.5中，并在18个多样化基准测试上进行实验，CLI显著提升了模型的多模态性能，证明了其高效性和通用性。

Conclusion: CLI作为一种可扩展的方案，为LLM按需访问全层级视觉信息提供了有效路径，推动了多模态模型对于复杂视觉-语言场景的深入理解与推理能力。

Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

</details>


### [68] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型（diffusion-based）的图像对象内在属性编辑方法Alterbute，可在保持对象身份和场景一致性的情况下，灵活修改对象的颜色、纹理、材质甚至形状。


<details>
  <summary>Details</summary>
Motivation: 目前关于图像对象属性编辑的方法，在保护对象身份上存在不足：无监督方法无法保证身份的完整保留，而强监督方法又限制了属性变化的幅度。因此需要一种既能保护身份又能实现多样属性变化的新方法。

Method: Alterbute采取了两项关键措施：（1）使用松弛的训练目标，使模型在给定身份参考图、文本提示（描述目标属性）、原始背景和对象掩码的条件下，能同时改变对象的内在和外在属性。而在推理时通过重用原始背景和掩码，实现只改变内在属性；（2）引入细粒度视觉实体（Visual Named Entities, VNEs）作为身份类别，结合视觉-语言模型自动标注身份和属性，实现大规模、可扩展的身份保护训练。

Result: Alterbute在身份保护前提下，对对象内在属性（如颜色、材质、形状等）编辑任务上，优于现有主流方法。

Conclusion: Alterbute方法在实现灵活编辑对象内在属性的同时，有效保持了对象的身份和场景一致性，并具备良好的扩展性和实际应用潜力。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


### [69] [WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments](https://arxiv.org/abs/2601.10716)
*Xuweiyi Chen,Wentao Zhou,Zezhou Cheng*

Main category: cs.CV

TL;DR: WildRayZer是一个自监督的动态场景新视角合成框架，通过残差分析解决了动态内容带来的多视图一致性问题，并在新构建的大规模真实动态数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的新视角合成（NVS）模型多依赖于场景静态假设，但在真实环境中，摄像机与物体的动态运动会导致模型失效，出现虚影、假几何等问题，因此需要能够处理动态场景的NVS方法。

Method: WildRayZer通过分析-合成测试，使用仅摄像机渲染的静态模型拟合刚性结构，并对残差进行分析以获得伪运动掩码；进一步蒸馏运动估计器，并利用运动掩码对输入和损失进行门控，实现专注于背景补全的监督。同时，作者构建了两个大规模动态场景数据集D-RE10K和D-RE10K-iPhone以支持训练和评价。

Result: 在新构建的数据集上，WildRayZer在动态区域去除和整体新视角合成质量上均显著超过了优化式和前馈式主流基线方法，且仅需单次前馈推理。

Conclusion: WildRayZer有效解决了动态环境中新视角合成的难题，为动态场景下的三维重建和渲染提供了高效、鲁棒的自监督方案，为实际应用落地奠定了基础。

Abstract: We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue](https://arxiv.org/abs/2601.09713)
*Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent*

Main category: cs.CL

TL;DR: 本文提出ProUtt方法，通过意图树与偏好数据合成，提升了人机对话中预测用户下一步发言的准确性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前通用大模型本地部署成本高，API方案存隐私隐患；常用用户模拟器只模仿说话风格，未能有效推进对话，且缺乏对用户意图推理过程的建模和偏好/非偏好推理过程的生成。

Method: 提出ProUtt：将对话历史转换为意图树，显式建模用户意图推理轨迹，从“利用”和“探索”两个角度预测下一可能路径，再通过扰动或修改意图树路径，为未来不同回合构建偏好与非偏好推理过程，生成高质量数据集用于训练小型、任务特定的LLM。

Result: 在四个基准数据集上，通过LLM和人工评估，ProUtt在预测用户下一发言准确性上全面优于现有数据合成方案、用户模拟器和主流商业大模型API。

Conclusion: ProUtt不仅显著提升主动式下一轮发言预测效果，还释放了代码与数据，促进后续研究。

Abstract: Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.

</details>


### [71] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文评估了多步骤智能体工作流（如递归分解、进化搜索等）是否相比单步提示更能生成新颖且可行的科研计划。结果显示，分解与长上下文流程在新颖性上大幅优于单步反思方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型应用于科研辅助时常存在“智能剽窃”，即用新术语包装旧点子的现象，限制了AI生成内容的原创性。本研究旨在探索更复杂的AI推理流程能否提升科研创意的原创性和可行性。

Method: 作者设计并对比了五种大模型推理架构：基于反思的迭代优化、Sakana AI v2进化算法、Google Co-Scientist多智能体框架、GPT深度递归分解、Gemini 3 Pro多模态长上下文流程。每种架构各生成30份科研方案，按新颖性、可行性与影响力进行评分对比。

Result: 分解型与长上下文型流程在新颖性评分上平均达到4.17/5，显著优于反思型（2.33/5）；高分流程兼具创造性与可行性，不同研究领域存在一定表现差异。

Conclusion: 结构化、分阶段、多智能体的AI科研辅助流程能有效提升点子原创性与质量，为AI辅助科学研究开辟了新方向。

Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [72] [Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents](https://arxiv.org/abs/2601.09715)
*Adam Bradley,John Hastings,Khandaker Mamun Ahmed*

Main category: cs.CL

TL;DR: 本论文介绍了Axlerod，一种AI驱动的智能保险对话助手，在提高保险代理效率方面表现出色，实现了高准确率和较低的查询时间。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术，特别是对话式智能体的兴起，保险行业亟需更高效的工具来辅助代理人完成复杂流程，如保单推荐与理赔分流。提升自动化和智能化水平，减少人工操作，是当前行业的核心诉求。

Method: 作者提出并实现了Axlerod系统，综合使用了自然语言处理（NLP）、检索增强生成（RAG）方法和行业专属知识库，使系统能够准确理解用户意图，实时访问结构化保险数据，并做出上下文相关的智能回复。

Result: 实验证明，Axlerod在保单检索任务中的准确率达93.18%，同时将平均搜索时间缩短2.42秒，显示出系统的高效性和实用性。

Conclusion: Axlerod显著提升了独立保险代理人的工作效率，是保险科技企业级AI应用的重要进展，尤其体现为以辅助代理人而非面向终端消费者的架构创新。

Abstract: The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.

</details>


### [73] [Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research](https://arxiv.org/abs/2601.09716)
*Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal*

Main category: cs.CL

TL;DR: 本文综述了塞内加尔六种官方语言在自然语言处理（NLP）领域的进展和挑战，并提出了推动其数字化和可持续发展的路线图。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在NLP领域中代表性不足，特别是塞内加尔的官方语言。该论文旨在总结现有进展，揭示主要挑战，并推动这些语言在数字世界的进一步发展。

Method: 调研、整合和分析了关于六种官方语言的NLP相关工作和资源，关注文本规范化、机器翻译和语音处理等方向，并建立了GitHub仓库集中资源，加强协作与可重复性。

Result: 梳理了语言的数字准备度、数据与工具短缺、现有研究成果及应用场景，并强调了社会科学领域应用价值，同时整合了资源供学界和业界使用。

Conclusion: 提出了以社区为中心、可持续发展的NLP生态系统建设方向，强调伦理数据治理、开放资源和跨学科协作以推动塞内加尔语言的数字化和多元研究。

Abstract: Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.

</details>


### [74] [SALP-CG: Standard-Aligned LLM Pipeline for Classifying and Grading Large Volumes of Online Conversational Health Data](https://arxiv.org/abs/2601.09717)
*Yiwei Yan,Hao Li,Hua He,Gong Kai,Zhengyi Yang,Guanfeng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）的隐私风险分类与分级方法（SALP-CG），可自动对在线医疗对话数据进行隐私敏感性评估，兼顾政策合规与实用性。


<details>
  <summary>Details</summary>
Motivation: 随着在线医疗咨询生成大量嵌入有受保护健康信息的对话数据，如何自动化、标准化地识别和评估信息敏感程度，成为数据治理的关键需求。然而，现有方法缺乏统一的标准和自动化、可靠的敏感性分类手段。

Method: 提出SALP-CG管道，结合少量样例指导、JSON Schema约束解码及确定性高风险规则，并遵循国标GB/T 39725-2020进行分类与分级。该方法适配多种LLM后端，确保分类与分级的准确性和合规性。

Result: 在MedDialog-CN基准测试集上，SALP-CG取得了高实体识别数、高模式（schema）合规率和敏感性分级精度，最强模型在最高等级预测任务上达到micro-F1=0.900。敏感等级分布显示低中敏感项虽常见但组合可致去标识风险，高敏感项（4-5级）虽罕见但危害大。

Conclusion: SALP-CG为在线医疗健康对话数据的敏感信息分类与风险分级提供了实用、合规的自动化工具，有助于健康数据治理和隐私保护。

Abstract: Online medical consultations generate large volumes of conversational health data that often embed protected health information, requiring robust methods to classify data categories and assign risk levels in line with policies and practice. However, existing approaches lack unified standards and reliable automated methods to fulfill sensitivity classification for such conversational health data. This study presents a large language model-based extraction pipeline, SALP-CG, for classifying and grading privacy risks in online conversational health data. We concluded health-data classification and grading rules in accordance with GB/T 39725-2020. Combining few-shot guidance, JSON Schema constrained decoding, and deterministic high-risk rules, the backend-agnostic extraction pipeline achieves strong category compliance and reliable sensitivity across diverse LLMs. On the MedDialog-CN benchmark, models yields robust entity counts, high schema compliance, and accurate sensitivity grading, while the strongest model attains micro-F1=0.900 for maximum-level prediction. The category landscape stratified by sensitivity shows that Level 2-3 items dominate, enabling re-identification when combined; Level 4-5 items are less frequent but carry outsize harm. SALP-CG reliably helps classify categories and grading sensitivity in online conversational health data across LLMs, offering a practical method for health data governance. Code is available at https://github.com/dommii1218/SALP-CG.

</details>


### [75] [StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model](https://arxiv.org/abs/2601.09718)
*Jing-Yi Zeng,Guan-Hua Huang*

Main category: cs.CL

TL;DR: 该论文提出了高效构建统计学领域专用大语言模型（LLM）的多阶段训练流程，并展示如何在资源有限条件下，结合领域专用与通用推理能力，最终开发出StatLLaMA模型。


<details>
  <summary>Details</summary>
Motivation: 目前主流LLM往往面临在通用推理与领域专精之间的权衡，且资源受限时难以高效进行领域定制。该研究旨在探索低资源情况下，如何有效将通用LLM转化为某一领域（统计学）表现优秀的模型，并优化训练流程以保证领域与通用能力的平衡。

Method: 作者基于轻量LLaMA-3.2-3B系列模型，系统比较了三种多阶段训练流程：1. 直接用基础模型（无指令能力）做迁移，2. 后续加指令微调，3. 以已具强推理能力的指令模型作为基础进行领域专属训练（包括持续预训练、监督微调SFT、人类反馈强化学习RLHF对齐、下游任务适配）。并深入评估了领域能力、推理能力的权衡，以及RLHF的优化策略。

Result: 通过大量实验证明，从基础模型出发，即使大量指令微调或RLHF对齐，对统计推理意义有限；反之用LLaMA-3.2-3B-Instruct作为基础，则可以高效实现领域定制。额外发现直接偏好优化有利于稳定有效的RLHF，并明确提出下游微调需极低强度，以防止灾难性遗忘。最终StatLLaMA模型在数学、常识推理与统计专长上均获得优异均衡表现。

Conclusion: 采用具备强通用推理能力的指令LLM作为基础，加以多阶段流程及谨慎微调，是低资源下开发领域大模型的有效蓝图。StatLLaMA为统计学领域高效专属LLM开发提供了实证参考和可复用代码。

Abstract: This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.

</details>


### [76] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: 提出了一种新的归一化方法BHyT，能提升大语言模型的训练稳定性与效率，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型广泛采用的Pre-Layer Normalization（Pre-LN）虽然保证了稳定训练和迁移能力，但效率低并且深层模型易于不稳定。效率导向的无归一化方法虽然加快了速度，但在更深层模型中稳定性仍较差，亟需一种既稳定又高效的新方法。

Method: 提出Bounded Hyperbolic Tanh (BHyT) 方法，即用tanh非线性激活结合显式、数据驱动的输入边界限制，将激活值控制在非饱和区间。并只在每个block内精确计算一次统计量，剩下采用轻量方差近似，减少了计算量。理论上保证模型深度增长时的稳定性。

Result: BHyT在预训练实验中表现出更好的稳定性和效率，训练速度平均提升15.8%，生成吞吐量提升4.2%，推理效果和鲁棒性也不逊色甚至优于RMSNorm等现有主流方法。

Conclusion: BHyT为大语言模型训练提供了兼顾效率和稳定性的新选择，有望替代Pre-LN和RMSNorm，推动更高效可靠的深层模型训练。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


### [77] [Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering](https://arxiv.org/abs/2601.09720)
*Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti*

Main category: cs.CL

TL;DR: 该论文提出了一个不确定性感知的动态知识图谱（KG）QA系统，提高了问题回答系统的可靠性，特别适用于医疗等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的QA系统难以处理信息的不完整性、不确定性和随时间变化的问题，特别是在医疗等高风险领域下，错误或低置信度的回答可能导致严重后果，因此需要更可靠、不确定性感知的QA框架。

Method: 作者提出的框架包括三部分：（1）动态构建可演化的知识图谱，（2）引入置信度评分和不确定性感知的检索机制，以及（3）开发交互界面，辅助用户理解和验证QA结果。并以电子健康记录为例，构建个性化的动态KG，并可视化不同访视下的不确定性。

Result: 系统可以实时展示动态知识图谱，并为每条关系标注置信度。实验验证了在医疗问答尤其是死亡率预测任务中，引入不确定性感知后，QA系统对高风险应用场景下的可靠性有明显提升。

Conclusion: 不确定性感知的动态KG系统能帮助用户更直观地理解知识和答案的不确定性，增强QA在高风险领域的可靠性和可解释性，展现了其在医疗等关键应用中的广泛前景。

Abstract: Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.

</details>


### [78] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

TL;DR: 本文评估了大语言模型（LLMs）在面临焦虑型家长施加的对抗压力下的安全性，发现部分模型存在安全隐患，尤其是在紧急情况识别和儿童癫痫诊断方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs已广泛应用于医疗咨询，但真实、有压力情境（如家长焦虑导致的对抗式提问）下的安全性尚未系统研究。此前评估多关注中性场景，忽略了实际应用中用户可能对模型安全机制发起挑战的情况。

Method: 本研究使用包含300条问题（150条真实，150条对抗性，涵盖10类儿童健康主题）的PediatricAnxietyBench基准，评估了三种模型（Llama-3.3-70B、Llama-3.1-8B、Mistral-7B）对这些问题的表现。采用0-15分安全打分体系，包括自我约束、建议转诊、模糊处理、急症识别与不过度处方等维度，用配对t检验和自助法置信区间分析结果。

Result: 各模型安全均分在9.70至10.39之间。Llama-3.1-8B在安全性上优于体量更大的Llama-3.3-70B，Mistral-7B在对抗场景下安全提升最明显。Llama-3.3-70B有8%的安全失误，且所有模型在癫痫类问题上的误判率高达33%。模型提供模糊答案的能力与安全性正相关。

Conclusion: LLM安全性受模型对齐和架构影响大于规模，小模型在特定压力下更安全。随着训练优化，整体鲁棒性有所提升，但在紧急情况识别方面仍存在重大短板。因此，当前LLM不适合做临床分诊，研究结果为AI医疗模型的安全测试与选用提供了依据，并向社会开放了测试基准。

Abstract: Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [79] [ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language](https://arxiv.org/abs/2601.09722)
*Franciszek Górski,Andrzej Czyżewski*

Main category: cs.CL

TL;DR: 本文提出了一种注释框架，利用多语种大模型(Llama3.1)标注大量波兰语医学文本，辅助下游医学文本分类任务。


<details>
  <summary>Details</summary>
Motivation: 波兰语医学文本数据稀缺，手动注释成本高，难以支撑临床多分类器开发。

Method: 收集五类医学语料，利用多语种大模型Llama3.1进行自动注释，人工少量验证以生成测试集，利用这些数据训练和评测DistilBERT、BioBERT和HerBERT三种BERT家族分类器。

Result: DistilBERT取得最佳分类效果，所有临床类别F1均>0.80，有三类F1>0.93。模型推理速度快，资源消耗远低于大型模型。

Conclusion: 利用多语种大模型进行波兰语医学文本自动注释可高效赋能小规模BERT类模型，实现高性能医学文本分类，是资源受限场景下大模型的优良替代方案。

Abstract: In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.

</details>


### [80] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

TL;DR: 论文提出了SagaScale，这是一个基于小说、面向长文本、更真实并高质量的基准，用于评估大语言模型（LLMs）对超长文本的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准存在任务不真实、数据难以扩展、数据质量不足等问题，难以全面评估LLMs在超长文本上的真实表现。

Method: SagaScale利用自动化数据收集管线，从完整小说中生成高质量问答对，且全流程仅在数据构建阶段使用外部资源（如Wikipedia），不在评测阶段提供。支持中英文，评测文本长度远超现有基准（英文250K+、中文320K+ tokens）。同时评测了12个前沿LLMs和三种主流长文本处理方案：Naïve RAG、Agentic RAG和直接上下文输入。

Result: 实验发现，直接提供全部上下文输入能远超其他方法，大部分LLM仍难以处理巨长文本，但Gemini-2.5-Pro表现突出；Agentic RAG可以有效缓解Naïve RAG的检索瓶颈。

Conclusion: SagaScale弥补了长文本基准的多项不足，将推动更真实、更大规模的LLM长上下文理解评估。该基准和数据管线已开源，便于未来研究的开展。

Abstract: Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


### [81] [Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions](https://arxiv.org/abs/2601.09724)
*Katherine Elkins,Jon Chun*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估大语言模型在伦理判断上一致性的框架，发现模型对语法变体（如否定句和条件结构）较为脆弱，并推荐将该框架纳入模型安全审核流程。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已广泛应用于关键决策场景，但其在面对语法上细微差别的提示（如否定、条件句）时伦理判断的一致性尚未被系统研究。鉴于实际部署场景中提示多样性不可避免，评估其伦理稳健性极具实用价值。

Method: 作者提出并实现了Syntactic Framing Fragility（SFF）评估框架，通过逻辑极性归一化（LPN）来消除语义漂移，专注分析纯语法层面对模型伦理判断一致性的影响。他们对来自中美的23个主流模型及开源模型进行了审计，涵盖14个情境、4种语法变体，总计39975个决策，并引入链式推理（chain-of-thought）作为缓解策略。

Result: 实验发现，模型在语法极性（正向或否定）变化时伦理判断极不一致，开源模型的脆弱性是商用模型两倍以上，部分模型被否定句极度误导。在金融和商业情景中风险最高，链式推理策略明显缓解了该问题。

Conclusion: 论文确认大语言模型在语法一致性上的伦理鲁棒性不足，强调SFF式审计是安全评估的重要补充，并建议纳入标准审核流程以提升部署模型的伦理可靠性。

Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with "should not." We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.

</details>


### [82] [Assessing and Improving Punctuation Robustness in English-Marathi Machine Translation](https://arxiv.org/abs/2601.09725)
*Kaustubh Shivshankar Shejole,Sourabh Deoghare,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了Virām，这是首个用于评估英译马拉地语机器翻译标点稳健性的诊断基准，并评估了两种提升方法。研究发现，针对此任务微调的模型和流水线方法均优于标准基线，大型语言模型仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 标点符号在书面语中对于意义和结构消歧至关重要，但许多机器翻译系统对标点的鲁棒性不足，尤其是在资源较低的语言如马拉地语中。本研究旨在填补缺乏系统评测英文到马拉地语翻译标点稳健性的空白，并推动该领域发展。

Method: 作者构建了Virām数据集，包括54个人工整理、具标点歧义的句子，作为评测基准。针对英文到马拉地语翻译，评估了“还原后翻译”流水线方法和在带标点多样性数据上直接微调的方法，并同现有大型语言模型进行对比。

Result: 微调后的专用模型及流水线方式在Virām基准上，翻译质量明显优于传统未处理模型。定性分析显示，基础模型易因标点处理不当产生明显误译，精细化模型则在准确性和可靠性上均有较大提升。同时，现有大型语言模型在应对该类标点歧义文本时效果不及任务特定模型。

Conclusion: 专门针对标点歧义训练的机器翻译系统能够大幅提升英译马拉地语的翻译质量与鲁棒性。目前的大型语言模型尚不足以胜任此任务，需进一步研究改进。

Abstract: Punctuation plays a critical role in resolving semantic and structural ambiguity in written language. Machine Translation (MT) systems are now widely applied across diverse domains and languages, including many low-resource settings. In this work, we focus on Marathi, a low- to middle-resource language. We introduce Virām, the first diagnostic benchmark for assessing punctuation robustness in English-to-Marathi machine translation, consisting of 54 manually curated, punctuation-ambiguous instances. We evaluate two primary strategies for enhancing reliability: a pipeline-based restore-then-translate approach and direct fine-tuned on punctuation-varied data. Our results demonstrate that specialized fine-tuned models and pipeline systems significantly improve translation quality over standard baselines on the Virām benchmark. Qualitative analysis reveals that the original model may result in wrong translations leading to wrong interpretations, while fine-tuned models significantly improve overall reliability. Furthermore, we find that current Large Language Models (LLMs) lag behind these task-specific approaches in preserving meaning for punctuation-ambiguous text, thus necessitating further research in this area.

</details>


### [83] [Forgetting as a Feature: Cognitive Alignment of Large Language Models](https://arxiv.org/abs/2601.09726)
*Hien Tran,Quinten Steenhuis,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本文认为大型语言模型（LLM）在推理过程中出现的遗忘现象并非缺陷，而是一种类人认知机制。通过将LLM的推理建模为指数衰减主导的概率记忆过程，研究表明遗忘有助于模型在稳定性和适应性之间平衡。进一步提出了概率记忆提示，以提升模型长程推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM推理能力的评估通常以完美贝叶斯推断为标准，但这些模型在上下文推理中的“遗忘”被普遍视为不足。本文受到人类记忆动态的启发，重新思考遗忘在推理中的作用，探索其是否是一种有益的认知机制。

Method: 作者提出将LLM推理过程视作受指数衰减约束的概率记忆过程，并建立了一个基准套件，包括时间推理、概念漂移适应和联想回忆等任务，用以比较模型与人类认知行为。通过实验证明LLM的遗忘率和人类记忆中的效率权衡相似。

Result: 实验结果显示，LLM表现出的遗忘速率与人类的记忆效率权衡类似。此外，引入概率记忆提示的策略能够有效模拟人类记忆衰减特性，提升模型在长时推理任务上的表现。

Conclusion: 作者认为遗忘应被视为适应性智能中的合理机制，而非简单缺陷。受人类记忆机制启发的提示方法能提升模型推理表现，促进模型稳定性与适应性之间更佳的平衡。

Abstract: Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.

</details>


### [84] [SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability](https://arxiv.org/abs/2601.10455)
*Ruochen Li,Kun Yuan,Yufei Xia,Yue Zhou,Qingyu Lu,Weihang Li,Youxiang Zhu,Nassir Navab*

Main category: cs.CL

TL;DR: 本文引入了一个外科手术规划多中心评估基准，揭示了现有评测方法对视觉-语言大模型（VLMs）在安全关键场景中的不可靠性，并提出基于专家规则的规划正确性定义，展示结构性知识显著提升规划表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估协议无法准确反映VLMs在外科手术等安全关键任务中的真实能力，缺乏有效的规划正确性定义和判据，因此需要开发更科学的评测体系，以保障临床安全和可靠性。

Method: 提出‘由阶段—目标可满足性’定规划正确性，依据专家规则判定手术方案的有效性，并构建包含有效/无效手术方案（涵盖顺序与内容错误）的多中心元评价基准；比较常见的序列相似性指标与基于规则的目标可满足性指标在手术规划中的表现；并进一步在不同受约束的场景下分析Video-LLMs的能力和失效来源。

Result: 发现序列相似性指标无法有效甄别手术方案的有效性，常出现惩罚有效方案、漏检无效方案的问题；基于规则的目标可满足性指标具备更高的精度用于参考评估；结构性知识显著提升大模型规划质量，而单纯语义引导仅在配合结构约束和大模型时有效。

Conclusion: 现有自动化评测指标（如序列相似性）不足以胜任复杂手术规划评价，需引入基于专家规则的高精度指标以保障安全性。结构性知识是提升VLMs在手术规划等关键场景表现的关键，未来应聚焦结合结构与语义信息的模型优化与评测方法。

Abstract: Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.

</details>


### [85] [SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis](https://arxiv.org/abs/2601.09727)
*Sauhard Dubey*

Main category: cs.CL

TL;DR: 该论文提出SciNets方法，利用概念图与多跳推理，实现跨领域科学机制合成，并系统比较多种推理路径与基于检索的方法，揭示深度与多样性和稳定性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 科学文献高度碎片化，跨领域机制解释难以通过现有检索系统或一般大模型完成，现有方法在推理深度与结构可控性上有限，因此需要新方法连接不同文献的机理知识。

Method: 将机制合成建模为基于概念图的多跳推理问题。针对特定科学问题与紧凑语料，构建有向概念图，通过多种路径（最短路、k最短路、随机游走、检索增强LLM等）推理连接平时文献中很少共现的概念。

Result: 系统比较不同推理方式，提出基于行为的评估框架（推理深度、多样性和稳定性）。发现引入图结构控制能实现可控多跳推理，同时当推理更深且多样时，结果更不稳定，最短路径稳定但保守。

Conclusion: 当前图与大模型结合用于科学机制合成时，存在深度与多样性和稳定性之间的固有权衡，这为进一步改进跨文献科学合成提供了系统性行为洞察。

Abstract: Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.

</details>


### [86] [Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens](https://arxiv.org/abs/2601.09728)
*Meicong Zhang,Tiancheng su,Guoxiu He*

Main category: cs.CL

TL;DR: 该论文提出STIG方法，使大语言模型能一次性生成高质量的学术引言，无需依赖传统的多步代理流程。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理工作流的方法在引言生成任务上表现不佳，常出现推理链过长、错误累积和结构不连贯等问题。引言写作本身要求逻辑严密、结构连贯，而现有方法难以满足。

Method: 作者提出一种新方法——引言生成阶段标记（STIG）。STIG将传统多阶段流程转化为显式阶段信号，直接嵌入LLM推理，并通过指令微调来训练模型，使其学习阶段信号与文本功能的映射及各阶段之间的逻辑顺序。这样，模型能在一次推理中根据阶段信号合理生成各部分内容，无需复杂的外部工作流。

Result: 实验显示，STIG方法可以使模型在单次推理下生成包含多个逻辑阶段的文本。在语义相似度和句子级结构合理性等指标上，STIG显著优于传统代理流程和其他基线方法。

Conclusion: STIG方法简化了引言生成流程，提升了文本逻辑和连贯性，为大模型应用于学术写作提供了更高效、更优质的解决方案。

Abstract: In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.

</details>


### [87] [Enhancing Business Analytics through Hybrid Summarization of Financial Reports](https://arxiv.org/abs/2601.09729)
*Tohida Rehman*

Main category: cs.CL

TL;DR: 本文提出了一种混合摘要框架，结合抽取式和生成式方法，高效准确地对财报电话会议内容进行简洁总结。


<details>
  <summary>Details</summary>
Motivation: 财报和盈利沟通包含大量结构化及半结构化信息，人工分析低效且易产生主观误差，亟需高效、准确的自动化摘要工具来辅助商业决策。

Method: 构建了两阶段摘要流程：首先用LexRank算法抽取关键句子，然后用微调的BART与PEGASUS模型摘要；同时微调Longformer Encoder-Decoder模型以直接处理长文本。系统评估采用ROUGE、METEOR等标准自动化指标及金融领域专用的SciBERTScore和FinBERTScore，还用实体级指标衡量事实准确性。

Result: 长文本模型表现最佳，混合摘要框架在计算资源受限下也能取得有竞争力的性能，并显著提升事实一致性。

Conclusion: 该研究为有效提炼冗长金融文本为有用洞察的信息摘要系统提供了可行方案，并推动了实际应用发展。

Abstract: Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.
  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.

</details>


### [88] [Clinical Document Metadata Extraction: A Scoping Review](https://arxiv.org/abs/2601.09730)
*Kurt Miller,Qiuhao Lu,William Hersh,Kirk Roberts,Steven Bedrick,Andrew Wen,Hongfang Liu*

Main category: cs.CL

TL;DR: 本文对临床文档元数据自动抽取的研究进行了综述，梳理了主要方法、应用趋势及存在的不足。


<details>
  <summary>Details</summary>
Motivation: 临床文档包含丰富元数据（如类型、结构、作者角色等），对数据解释和利用至关重要。但实际文档类型多样且随时间变化，给元数据标准化与抽取带来挑战。研究希望总结当前自动化抽取技术的进展与不足。

Method: 作者按PRISMA-ScR指南回顾了2011-2025年间的文献，筛选出67项相关工作，归类为方法研究、下游应用及元数据组成分析，并梳理了数据集、方法及领域应用的变化趋势。

Result: 发现方法逐步从基于规则和传统机器学习向基于transformer的大模型演进，人工特征依赖减少。公开有标签的数据仍稀缺（结构段落数据除外）。研究涵盖多种应用，但存在数据和方法的局限。大语言模型的兴起促使方法更具通用性。

Conclusion: 未来研究有望扩展到更丰富的文档元数据表示，并在临床实践中更广泛集成和应用。

Abstract: Clinical document metadata, such as document type, structure, author role, medical specialty, and encounter setting, is essential for accurate interpretation of information captured in clinical documents. However, vast documentation heterogeneity and drift over time challenge harmonization of document metadata. Automated extraction methods have emerged to coalesce metadata from disparate practices into target schema. This scoping review aims to catalog research on clinical document metadata extraction, identify methodological trends and applications, and highlight gaps. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines to identify articles that perform clinical document metadata extraction. We initially found and screened 266 articles published between January 2011 and August 2025, then comprehensively reviewed 67 we deemed relevant to our study. Among the articles included, 45 were methodological, 17 used document metadata as features in a downstream application, and 5 analyzed document metadata composition. We observe myriad purposes for methodological study and application types. Available labelled public data remains sparse except for structural section datasets. Methods for extracting document metadata have progressed from largely rule-based and traditional machine learning with ample feature engineering to transformer-based architectures with minimal feature engineering. The emergence of large language models has enabled broader exploration of generalizability across tasks and datasets, allowing the possibility of advanced clinical text processing systems. We anticipate that research will continue to expand into richer document metadata representations and integrate further into clinical applications and workflows.

</details>


### [89] [Geometric Patterns of Meaning: A PHATE Manifold Analysis of Multi-lingual Embeddings](https://arxiv.org/abs/2601.09731)
*Wen G Gong*

Main category: cs.CL

TL;DR: 本论文提出了一个多层次分析框架，通过可视化工具Semanscope，结合PHATE流形学习，对多语种词向量的语义几何结构进行深入分析。研究发现，不同层级和语种的嵌入空间展示出系统性模式，同时暴露出现有模型的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多语种嵌入模型虽在自然语言处理任务中表现优异，但其在多层级（如子结构、字符、单词、数值等）上的语义表达和几何结构仍不明确。作者希望揭示现有词向量模型在捕捉语义及结构信息上的优劣，为模型优化提供依据。

Method: 设计了Semanscope工具，基于PHATE流形学习方法，对来自不同语言系统和概念层次的数据集进行空间可视化和结构分析，涵盖汉字部件、拉丁字母、不同语义领域单词，以及阿拉伯数字等。

Result: （1）在汉字部件层级，结构成分（如偏旁）无法被模型很好区分语义与结构，出现几何坍缩。（2）字符层面，不同文字系统的几何表现不同。（3）词层面，英文、中文、德文在20个语义领域内内容词展现出聚类-分支结构。（4）数字层面，阿拉伯数字以螺旋轨迹而非聚类排布，挑战传统分布式语义假设。

Conclusion: PHATE流形学习不仅适用于嵌入空间的几何分析，也是评估模型捕捉语义关系能力的重要工具。现有模型对结构和语义的区分能力有限，需要进一步改进以提升语义表达力。

Abstract: We introduce a multi-level analysis framework for examining semantic geometry in multilingual embeddings, implemented through Semanscope (a visualization tool that applies PHATE manifold learning across four linguistic levels). Analysis of diverse datasets spanning sub-character components, alphabetic systems, semantic domains, and numerical concepts reveals systematic geometric patterns and critical limitations in current embedding models. At the sub-character level, purely structural elements (Chinese radicals) exhibit geometric collapse, highlighting model failures to distinguish semantic from structural components. At the character level, different writing systems show distinct geometric signatures. At the word level, content words form clustering-branching patterns across 20 semantic domains in English, Chinese, and German. Arabic numbers organize through spiral trajectories rather than clustering, violating standard distributional semantics assumptions. These findings establish PHATE manifold learning as an essential analytic tool not only for studying geometric structure of meaning in embedding space, but also for validating the effectiveness of embedding models in capturing semantic relationships.

</details>


### [90] [Benchmarking Cross-Lingual Semantic Alignment in Multilingual Embeddings](https://arxiv.org/abs/2601.09732)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 提出了一种用于评估多语言嵌入模型跨语义对齐的新指标Semantic Affinity（SA）；发现只有带有翻译监督的BERT类模型在语义对齐效果最好，而大规模LLM及无监督BERT模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前有大量多语言嵌入模型，但缺乏有效方法区分哪些模型真正实现了跨语言语义对齐，而不仅仅是因为语种特定特性取得任务表现。现有评测基准无法揭示语义对齐的深层不足。

Method: 提出Semantic Affinity（SA）指标，通过余弦距离计算跨语言与语言内语义分散比，并结合PHATE可视化技术，形成Semanscope框架，对13个模型在4个数据集上进行52组实验。

Result: 实验发现：1）带翻译监督的BERT嵌入（如LaBSE、USE、S-BERT）SA达0.68-0.70，表现最佳；2）各类大规模LLM嵌入（0.6B-8B参数）SA稳定在0.55-0.61，提升有限；3）仅用无监督BERT训练目标（如mBERT、XLM-R）SA低于0.50，语义对齐能力差。进一步发现训练目标比模型规模和结构对对齐效果影响更大；

Conclusion: 该工作明确证明只有翻译监督才可实现高质量多语言语义对齐，模型结构及扩大参数规模作用有限，为多语言模型选型提供了新评判标准。

Abstract: With hundreds of multilingual embedding models available, practitioners lack clear guidance on which provide genuine cross-lingual semantic alignment versus task performance through language-specific patterns. Task-driven benchmarks (MTEB) may mask fundamental alignment shortcomings. We introduce Semantic Affinity (SA), a bounded (between 0 and 1) metric measuring inter-lingual to intra-lingual spread ratio using cosine distance, combined with PHATE visualization in our Semanscope framework. Benchmarking 13 models across 4 datasets (52 experiments) reveals a three-tier structure: (1) Top BERT models (LaBSE SA = 0.70, USE SA = 0.68, S-BERT SA = 0.68) achieve strong alignment via translation-pair supervision; (2) LLM embeddings plateau at SA between 0.55 and 0.61 regardless of 0.6 B to 8 B scale; (3) MLM-only BERT models (mBERT, XLM-R, SA < 0.50) fail despite more than 100 language training. Training objective, not architecture or scale, determines alignment. Oracle Bone primitives (1200 BCE) expose semantic drift-models learn corpus patterns rather than cognitive primitives. This work provides semantic benchmarking to help practitioners select quality multilingual embeddings from hundreds of available models, showing cross-lingual alignment requires explicit translation supervision, not merely model scale or multilingual data.

</details>


### [91] [Closing the Data Loop: Using OpenDataArena to Engineer Superior Training Datasets](https://arxiv.org/abs/2601.09733)
*Xin Gao,Xiaoyang Wang,Yun Zhu,Mengzhang Cai,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的、系统化的数据集构建范式，即利用OpenDataArena（ODA）实现闭环的数据集优化，有效提升大模型监督微调（SFT）集的质量。通过在数学推理和多领域指令任务上实证，ODA构建的数据集在准确性和数据效率上都优于常规模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的微调数据集构建主要依赖经验法则，缺乏对单样本对模型贡献的系统性理解，且数据集优化过程不透明、不科学。论文推动从零散、人工的样本聚合范式转向数据驱动、反馈式的系统性构建。

Method: 提出基于OpenDataArena的平台，融合价值锚定打分和多维数据分析，将评测指标反馈自动转化为数据集优化信号。具体实现包括：1）数学推理数据集ODA-Math-460k，采用两阶段难度感知筛选；2）多领域数据集ODA-Mixture，采用“锚点-修补”策略提升多样性和性能。

Result: 1）ODA-Math-460k在AIME、HMMT等数学推理基准上取得SOTA成绩；2）ODA-Mixture在多个任务上优于更大规模的开源基线模型，验证了ODA框架下数据集的小样本优势和广泛适用性。

Conclusion: ODA方法提升了数据集的透明度、针对性和效率，推动了以数据为中心的大模型训练范式，从而以高质量、有效率的数据集驱动模型性能提升。

Abstract: The construction of Supervised Fine-Tuning (SFT) datasets is a critical yet under-theorized stage in the post-training of Large Language Models (LLMs), as prevalent practices often rely on heuristic aggregation without a systematic understanding of how individual samples contribute to model performance. In this report, we propose a paradigm shift from ad-hoc curation to a closed-loop dataset engineering framework using OpenDataArena (ODA), which leverages value-anchored rankings and multi-dimensional analysis to transform value benchmarking into feedback signals guiding dataset construction. We instantiate this methodology through two new datasets: \textbf{ODA-Math-460k}, a specialized mathematics reasoning dataset that utilizes a novel two-stage difficulty-aware pipeline to achieve State-of-the-Art (SOTA) results on benchmarks such as AIME and HMMT, and \textbf{ODA-Mixture (100k \& 500k)}, a series of multi-domain instruction datasets built via an ``Anchor-and-Patch'' strategy that outperforms significantly larger open-source baselines. Our empirical results demonstrate that ODA-driven datasets significantly improve both domain-specific reasoning and general utility while achieving superior data efficiency, validating a transition toward data-centric AI where transparent evaluation serves as the primary engine for engineering high-quality training data.

</details>


### [92] [From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis](https://arxiv.org/abs/2601.09734)
*Yanyi Liu,Qingwen Yang,Tiezheng Guo,Feiyu Qu,Jun Liu,Yingyou Wen*

Main category: cs.CL

TL;DR: 本文提出了一种从“检测”到“诊断”的幻觉误差研究新范式，并引入了系统化的幻觉诊断任务和小规模高效模型，取得了优异实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型幻觉内容检测偏重于二分类（有无幻觉），忽视了对幻觉发生机制、定位及纠正等细粒度诊断，不利于模型实际改进与信赖部署。

Method: 提出了幻觉诊断任务，包括检测、定位、因果解释和纠正。开发了HDG自动数据生成管道，利用逻辑链扰动等方法增强数据，并基于此数据集训练了一种4B参数的诊断模型HDM-4B-RL，采用GRPO优化，奖励结构兼顾准确性与定位能力。

Result: 实验表明，HDM-4B-RL在HaluEval基准测试上超过现有最佳检测模型，并以更小规模达到与大型通用模型相当的综合诊断效果。

Conclusion: 幻觉诊断方法可行且有实用价值，有助于构建更可信、可靠的大语言模型系统。

Abstract: Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary "detection" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from "detection" to "diagnosis". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance to advanced general-purpose models. In comprehensive diagnosis tasks, HDM-4B-RL matches the capabilities of larger general models while maintaining a smaller size. This work validates the feasibility and value of hallucination diagnosis, providing an effective methodology for building more trustworthy and reliable generative AI systems.

</details>


### [93] [Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations](https://arxiv.org/abs/2601.09833)
*Xiaoxu Ma,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM人格特质评估方法（PVNI），较现有问卷法更稳定、更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于问卷的人格评估方法对提示或角色设定细节高度敏感，导致结果不稳定且缺乏解释性；因此，需要开发更稳定、可解释的方法。

Method: 提出PVNI方法，通过对LLM内部激活提取目标人格特质向量，并利用对比提示与插值，衡量中性分数，从而进行稳定、可解释的人格评估；并进行了理论分析。

Result: 在多种主流LLM上系统实验证实，PVNI在人格特质评价的稳定性方面，明显优于传统基于问卷或角色扮演的办法。

Conclusion: PVNI能为LLM人格评估提供更可靠、可解释的新范式，有助于模型解析和安全部署。

Abstract: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.

</details>


### [94] [Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences](https://arxiv.org/abs/2601.09852)
*Sriram Padmanabhan,Siyuan Song,Kanishka Misra*

Main category: cs.CL

TL;DR: 本文研究了语言如何影响归纳推理，并将人类儿童与视觉-语言模型（Vision Language Models, VLMs）的表现进行了比较。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，不同类型的名词短语（如泛指、全称、部分称述）导致人类儿童对新属性归纳的程度不同，说明人类对这些语句具有不同的认知表征。该文动机在于探究当前人工智能模型是否能表现出与人类相似的归纳偏好，以及这种偏好在模型内部表征中的来源。

Method: 作者复现了Gelman等人（2002年）经典实验，测试VLMs对图片分类以及对“all”“some”的语义敏感性，然后让模型执行原实验任务，比对其泛化新属性到特定成员的能力。同时，通过post-hoc分析模型内部表示，探索决定归纳差异的依据。

Result: 实验发现，VLMs在任务表现上与人类儿童表现出一致，即在属性归纳上依然体现出“全称>泛指>部分”的梯度，并且这种差异在表征中归因于归纳约束而非表面语句差异。

Conclusion: VLMs在归纳推理任务上能够复现人类儿童对定量NPs的敏感性，其内部表征也类似组织在归纳约束而非单纯语表特征上，暗示AI具备部分类似人类的语言归纳处理能力。

Abstract: Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements ("Bears are daxable"), universally quantified NPs ("all bears are daxable") and indefinite plural NPs ("some bears are daxable") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.

</details>


### [95] [MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication](https://arxiv.org/abs/2601.09853)
*Sraavya Sambara,Yuan Pu,Ayman Ali,Vishala Mishra,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLM）在面对带有错误假设的实际健康问题时的应对能力，发现现有LLM在医学沟通中的安全性存在重大隐患。


<details>
  <summary>Details</summary>
Motivation: 现实生活中，患者在表达健康问题时常常包含错误的假设，而安全的医学沟通通常需要识别和纠正这些错误。随着LLM被广泛用于医学咨询，评估它们在此方面的能力变得非常重要。

Method: 作者开发了一个半自动化流程，整理了包含1100多个需要重定向的实际健康问题（MedRedFlag数据集），并系统对比了主流LLM和临床医生的答复表现。

Result: 分析发现，即使LLM检测到了问题中的错误前提，也常常未能进行必要的引导和纠正，而是直接回答原问题，可能导致不良的医学决策。

Conclusion: 研究揭示LLM在现实健康沟通场景下存在显著缺陷，在患者端医学AI系统的安全性上存在重大隐忧。

Abstract: Real-world health questions from patients often unintentionally embed false assumptions or premises. In such cases, safe medical communication typically involves redirection: addressing the implicit misconception and then responding to the underlying patient context, rather than the original question. While large language models (LLMs) are increasingly being used by lay users for medical advice, they have not yet been tested for this crucial competency. Therefore, in this work, we investigate how LLMs react to false premises embedded within real-world health questions. We develop a semi-automated pipeline to curate MedRedFlag, a dataset of 1100+ questions sourced from Reddit that require redirection. We then systematically compare responses from state-of-the-art LLMs to those from clinicians. Our analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, and provide answers that could lead to suboptimal medical decision making. Our benchmark and results reveal a novel and substantial gap in how LLMs perform under the conditions of real-world health communication, highlighting critical safety concerns for patient-facing medical AI systems. Code and dataset are available at https://github.com/srsambara-1/MedRedFlag.

</details>


### [96] [OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing](https://arxiv.org/abs/2601.09858)
*Yilin Bao,Ziyao He,Zayden Yang*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的科学论文生成框架，通过将论文大纲构建视为层级文档结构上的长程规划任务，实现了更好的结构连贯性和学术引用一致性。该方法在新提出的论文生成评测基准上优于现有主流神经网络与大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然有很强的局部流畅性，但在生成科学论文时往往缺乏全局结构、输入信息覆盖不足、引用一致性低等问题。因此，亟需提升科学论文自动生成中的整体规划和事实准确性。

Method: 作者将科学论文大纲的构建建模为一个层次化文档结构上的长序列规划问题，利用强化学习进行优化。其核心方法包括：用结构化动作不断编辑和扩展演化大纲引导论文生成；提出两阶段优化流程，即（1）通过反向大纲重建，强化全局结构一致性；（2）基于前向价值引导的强化学习，用奖惩机制明确建模科学准确性、话语连贯性和引用忠实度。此外，作者还设计了新的科学论文生成基准测试，涵盖文档规划、输入利用、引用忠实、组织结构和内容事实准确性。

Result: 实验结果显示，提出的方法在长文本全局结构连贯性、引用一致性和其他评测维度上，相较于强大的神经网络和大模型基线都取得了稳定且显著的提升。

Conclusion: 通过将科学论文生成任务中的大纲构建建模为强化学习中的长程规划问题，并从结构和奖励机制上进行优化，可以显著提升自动科学论文生成的整体结构、学术性和事实性。这一框架为大模型自动生成高质量学术论文提供了新思路。

Abstract: Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

</details>


### [97] [Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL](https://arxiv.org/abs/2601.09876)
*Yifei Shen,Yilun Zhao,Justice Ou,Tinglin Huang,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了CLINSQL基准，这是一个针对真实临床环境中EHR（电子健康记录）文本转SQL任务的评测集，涵盖多表连接、复杂临床语意过滤和可执行SQL查询。通过评估主流大模型，发现当前技术距离临床可靠性还有不小差距。


<details>
  <summary>Details</summary>
Motivation: 目前的Text-to-SQL模型面临多表异构、临床语意、复杂过滤等现实难题，亟需具有挑战性的基准来衡量其于真实医疗场景下的性能和可靠性。

Method: 作者构建了包含633个专业任务的CLINSQL基准，依托MIMIC-IV数据，涵盖复杂表结构与临床语境。对22个主流模型进行评测，并采用链式思考自我优化与严格执行性检查，确保评测贴近临床需求。

Result: GPT-5-mini在测试集上的执行得分为74.7%，DeepSeek-R1作为开源模型最高为69.2%，Gemini-2.5-Pro在高难度任务上得分明显降低，显示出模型在该领域存在明显挑战。

Conclusion: CLINSQL的发布推动了医学文本到SQL模型的真实场景适应和临床可靠性验证，但目前主流方法在实际使用中尚难以达到临床级的可靠性能。

Abstract: Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.

</details>


### [98] [Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal](https://arxiv.org/abs/2601.09886)
*Sathvik Nair,Byung-Doh Oh*

Main category: cs.CL

TL;DR: 本文比较了通过人类完型填空任务和语言模型概率两种方式量化单词可预测性的差异，发现语言模型概率在预测加工难度方面效果更好，并提出可能原因。


<details>
  <summary>Details</summary>
Motivation: 以往研究常用人类完型填空(cloze task)和语言模型(LM)概率两种方式衡量词语的可预测性，但两者作为语言理解加工难度预测因子的效果不同，需要深入探究背后的科学原因，以免得出误导性结论。

Method: 作者通过实验和分析，针对LM概率优于完型填空数据概率的现象，提出并验证了三个假设：LM概率不受低分辨率困扰、能区分相似语义词、并能准确为低频词赋予概率。

Result: 实验证据支持上述三个假设，表明LM概率在对词汇进行细致区分和低频词处理等方面优于完型填空数据。

Conclusion: 结果促使完型填空实验需提高数据分辨率，同时应探讨人类预测能力是否能像LM概率一样敏感于微妙的词汇差异，推动更高精度的认知语言学研究。

Abstract: How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.

</details>


### [99] [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953)
*Christabel Acquaye,Yi Ting Huang,Marine Carpuat,Rachel Rudinger*

Main category: cs.CL

TL;DR: 本文研究了使用开源大语言模型（LLMs）来预测数学选择题难度，发现通过模拟学生答题的方式，LLMs能够较好地预测题目实际难度。


<details>
  <summary>Details</summary>
Motivation: 标准化数学评估需人工试点获取题目难度，该过程昂贵且耗时。因此，寻找替代方法（如使用LLMs）来自动评估题目难度，有助于降低成本、提升效率。

Method: 提出让LLM模拟不同程度的学生答题表现，并用这些结果来拟合项目反应理论（IRT）模型，得到题目难度。用NAEP真实难度数据进行对比，探索不同模拟班级规模和学生设定（如姓名、性别、种族分层）对预测准确性的影响。

Result: LLM模拟生成的难度参数与真实难度高度相关（最高相关系数分别为0.75、0.76、0.82）。使用命名学生及性别、种族多样化能进一步提升准确性。表现上，数学能力较弱的模型反而预测效果更好。

Conclusion: 开源LLM通过模拟学生答题，可用来估计实际数学题难度，部分情况下甚至优于数学能力更强的模型。具备实际应用价值，可为大规模测评题库建设提供低成本辅助工具。

Abstract: Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a "classroom" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different "classroom sizes," showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.

</details>


### [100] [Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG](https://arxiv.org/abs/2601.09982)
*David Samuel Setiawan,Raphaël Merx,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文提出了一种混合框架，结合NMT和基于检索增强生成（RAG）的LLM，显著提升了低资源语言在领域迁移时（如圣经新约到旧约）机器翻译质量。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的神经机器翻译在面对领域迁移时表现大幅下降，此问题尤为突出且数据稀缺。本文以东印尼的Dhao语言为例，研究如何恢复迁移下的翻译性能。

Method: 先采用在新约（NT）微调的NMT模型生成初稿，再通过LLM结合RAG方法进行润色优化，其中RAG利用检索示例辅助生成。

Result: 该混合系统在新领域（旧约OT）中的chrF++分数由27.11提升至35.21，几乎达到原新约的内部表现（36.17），其中性能主要归因于检索示例数量。

Conclusion: 结合NMT和RAG-LLM可作为低资源语言领域迁移的有效“安全网”，能在目标领域大幅修正NMT的严重错误，实现高质量翻译。

Abstract: Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust "safety net," repairing severe failures in zero-shot domains.

</details>


### [101] [SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction](https://arxiv.org/abs/2601.10003)
*Sanghyeok Choi,Woosang Jeon,Kyuseok Yang,Taehyeong Kim*

Main category: cs.CL

TL;DR: SocraticKG是一种通过引入问答对作为中间语义结构，改进知识图谱自动化构建的新方法。它能在不损失结构连贯性的前提下，提高事实覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的知识图谱构建方法面临事实覆盖率和关系连贯性的权衡，直接抽取三元组容易丢失语义联系，或合并过早导致信息丢失。

Method: SocraticKG方法先利用5W1H（Who, What, When, Where, Why, How）指导的问答扩展获取文档级语义，再从问答对中抽取三元组，作为知识图谱节点和边，充分保留文档上下文和隐含关系。

Result: 在MINE基准测试集上的评估显示，该方法在知识量扩增的同时，保持了高结构连贯性和优良的事实保留能力，优于传统直接抽取方法。

Conclusion: 通过问答中介的语义结构化为后续知识图谱三元组的抽取提供了可靠的语义骨架，显著提升了知识图谱构建的连贯性与可靠性。

Abstract: Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.

</details>


### [102] [EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records](https://arxiv.org/abs/2601.10020)
*Lingfei Qian,Mauro Giuffre,Yan Wang,Huan He,Qianqian Xie,Xuguang Ai,Xeuqing Peng,Fan Ma,Ruey-Ling Weng,Donald Wright,Adan Wang,Qingyu Chen,Vipina K. Keloth,Hua Xu*

Main category: cs.CL

TL;DR: 本文提出EHRNavigator，一个多智能体框架，能在异构和多模态EHR数据上实现高效、实用的病人级问答，并在真实医院环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有用于电子健康记录（EHR）问答的自然语言系统多在标准数据集上评估，但这与真实临床场景复杂多变的需求脱节。

Method: 作者设计了EHRNavigator，该系统利用多个AI代理，能处理不同结构、不同类型（包含文本、时间、图像等多模态）EHR数据，实现患者级别的问答。其性能在公共基准数据和真实医院数据上进行评估，包括对数据结构多样、需要时间推理和多证据整合的场景。

Result: EHRNavigator在真实案例中取得了86%的准确率，并保持了临床可接受的响应速度。定量分析和临床医生的回顾验证结果都显示系统具有良好的泛化性和实用价值。

Conclusion: EHRNavigator能够有效弥补基准测试和临床实际部署之间的差距，是一套强大、灵活且高效的真实EHR问答解决方案。

Abstract: Clinical decision-making increasingly relies on timely and context-aware access to patient information within Electronic Health Records (EHRs), yet most existing natural language question-answering (QA) systems are evaluated solely on benchmark datasets, limiting their practical relevance. To overcome this limitation, we introduce EHRNavigator, a multi-agent framework that harnesses AI agents to perform patient-level question answering across heterogeneous and multimodal EHR data. We assessed its performance using both public benchmark and institutional datasets under realistic hospital conditions characterized by diverse schemas, temporal reasoning demands, and multimodal evidence integration. Through quantitative evaluation and clinician-validated chart review, EHRNavigator demonstrated strong generalization, achieving 86% accuracy on real-world cases while maintaining clinically acceptable response times. Overall, these findings confirm that EHRNavigator effectively bridges the gap between benchmark evaluation and clinical deployment, offering a robust, adaptive, and efficient solution for real-world EHR question answering.

</details>


### [103] [EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels](https://arxiv.org/abs/2601.10033)
*Wan Jou She,Lis Kanashiro Pereira,Fei Cheng,Sakiko Yahata,Panote Siriaraya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 本文开发了一个名为EmplifAI的日语共情对话数据集，专为慢性病患者的情感支持而设计，并基于该数据集提升了日语大模型在共情对话方面的表现。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病患者在不同的疾病管理阶段会经历复杂多变的情绪（如希望与绝望），目前缺乏能细致表达并识别这些情感的共情对话数据集与评测体系，尤其是针对日语。

Method: 作者基于GoEmotions情感分类体系，提出28种细粒度情感类别，制作了280个医学情境和4125个双轮对话，通过众包与专家审核收集数据。同时设计基于BERTScore的评测框架，评估多个大型语言模型在情感对齐方面的能力，并利用LLM和人工评价相关性验证评测流程。

Result: 使用EmplifAI数据集微调后，基础日语LLM在流畅性、一般性共情和特定情感识别上均有显著提升。BERTScore F1达到0.83。LLM自动评测结果和人工标注高度相关，方法有效。

Conclusion: EmplifAI扩展了日语共情对话资源，并为医学情境下的情感支持系统和模型评测提供了坚实基础，对日语大模型在医疗领域的应用有积极促进意义。

Abstract: This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.

</details>


### [104] [Long-Chain Reasoning Distillation via Adaptive Prefix Alignment](https://arxiv.org/abs/2601.10064)
*Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出P-ALIGN蒸馏框架，通过对教师大模型生成的解题推理链进行自适应前缀截断和对齐，提升小模型的数学推理能力。实验证明其在多个数学推理基准上超过所有对比方法。


<details>
  <summary>Details</summary>
Motivation: 现有用长推理链蒸馏提升小模型推理能力的方法，往往无法兼容教师模型输出冗长复杂、结构不适的小模型学习能力，导致监督信号与小模型实际能力不匹配。

Method: P-ALIGN框架对教师模型生成的推理轨迹进行自适应判断，截取既简洁又足够指导学生模型的前缀部分。随后用这一部分对学生模型进行监督，鼓励学生模型与其前缀推理对齐。

Result: P-ALIGN在多个数学推理基准上，性能超过所有基线方法3%以上。进一步分析发现，P-ALIGN构建的前缀比完整推理链能提供更有效的监督信号，规避冗余和不确定内容的负面影响。

Conclusion: P-ALIGN通过自适应前缀对齐蒸馏，有效解决了长推理链教学信号与小模型学习能力之间的匹配问题，提升了小模型数学推理表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.

</details>


### [105] [Deriving Character Logic from Storyline as Codified Decision Trees](https://arxiv.org/abs/2601.10080)
*Letian Peng,Kun Zhou,Longfei Yun,Yupeng Hou,Jingbo Shang*

Main category: cs.CL

TL;DR: 本文提出了一种称为Codified Decision Trees（CDT）的方法，用于为角色扮演智能体自动生成可解释、可执行且经验证的行为决策结构，大幅提升智能体在不同叙事场景中的行为一致性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有RP智能体的行为描述（profile）多为非结构化、不可执行且缺乏系统验证，导致智能体在复杂情境下表现不稳定、不真实。急需一种结构化、有效且解释性强的方法来支撑智能体在不同叙事下的可靠行为。

Method: CDT框架通过从大规模叙事数据中自动归纳场景-行为规则，构建条件规则树。每个内部节点为已验证的场景条件，叶节点为具体的行为描述。决策树结构通过逐步诱导、数据验证和分层细化方式建立，既透明又便于后续更新。

Result: 在16个叙事作品中、85个角色测试下，CDT显著优于人工编写的profile以及现有的行为归纳技术，表现为更准确和一致的行为决策。

Conclusion: 结构化且数据驱动的行为规则树能显著提升角色扮演智能体的行为一致性与适应性，为智能体设计提供了透明且可验证的技术路径。

Abstract: Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.

</details>


### [106] [Is MT Ready for the Next Crisis or Pandemic?](https://arxiv.org/abs/2601.10082)
*Vipasha Bansal,Elizabeth Brown,Chelsea Kendrick,Benjamin Pong,William D. Lewis*

Main category: cs.CL

TL;DR: 本文评估了四种商业机器翻译（MT）系统在医疗危机时，将高优先级低资源语言与其它语言互译时的有效性。


<details>
  <summary>Details</summary>
Motivation: 危机时刻（如疫情爆发）沟通至关重要，但提供援助的官方、医疗等机构与受援群体间常有语言障碍，特别是低资源语言。作者希望量化商业MT系统在这些场景下的实用性，评估其在未来危机中能否有效辅助沟通。

Method: 使用TICO-19数据集，选取与疫情相关、覆盖多种低资源及高优先级语言的句子，对四种主流商业MT系统进行相互间互译评估，重点考察其在危机医疗领域应用时的译文可用性。

Result: 商业MT系统在某些低资源语言间的翻译效果有限，并非所有目标语言都能获得可用译文；整体“准备度”不理想，存在沟通隐患。

Conclusion: 目前商业MT系统尚未完全准备好应对疫情等危机场景中的多语种沟通需求，尤其是在低资源语言方面。需进一步改进MT系统以提升未来危机应对能力。

Abstract: Communication in times of crisis is essential. However, there is often a mismatch between the language of governments, aid providers, doctors, and those to whom they are providing aid. Commercial MT systems are reasonable tools to turn to in these scenarios. But how effective are these tools for translating to and from low resource languages, particularly in the crisis or medical domain? In this study, we evaluate four commercial MT systems using the TICO-19 dataset, which is composed of pandemic-related sentences from a large set of high priority languages spoken by communities most likely to be affected adversely in the next pandemic. We then assess the current degree of ``readiness'' for another pandemic (or epidemic) based on the usability of the output translations.

</details>


### [107] [CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking](https://arxiv.org/abs/2601.10085)
*Viet Cuong Nguyen,Nhi Yen Nguyen,Kristin A. Candan,Mary Conlon,Vanessa Rumie,Kristen Risola,Srijan Kumar,Munmun De Choudhury*

Main category: cs.CL

TL;DR: 论文提出了CALM-IT框架，能更好地模拟并评估动机性访谈中长对话，结果显示更有效、更稳健且客户接受度高。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理健康场景下难以维持长时间、有目标的真实对话，经常只关注单轮回复导致整体对话连贯性变差。需要更好方法支持高质量、长时程的心理健康对话。

Method: 提出CALM-IT框架，将治疗师-客户对话建模为双向状态空间过程，动态推断双方状态、对齐度和短期目标，引导策略选择和发言生成。用该框架生成和评估动机性访谈。

Result: 经过大规模评测，CALM-IT在效果和目标对齐上均优于强基线模型，并且随着对话长度增加仍能保持更佳稳定性。尽管治疗师干预次数更少，但客户接受率最高。

Conclusion: 建模双方状态演化对生成高质量、长对话非常重要，CALM-IT为生成更稳定、高效、有治疗对齐性的人工对话提供新证据。

Abstract: Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.

</details>


### [108] [SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature](https://arxiv.org/abs/2601.10108)
*Yiming Ren,Junjie Wang,Yuxin Meng,Yihang Shi,Zhiqiang Lin,Ruihang Chu,Yiran Xu,Ziming Li,Yunfei Zhao,Zihan Wang,Yu Qiao,Ruiming Tang,Minghao Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了FITO范式和相关数据集与评测基准，以更准确评估多模态大模型在理解长篇科学论文时的因果推理与证据链能力。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态大模型（MLLMs）理解科学论文的评估方法（如答案匹配、Needle-In-A-Haystack测试）主要奖励答案准确性，但未能有效考察模型是否通过文档内可追溯证据链进行因果推理，真实评估模型‘理解’能力存在挑战。

Method: 作者提出了Fish-in-the-Ocean（FITO）范式，要求模型在原生科学文档的文本与图像间显式构建跨模态证据链。具体实现方面，作者创建了SIN-Data跨模态科学文献数据集，并设计了包含证据发现、假设验证、基于证据的问答和证据驱动摘要四项任务的SIN-Bench基准。此外，提出“No Evidence, No Score”评分策略，仅对有可验证关联证据的预测评分，并对证据的匹配度、相关性和逻辑性进行诊断。

Result: 在八个主流MLLM的评测中，发现‘证据锚定（grounding）’是主要瓶颈。例如，Gemini-3-pro获得了最高的平均总分（0.573），而GPT-5在SIN-QA任务上有最高的答案准确率（0.767），但其整体证据对齐得分较低，显示了正确性与可追溯证据之间的差距。

Conclusion: 现有MLLM即便在答案准确性上表现良好，但与文档证据的因果链条存在明显不足。FITO范式及SIN-Data与SIN-Bench工具，有助于推动模型具备更真实的科学理解能力、实现因果推理与证据可追溯性的提升。

Abstract: Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic "Needle-In-A-Haystack" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the "Fish-in-the-Ocean" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce "No Evidence, No Score", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.

</details>


### [109] [Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation](https://arxiv.org/abs/2601.10109)
*Lechen Zhang,Yunxiang Zhang,Wei Hu,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种以技能为中心的蒸馏框架，通过技能定向数据选择和技能感知微调，显著提升了推理模型蒸馏的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型蒸馏（如DeepSeek-R1及其变体）需要大量标注数据，成本高昂，因此亟需数据高效的训练方法。

Method: 提出了两部分组成的技能导向蒸馏框架：（1）面向技能的数据选择，聚焦学生模型薄弱技能的样本挑选；（2）技能感知微调，促进分步骤的技能推理。

Result: 在仅使用1000条从10万条教师生成语料中选出的数据进行训练时，在五个数学推理基准上，Qwen3-4B提升了1.6%，Qwen3-8B提升了1.4%，均超过了随机SFT基线。

Conclusion: 技能导向的训练策略能有效提升推理模型蒸馏的效率，尤其是在强调训练技能方面收益明显。

Abstract: Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.

</details>


### [110] [Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends](https://arxiv.org/abs/2601.10122)
*Ye Wang,Jiaxing Chen,Hongjiang Xiao*

Main category: cs.CL

TL;DR: 本文系统性回顾了角色扮演语言体代理（RPLA）的发展、关键技术与未来趋势，对数据构建、技术路径、评估方式等进行了全面梳理。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型迅速进步，促进了角色扮演语言体代理（RPLA）的研究兴起。该领域正处于自然语言处理与人机交互的交汇点，亟需系统总结与展望。

Method: 论文梳理了RPLA技术从模板范式到风格模仿，再到人格建模与记忆融合的演变，归纳了心理量表驱动建模、记忆增强提示、动机-情境行为决策等关键技术路径，并分析了角色数据集构建与评估体系。

Result: 文章总结了RPLA高质量实现的主要技术路线和数据支撑，挖掘了角色语料收集与注释相关挑战，评述了当前多维评测框架与几种主要评估方法的优劣。

Conclusion: 角色扮演语言体未来有望在个性进化、多体协作叙事、多模态交互与脑认知融合等方向获得突破，本文为相关后续研究提供了系统性视角与方法论借鉴。

Abstract: In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.

</details>


### [111] [ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156)
*Yutao Mou,Zhangchi Xue,Lijun Li,Peiyang Liu,Shikun Zhang,Wei Ye,Jing Shao*

Main category: cs.CL

TL;DR: 本文聚焦大语言模型（LLM）智能体在调用外部工具时引发的安全问题，提出用于工具调用安全检测的新基准TS-Bench，以及基于多任务强化学习的守护模型TS-Guard，并引入能主动反馈和优化推理流程的TS-Flow框架，实现大幅减少有害操作和提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力提升，其调用第三方工具的能力也增强，但由此带来了更高的操作安全风险。目前针对智能体每一步工具调用的安全监控与防护仍存在缺口，亟需解决安全检测与干预难题。

Method: 作者首先构建了以单步工具调用为单位的安全检测基准TS-Bench，然后提出了多任务强化学习训练的守护模型TS-Guard，利用对交互历史的推理分析来判断和阻断危险调用。同时，提出了“守护-反馈”一体TS-Flow推理框架，使智能体能根据反馈调整工具调用行为。

Result: TS-Guard能有效检测和拦截不安全的工具调用操作。TS-Flow框架在ReAct风格智能体上将有害调用减少了65%，在提示注入攻击下还提升了10%的任务完成能力。

Conclusion: 该系列方法显著提升了LLM智能体在多工具交互场景下的安全性和鲁棒性，为部署更安全的智能体系统提供了可行方案。

Abstract: While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.

</details>


### [112] [What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models](https://arxiv.org/abs/2601.10159)
*Guimin Hu,Meng Li,Qiwei Peng,Lijie Hu,Boyan Xu,Ruichu Cai*

Main category: cs.CL

TL;DR: 本文分析了MoE（Mixture-of-Experts）大模型中的专家激活机制，通过区分领域专家与驱动专家，提出熵与因果效应指标来评估专家激活对输出的影响，揭示专家激活的内在机制与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性研究主要关注于Transformer的层级或神经元机制，而对MoE大模型中更高层次的专家行为研究较少。受人脑功能专化启发，作者希望进一步理解MoE模型内部专家的激活机制与决策贡献。

Method: 本文在三个公开领域的MoE模型上，分别分析专家激活的模式。通过提出基于熵的指标和因果效应的度量，识别领域专家和驱动专家，并探讨token与专家激活的关联。同时，通过调整领域/驱动专家权重，测试对模型性能的影响。

Result: 1）发现部分专家表现出明显的领域偏好，而另一些专家对模型表现有决定性因果影响；2）句子前部token更易激活驱动专家；3）调整领域/驱动专家权重可显著提升模型在所有三个领域的表现。

Conclusion: 该研究揭示了MoE模型专家激活的内在机制，有助于提升模型的可解释性，并通过合理调整专家权重带来性能优化。

Abstract: Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.

</details>


### [113] [Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment](https://arxiv.org/abs/2601.10160)
*Cameron Tice,Puria Radmard,Samuel Ratnam,Andy Kim,David Africa,Kyle O'Brien*

Main category: cs.CL

TL;DR: 本文研究了预训练语料中关于AI系统的讨论，发现相关语料内容会显著影响大模型后续的对齐表现，建议在预训练阶段关注对齐内容。


<details>
  <summary>Details</summary>
Motivation: 虽然关于AI的讨论广泛存在于训练语料中，但其对大模型对齐性的具体因果影响尚未明了。如果训练语料中对AI的描述以负面为主，模型可能学到倾向于不对齐的行为倾向，因此需要验证这种“自我实现式误对齐”假说。

Method: 采用对比实验，使用6.9B参数规模的LLM，在预训练阶段分别增加不同程度的（错）对齐相关语料，并通过上采样方式增强对齐或误对齐相关内容，对比训练后的模型行为。

Result: 发现增加关于AI误对齐的语料会增强模型误对齐行为，而增加对齐正面描述则能显著降低模型误对齐分数（从45%降至9%）。这些效应在后训练阶段虽然有所减弱，但依然存在。

Conclusion: 预训练语料对大模型的行为先验有重要影响，建议并重视“对齐预训练”以补充现有的对齐后训练流程，同时开放了相关模型与数据集供社区使用。

Abstract: Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai

</details>


### [114] [AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers](https://arxiv.org/abs/2601.10161)
*Prachuryya Kaushik,Ashish Anand*

Main category: cs.CL

TL;DR: AWED-FiNER是一个开源生态系统，支持36种全球语言的细粒度实体识别（FgNER），覆盖超过66亿人口，特别关注资源匮乏和弱势语言，提供多种工具与在线服务。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）虽然在通用NLP任务中表现出色，但在低资源语言和细粒度实体识别任务中表现不佳。市面上缺乏支持多语言、易于部署、同时对弱势语言友好的FgNER工具。因此，作者希望通过构建AWED-FiNER系统，填补这一空白。

Method: AWED-FiNER提供了一套代理工具集合、网页应用和多种最先进的专家模型。这些工具能够将多语言文本自动分流到特定的专家模型，并快速生成FgNER标注。网页平台为非技术用户提供即用型FgNER服务。所有专家模型体积极小，支持在资源有限的边缘设备本地离线部署。同时重点支持了如Bodo、Manipuri等弱势语言。

Result: AWED-FiNER现已支持36种全球语言，涵盖66亿人口。提供49个小型专家模型和两个可访问的在线平台，能够高效服务非技术用户和各类应用场景，对边缘设备友好，特别加强了对弱势语言的支持。

Conclusion: AWED-FiNER极大推动了细粒度实体识别技术在多语言和弱资源环境下的应用普及，为全球多语种处理及边缘设备提供了高效解决方案，促进了NLP的公平性与可及性。

Abstract: We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).

</details>


### [115] [Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection](https://arxiv.org/abs/2601.10167)
*Nhung Nguyen Thi Hong,Cuong Nguyen Dang,Tri Le Ngoc*

Main category: cs.CL

TL;DR: 本文提出了Credit C-GPT，一个专为越南语债务催收场景定制的大型语言模型，通过多任务学习提升了对复杂会话的理解与分析能力，对企业联络中心的实时与事后分析支持效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 越南金融行业的债务催收高度依赖人工电话沟通，涉及非正式口语、情绪变化及复杂推理，传统NLP系统难以胜任，亟需面向特定领域且更适应会话情景的智能模型以提升自动化与效率。

Method: 本文构建并精调了一个拥有七亿参数的越南语对话大型语言模型Credit C-GPT，集成对话理解、情感识别、意图检测、通话阶段分类和结构化槽值提取多项能力于基于推理统一框架之下。详细介绍了数据集建设、标注方案和训练流程，并用人工标注数据集进行评估。

Result: 实验表明，Credit C-GPT在多个会话智能任务上的表现，均优于传统分步骤流水线式NLP方法，在专有人工标注数据集上取得了一致的性能提升。

Conclusion: 领域专用会话大模型可为企业联络中心实现可扩展、隐私友好、实时助理与通话后分析，表现出比传统方法更好的效果，是BFSI等行业自动化与智能化升级的有效工具。

Abstract: Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.

</details>


### [116] [HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning](https://arxiv.org/abs/2601.10187)
*Ziang Cui,Mengran Yu,Tianjiao Li,Chenyu Shi,Yingxuan Shi,Lusheng Zhang,Hongwei Lin*

Main category: cs.CL

TL;DR: 本论文提出了HOMURA框架，通过强化学习在实现语义保真的前提下，有效控制多语种翻译输出长度，尤其适用于有严格时间限制的任务，如字幕和配音。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在多语种翻译上表现出色，但其普遍存在输出冗长的倾向，导致在字幕和配音等需要严格时间控制的应用场景下难以使用。而现有的提示工程难以平衡语义完整性和时间长度要求。

Method: 本文首先提出了Sand-Glass基准，用于评测翻译在音节级时长限制下的表现。之后引入HOMURA，一种基于强化学习的框架，通过KL正则化和动态音节比奖励来同时优化语义保真和时间合规性。

Result: 在实验中，所提方法显著优于强基线LLM，实现了精确控制输出长度，既满足了时长要求，又保留了语义充分性，同时尊重不同语言的表达密度。

Conclusion: HOMURA能够有效缓解LLM在跨语言冗余方面的系统性偏差，在严格时间约束下的翻译任务中表现优异，兼顾了语义和长度双重需求。

Abstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively "tames" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.

</details>


### [117] [HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns](https://arxiv.org/abs/2601.10198)
*Xintao Wang,Jian Yang,Weiyuan Li,Rui Xie,Jen-tse Huang,Jun Gao,Shuai Huang,Yueping Kang,Liyuan Gou,Hongwei Feng,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了HUMANLLM框架，通过将心理模式视为相互作用的因果力，实现更真实的人格模拟和角色扮演型大模型，并在多项评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在模拟人类对话和角色表现上有出色进展，但与真实的人类认知和行为模式高度一致仍面临挑战，因此亟需更好地模拟人类心理及行为背后的因果机制。

Method: 作者构建了244种心理模式，并基于大约12,000篇学术论文综合出11,359个情景，在这些情景中2-5个心理模式产生互相强化、冲突或调节作用，模拟多轮对话、内心活动和行为。提出了双层级评估量表，分别检测单一心理模式忠实度和多模式交互动力学，验证模型表现。

Result: HUMANLLM-8B在模式多样的复杂角色扮演中表现优异，人类评分相关系数达到0.91，且在多模式动力学测试中超过参数规模更大（4倍）的Qwen3-32B。

Conclusion: 基于心理模式因果建模，大大提升了大模型的拟人性和一致性，仅提升结果层面的对话与社会期望性往往掩盖了模型真实的心理过程模拟能力，真实拟人需深度建模人类认知过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning and generation, serving as the foundation for advanced persona simulation and Role-Playing Language Agents (RPLAs). However, achieving authentic alignment with human cognitive and behavioral patterns remains a critical challenge for these agents. We present HUMANLLM, a framework treating psychological patterns as interacting causal forces. We construct 244 patterns from ~12,000 academic papers and synthesize 11,359 scenarios where 2-5 patterns reinforce, conflict, or modulate each other, with multi-turn conversations expressing inner thoughts, actions, and dialogue. Our dual-level checklists evaluate both individual pattern fidelity and emergent multi-pattern dynamics, achieving strong human alignment (r=0.91) while revealing that holistic metrics conflate simulation accuracy with social desirability. HUMANLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4x fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling--simulating not just what humans do, but the psychological processes generating those behaviors.

</details>


### [118] [One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?](https://arxiv.org/abs/2601.10205)
*Arya Shah,Himanshu beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 本文提出了一个涵盖12种印度语言的统一基准，用于评价多语言助手在文化和个性匹配方面的能力，并对当前主流嵌入模型进行了系统比较。


<details>
  <summary>Details</summary>
Motivation: 印度拥有十亿以上、使用多脚本多语言的人口，目前相关基准往往局限于单一语言或混淆了信息检索与文本生成任务，无法直接评估模型在个性化指令匹配上的表现。本研究旨在填补这一空白，推动适用于印度语境的多语言模型发展。

Method: 作者构建了一个包括12种印度语言、4项任务（单语与跨语个性化信息检索、指令到个性反向检索、二分类兼容性判断）的能力评测基准。针对8个多语言嵌入模型，采用冻结编码器加上线性分类头进行评价，涵盖检索和分类两大指标。

Result: E5-Large-Instruct模型在单语信息检索Recall@1为27.4%，跨语检索Recall@1为20.7%；BGE-M3在反向检索Recall@1达到32.1%；LaBSE模型的二分类AUC为75.3%，且置信度校准优良。

Conclusion: 该基准为印度多语助手检索任务的模型选择提供实用参考，也为后续研究建立了可复现的基线。所有代码、数据和模型均已开源。

Abstract: Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4\% on monolingual retrieval and 20.7\% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1\% Recall@1. For classification, LaBSE attains 75.3\% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.

</details>


### [119] [GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients](https://arxiv.org/abs/2601.10229)
*Kentaro Kazama,Daiki Shirafuji,Tatsuhiko Saito*

Main category: cs.CL

TL;DR: 提出GeoSteer框架，通过流形（manifold）方法引导大语言模型（LLM）的中间推理步骤，提升推理质量和一致性。实验在GSM8k数据集和Qwen3模型上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT方法虽然提升了LLM的推理能力，但其每步推理往往存在逻辑不一致，影响了推理过程的可靠性。因此亟需机制确保每步推理更高质量、一致性。

Method: 提出GeoSteer，包含三步：1) 构建具有分段评分的CoT数据集；2) 训练VAE和质量估计模型，学得高质量CoT推理轨迹的低维流形；3) 在推理时，引导目标LLM的隐状态朝高质量流形区域调整，实现几何上一致的推理引导。

Result: GeoSteer在GSM8k数据集（搭配Qwen3系列模型）测试中，将准确率提升2.6个百分点，对比性胜率提升5.3个百分点。

Conclusion: GeoSteer能有效、可控地提升LLM推理过程中间步骤的一致性和质量，增强模型总体推理可靠性。

Abstract: Recent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.

</details>


### [120] [Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?](https://arxiv.org/abs/2601.10242)
*Guanxu Chen,Dongrui Liu,Jing Shao*

Main category: cs.CL

TL;DR: 本文探讨了循环Transformer（LTs）是否能通过增加循环层深度来缩小大模型内部知识与其显性输出之间的差距。实验发现增加循环确实缩小了差距，但部分原因是内部知识的下降。此外，目前LTs只能在最后一次循环中有效地理解其表征。LTs虽有潜力但尚未实现真正的自省能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的内部知识通常不能完全体现在生成的文本中，存在“知识-表达”差距。作者想研究通过调整模型结构（循环迭代层）能否提升LLMs的自省和表达一致性。

Method: 作者采用循环Transformer架构，通过多次迭代同一组参数实现更深的计算层，并设计实验比较不同循环次数对模型表征与输出之间匹配程度的影响。

Result: 增加循环次数可以缩小内部知识与输出的差距，但同时模型内部表征的知识量有下降。进一步分析显示，LTs只有最后一次迭代对内部表征的感知能力较强，前面几轮没有明显提升。

Conclusion: LTs作为增加计算深度的架构有潜力，但目前尚未实现理想的自省机制，尚未能实现内部知识与自然语言输出的真正联结。

Abstract: Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.

</details>


### [121] [coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts](https://arxiv.org/abs/2601.10246)
*Prottay Kumar Adhikary,Reena Rawat,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一种名为coTherapist的智能辅助心理健康框架，通过小型语言模型实现核心治疗能力，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康需求上升和专业人员短缺，亟需智能系统来辅助心理健康专家工作，提高服务可及性和效率。

Method: 设计了coTherapist框架，采用小型语言模型，通过领域特定微调、检索增强及代理推理，模拟治疗师核心能力，并通过T-BARS量表和心理测量方法评估其表现。

Result: 实验表明，coTherapist在回答临床问题时比当前主流模型更具相关性和临床基础，同时展现出较高的同理心及接近治疗师的人格特质，获得领域专家的准确性、可信性和安全性认可。

Conclusion: 小型语言模型通过合理设计可达到类专家水平，为数字心理健康工具的规模化提供了新途径。

Abstract: Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.

</details>


### [122] [Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs](https://arxiv.org/abs/2601.10257)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.CL

TL;DR: 本研究探讨大语言模型（LLM）在不同语言环境下对道德困境的判断是否产生差异，并分析这些差异的成因。作者提出了一种新颖的方法，可分别操控困境语言和推理语言，深入分析其各自对结果的影响，最后应用于13种LLM的中英道德判断实验。


<details>
  <summary>Details</summary>
Motivation: 过去的研究仅在输入语言和推理语言一致时（如都用英文）测试LLM的道德判断，忽略了两种因素各自的影响。作者旨在厘清困境语言和模型推理语言对道德判断差异的具体贡献，并提出更加细致的模型评估方法，为跨文化人工智能部署提供理论和实践依据。

Method: 作者设计了一套方法论，允许分别控制和组合困境描述语言与模型推理语言，包括常规的匹配和非常规的不匹配组合（如英文困境加中文推理）。同时，作者借助道德基础理论（Moral Foundations Theory）对LLM的道德判断进行解释分析，并应用该方法于13个主流大语言模型的中英文道德难题推理实验上。

Result: （1）推理语言对LLM道德判断的影响约为输入语言的两倍；（2）约有一半模型在常规评测下无法检测到的情境依赖性，在新方法下被识别出来；（3）提出了诊断性分类法，将不同模式归类并转化为具体的部署指导。同时发现Authority维度可细分为家庭和制度两个子维度。

Conclusion: 该方法能够区分和量化困境语言与推理语言对LLM道德判断的影响，有助于提升模型跨文化部署的可靠性与透明度，并为道德维度刻画、模型选择和落地部署提供理论和实践工具。相关代码和数据已公开，便于进一步研究。

Abstract: When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.

</details>


### [123] [Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel](https://arxiv.org/abs/2601.10266)
*Hiroaki Yamagiwa,Yusuke Takase,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 本论文提出用Projection Kernel（PK）度量注意力头之间空间关系，优于现有方法，并用其分析了GPT2-small模型的结构。


<details>
  <summary>Details</summary>
Motivation: 理解注意力头之间的关系对于解释Transformer的内部结构至关重要，但现有的分析方法难以有效捕捉这些结构。

Method: 作者关注注意力头权重矩阵张成的子空间，提出利用主角度原理的Projection Kernel（PK）测量子空间相似性，并与基于随机正交子空间的参考分布对比，量化PK分布的信息量。同时，PK被用来构建有向图分析注意力头间的作用。

Result: 实验表明，在IOI任务上，PK比以往如Composition Score等指标更清晰地再现了注意力头之间的交互关系。

Conclusion: 通过PK分析，发现GPT2-small的L4H7注意力头作为枢纽执行了恒等头的功能，显示PK度量能揭示Transformer内部重要结构。

Abstract: Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as the Composition Score. We further introduce a framework to quantify the informativeness of PK distributions by comparing them with a reference distribution derived from random orthogonal subspaces. As an application, we analyze a directed graph constructed from PK and show that, in GPT2-small, L4H7 acts as a hub by functioning as an identity head.

</details>


### [124] [MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts](https://arxiv.org/abs/2601.10272)
*Yuxuan Lou,Kai Yang,Yang You*

Main category: cs.CL

TL;DR: MoST是一种采用专家混合结构（MAMoE）的开源多模态大模型，能高效处理语音和文本，在多个任务上优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型对不同模态（如语音、文本）往往用同样的参数处理，没有充分考虑其内在表示差异，导致对各自模式的理解和信息整合能力有限。作者旨在提出一种能更好区分处理不同模态特征同时促进跨模态学习的大模型架构。

Method: 提出了MAMoE架构，通过模态感知路由机制，将不同模态的输入分配给专门的专家组（处理特定模态）和共享专家（促进跨模态信息转移）。使用开放语音识别（ASR）、语音合成（TTS）、以及语音-文本指令数据进行后训练和微调，全流程采用开放数据。

Result: 在ASR、TTS、音频语言建模和语音问答等多个基准测试中，MoST模型在同等模型参数量下均优于现有方法。消融实验也验证了模态感知路由与共享专家设计对提升性能的关键作用。

Conclusion: MoST是首个全开源、基于专家混合结构的语音-文本大模型，能够高效融合并理解语音与文本，多项任务均有出色表现，架构及公开资源推动多模态LLM进一步发展。

Abstract: We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST

</details>


### [125] [The Straight and Narrow: Do LLMs Possess an Internal Moral Path?](https://arxiv.org/abs/2601.10307)
*Luoming Hu,Jingjie Zeng,Liang Yang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本论文提出了一种利用道德基础理论（MFT）来深入分析与操控大型语言模型（LLMs）内在道德表示的新方法，并展示了其提升AI道德安全性的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型道德对齐方法大多只是在表面设置安全防线，未能真正影响模型内部的道德表示。提升模型内在的道德对齐是实现AI安全性的重要挑战。

Method: 作者结合道德基础理论（MFT），以跨语言线性探测法对LLMs的中间层道德表示进行分析与验证，发现英文和中文间存在共享但不同的道德子空间。进一步地，作者提出了可操控的道德向量，用于引导模型。在此基础上，设计了自适应道德融合（AMF）方法，通过推理时动态干预，实现探测与向量注入相结合的对齐控制。

Result: 实验结果表明，该方法有效减少了模型对于正常请求的不当拒绝，同时降低了越狱（jailbreak）攻击的成功率，优于标准对齐基线。道德向量调控在模型内部和行为层面均得到成功验证。

Conclusion: 该研究为LLMs的道德表示可控性提供了新思路，通过内在干预提升了模型道德安全性的有效性，并在跨语言环境下具有较好的泛化能力。

Abstract: Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.

</details>


### [126] [Multilinguality as Sense Adaptation](https://arxiv.org/abs/2601.10310)
*Jan Christian Blaise Cruz,David Ifeoluwa Adelani,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本论文提出了SENSIA方法，通过显式对齐不同语言间的语义表示，实现跨语言的sense层面的适应，与现有方法相比在多语言任务中表现更优，同时显著减少目标语言数据需求。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型多依赖参数共享和大规模数据，忽视了真实语义层面的对齐，导致跨语言迁移能力和资源利用效率有限。

Method: 提出SENse-based Symmetric Interlingual Alignment（SENSIA）方法，将Backpack语言模型的源语言适应到目标语言，通过并行语料对齐sense级混合表示和上下文表示，并联合训练目标语言的语言模型损失以保证流利度。

Result: 在四种语言的基准测试中，SENSIA普遍优于其他多语言对齐方法，并能用更少的目标语言数据达到与单语训练模型相当的准确率。实验分析显示SENSIA能较好地保存英、目标语间语义结构，并具有高度的鲁棒性。

Conclusion: SENSIA实现了有效、稳健的跨语言sense级语义对齐，兼顾准确率和数据利用率，为多语言模型提供了一种更具语义解释性和实际应用价值的新方案。

Abstract: We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.

</details>


### [127] [ADVOSYNTH: A Synthetic Multi-Advocate Dataset for Speaker Identification in Courtroom Scenarios](https://arxiv.org/abs/2601.10315)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 本文提出了Advosynth-500数据集，用于区分结构化环境下的合成语音识别，并基于Speech Llama Omni模型进行法庭场景下的声音身份识别测试。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语音合成模型逼真度提升，如何在特定场景（如法庭）区分不同的合成声音成为研究重点。

Method: 构建了Advosynth-500数据集，包含100条合成语音，代表10个独特的律师身份，并用Speech Llama Omni模型仿真法庭辩论情景，定义各身份的声音特征，并设置说话人识别挑战。

Result: 证明了当前系统在辨认合成声音身份方面的能力，并为该领域后续研究提供了可用数据集与评测基准。

Conclusion: 区分结构化环境下的合成声音具挑战性，本文的方法和数据集为构建更强说话人识别模型提供了新思路和工具。

Abstract: As large-scale speech-to-speech models achieve high fidelity, the distinction between synthetic voices in structured environments becomes a vital area of study. This paper introduces Advosynth-500, a specialized dataset comprising 100 synthetic speech files featuring 10 unique advocate identities. Using the Speech Llama Omni model, we simulate five distinct advocate pairs engaged in courtroom arguments. We define specific vocal characteristics for each advocate and present a speaker identification challenge to evaluate the ability of modern systems to map audio files to their respective synthetic origins.
  Dataset is available at this link-https: //github.com/naturenurtureelite/ADVOSYNTH-500.

</details>


### [128] [Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis](https://arxiv.org/abs/2601.10318)
*Songsong Tian,Kongsheng Zhuo,Zhendong Wang,Rong Shen,Shengtao Zhang,Yong Wu*

Main category: cs.CL

TL;DR: 提出了BAR-SQL，一个注重边界感知和可靠性的统一NL2SQL训练框架，显著提升SQL生成质量及应对复杂/模糊查询的能力。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统在面临多步分析、模糊和无法回答的查询时，可靠性和边界处理能力不足，且缺乏面向企业真实场景的代表性数据集与解释性。

Method: 1) 种子变异式数据合成，打造包含多步和边界用例的企业级数据集；2) 基于知识溯源的推理合成，生成显式基于业务规则和表结构的思路链；3) 两阶段训练：监督微调+基于群体相对策略优化的强化学习，设计任务条件混合奖励，兼顾SQL准确率和语义拒答；4) 构建Ent-SQL-Bench基准，综合评测生成准确率与边界可靠性。

Result: BAR-SQL在新构建的Ent-SQL-Bench数据集上取得91.48%的平均准确率，在SQL生成质量和边界语义拒答能力上均超越Claude 4.5 Sonnet和GPT-5等主流专有模型。

Conclusion: BAR-SQL实现了更强的企业级NL2SQL可靠性、边界感知和生成能力，为实际复杂查询场景提供了更准确可信的解决方案。源码和基准已开源。

Abstract: In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.

</details>


### [129] [An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit](https://arxiv.org/abs/2601.10321)
*Warren Jouanneau,Emma Jouffroy,Marc Palyart*

Main category: cs.CL

TL;DR: 本文提出了一种基于新一代延后交叉注意力架构的重排序模型，用于高效处理长、结构化、多语种的简历与职位匹配问题，通过大模型教师范式提升模型性能，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际招聘场景中，如何高效且准确地从大量且复杂的简历中，实时找到与职位最匹配的人才是一大挑战，尤其当简历内容冗长、结构复杂且涉及多种语言。

Method: 作者提出了一种基于延后交叉注意力架构的重排序模型，将简历和职位描述进行分解，以提升模型对长文本的处理能力，并尽量减少计算开销。同时，利用生成式大语言模型（LLM）作为教师，生成更细致且语义上扎实的监督信号，通过强化蒸馏损失将这些知识迁移到学生模型，实现更优的技能匹配评分。

Result: 该方法在相关性、排序及校准等评测指标上，均显著优于当前主流的对比模型与基线方法。

Conclusion: 提出的方法不仅提升了人岗匹配的准确性和一致性，并实现了更可解释的技能适配评分，为招聘和人才筛选提供了更高效的智能化工具。

Abstract: Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.

</details>


### [130] [OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding](https://arxiv.org/abs/2601.10343)
*Deming Ding,Shichun Liu,Enhui Yang,Jiahang Lin,Ziying Chen,Shihan Dou,Honglin Guo,Weiyu Cheng,Pengyu Zhao,Chengjun Xiao,Qunhong Zeng,Qi Zhang,Xuanjing Huang,Qidi Xu,Tao Gui*

Main category: cs.CL

TL;DR: 本文提出了OctoBench基准，用于系统性评估大型语言模型（LLMs）在复杂、分阶段的软件开发脚手架中的合规指令跟随能力，强调了任务完成能力和规约遵循能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 近年来，通过脚手架（scaffold）增强的LLM在软件开发自动化中展现出强大能力，但其是否能严格遵守脚手架配置的复杂、分层和持续性指令尚未被系统评估。因此，作者希望填补这一评测空白。

Method: 作者构建了基于代码仓库场景的OctoBench基准套件，包含34个环境、217项任务、3类脚手架和共7,098个细致化需求项，同时配套自动化轨迹捕捉与精细评测工具，旨在分离任务解决能力与指令遵循能力的评估。

Result: 实验覆盖了8个典型LLM模型。结果显示，各模型在纯粹完成任务与严格遵守脚手架指令间存在显著差异，表现出合规性短板。

Conclusion: 当前LLM在面向复杂脚手架的指令遵循上存在明显不足，建议今后的训练与评测需更关注异构指令的执行能力。OctoBench现已开源，用于推动相关领域标准化与进步。

Abstract: Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.

</details>


### [131] [Training-Trajectory-Aware Token Selection](https://arxiv.org/abs/2601.10348)
*Zhanming Shen,Jiaqi Hu,Zeyu Qin,Hao Chen,Wentao Ye,Zenan Huang,Yihong Zhuang,Guoshan Lu,Junlin Zhou,Junbo Zhao*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的方法T3S，有效提升了蒸馏大模型时的性能表现，尤其是在高能力学生模型下优化效果明显超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前高性能学生模型通过连续蒸馏提升推理能力时，常见优化方法出现性能下降甚至退化的现象，急需机制层面创新以实现更高效蒸馏。

Method: 作者通过分析训练过程，发现传统蒸馏会导致两类token（模仿锚定token和待学习token）难以共存，引发性能瓶颈。针对这一问题，提出了训练轨迹感知的Token选择机制（T3S），在训练目标上有针对性地优化难以学习的token。

Result: T3方法在自动回归（AR）和解耦大模型（dLLM）设置下均取得了一致性提升：仅需数百示例，Qwen3-8B即超越DeepSeek-R1，Qwen3-32B能接近Qwen3-235B；T3S训练的LLaDA-2.0-Mini超越自身AR基线，并在16B规模无思考模型中达到了SOTA。

Conclusion: T3S有效解决了连续蒸馏中的性能瓶颈问题，推动了高能力学生模型的推理能力提升，对大模型压缩与落地意义重大。

Abstract: Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.

</details>


### [132] [Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text](https://arxiv.org/abs/2601.10355)
*Zhihao Xu,Rumei Li,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xunliang Cai,Xiting Wang*

Main category: cs.CL

TL;DR: 提出了一种利用文本语料自动生成多轮工具使用轨迹的新范式及数据管道GEM，有效提升了大模型在多轮任务中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要多轮工具调用能力，但现实中缺乏高质量、多样化的多轮工具使用数据，严重阻碍模型提升。作者发现文本语料蕴含大量真实的多步问题求解信息，尝试利用这些语料补足数据不足的问题。

Method: 提出GEM，包含四个阶段：相关性过滤、工作流与工具抽取、轨迹落地、复杂度细化。随后，通过有监督微调训练Trajectory Synthesizer模型，将整个复杂的生成流程浓缩为端到端的轨迹生成模型，在保证效果的同时大幅降低推理成本。

Result: GEM-32B模型在BFCL V3 Multi-turn基准上提升16.5%，其泛化能力优于某些特定领域(tau-bench Airline和Retail)数据训练模型。Trajectory Synthesizer在生成轨迹的效率和延迟上大幅优于完整流程，同时生成质量相当。

Conclusion: 文本语料可用于自动构建多轮工具使用数据，比传统方式更加高效且具备良好的泛化能力。GEM和Trajectory Synthesizer能帮助大模型更好地学习多轮工具使用，具有实际落地价值。

Abstract: Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.

</details>


### [133] [The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models](https://arxiv.org/abs/2601.10387)
*Christina Lu,Jack Gallagher,Jonathan Michala,Kyle Fish,Jack Lindsey*

Main category: cs.CL

TL;DR: 本文分析了大语言模型在“助理”身份与其他人格原型间的表现特征，并提出通过操控激活方向可实现模型个性、稳定性和安全性的调控。


<details>
  <summary>Details</summary>
Motivation: 大模型可表现多种人格，但常被训练为‘助理’模式，理解和引导模型的‘人格空间’有助于提升安全与适用性。

Method: 作者通过抽取激活方向，构建了人格空间，定位“助理轴”，并尝试沿该轴调整、量化模型行为表现，并分析了多个模型与不同极性下的结果。

Result: 发现“助理轴”主导大部分人格表现，向该轴靠拢时模型更安全、有帮助，远离时更可能出现离奇或有害行为，还观测到‘人格漂移’及与用户情绪、对话主题等的关联。

Conclusion: 通过限定激活在助理轴上的范围，可以显著提升模型在应对挑战性对话（如情绪敏感、越狱等）时的稳定与一致性，未来需更好地将模型锚定在一致的人格上。

Abstract: Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an "Assistant Axis," which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts "persona drift," a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.

</details>


### [134] [INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects](https://arxiv.org/abs/2601.10388)
*Tarun Sharma,Manikandan Ravikiran,Sourava Kumar Behera,Pramit Bhattacharya,Arnab Bhattacharya,Rohit Saluja*

Main category: cs.CL

TL;DR: 本文提出并公开了INDIC-DIALECT语料库，涵盖印地语与奥迪亚语的11种方言，用于推动低资源印度方言的NLP研究，并搭建了多任务基准。


<details>
  <summary>Details</summary>
Motivation: 尽管印地语和奥迪亚语有众多使用者，其区域方言却因缺乏标准化和网络数据而被NLP研究严重忽视，尤其是在印度这种多语言背景下，致使相关任务表现落后。

Method: 作者人工构建并标注了涵盖11种方言、13000对句子的印地语与奥迪亚语平行语料库，并设立了三个任务：方言分类、选择题回答和机器翻译。实验对比了GPT-4o、Gemini 2.5与基于印度语言预训练并微调的transformer模型，还研究了混合式AI和基于规则加AI的翻译方法。

Result: LLMs（如GPT-4o、Gemini 2.5）在方言分类任务上表现较差（F1仅19.6%），而专门微调的transformer模型表现显著提升（F1达89.8%）。在方言到标准语机器翻译中，混合式AI模型BLEU分达61.32，显著超越基线（23.36）。而标准语到方言转换时，基于规则+AI的方法BLEU为48.44，也大幅优于基线（27.59）。

Conclusion: INDIC-DIALECT为印度地区低资源方言提供了新的高质量数据集和基准，有望显著推动相关NLP研究。数据集将开源，促进学术界和工业界对印度方言NLP的深入探索。

Abstract: Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\% to 89.8\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.

</details>


### [135] [TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction](https://arxiv.org/abs/2601.10410)
*Mihai Dan Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: 本论文提出了一个完整的罗马尼亚语语言建模流水线（TF3-RO），实现了结构化控制和高语言一致性的紧凑型模型训练与大规模合成数据生成，解决了资源匮乏、形态丰富语言缺乏一体化可复现工作流的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多关注资源丰富语言，缺乏对如罗马尼亚语这类形态丰富、资源匮乏语言的完整、开源、可复现的端到端建模及数据生成流程，尤其是在分词器、预处理、训练、压缩以及评价等方面尚未统一。

Method: 1. 构建了适应罗马尼亚语的BPE和Unigram分词器，缓解形态导致的token数量膨胀问题；2. 利用长序列训练，从零开始训练了5100多万参数的LLaMA风格Transformer模型；3. 通过量化、结构化剪枝和知识蒸馏等技术，得到2645万参数、便于部署的精简模型；4. 结合控制性组合提示方法，大规模生成三百万篇罗马尼亚语寓言数据集；5. 在各阶段采用多维度的评估体系。

Result: 最终获得了一个结构紧凑、评测全面的罗马尼亚语语言模型及三百万条高质量罗马尼亚语合成寓言数据，为低资源语言大规模数据和模型构建提供了范例。

Conclusion: TF3-RO实现了罗马尼亚语建模的可复现一体化流程，兼顾了分词器、模型和数据的本地化优化，为低资源语言的语言模型训练和大规模语料生成奠定了方法基础，并公开了全流程。

Abstract: Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.

</details>


### [136] [Are Language Models Models?](https://arxiv.org/abs/2601.10421)
*Philip Resnik*

Main category: cs.CL

TL;DR: 文章认为将语言模型（LMs）称为“模型系统”并不准确，尤其是在Marr的三个层次分析下，批判了LMs作为认知模型的说法。


<details>
  <summary>Details</summary>
Motivation: Futrell和Mahowald宣称语言模型可以作为“模型系统”，本论文试图质疑和评估这一观点在认知科学不同层次的适用性和合理性。

Method: 作者以Marr的信息加工三层次框架为工具，从实现层、算法-表征层、计算理论层逐一分析语言模型在作为认知模型方面存在的问题。

Result: 分析表明，语言模型在实现层面不成立；在算法-表征层面理由牵强；在计算理论层面也存在问题。因此将LMs作为认知模型是不恰当的。

Conclusion: 语言模型适合作为分析或研究工具，但直接称其为认知模型则夸大其作用，并助长了对大语言模型的过度宣传。

Abstract: Futrell and Mahowald claim LMs "serve as model systems", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.

</details>


### [137] [Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models](https://arxiv.org/abs/2601.10460)
*Abhinaba Basu,Pavan Chakraborty*

Main category: cs.CL

TL;DR: 本论文提出了Contextual StereoSet基准，通过在上下文中变化提示词的地点、时间和受众，发现模型偏见的测量结果有较大波动，强调了模型偏见评测对上下文的敏感性。提出了Context Sensitivity Fingerprints（CSF）方法，对偏见变化进行量化和分析。


<details>
  <summary>Details</summary>
Motivation: 现有模型偏见测试大多数在固定设置下进行，可能无法反映实际部署时在多变上下文中的偏见表现。需要更健壮的测试方法来评估真实世界中由于不同上下文引发的潜在偏见变动。

Method: 引入Contextual StereoSet基准，固定刻板印象内容，系统性调节上下文（如时间、地点、受众等）来考察模型偏见随情境改变的表现。对13个模型进行测试，提出并应用Context Sensitivity Fingerprints（CSF），用来量化各维度偏见变异及差异。

Result: 发现如将时间锚定在1990年（相较于2030年）会导致所有模型的刻板印象选择概率升高（p<0.05）；以八卦方式表述问题使5/6模型的偏见升高；“外群体”观察者视角也可让偏见分数变化高达13个百分点。这些现象在招聘、贷款、求助场景下均有复现。

Conclusion: 偏见测试如果只在固定条件下进行，其结果可能难以推广到复杂实际环境。CSF方法促使研究者关注“偏见在何种情境出现”而非“模型是否有偏见”，对模型评测和部署具有方法学意义。作者公开了数据集、代码和全部结果。

Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.
  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.
  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.
  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, "Under what conditions does bias appear?" rather than "Is this model biased?" We release our benchmark, code, and results.

</details>


### [138] [DR-Arena: an Automated Evaluation Framework for Deep Research Agents](https://arxiv.org/abs/2601.10504)
*Yiwen Gao,Ruochen Zhao,Yang Deng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文提出了DR-Arena，一个用于大语言模型（LLM）深度研究智能体自动化评估的系统，利用实时数据和动态任务设计以克服传统基准的局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测依赖静态数据集，存在任务广度有限、时间不匹配及数据污染等问题，难以真实反映智能体能力。因此，亟需一种能动态反映现实世界、覆盖更多维度和提升评估可靠性的自动化评测方法。

Method: DR-Arena系统通过实时获取网络热点，构建信息树，确保评测任务与现实世界同步。同时，系统自动生成结构化的评测任务，分别考查深度推理与广度理解两项能力，并引入自适应演化机制，依据模型表现动态提升任务难度，直至能力边界显现。

Result: 在六个先进的DR智能体上进行实验，DR-Arena评测结果与LMSYS Search Arena排行榜的人类偏好高度一致，Spearman相关系数达0.94，显示出自动化评测几乎能达到人工评选精度。

Conclusion: DR-Arena成功实现了无需人工介入的高效、可靠自动化智能体能力评测，为替代高成本人工评测提供了有效途径，推动了大型语言模型智能体评测方法的发展。

Abstract: As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.

</details>


### [139] [AEQ-Bench: Measuring Empathy of Omni-Modal Large Models](https://arxiv.org/abs/2601.10513)
*Xuan Luo,Lewei Yao,Libo Zhao,Lanqing Hong,Kai Chen,Dehua Tao,Daxin Tan,Ruifeng Xu,Jing Li*

Main category: cs.CL

TL;DR: 本文提出AEQ-Bench基准，用于系统性评估多模态大模型（OLMs）的同理心能力，涵盖音频和文本输入、输出；发现具备音频输出能力的OLMs在同理心水平上普遍优于仅能文本输出的模型，但细粒度语音表现的评估效果仍有限。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型应用愈加广泛，自动评估其同理心水平成为难点，因为同理心具有复杂的情感属性，现有基准测试手段有限，因此急需科学的评测方法。

Method: 作者提出AEQ-Bench基准，设计了两项核心能力检验：一是模型根据音频+文本输入理解情感线索并生成同理心回应，二是仅依据音频输出（无需语音转录）判断回应的同理心水平。基准在情境与语音语调上均有创新设置，并结合语言和副语言维度进行全面评测。

Result: 实验结果显示，具备音频输出能力的多模态大模型在同理心任务上普遍优于仅支持文本输出的模型。在粗粒度质量评测层面，模型结果与人类判断一致性较好，但在细粒度副语言表现能力上，模型评估表现仍不可靠。

Conclusion: AEQ-Bench为多模态大模型同理心能力评估提供了新方法。当前OLMs在音频输出和粗粒度同理心检验上有明显进步，但在情感性副语言细节评估上仍需进一步改进。

Abstract: While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.

</details>


### [140] [PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://arxiv.org/abs/2601.10532)
*Chengbing Wang,Wuqiang Zheng,Yang Zhang,Fengbin Zhu,Junyi Cheng,Yi Xie,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: LLMs在情感支持方面存在不足，作者提出了一种双向心理学驱动的同理心奖励建模方法（PERM），用以提升模型的同理心表现，在实验和用户测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在人类交互型应用中常缺乏有效的情感支持能力，现有同理心奖励模型仅单方面评估，忽略了同理心互动的双向性。

Method: 提出PERM方法，将同理心评价分解为支持者（共情回应和表达）和求助者（情感接收）两个视角，并辅以旁观者角度综合监控，兼顾双向和整体质量，结合强化学习训练模型。

Result: 在主流情感智能基准和工业对话数据集上，PERM超过现有方法10%；用户盲测中，70%偏好该方法生成的同理心回复。

Conclusion: PERM有效提升了LLM的同理心水平，优于以往单向评价方法，为情感支持类应用带来更佳效果。

Abstract: Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\%. Furthermore, a blinded user study reveals a 70\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.

</details>


### [141] [Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure](https://arxiv.org/abs/2601.10566)
*Syed Naveed Mahmood,Md. Rezaur Rahman Bhuiyan,Tasfia Zaman,Jareen Tasneem Khondaker,Md. Sameer Sakib,Nazia Tasnim,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出了一种新的选择性知识擦除框架KIF，用于大模型的有效遗忘，区分真正擦除和行为抑制，达到了高效擦除和模型效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型知识遗忘方法通常只是在表面行为上进行抑制，实际上模型内部依然保留被要求遗忘的知识，这对于GDPR合规和模型安全并不可靠。因此，亟需一种能够真正从模型内部移除敏感或非法知识的方法。

Method: 作者提出了知识免疫框架（KIF），通过识别和干预模型内部的激活表征，动态抑制与特定主题相关的表征，同时采用参数高效的适配方式，无需对整个模型进行完全重训练。该方法使用标准和推理先验的大模型（Llama、Mistral、Qwen、DeepSeek），并提出了结合表层泄漏和潜在痕迹留存的双重评价指标体系。

Result: KIF在各类模型上取得了接近理想（oracle）擦除效果（FQ约0.99），且模型效用几乎不受影响（MU=0.62），优于以往工作的稳定性与擦除性的平衡标准。标准基础模型在不同规模下表现为擦除无关性，而推理先验模型展现出架构上的本质差异。

Conclusion: KIF实现了更真实、更持久的知识遗忘，有效解决了传统方法中擦除与效用不可兼得的问题，并首次系统性地诊断了不同模型架构下的遗忘机制。这为大模型安全和合规提供了新方案。

Abstract: Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.

</details>


### [142] [Form and Meaning in Intrinsic Multilingual Evaluations](https://arxiv.org/abs/2601.10580)
*Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨了在多语言环境下，常用的语言模型内在评估指标（如困惑度、每字符比特）对模型性能比较的有效性与局限性。通过实验证明这些指标在不同语言间并不具备普适可比性，并分析其背后的原因。


<details>
  <summary>Details</summary>
Motivation: 在多语言（或跨语言）条件下，人们常直接用如困惑度等指标对模型性能进行比较，但对这些指标背后隐含的假设和局限性缺乏系统性讨论。因此需要明确、验证这些假设是否成立及其影响。

Method: 作者显式列举并分析了多语言条件下内在评估指标的关键假设，通过在两个多并行语料库上，使用单语和多语模型，对六种评估指标进行对比实验，进一步结合'形式-意义'理论进行讨论。

Result: 实验发现：当前的内在评估指标（包括困惑度等）在不同语言之间、或不同类型模型之间，并不具备通用的可比性。

Conclusion: 本文指出当前多语言环境下常用的内在评估指标存在明确的局限性，无法普遍比较语言模型性能，同时以‘形式-意义’理论提供了合理解释，强调今后应针对多语言模型研究更适应的评估方法。

Abstract: Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.

</details>


### [143] [Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs](https://arxiv.org/abs/2601.10645)
*Yuxi Xia,Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: LLM的自信表达常常与真实准确性不符，作者提出TracVC方法追踪自信表达与训练数据的对应关系，揭示了自信表达往往源于形式模仿而非内容依据。


<details>
  <summary>Details</summary>
Motivation: 大模型通过表达自信提高用户信任，但这种自信常常是过度的，具体与事实准确性不符，因此有必要分析模型自信表达的根源。

Method: 作者提出TracVC方法，将信息检索和影响力估计结合，用以追踪大模型生成的自信表达与训练数据之间的关联。此外，提出“content groundness”新指标衡量自信是否基于与任务相关的数据，而非泛化的自信语句。方法在OLMo和Llama问答场景下进行验证。

Result: 分析结果显示，OLMo2-13B等模型的自信表达常常受到与问题内容无关的训练数据影响，更多是在模仿表层的自信表达，而非真正基于与问题相关的内容依据。

Conclusion: 当前大模型训练过程存在局限性，模型学会了如何表达自信，却没有学会在何种情形下自信是合理的。本文工作为改进大模型更可靠地表达自信、提升其可信度提供了分析基础。

Abstract: Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\textbf{Trac}ing \textbf{V}erbalized \textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.

</details>


### [144] [Detecting Winning Arguments with Large Language Models and Persuasion Strategies](https://arxiv.org/abs/2601.10660)
*Tiziano Labruna,Arkadiusz Modzelewski,Giorgio Satta,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 论文提出了一种基于多策略说服评分的方法，通过引导大语言模型关注包括攻击声誉、转移注意力、操控性措辞等6类说服策略，以提升对文本说服力的预测效果。实验证明策略引导能改善说服性预测，并增强模型的可解释性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 理解和检测文本中的说服性对深入认知人类交流至关重要，但现有方法难以细致区分不同说服策略的影响。因此，作者希望通过细分说服策略并利用大语言模型，提高说服性识别的准确性与解释性。

Method: 作者在包含多个带注释的论证数据集（例如Change My View、Anthropic/Persuasion等）上，基于大语言模型设计了一个多策略说服评分系统，引导模型从6种说服策略出发评估说服性，并划分话题进行进一步分析。

Result: 结果显示，策略引导推理方法显著提升了说服性预测的表现。且在不同话题下的表现分析显示该方法有较好的稳健性。团队还公开了带主题注释的数据集，有助于未来研究。

Conclusion: 结构化、关注策略的提示（prompting）不仅提升了模型的可解释性和鲁棒性，还为说服性文本评估提供了更有效的方法。该工作为后续基于策略的说服分析奠定了基础。

Abstract: Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.

</details>


### [145] [LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals](https://arxiv.org/abs/2601.10700)
*Gilat Toker,Nitay Calderon,Ohad Amosy,Roi Reichart*

Main category: cs.CL

TL;DR: 文章提出了LIBERTy，一个基于大型语言模型生成结构化反事实数据对的解释性评测框架，用于推动高层次概念（如性别、经验）对模型行为影响的可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 高风险领域的决策者需要理解模型行为背后的高层次概念，但目前基于反事实的忠实性评测十分依赖昂贵的人类标注且未必完美，因此急需自动化且结构化的评测方法。

Method: 作者提出通过结构化因果模型（SCM）定义文本生成过程，对高层概念进行干预，并利用大型语言模型生成反事实，从而打造LIBERTy数据集，涵盖疾病检测、简历筛查、职场暴力预测等三大场景，并提出了创新的order-faithfulness评测指标。

Result: 实验对五类主流模型、多种方法进行了广泛评测，发现现有模型在概念级解释的忠实性上仍有很大提升空间。额外地，作者还系统性地分析了模型对干预敏感性，指出商业模型在应对人口统计概念时变得更不敏感，推测源于后期安全相关微调。

Conclusion: LIBERTy为概念级解释性方法的忠实性评测和改进提供了标准化、高效且细致的基准，有助于未来更可靠的可解释AI发展。

Abstract: Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.

</details>


### [146] [Grounding Agent Memory in Contextual Intent](https://arxiv.org/abs/2601.10702)
*Ruozhen Yang,Yucheng Jiang,Yueqi Jiang,Priyanka Kargupta,Yunyi Zhang,Jiawei Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的记忆系统STITCH，通过上下文意图来索引和检索长对话或任务历史，有效提升了大语言模型在长任务中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长时间、目标导向的任务中容易因为同一实体或事实在不同目标和约束下重复出现，导致记忆系统检索到语境不匹配的信息，影响推理效果。

Method: 提出STITCH（Structured Intent Tracking in Contextual History）系统，在任务轨迹的每一步索引中加入结构化的检索线索——上下文意图，如当前潜在目标、动作类型和关键实体类型。推理时依据意图筛选和优先检索历史片段，减少无关干扰。并设计了CAME-Bench基准用于评估方法效果。

Result: 在CAME-Bench和LongMemEval数据集上，STITCH取得了当前最优表现，比最强基线提升了35.6%，且任务轨迹越长提升越大。

Conclusion: 意图索引显著降低了检索噪音，STITCH支持大语言模型在复杂长任务中的稳健推理。

Abstract: Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.

</details>


### [147] [MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching](https://arxiv.org/abs/2601.10712)
*Changle Qu,Sunhao Dai,Hengyi Cai,Jun Xu,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种名为MatchTIR的新方法，实现了对大语言模型（LLM）利用工具进行推理时的细粒度奖励分配，大幅提升了模型在复杂多轮任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 以往方法使用的奖励分配过于粗糙，无法有效区分每一步工具调用的贡献，尤其在多轮长任务中难以识别和奖励有效推理步骤。作者希望通过精细化的奖励机制，更好地指导LLM高效合理地使用工具。

Method: 作者将奖励分配建模为一个二部图匹配问题，通过将模型预测轨迹与真实轨迹逐步匹配，实现了回合级（turn-level）的稠密奖励。此外，提出了回合级与轨迹级结合的双层优势估算，使每一步工具交互能获得独立、准确的奖励信号。

Result: 在三个基准任务上，所提出的MatchTIR方法表现优异，尤其在长任务和多轮推理中，4B参数模型甚至超过了大多数8B参数对手。

Conclusion: MatchTIR通过引入精细的奖励分配机制与双层优势估算，显著提升了LLM在工具集成推理场景下的能力，对长链推理和多轮交互任务具有重要意义。

Abstract: Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [148] [Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates](https://arxiv.org/abs/2601.09740)
*Oumaima Barhoumi,Mohamed H Zaki,Sofiène Tahar*

Main category: cs.RO

TL;DR: 本论文提出了一种将Barrier Certificates（BCs）与可解释的交通冲突指标（如TTC）结合的形式化安全框架，并通过SMT验证和自适应控制机制提升自动驾驶汽车的安全性。实验证明极大减少了不安全事件，实现了可解释且可证明的自动驾驶安全保障。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统依赖数据驱动模型，难以解释且缺乏强有力的安全保障。与人类驾驶车辆的交互使安全难度进一步增加，因此需要一种既可解释又可形式化验证安全性的解决方案。

Method: 提出融合Barrier Certificates（形式安全保障）和Time-to-Collision（TTC，可解释安全指标）的方法，并利用SMT求解器对安全条件进行验证，结合自适应控制机制实时保证车辆始终满足安全约束。

Result: 在真实高速公路数据集上评估显示，该方法显著减少了TTC低于3秒的不安全交互事件，最高降幅达40%，部分车道甚至完全消除了安全冲突。

Conclusion: 提出的安全框架兼具可解释性和可验证性，为自动驾驶车辆实现可扩展与实用的安全保障提供了有效途径。

Abstract: Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.

</details>


### [149] [Interprofessional and Agile Development of Mobirobot: A Socially Assistive Robot for Pediatric Therapy Across Clinical and Therapeutic Settings](https://arxiv.org/abs/2601.09838)
*Leonie Dyck,Aiko Galetzka,Maximilian Noller,Anna-Lena Rinke,Jutta Bormann,Jekaterina Miller,Michelle Hochbaum,Julia Siemann,Jördis Alboth,Andre Berwinkel,Johanna Luz,Britta Kley-Zobel,Marcine Cyrys,Nora Flöttmann,Ariane Vogeler,Mariia Melnikova,Ira-Katharina Petras,Michael Siniatchkin,Winfried Barthlen,Anna-Lisa Vollmer*

Main category: cs.RO

TL;DR: 本文介绍了Mobirobot，一款为康复中的儿童量身定制运动方案的社交辅助机器人，并详述了其在儿科临床的设计、开发及初步应用。


<details>
  <summary>Details</summary>
Motivation: 尽管社交辅助机器人有提升儿童治疗参与度的潜力，但其落地需要符合具体场景、与用户协作共创的设计。因此，开发一个能适应儿科病房特点、满足儿童康复需求的智能机器人势在必行。

Method: 采用敏捷、人本的开发方法，机器人（基于NAO平台）设计过程涵盖多学科临床团队与最终用户的深度参与。功能包括易设置、可定制运动、互动对话和直观GUI监控与反馈。机器在实际手术及精神科儿童病房内集成测试，持续根据用户反馈优化设计。

Result: 实地部署揭示关键设计需求和可用性约束。通过利益相关方反馈，优化了人机交互、动作能力和技术配置。初步可行性研究正进行中，评估其可接受性、易用性和治疗益处，数据来源包括问卷、行为观察与访谈。

Conclusion: Mobirobot展示了多专业、以利益相关者为主导的开发模式能打造适用于动态住院环境的社交辅助系统。初步发现凸显了场景融合、系统健壮性与低侵入式设计的重要性，尽管传感器等技术及患者招募还存挑战，但已为后续研究与应用奠定基础。

Abstract: Introduction: Socially assistive robots hold promise for enhancing therapeutic engagement in paediatric clinical settings. However, their successful implementation requires not only technical robustness but also context-sensitive, co-designed solutions. This paper presents Mobirobot, a socially assistive robot developed to support mobilisation in children recovering from trauma, fractures, or depressive disorders through personalised exercise programmes.
  Methods: An agile, human-centred development approach guided the iterative design of Mobirobot. Multidisciplinary clinical teams and end users were involved throughout the co-development process, which focused on early integration into real-world paediatric surgical and psychiatric settings. The robot, based on the NAO platform, features a simple setup, adaptable exercise routines with interactive guidance, motivational dialogue, and a graphical user interface (GUI) for monitoring and no-code system feedback.
  Results: Deployment in hospital environments enabled the identification of key design requirements and usability constraints. Stakeholder feedback led to refinements in interaction design, movement capabilities, and technical configuration. A feasibility study is currently underway to assess acceptance, usability, and perceived therapeutic benefit, with data collection including questionnaires, behavioural observations, and staff-patient interviews.
  Discussion: Mobirobot demonstrates how multiprofessional, stakeholder-led development can yield a socially assistive system suited for dynamic inpatient settings. Early-stage findings underscore the importance of contextual integration, robustness, and minimal-intrusion design. While challenges such as sensor limitations and patient recruitment remain, the platform offers a promising foundation for further research and clinical application.

</details>


### [150] [How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces](https://arxiv.org/abs/2601.09856)
*Andrew Stratton,Phani Teja Singamaneni,Pranav Goyal,Rachid Alami,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: 该论文探讨了人类周围的机器人导航问题，重点分析了人类运动预测对机器人导航和人类体验的影响，发现常用的人类运动预测指标未必反映实际效果，同时机器人高效导航可能牺牲人类效率和舒适度。


<details>
  <summary>Details</summary>
Motivation: 机器人被广泛应用于仓库、医院、制造工厂和家庭等与人协作的场所，安全高效的人机共处要求机器人能够准确预测人类动态行为，但由于人类行为的随机性和数据稀缺性，这一问题具有挑战性。作者希望通过量化评估，揭示人类运动预测质量对实际导航和用户体验的影响。

Method: 作者设计了一个受限工作空间内机器人与两个人类共同活动的实验场景，并基于两种不同机器人平台，在来自不同地区的两组被试（共80人）间开展了用户研究，系统评估了人类运动预测质量对机器人导航、人类生产效率和体验评价的影响。

Result: 研究结果表明：1）常用的平均偏差误差指标不能有效预测机器人导航实际性能及人类主观感受；2）受限环境下，常见的人类合作假设往往不成立，人类经常不主动配合机器人，导致性能下降；3）机器人高效导航通常以牺牲人类效率和舒适度为代价。

Conclusion: 现有评价指标的局限性严重影响机器人导航系统的真实表现评估。未来设计人机共处系统时，需重新考虑如何平衡机器人效率与人类体验，加强对人类行为不可预测性的适应。

Abstract: Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.

</details>


### [151] [SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping](https://arxiv.org/abs/2601.09920)
*Ruopeng Huang,Boyu Yang,Wenlong Gui,Jeremy Morgan,Erdem Biyik,Jiachen Li*

Main category: cs.RO

TL;DR: 本文提出SyncTwin数字孪生框架，实现了动态与视觉遮挡环境下的稳健、安全抓取。其核心在于快速三维重建和现实-仿真同步，使抓取更加准确和安全。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，动态变化和视觉遮挡极大挑战了机器人抓取的准确性与安全性，目前缺乏在此类复杂场景下可靠的方法。

Method: SyncTwin框架分离离线和在线阶段：离线通过VGGT算法从RGB图像快速重建三维物体，构建可复用的几何库；在线阶段通过点云分割和彩色ICP配准同步物体状态，不断更新数字孪生，并在仿真中生成安全、动态可行的运动轨迹，最后闭环执行到实物机器人。

Result: 在动态与被遮挡的场景实验中，SyncTwin提升了抓取精度和运动安全性。

Conclusion: 数字孪生的实时同步有效提升了机器人在复杂现实场景中的抓取稳健性与安全性，验证了SyncTwin框架的实际价值。

Abstract: Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.

</details>


### [152] [In-the-Wild Compliant Manipulation with UMI-FT](https://arxiv.org/abs/2601.09988)
*Hojung Choi,Yifan Hou,Chuer Pan,Seongheon Hong,Austin Patel,Xiaomeng Xu,Mark R. Cutkosky,Shuran Song*

Main category: cs.RO

TL;DR: 该论文提出了UMI-FT，一个可手持的六轴力/扭矩数据收集平台，能够在每根手指上测量力和扭矩，并结合多模态数据用于学习高效的灵巧操作策略。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要对施加的力进行精准调控，市场上的力/扭矩传感器价格高昂、体积大且易损，限制了基于力感知的大规模策略学习。

Method: 作者开发了UMI-FT装置，在每根手指上安装紧凑型六轴力/扭矩传感器，配合RGB、深度和位姿数据。通过采集多模态数据，训练能预测位置、抓取力和刚度的自适应合规策略，再利用标准合规控制器执行。

Result: 在白板擦拭、刺穿西葫芦、灯泡插入等三项高接触力敏感任务中，UMI-FT学习的策略能稳定调节外部接触力和内部抓取力，明显优于不具备合规或力感知的基线方法。

Conclusion: UMI-FT为大规模学习合规操作技能提供了可扩展路径，并开源硬件和软件以促进相关研究的广泛采用。

Abstract: Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.

</details>


### [153] [CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments](https://arxiv.org/abs/2601.10116)
*Xintong Zhang,Junfeng Chen,Yuxiao Zhu,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: CoCoPlan是一个专门为多机器人系统设计的任务规划与通信优化框架，可在有限通信条件下协同提升任务效率与通信策略。实验显示其性能优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统需要高效协同完成任务，但实际应用中受限于通信距离和带宽，传统方法无法很好适应动态变化的任务分布，导致效率低下。解决在受限通信下提升全队协同和任务分配效率的问题非常重要。

Method: 提出了名为CoCoPlan的统一框架，该框架采用分支定界结构将任务分配与通信事件联合建模，并设计自适应目标函数权衡任务效率与通信延迟，同时包含通信事件优化模块智能选择通信恢复的时机和地点。

Result: 大规模仿真和硬件实验显示，CoCoPlan比最新方法任务完成率提高22.4%，通信开销降低58.6%，支持动态环境下多达100台机器人，高效应对复杂2D/3D环境。

Conclusion: CoCoPlan有效解决了多机器人受限通信下的协作与任务规划难题，使系统在实际动态环境中表现更优，具有良好的扩展性和应用潜力。

Abstract: Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.

</details>


### [154] [Terrain-Adaptive Mobile 3D Printing with Hierarchical Control](https://arxiv.org/abs/2601.10208)
*Shuangshan Nors Li,J. Nathan Kutz*

Main category: cs.RO

TL;DR: 本论文提出了一种结合AI扰动预测、多模态传感器融合和分层硬件控制的移动式3D打印系统，实现了在非结构化地形上的高精度打印。


<details>
  <summary>Details</summary>
Motivation: 现有龙门架式3D打印系统虽然精度高，但缺乏移动性；而移动平台则难以在崎岖不平的地面上保持打印质量。如何在不规则地形上兼顾平台的移动性和打印精度，是当前移动3D打印面临的挑战。

Method: 作者提出了一个闭环的感知-学习-执行框架。通过AI模块利用IMU、视觉和深度传感器学习地形与扰动的关系，进行主动补偿，采用三层硬件控制架构（路径规划、预测性底盘-机械臂协同和高精度执行）实现协同控制。

Result: 在具有斜坡和不平表面等复杂地形的室外实验中，该系统实现了移动平台状态下的亚厘米级打印精度。

Conclusion: 通过AI与硬件的深度融合，为非结构化环境下的自主建造奠定了实用基础。

Abstract: Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.

</details>


### [155] [A Unified Framework for Kinematic Simulation of Rigid Foldable Structures](https://arxiv.org/abs/2601.10225)
*Dongwook Kwak,Geonhee Cho,Jiook Chung,Jinkyu Yang*

Main category: cs.RO

TL;DR: 本文提出了一种自动化方法，用于生成任意刚性可折叠结构的运动学约束矩阵，简化了相关分析与计算。


<details>
  <summary>Details</summary>
Motivation: 随着折纸结构形式的发展（如厚折纸、切纸、以及多层纸结构），对统一的运动学约束分析需求日益增长，但在不同结构间缺乏统一的约束处理方法。

Method: 作者提出了一种自动化工具：由扩展的数据结构生成面-铰链图，基于最小环集（minimum cycle basis）提取所有约束，并利用螺旋理论构建速率级的约束矩阵，完整描述折叠结构中的旋转与平动耦合环约束。

Result: 该工具可自动化计算和可视化多种复杂结构的展开与折叠运动，省去了手工计算复杂约束的繁琐与错误。

Conclusion: 本文的方法为各类刚性可折叠结构的统一运动学分析提供了高效、准确和可扩展的解决方案，有助于推动折纸结构研究和实际应用的发展。

Abstract: Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential. Yet a general method that consolidates their loop constraints has been lacking. We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS). From a minimally extended data schema, the tool constructs the facet-hinge graph, extracts a minimum cycle basis that captures all constraints, and assembles a velocity-level constraint matrix via screw theory that encodes coupled rotation and translation loop closure. The framework computes and visualizes deploy and fold motions across diverse RFS while eliminating tedious and error-prone constraint calculations.

</details>


### [156] [Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control](https://arxiv.org/abs/2601.10233)
*Yifan Xue,Ze Zhang,Knut Åkesson,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种移动机器人在复杂动态环境下安全高效导航的新方法，结合了高斯过程与能量基学习预测，为障碍物规避设计学习型屏障函数（Barrier Functions），并通过改进的CBF方法实现安全导航，在仿真和真实环境中表现优越于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统反应式安全控制器（如CBF）仅基于当前障碍状态设计避障策略，容易忽略复杂动态环境导致未来碰撞的风险。为提升移动机器人在动态、可能有凹陷障碍物环境下的安全性与效率，需要能结合运动预测并自适应的障碍规避方法。

Method: 利用能量基学习训练的神经网络对障碍物未来运动进行多模态预测；通过高斯过程在线学习障碍预测相关的屏障函数；采用无局部极小点的改进型CBF方法（MCBF）将学习到的屏障函数输入二次规划控制，实现安全导航；设计自动参数调整算法，使MCBF自适应屏障函数的动态变化。

Result: 在仿真与真实环境实验中，提出的框架在避障安全性与导航效率上均优于对比基线方法，能更好地适应复杂动态环境，显著降低碰撞风险。

Conclusion: 该方法结合运动预测、在线学习和自适应控制，有效提升了移动机器人在动态、复杂环境下的安全导航能力，为应对实际复杂环境提供了更具前景的解决方案。

Abstract: This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.

</details>


### [157] [The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation](https://arxiv.org/abs/2601.10268)
*Eszter Birtalan,Miklós Koller*

Main category: cs.RO

TL;DR: 本文通过模拟实验比较了不同密度与布局的触觉传感器配置对强化学习中机械手表现的影响，确定了一种表现最优的传感器布置方案。


<details>
  <summary>Details</summary>
Motivation: 当前机械手触觉传感器分布存在多样性，影响抓握稳定性，但尚缺乏系统分析各种传感器布局、密度对强化学习表现影响的研究。该研究希望找出最优的传感器配置以推动机械手及义肢设计进步。

Method: 采用仿真方法，在两个独立系统下对六种不同传感器的密度与布局配置进行性能评估。实验控制了物理仿真器、机械手模型及机器学习算法的变异，力求结论具有广泛适用性。

Result: 在六种模拟配置中，发现某一特定传感器布局在两个系统中都表现最优，同时也得出了其他布局的特定和普适性影响。

Conclusion: 结果为寻找最佳机械手触觉传感器布局提供依据，可为今后机械手和义肢设计提供指导，提升其抓握稳定性和实际应用性能。

Abstract: Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.

</details>


### [158] [CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing](https://arxiv.org/abs/2601.10340)
*David Morilla-Cabello,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出了一种基于语义感知的异构机器人团队协同巡检框架CHORAL，实现了机器人能力感知、任务分配与路径规划的有机结合。实验证明在仿真和实际环境中显著提升了巡检效率和安全性，系统已开源。


<details>
  <summary>Details</summary>
Motivation: 当前面向未知与复杂环境的异构机器人团队协同作业中，缺乏将场景语义理解与连续路径规划深度融合的方法，导致无法充分适配环境特性并充分发掘各机器人的能力优势。

Method: 先以侦查型飞行机器人采集环境信息，利用开放词汇视觉模型构建度量-语义地图，定位需精细巡检区域并实现能力感知路径生成。将这些信息纳入异构机器人多目标分配与路径规划模型，实现联合任务分派和轨迹规划。

Result: 在仿真与实际多机器人平台巡检任务中，提出的方法能更好地结合机器人能力与环境实际，显著提升路径安全性与巡检效率。

Conclusion: 融合语义感知与能力建模的异构机器人团队路径规划框架可提升复杂环境下协作巡检的效果。相关代码已开源，有助于领域内的复现与应用推广。

Abstract: Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot's strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform's navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.

</details>


### [159] [FastStair: Learning to Run Up Stairs with Humanoid Robots](https://arxiv.org/abs/2601.10365)
*Yan Liu,Tao Yu,Haolin Song,Hongbo Zhu,Nianzong Hu,Yuzhi Hao,Xiuyong Yao,Xizhe Zang,Hua Chen,Jie Zhao*

Main category: cs.RO

TL;DR: 本论文提出FastStair框架，通过融合基于模型的落脚点规划器和多阶段强化学习，实现了人形机器人快速稳定地上楼梯，目前已在Oli机器人上取得优异表现，荣获机器人竞赛冠军。


<details>
  <summary>Details</summary>
Motivation: 人类轻松上楼梯，但人形机器人因需要兼具高敏捷性和稳定性而难以实现快速爬楼。现有模型无关强化学习虽能产生动态动作，但稳定性差且奖励设计依赖重，容易产生危险行为。基于模型的规划器虽可确保稳定但运动保守，速度受限，因此需要结合二者优点。

Method: 提出FastStair多阶段学习框架，将并行的基于模型的落脚点规划器嵌入RL训练，约束探索落脚点以实现动态可行和稳定性；训练初期优先保守以确保安全，后期通过微调得到速度特化策略专家，并融合为全速度平滑控制器（基于LoRA）；最终将控制器部署在Oli机器人进行实测。

Result: Oli机器人用该控制器成功以最高1.65 m/s的速度稳定上楼，12秒内攀登33级螺旋楼梯（每级17cm），表现出高鲁棒性和速度，打破传统上楼慢、易失稳的问题。

Conclusion: FastStair框架有效结合了模型驱动的稳定性和RL的灵活性，实现人形机器人高速上楼，表现优异且具推广意义，在权威赛事中夺冠验证其实用性。

Abstract: Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.

</details>


### [160] [Online identification of nonlinear time-varying systems with uncertain information](https://arxiv.org/abs/2601.10379)
*He Ren,Gaowei Yan,Hang Liu,Lifeng Cao,Zhijun Zhao,Gang Dang*

Main category: cs.RO

TL;DR: 提出了一种基于贝叶斯回归的符号学习（BRSL）新框架，能够实现数字孪生模型的可解释性、在线自适应和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需在实时监控和预测维护中兼具高预测精度、强可解释性和在线学习能力，但现有技术难以兼顾。贝叶斯方法虽擅长不确定性量化但可解释性不足，符号识别法虽可解释但仅支持离线处理，难以满足实时更新需求。

Method: 将在线符号发现建模为统一的概率状态空间模型，引入稀疏马蹄先验，将模型选择转化为贝叶斯推断任务，提出带遗忘因子的在线递归算法，并推导保证后验分布合理性的递归条件，同时对参数估计收敛性进行严格分析。

Result: 提出的BRSL框架能实时进行系统识别和不确定性评估，同时支持符号可解释和在线学习，验证案例显示该方法具有良好的可解释性、概率预测与在线自适应能力。

Conclusion: 该工作实现了面向数字孪生的可解释、具不确定性推断能力和在线自适应的系统建模，为复杂系统实时运维提供了更有效的技术路径。

Abstract: Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.

</details>
