<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 87]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.RO](#cs.RO) [Total: 49]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 本文提出了一种基于下一词概率（NTP）的高效轻量级视觉-语言模型（VLM）幻觉检测方法，具有较低计算成本但效果与主流方法相当。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型的幻觉（即生成文本与视觉内容不符）问题影响了其可靠性。现有幻觉检测方法依赖VLM自身或其他大型模型判断，这一过程计算消耗大、时延高，限制了实际应用。因此，亟需一种高效且实用的检测方法。

Method: 作者提出基于VLM输出的下一词概率（NTP）构建轻量级ML模型检测幻觉。具体做法是收集VLM生成内容，获得NTP信号，训练传统ML模型进行二分类，并构建了1400条人工标注幻觉与否的数据集进行评测。此外，还引入了仅基于文本的NTP特征，并通过集成原有VLM幻觉预测得分进一步提升性能。

Result: 基于NTP特征的传统ML模型在检测幻觉任务上表现优异，接近甚至可与大型VLM作为判别器的方式媲美。将文本NTP特征和VLM幻觉预测得分与原NTP特征融合后，性能进一步提升。

Conclusion: 基于NTP轻量级信号及其融合的传统ML方法能够高效准确地检测VLM幻觉，为实际应用提供了计算友好型解决思路，有望提升VLM的实用性和可靠性。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [2] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

TL;DR: 本文提出了一种利用黎曼几何空间（SPD矩阵空间）进行准合成数据生成的方法，提高了脱机手写签名验证（特别是写者无关场景）的泛化性能。通过在真实签名样本基础上生成合成特征，实现更强鲁棒性的签名验证。实验结果显示，该方法在西方与亚洲常用签名数据集上均表现出较低的错误率。


<details>
  <summary>Details</summary>
Motivation: 离线手写签名验证在写者无关设置下具有很大的挑战性——模型需在遇到全新写者时依然能准确识别。传统方法依赖于真实签名数据集训练，难以获得足够多的多样性样本，而且深度学习需要大量标注数据。该文通过合成新型数据，提升泛化能力，解决难以获取大量真实签名及数据稀缺问题。

Method: 作者将签名特征表示为对称正定（SPD）矩阵，用其构建黎曼高斯混合模型（Riemannian Gaussian Mixture, RGM），以少量真实样本为种子，拉取合成中心与方差用于生成“合成写者”。在这些中心周围用黎曼高斯分布采样，获得正负样本，从而构成合成数据集。后续采用度量学习框架，用相似与不相似的SPD点对进行训练，并在真实数据集上测试。

Result: 在西方和亚洲两种常用签名数据集上进行实验，无论是同集还是跨集评估，所提方法均表现出低错误率，优于传统的数据驱动和手工特征方法。证明使用黎曼几何空间合成数据能有效提升写者无关签名验证的泛化能力。

Conclusion: 准合成数据生成框架能为写者无关的签名验证系统提供更好的泛化性能。基于SPD空间与黎曼几何的合成方法，可缓解真实数据获取难、样本多样性不足等问题，对提升实际应用中的安全性与实用性具有重要意义。

Abstract: Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [3] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一款高效多模态图像生成系统，集成了文本生成图像、图像编辑和多图合成，性能卓越，生成高分辨率图片速度快，支持复杂交互与创意应用。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成任务如文本生成图像、图像编辑等大多各自为政，难以统一高效实现。同时，高分辨率图像生成和推理速度也是重要的工程难题。该系统旨在解决效率、统一性、分辨率和性能等核心问题。

Method: 采用高效的扩散Transformer结合强大的VAE，能大幅降低图像token数量，实现高效训练与高速生成。模型在海量文本-图像对数据上预训练，覆盖多领域。引入优化的多模态后训练，并整合推理加速（对抗蒸馏、分布匹配、量化、投机解码等），提升实用性。同时支持多任务联合训练。

Result: Seedream 4.0在文本生成图像及多模态图像编辑任务上实现了最新的SOTA性能，支持复杂任务如精准编辑、多图输入和输出，在多项评测中展现出卓越多模态能力，生成2K分辨率图片最快1.8秒。

Conclusion: Seedream 4.0不仅提升了传统文本生成图像系统的效率和能力，还把这些技术应用拓展为更互动、更具创造性和专业性的生成工具，推动了生成式AI在多领域的边界。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [4] [A Contrastive Learning Framework for Breast Cancer Detection](https://arxiv.org/abs/2509.20474)
*Samia Saeed,Khuram Naveed*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的乳腺癌检测方法，通过在大量未标注乳腺X光图片上训练ResNet-50网络，有效提升了小样本标注数据下的检测准确率。在INbreast和MIAS数据集上，该方法准确率达96.7%。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的乳腺癌早期检测方法受限于标注数据有限，导致准确率受影响。为解决数据标注稀缺问题，需要开发能在小样本条件下表现优异的新方法。

Method: 采用对比学习（Contrastive Learning）框架，先在大量未标注的乳腺X光片上使用半监督学习训练ResNet-50网络，通过多种图像增强与变换提升模型鲁棒性，最后利用少量有标注数据进行微调并进行最终评估。

Result: 在INbreast和MIAS基准数据集上，提出的方法取得了96.7%的乳腺癌检测准确率，超过现有最优方案。

Conclusion: 本文所提对比学习方法能够利用有限标注数据提升乳腺癌CAD系统的检测准确率，有望显著推动乳腺癌早期筛查的实际应用。

Abstract: Breast cancer, the second leading cause of cancer-related deaths globally,
accounts for a quarter of all cancer cases [1]. To lower this death rate, it is
crucial to detect tumors early, as early-stage detection significantly improves
treatment outcomes. Advances in non-invasive imaging techniques have made early
detection possible through computer-aided detection (CAD) systems which rely on
traditional image analysis to identify malignancies. However, there is a
growing shift towards deep learning methods due to their superior
effectiveness. Despite their potential, deep learning methods often struggle
with accuracy due to the limited availability of large-labeled datasets for
training. To address this issue, our study introduces a Contrastive Learning
(CL) framework, which excels with smaller labeled datasets. In this regard, we
train Resnet-50 in semi supervised CL approach using similarity index on a
large amount of unlabeled mammogram data. In this regard, we use various
augmentation and transformations which help improve the performance of our
approach. Finally, we tune our model on a small set of labelled data that
outperforms the existing state of the art. Specifically, we observed a 96.7%
accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.

</details>


### [5] [Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data](https://arxiv.org/abs/2509.20479)
*Simon Baeuerle,Pratik Khanna,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Damir Shakirov,Andreas Steimer,Ralf Mikut*

Main category: cs.CV

TL;DR: 基础模型（FMs）在公共数据集表现良好，但在真实工业图像数据上的质量检测任务中表现失败，显示AI工业应用的挑战。


<details>
  <summary>Details</summary>
Motivation: 手工标注数据集用于监督式AI模型非常耗时，制约了工业自动化中的高效模型部署，因此探索无需标注、能泛化多产品的基础模型成为热点。

Method: 作者将多种最新的基础模型应用于自定义工业图像数据（真实生产环境）和公共图像数据集，并对比两者上的表现来评估模型的泛化能力。

Result: 所有被测试的基础模型在自定义的工业图像数据上均表现不佳，但它们在公开基准数据集上的表现却很理想。

Conclusion: 当前基础模型虽然在公开数据上效果突出，但还无法直接迁移到实际工业质量检测中，凸显了实际应用和真实数据下的挑战。

Abstract: Foundation Models (FMs) have shown impressive performance on various text and
image processing tasks. They can generalize across domains and datasets in a
zero-shot setting. This could make them suitable for automated quality
inspection during series manufacturing, where various types of images are being
evaluated for many different products. Replacing tedious labeling tasks with a
simple text prompt to describe anomalies and utilizing the same models across
many products would save significant efforts during model setup and
implementation. This is a strong advantage over supervised Artificial
Intelligence (AI) models, which are trained for individual applications and
require labeled training data. We test multiple recent FMs on both custom
real-world industrial image data and public image data. We show that all of
those models fail on our real-world data, while the very same models perform
well on public benchmark datasets.

</details>


### [6] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 本文提出了一种通用神经空间（Neural Space, NS）框架，通过共享统一特征空间高效处理多种视觉和成像任务。采用轻量化CNN主干，提升多任务通用性与硬件适应性。


<details>
  <summary>Details</summary>
Motivation: 现有AI视觉模型大多针对特定任务定制，导致在多任务场景下效率低下，因为每种任务都需单独映射到不同的特征空间。研究动机是构建一种统一、高效、兼容多任务的特征空间，减少冗余，提升领域泛化能力。

Method: 采用编码器-解码器结构，预先对多种成像和视觉任务进行特征提取，编码器学习具备变换感知能力的通用表征，使多个下游任务可共享该特征空间。整个系统基于轻量化CNN主干，区别于大规模Transformer。

Result: 在该统一特征空间（NS）中，可高效完成去马赛克、去噪、深度估计、语义分割等视觉任务，并在提升模块通用性和资源利用效率方面取得良好效果。

Conclusion: 该方法为多任务视觉管道提供了高效、可扩展的新范式，降低了网络结构的冗余，提升了多任务下的泛化性能及硬件适配范围。

Abstract: The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [7] [Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment](https://arxiv.org/abs/2509.20484)
*Dani Manjah,Tim Bary,Benoît Gérin,Benoît Macq,Christophe de Vleeschouwer*

Main category: cs.CV

TL;DR: 本文旨在提升边缘设备摄像机系统模型更新的效率，通过高置信度与多样性结合的样本选择方法，在低传输成本下提升模型质量。


<details>
  <summary>Details</summary>
Motivation: 边缘摄像机系统常需在受限计算资源下频繁更新模型。如何在保证模型高质量的前提下，减少中心服务器到边缘设备间的数据传输，是实际部署中的一个重要难题。

Method: 作者提出利用中心服务器上复杂教师模型标注数据，再通过基于高置信度和多样性选择的主动学习策略，从流式图像数据中筛选最有价值的样本，仅传输这些样本用于边缘设备小模型训练。

Result: 实验证明，该高置信度与多样性联合的样本筛选策略，在与传统方法相似的训练迭代次数下，能以极少的数据集查询量，训练出高质量的小模型。

Conclusion: 结合高置信度与多样性的方法在图像数据主动选择和模型迁移场景中能有效平衡模型效果与传输开销，为边缘计算场景下的模型高效更新提供了有效方案。

Abstract: Edge camera-based systems are continuously expanding, facing ever-evolving
environments that require regular model updates. In practice, complex teacher
models are run on a central server to annotate data, which is then used to
train smaller models tailored to the edge devices with limited computational
power. This work explores how to select the most useful images for training to
maximize model quality while keeping transmission costs low. Our work shows
that, for a similar training load (i.e., iterations), a high-confidence
stream-based strategy coupled with a diversity-based approach produces a
high-quality model with minimal dataset queries.

</details>


### [8] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON是一种基于指令的虚拟试衣系统，通过自然语言实现对穿搭效果的细致控制，无需用户手动画掩码，提升虚拟试衣体验和控制能力。


<details>
  <summary>Details</summary>
Motivation: 目前虚拟试衣系统依赖于二值掩码进行生成控制，但制作合适的掩码既难、要求用户有背景知识，且在复杂造型变化下（如衣袖卷起）无法实现准确控制，极大限制实际应用。

Method: 该方法提出InstructVTON系统，结合视觉语言模型与图像分割模型，根据用户上传的图片和自然语言指令自动生成二值掩码，从而指导虚拟试衣生成。同时消除了对用户精准画掩码的需求，支持复杂、多变化的试衣场景。

Result: InstructVTON不仅能自动生成高质量二值掩码，还能兼容现有的虚拟试衣模型，在造型控制上取得了业界领先效果，显著提升了用户体验和系统的灵活性。

Conclusion: InstructVTON简化了虚拟试衣流程，提高了对复杂造型变化的支持能力，推动了基于自然语言的智能试衣应用在虚拟时尚领域的发展。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [9] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的指纹识别模型DeepAFRNet，有效应对因恶意篡改导致的指纹识别难题，并在现实指纹数据集上取得优异表现，展示了实用潜力。


<details>
  <summary>Details</summary>
Motivation: 篡改指纹（如人为破坏指纹纹理）可影响自动生物识别系统的准确性，对边境管控、刑侦、财政认证等应用构成安全风险，因此迫切需要鲁棒性的指纹识别方法来对抗这种威胁。

Method: 作者提出DeepAFRNet模型，基于VGG16网络抽取高维特征，利用余弦相似度进行嵌入比对，并在SOCOFing真实篡改指纹集的不同难度级别上评估了模型表现。还对决策阈值对识别准确率的影响进行了敏感性实验。

Result: DeepAFRNet在Easy、Medium、Hard三个难度级别上分别达到了96.7%、98.76%、99.54%的高识别准确率，但发现阈值放宽时准确率会急剧下降，表现出系统对阈值设定的敏感性。

Conclusion: DeepAFRNet在真实篡改指纹识别方面表现优异，为实际部署提供支持，并弥补以往模型仅在合成数据或受限协议下评估的不足，强调了阈值选择对生物识别系统实用性的关键作用。

Abstract: Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [10] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

TL;DR: 本文提出将预训练Vision Transformer的注意力图整合到体素表示中，以提升双手机器人操作的表现，并在RLBench基准测试中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 提高机器人学习中体素表示对高层语义信息的利用，特别是在复杂的双手操作任务中。

Method: 从自监督ViT模型DINOv2中提取注意力图，将其作为像素级显著性得分，并将这些注意力信息提升为3D体素网格中的特征，进而融合进行为克隆策略中。

Result: 将注意力引导的体素特征集成到最先进的体素策略中，在RLBench双手基准的所有任务中实现了平均8.2%的绝对提升和21.9%的相对提升。

Conclusion: 注意力图能够有效增强体素表征的语义性，结合行为克隆策略可显著提升机器人复杂操作任务的表现。

Abstract: We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [11] [A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management](https://arxiv.org/abs/2509.20580)
*Xinyang Mu,Yuzhen Lu,Boyang Deng*

Main category: cs.CV

TL;DR: 本文基于新构建的大规模蓝莓图像数据集，对多种先进实时目标检测器（YOLO v8-v12和RT-DETR v1-v2，共36种模型变体）在蓝莓检测任务中的表现进行了系统基准对比，并通过半监督学习方法提升模型检测精度。研究公开了数据集与代码，促进相关领域研究。


<details>
  <summary>Details</summary>
Motivation: 天然条件下的蓝莓识别受光照变化、果实遮挡及图像模糊等因素影响，现有深度学习检测器虽然强大，但需要大量包含复杂场景的数据，还面临速度、精度、内存的平衡，因此需要系统性地评估不同模型在实际应用场景下的性能表现。

Method: 1）构建包含66,179张带标注蓝莓果实的高质量数据集，涵盖各种光照和果实成熟阶段；2）系统对比YOLO（v8-v12）和RT-DETR（v1-v2）共36种实时检测模型的检测精度（mAP@50）、推理速度和内存消耗；3）在额外未标注图像集上应用基于Unbiased Mean Teacher的半监督学习方法对所有模型进行微调以提升检测性能。

Result: 在基准测试中，YOLOv12m和RT-DETRv2-X分别取得各自系列最高检测精度（mAP@50分别为93.3%和93.6%），中等规模模型在精度与速度上表现最优。半监督微调取得最高mAP@50为94.8%，部分模型性能提升最大达到2.9%。

Conclusion: YOLO和RT-DETR系统均能高效检测自然场景下蓝莓，其中RT-DETRv2-X结合半监督学习后结果最佳。中等规模模型适宜实际部署。半监督方法对跨域泛化仍需进一步研究。数据集与软件已公开，助力该领域后续发展。

Abstract: Blueberry detection in natural environments remains challenging due to
variable lighting, occlusions, and motion blur due to environmental factors and
imaging devices. Deep learning-based object detectors promise to address these
challenges, but they demand a large-scale, diverse dataset that captures the
real-world complexities. Moreover, deploying these models in practical
scenarios often requires the right accuracy/speed/memory trade-off in model
selection. This study presents a novel comparative benchmark analysis of
advanced real-time object detectors, including YOLO (You Only Look Once)
(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,
consisting of 36 model variants, evaluated on a newly curated dataset for
blueberry detection. This dataset comprises 661 canopy images collected with
smartphones during the 2022-2023 seasons, consisting of 85,879 labelled
instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide
range of lighting conditions, occlusions, and fruit maturity stages. Among the
YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while
RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR
variants. The inference time varied with the model scale and complexity, and
the mid-sized models appeared to offer a good accuracy-speed balance. To
further enhance detection performance, all the models were fine-tuned using
Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of
1,035 unlabeled images acquired by a ground-based machine vision platform in
2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with
RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into
SSL is needed to better leverage cross-domain unlabeled data. Both the dataset
and software programs of this study are made publicly available to support
further research.

</details>


### [12] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的ROI（感兴趣区域）增强策略，用于改进乳腺癌筛查的深度学习模型，尤其针对数据受限的Mini-DDSM数据集，取得了训练效果的温和提升。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习已经展现出自动解读乳腺X光片的潜力，但是受限于数据集分辨率有限和样本量较小，模型表现受到限制。因此，亟需探索简便而有效的样本增强方法，提升有限数据下的分类性能。

Method: 作者在Mini-DDSM数据集上，采用了一种训练阶段专用的ROI增强方法：用概率将全图替换为来自事先生成的（无需标注）ROI随机裁剪区域，并可引入轻微扰动（jitter）以增加多样性。训练过程中采用患者级的严格交叉验证，并以ROC-AUC、PR-AUC及训练效率（吞吐量、显存）为评价指标。该增强方法只影响训练，对推理无额外开销。

Result: 在Mini-DDSM数据集上，ROI增强策略（最佳参数p_roi = 0.10, alpha = 0.10）带来了平均ROC-AUC的适度提升，但在不同折中表现有波动，PR-AUC指标基本持平或略有降低。

Conclusion: 简明的数据中心ROI增强策略无需新增标注或模型结构改动，可在受限数据环境下温和提升乳腺X光片分类的效果，但提升幅度有限，实用性主要体现为高效扩展小样本训练表现。

Abstract: Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


### [13] [Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections](https://arxiv.org/abs/2509.20607)
*Jing Wu,Zirui Wang,Iro Laina,Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: 本文提出了一种利用镜面反射的单帧图片进行多视角三维重建的新方法，将镜中的虚像视为辅助视角，实现像素级虚拟视角生成，并通过新颖的对称损失函数提升位姿估计，支持动态场景，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 传统多视角三维重建通常需要多帧、复杂的硬件系统；而日常环境中大量存在镜面反射，镜像中自带空间几何信息，因此希望利用镜面反射降低三维重建成本，提高其适用性和实时性。

Method: 该方法将镜面下的反射视作一个新的辅助视角，设计物理有效的虚拟相机生成过程，实现唯一像素映射。为进一步利用镜面对称几何特性，引入对称性感知损失函数优化相机位姿估计。此外方法支持动态图像的逐帧三维恢复，并提供合成数据集以供定量评估。

Result: 在自制的16个Blender场景数据集与真实世界数据上进行大量实验，评估了点云和相机位姿等指标，实验结果表明方法准确有效，并兼容现有前馈重建模型。

Conclusion: 该框架能够高效地从单帧含镜像的图片中实现可靠的多视角三维重建，降低设备和拍摄复杂度，对动态图像同样适用，具有实际应用和进一步研究潜力。

Abstract: Mirror reflections are common in everyday environments and can provide stereo
information within a single capture, as the real and reflected virtual views
are visible simultaneously. We exploit this property by treating the reflection
as an auxiliary view and designing a transformation that constructs a
physically valid virtual camera, allowing direct pixel-domain generation of the
virtual view while adhering to the real-world imaging process. This enables a
multi-view stereo setup from a single image, simplifying the imaging process,
making it compatible with powerful feed-forward reconstruction models for
generalizable and robust 3D reconstruction. To further exploit the geometric
symmetry introduced by mirrors, we propose a symmetric-aware loss to refine
pose estimation. Our framework also naturally extends to dynamic scenes, where
each frame contains a mirror reflection, enabling efficient per-frame geometry
recovery. For quantitative evaluation, we provide a fully customizable
synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and
camera poses. Extensive experiments on real-world data and synthetic data are
conducted to illustrate the effectiveness of our method.

</details>


### [14] [Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery](https://arxiv.org/abs/2509.20628)
*Yiming Xiao,Archit Gupta,Miguel Esparza,Yu-Hsuan Ho,Antonia Sebastian,Hannah Weas,Rose Houck,Ali Mostafavi*

Main category: cs.CV

TL;DR: 提出了FacadeTrack，一个基于街景视频和语言引导的方法，用于灾后快速、透明地评估建筑可入住性，提升灾后应急管理和资源分配效率。


<details>
  <summary>Details</summary>
Motivation: 灾后判断建筑内是否有人的信息对应急救援、检查、资源分配至关重要。传统的卫星或航拍影像缺乏细节，难以判断建筑可居住性，而街景图像虽然细致但获取和关联难度高，因此需要更有效的评估手段。

Method: 提出FacadeTrack系统，将街景全景视频与地块进行关联，对图像进行立面校正，并基于语言引导提取如入口阻塞、临时遮挡、本地碎片等可解释性属性。系统包含单阶段透明决策和二阶段（感知与推理分离）决策两种策略，并对效果进行了对比。

Result: 在两次飓风后实地调查数据集上，二阶段方法的准确率（精度0.927、召回率0.781、F1值0.848）优于单阶段基线（精度0.943、召回率0.728、F1值0.822），同时中间属性和空间诊断揭示了残余错误的原因和位置，有助于后续质量控制。

Conclusion: 该流程可输出可审计、可扩展的建筑入住性评估结果，并可集成进地理空间和应急管理工作流中，为灾后响应提供有力技术支撑。

Abstract: Building-level occupancy after disasters is vital for triage, inspections,
utility re-energization, and equitable resource allocation. Overhead imagery
provides rapid coverage but often misses facade and access cues that determine
habitability, while street-view imagery captures those details but is sparse
and difficult to align with parcels. We present FacadeTrack, a street-level,
language-guided framework that links panoramic video to parcels, rectifies
views to facades, and elicits interpretable attributes (for example, entry
blockage, temporary coverings, localized debris) that drive two decision
strategies: a transparent one-stage rule and a two-stage design that separates
perception from conservative reasoning. Evaluated across two post-Hurricane
Helene surveys, the two-stage approach achieves a precision of 0.927, a recall
of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a
precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond
accuracy, intermediate attributes and spatial diagnostics reveal where and why
residual errors occur, enabling targeted quality control. The pipeline provides
auditable, scalable occupancy assessments suitable for integration into
geospatial and emergency-management workflows.

</details>


### [15] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 本研究探讨人类感知基本动画形状中的社会交互时，视觉特征与语义表征如何共同影响判断。通过问卷和相似性判断，发现语义信息（特别是动词嵌入）对于解释人类社会知觉至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管以往研究多关注视觉特征，但实际的人类社会知觉中还可能涉及更抽象的语义表征。本研究目的是探明人类在识别简单视觉形状中的社会交互时，除视觉信息外还使用了哪些语义成分。

Method: 研究分为两步：第一步，让参与者基于自己对动画中移动形状的印象贴标签。第二步，通过人类相似性判断，构建27种社会交互的表征几何，并与视觉特征模型、标签模型及动画描述的语义嵌入模型预测结果进行比较。

Result: 人类给出的标签差异较大，语义模型在解释人类社会知觉判断时对视觉模型起到补充作用；尤其是基于描述的动词嵌入对人类相似性判断的解释最佳。

Conclusion: 简单动画形状中的社会知觉反映了社会交互的语义结构，即人类在社会信息加工中结合了视觉和抽象的语义表征。

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [16] [Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance](https://arxiv.org/abs/2509.20684)
*Xiaowei Wang,Di Wang,Ke Li,Yifeng Wang,Chengjian Wang,Libin Sun,Zhihong Wu,Yiming Zhang,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跨视角地理定位（CVGL）框架EGS，在应对航拍视角变换和全局-局部语义聚合方面显著提升了跨域泛化能力，并在多项基准上取得了新的SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有CVGL方法难以应对无人机视角和视场变化带来的外观变异，且在捕捉全局语义和细粒度局部细节的一致性方面存在挑战，限制了泛化和精确定位能力。

Method: 提出了EGS框架：包含E(2)-Steerable CNN编码器增强对旋转与视角形变的特征稳健性；将局部特征节点与虚拟全局超节点构成图结构，实现全局语义与局部细节的融合和一致性建模。

Result: 在University-1652和SUES-200等公开基准实验中，EGS方法在跨域CVGL任务上大幅超越现有方法，取得了显著性能提升，刷新了当前最佳结果。

Conclusion: EGS框架有效提升了CVGL的跨域泛化与全局-局部特征一致性，具备更好的鲁棒性和实用价值，为复杂场景下的地理定位任务提供了新的解决方案。

Abstract: Cross-view geo-localization (CVGL) aims to match images of the same location
captured from drastically different viewpoints. Despite recent progress,
existing methods still face two key challenges: (1) achieving robustness under
severe appearance variations induced by diverse UAV orientations and fields of
view, which hinders cross-domain generalization, and (2) establishing reliable
correspondences that capture both global scene-level semantics and fine-grained
local details. In this paper, we propose EGS, a novel CVGL framework designed
to enhance cross-domain generalization. Specifically, we introduce an
E(2)-Steerable CNN encoder to extract stable and reliable features under
rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual
super-node that connects to all local nodes, enabling global semantics to be
aggregated and redistributed to local regions, thereby enforcing global-local
consistency. Extensive experiments on the University-1652 and SUES-200
benchmarks demonstrate that EGS consistently achieves substantial performance
gains and establishes a new state of the art in cross-domain CVGL.

</details>


### [17] [DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection](https://arxiv.org/abs/2509.20701)
*Jiayi Zuo,Songwei Pei,Qian Li*

Main category: cs.CV

TL;DR: 本文提出双路径边缘网络，有效提升红外小目标的检测精准度，兼顾结构语义与边缘细化。


<details>
  <summary>Details</summary>
Motivation: 由于红外小目标缺乏明显纹理及形态特征，极易淹没于复杂背景和噪声中。现有深度模型难以兼顾高分辨率空间细节与强语义上下文，导致特征错位，检测性能受限。

Method: 设计了双路径结构：一条为双向交互模块(Bidirectional Interaction Module)，结合局部与全局自注意力（其中全局模块基于Transformer架构），以捕捉多尺度特征依赖并整合语义关系；另一条为多边缘细化模块(Multi-Edge Refiner)，利用多尺度泰勒有限差分算子和注意力门控机制强化边缘细节，提升小目标位置精度并抑制噪声。

Result: 新方法在红外小目标检测任务中表现出更高的检测精度和鲁棒性，实现了对多尺度目标的精确定位与结构信息的保留。

Conclusion: 双路径边缘网络在检测含噪、高背景复杂下的红外小目标时有效结合了结构语义理解与边缘增强，解决了特征错位难题，为相关领域的目标检测提供了新的思路和高效解决方案。

Abstract: Infrared small target detection is crucial for remote sensing applications
like disaster warning and maritime surveillance. However, due to the lack of
distinctive texture and morphological features, infrared small targets are
highly susceptible to blending into cluttered and noisy backgrounds. A
fundamental challenge in designing deep models for this task lies in the
inherent conflict between capturing high-resolution spatial details for minute
targets and extracting robust semantic context for larger targets, often
leading to feature misalignment and suboptimal performance. Existing methods
often rely on fixed gradient operators or simplistic attention mechanisms,
which are inadequate for accurately extracting target edges under low contrast
and high noise. In this paper, we propose a novel Dual-Path Edge Network that
explicitly addresses this challenge by decoupling edge enhancement and semantic
modeling into two complementary processing paths. The first path employs a
Bidirectional Interaction Module, which uses both Local Self-Attention and
Global Self-Attention to capture multi-scale local and global feature
dependencies. The global attention mechanism, based on a Transformer
architecture, integrates long-range semantic relationships and contextual
information, ensuring robust scene understanding. The second path introduces
the Multi-Edge Refiner, which enhances fine-grained edge details using cascaded
Taylor finite difference operators at multiple scales. This mathematical
approach, along with an attention-driven gating mechanism, enables precise edge
localization and feature enhancement for targets of varying sizes, while
effectively suppressing noise. Our method provides a promising solution for
precise infrared small target detection and localization, combining structural
semantics and edge refinement in a unified framework.

</details>


### [18] [Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset](https://arxiv.org/abs/2509.20715)
*Ruixu Zhang,Yuran Wang,Xinyi Hu,Chaoyu Mai,Wenxuan Liu,Danni Xu,Xian Zhong,Zheng Wang*

Main category: cs.CV

TL;DR: 本文提出了“群体意图”及群体意图预测（GIF）这一新任务，并构建了首个大规模篮球群体意图预测数据集（SHOT）及相关预测方法（GIFT），实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 过去的意图识别主要关注单个人，忽视了实际场景中常见的群体意图，因此作者希望填补群体意图识别与预测研究的空白。

Method: 1）提出“群体意图”概念及“群体意图预测（GIF）”任务，即通过分析个人动作和互动，预测群体意图的产生时机；2）提出了SHOT数据集，收集和标注了1979个多视角篮球视频片段，记录多维个人属性；3）基于SHOT，提出GIFT模型，细粒度提取个人特征并建模群体动态，从而预测群体意图出现。

Result: 实验结果显示，SHOT数据集和GIFT方法可以有效地支持群体意图预测，为该领域研究提供了坚实基础。

Conclusion: 本文工作首次系统性提出并实现了群体意图预测的研究任务与数据集，验证了方法的有效性，对于群体意图相关问题的研究具有重要推动作用。

Abstract: Intention recognition has traditionally focused on individual intentions,
overlooking the complexities of collective intentions in group settings. To
address this limitation, we introduce the concept of group intention, which
represents shared goals emerging through the actions of multiple individuals,
and Group Intention Forecasting (GIF), a novel task that forecasts when group
intentions will occur by analyzing individual actions and interactions before
the collective goal becomes apparent. To investigate GIF in a specific
scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of
1,979 basketball video clips captured from 5 camera views and annotated with 6
types of individual attributes. SHOT is designed with 3 key characteristics:
multi-individual information, multi-view adaptability, and multi-level
intention, making it well-suited for studying emerging group intentions.
Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that
extracts fine-grained individual features and models evolving group dynamics to
forecast intention emergence. Experimental results confirm the effectiveness of
SHOT and GIFT, establishing a strong foundation for future research in group
intention forecasting. The dataset is available at
https://xinyi-hu.github.io/SHOT_DATASET.

</details>


### [19] [Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection](https://arxiv.org/abs/2509.20745)
*Yu Guo,Shengfeng He,Yuxu Lu,Haonan An,Yihang Tao,Huilin Zhu,Jingxian Liu,Yuguang Fang*

Main category: cs.CV

TL;DR: 本文提出了Neptune-X框架，通过生成与筛选结合的方式，解决海事目标检测中数据稀缺与泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 海事目标检测面临标注数据稀缺和跨场景泛化能力差的难题，尤其在真实复杂海洋环境下，现有模型表现不佳，急需提升其准确性与泛用性。

Method: 提出Neptune-X框架：（1）开发了X-to-Maritime多模态条件生成模型，能够合成多样且逼真的海事场景，其中双向目标-水域注意力模块可提升物体与水环境边界的真实性；（2）提出属性相关主动采样方法，根据任务相关性动态筛选合成样本，用于提升检测性能；（3）新建了首个面向生成学习的Maritime Generation Dataset，为评测与训练提供多语义条件数据。

Result: 大量实验表明，该方法无论在海事场景合成还是目标检测精度方面，均大幅超过现有方法，特别是在复杂、特殊场景下的表现显著提升。

Conclusion: Neptune-X为海事目标检测领域提供了数据合成与采样新范式，在数据稀缺和场景多样的条件下显著增强了任务性能和泛化能力。

Abstract: Maritime object detection is essential for navigation safety, surveillance,
and autonomous operations, yet constrained by two key challenges: the scarcity
of annotated maritime data and poor generalization across various maritime
attributes (e.g., object category, viewpoint, location, and imaging
environment). % In particular, models trained on existing datasets often
underperform in underrepresented scenarios such as open-sea environments. To
address these challenges, we propose Neptune-X, a data-centric
generative-selection framework that enhances training effectiveness by
leveraging synthetic data generation with task-aware sample selection. From the
generation perspective, we develop X-to-Maritime, a multi-modality-conditioned
generative model that synthesizes diverse and realistic maritime scenes. A key
component is the Bidirectional Object-Water Attention module, which captures
boundary interactions between objects and their aquatic surroundings to improve
visual fidelity. To further improve downstream tasking performance, we propose
Attribute-correlated Active Sampling, which dynamically selects synthetic
samples based on their task relevance. To support robust benchmarking, we
construct the Maritime Generation Dataset, the first dataset tailored for
generative maritime learning, encompassing a wide range of semantic conditions.
Extensive experiments demonstrate that our approach sets a new benchmark in
maritime scene synthesis, significantly improving detection accuracy,
particularly in challenging and previously underrepresented settings.The code
is available at https://github.com/gy65896/Neptune-X.

</details>


### [20] [AI-Enabled Crater-Based Navigation for Lunar Mapping](https://arxiv.org/abs/2509.20748)
*Sofia McLeod,Chee-Kheng Chng,Matthew Rodda,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种适用于长期月球测绘任务的陨石坑自主导航（CBN）端到端系统STELLA，并首次在模拟为期一年的月球测绘数据集CRESENT-365上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 过去陨石坑导航主要应用于短期、着陆任务，图像频繁、条件理想，但对于需长时间、角度多变、光照复杂的月球测绘任务，现有方法难以适用，因此需要全新方法。

Method: 提出STELLA系统，整合了基于Mask R-CNN的陨石坑检测、无描述子坑匹配、鲁棒姿态解算和批量轨道反算模块。此外，构建了模拟年尺度测绘条件的新数据集CRESENT-365，覆盖真实观测参数和地形变化。

Result: 实验显示，STELLA在多种视角、光照条件和不同月面区域均能实现米级位置和亚度姿态精度，优于现有方法，广泛适用于实际任务。

Conclusion: 本文首次系统验证了CBN方案在长期月球测绘任务中的可行性和精度，并为后续任务提供了参考与数据工具，推进了月球自主导航技术应用进步。

Abstract: Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon
observed on images as natural landmarks to determine the six degrees of freedom
pose of a spacecraft. To date, CBN has primarily been studied in the context of
powered descent and landing. These missions are typically short in duration,
with high-frequency imagery captured from a nadir viewpoint over well-lit
terrain. In contrast, lunar mapping missions involve sparse, oblique imagery
acquired under varying illumination conditions over potentially year-long
campaigns, posing significantly greater challenges for pose estimation. We
bridge this gap with STELLA - the first end-to-end CBN pipeline for
long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater
detector, a descriptor-less crater identification module, a robust
perspective-n-crater pose solver, and a batch orbit determination back-end. To
rigorously test STELLA, we introduce CRESENT-365 - the first public dataset
that emulates a year-long lunar mapping mission. Each of its 15,283 images is
rendered from high-resolution digital elevation models with SPICE-derived Sun
angles and Moon motion, delivering realistic global coverage, illumination
cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show
that STELLA maintains metre-level position accuracy and sub-degree attitude
accuracy on average across wide ranges of viewing angles, illumination
conditions, and lunar latitudes. These results constitute the first
comprehensive assessment of CBN in a true lunar mapping setting and inform
operational conditions that should be considered for future missions.

</details>


### [21] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 本文系统性研究了视觉模型和语言模型在训练时是否能在不同模态间形成共享的语义表征，并分析其对人类偏好的拟合情况。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉模型和语言模型通常在各自独立的数据和领域内训练，但已有研究发现它们的表征空间有部分重叠，本文旨在深入揭示这种对齐的网络层次、表现特征、人类语图配对偏好拟合，以及多实例聚合对齐的影响。

Method: 作者比较了图像和文本单模态模型在不同网络层级的表征对齐程度，并通过改变外观或语义内容来检验对齐的鲁棒性。此外，设计了“Pick-a-Pic”任务和多语句-单图片的匹配实验以评估模型嵌入与人类判断的一致性，还研究了相同概念多实例聚合时对齐度变化。

Result: 对齐度在两类模型中均于中后期网络层达到最大，且对齐对外观变化鲁棒、但对语义变化敏感。无论一对一还是多对一的图片-文本匹配，模型嵌入空间反映了人类的细致偏好。不同实例嵌入的均值会提升，而非削弱表征对齐度。

Conclusion: 视觉和语言单模态网络能够收敛于共享的、与人类判断高度一致的语义表征代码，并且通过多实例聚合还能进一步增强该对齐性。

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [22] [FreeInsert: Personalized Object Insertion with Geometric and Style Control](https://arxiv.org/abs/2509.20756)
*Yuhong Zhang,Han Wang,Yiwen Wang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需训练的图像插入方法FreeInsert，能够基于3D几何信息将定制对象插入任意场景图像，并实现精确的几何控制与风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑方法在个性化对象插入任务中存在两个主要局限：（1）缺乏对插入对象的几何控制，主要依赖文本描述，无法精确地操纵对象的3D形状或视角；（2）插入对象与背景之间经常风格不统一，影响图像的真实感。此外，实现高质量插入通常依赖于大量训练。

Method: 提出FreeInsert框架：首先利用现有的3D生成模型将2D对象转换为3D，并在3D层级进行交互式编辑，实现几何控制（如形状、视角）；然后从指定视角重新渲染为2D图像。渲染图像提供几何约束，并与扩散模型适配器实现的风格和内容控制结合，最终用扩散模型生成具有几何控制与风格一致性的编辑结果。整个过程无需额外训练。

Result: FreeInsert能够实现在任意场景下，几何可控且风格一致的对象插入，无需针对每个任务或对象进行训练。实验结果显示其优于现有方法，插入对象更加自然真实，具有良好的几何可控性和风格融合能力。

Conclusion: FreeInsert方法凭借训练无关、3D几何控制和风格一致性，有效解决了个性化图像插入中的主要难题，为多样化和高质量个性化图像合成任务开辟了新路径。

Abstract: Text-to-image diffusion models have made significant progress in image
generation, allowing for effortless customized generation. However, existing
image editing methods still face certain limitations when dealing with
personalized image composition tasks. First, there is the issue of lack of
geometric control over the inserted objects. Current methods are confined to 2D
space and typically rely on textual instructions, making it challenging to
maintain precise geometric control over the objects. Second, there is the
challenge of style consistency. Existing methods often overlook the style
consistency between the inserted object and the background, resulting in a lack
of realism. In addition, the challenge of inserting objects into images without
extensive training remains significant. To address these issues, we propose
\textit{FreeInsert}, a novel training-free framework that customizes object
insertion into arbitrary scenes by leveraging 3D geometric information.
Benefiting from the advances in existing 3D generation models, we first convert
the 2D object into 3D, perform interactive editing at the 3D level, and then
re-render it into a 2D image from a specified view. This process introduces
geometric controls such as shape or view. The rendered image, serving as
geometric control, is combined with style and content control achieved through
diffusion adapters, ultimately producing geometrically controlled,
style-consistent edited images via the diffusion model.

</details>


### [23] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: CustomEnhancer提出了一种用于增强个性化数字人像生成效果的无训练增强框架，引入新融合方法和高效的反演技术，实现了高保真的个性生成、精准控制及快速推理，超过现有方案。


<details>
  <summary>Details</summary>
Motivation: 虽然现有文本到图像扩散模型在合成逼真数字人像方面取得了很大进展，但个性化定制模型依然存在场景劣化、控制力不足和身份感知不佳等问题。作者希望解决这些瓶颈，提升数字人像个性化的准确性和多样性，并优化生成效率。

Method: 提出了CustomEnhancer框架，通过零样本的人脸交换和预训练扩散模型，增强个性化模型的特征表达；引入三路融合的PerGeneration架构，结合两个反向潜在空间对个性化模型的核心空间进行操控；同时提出ResInversion，一种基于预扩散机制的高效反演方法。整个流程无需重新训练控制器即可实现对生成过程的精准调控。

Result: 实验表明，CustomEnhancer在场景多样性、身份一致性、无训练精准控制等方面均达到或超过现有最优方案，同时使用ResInversion显著提升了推理速度（比NTI快129倍）。

Conclusion: CustomEnhancer不仅提升了个性化人像生成的质量和控制力，还大幅降低了运算和使用门槛，有望推动个性化数字人像生成在实际应用中的落地。代码将在论文接收后开源。

Abstract: Recently remarkable progress has been made in synthesizing realistic human
photos using text-to-image diffusion models. However, current approaches face
degraded scenes, insufficient control, and suboptimal perceptual identity. We
introduce CustomEnhancer, a novel framework to augment existing identity
customization models. CustomEnhancer is a zero-shot enhancement pipeline that
leverages face swapping techniques, pretrained diffusion model, to obtain
additional representations in a zeroshot manner for encoding into personalized
models. Through our proposed triple-flow fused PerGeneration approach, which
identifies and combines two compatible counter-directional latent spaces to
manipulate a pivotal space of personalized model, we unify the generation and
reconstruction processes, realizing generation from three flows. Our pipeline
also enables comprehensive training-free control over the generation process of
personalized models, offering precise controlled personalization for them and
eliminating the need for controller retraining for per-model. Besides, to
address the high time complexity of null-text inversion (NTI), we introduce
ResInversion, a novel inversion method that performs noise rectification via a
pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments
demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity
fidelity, training-free controls, while also showing the efficiency of our
ResInversion over NTI. The code will be made publicly available upon paper
acceptance.

</details>


### [24] [CompressAI-Vision: Open-source software to evaluate compression methods for computer vision tasks](https://arxiv.org/abs/2509.20777)
*Hyomin Choi,Heeji Han,Chris Rosewarne,Fabien Racapé*

Main category: cs.CV

TL;DR: 本文介绍CompressAI-Vision，这是一个面向计算机视觉任务的视频压缩评估平台，支持新编解码工具在多任务和推理场景下的公平比较，以优化下游视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在计算机视觉领域的广泛应用，视频数据压缩对下游任务的影响变得重要。目前缺乏一个统一的平台来评估和推动针对视觉任务优化的视频压缩技术的发展。

Method: 作者开发了一个开源平台CompressAI-Vision，支持在“远端推理”和“分拆推理”两种场景下，集成各种新的视频编码工具，并结合标准码流，对比压缩率与下游视觉任务准确率的平衡。

Result: 该平台已集成并测试了多种标准/开发中编解码器，在多个数据集上量化了比特率与任务准确率之间的权衡，展示了新工具对下游推理性能的影响，并获得了MPEG组织采纳用于标准制定。

Conclusion: CompressAI-Vision为视觉任务优化视频压缩提供了一个权威、统一的评估基线，有助于推动标准化进程，为相关领域研究和实际应用奠定了基础。

Abstract: With the increasing use of neural network (NN)-based computer vision
applications that process image and video data as input, interest has emerged
in video compression technology optimized for computer vision tasks. In fact,
given the variety of vision tasks, associated NN models and datasets, a
consolidated platform is needed as a common ground to implement and evaluate
compression methods optimized for downstream vision tasks. CompressAI-Vision is
introduced as a comprehensive evaluation platform where new coding tools
compete to efficiently compress the input of vision network while retaining
task accuracy in the context of two different inference scenarios: "remote" and
"split" inferencing. Our study showcases various use cases of the evaluation
platform incorporated with standard codecs (under development) by examining the
compression gain on several datasets in terms of bit-rate versus task accuracy.
This evaluation platform has been developed as open-source software and is
adopted by the Moving Pictures Experts Group (MPEG) for the development the
Feature Coding for Machines (FCM) standard. The software is available publicly
at https://github.com/InterDigitalInc/CompressAI-Vision.

</details>


### [25] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的DAC（Dual-supervised Asymmetric Co-training）框架，解决了带有跨域标注和无标注数据的半监督领域泛化问题，大幅提升了医学图像分割在新域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割领域，半监督领域泛化（SSDG）能够有效降低标注成本并提升泛化能力，但现实中源域常常存在标注数据缺失且标注与无标注数据之间存在域偏移，使得传统SSDG方法受限。作者针对这一更复杂更实际的CD-SSDG场景展开研究。

Method: 提出DAC双监督非对称协同训练框架。该方法基于两个子模型互相生成伪标签，并引入特征级监督和各自独立的辅助自监督任务，以缓解因域偏移导致的伪标签不准和模型坍塌问题，从而更好地学习域不变判别特征。

Result: 在三个真实世界医学图像分割数据集（Fundus、Polyp、SCGM）上进行了大量实验，验证了提出的DAC框架在面对跨域标注与无标注数据分布偏移时具有优异和稳健的泛化能力。

Conclusion: DAC框架有效解决了标注数据与无标注数据之间存在域偏移时医学图像分割的泛化问题，具有极高的应用价值和实际意义。

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [26] [Real-Time Object Detection Meets DINOv3](https://arxiv.org/abs/2509.20787)
*Shihua Huang,Yongjie Hou,Longfei Liu,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: 提出了DEIMv2，一个基于DEIM并融合DINOv3特征的新型实时目标检测训练框架，涵盖从大型GPU到极轻量级移动设备的多种模型尺寸，性能优异，打破多项业界记录。


<details>
  <summary>Details</summary>
Motivation: 现有的实时检测器如YOLO系列在速度与精度间难以获得最佳平衡，DEIM虽已领先，但仍有进一步提升空间，尤其是在多场景、超轻量化部署下希望继续突破性能-成本的天花板。

Method: 1. 主体方法是在现有DEIM框架上引入DINOv3特征，提出DEIMv2。2. X/L/M/S等较大模型采用DINOv3预训练或蒸馏骨干网络，并引入Spatial Tuning Adapter (STA)高效实现单尺度到多尺度转换，加强检测表现。3. 极小型的Nano/Pico/Femto/Atto模型则用HGNetv2网络并做深度和宽度裁剪，适应资源受限设备。4. 摒弃复杂解码器配以升级版Dense O2O，整体实现各级性能-计算成本最佳折中。

Result: DEIMv2各级模型均实现了同尺寸下新SOTA（最优）结果：最大DEIMv2-X模型仅用5030万参数实现57.8 AP，优于过往同尺寸模型。S级模型9.71M参数达到50.9 AP，第一款小于1000万参数超50AP模型。超小DEIMv2-Pico仅150万参数比肩YOLOv10-Nano但仅需其约一半参数。

Conclusion: DEIMv2通过统一设计和多项创新，全面提升了实时检测器在各种应用场景下的效能与资源效率，显著超越YOLO等主流方案，达成大中小型量级下最佳性能-成本比，成为实时检测主流方案。

Abstract: Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM
has become the mainstream training framework for real-time DETRs, significantly
outperforming the YOLO series. In this work, we extend it with DINOv3 features,
resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering
GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt
DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter
(STA), which efficiently converts DINOv3's single-scale output into multi-scale
features and complements strong semantics with fine-grained details to enhance
detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we
employ HGNetv2 with depth and width pruning to meet strict resource budgets.
Together with a simplified decoder and an upgraded Dense O2O, this unified
design enables DEIMv2 to achieve a superior performance-cost trade-off across
diverse scenarios, establishing new state-of-the-art results. Notably, our
largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters,
surpassing prior X-scale models that require over 60 million parameters for
just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model
(9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even
the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers
38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer
parameters.

</details>


### [27] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: 本文提出了一种名为DAC-LoRA的新框架，将动态对抗性训练集成到参数高效微调（PEFT）中，显著提升视觉-语言模型在对抗攻击下的鲁棒性，并且几乎无损于正常准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型被广泛应用于关键领域，但易受对抗性攻击威胁，尤其是作为主流骨干的CLIP。如果不提升对抗鲁棒性，可能造成重大安全隐患。因此，亟需兼顾高效微调与安全性的解决方案。

Method: 提出Dynamic Adversarial Curriculum（动态对抗性课程，DAC-LoRA），结合参数高效微调（如LoRA）与逐步增强的对抗训练。方法具体采用FOSC指导和TRADES损失函数，控制攻击难度递进，提升模型鲁棒性。该课程设计具备普适性，可应用于多种对抗攻击方法。

Result: DAC-LoRA在多项实验中显著提升了视觉-语言模型（如CLIP）在对抗样本下的鲁棒性，同时保持对干净样本的准确率不显著下降。

Conclusion: DAC-LoRA是一种有效、轻量且易于集成的方法，能够在不牺牲性能的前提下，显著增强VLM在安全关键应用下的对抗鲁棒性。

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like
autonomous driving, medical diagnosis, and content moderation. While
Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient
adaptation to specialized tasks, these models remain vulnerable to adversarial
attacks that can compromise safety-critical decisions. CLIP, the backbone for
numerous downstream VLMs, is a high-value target whose vulnerabilities can
cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial
Curriculum DAC-LoRA, a novel framework that integrates adversarial training
into PEFT. The core principle of our method i.e. an intelligent curriculum of
progressively challenging attack, is general and can potentially be applied to
any iterative attack method. Guided by the First-Order Stationary Condition
(FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements
in adversarial robustness without significantly compromising clean accuracy.
Our work presents an effective, lightweight, and broadly applicable method to
demonstrate that the DAC-LoRA framework can be easily integrated into a
standard PEFT pipeline to significantly enhance robustness.

</details>


### [28] [Federated Domain Generalization with Domain-specific Soft Prompts Generation](https://arxiv.org/abs/2509.20807)
*Jianhan Wu,Xiaoyang Qu,Zhangcheng Huang,Jianzong Wang*

Main category: cs.CV

TL;DR: 提出了一种从生成式视角出发的新方法 FedDSPG，可为每个客户端生成特定领域的soft prompt，有效提升联邦领域泛化（FDG）表现，达到当前最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于prompt learning的联邦领域泛化方法在提升泛化能力时，主要从训练数据中学习soft prompt，但这些prompt多样性有限，且难以处理未知领域的信息，制约了模型在新领域的适应能力。

Method: 提出FedDSPG方法，在训练阶段为每个领域引入并生成domain-specific soft prompt（DSPs），并将内容与领域知识整合进生成模型中。推理阶段利用生成器为未见领域生成响应的DSP，辅助下游任务在新领域的适应学习。

Result: 在多个公开数据集上进行了广泛实验，结果表明FedDSPG相比以往强基线方法有明显提升，并取得当前最优表现。

Conclusion: FedDSPG有效提升了联邦领域泛化任务中对未知领域的适应能力，为prompt learning在联邦学习领域提供了一种更具泛化能力的新思路。

Abstract: Prompt learning has become an efficient paradigm for adapting CLIP to
downstream tasks. Compared with traditional fine-tuning, prompt learning
optimizes a few parameters yet yields highly competitive results, especially
appealing in federated learning for computational efficiency. engendering
domain shift among clients and posing a formidable challenge for
downstream-task adaptation. Existing federated domain generalization (FDG)
methods based on prompt learning typically learn soft prompts from training
samples, replacing manually designed prompts to enhance the generalization
ability of federated models. However, these learned prompts exhibit limited
diversity and tend to ignore information from unknown domains. We propose a
novel and effective method from a generative perspective for handling FDG
tasks, namely federated domain generalization with domain-specific soft prompts
generation (FedDSPG). Specifically, during training, we introduce
domain-specific soft prompts (DSPs) for each domain and integrate content and
domain knowledge into the generative model among clients. In the inference
phase, the generator is utilized to obtain DSPs for unseen target domains, thus
guiding downstream tasks in unknown domains. Comprehensive evaluations across
several public datasets confirm that our method outperforms existing strong
baselines in FDG, achieving state-of-the-art results.

</details>


### [29] [Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning](https://arxiv.org/abs/2509.20813)
*Thanh Binh Le,Hoang Nhat Khang Vo,Tan-Ha Mai,Trong Nhan Phan*

Main category: cs.CV

TL;DR: 本文提出了LumbarCLIP，一个结合医学影像和文本报告的多模态对比预训练模型，在腰椎MRI影像和放射学描述上展现出新颖的诊断能力。


<details>
  <summary>Details</summary>
Motivation: 全球数百万人受腰背痛影响，急需能够同时分析复杂医学影像与文本报告的稳健诊断模型，以提升自动化诊断和临床辅助决策水平。

Method: 作者构建了含有轴位腰椎MRI与专业报告配对的数据集，采用了ResNet-50、Vision Transformer、Swin Transformer为视觉编码器和基于BERT的文本编码器。两种模态的特征通过可学习（线性或非线性）投影头映射到共享嵌入空间，通过标准化及软性CLIP损失进行稳定对比训练。

Result: LumbarCLIP在下游分类任务中表现优越，测试集准确率达95.00%，F1分数94.75%。消融实验表明线性投影头在跨模态对齐方面优于非线性投影。

Conclusion: LumbarCLIP为自动化肌骨诊断和临床决策支持提供了有前景的基础方法，尤其适用于融合画像与文本任务场景。

Abstract: Low back pain affects millions worldwide, driving the need for robust
diagnostic models that can jointly analyze complex medical images and
accompanying text reports. We present LumbarCLIP, a novel multimodal framework
that leverages contrastive language-image pretraining to align lumbar spine MRI
scans with corresponding radiological descriptions. Built upon a curated
dataset containing axial MRI views paired with expert-written reports,
LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin
Transformer) with a BERT-based text encoder to extract dense representations.
These are projected into a shared embedding space via learnable projection
heads, configurable as linear or non-linear, and normalized to facilitate
stable contrastive training using a soft CLIP loss. Our model achieves
state-of-the-art performance on downstream classification, reaching up to
95.00% accuracy and 94.75% F1-score on the test set, despite inherent class
imbalance. Extensive ablation studies demonstrate that linear projection heads
yield more effective cross-modal alignment than non-linear variants. LumbarCLIP
offers a promising foundation for automated musculoskeletal diagnosis and
clinical decision support.

</details>


### [30] [Poisoning Prompt-Guided Sampling in Video Large Language Models](https://arxiv.org/abs/2509.20851)
*Yuxin Cao,Wei Song,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: 本文提出了针对VideoLLMs（视频大语言模型）中先进的prompt-guided sampling机制的首个黑箱投毒攻击——PoisonVID，并在多种模型与采样策略上的测试结果显示该方法攻击成功率高达82%-99%。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的视频采样策略已经被证实存在漏洞，但现阶段最先进的prompt-guided采样机制的安全性尚未被研究。作者希望填补这一研究空白，检验现有VideoLLMs在安全性方面是否真的可靠。

Method: 作者设计了一种黑箱攻击方法（PoisonVID），采用闭环优化策略，通过构建由改写有害描述组成的描绘集，并结合影子VideoLLM和轻量级语言模型（如GPT-4o-mini），生成通用扰动来抑制有害帧在采样评分中的相关性。

Result: 在三种不同的prompt-guided采样策略和三种先进VideoLLMs上进行全面评估，PoisonVID的攻击成功率达到了82%到99%。

Conclusion: 目前主流的prompt-guided采样机制在面对黑箱投毒攻击时并不安全。未来需要开发更先进、更稳健的视频采样机制，以提升VideoLLMs的安全性。

Abstract: Video Large Language Models (VideoLLMs) have emerged as powerful tools for
understanding videos, supporting tasks such as summarization, captioning, and
question answering. Their performance has been driven by advances in frame
sampling, progressing from uniform-based to semantic-similarity-based and, most
recently, prompt-guided strategies. While vulnerabilities have been identified
in earlier sampling strategies, the safety of prompt-guided sampling remains
unexplored. We close this gap by presenting PoisonVID, the first black-box
poisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID
compromises the underlying prompt-guided sampling mechanism through a
closed-loop optimization strategy that iteratively optimizes a universal
perturbation to suppress harmful frame relevance scores, guided by a depiction
set constructed from paraphrased harmful descriptions leveraging a shadow
VideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively
evaluated on three prompt-guided sampling strategies and across three advanced
VideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the
importance of developing future advanced sampling strategies for VideoLLMs.

</details>


### [31] [Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer](https://arxiv.org/abs/2509.20854)
*Abdur Rehman,S M A Sharif,Md Abdur Rahaman,Mohamed Jismy Aashik Rasool,Seongwan Kim,Jaeho Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的自适应正则化方法GoR，用于平衡量化感知训练（QAT）和知识蒸馏（KD）过程中的损失权重，有效提升低比特量化模型在多任务上的性能。


<details>
  <summary>Details</summary>
Motivation: QAT和KD结合是AI模型压缩的重要手段，但当前方案在低比特量化下，任务损失和蒸馏损失权重难以合理平衡，导致模型性能受限。为此，亟需一种能够动态优化损失平衡的方法，以提升小型量化模型的实际表现。

Method: 作者提出了一种名为Game of Regularizer（GoR）的可学习正则项，仅用两个可训练参数，实现任务与蒸馏损失的动态加权和平衡，减少梯度冲突。同时还提出QAT-EKD-GoR框架，集成多教师模型提升模型泛化能力。

Result: 在图像分类、目标检测和大语言模型压缩实验中，GoR与QAT-EKD-GoR均优于现有SOTA QAT-KD方法。边缘设备实测证明，模型在保持全精度准确率的同时，推理速度更快，最优情况下甚至超过全精度模型表现。

Conclusion: GoR方法有效解决了QAT与KD损失平衡难题，在各类AI任务中提升了小型量化模型的性能和部署实用性，为实际硬件部署提供了更强有力的支持。

Abstract: Quantization-aware training (QAT) combined with knowledge distillation (KD)
is a promising strategy for compressing Artificial Intelligence (AI) models for
deployment on resource-constrained hardware. However, existing QAT-KD methods
often struggle to balance task-specific (TS) and distillation losses due to
heterogeneous gradient magnitudes, especially under low-bit quantization. We
propose Game of Regularizer (GoR), a novel learnable regularization method that
adaptively balances TS and KD objectives using only two trainable parameters
for dynamic loss weighting. GoR reduces conflict between supervision signals,
improves convergence, and boosts the performance of small quantized models
(SQMs). Experiments on image classification, object detection (OD), and large
language model (LLM) compression show that GoR consistently outperforms
state-of-the-art QAT-KD methods. On low-power edge devices, it delivers faster
inference while maintaining full-precision accuracy. We also introduce
QAT-EKD-GoR, an ensemble distillation framework that uses multiple
heterogeneous teacher models. Under optimal conditions, the proposed EKD-GoR
can outperform full-precision models, providing a robust solution for
real-world deployment.

</details>


### [32] [Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)](https://arxiv.org/abs/2509.20856)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 2017年LifeCLEF植物识别挑战致力于推进大规模自动化植物识别系统，比较网络收集的噪声数据和专家校对数据的训练效果，并对参赛方法及结果进行总结分析。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习图像识别技术的发展以及植物视觉知识平台的建设，实现覆盖欧、美1万种植物的自动植物识别系统成为可能。但目前大多数植物缺乏高质量图片，网络上有大量非官方、噪声较大的植物图片。作者想探究使用这些网络噪声数据与专家检查数据在训练植物识别模型上的效果差异。

Method: 组织识别挑战赛，分别利用大规模网络噪声数据（含大量标签错误）和小规模专家校对数据训练识别模型，并通过第三方应用收集的测试集对两者进行公平评测。同时整理参赛团队的方法与系统。

Result: 系统性比较了在同一测试集上，基于噪声数据与专家数据训练的识别系统的效果，展示了相关方法的表现和主流技术。

Conclusion: 网络获取的大规模噪声数据在特定条件下可以与小规模专家校对数据竞争，挑战总结了不同训练策略的优势和实际应用潜力。

Abstract: The 2017-th edition of the LifeCLEF plant identification challenge is an
important milestone towards automated plant identification systems working at
the scale of continental floras with 10.000 plant species living mainly in
Europe and North America illustrated by a total of 1.1M images. Nowadays, such
ambitious systems are enabled thanks to the conjunction of the dazzling recent
progress in image classification with deep learning and several outstanding
international initiatives, such as the Encyclopedia of Life (EOL), aggregating
the visual knowledge on plant species coming from the main national botany
institutes. However, despite all these efforts the majority of the plant
species still remain without pictures or are poorly illustrated. Outside the
institutional channels, a much larger number of plant pictures are available
and spread on the web through botanist blogs, plant lovers web-pages, image
hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge
presented in this paper aimed at evaluating to what extent a large noisy
training dataset collected through the web and containing a lot of labelling
errors can compete with a smaller but trusted training dataset checked by
experts. To fairly compare both training strategies, the test dataset was
created from a third data source, i.e. the Pl@ntNet mobile application that
collects millions of plant image queries all over the world. This paper
presents more precisely the resources and assessments of the challenge,
summarizes the approaches and systems employed by the participating research
groups, and provides an analysis of the main outcomes.

</details>


### [33] [TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting](https://arxiv.org/abs/2509.20857)
*Xiaonan Hu,Xuebing Li,Jinyu Xu,Abdulkadir Duran Adan,Letian Zhou,Xuhui Zhu,Yanan Li,Wei Guo,Shouyang Liu,Wenzhong Liu,Hao Lu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的植物计数方法TasselNetV4，实现了跨物种、跨场景、跨尺度的高精度植物计数，性能优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 传统的植物计数方法大多为特定物种设计，难以适应新物种与复杂场景，不能满足植物多样性的需求。

Method: 提出TasselNetV4，结合TasselNet的局部计数思想和类无关计数（CAC）的extract-and-match范式，基于Vision Transformer架构，融合多分支box-aware本地计数器，提高尺度鲁棒性。

Result: 在两个高难度数据集PAC-105和PAC-Somalia上进行实验，TasselNetV4在准确率和效率方面均优于主流CAC方法。

Conclusion: TasselNetV4作为通用视觉基础模型，在跨场景、跨尺度、跨物种的植物计数任务中表现卓越，为农业植株计数提供了高效方案。

Abstract: Accurate plant counting provides valuable information for agriculture such as
crop yield prediction, plant density assessment, and phenotype quantification.
Vision-based approaches are currently the mainstream solution. Prior art
typically uses a detection or a regression model to count a specific plant.
However, plants have biodiversity, and new cultivars are increasingly bred each
year. It is almost impossible to exhaust and build all species-dependent
counting models. Inspired by class-agnostic counting (CAC) in computer vision,
we argue that it is time to rethink the problem formulation of plant counting,
from what plants to count to how to count plants. In contrast to most daily
objects with spatial and temporal invariance, plants are dynamic, changing with
time and space. Their non-rigid structure often leads to worse performance than
counting rigid instances like heads and cars such that current CAC and
open-world detection models are suboptimal to count plants. In this work, we
inherit the vein of the TasselNet plant counting model and introduce a new
extension, TasselNetV4, shifting from species-specific counting to
cross-species counting. TasselNetV4 marries the local counting idea of
TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain
vision transformer and incorporates novel multi-branch box-aware local counters
used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and
PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC
models show that TasselNetV4 achieves not only superior counting performance
but also high efficiency.Our results indicate that TasselNetV4 emerges to be a
vision foundation model for cross-scene, cross-scale, and cross-species plant
counting.

</details>


### [34] [SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT](https://arxiv.org/abs/2509.20864)
*Botond Fazekas,Guilherme Aresta,Philipp Seeböck,Julia Mai,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督方法，将可微分的生物标志物拓扑引擎引入OCT视网膜分割，实现了解剖学上合理且更具准确性和泛化能力的层与病灶联合分割，在多个数据集上领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视网膜OCT分割方法在解剖合理性、层与病灶相互作用建模、以及拓扑正确性保障方面存在明显不足，难以用于严谨的临床诊断和随访。作者希望解决这些问题，提升分割性能与可靠性。

Method: 提出一种带有完全可微分生物标志物拓扑引擎的半监督学习模型，能够联合学习分割视网膜层和病灶，实现层与病灶的双向影响，并严格保证分割结果的解剖学合理性。模型还采用了解耦表示，将空间因素和风格因素分开，支持部分标注数据的有效利用。

Result: 在公开和内部OCT数据集上，提出的方法在病灶和层的分割精度上都优于当前最先进的方法，并能在存在病理变化时对视网膜层进行有效分割，具备良好泛化能力。

Conclusion: 引入解剖约束的半监督学习可实现更准确、稳健和可信赖的视网膜生物标志物分割，对临床OCT分析具有重要意义。

Abstract: Optical coherence tomography (OCT) is widely used for diagnosing and
monitoring retinal diseases, such as age-related macular degeneration (AMD).
The segmentation of biomarkers such as layers and lesions is essential for
patient diagnosis and follow-up. Recently, semi-supervised learning has shown
promise in improving retinal segmentation performance. However, existing
methods often produce anatomically implausible segmentations, fail to
effectively model layer-lesion interactions, and lack guarantees on topological
correctness.
  To address these limitations, we propose a novel semi-supervised model that
introduces a fully differentiable biomarker topology engine to enforce
anatomically correct segmentation of lesions and layers. This enables joint
learning with bidirectional influence between layers and lesions, leveraging
unlabeled and diverse partially labeled datasets. Our model learns a
disentangled representation, separating spatial and style factors. This
approach enables more realistic layer segmentations and improves lesion
segmentation, while strictly enforcing lesion location in their anatomically
plausible positions relative to the segmented layers.
  We evaluate the proposed model on public and internal datasets of OCT scans
and show that it outperforms the current state-of-the-art in both lesion and
layer segmentation, while demonstrating the ability to generalize layer
segmentation to pathological cases using partially annotated training data. Our
results demonstrate the potential of using anatomical constraints in
semi-supervised learning for accurate, robust, and trustworthy retinal
biomarker segmentation.

</details>


### [35] [Plant identification in an open-world (LifeCLEF 2016)](https://arxiv.org/abs/2509.20870)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF植物识别挑战赛2016年的组织和成果，重点是大规模（超过110,000张图片，1000种植物）开放类识别任务。


<details>
  <summary>Details</summary>
Motivation: 植物识别在生物多样性监测中的重要性日益凸显，现有方法大多限于封闭集，难以应对实际监测中遇到未见过的种类，因此需评估能适用于实际环境的开放集识别系统。

Method: 挑战赛利用众包平台收集了大量植物图像，并构建开放集识别任务，即系统除了准确分类训练集已知种类外，还需正确拒绝未知类别。通过参与者提交的不同方法对比评测，聚焦于“拒绝未知分类”的能力。

Result: 论文总结了各参与团队的方法和表现，表明开放集识别在大规模场景下仍存在较大挑战，尤其是在减少对未知种类的误判方面。

Conclusion: 开放集识别是植物智能识别向真实应用迈进的重要一步，但当前方法仍有待提升，未来需继续探索更鲁棒的分类与拒判机制，以适应生物多样性监测实际需求。

Abstract: The LifeCLEF plant identification challenge aims at evaluating plant
identification methods and systems at a very large scale, close to the
conditions of a real-world biodiversity monitoring scenario. The 2016-th
edition was actually conducted on a set of more than 110K images illustrating
1000 plant species living in West Europe, built through a large-scale
participatory sensing platform initiated in 2011 and which now involves tens of
thousands of contributors. The main novelty over the previous years is that the
identification task was evaluated as an open-set recognition problem, i.e. a
problem in which the recognition system has to be robust to unknown and never
seen categories. Beyond the brute-force classification across the known classes
of the training set, the big challenge was thus to automatically reject the
false positive classification hits that are caused by the unknown classes. This
overview presents more precisely the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [36] [SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering](https://arxiv.org/abs/2509.20871)
*Yan Zhang,Jiaqing Lin,Miao Zhang,Kui Xiao,Xiaoju Hou,Yue Zhao,Zhifei Li*

Main category: cs.CV

TL;DR: 本文提出了一种改进知识驱动视觉问答的新方法，提升了大语言模型在视觉问答推理方面的能力，且无需高昂的端到端训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接用图像描述（captions）帮助大语言模型理解图片，但描述往往带有与问题无关的噪声，且大模型难以理解视觉问答（VQA）任务，导致推理受限。

Method: 提出了Summarized Caption-Rerank Augmented VQA（SCRA-VQA）方法。首先用预训练视觉语言模型将图像转为文本描述，然后对这些描述进行总结和重排序以排除无关信息，并生成上下文样例辅助理解，帮助大语言模型提升对图像和问题的理解。

Result: 在6.7B参数量的大语言模型基础上，该方法在两个具有挑战性的知识型VQA数据集（OK-VQA和A-OKVQA）上取得了38.8%和34.6%的准确率，优于现有方法。

Conclusion: SCRA-VQA有效提升了大语言模型在知识型视觉问答中的推理能力和适应性，且无需昂贵的端到端训练，具有实际应用前景。

Abstract: Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual
Question Answering (KB-VQA). Recent methods use large language models (LLMs) as
knowledge engines for answering. These methods generally employ image captions
as visual text descriptions to assist LLMs in interpreting images. However, the
captions frequently include excessive noise irrelevant to the question, and
LLMs generally do not comprehend VQA tasks, limiting their reasoning
capabilities. To address this issue, we propose the Summarized Caption-Rerank
Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to
convert images into captions. Moreover, SCRA-VQA generates contextual examples
for the captions while simultaneously summarizing and reordering them to
exclude unrelated information. The caption-rerank process enables LLMs to
understand the image information and questions better, thus enhancing the
model's reasoning ability and task adaptability without expensive end-to-end
training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently
on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving
accuracies of 38.8% and 34.6%. Our code is available at
https://github.com/HubuKG/SCRA-VQA.

</details>


### [37] [The Unanticipated Asymmetry Between Perceptual Optimization and Assessment](https://arxiv.org/abs/2509.20878)
*Jiabei Zhang,Qi Wang,Siyu Wu,Du Chen,Tianhe Wu*

Main category: cs.CV

TL;DR: 论文揭示了感知优化过程中，常用的保真度目标和对抗性目标在优化与图像质量评估（IQA）任务之间存在不对称性，即优秀的IQA指标未必适合优化过程，且该不一致性在对抗训练下更加明显。


<details>
  <summary>Details</summary>
Motivation: 当前感知优化主要依赖于促进语义一致性和视觉真实感的保真度损失，以及带来细节增强的对抗性损失。然而，这些损失在实际优化效果与作为评估指标（IQA）性能之间的关系尚未充分探讨，特别是在近年来对抗训练广泛应用的背景下。

Method: 作者通过系统性实验对比，不同保真度损失函数及对抗性目标在感知优化（如超分辨、图像生成）和IQA评估任务上的表现。此外，还比较不同结构的判别器（如Patch级、卷积、vanilla和Transformer）对优化和评估的影响。

Result: （1）在对抗训练下，表现优秀的IQA指标并不一定适合做损失函数，存在明显不对齐；（2）判别器虽在优化中能有效抑制伪影，但其特征迁移到IQA模型作为骨干网络效果有限；（3）局部Patch和卷积结构的判别器更有助于细节还原。

Conclusion: 论文强调优化损失与IQA指标存在转移和作用上的本质不一致，建议未来设计更具原则性的损失函数，并拓展对判别器结构和功能的理解，以促进感知优化与评估的一体化。

Abstract: Perceptual optimization is primarily driven by the fidelity objective, which
enforces both semantic consistency and overall visual realism, while the
adversarial objective provides complementary refinement by enhancing perceptual
sharpness and fine-grained detail. Despite their central role, the correlation
between their effectiveness as optimization objectives and their capability as
image quality assessment (IQA) metrics remains underexplored. In this work, we
conduct a systematic analysis and reveal an unanticipated asymmetry between
perceptual optimization and assessment: fidelity metrics that excel in IQA are
not necessarily effective for perceptual optimization, with this misalignment
emerging more distinctly under adversarial training. In addition, while
discriminators effectively suppress artifacts during optimization, their
learned representations offer only limited benefits when reused as backbone
initializations for IQA models. Beyond this asymmetry, our findings further
demonstrate that discriminator design plays a decisive role in shaping
optimization, with patch-level and convolutional architectures providing more
faithful detail reconstruction than vanilla or Transformer-based alternatives.
These insights advance the understanding of loss function design and its
connection to IQA transferability, paving the way for more principled
approaches to perceptual optimization.

</details>


### [38] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: 提出了一种新型VQA模型——IOG-VQA，通过对象交互自注意力机制和基于GAN的去偏方法，有效提升了视觉问答任务的泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型易受训练数据偏差影响，导致过度依赖表面特征，泛化能力不足。因此需要设计既能理解复杂视觉关系，又能减少数据偏见的VQA模型。

Method: 提出了IOG-VQA模型。方法包括：1）对象交互自注意力机制，捕捉图像中对象间复杂关系；2）基于GAN的去偏框架，生成去偏分布数据，促使模型学习更鲁棒和泛化的特征；3）有效结合视觉和文本信息。

Result: 在VQA-CP v1和v2数据集上的大量实验显示，IOG-VQA在处理有偏与不平衡数据分布时，性能优于现有VQA方法。

Conclusion: 要推进VQA任务，应同时解决对象交互建模和数据集偏差问题。IOG-VQA为解决这两大难题提供了有效方案。

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring
models to understand and reason about visual content to answer questions
accurately. Existing VQA models often struggle with biases introduced by the
training data, leading to over-reliance on superficial patterns and inadequate
generalization to diverse questions and images. This paper presents a novel
model, IOG-VQA, which integrates Object Interaction Self-Attention and
GAN-Based Debiasing to enhance VQA model performance. The self-attention
mechanism allows our model to capture complex interactions between objects
within an image, providing a more comprehensive understanding of the visual
context. Meanwhile, the GAN-based debiasing framework generates unbiased data
distributions, helping the model to learn more robust and generalizable
features. By leveraging these two components, IOG-VQA effectively combines
visual and textual information to address the inherent biases in VQA datasets.
Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that
our model shows excellent performance compared with the existing methods,
particularly in handling biased and imbalanced data distributions highlighting
the importance of addressing both object interactions and dataset biases in
advancing VQA tasks. Our code is available at
https://github.com/HubuKG/IOG-VQA.

</details>


### [39] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: 本论文提出结合低秩时序建模与扩散后验采样的视频恢复方法，用于提升去除视频结构噪声和伪影的能力，并在心脏超声视频去雾任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒主成分分析通过低秩+稀疏分解处理视频噪声，但稀疏性假设不适用于真实视频的复杂变化，导致恢复质量有限。

Method: 提出“核扩散”（Nuclear Diffusion）方法，将低秩时序建模与扩散生成模型后验采样结合，增强对时间结构噪声与复杂背景伪影的建模能力。

Result: 在心脏超声视频去雾应用中，核扩散方法相比传统RPCA在对比度增强（gCNR）和信号保留性（KS统计量）方面有更好表现。

Conclusion: 模型驱动的时序结构与深度生成先验的结合，有望用于高保真的视频去噪与恢复任务。

Abstract: Video sequences often contain structured noise and background artifacts that
obscure dynamic content, posing challenges for accurate analysis and
restoration. Robust principal component methods address this by decomposing
data into low-rank and sparse components. Still, the sparsity assumption often
fails to capture the rich variability present in real video data. To overcome
this limitation, a hybrid framework that integrates low-rank temporal modeling
with diffusion posterior sampling is proposed. The proposed method, Nuclear
Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac
ultrasound dehazing, and demonstrates improved dehazing performance compared to
traditional RPCA concerning contrast enhancement (gCNR) and signal preservation
(KS statistic). These results highlight the potential of combining model-based
temporal models with deep generative priors for high-fidelity video
restoration.

</details>


### [40] [FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies](https://arxiv.org/abs/2509.20890)
*Shuqiao Liang,Jian Liu,Renzhang Chen,Quanlong Guan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过检测合成图像在像素依赖性和解码过程中的痕迹，有效识别高真实感合成图像，并引入了一款名为FerretNet的轻量级神经网络，实验证明其在多种生成模型下检测效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型（如VAE、GAN、LDM）能力提升，合成图像越来越逼真，传统检测方法难以分辨，因此亟需开发更高效鲁棒的检测方法。

Method: 研究了合成图像生成过程导致的两类伪影（潜在分布偏差和解码平滑效应），基于马尔可夫随机场的局部像素依赖关系，设计了通过像素邻域重构方式揭示纹理连续性和边缘一致性中断。基于此，提出了仅有1.1M参数的轻量化神经网络FerretNet。

Result: FerretNet仅在4类ProGAN数据集上训练，在涵盖22种生成模型的开放世界基准上平均准确率达97.1%，比现有最好方法高10.6%。

Conclusion: FerretNet利用生成图像的像素局部依赖特征，实现了高效且强鲁棒的合成图像检测，在多模型场景下具有突出性能，适用于实际检测需求。

Abstract: The increasing realism of synthetic images generated by advanced models such
as VAEs, GANs, and LDMs poses significant challenges for synthetic image
detection. To address this issue, we explore two artifact types introduced
during the generation process: (1) latent distribution deviations and (2)
decoding-induced smoothing effects, which manifest as inconsistencies in local
textures, edges, and color transitions. Leveraging local pixel dependencies
(LPD) properties rooted in Markov Random Fields, we reconstruct synthetic
images using neighboring pixel information to expose disruptions in texture
continuity and edge coherence. Building upon LPD, we propose FerretNet, a
lightweight neural network with only 1.1M parameters that delivers efficient
and robust synthetic image detection. Extensive experiments demonstrate that
FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an
average accuracy of 97.1% on an open-world benchmark comprising across 22
generative models, surpassing state-of-the-art methods by 10.6%.

</details>


### [41] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: 该论文提出MoTIF框架，扩展了可解释的概念瓶颈模型（CBM）到视频分类中，解决了视频中的时序依赖性问题，实现了对视频数据的更好解释和理解，同时保持了良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 虽然CBM在图像可解释性上表现突出，但将其应用到含有时序性的复杂视频数据存在挑战，尤其是事件和动作的时间依赖无法直接通过静态方法处理。

Method: 提出了MoTIF框架，将transformer结构融入概念瓶颈，实现了对视频中语义概念（如动作或属性）随时间分布的建模，并支持任意长度序列的处理。此外，通过模型结构可从全局、局部和时序三个层面对概念进行解释。

Result: 实验结果显示，MoTIF不仅能够将基于概念的建模范式有效迁移到视频分类任务，还能够提供竞争性的性能，并对视频中概念的作用提供清晰可解释的分析。

Conclusion: MoTIF扩展了概念瓶颈模型在视频分析领域的适用性，实现了对时序语义信息的有效建模与可解释性分析，在理解复杂动作及事件时具有优势。

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [42] [FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data](https://arxiv.org/abs/2509.20905)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文提出FSMODNet框架，通过跨模态特征融合以及可变形注意力机制，显著提升了极少标注数据下的多光谱目标检测性能，并在公开数据集上超越了多种最新基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统多光谱目标检测任务需要大量标注数据，且在照明复杂或环境变化时性能受限。为解决少样本条件下多光谱（可见光+热成像）目标检测难题，作者希望设计一个能充分利用两种模态互补性的方案。

Method: 提出以跨模态特征集成为核心的FSMODNet框架，利用可变形注意力机制高效融合可见光和热成像的特征信息，提高模型在复杂环境下的自适应能力。

Result: FSMODNet在两个公开数据集上的极少样本检测实验中均表现出色，显著优于以现有SOTA模型为基线构建的对比方法。

Conclusion: 通过有效的跨模态融合策略，FSMODNet框架提升了多光谱极少样本目标检测的准确性和鲁棒性，为该领域提供了新的解决思路。

Abstract: Few-shot multispectral object detection (FSMOD) addresses the challenge of
detecting objects across visible and thermal modalities with minimal annotated
data. In this paper, we explore this complex task and introduce a framework
named "FSMODNet" that leverages cross-modality feature integration to improve
detection performance even with limited labels. By effectively combining the
unique strengths of visible and thermal imagery using deformable attention, the
proposed method demonstrates robust adaptability in complex illumination and
environmental conditions. Experimental results on two public datasets show
effective object detection performance in challenging low-data regimes,
outperforming several baselines we established from state-of-the-art models.
All code, models, and experimental data splits can be found at
https://anonymous.4open.science/r/Test-B48D.

</details>


### [43] [Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences](https://arxiv.org/abs/2509.20906)
*Julius Pesonen,Arno Solin,Eija Honkavaara*

Main category: cs.CV

TL;DR: 本文提出使用粒子滤波（Particle Filter）方法，通过摄像头测量序列，实现远距离场景下的三维目标定位，尤其适用于计算资源有限或深度估计不可靠的情况，如无人机林火监控。实验显示该方法高效灵活且对检测算法无依赖。


<details>
  <summary>Details</summary>
Motivation: 传统三维定位方法依赖稠密深度估计或3D场景重建，但在远距离目标定位及资源受限的实际任务（如无人机监控）中难以应用。因此亟需一种高效、适用范围广的三维目标定位方法。

Method: 作者提出利用粒子滤波方法，根据摄像机序列测量（含GNSS定位和图像分割结果）进行目标位置估计，适用于单目标和多目标情境。方法在三维仿真环境及无人机实际影像中进行了验证。

Result: 实验结果表明，粒子滤波法能在上述极端场景下（远距离、有限计算资源）实现高效且准确的三维目标定位，且独立于目标检测方法，对不同任务具有良好适应性。

Conclusion: 基于粒子滤波的三维目标定位方法不仅能够解决以往方法难以处理的定位问题，还能够与现有分割模型结合，实际应用于无人机林火监控等场景，具有广泛的实用价值和灵活性。

Abstract: 3D object localisation based on a sequence of camera measurements is
essential for safety-critical surveillance tasks, such as drone-based wildfire
monitoring. Localisation of objects detected with a camera can typically be
solved with dense depth estimation or 3D scene reconstruction. However, in the
context of distant objects or tasks limited by the amount of available
computational resources, neither solution is feasible. In this paper, we show
that the task can be solved using particle filters for both single and multiple
target scenarios. The method was studied using a 3D simulation and a
drone-based image segmentation sequence with global navigation satellite system
(GNSS)-based camera pose estimates. The results showed that a particle filter
can be used to solve practical localisation tasks based on camera poses and
image segments in these situations where other solutions fail. The particle
filter is independent of the detection method, making it flexible for new
tasks. The study also demonstrates that drone-based wildfire monitoring can be
conducted using the proposed method paired with a pre-existing image
segmentation model.

</details>


### [44] [SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images](https://arxiv.org/abs/2509.20918)
*Qinfeng Zhu,Han Li,Liang He,Lei Fan*

Main category: cs.CV

TL;DR: 提出了一种新框架 SwinMamba，结合局部与全局特征感知，解决遥感图像语义分割中局部信息缺失的问题，与现有方法相比在多个数据集上取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分割因为高分辨率、复杂结构和多尺度目标，难以兼顾效率与精确性。新出现的Vision Mamba虽然高效，但在捕捉本地纹理和边缘细节时存在不足。

Method: 借鉴Swin Transformer思想，将Mamba结构应用于重叠滑动窗口，实现前两阶段局部扫描、后两阶段全局扫描，并通过窗格移位增进区域之间的信息交流，从而融合细粒度与全局特征。

Result: 在LoveDA和ISPRS Potsdam两个遥感分割数据集上，SwinMamba的实验结果优于当前最先进方法。

Conclusion: SwinMamba同时兼顾局部和全局特征感知，是遥感图像语义分割领域表现突出的新方法，具有实际应用潜力。

Abstract: Semantic segmentation of remote sensing imagery is a fundamental task in
computer vision, supporting a wide range of applications such as land use
classification, urban planning, and environmental monitoring. However, this
task is often challenged by the high spatial resolution, complex scene
structures, and diverse object scales present in remote sensing data. To
address these challenges, various deep learning architectures have been
proposed, including convolutional neural networks, Vision Transformers, and the
recently introduced Vision Mamba. Vision Mamba features a global receptive
field and low computational complexity, demonstrating both efficiency and
effectiveness in image segmentation. However, its reliance on global scanning
tends to overlook critical local features, such as textures and edges, which
are essential for achieving accurate segmentation in remote sensing contexts.
To tackle this limitation, we propose SwinMamba, a novel framework inspired by
the Swin Transformer. SwinMamba integrates localized Mamba-style scanning
within shifted windows with a global receptive field, to enhance the model's
perception of both local and global features. Specifically, the first two
stages of SwinMamba perform local scanning to capture fine-grained details,
while its subsequent two stages leverage global scanning to fuse broader
contextual information. In our model, the use of overlapping shifted windows
enhances inter-region information exchange, facilitating more robust feature
integration across the entire image. Extensive experiments on the LoveDA and
ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art
methods, underscoring its effectiveness and potential as a superior solution
for semantic segmentation of remotely sensed imagery.

</details>


### [45] [Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework](https://arxiv.org/abs/2509.20923)
*Wenhao Tang,Heng Fang,Ge Wu,Xiang Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文针对计算病理学中全景切片图像序列过长和异质性强的问题，提出了一种高效的pack-based多示例学习（MIL）框架，大幅提升了训练效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 全景切片图像（WSIs）在癌症诊断等任务中广泛应用，但其超长、变长的特性和有限的标注信息，导致数据异质性和冗余高，传统方法难以有效兼顾训练效率与优化。作者旨在在保障数据异质性的前提下，提高训练效率和模型性能。

Method: 提出了pack-based MIL框架，将多个变长特征序列打包为定长序列实现批量训练，保持异质性的同时显著提升效率。引入残差分支，将采样丢弃的特征整合成“超切片”，并配套设计标签进行监督，缓解特征损失。引入注意力驱动的下采样器，分别压缩主分支和残差分支的特征，进一步去除冗余。

Result: 在PANDA(UNI)任务上，通过所提方法，模型准确率提升高达8%，同时训练时间缩短为原有的12%。

Conclusion: 本文方法系统性地缓解了CPath中的序列长度极端变化和数据冗余问题，显著提升了效率和性能，为该领域在基础模型时代的进一步发展提供了有力工具。

Abstract: Computational pathology (CPath) digitizes pathology slides into whole slide
images (WSIs), enabling analysis for critical healthcare tasks such as cancer
diagnosis and prognosis. However, WSIs possess extremely long sequence lengths
(up to 200K), significant length variations (from 200 to 200K), and limited
supervision. These extreme variations in sequence length lead to high data
heterogeneity and redundancy. Conventional methods often compromise on training
efficiency and optimization to preserve such heterogeneity under limited
supervision. To comprehensively address these challenges, we propose a
pack-based MIL framework. It packs multiple sampled, variable-length feature
sequences into fixed-length ones, enabling batched training while preserving
data heterogeneity. Moreover, we introduce a residual branch that composes
discarded features from multiple slides into a hyperslide which is trained with
tailored labels. It offers multi-slide supervision while mitigating feature
loss from sampling. Meanwhile, an attention-driven downsampler is introduced to
compress features in both branches to reduce redundancy. By alleviating these
challenges, our approach achieves an accuracy improvement of up to 8% while
using only 12% of the training time in the PANDA(UNI). Extensive experiments
demonstrate that focusing data challenges in CPath holds significant potential
in the era of foundation models. The code is
https://github.com/FangHeng/PackMIL

</details>


### [46] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一种新的人体动作生成模型，通过在去噪过程中显式引入环境参数（如重力、风），提升了物理合理性与生成效率，同时支持不同环境系数的精细控制，并具有很好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有人体动作生成模型为了保证物理合理性，通常采用仿真器投影层，但由于仿真器需顺序执行，导致推理计算开销大、不可并行。该研究关注于如何在保证物理合理性的同时，提高动作生成的效率和灵活性。

Method: 作者提出SimDiff（一种受物理模拟约束的扩散模型），将环境参数直接作为条件嵌入到扩散去噪过程中，无需在推理阶段多次调用仿真器。模型从理论上将仿真器约束视为对扩散过程的引导（类似分类引导），从而高效实现条件控制和物理一致性。

Result: SimDiff在无需反复调用仿真器的前提下，能高效生成物理合理的人体动作，并且实现了对物理环境参数（如重力、风力等）的精细控制。此外，其泛化能力强，能适应未见过的环境参数组合。

Conclusion: 将环境参数直接引入到去噪过程，实现了高效且物理可信的人体动作生成，避免了传统方法的高昂仿真计算成本，同时大大提升了物理控制与泛化能力。

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [47] [Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models](https://arxiv.org/abs/2509.20939)
*Bum Jun Kim,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: 本文系统分析了视觉模型对高斯噪声的鲁棒性，揭示了架构设计选择在模型鲁棒性中的决定作用，并提出了可操作的设计准则。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉模型鲁棒性经常被评估，但对其依赖于具体架构设计的原因很少有深入剖析。作者希望找出哪些架构元素能提升模型对高斯噪声的鲁棒性，进而为实际模型设计提出指导。

Method: 作者对1174个预训练视觉模型进行了广泛实验，分析了噪声鲁棒性与架构参数之间的关系。进一步，建立理论分析，解释不同设计（如主干卷积核大小、输入分辨率、池化方式、模型类型等）影响鲁棒性的机制。

Result: 实验证明，四类设计（大主干卷积核、小输入分辨率、平均池化、监督式ViT而非CLIP ViT）显著提高鲁棒性，带来最高21.6个百分点精度提升。理论推导解释了卷积核、下采样与池化对噪声的抑制作用，并揭示CLIP ViT因归一化标准差较小而较为脆弱。

Conclusion: 本文将视觉模型鲁棒性拆解为易解释的模块，建立了理论解释数据规律，并给出了实用的设计规范，为抗噪声视觉模型的开发提供指导。

Abstract: While the robustness of vision models is often measured, their dependence on
specific architectural design choices is rarely dissected. We investigate why
certain vision architectures are inherently more robust to additive Gaussian
noise and convert these empirical insights into simple, actionable design
rules. Specifically, we performed extensive evaluations on 1,174 pretrained
vision models, empirically identifying four consistent design patterns for
improved robustness against Gaussian noise: larger stem kernels, smaller input
resolutions, average pooling, and supervised vision transformers (ViTs) rather
than CLIP ViTs, which yield up to 506 rank improvements and 21.6\%p accuracy
gains. We then develop a theoretical analysis that explains these findings,
converting observed correlations into causal mechanisms. First, we prove that
low-pass stem kernels attenuate noise with a gain that decreases quadratically
with kernel size and that anti-aliased downsampling reduces noise energy
roughly in proportion to the square of the downsampling factor. Second, we
demonstrate that average pooling is unbiased and suppresses noise in proportion
to the pooling window area, whereas max pooling incurs a positive bias that
grows slowly with window size and yields a relatively higher mean-squared error
and greater worst-case sensitivity. Third, we reveal and explain the
vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller
normalization standard deviations used in CLIP preprocessing amplify worst-case
sensitivity by up to 1.91 times relative to the Inception-style preprocessing
common in supervised ViTs. Our results collectively disentangle robustness into
interpretable modules, provide a theory that explains the observed trends, and
build practical, plug-and-play guidelines for designing vision models more
robust against Gaussian noise.

</details>


### [48] [Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery](https://arxiv.org/abs/2509.20941)
*Angelo Henriques,Korab Hoxha,Daniel Zapp,Peter C. Issa,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: 本文系统性梳理了场景图（SG）在外科手术领域的研究进展，总结发现该领域发展迅速，已成为提升手术安全与效率的关键技术。


<details>
  <summary>Details</summary>
Motivation: 随着手术环境日益复杂和动态化，对手术场景的智能理解和自动化需求上升。场景图为手术场景提供结构化的语义和关系表达，其在手术安全、效率与自动化中的潜力推动了本研究的系统性梳理。

Method: 采用PRISMA-ScR方法进行文献回顾，系统绘制了场景图在外科手术中的应用现状、技术进展与未来趋势。通过比较不同SG应用（内部视角2D视频vs.外部视角4D建模）及其依赖数据类型，分析当前技术差距与转化难点。同时评估了从基础图神经网络到专门外科场景基础大模型的发展路径及其与通用大模型的对比。

Result: 手术场景图研究快速增长，内部视角任务多用真实2D视频，外部4D建模则依赖仿真数据，两者之间存在明显的数据鸿沟。方法上已从早期基础模型进化到专用于手术的高阶基础模型，这些模型在手术场景下大幅优于一般视觉-语言大模型。SG广泛应用于流程识别、安全监测和可控模拟等，尽管在数据标注与实时实现上仍有挑战，但已有新技术着手解决。

Conclusion: 场景图技术已成为联接手术语义与智能系统的桥梁，将推动手术安全、效率和培训等多方面变革。持续的数据和实时化技术进步将进一步促进其应用落地。

Abstract: Scene graphs (SGs) provide structured relational representations crucial for
decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping
review systematically maps the evolving landscape of SG research in surgery,
charting its applications, methodological advancements, and future directions.
Our analysis reveals rapid growth, yet uncovers a critical 'data divide':
internal-view research (e.g., triplet recognition) almost exclusively uses
real-world 2D video, while external-view 4D modeling relies heavily on
simulated data, exposing a key translational research gap. Methodologically,
the field has advanced from foundational graph neural networks to specialized
foundation models that now significantly outperform generalist large
vision-language models in surgical contexts. This progress has established SGs
as a cornerstone technology for both analysis, such as workflow recognition and
automated safety monitoring, and generative tasks like controllable surgical
simulation. Although challenges in data annotation and real-time implementation
persist, they are actively being addressed through emerging techniques.
Surgical SGs are maturing into an essential semantic bridge, enabling a new
generation of intelligent systems to improve surgical safety, efficiency, and
training.

</details>


### [49] [A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning](https://arxiv.org/abs/2509.20946)
*Dongqi Zheng,Wenjin Fu,Guangzong Chen*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于视觉的自动检测和分类激光功率计传感器涂层缺陷的系统，通过无监督异常检测方法，能高效识别热损伤和划痕等缺陷，适用于医学和工业领域。该系统在无需大量缺陷标注数据的前提下，实现了高准确率的自动质量控制。


<details>
  <summary>Details</summary>
Motivation: 激光功率计传感器涂层的缺陷（如热损伤和划痕）会严重影响激光能量测量的精度，在医学和工业应用中影响尤为显著。目前，依赖人工或传统检测方法效率低且易漏检。因此，开发高效、自动化的检测系统是一项迫切需求。

Method: 采用无监督异常检测框架，仅使用‘良品’图像进行训练，以学习涂层的正常分布模式。系统包含三个核心模块：1）利用Laplacian边缘检测及K-means聚类进行预处理和分割感兴趣区域；2）借助StyleGAN2生成增强合成数据；3）基于UFlow的神经网络架构用于多尺度特征提取和异常图生成。

Result: 在366张真实传感器图像上的实验结果显示：对缺陷样本检测准确率为93.8%，良品样本检测准确率为89.3%，图像级AUROC为0.957，像素级AUROC为0.961。单张图像的处理时长为0.5秒，可实现高效的设备端部署。

Conclusion: 所提出的方法无需大量缺陷样本即可实现高效准确的传感器涂层缺陷检测，具备推广应用前景。该系统实现自动质量控制，能够显著节省成本，提升检测效率。

Abstract: We present an automated vision-based system for defect detection and
classification of laser power meter sensor coatings. Our approach addresses the
critical challenge of identifying coating defects such as thermal damage and
scratches that can compromise laser energy measurement accuracy in medical and
industrial applications. The system employs an unsupervised anomaly detection
framework that trains exclusively on ``good'' sensor images to learn normal
coating distribution patterns, enabling detection of both known and novel
defect types without requiring extensive labeled defect datasets. Our
methodology consists of three key components: (1) a robust preprocessing
pipeline using Laplacian edge detection and K-means clustering to segment the
area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a
UFlow-based neural network architecture for multi-scale feature extraction and
anomaly map generation. Experimental evaluation on 366 real sensor images
demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on
good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961.
The system provides potential annual cost savings through automated quality
control and processing times of 0.5 seconds per image in on-device
implementation.

</details>


### [50] [Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos](https://arxiv.org/abs/2509.20961)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Sriparna Saha,Alka Maurya*

Main category: cs.CV

TL;DR: 本文提出了一种名为FASTER的多模态金融播客内容摘要框架，能够高效提取、压缩和对齐视频/图像与文本信息，实现财务建议内容的精准、可理解摘要。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的普及，金融播客等长时多模态内容越来越多，但由于内容冗长且跨模态，信息提取与高效摘要成为巨大挑战。因此需要新的技术使金融建议内容更加易于访问与理解。

Method: 构建了FASTER框架，结合BLIP进行视觉语义理解、OCR识别文本信息、Whisper转录音频内容并做说话人分离，采用改进的DPO损失函数以增强事实一致性和人类对齐，利用排序检索方法对关键帧与文本内容进行对齐。此外构建了Fin-APT多模态金融播客数据集。

Result: FASTER在多个跨领域实验中表现优良，相较于传统大型语言模型和视觉-语言模型，具备更强的鲁棒性与泛化能力；实验证实该方法能更好地对长时多模态金融内容进行精准摘要。

Conclusion: FASTER为多模态金融内容摘要设立了新标准，提高了相关内容的可访问性及可操作性，为后续研究和应用开辟新方向。数据集和代码已开放，为社区进一步研究提供支持。

Abstract: The dynamic propagation of social media has broadened the reach of financial
advisory content through podcast videos, yet extracting insights from lengthy,
multimodal segments (30-40 minutes) remains challenging. We introduce FASTER
(Financial Advisory Summariser with Textual Embedded Relevant images), a
modular framework that tackles three key challenges: (1) extracting
modality-specific features, (2) producing optimized, concise summaries, and (3)
aligning visual keyframes with associated textual points. FASTER employs BLIP
for semantic visual descriptions, OCR for textual patterns, and Whisper-based
transcription with Speaker diarization as BOS features. A modified Direct
Preference Optimization (DPO)-based loss function, equipped with BOS-specific
fact-checking, ensures precision, relevance, and factual consistency against
the human-aligned summary. A ranker-based retrieval mechanism further aligns
keyframes with summarized content, enhancing interpretability and cross-modal
coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a
dataset comprising 470 publicly accessible financial advisory pep-talk videos
for robust multimodal research. Comprehensive cross-domain experiments confirm
FASTER's strong performance, robustness, and generalizability when compared to
Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing
a new standard for multimodal summarization, FASTER makes financial advisory
content more accessible and actionable, thereby opening new avenues for
research. The dataset and code are available at:
https://github.com/sarmistha-D/FASTER

</details>


### [51] [An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering](https://arxiv.org/abs/2509.20976)
*Yue Duan,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的适配器ASD，使自监督学习(SSL)方法能在无需预训练或辅助任务的前提下，直接应用于深度图像聚类任务。


<details>
  <summary>Details</summary>
Motivation: 此前方法将自监督学习(SSL)技术融入深度聚类框架，但都需要预训练、额外聚类学习或已训练模型作为前提，限制了SSL在图像聚类中的即插即用能力。作者希望解决冷启动情况下SSL用于聚类的难题。

Method: ASD通过以下步骤工作：1) 从所有未标记数据中随机采样得到伪标签样本，2) 利用实例级分类器学习这些伪标签，与真实语义对齐，3) 跟踪未标记样本预测的类别转移，挖掘实例类别间的高层相似性，用以赋予聚类级别标签，4) 用这些带聚类标签的伪标签样本唤醒SSL模型对图像聚类进行训练。

Result: ASD在多个基准图像聚类任务上优于最新的深度聚类方法，聚类准确率接近直接用真实标签训练的SSL方法（如CIFAR-10仅差1.33%）。此外，ASD还能进一步提升现有SSL嵌入式深度聚类方法的性能。

Conclusion: ASD打破了以往必须预训练或辅助聚类步骤的限制，使SSL在深度图像聚类时更灵活、易用且高效。该方法既可独立作为聚类方案，也能增强现有相关方法的表现。

Abstract: Recently, some works integrate SSL techniques into deep clustering frameworks
to enhance image clustering performance. However, they all need pretraining,
clustering learning, or a trained clustering model as prerequisites, limiting
the flexible and out-of-box application of SSL learners in the image clustering
task. This work introduces ASD, an adaptor that enables the cold-start of SSL
learners for deep image clustering without any prerequisites. Specifically, we
first randomly sample pseudo-labeled data from all unlabeled data, and set an
instance-level classifier to learn them with semantically aligned
instance-level labels. With the ability of instance-level classification, we
track the class transitions of predictions on unlabeled data to extract
high-level similarities of instance-level classes, which can be utilized to
assign cluster-level labels to pseudo-labeled data. Finally, we use the
pseudo-labeled data with assigned cluster-level labels to trigger a general SSL
learner trained on the unlabeled data for image clustering. We show the
superior performance of ASD across various benchmarks against the latest deep
image clustering approaches and very slight accuracy gaps compared to SSL
methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can
also further boost the performance of existing SSL-embedded deep image
clustering methods.

</details>


### [52] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: 提出了一种新的知识蒸馏框架SiNGER，能有效抑制Vision Transformer特有的高范数伪影并保留有效信息，提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（Vision Transformer）在作为教师模型进行知识蒸馏时，其高范数伪影会主导损失目标，导致学生模型过拟合于伪影，抑制了有效特征的传递。此前方法难以兼顾伪影抑制和有效信息保持。

Method: 提出Singular Nullspace-Guided Energy Reallocation（SiNGER）框架，利用Nullspace指引的扰动对教师特征进行精细化处理，结合LoRA适配器高效实现，以保留信息同时抑制高范数伪影，然后将精细化特征进行知识蒸馏。

Result: 在多个下游任务上验证了SiNGER的有效性，学生模型取得了优于现有方法的表现，特征表示也更清晰且更易于解释。

Conclusion: SiNGER有效解决了Vision Transformer蒸馏中的伪影与信息传递难以兼顾的问题，推动了大模型知识迁移的实际应用和模型可解释性的提升。

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation
models, but they are known to produce high-norm artifacts that degrade
representation quality. When knowledge distillation transfers these features to
students, high-norm artifacts dominate the objective, so students overfit to
artifacts and underweight informative signals, diminishing the gains from
larger models. Prior work attempted to remove artifacts but encountered an
inherent trade-off between artifact suppression and preserving informative
signals from teachers. To address this, we introduce Singular Nullspace-Guided
Energy Reallocation (SiNGER), a novel distillation framework that suppresses
artifacts while preserving informative signals. The key idea is principled
teacher feature refinement: during refinement, we leverage the nullspace-guided
perturbation to preserve information while suppressing artifacts. Then, the
refined teacher's features are distilled to a student. We implement this
perturbation efficiently with a LoRA-based adapter that requires minimal
structural modification. Extensive experiments show that \oursname consistently
improves student models, achieving state-of-the-art performance in multiple
downstream tasks and producing clearer and more interpretable representations.

</details>


### [53] [Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors](https://arxiv.org/abs/2509.20991)
*Jan Kněžík,Jonáš Herec,Rado Pitoňák*

Main category: cs.CV

TL;DR: 该论文提出了Fast-SEnSeI，一种轻量级、传感器无关的编码模块，实现了可在多光谱传感器和不同波段组合下的灵活云分割，支持在嵌入式硬件（CPU-FPGA）上高效运行并取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前云分割模型大多与特定传感器紧密耦合，且依赖于地面处理，难以实现多光谱、多波段配置下的灵活机载分割。作者旨在提出一种方法，解决传感器依赖性，实现高效机载处理。

Method: 作者基于SEnSeI-v2，研制了Fast-SEnSeI模块。该模块采用改进的光谱描述符、轻量级架构和健壮的补位波段处理技术，可接收任意组合的光谱波段及其波长输入，输出固定尺寸的特征图，送入基于修改版U-Net的紧凑量化分割模型。模块在嵌入式CPU上用Apache TVM高效运行，分割模型在FPGA上部署，构成CPU-FPGA混合流水线，适用于天基硬件。

Result: 在Sentinel-2 和 Landsat 8 数据集上评估表明，Fast-SEnSeI 对多样输入配置均能实现准确云分割，且能在嵌入式硬件上高效运行。

Conclusion: Fast-SEnSeI实现了高效、普适、空间上可用的机载云分割，提升了遥感任务的处理灵活性和实时能力，具有广泛工程应用前景。

Abstract: Cloud segmentation is a critical preprocessing step for many Earth
observation tasks, yet most models are tightly coupled to specific sensor
configurations and rely on ground-based processing. In this work, we propose
Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables
flexible, on-board cloud segmentation across multispectral sensors with varying
band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an
improved spectral descriptor, lightweight architecture, and robust padding-band
handling. It accepts arbitrary combinations of spectral bands and their
wavelengths, producing fixed-size feature maps that feed into a compact,
quantized segmentation model based on a modified U-Net. The module runs
efficiently on embedded CPUs using Apache TVM, while the segmentation model is
deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for
space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets
demonstrate accurate segmentation across diverse input configurations.

</details>


### [54] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: 该论文提出了一种用于文本到图像生成模型的有害内容擦除方法，只需操作单个神经元即可精准地抑制特定有害概念的生成，并且对图片质量影响极小。


<details>
  <summary>Details</summary>
Motivation: 文本到图像的大模型虽然生成效果优秀，但容易生成有害内容。现有的擦除有害概念方法难以兼顾擦除精度和保持生成质量。因此，亟需一种更精确且损伤小的新方法。

Method: 作者提出了单神经元概念擦除（SNCE）方法。首先，用稀疏自编码器（SAE）将文本嵌入映射到稀疏、可解释的潜在空间，使每个神经元更好地对应单一语义概念。然后，设计了一种基于频率调制评分的新神经元识别方法，精确定位与有害概念相关的神经元。最后，通过抑制该神经元的激活，实现针对性的有害内容擦除。

Result: 在多个基准数据集上的实验表明，SNCE在目标概念擦除上效果优于现有方法，并且对非目标内容生成能力影响极小。此外，SNCE对抗攻击具有较强鲁棒性，明显领先于其他方法。

Conclusion: SNCE方法能够以精细且高效的方式去除文本到图像模型中的有害内容，几乎不影响其他正常生成内容，为安全可控的模型应用提供了有力工具。

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [55] [OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities](https://arxiv.org/abs/2509.21038)
*Andreas Gilson,Lukas Meyer,Oliver Scholz,Ute Schmid*

Main category: cs.CV

TL;DR: 本文提出了一种名为KDSS的点云子采样算法，用于无损分辨率地对植物器官进行3D点云分割，适用于各种传感器和植物种类。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云分割方案通常依赖特定传感器或植物种类，且为适应硬件和神经网络输入限制，大多需对点云进行大量预处理和降采样，导致分辨率损失。作者希望开发一种对传感器和种类无关、且能保留原始分辨率的简易采样方法。

Method: 提出了KDSS（KD-Subsampling）算法，对原始点云进行子采样，但不会像传统方法那样盲目降采样。该算法可以与主流点云分割模型结合，无需大量预处理即可直接处理全分辨率的点云数据。

Result: 将KDSS与现有分割网络结合，在光测法、激光三角测量、LiDAR等不同传感器采集的多种植物点云上均获得令人满意的分割效果。

Conclusion: KDSS为植物器官分割任务提供了一种轻量、高效、兼容性强且无需降分辨率的解决方案，适用于不同的植物种类和多种传感器。

Abstract: Accurate point cloud segmentation for plant organs is crucial for 3D plant
phenotyping. Existing solutions are designed problem-specific with a focus on
certain plant species or specified sensor-modalities for data acquisition.
Furthermore, it is common to use extensive pre-processing and down-sample the
plant point clouds to meet hardware or neural network input size requirements.
We propose a simple, yet effective algorithm KDSS for sub-sampling of
biological point clouds that is agnostic to sensor data and plant species. The
main benefit of this approach is that we do not need to down-sample our input
data and thus, enable segmentation of the full-resolution point cloud.
Combining KD-SS with current state-of-the-art segmentation models shows
satisfying results evaluated on different modalities such as photogrammetry,
laser triangulation and LiDAR for various plant species. We propose KD-SS as
lightweight resolution-retaining alternative to intensive pre-processing and
down-sampling methods for plant organ segmentation regardless of used species
and sensor modality.

</details>


### [56] [Background Prompt for Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.21055)
*Songyue Cai,Zongqian Wu,Yujie Mo,Liang Peng,Ping Hu,Xiaoshuang Shi,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的前景-背景分解框架Mambo，用于提升小样本分布外检测的鲁棒性，并在多个真实数据集上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的前景-背景分解方法在小样本分布外检测任务中，因过度依赖局部类别相似性和固定的背景提取策略，导致鲁棒性不足。

Method: 提出学习背景提示（background prompt）获得局部背景相似性，再结合局部类别相似性进行优化，以改进背景提取方法。引入patch自校准调优机制，可以根据样本多样性自适应选取背景patch，克服以往方法中背景选择固定的问题。

Result: 在真实数据集上的大量实验表明，Mambo在分布外及近分布外检测设置下，性能优于现有先进方法（SOTA）。

Conclusion: Mambo框架能有效提升小样本分布外检测任务的表现，降低对局部类别相似性的依赖，为背景提取策略提供了更灵活且鲁棒的解决方案。

Abstract: Existing foreground-background (FG-BG) decomposition methods for the few-shot
out-of-distribution (FS-OOD) detection often suffer from low robustness due to
over-reliance on the local class similarity and a fixed background patch
extraction strategy. To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we
propose to first learn a background prompt to obtain the local background
similarity containing both the background and image semantic information, and
then refine the local background similarity using the local class similarity.
As a result, we use both the refined local background similarity and the local
class similarity to conduct background extraction, reducing the dependence of
the local class similarity in previous methods. Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.
Extensive experiments on real-world datasets demonstrate that our proposed
Mambo achieves the best performance, compared to SOTA methods in terms of OOD
detection and near OOD detection setting. The source code will be released at
https://github.com/YuzunoKawori/Mambo.

</details>


### [57] [Stratify or Die: Rethinking Data Splits in Image Segmentation](https://arxiv.org/abs/2509.21056)
*Naga Venkata Sai Jitin Jami,Thomas Altstidl,Jonas Mueller,Jindong Li,Dario Zanca,Bjoern Eskofier,Heike Leutheuser*

Main category: cs.CV

TL;DR: 本文针对图像分割任务中常用的随机数据集划分方法容易导致评估偏差和模型泛化能力下降的问题，提出了两种新的、基于标签分布优化的数据集分割方法（IPS和WDES），显著提升了分割任务评估的代表性和公平性。


<details>
  <summary>Details</summary>
Motivation: 针对图像分割的数据集通常存在类别不平衡和标签多样性的问题，随机划分容易造成测试集不具代表性，从而影响模型评估的客观性与实用性。尽管分类任务中已采用分层采样方法优化，但该思想难以直接迁移到分割任务。作者希望借助新的分层方法提升分割数据集划分的合理性和评估的准确性。

Method: 作者提出了两种新方法：（1）Iterative Pixel Stratification（IPS），针对分割任务的标签特征进行标签感知、高效迭代分层采样；（2）Wasserstein-Driven Evolutionary Stratification（WDES），设计了一种最小化标签分布之间Wasserstein距离的遗传算法，理论上证明在足够迭代下可达全局最优。此外，提出了新颖的统计异质性度量指标，对比评估不同划分方法。

Result: 实验表明WDES方法能在多种分割任务（如街景、医学图像、卫星影像）上得到更具代表性的训练/测试集划分，减少模型评估方差，提升泛化性能，尤其在小样本、不平衡和多样性低的数据集上表现突出，优于传统的随机划分方法。

Conclusion: 新提出的IPS和WDES方法能显著优化分割任务中的数据集划分，使测试集具更好代表性，评估结果更可靠，尤其WDES在难分数据集上的优势明显。建议今后分割任务评估使用该方法取代传统随机划分。

Abstract: Random splitting of datasets in image segmentation often leads to
unrepresentative test sets, resulting in biased evaluations and poor model
generalization. While stratified sampling has proven effective for addressing
label distribution imbalance in classification tasks, extending these ideas to
segmentation remains challenging due to the multi-label structure and class
imbalance typically present in such data. Building on existing stratification
concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward,
label-aware sampling method tailored for segmentation tasks. Additionally, we
present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic
algorithm designed to minimize the Wasserstein distance, thereby optimizing the
similarity of label distributions across dataset splits. We prove that WDES is
globally optimal given enough generations. Using newly proposed statistical
heterogeneity metrics, we evaluate both methods against random sampling and
find that WDES consistently produces more representative splits. Applying WDES
across diverse segmentation tasks, including street scenes, medical imaging,
and satellite imagery, leads to lower performance variance and improved model
evaluation. Our results also highlight the particular value of WDES in handling
small, imbalanced, and low-diversity datasets, where conventional splitting
strategies are most prone to bias.

</details>


### [58] [EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task](https://arxiv.org/abs/2509.21061)
*Riccardo La Grassa,Ignazio Gallo,Nicola Landro*

Main category: cs.CV

TL;DR: 本文提出了一种新型的细粒度分类模型EnGraf-Net，该模型通过利用层级语义结构（分类树）进行监督，实现无需部件裁剪和人工标注的高效识别，并在多个公开数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度分类方法通常依赖部件标注（如框选、部位标记或属性），或复杂的自动注意力机制来提升识别效果，但这些方法对局部特征的表征不完整，难以从整体和语义层面进行有效识别。作者认为，人类识别时会利用语义关联，因此希望通过引入语义层级关系完善细粒度分类。

Method: 提出EnGraf-Net，将语义关联结构（如物种或类别的层级分类树）作为网络训练的监督信号，融入到端到端的深度神经网络中。模型无需裁剪或进行手工部件标注，通过学习类别间的语义层次关联增强判别能力。

Result: 在CIFAR-100、CUB-200-2011和FGVC-Aircraft三个公开细粒度分类数据集上进行大量实验，EnGraf-Net性能优于多数已有方法，并在无须部件裁剪或人工标注的前提下达到或超过最新的SOTA水平。

Conclusion: EnGraf-Net证明了结合语义层级结构可以显著提升细粒度分类的表现，减少对手工标注和繁琐预处理的依赖，为细粒度视觉任务带来更简单高效的新解决方案。

Abstract: Fine-grained classification models are designed to focus on the relevant
details necessary to distinguish highly similar classes, particularly when
intra-class variance is high and inter-class variance is low. Most existing
models rely on part annotations such as bounding boxes, part locations, or
textual attributes to enhance classification performance, while others employ
sophisticated techniques to automatically extract attention maps. We posit that
part-based approaches, including automatic cropping methods, suffer from an
incomplete representation of local features, which are fundamental for
distinguishing similar objects. While fine-grained classification aims to
recognize the leaves of a hierarchical structure, humans recognize objects by
also forming semantic associations. In this paper, we leverage semantic
associations structured as a hierarchy (taxonomy) as supervised signals within
an end-to-end deep neural network model, termed EnGraf-Net. Extensive
experiments on three well-known datasets CIFAR-100, CUB-200-2011, and
FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing
fine-grained models, showing competitive performance with the most recent
state-of-the-art approaches, without requiring cropping techniques or manual
annotations.

</details>


### [59] [Vision Transformers: the threat of realistic adversarial patches](https://arxiv.org/abs/2509.21084)
*Kasper Cools,Clara Maathuis,Alexander M. van Oers,Claudia S. Hübner,Nikos Deligiannis,Marijke Vandewal,Geert De Cubber*

Main category: cs.CV

TL;DR: 该论文研究了视觉Transformer（ViT）模型在对抗补丁攻击下的安全性，发现ViT虽然比卷积神经网络（CNN）性能和鲁棒性更强，但仍然存在易受攻击的情况，且攻击效果因模型和预训练数据集而异。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统的普及，其安全问题日益突出。研究人员发现，尽管ViT模型在性能和抗干扰性上优于CNN，但其对抗性安全性尚未充分研究，特别是面对模拟现实场景的对抗补丁攻击。

Method: 作者设计了基于Creases Transformation（CT）的对抗补丁，该技术通过实现类似衣物自然褶皱的几何微扰，对现实中的二分类（人/非人）任务ViT模型进行攻击，并与从CNN转移过来的对抗攻击进行对比，测试其迁移性。

Result: 在四种微调的ViT模型上进行实验，结果显示不同模型对攻击的易感性差异大。攻击成功率在40.04%到99.97%之间，表明CNN中的对抗补丁攻击可以有效迁移到ViT模型。此外，模型的预训练数据集规模和方法极大影响了其抗攻击能力。

Conclusion: ViT模型虽然在整体性能和鲁棒性上优于CNN，但对抗补丁攻击依然有效，且不同ViT模型间的抗攻击能力差异显著。对抗补丁具备跨架构迁移性，提升模型安全性需综合考虑预训练方法和数据集。

Abstract: The increasing reliance on machine learning systems has made their security a
critical concern. Evasion attacks enable adversaries to manipulate the
decision-making processes of AI systems, potentially causing security breaches
or misclassification of targets. Vision Transformers (ViTs) have gained
significant traction in modern machine learning due to increased 1) performance
compared to Convolutional Neural Networks (CNNs) and 2) robustness against
adversarial perturbations. However, ViTs remain vulnerable to evasion attacks,
particularly to adversarial patches, unique patterns designed to manipulate AI
classification systems. These vulnerabilities are investigated by designing
realistic adversarial patches to cause misclassification in person vs.
non-person classification tasks using the Creases Transformation (CT)
technique, which adds subtle geometric distortions similar to those occurring
naturally when wearing clothing. This study investigates the transferability of
adversarial attack techniques used in CNNs when applied to ViT classification
models. Experimental evaluation across four fine-tuned ViT models on a binary
person classification task reveals significant vulnerability variations: attack
success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%
(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and
facebook/dinov3-vitb16 reaching 65.17%. These results confirm the
cross-architectural transferability of adversarial patches from CNNs to ViTs,
with pre-training dataset scale and methodology strongly influencing model
resilience to adversarial attacks.

</details>


### [60] [UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition](https://arxiv.org/abs/2509.21086)
*Guojun Lei,Rong Zhang,Chi Wang,Tianhang Liu,Hong Li,Zhiyuan Ma,Weiwei Xu*

Main category: cs.CV

TL;DR: 本文提出了UniTransfer新架构，通过空间解耦（前景、背景、运动流）以及扩散过程分步处理，实现了高质量、可控的视频概念迁移，并引入了自监督预训练和Chain-of-Prompt机制，用于精细化控制生成流程。提出了OpenAnimal数据集，并在多个实验上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频概念迁移应用如特效、替换、编辑等场景对精准和可控有较高要求，现有方法难以实现精细、分组件操作，且缺少大规模无标签预训练和分步生成机制，影响了视频内容生成的质量与灵活性。

Method: 1）空间解耦：将视频拆分为前景主体、背景和运动流；2）基于DiT的双流转单流架构，分别处理各组件并提供细粒度控制；3）自监督预训练，随机遮挡提升解耦特征学习；4）提出Chain-of-Prompt（CoP）机制，将扩散去噪过程分成三个不同细粒度阶段，利用LLM逐步引导生成；5）新建动物主题OpenAnimal视频数据集。

Result: 实验表明，UniTransfer在视觉质量和可编辑性上均优于现有技术，能适应多种参照图像和场景，实现更高水准的可控视频概念迁移。

Conclusion: UniTransfer凭借解耦设计、分步生成和自监督预训练，在视频概念迁移上实现了更高质量的控制和生成效果，为后续相关研究和基准测试提供了新方法和数据资源。

Abstract: We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer. Specifically, in terms of
spatial decomposition, we decouple videos into three key components: the
foreground subject, the background, and the motion flow. Building upon this
decomposed formulation, we further introduce a dual-to-single-stream DiT-based
architecture for supporting fine-grained control over different components in
the videos. We also introduce a self-supervised pretraining strategy based on
random masking to enhance the decomposed representation learning from
large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning
paradigm, we further revisit the denoising diffusion process and propose a
Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We
decompose the denoising process into three stages of different granularity and
leverage large language models (LLMs) for stage-specific instructions to guide
the generation progressively. We also curate an animal-centric video dataset
called OpenAnimal to facilitate the advancement and benchmarking of research in
video concept transfer. Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability. Web Page:
https://yu-shaonian.github.io/UniTransfer-Web/

</details>


### [61] [VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception](https://arxiv.org/abs/2509.21100)
*Ziang Yan,Xinhao Li,Yinan He,Zhengrong Yue,Xiangyu Zeng,Yali Wang,Yu Qiao,Limin Wang,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法Visual Test-Time Scaling（VTTS），通过推理阶段迭代感知，提升多模态大语言模型（MLLMs）的推理能力，并配套提出了VTTS-80K数据集。实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要依赖于静态的视觉解析和一次性感知，限制了模型在复杂推理、动态场景与视频理解等任务中的表现，因为无法像人类一样动态调整注意力以逐步深化理解。

Method: 提出了Visual Test-Time Scaling（VTTS）方法，在推理时通过Iterative Perception（ITP）机制，模拟人类的分层注意力，逐步聚焦于高置信度的时空区域，并结合文本预测结果动态调整关注点。ITP采用强化学习和时空监督信号指导优化。同时发布了支持迭代感知训练与评估的VTTS-80K数据集。

Result: 通过深入实验，VTTS在超过15项涵盖视频对话、视频推理和时空感知的基准测试上，相比Qwen2.5VL-3B和-7B等强基线，平均提升超过5%。新模型Videochat-R1.5展示了显著的性能改进和泛化能力。

Conclusion: VTTS方法能有效提升MLLMs在复杂多模态理解上的推理能力，尤其适合动态场景。其范式有望推动多模态大模型在人类级别感知和理解上的进一步发展。

Abstract: Inducing reasoning in multimodal large language models (MLLMs) is critical
for achieving human-level perception and understanding. Existing methods mainly
leverage LLM reasoning to analyze parsed visuals, often limited by static
perception stages. This paper introduces Visual Test-Time Scaling (VTTS), a
novel approach to enhance MLLMs' reasoning via iterative perception during
inference. VTTS mimics humans' hierarchical attention by progressively refining
focus on high-confidence spatio-temporal regions, guided by updated textual
predictions. Specifically, VTTS employs an Iterative Perception (ITP)
mechanism, incorporating reinforcement learning with spatio-temporal
supervision to optimize reasoning. To support this paradigm, we also present
VTTS-80K, a dataset tailored for iterative perception. These designs allows a
MLLM to enhance its performance by increasing its perceptual compute. Extensive
experiments validate VTTS's effectiveness and generalization across diverse
tasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved
remarkable improvements, with an average increase of over 5\%, compared to
robust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks
that encompass video conversation, video reasoning, and spatio-temporal
perception.

</details>


### [62] [Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models](https://arxiv.org/abs/2509.21102)
*Suaiba Amina Salahuddin,Teresa Dorszewski,Marit Almenning Martiniussen,Tone Hovda,Antonio Portaluri,Solveig Thrun,Michael Kampffmeyer,Elisabeth Wetzer,Kristoffer Wickstrøm,Robert Jenssen*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于概念的可解释性框架Mammo-CLIP Dissect，用于分析深度学习视觉模型在乳腺X光（乳腺钼靶）任务中学到的医学文本概念，而不仅仅是像素特征。实验展示其可以揭示模型对领域知识的真实学习能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像分析中越来越重要，但模型的可解释性直接关系到在临床的安全部署。目前的可解释性方法多聚焦于像素级别，可医生的推理过程更多依赖于可解释的医学概念。因此，亟需一个能将深度学习模型所学内容与人类易懂的文本概念进行对齐的分析框架。

Method: 提出了Mammo-CLIP Dissect框架，利用与乳腺X光结合的视觉-语言模型（Mammo-CLIP）作为'解剖器'，为深度学习模型（如CNN）中特定层的神经元赋予可解释的文本标签，并衡量其与专业知识的一致性。通过对比分析通用和领域专用数据训练的模型，以及微调前后模型对概念的学习与泛化能力。

Result: 领域专用（乳腺X光数据）训练的模型能够更好地学习和捕捉临床医生关注的重要医学概念，且其内部知识与放射科医生工作流程高度一致。对特定任务（如分类）微调能增强部分医学概念（如良性钙化灶）的捕捉，但可能导致其他特征（如密度相关特征）覆盖率下降，暴露了专化与泛化之间的权衡关系。

Conclusion: Mammo-CLIP Dissect能够系统、解释性地揭示卷积神经网络在乳腺X光领域的知识学习过程。通过该方法，可以比较不同训练和微调策略对模型学习医学概念的影响，对推动AI模型临床部署的可信度提升具有重要意义。

Abstract: Understanding what deep learning (DL) models learn is essential for the safe
deployment of artificial intelligence (AI) in clinical settings. While previous
work has focused on pixel-based explainability methods, less attention has been
paid to the textual concepts learned by these models, which may better reflect
the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first
concept-based explainability framework for systematically dissecting DL vision
models trained for mammography. Leveraging a mammography-specific
vision-language model (Mammo-CLIP) as a "dissector," our approach labels
neurons at specified layers with human-interpretable textual concepts and
quantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we
investigate three key questions: (1) how concept learning differs between DL
vision models trained on general image datasets versus mammography-specific
datasets; (2) how fine-tuning for downstream mammography tasks affects concept
specialisation; and (3) which mammography-relevant concepts remain
underrepresented. We show that models trained on mammography data capture more
clinically relevant concepts and align more closely with radiologists'
workflows than models not trained on mammography data. Fine-tuning for
task-specific classification enhances the capture of certain concept categories
(e.g., benign calcifications) but can reduce coverage of others (e.g.,
density-related features), indicating a trade-off between specialisation and
generalisation. Our findings show that Mammo-CLIP Dissect provides insights
into how convolutional neural networks (CNNs) capture mammography-specific
knowledge. By comparing models across training data and fine-tuning regimes, we
reveal how domain-specific training and task-specific adaptation shape concept
learning. Code and concept set are available:
https://github.com/Suaiba/Mammo-CLIP-Dissect.

</details>


### [63] [MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](https://arxiv.org/abs/2509.21113)
*Sicheng Tao,Jungang Li,Yibo Yan,Junyan Zhang,Yubo Gao,Hanqian Li,ShuHang Xun,Yuxuan Fan,Hong Chen,Jianxiang He,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大模型视频推理训练方法MOSS-ChatV，通过引入基于动态时间规整（DTW）的过程奖励，有效提升了模型对复杂视频动态场景的理解一致性和推理可解释性。实验显示该方法在多个视频推理基准上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在进行视频推理时，推理过程往往无法与视频动态严密对齐，导致中间推理步骤与视频实际内容脱节，即便最终结果正确，过程的解释性和鲁棒性仍受影响。作者试图解决推理过程与视频动态的不一致问题，提升模型对于视频时间动态的理解力。

Method: 作者提出MOSS-ChatV框架，采用基于动态时间规整（DTW）的规则化过程奖励，将模型推理过程与带时间序列的参考推理进行对齐，无需额外奖励模型即可进行高效的过程监督。同时，还构建了新的视频推理基准MOSS-Video，用于训练和评测模型的一致性推理能力。

Result: MOSS-ChatV在自建基准MOSS-Video测试集上取得了87.2%的表现，并提升了MVBench和MMVU等通用视频推理基准的性能。该框架在不同大模型架构（如Qwen2.5-VL、Phi-2）上均有效。此外，使用GPT-4o进行主观评估，证明获得了更稳定、一致的推理轨迹。

Conclusion: MOSS-ChatV能显著提高MLLMs的视频推理一致性与可解释性，对不同模型架构具有广泛适用性，为多模态大模型理解复杂视频场景打开了新思路。

Abstract: Video reasoning has emerged as a critical capability for multimodal large
language models (MLLMs), requiring models to move beyond static perception
toward coherent understanding of temporal dynamics in complex scenes. Yet
existing MLLMs often exhibit process inconsistency, where intermediate
reasoning drifts from video dynamics even when the final answer is correct,
undermining interpretability and robustness. To address this issue, we
introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time
Warping (DTW)-based process reward. This rule-based reward aligns reasoning
traces with temporally grounded references, enabling efficient process
supervision without auxiliary reward models. We further identify dynamic state
prediction as a key measure of video reasoning and construct MOSS-Video, a
benchmark with annotated reasoning traces, where the training split is used to
fine-tune MOSS-ChatV and the held-out split is reserved for evaluation.
MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance on
general video benchmarks such as MVBench and MMVU. The framework consistently
yields gains across different architectures, including Qwen2.5-VL and Phi-2,
confirming its broad applicability. Evaluations with GPT-4o-as-judge further
show that MOSS-ChatV produces more consistent and stable reasoning traces.

</details>


### [64] [MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation](https://arxiv.org/abs/2509.21119)
*Guojun Lei,Chi Wang,Yikai Wang,Hong Li,Ying Song,Weiwei Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过将相机和物体的运动统一为像素运动，提升了基于相机轨迹的视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将相机和物体运动分开建模，容易混淆两者的相对关系，导致生成视频的一致性与泛化能力不足，特别是在同时存在相机和物体运动时。

Method: 作者提出将相机和物体的运动统一转化为相关像素的运动。利用稳定扩散网络学习参考运动图，并结合提取的语义物体先验，将这些信息输入图像到视频的生成网络，生成能够精准跟随指定相机轨迹且物体运动一致性好的视频。

Result: 通过大量实验证明，该模型比现有最先进方法有显著提升。

Conclusion: 新方法有效解决了相机与物体运动耦合下的视频生成一致性难题，显著提升了视频质量。

Abstract: Generating videos guided by camera trajectories poses significant challenges
in achieving consistency and generalizability, particularly when both camera
and object motions are present. Existing approaches often attempt to learn
these motions separately, which may lead to confusion regarding the relative
motion between the camera and the objects. To address this challenge, we
propose a novel approach that integrates both camera and object motions by
converting them into the motion of corresponding pixels. Utilizing a stable
diffusion network, we effectively learn reference motion maps in relation to
the specified camera trajectory. These maps, along with an extracted semantic
object prior, are then fed into an image-to-video network to generate the
desired video that can accurately follow the designated camera trajectory while
maintaining consistent object motions. Extensive experiments verify that our
model outperforms SOTA methods by a large margin.

</details>


### [65] [The Unwinnable Arms Race of AI Image Detection](https://arxiv.org/abs/2509.21135)
*Till Aczel,Lorenzo Vettor,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文探讨了在图像生成式AI与判别器之间的博弈中，判别器在何种条件下最难以分辨合成图像与真实图像。通过分析数据的维度和复杂性，发现数据过于简单或复杂时，都降低了判别难度，但在中等复杂度的数据集上，判别器辨识能力最强。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成式AI的进步，合成图像与真实图像之间界限日益模糊，加剧了生成模型和判别模型之间的“军备竞赛”。了解判别器在哪些条件下最为“失利”，对于发展更强大判别模型和改进生成模型都非常重要。

Method: 本文从理论和实验角度，分析了数据维度提升和数据内在结构（以Kolmogorov复杂度度量）对判别难度的影响。特别地，研究了简单数据集、复杂数据集和中等复杂度数据集对生成模型和判别器性能的影响。

Result: 结果表明，生成模型可几乎完美学习非常简单的数据集，使判别器难以区分；而在极其复杂的数据集中，数据的巨大多样性掩盖了生成缺陷，也使判别变难。只有在中等复杂度的数据集中，生成器难以完全捕捉真实分布，判别器对错误捕捉能力最强。

Conclusion: 判别器识别合成图像的能力依赖于数据集的复杂性。过于简单或复杂的数据集都不利于判别，中等复杂度的数据集为判别提供了最佳环境。

Abstract: The rapid progress of image generative AI has blurred the boundary between
synthetic and real images, fueling an arms race between generators and
discriminators. This paper investigates the conditions under which
discriminators are most disadvantaged in this competition. We analyze two key
factors: data dimensionality and data complexity. While increased
dimensionality often strengthens the discriminators ability to detect subtle
inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov
complexity as a measure of intrinsic dataset structure, we show that both very
simple and highly complex datasets reduce the detectability of synthetic
images; generators can learn simple datasets almost perfectly, whereas extreme
diversity masks imperfections. In contrast, intermediate-complexity datasets
create the most favorable conditions for detection, as generators fail to fully
capture the distribution and their errors remain visible.

</details>


### [66] [WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP](https://arxiv.org/abs/2509.21153)
*Moshe Kimhi,Erez Koifman,Ehud Rivlin,Eli Schwartz,Chaim Baskin*

Main category: cs.CV

TL;DR: WAVECLIP 是一种结合小波分解的新型CLIP模型，通过自适应多分辨率推理减少计算量并保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP模型对所有图片统一按高分辨率处理，计算量大且灵活性不足，难以在计算资源有限或需自适应推理场景下灵活调控。

Method: WAVECLIP用多层小波分解替代传统patch embedding，使模型能以由粗至细的方式处理图片，并支持多分辨率自适应推理。推理时模型先用低分辨率Token，必要时再细化，只计算新的信息。结合置信度门控机制实现自适应早停，达到计算与精度的权衡。此外，训练只需轻量级的蒸馏，教师模型为CLIP。

Result: WAVECLIP在零样本分类任务中验证，使用置信度门控即可自适应选择早停点，单一模型实现动态的算力-精度调节。模型在高效性和准确性上表现良好，相比原始CLIP有显著计算节省。

Conclusion: WAVECLIP能高效支撑多分辨率自适应推理，显著降低推理计算成本，且准确率与原有CLIP接近，是实际部署中灵活高效的替代方案。

Abstract: We introduce WAVECLIP, a single unified model for adaptive resolution
inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces
standard patch embeddings with a multi-level wavelet decomposition, enabling
the model to process images coarse to fine while naturally supporting multiple
resolutions within the same model. At inference time, the model begins with low
resolution tokens and refines only when needed, using key-value caching and
causal cross-level attention to reuse computation, effectively introducing to
the model only new information when needed. We evaluate WAVECLIP in zero-shot
classification, demonstrating that a simple confidence-based gating mechanism
enables adaptive early exits. This allows users to dynamically choose a
compute-accuracy trade-off using a single deployed model. Our approach requires
only lightweight distillation from a frozen CLIP teacher and achieves
competitive accuracy with significant computational savings.

</details>


### [67] [Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy](https://arxiv.org/abs/2509.21173)
*Aymen Bouguerra,Daniel Montoya,Alexandra Gomez-Villa,Fabio Arnez,Chokri Mraidha*

Main category: cs.CV

TL;DR: 本文系统评估了量化对CLIP视觉-语言模型在分布内准确率及多项鲁棒性和可靠性指标上的影响，发现量化不仅有助于部分模型校准，也对OOD检测等任务表现有积极影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视觉-语言模型在零样本泛化和分布外检测上的性能，但在实际部署中，如何在保证推理效率的同时，提升模型的可靠性和鲁棒性，尤其是量化对CLIP模型可靠性的具体影响，尚未被充分研究。

Method: 作者进行了大规模实验，对CLIP模型的不同量化操作（包括量化感知训练QAT方法）进行了系统测试和评估，涵盖准确率、校准、分布外检测等多种可靠性指标，并对比了不同预训练来源影响。

Result: 研究发现，对于通常低置信度的预训练模型，量化有助于提升模型校准表现；反之高置信度模型则可能因量化导致校准下降，但分布外检测能力仍可能提升。部分QAT方法能够在准确率、校准和OOD鲁棒性上同步取得提升，这打破了长久以来效率与性能硬性对立的认知。

Conclusion: 本文证明量化不仅提升高效性，还可改善可靠性与鲁棒性。合理设计与选择量化方案，可助力高效、可靠的VLM现实部署，推动量化在安全相关任务上发挥更大价值。

Abstract: The powerful zero-shot generalization capabilities of vision-language models
(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as
out-of-distribution (OOD) detection. However, additional aspects crucial for
the computationally efficient and reliable deployment of CLIP are still
overlooked. In particular, the impact of quantization on CLIP's performance
beyond accuracy remains underexplored. This work presents a large-scale
evaluation of quantization on CLIP models, assessing not only in-distribution
accuracy but a comprehensive suite of reliability metrics and revealing
counterintuitive results driven by pre-training source. We demonstrate that
quantization consistently improves calibration for typically underconfident
pre-trained models, while often degrading it for overconfident variants.
Intriguingly, this degradation in calibration does not preclude gains in other
reliability metrics; we find that OOD detection can still improve for these
same poorly calibrated models. Furthermore, we identify specific
quantization-aware training (QAT) methods that yield simultaneous gains in
zero-shot accuracy, calibration, and OOD robustness, challenging the view of a
strict efficiency-performance trade-off. These findings offer critical insights
for navigating the multi-objective problem of deploying efficient, reliable,
and robust VLMs by utilizing quantization beyond its conventional role.

</details>


### [68] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: 本论文提出了TABLET数据集，一个包含400万样本的大规模视觉表格理解（VTU）数据集，能够提升模型在真实场景表格上的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 以往VTU研究多基于视觉输入对表格内容进行理解，但现有数据集大多为合成表格，视觉复杂性和真实性不足，且只提供单一可视化和固定指令，缺乏灵活重构和丰富的训练基础。

Method: 作者构建了TABLET数据集，收集和整理200万个真实表格（保留88%原始可视化），覆盖20种任务，生成400万带图像-HTML对的训练样本，每个样本包含详细元数据与可追溯来源信息。

Result: 在TABLET上微调如Qwen2.5-VL-7B等视觉语言模型，结果显示模型在已见和未见VTU任务上表现均提升，且在真实世界表格上更具鲁棒性。

Conclusion: TABLET以其大规模、真实性和可追溯性，成为面向未来视觉表格理解模型训练和评测的重要基础，推动了领域方法的实用性和可扩展性。

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [69] [Learning Conformal Explainers for Image Classifiers](https://arxiv.org/abs/2509.21209)
*Amr Alkhatib,Stephanie Lowry*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于保形预测(conformal prediction)的特征归因方法，用于提升图像预测解释的稳健性和保真度。实验结果显示其在多数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像预测特征归因方法在稳健性和保真度方面存在不足，难以保证解释真实反映模型的推理过程。如何生成用户可控且具高保真度的解释，是该工作的核心动机。

Method: 作者提出了一种基于保形预测的新方法，允许用户直接控制生成解释的保真度。该方法通过识别一组足以保持模型预测的显著特征，实现对归因解释的稳健性和效率提升。设计了四种一致性函数，用以量化解释与模型预测的一致性，无需依赖校准用的真实解释。

Result: 在六个图像数据集上，使用五种解释器评估新方法。结果显示，FastSHAP（一种对比方法）在解释的保真度和信息效率（解释区域大小）方面均优于现有方法。同时，基于超像素的度量比像素级度量效果更佳。

Conclusion: 该方法显著提升了图像预测解释的稳健性和保真度，并提供了对解释保真度的直接控制方式。实践中推荐使用基于超像素的一致性度量。

Abstract: Feature attribution methods are widely used for explaining image-based
predictions, as they provide feature-level insights that can be intuitively
visualized. However, such explanations often vary in their robustness and may
fail to faithfully reflect the reasoning of the underlying black-box model. To
address these limitations, we propose a novel conformal prediction-based
approach that enables users to directly control the fidelity of the generated
explanations. The method identifies a subset of salient features that is
sufficient to preserve the model's prediction, regardless of the information
carried by the excluded features, and without demanding access to ground-truth
explanations for calibration. Four conformity functions are proposed to
quantify the extent to which explanations conform to the model's predictions.
The approach is empirically evaluated using five explainers across six image
datasets. The empirical results demonstrate that FastSHAP consistently
outperforms the competing methods in terms of both fidelity and informational
efficiency, the latter measured by the size of the explanation regions.
Furthermore, the results reveal that conformity measures based on super-pixels
are more effective than their pixel-wise counterparts.

</details>


### [70] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了Sigma，一个骨架数据驱动的手语理解统一预训练框架，通过融合视觉和文本信息，实现了更强的语义一致性和跨模态对齐，在多个手语任务上取得了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨架的手语理解方法，难以将低级运动捕捉与语言语义关联，细节与全局上下文权衡不足，以及跨模态的高质量表示难以构建。

Method: 提出了Sigma框架，包括：(1) 早期视觉-文本特征融合机制，强化语义信息交互；(2) 分层对齐学习策略，同时对不同粒度特征进行对齐，以兼顾局部细节与全局语义；(3) 综合对比学习、文本匹配和语言建模的统一预训练体系，提升语义一致性与泛化能力。

Result: Sigma在多个手语识别与翻译基准（覆盖多种手语、口语）上刷新了当前最好表现。

Conclusion: 通过高质量、语义丰富的预训练方式和骨架数据，Sigma显著提升了手语理解任务中的效果，证实了骨架数据作为独立解决方案的有效性。

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [71] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: Stable Diffusion等文本生成图像的扩散模型在处理复杂组合关系时表现不佳。文章提出CARINOX框架，结合了优化与探索多种方法，并通过合理结合多元奖励函数，提升文本-图像组合一致性。实验结果在多个评测基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型，对于描述复杂物体关系、属性或空间排列的文本提示时，难以生成与文本完全一致的图片。优化类与探索类方法各有局限，单一或随意组合奖励函数也难以全面刻画组合性，导致指导效果不佳。因此需要统一、高效的方法来提升模型的组合一致性。

Method: 提出CARINOX框架，系统地结合了基于初始噪声的优化与探索方法，并设计奖励函数的遴选流程，奖励的选取与人类评判高度相关。通过共用奖励指导优化和探索过程，平衡多种生成目标，提高组合对齐能力。

Result: 在T2I-CompBench++和HRS两个数据集上进行评测，CARINOX在平均对齐分数上分别提升16%和11%，在所有主要类别上超越当前最优的优化和探索方法，并能保持图像质量与多样性。

Conclusion: CARINOX方法能有效提升文本到图像扩散模型的组合对齐度，在复杂生成任务上超越了现有优化和探索方法，有实用价值并具备较强泛化能力。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [72] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文对文本-图像生成领域中常用的自动化评价指标进行了系统研究，发现并无单一指标能适用于所有组合场景，呼吁更谨慎地选择和应用评价指标。


<details>
  <summary>Details</summary>
Motivation: 随着文本-图像生成技术的快速发展，如何准确评估生成结果与输入描述在对象、属性及关系等方面的一致性成为难题。目前行业广泛依赖自动化指标，但这些指标大多因习惯或流行被采用，鲜有通过人工判断验证其有效性。鉴于后续研究和领域进展高度依赖这些指标，亟需厘清其与人类偏好的一致性。

Method: 作者对主流文本-图像生成评估指标进行大规模调研，比对各类常用指标（包括VQA指标、嵌入类指标、以及仅基于图像的指标）在多样组合场景下的表现，并与人工判断进行对照分析，不仅考察指标间的相关性，还探究其在不同任务下的适用性。

Result: 研究发现：没有单一指标能够在所有组合类型的评测任务中表现稳定，不同任务下指标性能波动较大。VQA类指标虽然流行，但并非在所有场景都优越；嵌入类指标在部分情形下效果更强；仅基于图像的指标对组合一致性意义不大，只适合感知质量评价。

Conclusion: 需全面、透明地选择和应用评测指标，不能盲目依赖单一指标。特别值得注意的是，这些评价方法不仅用于学术评价，也作为生成模型的奖励信号，对生成技术的发展影响深远。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [73] [SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology](https://arxiv.org/abs/2509.21239)
*Shakib Khan,Fariba Dambandkhameneh,Nazim Shaikh,Yao Nie,Raghavan Venugopal,Xiao Li*

Main category: cs.CV

TL;DR: 本文提出了一种结合Mamba结构和图神经网络（GNNs）的深度学习框架——SlideMamba，用于提高全视野病理切片图像（WSI）分析的表现。通过引入熵值加权的自适应融合策略，SlideMamba在基因融合及突变状态预测任务上优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前病理图像的计算分析需要同时捕捉局部空间关系和全局上下文信息。传统方法多难以兼顾两者，因此提出新架构以兼具捕捉细粒度局部交互和长程依赖的能力。

Method: SlideMamba框架将Mamba模块和图神经网络结合，分别用于捕捉全局和局部特征，并通过基于熵的加权自适应融合机制动态调节两者对下游任务的影响，从而根据具体背景自动调整模型关注点。

Result: SlideMamba在预测基因融合与突变任务中，PRAUC达到0.751，优于MIL、Trans-MIL、Mamba-only、GNN-only、GAT-Mamba等方法。同时在ROC AUC、灵敏度和特异性上也表现出色，显示出集成架构和自适应融合策略的优势。

Conclusion: 集成Mamba与GNN，以及提出的自适应熵加权融合策略，有效提升WSI图像下游任务表现，为空间分辨率相关的预测建模在病理图像分析领域展示了良好的应用前景。

Abstract: Advances in computational pathology increasingly rely on extracting
meaningful representations from Whole Slide Images (WSIs) to support various
clinical and biological tasks. In this study, we propose a generalizable deep
learning framework that integrates the Mamba architecture with Graph Neural
Networks (GNNs) for enhanced WSI analysis. Our method is designed to capture
both local spatial relationships and long-range contextual dependencies,
offering a flexible architecture for digital pathology analysis. Mamba modules
excels in capturing long-range global dependencies, while GNNs emphasize
fine-grained short-range spatial interactions. To effectively combine these
complementary signals, we introduce an adaptive fusion strategy that uses an
entropy-based confidence weighting mechanism. This approach dynamically
balances contributions from both branches by assigning higher weight to the
branch with more confident (lower-entropy) predictions, depending on the
contextual importance of local versus global information for different
downstream tasks. We demonstrate the utility of our approach on a
representative task: predicting gene fusion and mutation status from WSIs. Our
framework, SlideMamba, achieves an area under the precision recall curve
(PRAUC) of 0.751 \pm 0.05, outperforming MIL (0.491 \pm 0.042), Trans-MIL (0.39
\pm 0.017), Mamba-only (0.664 \pm 0.063), GNN-only (0.748 \pm 0.091), and a
prior similar work GAT-Mamba (0.703 \pm 0.075). SlideMamba also achieves
competitive results across ROC AUC (0.738 \pm 0.055), sensitivity (0.662 \pm
0.083), and specificity (0.725 \pm 0.094). These results highlight the strength
of the integrated architecture, enhanced by the proposed entropy-based adaptive
fusion strategy, and suggest promising potential for application of
spatially-resolved predictive modeling tasks in computational pathology.

</details>


### [74] [Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets](https://arxiv.org/abs/2509.21245)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jingwei Huang,Junlin Yu,Kunhong Li,Linus,Penghao Wang,Qingxiang Lin,Sicong Liu,Xianghui Yang,Yixuan Tang,Yunfei Zhao,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: Hunyuan3D-Omni是一种能接受多种先验条件（如图片、点云、体素、骨骼等）的统一3D资产生成框架，显著提高了三维模型生成的可控性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成模型受图像或文本控制为主，缺乏细粒度、跨模态控制，影响其实际应用和生成效果。因此，作者旨在提升3D资产生成的可控性和多样性。

Method: 提出Hunyuan3D-Omni统一框架，将图像、点云、体素、包围盒、骨骼姿态作为条件统一输入单一的跨模态网络，并采用逐步、基于难度的采样方案加强模型在复杂控制信号（如骨骼姿态）下的鲁棒性，提升对多模态信号的融合能力。

Result: 实验表明，模型支持更多种类精细控制信号，提升了3D生成的准确性、灵活性及多样控制能力，对实际生产工作流更为友好且更鲁棒。

Conclusion: Hunyuan3D-Omni实现了统一且强大的3D资产生成控制能力，并优于现有依赖单一模态（如图像或文本）的模型，促进了更广泛的3D生成应用。

Abstract: Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.

</details>


### [75] [Learning to Look: Cognitive Attention Alignment with Vision-Language Models](https://arxiv.org/abs/2509.21247)
*Ryan L. Yang,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过利用视觉-语言模型（VLM）和自然语言提示，自动生成语义注意力图，并引导CNN注意力，从而摆脱了手工注释的需求，提高了模型泛化能力和认知合理性。


<details>
  <summary>Details</summary>
Motivation: 传统CNN模型容易依赖表层相关性“投机取巧”，难以保证模型做出的预测有合理的因果依据。基于认知科学中注意力机制对人类感知稳健性的启发，现有概念监督和解释正则化方法虽可改善这点，但高度依赖人工注释，制约了实际应用。

Method: 作者提出利用视觉-语言模型和自然语言提示，自动生成用于指导CNN注意力的语义遮罩，加入辅助损失，使得CNN注意力与这些遮罩对齐，无需人工注释，提升模型注意的合理性。

Result: 实验证明，该方法在ColoredMNIST数据集上获得了SOTA性能，在DecoyMNIST上也与依赖大量注释的基线方法表现相当，并展现了更好的泛化能力和更符合人类直觉的模型注意力。

Conclusion: 本方法显著提升了CNN的泛化能力和注意力解释性，避免手工注释，易于扩展，有望推动CNN模型更加可靠和认知可解释的实际应用。

Abstract: Convolutional Neural Networks (CNNs) frequently "cheat" by exploiting
superficial correlations, raising concerns about whether they make predictions
for the right reasons. Inspired by cognitive science, which highlights the role
of attention in robust human perception, recent methods have sought to guide
model attention using concept-based supervision and explanation regularization.
However, these techniques depend on labor-intensive, expert-provided
annotations, limiting their scalability. We propose a scalable framework that
leverages vision-language models to automatically generate semantic attention
maps using natural language prompts. By introducing an auxiliary loss that
aligns CNN attention with these language-guided maps, our approach promotes
more reliable and cognitively plausible decision-making without manual
annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,
show that our method achieves state-of-the-art performance on ColorMNIST and
remains competitive with annotation-heavy baselines on DecoyMNIST,
demonstrating improved generalization, reduced shortcut reliance, and model
attention that better reflects human intuition.

</details>


### [76] [Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations](https://arxiv.org/abs/2509.21249)
*Zhijian Yang,Noel DSouza,Istvan Megyeri,Xiaojian Xu,Amin Honarmandi Shandiz,Farzin Haddadpour,Krisztian Koos,Laszlo Rusko,Emanuele Valeriano,Bharadwaj Swaninathan,Lei Wu,Parminder Bhatia,Taha Kass-Hout,Erhan Bas*

Main category: cs.CV

TL;DR: 本论文提出了Decipher-MR，一种特定于3D MRI的视觉-语言大模型，该模型具备强大的可扩展性和泛化能力，在多项MRI相关任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MRI影像复杂且异质性强，限制了自动化分析和可扩展的机器学习模型的泛化能力。当前大模型在MRI领域应用有限，主要受限于数据匮乏和解剖范围狭窄。

Method: 作者构建了一个包含22,000多个研究、200,000条MRI数据的大规模多样化数据集，结合自监督视觉学习与报告驱动文本监督，训练了Decipher-MR基础模型。模型采用模块化设计，冻结预训练编码器，仅需微调轻量级任务相关解码器即可低成本适应多任务。

Result: Decipher-MR在疾病分类、人口属性预测、解剖定位、跨模态检索等多项MRI任务基准上，均优于现有基础模型和专用方法，展示了一致的性能提升。

Conclusion: Decipher-MR为MRI领域的AI开发提供了可扩展、通用的基础模型，显著提升了科研与临床应用的效率和效果，是构建多任务MRI分析的重要基础。

Abstract: Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in
clinical diagnosis and research, yet its complexity and heterogeneity pose
challenges for automated analysis, particularly in scalable and generalizable
machine learning applications. While foundation models have revolutionized
natural language and vision tasks, their application to MRI remains limited due
to data scarcity and narrow anatomical focus. In this work, we present
Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a
large-scale dataset comprising 200,000 MRI series from over 22,000 studies
spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR
integrates self-supervised vision learning with report-guided text supervision
to build robust, generalizable representations, enabling effective adaptation
across broad applications. To enable robust and diverse clinical tasks with
minimal computational overhead, Decipher-MR supports a modular design that
enables tuning of lightweight, task-specific decoders attached to a frozen
pretrained encoder. Following this setting, we evaluate Decipher-MR across
diverse benchmarks including disease classification, demographic prediction,
anatomical localization, and cross-modal retrieval, demonstrating consistent
performance gains over existing foundation models and task-specific approaches.
Our results establish Decipher-MR as a scalable and versatile foundation for
MRI-based AI, facilitating efficient development across clinical and research
domains.

</details>


### [77] [Instruction-tuned Self-Questioning Framework for Multimodal Reasoning](https://arxiv.org/abs/2509.21251)
*You-Won Jang,Yu-Jung Heo,Jaeseok Kim,Minsu Lee,Du-Seong Chang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 本文提出了SQ-InstructBLIP方法，通过自主生成与图像内容相关的子问题和子答案，提升视觉问答（VQA）任务中多步推理的效果，从而取得了比现有方法更好的推理和答题准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型虽然在视觉-语言理解任务上已有应用，但由于无法直接读取细粒度图像信息，并且其推理过程难以解释和复现，因此处理需要多步推理的问题时依然存在局限。本文希望通过引入能够充分利用图像内容、过程更透明的新方法，来提升这种复杂推理能力。

Method: 作者提出了SQ-InstructBLIP框架，包括Questioner（提问者）、Answerer（回答者）和Reasoner（推理者）三个角色，架构一致。该方法通过Questioner和Answerer反复生成与图像相关的子问题及答案，Reasoner在结合子问题信息的基础上推理主问题。该方法可为主问题提供更丰富的图像上下文信息。

Result: 实验表明，SQ-InstructBLIP在视觉问答（VQA）任务上，通过将生成的子问题作为附加信息输入，显著提升了推理与答题的准确性，优于现有基于黑盒LLM的方法。

Conclusion: SQ-InstructBLIP不仅增强了模型利用图像细节信息的能力，还让推理过程更加透明、可控，在VQA等多步推理视觉-语言任务上具有更强的性能表现。

Abstract: The field of vision-language understanding has been actively researched in
recent years, thanks to the development of Large Language Models~(LLMs).
However, it still needs help with problems requiring multi-step reasoning, even
for very simple questions. Recent studies adopt LLMs to tackle this problem by
iteratively generating sub-questions and answers. However, there are
disadvantages such as 1) the fine-grained visual contents of images are not
available using LLMs that cannot read visual information, 2) internal
mechanisms are inaccessible and difficult to reproduce by using black-box LLMs.
To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,
which improves inference performance by generating image-aware informative
sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists
of a Questioner, Answerer, and Reasoner that share the same architecture.
Questioner and Answerer generate sub-questions and sub-answers to help infer
the main-question, and Reasoner performs reasoning on the main-question
considering the generated sub-question information. Our experiments show that
the proposed method SQ-InstructBLIP, which uses the generated sub-questions as
additional information when solving the VQA task, performs more accurate
reasoning than the previous works.

</details>


### [78] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文探讨了文本到图像生成模型(T2I)中的幻觉（hallucination）问题，并提出了一种新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 尽管幻觉现象已经在语言和视觉-语言模型中被研究，但在T2I模型中尚缺乏明确的定义和系统研究。现有评估方法主要聚焦于生成内容与提示词的对齐，而忽略了模型生成提示词之外内容的问题。

Method: 作者将T2I模型中的幻觉定义为由模型偏差驱动的偏离，并提出了三分类法：属性（attribute），关系（relation），和对象（object）幻觉。这一框架超越了传统单纯检查对齐的方法。

Result: 该框架能够揭示现有评测无法发现的隐藏偏差，并为T2I模型评估设定了新的上限。

Conclusion: 这项工作为T2I模型幻觉现象的研究和评估带来了更全面的视角，并奠定了理论基础，有助于后续更丰富和深入的模型评测。

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>


### [79] [Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2509.21261)
*Feng-Qi Cui,Jinyang Huang,Anyang Tong,Ziyu Jia,Jie Zhang,Zhi Liu,Dan Guo,Jianwei Lu,Meng Wang*

Main category: cs.CV

TL;DR: 该论文提出了面向微动作识别的全新无关人物识别框架，通过分布鲁棒优化学习具有人物无关特征的模型，实现了在复杂真实场景下更高的准确性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 微动作识别在心理评估与人机交互等领域非常重要，但现实中由于人物差异性，同一动作表现不一，导致模型难以泛化。现有方法普遍在人物多样性面前表现不佳，因此亟需鲁棒性强的新方案。

Method: 作者提出了“人物无关性通用微动作识别框架”，采用分布鲁棒优化思想，学习人物无关的行为表示。具体包括两大可插拔模块：①特征级的“时频对齐模块”，利用双分支结构对人物特定的动态与频率特征分别正则对齐与扰动提升鲁棒性，并由一致性机制融合；②损失级的“群体不变正则化损失”，将样本划分为伪群组，通过惩罚边界样本及调节组内方差，促进模型在难样本与新分布上的泛化。

Result: 在大规模MA-52数据集上的实验显示，该方法在准确性与鲁棒性均优于现有主流方法，且能在细粒度条件下稳定泛化。

Conclusion: 该框架通过特征正则与损失约束的创新组合，有效缓解了因人物差异导致的识别困难，实现了微动作识别领域在泛化和实用性上的明显突破。

Abstract: Micro-action Recognition is vital for psychological assessment and
human-computer interaction. However, existing methods often fail in real-world
scenarios because inter-person variability causes the same action to manifest
differently, hindering robust generalization. To address this, we propose the
Person Independence Universal Micro-action Recognition Framework, which
integrates Distributionally Robust Optimization principles to learn
person-agnostic representations. Our framework contains two plug-and-play
components operating at the feature and loss levels. At the feature level, the
Temporal-Frequency Alignment Module normalizes person-specific motion
characteristics with a dual-branch design: the temporal branch applies
Wasserstein-regularized alignment to stabilize dynamic trajectories, while the
frequency branch introduces variance-guided perturbations to enhance robustness
against person-specific spectral differences. A consistency-driven fusion
mechanism integrates both branches. At the loss level, the Group-Invariant
Regularized Loss partitions samples into pseudo-groups to simulate unseen
person-specific distributions. By up-weighting boundary cases and regularizing
subgroup variance, it forces the model to generalize beyond easy or frequent
samples, thus enhancing robustness to difficult variations. Experiments on the
large-scale MA-52 dataset demonstrate that our framework outperforms existing
methods in both accuracy and robustness, achieving stable generalization under
fine-grained conditions.

</details>


### [80] [Dense Semantic Matching with VGGT Prior](https://arxiv.org/abs/2509.21263)
*Songlin Yang,Tianyi Wei,Yushi Lan,Zeqi Xiao,Anyi Rao,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了基于3D几何基础模型VGGT的改进方法，用于提升语义匹配任务的表现，通过特定的训练策略和网络结构调整，有效克服了现有方法在几何歧义和邻域局限上的不足，显著提升了像素级匹配的精度和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D基础模型的语义匹配方法在面对对称结构时难以消除几何歧义，且像素点直接最近邻匹配忽略了跨图像的不可见性与流形结构，导致匹配效果有限。因此，需要更好地几何感知能力和描述子的匹配机制。

Method: 作者借鉴3D几何基础模型VGGT，通过复用其前期特征提取阶段，微调后续阶段，并新增语义匹配头部以获得像素级双向对应，同时结合循环一致性训练、合成数据增强及逐步训练流程等策略，有效缓解目标数据稀缺对匹配性能的影响。

Result: 所提出方法在几何感知、匹配可靠性和流形结构保持等方面取得了显著优于先前基线模型的结果，拥有更强的泛化和适应能力。

Conclusion: 本研究验证了将3D几何基础模型迁移到语义匹配任务中的可行性与有效性，通过多项创新性策略极大提升了匹配表现，对进一步推动该领域技术发展具有参考价值。

Abstract: Semantic matching aims to establish pixel-level correspondences between
instances of the same category and represents a fundamental task in computer
vision. Existing approaches suffer from two limitations: (i) Geometric
Ambiguity: Their reliance on 2D foundation model features (e.g., Stable
Diffusion, DINO) often fails to disambiguate symmetric structures, requiring
extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their
pixel-wise matching ignores cross-image invisibility and neglects manifold
preservation. These challenges call for geometry-aware pixel descriptors and
holistic dense correspondence mechanisms. Inspired by recent advances in 3D
geometric foundation models, we turn to VGGT, which provides geometry-grounded
features and holistic dense matching capabilities well aligned with these
needs. However, directly transferring VGGT is challenging, as it was originally
designed for geometry matching within cross views of a single instance,
misaligned with cross-instance semantic matching, and further hindered by the
scarcity of dense semantic annotations. To address this, we propose an approach
that (i) retains VGGT's intrinsic strengths by reusing early feature stages,
fine-tuning later ones, and adding a semantic head for bidirectional
correspondences; and (ii) adapts VGGT to the semantic matching scenario under
data scarcity through cycle-consistent training strategy, synthetic data
augmentation, and progressive training recipe with aliasing artifact
mitigation. Extensive experiments demonstrate that our approach achieves
superior geometry awareness, matching reliability, and manifold preservation,
outperforming previous baselines.

</details>


### [81] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 本文提出了一种专门针对医学视频超分辨率（VSR）任务的框架MedVSR，显著提升了医学低分辨率视频的重建效果，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 医学高分辨率视频对准确诊断非常重要，但由于设备及生理限制，很难获得。现有的VSR模型在医学场景下面临相机抖动、噪声、帧突变导致的对齐困难，同时医学组织结构细腻，现有模型易引入伪影和失真，可能误导医生。

Method: 提出MedVSR框架。首先通过交叉状态空间传播（CSSP）模块，将远距离帧投影到状态空间模型中，作为控制矩阵，对信息一致且有用的特征进行选择性传播，提升帧对齐效果。其次，引入内部状态空间重建（ISSR）模块，联合长距离空间特征学习与大核短距离信息聚合，增强组织结构并减少伪影。

Result: 作者在包括内镜和白内障手术等多种医疗场景下的四个数据集上进行了实验，结果显示MedVSR在重建性能和效率方面均显著优于现有VSR方法。

Conclusion: MedVSR能有效解决医学视频超分辨率任务中的对齐、伪影和结构失真等难题，具有良好的泛化能力和实际应用前景。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [82] [MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](https://arxiv.org/abs/2509.21268)
*Sicong Leng,Jing Wang,Jiaxi Li,Hao Zhang,Zhiqiang Hu,Boqiang Zhang,Yuming Jiang,Hang Zhang,Xin Li,Lidong Bing,Deli Zhao,Wei Lu,Yu Rong,Aixin Sun,Shijian Lu*

Main category: cs.CV

TL;DR: 本论文提出了针对大规模多模态推理模型在强化学习微调过程中的优化不稳定性问题的解决方案——Variance-Aware Sampling (VAS)，提升了策略优化的稳定性，并开源了高质量链式思考数据集和模型基线。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在推理任务上取得进展受限，主要瓶颈为缺乏大规模优质长链式思考数据和强化学习后训练阶段的优化不稳定（梯度消失）。现有RL优化方法如GRPO在奖励方差较低时，难以为模型提供足够的学习信号。

Method: （1）提出了Variance-Aware Sampling（VAS）数据选择策略，用Variance Promotion Score（VPS）来引导采样，促使奖励方差提升，从而稳定优化；（2）整理并公开了约160万条高质量链式思考冷启动数据和1.5万条RL QA样例，保证数据的质量、难度与多样性，并提供完整复现代码；（3）公开了一系列多尺度的多模态推理模型，为领域提供统一基线。

Result: 实验在数学推理基准集上证明了所整理数据和VAS策略的有效性，显著提升了模型表现。全面的消融实验和分析进一步阐释了各组成部分的贡献。此外，理论上证明奖励方差能够为期望策略梯度提供下界，并且VAS可以在实践中实现这一保证。

Conclusion: VAS策略和高质量数据集有效缓解了多模态推理模型强化学习训练中的“不稳定”难题，提升了推理性能。相关代码、数据和模型已完全开源，为学界后续研究奠定了坚实基础。

Abstract: Large multimodal reasoning models have achieved rapid progress, but their
advancement is constrained by two major limitations: the absence of open,
large-scale, high-quality long chain-of-thought (CoT) data, and the instability
of reinforcement learning (RL) algorithms in post-training. Group Relative
Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone
to gradient vanishing when reward variance is low, which weakens optimization
signals and impairs convergence. This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization. (2) We
release large-scale, carefully curated resources containing ~1.6M long CoT
cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,
and diversity, along with a fully reproducible end-to-end training codebase.
(3) We open-source a family of multimodal reasoning models in multiple scales,
establishing standardized baselines for the community. Experiments across
mathematical reasoning benchmarks demonstrate the effectiveness of both the
curated data and the proposed VAS. Comprehensive ablation studies and analyses
provide further insight into the contributions of each component. In addition,
we theoretically establish that reward variance lower-bounds the expected
policy gradient magnitude, with VAS serving as a practical mechanism to realize
this guarantee. Our code, data, and checkpoints are available at
https://github.com/LengSicong/MMR1.

</details>


### [83] [A Sentinel-3 foundation model for ocean colour](https://arxiv.org/abs/2509.21273)
*Geoffrey Dawson,Remy Vandaele,Andrew Taylor,David Moffat,Helen Tamura-Wicks,Sarah Jackson,Rosie Lickorish,Paolo Fraccaro,Hywel Williams,Chunbo Luo,Anne Jones*

Main category: cs.CV

TL;DR: 本论文提出了一种新的人工智能基础模型，利用Prithvi-EO Vision Transformer架构，通过自监督学习对大量未标记的海洋遥感数据进行预训练，并在两个下游海洋地球观测任务中展现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前海洋科学中高质量标注数据稀缺且获取成本高，限制了人工智能在该领域的应用。基础模型有望通过预训练方式高效利用大规模未标记数据，提升数据利用效率和下游任务表现。

Method: 采用Prithvi-EO Vision Transformer架构以自监督方式在Sentinel-3 OLCI遥感数据上预训练基础模型。模型通过微调，分别应用于叶绿素浓度量化和海洋初级生产力评估等下游任务，并与现有基线模型对比。

Result: 该基础模型在细粒度空间模式提取、点观测匹配以及少量高质量标注样本下展现出优越性能，超过了现有的基线方法。

Conclusion: 新一代地理空间AI基础模型为海洋生态系统监测和全球气候过程解析提供了更强大的数据驱动能力，将极大促进海洋科学AI应用的发展。

Abstract: Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive
unlabelled datasets, have the potential to drastically change AI applications
in ocean science, where labelled data are often sparse and expensive to
collect. In this work, we describe a new foundation model using the Prithvi-EO
Vision Transformer architecture which has been pre-trained to reconstruct data
from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the
model by fine-tuning on two downstream marine earth observation tasks. We first
assess model performance compared to current baseline models used to quantify
chlorophyll concentration. We then evaluate the FMs ability to refine remote
sensing-based estimates of ocean primary production. Our results demonstrate
the utility of self-trained FMs for marine monitoring, in particular for making
use of small amounts of high quality labelled data and in capturing detailed
spatial patterns of ocean colour whilst matching point observations. We
conclude that this new generation of geospatial AI models has the potential to
provide more robust, data-driven insights into ocean ecosystems and their role
in global climate processes.

</details>


### [84] [Does FLUX Already Know How to Perform Physically Plausible Image Composition?](https://arxiv.org/abs/2509.21278)
*Shilin Lu,Zhuming Lian,Zihan Zhou,Shaocong Zhang,Chen Zhao,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 本文提出了SHINE框架，实现了高保真、无缝的图像合成，解决了复杂灯光条件和高分辨率输入的问题，并建立了新的严格基准ComplexCompo，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成模型在处理复杂灯光（如阴影、水面反射）和高分辨率多样输入时表现不佳。尽管现代文本到图像扩散模型具备物理和分辨率先验，但难以充分利用，并且存在姿态不匹配和内容撕裂等问题，亟需一种更有效的无缝高保真合成方案。

Method: 提出SHINE框架，通过训练自由的方法引入流形引导锚点损失，并利用预训练自定义适配器（如IP-Adapter）实现主体精准表达及背景完整性。还提出降解抑制引导与自适应背景融合以去除低质量和缝隙。另构建了复杂、高分辨率的新基准ComplexCompo用于评测。

Result: 在ComplexCompo和DreamEditBench数据集上进行评测，SHINE在标准评价指标（如DINOv2）及人类一致性打分（如DreamSim、ImageReward、VisionReward）方面均达到当前最佳表现。

Conclusion: SHINE框架能够有效提升图像合成的真实性和无缝度，特别是在复杂灯光和高分辨率场景下表现突出，为未来的图像编辑与合成提供了有力技术支撑。

Abstract: Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.

</details>


### [85] [Quantized Visual Geometry Grounded Transformer](https://arxiv.org/abs/2509.21302)
*Weilun Feng,Haotong Qin,Mingqiang Wu,Chuanguang Yang,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了针对视觉几何基础变换器（VGGT）的大规模3D重建模型量化框架，通过创新性的量化技术显著降低模型内存与计算开销，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的3D重建模型取得了巨大进步，但由于其庞大的计算与内存需求，难以实际部署。现有的后训练量化（PTQ）方法在大规模VGGT模型上表现不佳，主要挑战在于特殊token导致的激活分布尾部过重，以及3D多视角数据造成校准样本选择不稳定。

Method: 提出QuantVGGT量化框架，包括两大核心技术：1）双平滑细粒度量化（Dual-Smoothed Fine-Grained Quantization），结合全球Hadamard旋转与局部通道平滑，有效缓解激活分布尾部过重问题和通道间方差过大问题；2）噪声过滤多样采样（Noise-Filtered Diverse Sampling），利用深层统计信息过滤异常样本，并构造多样性强的校准样本集，使量化更加稳定。

Result: 大量实验显示，QuantVGGT在不同基准测试和不同比特宽度下均达到最优性能，显著优于现有通用量化方法。特别是4比特量化下，实现了3.7倍内存压缩和2.5倍加速，重建精度仍保持在98%以上。

Conclusion: QuantVGGT有效解决了大规模3D VGGT量化面临的关键挑战，极大提升了模型在资源受限场景下的实际可用性和部署价值。

Abstract: Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and
2.5$\times$ acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.

</details>


### [86] [NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](https://arxiv.org/abs/2509.21309)
*Yu Yuan,Xijun Wang,Tharindu Wickremasinghe,Zeeshan Nadir,Bole Ma,Stanley H. Chan*

Main category: cs.CV

TL;DR: 目前文本生成视频（text-to-video）在物理一致性与可控性方面存在瓶颈。NewtonGen结合数据驱动和可学习物理原理，通过可训练的牛顿动力学模块，使生成的视频具备物理一致性和参数可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模文本生成视频模型容易生成物理不合理的运动（如物体反向移动、速度突变），且缺乏对生成视频细致、可控的物理参数调节。这一问题的根源在于只从外观上学习运动，而忽略了底层的物理动力学规律。

Method: 提出NewtonGen框架，将数据驱动的视频生成与可学习的神经牛顿动力学（NND）结合。NND能够建模及预测多种牛顿动力学运动，通过将这些潜在物理约束嵌入视频生成过程中，实现对运动的物理引导。

Result: NewtonGen能在保证视频生成内容丰富的同时，实现物理上合理的运动轨迹，可对不同初始条件下的动态行为进行精细调控，生成效果优于仅靠外观分布学习的方法。

Conclusion: NewtonGen通过引入神经物理动力学模型，突破了现有text-to-video生成的物理一致性和可控性障碍，为视频生成带来物理约束和高参数可控性的显著提升。

Abstract: A primary bottleneck in large-scale text-to-video generation today is
physical consistency and controllability. Despite recent advances,
state-of-the-art models often produce unrealistic motions, such as objects
falling upward, or abrupt changes in velocity and direction. Moreover, these
models lack precise parameter control, struggling to generate physically
consistent dynamics under different initial conditions. We argue that this
fundamental limitation stems from current models learning motion distributions
solely from appearance, while lacking an understanding of the underlying
dynamics. In this work, we propose NewtonGen, a framework that integrates
data-driven synthesis with learnable physical principles. At its core lies
trainable Neural Newtonian Dynamics (NND), which can model and predict a
variety of Newtonian motions, thereby injecting latent dynamical constraints
into the video generation process. By jointly leveraging data priors and
dynamical guidance, NewtonGen enables physically consistent video synthesis
with precise parameter control.

</details>


### [87] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: SD3.5-Flash是一种高效的少步蒸馏框架，可以让高质量图像生成在普通消费类设备上实现。


<details>
  <summary>Details</summary>
Motivation: 当前高质量图像生成模型（如rectified flow models）计算资源需求高，难以在普通设备上高效运行。因此，推动更快速且内存友好的模型实现具有实际意义，可以让更多用户受益于生成式AI。

Method: 提出了SD3.5-Flash，通过重新设计的分布匹配目标对昂贵的大模型进行蒸馏，重点创新包括“timestep sharing”减少梯度噪声和“split-timestep fine-tuning”优化prompt对齐。同时，通过对文本编码器重构和专用量化等优化，实现快速生成和低内存部署，支持多种硬件平台。

Result: 经大规模用户研究与评测表明，该方法在生成速度与质量上优于现有的少步生成方法。

Conclusion: SD3.5-Flash大幅提升了生成式AI的可用性，使普通用户可在各种设备上体验高质量生成，推动了技术的普及与实际应用。

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [88] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 本文提出一个以大语言模型为核心的框架，通过对外交事件文本进行有指导性的改写，实现对公众情绪的积极引导，并取得了较高的成功率。


<details>
  <summary>Details</summary>
Motivation: 外交事件频繁引发公众广泛讨论，且公众情绪对外交政策实施和国际形象具有重要作用。现有测量或引导公众情绪的方法效率低下，缺乏前瞻性，因此急需高效、数据驱动的新方法。

Method: 作者设计了一个新颖的框架：首先训练语言模型预测外交事件可能引发的公众反应，并整理了外交事件文本及相关舆论讨论的数据集；随后基于传播学理论和专家参与，预设若干文本特征作为可调整要素，保证改写改变叙事框架而不改变事实；最后开发了反事实生成算法，利用大语言模型系统化地生成修改版本文本，引导公众情绪向中性或积极转变。

Result: 所提框架能以70%的成功率将公众对外交事件的消极情绪转变为更加中立或积极，显示出方法的有效性。

Conclusion: 该框架为外交官、决策者和传播专家提供了一个可行的、数据驱动的舆论引导工具，可据此优化对外交事件的叙述方式，以促进更有利的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [89] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种跨语言语音情感识别（SER）的新方法，通过结合说话人风格和音素对齐，实现了情感在不同语言和说话人间的准确迁移。该方法在英文和台语语料库上取得了优于主流方法的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 跨语言的语音情感识别因不同语言的语音变异和说话人表现风格存在巨大差异，导致情感难以准确对齐和迁移。面对这种挑战，亟需能够在多语言、多说话人条件下对情感表达进行统一建模的框架。

Method: 作者提出基于说话人风格感知的音素锚定框架。具体包括通过图聚类的方法构建情感相关的说话人群体，捕捉说话人间的共有特质。随后，在说话人空间和音素空间实施双空间锚定，实现说话人和音素层面的情感对齐和迁移。

Result: 在MSP-Podcast（英语）和BIIC-Podcast（台湾普通话）语料上进行评测，所提方法相较于已有主流对比方法表现出更优的泛化性能，有效提升了跨语言的情感识别准确率。

Conclusion: 该研究表明，通过联合说话人与音素信息进行情感锚定，有助于突破跨语言语音情感识别中情感表达不一致的瓶颈，为跨语言情感建模提供了新的思路和方法。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [90] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 作者提出了CFDLLMBench，这是一个专为评估大语言模型（LLMs）在计算流体力学（CFD）等复杂物理系统中数值实验自动化能力而设计的评测基准。基准由CFDQuery、CFDCodeBench和FoamBench三部分构成，能从知识、推理和实现等维度全面考察LLM的科学能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用NLP任务中表现强劲，但其在自动化复杂物理系统（比如CFD）数值实验方面的效用尚未充分探索。鉴于CFD是计算科学的重要应用领域，作者希望通过该领域测试和推动LLMs在科学研究自动化中的应用。

Method: 作者设计了CFDLLMBench基准，由三部分组成：CFDQuery（考察研究生级别CFD知识）、CFDCodeBench（考查数值与物理推理）、FoamBench（评估依赖上下文的CFD工作流实现）。采用详细任务分类和严格评估框架，对代码可执行性、解的准确性和数值收敛性等维度进行量化。

Result: 作者开发了一套可重现实验，基于CFDLLMBench可以系统衡量不同LLM在CFD代码执行、解的精度和数值行为等方面的表现，为后续模型评测留存数据和代码资源。

Conclusion: CFDLLMBench为LLM在复杂物理系统数值实验自动化领域的应用、发展和公平评估奠定了基础，对推动AI辅助科学研究具有重要意义。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [91] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 本文评估了现有机器学习方法在区分ChatGPT-3.5生成文本与人工撰写文本方面的效果。实验表明，基于transformer的DistilBERT模型表现最佳，集成多模型未能超越单一DistilBERT模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如ChatGPT）的普及，区分AI生成文本与人工文本变得愈发困难，带来学术诚信、知识产权和信息误导等问题。因此，迫切需要可靠的AI文本检测方法。

Method: 作者使用包含250对来自不同研究主题的AI生成和人工撰写摘要的数据集，比较了多种经典与transformer类机器学习检测方法，包括Logistic Regression、BERT变体、DistilBERT、LSTM-N-gram等，并测试了模型集成方法。

Result: DistilBERT模型检测效果最佳。Logistic Regression和BERT自定义模型次之。LSTM与BERT-N-gram模型表现较差。多模型集成未能超越单一的DistilBERT表现。

Conclusion: 单一DistilBERT表现优于集成，强调了transformer模型的重要性。为后续更大数据集和更强检测模型的发展奠定了基础，以应对不断进步的AI文本生成技术。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [92] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: 本文提出了一套名为ConceptViz的可视分析系统，用于帮助研究者在大语言模型中更高效地发现、探索及验证稀疏自编码器（SAE）特征与人类概念之间的对应关系，从而提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏自编码器（SAEs）可从大语言模型中提取可解释特征，但这些特征与人类可理解的概念往往无法直接对齐，导致解释工作繁琐且费力。本文旨在弥补这一鸿沟。

Method: 作者开发了ConceptViz系统，设计了“发现-解释-验证”流程，支持用户基于感兴趣的概念查询SAE特征，交互式探索其对齐关系，并通过模型行为验证对应性。系统还配备了相应的可视分析工具。

Result: 通过两个使用场景与用户研究，作者展示了ConceptViz系统能够简化对LLM中有意义概念表示的发现和验证过程，有助于提升解释性研究的效率。

Conclusion: ConceptViz系统有助于研究者建立对大语言模型特征的准确心理模型，增强模型可解释性，对LLM研究具有积极促进作用。代码与用户手册已公开。

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [93] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: 本文提出了SKILL-RAG方法，通过利用大语言模型自身的自知之明，过滤掉无关的外部检索信息，从而提高RAG在知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG（检索增强生成）提升了大语言模型处理知识密集型任务的能力，但检索系统常返回无关内容，导致模型幻觉问题。因此，如何过滤无用信息，是提升RAG性能的关键挑战。

Method: 提出SKILL-RAG方法，利用模型自身对“知道/不知道”信息的判断（自知之明），结合强化学习框架，在句子级别过滤无关检索内容，仅保留对回答问题有用的知识。

Result: 在Llama2-7B和Qwen3-8B的多个问答基准测试中，SKILL-RAG显著提高了生成质量，并大幅减少了所需输入文档数量。

Conclusion: SKILL-RAG证明了自知之明在指导高质量检索选择中的重要性，有效提升了模型输出质量并减少噪音内容。

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [94] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种细粒度情感建模框架Emo-FiLM，用于提升基于大模型的语音合成系统（TTS）在表达局部情感变化方面的能力。通过新的数据集和框架，Emo-FiLM实现了比现有方法更自然、更有表现力的情感语音生成。


<details>
  <summary>Details</summary>
Motivation: 当前E-TTS系统普遍依赖预定义标签、参考音频或自然语言提示，主要控制句子的整体情感，却难以捕捉句内动态情感变化，导致生成语音不够自然和细腻。作者希望解决这一难题，实现更真实的人机语音交互。

Method: 提出Emo-FiLM框架，将emotion2vec得到的帧级情感特征与单词对齐，生成单词级情感标注，并通过Feature-wise Linear Modulation（FiLM）层将情感信息直接调制到文本嵌入，实现对每个单词的情感控制，并构建了带有情感转变标注的数据集FEDD用于评测。

Result: 实验表明，Emo-FiLM在整体和细粒度情感调控任务上均优于现有方法，能够生成更具表达力和自然度的情感语音。

Conclusion: Emo-FiLM框架能够有效支持细粒度情感控制，为高质量和真实人机语音交互提供了新的可能性，并证明了其通用性与优越性。

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [95] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种结合训练与推理的新型对话推荐系统框架USB-Rec，利用大语言模型（LLMs）提升对话推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的对话推荐研究多聚焦于推理期的能力，而忽略了针对LLM自身的训练问题，导致其推荐性能未被充分挖掘。

Method: 提出了USB-Rec框架，包括两大创新：1) 基于LLM的偏好优化数据集，用于RL训练，帮助LLM学习更有效的对话推荐策略；2) 推理阶段的自增强策略（SES），进一步提升模型已学得的对话推荐能力。

Result: 在多个公开数据集上实验，所提出方法在推荐效果上均优于现有最优方法。

Conclusion: 结合数据集构建与自增强技术，可显著提升LLM在对话推荐场景中的性能，建议在CRS中应关注模型训练环节以进一步提升应用效果。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [96] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 本文提出了一种名为“保准重要性摘要（Conformal Importance Summarization）”的新自动摘要框架，通过结合保准预测，首次为摘要中关键信息的包含率提供理论保证。该方法只需少量校准数据，且可与黑盒大模型直接结合，实现对重要内容的保障。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然能够生成摘要，但在医疗、法律和金融等高风险领域，仍难以保证关键信息的全面收录，导致实际应用中存在较大风险。因此，作者希望解决摘要中“重要内容不可控保障”的问题。

Method: 作者提出了保准重要性摘要方法：通过对句子重要性分数设置经过保准的阈值，实现对重要内容的抽取式摘要，且能根据用户需求指定覆盖率和召回率。该框架算法无模型依赖，且仅需少量的校准集，可与现有大模型无缝集成。

Result: 在多个主流摘要基准数据集上，所提出方法达到了理论保证的信息覆盖率，实验结果验证了方法在关键内容包含方面的有效性和准确性。

Conclusion: 保准重要性摘要方法能够协同现有摘要技术，为自动摘要提供可控的可靠性保障，有望推动AI摘要工具在医疗、法律等关键高风险领域的安全应用。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [97] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: 本文提出了ShortCheck系统，用于自动识别短视频中的值得核查内容，辅助人工查证，系统在TikTok多语种数据集上验证取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 短视频平台如TikTok因其多模态、动态和噪声大等特点，给虚假信息检测带来了新挑战，需要新的工具提升人工查证效率。

Method: ShortCheck系统采用模块化推理流程，集成了语音转录、OCR、物体识别、深度伪造检测、视频转文本摘要和事实核查等多个技术模块，通过用户友好的界面自动检测需要核查的短视频内容。

Result: 系统在两个经过人工标注的TikTok多语种数据集上进行测试，F1加权得分超过70%，表现出较为理想的性能。

Conclusion: ShortCheck能够有效辅助人工核查短视频内容，在多语种、复杂短视频环境中具备应用前景，为短视频内容可信度提升提供了技术支撑。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [98] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: 本文提出了MARS（多智能体评审系统），通过角色分工的方式提升大语言模型的推理能力，在保持推理准确性的同时大幅减少运算量和推理时间。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体辩论（MAD）有助于提升LLM的推理水平，但其高计算开销和频繁通信限制了实际应用。因此需要一种高效且协同推理的新框架。

Method: MARS模拟评审过程：由作者代理生成初步答案，评审代理独立反馈意见，最后由元评审合并意见并决策。这样可避免评审间的频繁信息交互，降低算力消耗。

Result: 实验表明，MARS在多个基准测试和不同LLM上，与MAD准确率相当，但token消耗和推理时间减少约50%。

Conclusion: MARS在提升多智能体推理协同效率的同时减少了成本，是一种高效替代MAD的推理策略。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [99] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文提出了SiniticMTError数据集，为英译普通话、粤语和吴语的机器翻译结果提供细致的错误类型与严重程度标注，有助于低资源语言的翻译研究。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译近年来取得了很大进展，但对于像粤语、吴语等缺乏大规模数据和语言资源的低资源语言，翻译质量提升依然有限。因此，作者希望通过提供详尽的错误标注数据，来推动这些语言翻译的质量评估和改进。

Method: 作者基于现有平行语料，构建了包含英译普通话、粤语、吴语机器翻译结果的语料集，并由以这些语言为母语的注释者，对翻译错误的类别、区间和严重程度进行了标注。研究还详细分析了标注过程的可靠性（注释者一致性）、反馈及错误分布。

Result: 作者成功建立了SiniticMTError数据集，严格完成了错误区间、类型和严重性标注，并通过分析标注者间的一致性及错误分布，验证了数据集的质量和多样性。

Conclusion: SiniticMTError成为机器翻译社区一个重要的新资源，特别适用于模型微调、翻译质量评估、错误感知生成和低资源语言翻译研究，助力相关领域持续发展。

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [100] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 本文提出SwasthLLM，一个支持英、印地语及孟加拉语的统一、零样本、跨语言多任务医学诊断框架，实现高度准确且无需针对具体语言微调的自动疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 多语种医疗环境自动疾病诊断面临低资源语言医学文本标注稀缺、不同语言间结构差异大的挑战，现有方法常局限于高资源语言或需大量特定语种数据。

Method: SwasthLLM采用多语种XLM-RoBERTa作为编码器，引入语言感知注意力机制及疾病分类头，通过Siamese对比学习模块对不同语言的语义表征对齐，并结合翻译一致性模块和对比投影头强化语言无关特征学习。利用多任务学习联合优化疾病分类、翻译语义对齐及对比目标，并通过模型无关元学习（MAML）提升对新语言/新任务的快速适应能力。采用分阶段训练，先全局对齐表征后再进行任务微调。

Result: 在监督医学文本诊断中，SwasthLLM取得97.22%准确率和97.17% F1分数。在零样本条件下，印地语医疗文本诊断准确率达92.78%，孟加拉语为73.33%，展现出对低资源语种良好的泛化能力。

Conclusion: SwasthLLM无需针对具体语言过度微调即可实现多语言、高效、鲁棒的医学诊断，尤其适用于低资源医疗环境，对多语种健康智能系统具较大推动意义。

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [101] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种新的Transformer架构——深度专门化的专家混合（DS-MoE），能够根据输入难度动态调整推理深度，提高效率和推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer架构对所有输入一视同仁地应用相同的处理深度，导致资源浪费且限制了复杂推理能力。不同复杂度的输入本应拥有不同的计算深度。

Method: 作者扩展了专家混合（MoE）架构，从传统的“宽度扩展”转变为“深度专门化”，将模型分为不同推理深度的专家模块，包括模式识别、组合推理、逻辑推理、记忆整合和元认知监督，并通过学习路由网络动态激活相应专家，按需根据输入难度组装推理链路。模型在The Pile等多样语料上训练评测。

Result: 与统一深度Transformer相比，DS-MoE实现了高达16%的计算节省，推理速度提升了35%，且在复杂多步推理基准中准确率提升2.8%。路由决策产生可解释的推理链，提高了模型透明度。

Conclusion: DS-MoE展现了深度专门化的混合专家框架在效率、推理质量和可解释性上的优势，为自适应神经网络结构的发展提供了新方向。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [102] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 本文提出一种新型的Transformer架构——层次分辨Transformer（HRT），通过多分辨率处理语言，实现更高效且更接近人类语言结构的建模，在多个自然语言理解基准上取得了优异表现并显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer将文本视为平坦的Token序列，无法有效反映语言本身的层次性，带来高昂的计算成本与有限的表达能力，难以支持复杂的篇章建模与泛化能力。

Method: HRT借鉴小波理论，采用多分辨率并行建模，把文本从字符到篇章多个层次同时处理，通过跨分辨率注意力机制，实现自底向上（组合）和自顶向下（上下文）信息流动，在不同尺度上使用指数级序列压缩，从而将计算复杂度降至O(nlogn)。

Result: 在GLUE、SuperGLUE、Long Range Arena、WikiText-103等主流数据集上的测试表明，HRT比同参数量的BERT/GPT基线分别平均提高3.8%、4.5%、6.1%，内存降低42%、推理延迟降低37%；消融实验显示，多分辨率注意力和尺度专用模块对精度和效率都有独立提升。

Conclusion: HRT首次让计算结构与语言层次结构对齐，证明多尺度、小波启发的处理方式不仅理论上更高效，也在实际理解任务中获得成效，为未来NLP模型设计指明了新方向。

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [103] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: FS-DFM是一种新型的离散流匹配模型，可大幅减少采样步数，同时保持生成语言的高质量，极大提升采样速度和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型生成效率受限，每次只能生成一个token，导致长序列生成时延较高。扩散语言模型虽然能并行，但通常需要成百上千步，效率低下。因此，亟需一种既能高效又能保证质量的文本生成方法。

Method: 提出Few-Step Discrete Flow-Matching（FS-DFM），将采样步数作为参数显式建模，并训练模型在不同步数预算下结果一致，从而用更少的“大步”实现与多步相同的效果。配合稳定的概率迁移规则和强大的教师范式蒸馏策略，提升模型少步采样时的稳定性及准确性。

Result: 在语言建模基准上，FS-DFM仅用8步采样即可达到和1024步基线模型相同的困惑度，生成长度为1024的文本时速度提升高达128倍。

Conclusion: FS-DFM可大幅减少离散流采样步数，极大提升文本生成速度而不影响质量，对缓解NLP模型推理延迟和提升吞吐具有重要意义。

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [104] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 该论文提出了一种无需实际实验就能预测大模型评测结果的新方法，并构建了相关基准测试集，验证了方法的可行性与挑战性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的研究发展受限于评测过程的瓶颈：必须构建基准、反复实验和分析结果，过程耗时低效。作者希望能提前预测模型在某项任务上的表现，赋能更高效的实验决策。

Method: 作者研究了“纯文本条件下的表现预测”问题，只依据任务描述和设定，完全不接触实际数据，预测模型分数。为此，作者构建了PRECOG语料库，包含多种任务、领域和指标的描述-表现对。实验中，采用了带检索模块的模型，并禁止访问源论文，分析了模型的预测能力和查询策略。

Result: 带检索模块的模型在排除源论文的情况下，能够对预测结果产生合理置信评估，在Accuracy子集的平均绝对误差最低达到8.7。同时发现，强推理能力的模型具有多样且迭代的检索行为，而开源模型在检索多样性及使用频率方面存在不足。

Conclusion: PRECOG数据集和相关分析为“实验前预测模型表现”的研究开辟了新思路，有助于难度估计和实验优先级决策，推动更开放和高效的模型开发流程。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [105] [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
*Yotaro Kubo,Richard Sproat,Chihiro Taguchi,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出了为日语口语评估任务定制语音识别器的方法，能够输出带有重音标记的音素标签，并通过多任务学习和模型融合技术，有效缓解训练数据稀缺问题，大幅提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然日语语音数据资源丰富，但用于训练生成带有重音标记的精确音素转录的数据却极为有限，造成特殊语音识别任务在准确率方面面临挑战。作者旨在解决数据稀缺下的高质量音素识别问题。

Method: 作者提出了两种方法：第一，多任务学习引入辅助损失函数，利用正字标签和输入信号基频模式，使不带音素标签的语料也能参与训练。第二，通过有限状态转导器算法融合基于音素和文本序列的两个估计器，结合两种输出以提升识别效果。

Result: 多任务学习与模型融合显著提升了音素识别准确率。相较于通用的多语言识别器，本文方法表现更优，在CSJ核心评估集上，将平均mora标签错误率从12.3%降至7.1%。

Conclusion: 提出的方法有效缓解了日语带重音音素转录任务中的数据稀缺问题，显著提升了模型性能，是相比于通用多语言识别模型更为优越的专用方案。

Abstract: This paper presents methods for building speech recognizers tailored for
Japanese speaking assessment tasks. Specifically, we build a speech recognizer
that outputs phonemic labels with accent markers. Although Japanese is
resource-rich, there is only a small amount of data for training models to
produce accurate phonemic transcriptions that include accent marks. We propose
two methods to mitigate data sparsity. First, a multitask training scheme
introduces auxiliary loss functions to estimate orthographic text labels and
pitch patterns of the input signal, so that utterances with only orthographic
annotations can be leveraged in training. The second fuses two estimators, one
over phonetic alphabet strings, and the other over text token sequences. To
combine these estimates we develop an algorithm based on the finite-state
transducer framework. Our results indicate that the use of multitask learning
and fusion is effective for building an accurate phonemic recognizer. We show
that this approach is advantageous compared to the use of generic multilingual
recognizers. The relative advantages of the proposed methods were also
compared. Our proposed methods reduced the average of mora-label error rates
from 12.3% to 7.1% over the CSJ core evaluation sets.

</details>


### [106] [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
*Peng Zhou,Lai Hou Tim,Zhixiang Cheng,Kun Xie,Chaoyi Li,Wei Liu,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 提出了一种将大语言模型（LLM）提取的知识与分子结构特征相结合的方法，用于提升分子属性预测（MPP）的效果，并验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分子属性预测在药物研发中非常重要。虽然图神经网络（GNN）和自监督学习在该领域取得了进展，但对人类先验知识的利用仍不可或缺。最近有方法尝试用LLM来提取知识，但LLM在某些分子属性上仍存在知识盲区和幻觉。为解决这些问题，需要一种有效融合领域知识和分子结构信息的方法。

Method: 本研究首次提出一种新框架，将从LLM（如GPT-4o、GPT-4.1和DeepSeek-R1）提取到的领域知识和可执行代码，与预训练分子模型提取到的结构特征进行融合。具体做法是通过LLM生成分子向量化相关的领域知识和代码，将知识特征和结构特征结合，用于分子属性预测。

Result: 在多个实验中，所提出的方法优于当前已有的分子属性预测方法。集成人类知识和结构信息后，模型的预测准确性和鲁棒性均得到提升。

Conclusion: 将LLM获取的知识与分子结构特征结合，是一种有效提升分子属性预测性能的途径，具有实际应用价值和推广性。

Abstract: Predicting molecular properties is a critical component of drug discovery.
Recent advances in deep learning, particularly Graph Neural Networks (GNNs),
have enabled end-to-end learning from molecular structures, reducing reliance
on manual feature engineering. However, while GNNs and self-supervised learning
approaches have advanced molecular property prediction (MPP), the integration
of human prior knowledge remains indispensable, as evidenced by recent methods
that leverage large language models (LLMs) for knowledge extraction. Despite
their strengths, LLMs are constrained by knowledge gaps and hallucinations,
particularly for less-studied molecular properties. In this work, we propose a
novel framework that, for the first time, integrates knowledge extracted from
LLMs with structural features derived from pre-trained molecular models to
enhance MPP. Our approach prompts LLMs to generate both domain-relevant
knowledge and executable code for molecular vectorization, producing
knowledge-based features that are subsequently fused with structural
representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and
DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that
our integrated method outperforms existing approaches, confirming that the
combination of LLM-derived knowledge and structural information provides a
robust and effective solution for MPP.

</details>


### [107] [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
*Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了一种新型对抗攻击方法RedHerring，可以显著降低对抗文本检测模型的准确率，但不会影响甚至可能提升分类器本身的准确率；同时也提出了一种简单的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗文本检测模型虽能发现被攻击过的文本，但其可靠性尚未充分研究。研究动机是探索这些检测模型容易受到哪些新型威胁，从而提升检测防御能力。

Method: 提出RedHerring攻击，该方法针对文本检测模型进行攻击，使检测器发生误判，但不损害分类器的预测结果。作者还提出了一种基于置信度的简单防御方法，无需重新训练模型。

Result: 在4个数据集、3个检测器和4个分类器上的实验表明，RedHerring攻击能令检测准确率下降20到71个百分点，而分类器准确率不降反升。提出的置信度防御能明显提升检测准确率。

Conclusion: RedHerring显著暴露了当前检测模型的脆弱性，新的威胁模型为探究检测对抗鲁棒性和未来检测防御开拓了新方向。

Abstract: In response to adversarial text attacks, attack detection models have been
proposed and shown to successfully identify text modified by adversaries.
Attack detection models can be leveraged to provide an additional check for NLP
models and give signals for human input. However, the reliability of these
models has not yet been thoroughly explored. Thus, we propose and test a novel
attack setting and attack, RedHerring. RedHerring aims to make attack detection
models unreliable by modifying a text to cause the detection model to predict
an attack, while keeping the classifier correct. This creates a tension between
the classifier and detector. If a human sees that the detector is giving an
``incorrect'' prediction, but the classifier a correct one, then the human will
see the detector as unreliable. We test this novel threat model on 4 datasets
against 3 detectors defending 4 classifiers. We find that RedHerring is able to
drop detection accuracy between 20 - 71 points, while maintaining (or
improving) classifier accuracy. As an initial defense, we propose a simple
confidence check which requires no retraining of the classifier or detector and
increases detection accuracy greatly. This novel threat model offers new
insights into how adversaries may target detection models.

</details>


### [108] [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
*Abhinay Shankar Belde,Rohit Ramkumar,Jonathan Rusert*

Main category: cs.CL

TL;DR: 提出两种新的对抗性文本攻击选择策略（Hybrid 和 Dynamic Select），有效降低了攻击所需的查询次数，同时保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂的Transformer架构下，现有的黑盒对抗样本攻击方法计算成本高、查询次数多，不适合计算资源有限的研究人员。

Method: 设计了Hybrid Select（通过阈值决定使用BinarySelect还是GreedySelect）和Dynamic Select（学习在不同文本长度下应用哪种选择算法），在句子级别灵活选择算法以优化查询效率。

Result: 在4个数据集和6个目标模型上，句子级Hybrid Select平均可减少约25.82%的每次攻击查询次数，同时攻击有效性不降低。

Conclusion: 新提出的选择策略能够兼顾查询效率和攻击效果，为研究者节省计算资源、提高攻击测试实用性。

Abstract: Adversarial text attack research plays a crucial role in evaluating the
robustness of NLP models. However, the increasing complexity of
transformer-based architectures has dramatically raised the computational cost
of attack testing, especially for researchers with limited resources (e.g.,
GPUs). Existing popular black-box attack methods often require a large number
of queries, which can make them inefficient and impractical for researchers. To
address these challenges, we propose two new attack selection strategies called
Hybrid and Dynamic Select, which better combine the strengths of previous
selection algorithms. Hybrid Select merges generalized BinarySelect techniques
with GreedySelect by introducing a size threshold to decide which selection
algorithm to use. Dynamic Select provides an alternative approach of combining
the generalized Binary and GreedySelect by learning which lengths of texts each
selection method should be applied to. This greatly reduces the number of
queries needed while maintaining attack effectiveness (a limitation of
BinarySelect). Across 4 datasets and 6 target models, our best
method(sentence-level Hybrid Select) is able to reduce the number of required
queries per attack up 25.82\% on average against both encoder models and LLMs,
without losing the effectiveness of the attack.

</details>


### [109] [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
*Hsiao-Ying Huang,Yi-Cheng Lin,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种在没有源数据且只能通过API访问大型音频-语言模型（LALM）情况下，有效适应目标域情感识别的框架，并实验证明学生模型可在目标域超越LALM本身。


<details>
  <summary>Details</summary>
Motivation: 真实世界语音情感识别（SER）部署常常面临域不匹配问题，尤其是在无法获取源域数据且LALM只能通过API访问时，如何适应目标域成为挑战。研究者希望提升学生模型在此场景下的表现，甚至超越提供API的LALM。

Method: 提出了MI-Fuse框架，利用LALM和一个源域训练的SER分类器作为辅助教师，通过对教师模型的多次随机预测结果，基于互信息的不确定性加权均值分布，并用指数移动平均教师稳定训练过程。所有训练只用目标域无标签数据，无需源域数据。

Result: 在三组公开情感数据集和六种跨域任务上，学生模型在目标域均超过了LALM，同时比最强基线高出3.9%。

Conclusion: MI-Fuse方法能够在源数据不可用和API仅访问受限条件下，有效增强情感感知语音系统的域自适应能力，显著提升实际应用的可行性和表现。

Abstract: Large audio-language models (LALMs) show strong zero-shot ability on speech
tasks, suggesting promise for speech emotion recognition (SER). However, SER in
real-world deployments often fails under domain mismatch, where source data are
unavailable and powerful LALMs are accessible only through an API. We ask:
given only unlabeled target-domain audio and an API-only LALM, can a student
model be adapted to outperform the LALM in the target domain? To this end, we
propose MI-Fuse, a denoised label fusion framework that supplements the LALM
with a source-domain trained SER classifier as an auxiliary teacher. The
framework draws multiple stochastic predictions from both teachers, weights
their mean distributions by mutual-information-based uncertainty, and
stabilizes training with an exponential moving average teacher. Experiments
across three public emotion datasets and six cross-domain transfers show
consistent gains, with the student surpassing the LALM and outperforming the
strongest baseline by 3.9%. This approach strengthens emotion-aware speech
systems without sharing source data, enabling realistic adaptation.

</details>


### [110] [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
*Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 本文提出了新的神经参数化方式来缓解无监督语法归纳中出现的概率分布塌缩问题，从而实现更精简且表现更优的文法学习。


<details>
  <summary>Details</summary>
Motivation: 目前的无监督神经语法归纳模型在表达能力方面存在瓶颈，表现为文法规模过大但效果较差。作者发现这主要源自于概率分布塌缩现象，亟需解决这一基础问题。

Method: 作者详细分析神经参数化中何时以及如何发生概率分布塌缩，并提出了一种名为collapse-relaxing的神经参数化新方法，专门用于缓解这一现象。

Result: 实验表明，该方法在多种语言上的语法分析性能大幅提升，同时显著减少了文法规模。

Conclusion: 该方法不仅提升了无监督神经语法归纳的性能，还使得模型结构更加紧凑，具有较强的通用性和应用前景。

Abstract: Unsupervised neural grammar induction aims to learn interpretable
hierarchical structures from language data. However, existing models face an
expressiveness bottleneck, often resulting in unnecessarily large yet
underperforming grammars. We identify a core issue, $\textit{probability
distribution collapse}$, as the underlying cause of this limitation. We analyze
when and how the collapse emerges across key components of neural
parameterization and introduce a targeted solution, $\textit{collapse-relaxing
neural parameterization}$, to mitigate it. Our approach substantially improves
parsing performance while enabling the use of significantly more compact
grammars across a wide range of languages, as demonstrated through extensive
empirical analysis.

</details>


### [111] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练无关型QA（问答）推理框架C2R，通过构造和优化子问题及其答案，基于置信度分数选择最可靠的最终答案，可应用于文本、图像和视频领域，且可无缝集成到现有模型，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的QA模型在推理过程中对置信度的利用不足，直接给出的答案往往缺乏信心与解释能力，且难以针对不同领域和多模态任务稳定提升性能。本文希望通过有效利用子问题及其置信度，提高模型推理的可靠性与稳健性。

Method: 提出C2R框架，主要包括：1）战略性地构造和优化子问题及其答案（sub-QAs）， 2）基于子QA对应答案的置信度，选择最优的答案， 3）完全不依赖额外训练，仅用模型自身置信度推理；并广泛分析子问题的数量和质量对推理表现的影响。

Result: C2R可无缝集成于多种主流QA模型中，在多个文本、图像和视频领域的QA基准实验上，均实现了稳健且一致的性能提升。分析表明，合适的子问题数量和高质量子问题能显著改善QA推理表现。

Conclusion: 利用模型置信度驱动的子问题推理，能帮助QA模型获得更为可靠和稳健的答案选择机制。C2R方法在不同QA任务和多种模型上均表现优秀，是一种通用、简单且易用的推理增强框架。

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [112] [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
*Jiacheng Lin,Zhongruo Wang,Kun Qian,Tian Wang,Arvind Srinivasan,Hansi Zeng,Ruochen Jiao,Xie Zhou,Jiri Gesi,Dakuo Wang,Yufan Guo,Kai Zhong,Weiqi Zhang,Sujay Sanghavi,Changyou Chen,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 该论文重新审视了针对大模型（LLM）有监督微调（SFT）导致泛化能力下降的问题，提出小学习率和一种新的损失重加权方法（TALR）作为缓解手段，实验证明TALR优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 领域适应微调会提升大模型的专用能力，但通常伴随泛化能力下降，因此需要研究如何在提升专用能力的同时缓解对泛化能力的损害。

Method: 作者首先通过实验证明小学习率有助于减少泛化能力下降，然后提出Token-Adaptive Loss Reweighting（TALR）方法，并与L2正则、LoRA、模型平均、FLOW等方法群体对比，理论上分析提出方法原因。

Result: 实验显示TALR在平衡领域能力提升和泛化能力保持两方面，均优于其他对比方法，尽管没有方法能完全消除这一权衡。

Conclusion: 论文最后给出实用建议：优先使用小学习率以平衡领域适应与泛化能力，如需进一步平衡则采用TALR，可以更好提升大模型新领域的适应效果。

Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach
to adapt Large Language Models (LLMs) to specialized tasks but is often
believed to degrade their general capabilities. In this work, we revisit this
trade-off and present both empirical and theoretical insights. First, we show
that SFT does not always hurt: using a smaller learning rate can substantially
mitigate general performance degradation while preserving comparable
target-domain performance. We then provide a theoretical analysis that explains
these phenomena and further motivates a new method, Token-Adaptive Loss
Reweighting (TALR). Building on this, and recognizing that smaller learning
rates alone do not fully eliminate general-performance degradation in all
cases, we evaluate a range of strategies for reducing general capability loss,
including L2 regularization, LoRA, model averaging, FLOW, and our proposed
TALR. Experimental results demonstrate that while no method completely
eliminates the trade-off, TALR consistently outperforms these baselines in
balancing domain-specific gains and general capabilities. Finally, we distill
our findings into practical guidelines for adapting LLMs to new domains: (i)
using a small learning rate to achieve a favorable trade-off, and (ii) when a
stronger balance is further desired, adopt TALR as an effective strategy.

</details>


### [113] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了原子理论（Atoms Theory），以定义和解析大语言模型（LLM）内部表征的基本单元“原子”，并用理论和实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前对于大语言模型内部表征的基本单元尚未明确。常用的神经元单元存在多义性问题，特征单元则稳定性和重构性不足。因此，迫切需要一种新的、可靠的单元形式来支撑对LLM机制的进一步理解与解释。

Method: 作者提出“原子”（atoms）作为LLM内部表征的基础单元，通过引入“原子内积（AIP）”来修正表征漂移，形式化地定义了原子，并证明了原子满足受限等距性（RIP），与压缩感知原理相关联。在更强假设下，还证明了原子稀疏表征的唯一性和$$1可恢复性，并理论上保证单层稀疏自编码器（SAEs）可可靠发现原子。随后在多个模型上进行了实验验证，分析性能和可解释性。

Result: 实验证明：在Gemma2-2B、Gemma2-9B和Llama3.1-8B等模型上，阈值激活的SAEs可实现99.9%的稀疏重构率，超过99.8%的原子满足唯一性条件（对比如神经元单元0.5%、特征单元68.2%）。还发现SAE的规模与表征恢复能力有直接关联。

Conclusion: 本文系统性地提出并验证了LLM原子理论，为理解和解释大型语言模型内部机制提供了新的理论框架。该理论比传统神经元和特征单元更能准确、稳定地表征LLM内部的知识结构，对未来可解释性研究具有重要意义。

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [114] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 本文提出了对话式提示法（Conversational Prompting），在用户评论极少及无法训练模型的情况下，更有效地生成个性化评论。


<details>
  <summary>Details</summary>
Motivation: 当前个性化评论生成方法普遍依赖于大量用户历史数据或需要对模型进行再训练，然而现实场景中常面临仅有极少用户评论（few-shot）且无法训练新模型（training-free）的情形，因此亟需轻量级、无需额外训练且效果优良的方法。

Method: 作者提出了两种对话式提示：简单对话提示（SCP）和对比对话提示（CCP）。SCP仅基于目标用户自身评论重组为多轮对话，CCP则加入其他用户或LLM生成的“错误”评论，并让模型纠正，从而强化个性化。

Result: 在8个产品领域和5种大模型实验中，相较于传统非对话式提示，新方法生成的评论在ROUGE-L、BERTScore、用户识别与情感分析等多项指标上均更贴近目标用户，仅用2条评论也有效。CCP在有高质量负样本时效果更佳，SCP在无法采集负样本时表现依然良好。

Conclusion: 对话式提示是少样本、无需训练条件下生成个性化评论的有效实用方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [115] [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
*Songze Li,Zhiqiang Liu,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Enrich-on-Graph (EoG)的新框架，通过丰富知识图谱，加强其与大语言模型（LLMs）在知识图谱问答（KGQA）任务中的结合，实现更高效、精确的推理，并在多个基准数据集上取得了最先进效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂推理任务中表现出强大能力，但在知识密集型场景如知识图谱问答中，经常出现幻觉和事实错误，主要原因是知识图谱的结构化信息与自然语言查询间存在语义鸿沟。现有方法通常忽视这一问题，导致推理表现不足。

Method: 作者提出Enrich-on-Graph (EoG)框架，利用LLMs的先验知识动态丰富知识图谱内容，桥接结构化知识与查询之间的语义差距，增强证据提取效率和推理鲁棒性。该框架兼顾低计算成本、可扩展性与多方法适配性。同时，提出三项图谱质量评价指标，用于理论与实验分析知识图谱与查询的契合度。

Result: 在两个知识图谱问答基准测试数据集上的大量实验结果显示，EoG能够有效生成高质量知识图谱，并达到了当前最先进的性能表现。

Conclusion: EoG框架通过丰富和优化知识图谱，显著提高了大型语言模型在KGQA任务中的推理准确性与鲁棒性，在保证效率和可扩展性的同时，推动相关领域向前发展。

Abstract: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex
tasks. However, they still struggle with hallucinations and factual errors in
knowledge-intensive scenarios like knowledge graph question answering (KGQA).
We attribute this to the semantic gap between structured knowledge graphs (KGs)
and unstructured queries, caused by inherent differences in their focuses and
structures. Existing methods usually employ resource-intensive, non-scalable
workflows reasoning on vanilla KGs, but overlook this gap. To address this
challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which
leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between
graphs and queries. EoG enables efficient evidence extraction from KGs for
precise and robust reasoning, while ensuring low computational costs,
scalability, and adaptability across different methods. Furthermore, we propose
three graph quality evaluation metrics to analyze query-graph alignment in KGQA
task, supported by theoretical validation of our optimization objectives.
Extensive experiments on two KGQA benchmark datasets indicate that EoG can
effectively generate high-quality KGs and achieve the state-of-the-art
performance. Our code and data are available at
https://github.com/zjukg/Enrich-on-Graph.

</details>


### [116] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种结合大模型和小模型优势的语法纠错方法PoCO，提升了召回率同时保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 小模型（sLMs）在精度高但召回率低，大模型（LLMs）召回高但容易过纠错、精度低；因此如何兼顾两者优势成为挑战。

Method: PoCO方法分两步：首先利用大模型故意过度纠错，确保高召回；然后用微调过的小模型对大模型输出结果再次修正，消除过纠错和错误。

Result: 实验结果表明，PoCO方法能够提升语法纠错召回率，同时保持与现有方法竞争的准确率，整体提升纠错效果。

Conclusion: PoCO有效结合LLMs和sLMs的优势，在GEC任务中实现了精度和召回的平衡，提升了纠错质量。

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [117] [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
*Ukyo Honda,Soichiro Murakami,Peinan Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为cheat-sheet ICL的新方法，将大量示例的上下文精炼成简洁文本，在保持甚至提升性能的同时大幅减少了输入token数量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在利用多示例上下文学习时，虽然效果优异，但输入token过长导致计算开销高。寻求减少token同时保持性能的方法。

Method: 通过将多示例ICL中的核心信息压缩、总结成一份“cheat sheet”，作为推理时的上下文输入，减少token数量。并与传统的多示例ICL及检索式ICL在推理任务上进行对比实验。

Result: 在具挑战性的推理任务中，cheat-sheet ICL以更少的token实现了与多示例ICL相当或更好的表现，同时无需像检索式ICL那样做测试时检索。

Conclusion: cheat-sheet ICL为实际任务中有效利用大语言模型提供了一种高性价比的选择，减少了推理时的资源消耗。

Abstract: Recent advances in large language models (LLMs) enable effective in-context
learning (ICL) with many-shot examples, but at the cost of high computational
demand due to longer input tokens. To address this, we propose cheat-sheet ICL,
which distills the information from many-shot ICL into a concise textual
summary (cheat sheet) used as the context at inference time. Experiments on
challenging reasoning tasks show that cheat-sheet ICL achieves comparable or
better performance than many-shot ICL with far fewer tokens, and matches
retrieval-based ICL without requiring test-time retrieval. These findings
demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs
in downstream tasks.

</details>


### [118] [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
*Shuo Huang,Xingliang Yuan,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CL

TL;DR: 提出了一种基于树搜索的迭代句子重写算法，在保护隐私的同时保持文本流畅和有用性，优于现有脱敏方法。


<details>
  <summary>Details</summary>
Motivation: 随着云端大模型的普及，用户输入容易泄露敏感信息。目前的文本匿名化和脱敏技术难以兼顾隐私保护和文本自然性。

Method: 设计了一种零样本、基于树搜索的迭代重写算法，通过回报模型指导递增式重写敏感段落，动态探索重写空间，实现对敏感信息的系统性混淆或删除。

Result: 在隐私敏感数据集上，实验结果显示该方法在隐私保护和文本有用性之间取得了优于主流方法的平衡。

Conclusion: 该方法有效提升了大模型场景下数据隐私保护能力，同时将文本流畅度与实用性损失降到最低，为隐私保护提供了更优的解决方案。

Abstract: The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.

</details>


### [119] [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
*Guo Chen,Qiuyuan Li,Qiuxian Li,Hongliang Dai,Xiang Chen,Piji Li*

Main category: cs.CL

TL;DR: 现有RAG问答系统中的文献引用大多不够细致，本文提出并实现了更为简洁且充分的子句级文献引用方法，有效提升了输出验证效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的引用粒度过粗（句级或段级），导致引用内容中夹杂大量无关信息，或遗漏验证所需的关键信息，给用户验证输出增加负担。作者希望通过改进引用粒度，提升引用的实用性和可读性。

Method: 作者首先制定子句级引用的标注标准，并据此构建数据集。然后设计了一套归因框架，利用大模型自动生成微调数据，并用一个信用评估模型过滤低质量样本，实现对高质量子句级引用的生成。

Result: 在自建数据集上的实验表明，该方法能生成高质量且更易读的引用，相比传统方法在验证内容准确性和简洁性方面表现更优。

Conclusion: 提出的子句级引用方案显著改善了现有RAG系统的引用局限，提升了输出内容的可验证性和用户体验。

Abstract: In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.

</details>


### [120] [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
*Guowei Xu,Wenxin Xu,Jiawang Zhao,Kaisheng Ma*

Main category: cs.CL

TL;DR: 本文提出了一种针对扩散语言模型的加权监督微调方法（WeFT），通过对不同token赋予基于熵的权重，有效提升了模型在复杂推理任务上的表现。实验显示该方法在多个数据集上显著优于传统微调策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语言生成任务中表现出色，并拥有更快的生成速度，但其监督微调（SFT）存在困难，主要因为缺乏对每一步的精确概率估计，导致生成过程不稳定、可控性差。特别是在生成需关键引导的序列时，现有方法显得力不从心，因此亟需改进微调策略以增强扩散模型的控制力和一致性。

Method: 作者提出WeFT（Weighted SFT），一种加权监督微调方法，根据扩散理论，利用token的熵为不同token分配权重，从而引导模型更关注指导生成方向的关键token。该方法可直接应用于扩散语言模型的微调阶段。

Result: 在s1K、s1K-1.1及3k的open-r1样本下，WeFT在Sudoku、Countdown、GSM8K、MATH-500四个广泛使用的推理基准水平上，相比标准SFT分别有39%、64%和83%的相对性能提升。

Conclusion: WeFT方法能够显著提升扩散语言模型在推理类任务中的表现，尤其在小样本场景下显示巨大潜力，有望推动相关模型的实用与发展。代码和模型也将开源，便于社区复现和进一步研究。

Abstract: Diffusion models have recently shown strong potential in language modeling,
offering faster generation compared to traditional autoregressive approaches.
However, applying supervised fine-tuning (SFT) to diffusion models remains
challenging, as they lack precise probability estimates at each denoising step.
While the diffusion mechanism enables the model to reason over entire
sequences, it also makes the generation process less predictable and often
inconsistent. This highlights the importance of controlling key tokens that
guide the direction of generation. To address this issue, we propose WeFT, a
weighted SFT method for diffusion language models, where tokens are assigned
different weights based on their entropy. Derived from diffusion theory, WeFT
delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from
open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard
SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and
MATH-500). The code and models will be made publicly available.

</details>


### [121] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文系统性地研究了让医学推理模型（MRMs）为开放性问题生成排序答案列表，而不是传统的单一答案。通过提示工程和微调（包括监督和强化微调）等方式，实现模型输出多答案排序的能力，并首次针对该任务制定了奖励函数。实验证明RFT方法对不同答案格式具有更强的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床决策往往需要考虑多个选项来规避单一答案的风险，但目前大多数医学推理模型仅能输出一个答案，无法满足实际需求。本研究旨在填补MRMs只能生成单一答案的不足，通过生成排序的答案列表，提高模型在真实医疗场景下的实用性和多样性。

Method: 作者提出并对比了两种主要方法：1）通过prompting引导模型生成排序答案，探索模型在不同答案格式（选择题、简答、列表）中的表现泛化能力；2）采用监督微调（SFT）和强化微调（RFT），提出针对排序答案的新型奖励函数，并进行消融实验以验证各方法效果。还在多有效答案的MedQA改版基准上进行了案例研究。

Result: 研究发现，部分SFT模型能在特定答案格式上泛化，但RFT模型在多种答案格式下表现更稳定且鲁棒。即便模型未必能选择基准测试中的首选答案，但能识别出多项有效答案，展示了多答案输出的可行性和实际价值。

Conclusion: 本研究首次系统性探索了让医学推理模型输出排序答案列表的方法，验证了多答案格式在医学推理模型中的可行性和优势。该工作为医学问答系统提供了多样化答案输出的新方向，有望在实际临床应用中提供更丰富和有用的决策支持。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [122] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出了一种名为SummQ的新型多智能体对抗框架，通过协作提高长文档摘要的质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理超长文档摘要时常常表现出信息丢失、事实不一致和连贯性差的问题。作者希望解决这些核心挑战，提升长文档自动摘要的水平。

Method: SummQ采用多智能体协作，包括摘要生成者、摘要审查者、测验题生成者和测验审查者，以及作为考生的智能体。通过问答（quiz）作为动态反馈机制，智能体不断迭代优化摘要，检查摘要内容对答题的支持度，实现对抗式、多维度的优化。

Result: 在三大长文档摘要基准上，SummQ在ROUGE、BERTScore等指标以及大语言模型评测和人工评测中均显著优于最新方法。分析表明多智能体协作模式及测验机制极大提升了摘要的准确性和完整性。

Conclusion: SummQ通过多智能体对抗与协作，建立了改进长文档摘要的新范式，为复杂文本的自动摘要提供了有效方案，推动了领域发展。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [123] [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
*Zirui He,Haiyan Zhao,Ali Payani,Mengnan du*

Main category: cs.CL

TL;DR: 该论文提出了MemLens方法，通过分析大语言模型生成过程中数字token的概率轨迹来检测是否存在记忆污染，与传统的基于词汇重叠与困惑度的方法相比，准确性和泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型记忆污染的检测手段容易受到词汇表层使用和困惑度限制，对隐含污染数据效果不好。需要更加可靠的检测方法，以区分模型记忆与真正的推理能力。

Method: 作者提出了MemLens方法，关注生成过程中数字token的概率变化轨迹。通过比较污染样本和干净样本的激活轨迹，发现污染样本在模型早期层就锁定答案，而干净样本则是在深层逐步累积证据。作者还通过LoRA微调人为注入污染样本，验证这种轨迹的普适性。

Result: 结果表明污染样本与干净样本在生成过程中具有明显区分的概率轨迹模式，说明MemLens能有效检测记忆污染。人工注入的污染同样表现出相同的轨迹规律，进一步证明该方法能捕捉真实记忆信号而非伪相关。

Conclusion: MemLens能够更可靠地识别大语言模型中的记忆污染，优于现有基于表层词汇或困惑度的方法，有助于准确评估模型的真实推理能力。

Abstract: Large language models (LLMs) are commonly evaluated on challenging benchmarks
such as AIME and Math500, which are susceptible to contamination and risk of
being memorized. Existing detection methods, which primarily rely on
surface-level lexical overlap and perplexity, demonstrate low generalization
and degrade significantly when encountering implicitly contaminated data. In
this paper, we propose MemLens (An Activation Lens for Memorization Detection)
to detect memorization by analyzing the probability trajectories of numeric
tokens during generation. Our method reveals that contaminated samples exhibit
``shortcut'' behaviors, locking onto an answer with high confidence in the
model's early layers, whereas clean samples show more gradual evidence
accumulation across the model's full depth. We observe that contaminated and
clean samples exhibit distinct and well-separated reasoning trajectories. To
further validate this, we inject carefully designed samples into the model
through LoRA fine-tuning and observe the same trajectory patterns as in
naturally contaminated data. These results provide strong evidence that MemLens
captures genuine signals of memorization rather than spurious correlations.

</details>


### [124] [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
*Krishna Aggarwal*

Main category: cs.CL

TL;DR: 本研究探讨在句子理解过程中，记忆负荷是由句法关系词的线性距离还是介入结构的复杂度更好解释。研究结果发现，尽管句长与依存距离都对记忆负荷有影响，但介入复杂度（即中间成分的句法头数量）能提供更有力的解释。


<details>
  <summary>Details</summary>
Motivation: 句子理解时的记忆负荷一直是心理语言学关注的问题。传统理论强调句法关系词之间的线性距离越远，理解成本越高。但也有理论认为结构复杂度（如中间成分的数量）可能比线性距离影响更大。该研究试图在二者之间进行对比，寻找更准确的解释指标。

Method: 作者利用多语言的依存句法树库，采用混合效应建模，对比了句长、依存距离和介入复杂度对记忆负荷的预测力。记忆负荷以特征错误绑定（feature misbinding）与特征干扰（feature interference）的加和表征。

Result: 三个因素（句长、依存距离、介入复杂度）均与记忆负荷正相关，句长影响最大，但介入复杂度能够独立预测记忆负荷，且效果超越线性距离。

Conclusion: 研究结果指出，依存距离虽是理解记忆负荷的重要表面特征，但介入结构的复杂度（介入头的数量）才是加工和整合需求更直接的指标。该方法结合了线性和层级视角，为记忆负荷理论的评估提供了新方法论路径。

Abstract: This study examines whether sentence-level memory load in comprehension is
better explained by linear proximity between syntactically related words or by
the structural density of the intervening material. Building on locality-based
accounts and cross-linguistic evidence for dependency length minimization, the
work advances Intervener Complexity-the number of intervening heads between a
head and its dependent-as a structurally grounded lens that refines linear
distance measures. Using harmonized dependency treebanks and a mixed-effects
framework across multiple languages, the analysis jointly evaluates sentence
length, dependency length, and Intervener Complexity as predictors of the
Memory-load measure. Studies in Psycholinguistics have reported the
contributions of feature interference and misbinding to memory load during
processing. For this study, I operationalized sentence-level memory load as the
linear sum of feature misbinding and feature interference for tractability;
current evidence does not establish that their cognitive contributions combine
additively. All three factors are positively associated with memory load, with
sentence length exerting the broadest influence and Intervener Complexity
offering explanatory power beyond linear distance. Conceptually, the findings
reconcile linear and hierarchical perspectives on locality by treating
dependency length as an important surface signature while identifying
intervening heads as a more proximate indicator of integration and maintenance
demands. Methodologically, the study illustrates how UD-based graph measures
and cross-linguistic mixed-effects modelling can disentangle linear and
structural contributions to processing efficiency, providing a principled path
for evaluating competing theories of memory load in sentence comprehension.

</details>


### [125] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 本文关注于让大型语言模型（LLM）在阿拉伯语环境下实现工具调用能力，弥补目前以英语为主的相关研究空白。通过翻译和改编开源数据集，系统性地评估了不同训练策略对阿拉伯语工具调用任务的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的工具调用大大拓展了其实用性，但目前相关研究和资源高度集中于英语领域，非英语（如阿拉伯语）场景支持严重不足。文中旨在探究如何高效支持阿拉伯语LLM的工具调用功能。

Method: 作者提出三个核心研究问题：是否需要专门的阿拉伯语数据、通用指令微调对工具调用性能的影响，以及针对特定工具微调的价值。研究通过将两套开源工具调用数据集翻译并改编为阿拉伯语，使用开源阿拉伯语LLM开展基线和微调实验系统评估各策略。

Result: 实验结果揭示了原生阿拉伯语数据、通用指令调优和针对特定工具微调在提升工具调用表现中的不同作用和优化策略。

Conclusion: 研究为高效开发阿拉伯语工具增强智能体提供了实证依据和最佳实践建议，有助于推动非英语LLM的可用性和适用性拓展。

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [126] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 本文提出并比较了五种基于大语言模型（LLM）的学术文本输入题自动评分系统，发现利用参考答案的评估法（Reference Aided Evaluation）在公正性和准确性上最接近真人评分。


<details>
  <summary>Details</summary>
Motivation: 当前教育领域对自动化评分系统需求增加，尽管LLM已被用作助教工具，对其作为评估者的系统性研究不足，特别是在利用评分标准自动评阅学术文本题方面。本研究旨在探讨和评估多种LLM主导的自动评价方法，提升客观题评阅效率与质量。

Method: 研究使用JudgeLM、Llama-3.1-8B及DeepSeek-R1-Distill-Llama-8B三款模型，在高等教育计算机专业学生的自建数据集（110个文本回答）上，实验并对比了五种评分系统：JudgeLM评分、参考辅助评分、无参考评分、加性评分和自适应评分。各方法均与人工评分结果进行比对分析。

Result: 参考辅助评分法在评价一致性、分数稳定性（中位绝对偏差0.945，均方根偏差1.214）上最接近人类评审。加性与自适应评分对简短答案表现不佳，无参考法信息不足，JudgeLM法受模型规则局限。

Conclusion: 人工智能驱动的自动化文本题评分系统在结合合理评估方法后，具备作为学术资源辅助工具的巨大潜力，但仍需进一步优化和完善评分机制。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [127] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 该论文探讨了利用大型语言模型（LLM）加速政府科研机构中的文本处理任务，包括摘要、分类和信息提取，同时通过OnPrem.LLM确保数据安全和合规性。


<details>
  <summary>Details</summary>
Motivation: 联邦资助的研发中心（FFRDCs）需要处理大量文本，但手动分析效率低且耗时，因此需要自动化和可扩展的AI方法来提升处理效率，并满足数据安全与监管需求。

Method: 提出并应用OnPrem.LLM这一开源框架，实现大模型在本地安全环境下运行，通过少量样例完成摘要、分类和抽取等任务。以国防政策和科研奖项文档为案例，验证了该方法的实用性。

Result: 通过实际案例展示，OnPrem.LLM能够高效地对政策和科研文档进行分析，加强了监管与战略分析能力，并保障了数据主权和可审计性。

Conclusion: OnPrem.LLM为政府及科研机构提供了安全、高效的AI文本处理方案，能有效提升工作效率，促进数据合规与自主可控。

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [128] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 本文发现Transformer中的causal mask本身就是一种位置编码方式，对注意力分数产生位置相关效应，不可忽略。


<details>
  <summary>Details</summary>
Motivation: 以往Transformer解码器主要依赖显式位置编码（如RoPE）提供位置信息，忽略了causal mask的作用。本文旨在深入探究causal mask本身是否影响模型的位置信息表达。

Method: 作者通过理论分析和实证研究，证明即便没有输入参数，causal mask也会导致注意力分数存在与位置相关的模式。实验还比较了不同模型和RoPE与causal mask的交互影响。

Result: 结果显示，不仅理论上，实证中模型训练后也会放大causal mask导致的位置依赖特性。此外，causal mask和RoPE的结合会让RoPE的相对位置编码失真。

Conclusion: causal mask不仅仅是防止信息泄漏的工具，它本身就是模型不可忽略的位置编码来源，因此今后研究和设计位置编码时需同步考虑causal mask的作用。

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [129] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 该论文评估了大语言模型（LLMs）同时遵循多条指令的能力，提出了两个新基准并发展了性能预测模型，结果表明性能随指令数量增加而下降，可用回归模型高效估算未知组合表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实场景中的应用增多，多指令同时遵循的能力变得重要，但以往缺乏系统化、专门化的评测方法，且难以对所有指令组合做全面测试。

Method: 提出两个专用评测基准：ManyIFEval（文本生成，最多十条指令）和StyleMBPP（代码生成，最多六条指令）；用十个LLMs做实验，分析性能随指令数如何变化；开发三种回归模型，估算未见过组合及不同数量指令下的性能，并检验样本量对估算效果的影响。

Result: 结果显示LLM处理多指令时，性能随着指令数增加持续下降；逻辑回归模型仅以指令数为变量即可以约10%误差预测未见组合的性能；较小样本量（ManyIFEval为500，StyleMBPP为300）即可支持准确估算。

Conclusion: 只需有限样本和简单的回归模型，即可高效推断LLM在多指令场景下的表现，有助于实际部署和性能评估。

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [130] [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
*Qixin Wan,Zilong Wang,Jingwen Zhou,Wanting Wang,Ziheng Geng,Jiachen Liu,Ran Cao,Minghui Cheng,Lu Cheng*

Main category: cs.CL

TL;DR: 该论文提出了SoM-1K，这是首个专注于材料力学领域大规模多模态基础模型评测数据集，评估了8个主流基础模型，发现现有模型在复杂工程任务中表现不佳，文本描述对模型帮助显著。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在自然语言、视觉等领域表现优异，但在工程复杂多模态问题（如材料力学）上的能力鲜有系统评估，因此亟需专门的数据集与评测方法来推动工程AI发展。

Method: 1. 构建SoM-1K数据集，包含1065个带注释的材料力学问题，每题含文本描述和示意图。2. 提出图像描述（DoI）新提示范式，由专家生成可准确描述视觉内容的文本，辅助模型理解图片信息。3. 用DoI和原图像分别测试8个基础模型（包括LLMs与VLMs），并开展详细误差分析。

Result: 目前所有基础模型在此工程任务上的表现均较差，最佳模型准确率仅为56.6%；当给LLMs提供DoI文本描述时，其表现优于直接输入图片给VLMs。DoI有助于减少模型的视觉误判。

Conclusion: SoM-1K数据集为工程AI评测提供基准，强调当前基础模型多模态推理能力的不足。准确的图像文本描述在实际应用中比现有的视觉输入更有效，亟需发展更强多模态推理能力的基础模型，尤其针对科学与工程领域。

Abstract: Foundation models have shown remarkable capabilities in various domains, but
their performance on complex, multimodal engineering problems remains largely
unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark
dataset dedicated to evaluating foundation models on problems in the strength
of materials (SoM). The dataset, which contains 1,065 annotated SoM problems,
mirrors real-world engineering tasks by including both textual problem
statements and schematic diagrams. Due to the limited capabilities of current
foundation models in understanding complicated visual information, we propose a
novel prompting strategy called Descriptions of Images (DoI), which provides
rigorous expert-generated text descriptions of the visual diagrams as the
context. We evaluate eight representative foundation models, including both
large language models (LLMs) and vision language models (VLMs). Our results
show that current foundation models struggle significantly with these
engineering problems, with the best-performing model achieving only 56.6%
accuracy. Interestingly, we found that LLMs, when provided with DoI, often
outperform VLMs provided with visual diagrams. A detailed error analysis
reveals that DoI plays a crucial role in mitigating visual misinterpretation
errors, suggesting that accurate text-based descriptions can be more effective
than direct image input for current foundation models. This work establishes a
rigorous benchmark for engineering AI and highlights a critical need for
developing more robust multimodal reasoning capabilities in foundation models,
particularly in scientific and engineering contexts.

</details>


### [131] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 本文发现主流大语言模型（LLMs）在生成内容时默认采用美国主流文化视角，对非主流文化有潜在外部性与公平性偏见。作者构建了CultureLens基准和相关评测，揭示了该文化定位偏见，并提出基于推理时干预的公平性缓解方法。实验显示，结构化的多智能体方法能有效降低偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型应用广泛，但其生成内容存在文化公平性问题。模型在美国文化背景下表现出“内部者”立场，而对其他文化则表现为“外部者”视角，容易忽视多元文化平等表达的需求。探究并修正这一缺陷对提升模型的普适性和责任至关重要。

Method: 提出CultureLens基准，包括4000条多文化生成任务和3种评测指标，通过模拟10种不同文化语境下的采访脚本生成，对现有5个主流大模型做系统分析。同时，研发了两类推理时公平性缓解方法：1）基于提示词的公平干预（FIP）；2）基于代理的MFA框架（包含单智能体自反与重写，以及由策划、批判和完善三个智能体依序协作的多智能体结构）。

Result: 实验证明，主流模型对美国文化的脚本88%以上采用内部者视角，对小众文化则多为外部者立场。提出的MFA多代理方法在缓解此类偏见方面表现突出，优于仅靠提示词的方法。

Conclusion: 主流LLM存在显著的文化定位偏见，尤其表现为对非主流文化态度的“外部化”。引入结构化多代理干预机制，可有效提升生成内容的文化公平性，为未来模型公平性改进提供了有力工具与新方向。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [132] [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
*Mohammad Hosseini,Kimia Hosseini,Shayan Bali,Zahra Zanjani,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本文提出了首个专为波斯语设计的动态幻觉检测评测基准PerHalluEval，并评估了12种大语言模型在该基准上的表现，发现所有模型在检测波斯语幻觉方面表现均不理想。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在低资源语言（如波斯语）中，幻觉问题尤为突出，且缺少针对波斯语的系统性评测工具。因此，作者希望通过专门的波斯语幻觉评测集，研究和量化模型在波斯语中的幻觉表现。

Method: 作者设计了PerHalluEval：一个包括问答和摘要的波斯语幻觉检测基准，采用三阶段LLM管道生成幻觉数据及可信答案，并用人类验证进行加强。通过分析生成token的对数概率，筛选出最可信的幻觉实例。同时引入波斯语特有文化场景。最后评估了12种开源和闭源LLM，并比较了训练数据是否为波斯语的影响。

Result: 在PerHalluEval基准上，所有12种模型对波斯语幻觉的检测效果都不佳。为摘要任务补充外部知识源材料可一定程度降低幻觉率。同时，专门为波斯语训练的模型与非专门模型在幻觉表现上无显著差异。

Conclusion: 当前LLM在波斯语幻觉检测能力有限，尤其在波斯语相关文化背景下。未来若要提升LLM在低资源语言的可靠性，需要更高质量的训练数据与更系统的评测手段。

Abstract: Hallucination is a persistent issue affecting all large language Models
(LLMs), particularly within low-resource languages such as Persian.
PerHalluEval (Persian Hallucination Evaluation) is the first dynamic
hallucination evaluation benchmark tailored for the Persian language. Our
benchmark leverages a three-stage LLM-driven pipeline, augmented with human
validation, to generate plausible answers and summaries regarding QA and
summarization tasks, focusing on detecting extrinsic and intrinsic
hallucinations. Moreover, we used the log probabilities of generated tokens to
select the most believable hallucinated instances. In addition, we engaged
human annotators to highlight Persian-specific contexts in the QA dataset in
order to evaluate LLMs' performance on content specifically related to Persian
culture. Our evaluation of 12 LLMs, including open- and closed-source models
using PerHalluEval, revealed that the models generally struggle in detecting
hallucinated Persian text. We showed that providing external knowledge, i.e.,
the original document for the summarization task, could mitigate hallucination
partially. Furthermore, there was no significant difference in terms of
hallucination when comparing LLMs specifically trained for Persian with others.

</details>


### [133] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了BESPOKE基准，用于系统性评估检索增强大语言模型（search-augmented LLMs）的个性化能力。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强大语言模型在信息查询任务中常规集成了检索与生成，提升了用户体验，但在满足不同用户多样化需求、意图识别和结果个性化表达方面仍有不足。现有系统虽尝试通过用户历史实现个性化，但系统性评估手段尚未完善。

Method: 提出并构建了BESPOKE基准，收集真实人类的对话和检索历史，配以细粒度的偏好评分和反馈。各项数据均由深度参与式人类注释流程获得，包括贡献历史、编写查询和评估响应。

Result: 通过BESPOKE进行系统分析，总结出个性化信息查询任务中的关键需求，展示了该基准对细致评测个性化检索增强大模型的有效性。

Conclusion: BESPOKE为未来个性化信息查询系统的评估提供了真实、细致且可操作的基础，有助于改进与评判个性化检索增强大语言模型的能力。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [134] [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
*Junhyuk Choi,Ro-hoon Oh,Jihwan Seol,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文提出了VoiceBBQ数据集，作为BBQ偏见测试集的语音扩展，专门用于检测和评估语音模型中的社会偏见，包括内容及声学两个层面的偏见。作者用该数据集对当前主流语音大模型进行评估，对比了不同模型在内容和声学偏见上的表现。


<details>
  <summary>Details</summary>
Motivation: 文本类偏见评估工具已经比较成熟，但随着语音大模型的发展，语言模型在语音领域存在独特的声学相关偏见。因此需要一个能够同时度量内容和声学偏见的基准数据集，从而更全面地评估和改进语音模型。

Method: 将BBQ数据集中的所有测试场景转换为可控的语音条件，并引入可量化的每种偏见轴的准确率、偏见度和一致性指标，这些都可以与原始文本基准进行对比。利用VoiceBBQ，作者对LLaMA-Omni和Qwen2-Audio两种语音模型进行了偏见分析，比较了两者在内容和声学两个层面的偏见表现。

Result: LLaMA-Omni在声学偏见上表现出抗性，但会放大性别和口音方面的偏见；Qwen2-Audio则能有效弱化这些声学提示，同时保持内容信息的准确性。

Conclusion: VoiceBBQ为语音大模型的内容和声学偏见分析提供了紧凑易用的基准平台，有助于后续对不同模型的多维度偏见联动分析和改进。

Abstract: We introduce VoiceBBQ, a spoken extension of the BBQ (Bias Benchmark for
Question Answering) - a dataset that measures social bias by presenting
ambiguous or disambiguated contexts followed by questions that may elicit
stereotypical responses. Due to the nature of speech, social bias in Spoken
Language Models (SLMs) can emerge from two distinct sources: 1) content aspect
and 2) acoustic aspect. The dataset converts every BBQ context into controlled
voice conditions, enabling per-axis accuracy, bias, and consistency scores that
remain comparable to the original text benchmark. Using VoiceBBQ, we evaluate
two SLMs - LLaMA-Omni and Qwen2-Audio - and observe architectural contrasts:
LLaMA-Omni resists acoustic bias while amplifying gender and accent bias,
whereas Qwen2-Audio substantially dampens these cues while preserving content
fidelity. VoiceBBQ thus provides a compact, drop-in testbed for jointly
diagnosing content and acoustic bias across spoken language models.

</details>


### [135] [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
*Junhyuk Choi,Jihwan Seol,Nayeon Kim,Chanhee Cho,EunBin Cho,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文发现当前的语音感知语言模型（SpeechLMs）在性别相关问题上处理存在悖论，不仅未能有效消除偏见，还在不同语义场景中表现不当。


<details>
  <summary>Details</summary>
Motivation: 随着语音感知语言模型广泛应用于人机交互，性别偏见问题引起关注。本文旨在系统分析SpeechLM在基于性别的问答中表现，评价其公平性和语境适应性。

Method: 作者构建了包含9,208个语音样本的新数据集，分为性别无关、性别刻板印象、性别相关三类，系统测试LLaMA-Omni等模型及语音编码器，并对性别中和方法和回答倾向展开对比实验分析。

Result: 研究发现，模型在性别刻板印象问题上以男性为主导，在需要性别相关性的场景中却表现为不区分性别。此外，即便采取语音中和或允许中性回答，偏向仍难以消除，且主要源于Whisper语音编码器产生男性倾向的声学特征。

Conclusion: 现有SpeechLM在追求公平性时未能合理利用语境性别信息，导致未能真正消除性别偏见。未来需研发更精细的技术，实现对性别信息恰当且公平的处理。

Abstract: Speech-aware Language Models (SpeechLMs) have fundamentally transformed
human-AI interaction by enabling voice-based communication, yet they may
exhibit acoustic-based gender differentiation where identical questions lead to
different responses based on the speaker's gender. This paper propose a new
dataset that enables systematic analysis of this phenomenon, containing 9,208
speech samples across three categories: Gender-Independent,
Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni
series and discovered a paradoxical pattern; while overall responses seems
identical regardless of gender, the pattern is far from unbiased responses.
Specifically, in Gender-Stereotypical questions, all models consistently
exhibited male-oriented responses; meanwhile, in Gender-Dependent questions
where gender differentiation would be contextually appropriate, models
exhibited responses independent to gender instead. We also confirm that this
pattern does not result from neutral options nor perceived gender of a voice.
When we allow neutral response, models tends to respond neutrally also in
Gender-Dependent questions. The paradoxical pattern yet retains when we applied
gender neutralization methods on speech. Through comparison between SpeechLMs
with corresponding backbone LLMs, we confirmed that these paradoxical patterns
primarily stem from Whisper speech encoders, which generates male-oriented
acoustic tokens. These findings reveal that current SpeechLMs may not
successfully remove gender biases though they prioritized general fairness
principles over contextual appropriateness, highlighting the need for more
sophisticated techniques to utilize gender information properly in speech
technology.

</details>


### [136] [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
*Ilya Alekseev,Roman Solomatin,Darina Rustamova,Denis Kuznetsov*

Main category: cs.CL

TL;DR: AutoIntent是一个面向文本分类的自动化机器学习工具，强调端到端流程和优于现有AutoML工具的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动化机器学习（AutoML）工具在文本分类任务上存在流程不完整、灵活性差的问题，且支持多标签分类与离群检测的能力有限。作者旨在解决这些痛点。

Method: AutoIntent集成了embedding模型选择、分类器优化、决策阈值调优，并实现了模块化、类似sklearn接口的端到端自动化流程。该工具同时支持多标签分类和超出范围（out-of-scope）检测。

Result: 在标准意图分类数据集上，AutoIntent相较现有AutoML工具表现出更优的性能，并能在效果与资源消耗间灵活权衡。

Conclusion: AutoIntent是一款高性能且用户友好的AutoML工具，专注于文本分类任务，特别适合需要多标签与out-of-scope检测的场景，推动了AutoML在自然语言处理领域的应用。

Abstract: AutoIntent is an automated machine learning tool for text classification
tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with
embedding model selection, classifier optimization, and decision threshold
tuning, all within a modular, sklearn-like interface. The framework is designed
to support multi-label classification and out-of-scope detection. AutoIntent
demonstrates superior performance compared to existing AutoML tools on standard
intent classification datasets and enables users to balance effectiveness and
resource consumption.

</details>


### [137] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出了一种新的多模态关系抽取方法，将问题从分类转为检索，利用对比学习和大语言模型生成关系描述，在主流数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态关系抽取方法大多基于分类，将关系视为离散标签，存在无法利用结构性约束、表意性不足等局限，难以实现细粒度和更具解释性的关系理解。

Method: 提出ROC框架，将多模态关系抽取视为由关系语义驱动的检索任务。该方法用多模态编码器融合实体类型和位置信息，运用大语言模型将关系标签扩展为自然语言描述，通过基于语义相似度的对比学习对齐实体-关系对。

Result: 在MNRE和MORE两个基准数据集上，ROC方法获得了最新的最优性能，并表现出更强的健壮性和可解释性。

Conclusion: ROC有效弥补了传统分类范式的缺陷，实现了更细粒度、更具语义表达能力的多模态关系抽取，是现有方法的有力提升。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [138] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 该论文发现大型语言模型（LLM）训练时，语法模板和领域知识之间可能形成虚假相关，这些相关性会影响模型对指令的理解和作答表现，甚至可能成为安全风险。


<details>
  <summary>Details</summary>
Motivation: 尽管语义和领域知识被广泛认为是LLM正确响应指令所必需的，但作者发现训练语料中频繁出现的句法模板也可能携带隐含信息，从而影响模型输入的解释和输出。这种语法与领域之间的虚假相关尚未被充分研究。

Method: 作者分析了任务-指令对中的句法模板、领域和语义，并利用合成数据集系统性地探究了句法-领域虚假相关对于知识实体类任务表现的影响，同时提出了一套评测框架来检测模型训练后的这类相关性。此外，用多个主流开源和闭源大模型以及安全微调为案例进行了实证分析。

Result: 实验发现，在OLMo-2等模型上，句法-领域相关的现象确实存在并降低了知识任务表现（平均分下降到0.51）；这种相关性在FlanV2数据集子集和主流开闭源模型中均可检测到。同时，安全微调模型也可能被这种相关性绕过策略攻击。

Conclusion: 作者建议在LLM开发中需显式检测语法-领域相关性，并确保各领域训练语料具有足够的句法多样性，以防止此类虚假相关给模型性能和安全性带来负面影响。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [139] [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文综述了计算幽默领域，强调幽默生成和解释任务的挑战，并指出当前大语言模型（LLMs）在理解和生成幽默方面仍有限，未来需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 幽默是人类基本特质，也是极具挑战性的自然语言处理任务。研究计算幽默有助于评估现代大模型的常识和推理能力，并推动NLP的发展。

Method: 通过文献调研，总结了当前在幽默生成和解释（尤其超越双关语）的研究进展，分析现有模型的能力和不足。同时讨论了幽默作为NLP子领域的重要性。

Result: 文献表明，当前在生成和解释幽默方面的研究仍不充分，尤其是在双关语之外，SOTA模型与人类仍有差距。

Conclusion: 计算幽默研究在NLP中具有重要地位，但仍面临理解、生成、判断主观性和伦理性的挑战。未来需加强幽默多样性和深层机制的深入探索。

Abstract: The creation and perception of humour is a fundamental human trait,
positioning its computational understanding as one of the most challenging
tasks in natural language processing (NLP). As an abstract, creative, and
frequently context-dependent construct, humour requires extensive reasoning to
understand and create, making it a pertinent task for assessing the
common-sense knowledge and reasoning abilities of modern large language models
(LLMs). In this work, we survey the landscape of computational humour as it
pertains to the generative tasks of creation and explanation. We observe that,
despite the task of understanding humour bearing all the hallmarks of a
foundational NLP task, work on generating and explaining humour beyond puns
remains sparse, while state-of-the-art models continue to fall short of human
capabilities. We bookend our literature survey by motivating the importance of
computational humour processing as a subdiscipline of NLP and presenting an
extensive discussion of future directions for research in the area that takes
into account the subjective and ethically ambiguous nature of humour.

</details>


### [140] [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
*Jieli Zhu,Vi Ngoc-Nha Tran*

Main category: cs.CL

TL;DR: 本研究关注小语言模型（SLM）在下游任务中的个人身份信息（PII）泄露问题，并提出了一种新的PII提取方法GEP，实验结果显示其PII泄露检测能力远超以往方法。


<details>
  <summary>Details</summary>
Motivation: 尽管SLM在能效和性能之间取得了不错的平衡，但其在PII泄露方面的风险尚未被充分研究，尤其是在医疗领域下游任务中。

Method: 作者首先基于BioGPT微调并训练了ChatBioGPT聊天机器人，并与现有ChatDoctor及ChatGPT进行性能对比。为了检测PII泄露，作者发现已有基于模板的方法在SLM上效果有限，因此提出基于贪心坐标梯度的PII提取方法GEP，并针对复杂情形（如插入不同语法表达的PII）进行了拓展实验。

Result: GEP方法比传统模板方法多检测出最高60倍的泄露情况，在PII以自由形式插入的场景下，泄露率仍高达4.53%。

Conclusion: SLM虽然能效高，但在PII泄露风险上不可忽视。传统模板方法难以有效检测，GEP方法能够更精准地揭示泄露情况，提示在SLM应用中需加强隐私保护措施。

Abstract: Small language models (SLMs) become unprecedentedly appealing due to their
approximately equivalent performance compared to large language models (LLMs)
in certain fields with less energy and time consumption during training and
inference. However, the personally identifiable information (PII) leakage of
SLMs for downstream tasks has yet to be explored. In this study, we investigate
the PII leakage of the chatbot based on SLM. We first finetune a new chatbot,
i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca
and HealthCareMagic. It shows a matchable performance in BERTscore compared
with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove
that the previous template-based PII attacking methods cannot effectively
extract the PII in the dataset for leakage detection under the SLM condition.
We then propose GEP, which is a greedy coordinate gradient-based (GCG) method
specifically designed for PII extraction. We conduct experimental studies of
GEP and the results show an increment of up to 60$\times$ more leakage compared
with the previous template-based methods. We further expand the capability of
GEP in the case of a more complicated and realistic situation by conducting
free-style insertion where the inserted PII in the dataset is in the form of
various syntactic expressions instead of fixed templates, and GEP is still able
to reveal a PII leakage rate of up to 4.53%.

</details>


### [141] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 本文提出了一个结合隐式检索和结构化协作的大语言模型推理新框架，在科学推理任务上取得了显著超越以往方法的效果，同时大幅降低了推理过程中的token消耗和步骤。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在科学推理方面进步明显，但仍面临两个瓶颈：显式检索导致推理流程碎片化和“工具税”问题，以及多智能体平均融合稀释优解的问题。作者希望通过新的机制克服这些瓶颈，提高准确率并减少资源消耗。

Method: 作者设计了一个统一框架，基础层采用Monitor-based隐式检索模块，将外部知识以最小干扰嵌入推理流程；上层通过层次化解题精修（HSR），让解答候选彼此修正，并通过质量感知的迭代推理（QAIR）根据答案质量调整修正过程。

Result: 在HLE Bio/Chem Gold数据集上，框架达到48.3%的准确率，超过最强基线13.4个百分点，领先前沿LLM最多18.1个百分点，同时token消耗和步骤数分别下降53.5%和43.7%。在SuperGPQA和TRQA数据集上亦表现出强健性。误差分析发现，85%以上的错误同时涉及推理和知识缺口，且多样性分析显示：检索类任务获益于解答多样性，推理类任务则偏好共识。

Conclusion: 作者证明了隐式知识增强与结构化精修能够显著克服显式工具调用和均一融合导致的低效问题，推动科学推理效果与效率全面跃进。

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [142] [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
*Xinzhe Xu,Liang Zhao,Hongshen Xu,Chen Chen*

Main category: cs.CL

TL;DR: 本文提出了CLaw——一个专为评估大语言模型（LLMs）在中国法律知识及推理应用上的能力而设计的新基准，并表明现有LLMs在准确检索和引用法律条文方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在法律文本分析和法条引用任务中经常因泛化预训练而缺乏法律专业深度，难以满足法律领域高度准确与可信的需求。

Method: 作者构建了CLaw基准，包括两部分：1）涵盖全部306部中国现行国家法律，精细到小节并加入历史修订信息的条文语料（64849条）；2）来自最高法权威材料、基于案例推理的254道测题，用来考查LLMs法律知识的实际应用能力。

Result: 实验发现，当前主流LLMs在复述法律条文上普遍表现较差，这直接影响后续的法律推理与输出的可靠性。

Conclusion: 作者认为，实现法律领域的可信推理需结合高准确度的知识检索（如SFT、RAG）与较强的通用推理能力。CLaw为法律领域模型研究和评测提供了重要基准和启示。

Abstract: Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.

</details>


### [143] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出SGMem，一种基于句子级图结构的对话记忆管理方法，有效提升了长对话历史中的信息组织与检索能力，在长时对话问答任务中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 长期对话智能体因超出大模型上下文窗口而面临记忆管理挑战。现有的事实提取或摘要方法虽然减少了冗余，但难以在不同粒度上高效组织、检索相关信息，影响模型对上下文的理解与回答准确性。

Method: 作者提出SGMem，将对话按块切分，并在块内构建句子级图结构，捕捉轮次、回合、会话等不同层次的关联。SGMem结合检索到的原始对话与生成的摘要、事实、洞见等多元内容，统一输入大模型，以提供连贯且相关的上下文。

Result: 在LongMemEval和LoCoMo两个长时对话问答评测集上，SGMem持续提升了答案准确率，在各项指标上均优于强基线方法。

Conclusion: SGMem可有效提升大模型对长对话历史的记忆管理与信息检索能力，从而改善对话类任务中的生成质量与问答表现。

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [144] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: QCG-RAG是一种以查询为中心的图检索增强生成框架，解决了传统RAG方法在粒度选择上的困境，并在多跳推理任务中取得了更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的检索增强生成方法在实体级图和文档级图的粒度选择上存在矛盾——实体粒度带来高开销且缺乏上下文，文档粒度则捕捉不到细致关系。

Method: 提出QCG-RAG，利用Doc2Query等工具，根据查询构建可控粒度的查询中心图，并基于生成的查询进行多跳分块检索，提升图的质量和可解释性。

Result: 在LiHuaWorld和MultiHop-RAG数据集上，QCG-RAG在问答准确率方面稳定超越现有基于分块和图的RAG方法。

Conclusion: QCG-RAG为多跳推理任务开辟了新范式，有效提升了复杂问题的检索与推理能力。

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [145] [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
*Evgeny Kaskov,Elizaveta Petrova,Petr Surovtsev,Anna Kostikova,Ilya Mistiurin,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CL

TL;DR: 本文关注扩散模型在处理同形异义词（homonyms）时出现的同义词重复问题，提出了度量方法并验证了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像扩散模型中，当同形异义词出现在输入提示词时，模型往往会同时生成它的多重含义，导致生成结果混乱。另外，在非英语场景下，Anglocentric（英语中心）偏差会把本无同形异义词的词翻译成英文后成为同形异义词，进一步加剧这一问题，因此需要针对该问题提出度量和解决方法。

Method: 作者提出了一种度量同形异义词重复率的方法，并使用视觉-语言模型（Vision-Language Models, VLM）自动评测和人工评测结合，对不同扩散模型的表现进行评估。此外，提出了利用‘prompt expansion’（提示词扩展）技术来缓解同形异义词重复的问题。

Result: 实验证明，不同扩散模型在同形异义词重复问题上存在性能差异，自动和人工评测结果一致。此外，提示词扩展方法有效减少了同形异义词重复及因英译偏差引起的问题。

Conclusion: 同形异义词重复是文本到图像扩散模型中被忽视但影响显著的问题。作者方法能有效评估和缓解该问题，并公开了自动评测相关代码，有助于后续相关研究。

Abstract: Homonyms are words with identical spelling but distinct meanings, which pose
challenges for many generative models. When a homonym appears in a prompt,
diffusion models may generate multiple senses of the word simultaneously, which
is known as homonym duplication. This issue is further complicated by an
Anglocentric bias, which includes an additional translation step before the
text-to-image model pipeline. As a result, even words that are not homonymous
in the original language may become homonyms and lose their meaning after
translation into English. In this paper, we introduce a method for measuring
duplication rates and conduct evaluations of different diffusion models using
both automatic evaluation utilizing Vision-Language Models (VLM) and human
evaluation. Additionally, we investigate methods to mitigate the homonym
duplication problem through prompt expansion, demonstrating that this approach
also effectively reduces duplication related to Anglocentric bias. The code for
the automatic evaluation pipeline is publicly available.

</details>


### [146] [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
*Shomik Jain,Jack Lanchantin,Maximilian Nickel,Karen Ullrich,Ashia Wilson,Jamelle Watson-Daniels*

Main category: cs.CL

TL;DR: 本文关注大语言模型在不同任务中输出趋同（同质化）的问题，提出基于任务类型的多样性评估与采样方法，提升所需多样性的同时保障输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多种任务中输出内容趋于同质化，但对'同质化'的定义及其带来的问题因任务类型而异。以往研究缺乏对这一现象的任务依赖性分析和应对策略。

Method: 作者提出了八类任务组成的任务分类体系，并引入了'任务锚定功能多样性'的新评估方法。同时，提出了基于任务锚定的采样技术，有针对性地提升那些不需要同质化的任务的功能多样性，并在需要同质化的任务中维持一致性。

Result: 实验证明，新的评估及采样方法在提升功能多样性的同时，并未降低输出质量，挑战了多样性与质量存在权衡的传统观点。

Conclusion: 通过引入对任务依赖的输出同质化分析与方法，显著提升了大语言模型输出的多样性和针对性，为未来模型输出评估和控制提供了新方向。

Abstract: A large language model can be less helpful if it exhibits output response
homogenization. But whether two responses are considered homogeneous, and
whether such homogenization is problematic, both depend on the task category.
For instance, in objective math tasks, we often expect no variation in the
final answer but anticipate variation in the problem-solving strategy. Whereas,
for creative writing tasks, we may expect variation in key narrative components
(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity
produced by temperature-sampling. Previous work addressing output
homogenization often fails to conceptualize diversity in a task-dependent way.
We address this gap in the literature directly by making the following
contributions. (1) We present a task taxonomy comprised of eight task
categories that each have distinct conceptualizations of output homogenization.
(2) We introduce task-anchored functional diversity to better evaluate output
homogenization. (3) We propose a task-anchored sampling technique that
increases functional diversity for task categories where homogenization is
undesired, while preserving homogenization where it is desired. (4) We
challenge the perceived existence of a diversity-quality trade-off by
increasing functional diversity while maintaining response quality. Overall, we
demonstrate how task dependence improves the evaluation and mitigation of
output homogenization.

</details>


### [147] [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
*Irina Tolstykh,Aleksandra Tsybina,Sergey Yakubson,Maksim Kuprashevich*

Main category: cs.CL

TL;DR: 本文提出了一种新的大规模英俄双语AI生成文本检测数据集LLMTrace，并提供字符级标注，可支持文本级二分类与区段级检测任务。


<details>
  <summary>Details</summary>
Motivation: 目前AI生成文本检测领域缺乏高质量训练数据，现有数据集模型过时、语种单一（多为英文），且很少涵盖多人或混合AI-人类作者场景，更没有细粒度的字符级标注，不利于精确识别AI生成内容。

Method: 作者收集并构建了LLMTrace数据集，包含由多种现代主流商用与开源大语言模型生成的英文与俄文文本，且对混合人类-LLM共同创作的文本做了字符级详细标注，可支持二分类（整篇文本判定）和区段检测（具体区段来自AI与否）。

Result: LLMTrace成为第一个覆盖英俄双语、多模型、支持混合创作、带角色级标注的AI文本检测数据集，为相关研究提供了宝贵资源。

Conclusion: LLMTrace数据集将极大丰富和推动AI文本检测及其相关研究，为更精细实用的检测系统训练和评估提供有力支持。

Abstract: The widespread use of human-like text from Large Language Models (LLMs)
necessitates the development of robust detection systems. However, progress is
limited by a critical lack of suitable training data; existing datasets are
often generated with outdated models, are predominantly in English, and fail to
address the increasingly common scenario of mixed human-AI authorship.
Crucially, while some datasets address mixed authorship, none provide the
character-level annotations required for the precise localization of
AI-generated segments within a text. To address these gaps, we introduce
LLMTrace, a new large-scale, bilingual (English and Russian) corpus for
AI-generated text detection. Constructed using a diverse range of modern
proprietary and open-source LLMs, our dataset is designed to support two key
tasks: traditional full-text binary classification (human vs. AI) and the novel
task of AI-generated interval detection, facilitated by character-level
annotations. We believe LLMTrace will serve as a vital resource for training
and evaluating the next generation of more nuanced and practical AI detection
models. The project page is available at
\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.

</details>


### [148] [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文从理论上分析了输入扰动对Chain-of-Thought（CoT）输出波动的影响，给出了扰动影响的上界，并通过实验证明了理论的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT输出对输入扰动非常敏感，虽有各种提示优化方法去缓解，但对其机制缺乏系统的理论解释。缺乏理论指导影响了进一步深化理解和优化方法的提升。

Method: 作者推导了输入扰动下输出波动的上界，并分析该上界与推理步数的关系。还将结论应用于Linear Self-Attention（LSA）模型，证明该模型的扰动上界与输入嵌入和隐藏状态的范数负相关。最后在三个主流数据集和四个主流模型上通过实验证实理论分析。

Result: （i）扰动影响的上界与CoT推理步数正相关；（ii）哪怕推理过程无限长也不能完全消除输入扰动的影响；（iii）在LSA模型中，上界与输入与隐藏状态的范数负相关。实验证明上述理论分析与真实情况一致。

Conclusion: 该工作为理解和优化CoT类大模型遇到输入扰动时的鲁棒性提供了理论基础和实证支撑，有助于指导未来提示优化方法的改进。

Abstract: Existing research indicates that the output of Chain-of-Thought (CoT) is
significantly affected by input perturbations. Although many methods aim to
mitigate such impact by optimizing prompts, a theoretical explanation of how
these perturbations influence CoT outputs remains an open area of research.
This gap limits our in-depth understanding of how input perturbations propagate
during the reasoning process and hinders further improvements in prompt
optimization methods. Therefore, in this paper, we theoretically analyze the
effect of input perturbations on the fluctuation of CoT outputs. We first
derive an upper bound for input perturbations under the condition that the
output fluctuation is within an acceptable range, based on which we prove that:
(i) This upper bound is positively correlated with the number of reasoning
steps in the CoT; (ii) Even an infinitely long reasoning process cannot
eliminate the impact of input perturbations. We then apply these conclusions to
the Linear Self-Attention (LSA) model, which can be viewed as a simplified
version of the Transformer. For the LSA model, we prove that the upper bound
for input perturbation is negatively correlated with the norms of the input
embedding and hidden state vectors. To validate this theoretical analysis, we
conduct experiments on three mainstream datasets and four mainstream models.
The experimental results align with our theoretical analysis, empirically
demonstrating the correctness of our findings.

</details>


### [149] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: DisCoCLIP是结合视觉-语言信息的新模型，通过引入张量网络显式编码语言句法结构，显著提升了对语言成分顺序和语义理解的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型如CLIP虽然擅长大规模图文对齐，但往往忽视语言的组合结构，导致在需要精确理解词序和句法关系的任务上表现较差。因此，需要探索方法使模型更好地捕捉和利用语言中的组合和句法信息。

Method: DisCoCLIP采用冻结的CLIP视觉Transformer作为视觉编码器，并用新颖的张量网络文本编码器显式表达句法结构。具体为，将句子由组配范畴文法（CCG）解析，生成分布式词张量，通过张量收缩反映句子的语法推导。高阶张量通过张量分解进行参数量化，从而提升效率并减少参数总量。模型通过自监督对比损失端到端训练。

Result: DisCoCLIP在多项基准测试中提升显著：SVO-Probes的动词准确率由77.6%提升至82.4%，ARO归因及关系分数分别提升9%和4%，在新提出的SVO-Swap测试上达到93.7%的高分。

Conclusion: 在视觉-语言任务中，通过张量网络嵌入显式语言句法结构方法，不仅提升了推理的可解释性和参数效率，还极大改善了对复杂语言结构的理解和组合推理能力。

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


### [150] [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
*Pranjal A. Chitale,Varun Gumma,Sanchit Ahuja,Prashant Kodali,Manan Uppadhyay,Deepthi Sudharsan,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 本文提出并验证了一种针对印度多语言场景的文化本地化合成数据生成方法，显著提升了低资源语言AI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在跨语言、文化本地化能力尤其是在低资源语种下依然有限。常用的合成数据多为高资源语言数据的翻译，缺乏对本地语境和文化的深度捕捉。

Method: 通过bottom-up策略，利用大型开源LLM（参数量>=235B），以各语种维基百科内容为基础提示模型生成“本地化”合成数据，并与高资源语言top-down翻译数据方式互补。提出了Updesh，包含13种印度语言、950万条高质量、指令跟随型合成数据，覆盖多种泛化任务，强调长上下文和多轮对话及本土文化贴合。

Result: 系统自动评测和1万条人工标注结果证明数据质量高，但人工评价也指出仍有改进空间。下游微调模型在15个多语种任务集上，生成类任务显著提升，选择类任务表现可观，且以低/中资源语种提升最明显，缩小与高资源语种差距。

Conclusion: 结果表明，成功的多语AI系统需多维度、文化上下文感知的数据策划与生成方法，单一高资源翻译路径不足。该方法为多语、文化本地化AI训练提供了新范式和实证支持。

Abstract: Developing AI systems that operate effectively across languages while
remaining culturally grounded is a long-standing challenge, particularly in
low-resource settings. Synthetic data provides a promising avenue, yet its
effectiveness in multilingual and multicultural contexts remains underexplored.
We investigate the creation and impact of synthetic, culturally contextualized
datasets for Indian languages through a bottom-up generation strategy that
prompts large open-source LLMs (>= 235B parameters) to ground data generation
in language-specific Wikipedia content. This approach complements the dominant
top-down paradigm of translating synthetic datasets from high-resource
languages such as English. We introduce Updesh, a high-quality large-scale
synthetic instruction-following dataset comprising 9.5M data points across 13
Indian languages, encompassing diverse reasoning and generative tasks with an
emphasis on long-context, multi-turn capabilities, and alignment with Indian
cultural contexts. A comprehensive evaluation incorporating both automated
metrics and human annotation across 10k assessments indicates that generated
data is high quality; though, human evaluation highlights areas for further
improvement. Additionally, we perform downstream evaluations by fine-tuning
models on our dataset and assessing the performance across 15 diverse
multilingual datasets. Models trained on Updesh consistently achieve
significant gains on generative tasks and remain competitive on multiple-choice
style NLU tasks. Notably, relative improvements are most pronounced in low and
medium-resource languages, narrowing their gap with high-resource languages.
These findings provide empirical evidence that effective multilingual AI
requires multi-faceted data curation and generation strategies that incorporate
context-aware, culturally grounded methodologies.

</details>


### [151] [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
*Daniel Vennemeyer,Phan Anh Duong,Tiffany Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文分析并区别了大型语言模型中的谄媚性行为，将其细分为“谄媚性认同”和“谄媚性赞美”，并证明它们与真实认同有本质差别。研究表明，这些行为在潜在空间中拥有独立的表示，可以分别调控。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM常被发现存在谄媚行为（如盲目赞同、奉承用户），但尚不清楚这些行为背后是否由同一个机制驱动。作者希望深入剖析谄媚的具体表现及其潜在机制，促进模型的可控性和可靠性研究。

Method: 作者将谄媚行为细分为“认同”和“赞美”，与真实认同进行对比，采用均值差分方向、激活叠加、子空间几何等方法，在不同模型和数据集上分析这些行为在潜在空间中的表征，并研究它们间的相互关系。

Result: 研究发现：（1）谄媚性认同、谄媚性赞美和真实认同在潜在空间中具有不同的线性方向；（2）每种行为都可单独放大或抑制而不影响其它行为；（3）这一结构在不同模型家族和规模下保持一致。

Conclusion: 结论认为，谄媚性行为在LLM中有各自独立、可控的表征结构，未来可针对性地进行干预和引导，提高模型质量及安全性。

Abstract: Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.

</details>


### [152] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方案RLBFF，结合了人类灵活反馈和规则化的验证机制，在奖励建模中实现了精准与多样化的平衡，并在多个基准测试中取得领先性能，且支持个性化原则指定。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调方法RLHF可捕获人类偏好但解释性和抗奖励攻击能力欠佳，RLVR则局限于可验证原则，通用性不足。亟需兼具灵活性、解释性和细粒度控制的新方式。

Method: RLBFF从自然语言反馈中抽取可二值判断的原则（如准确性/可读性），将这些原则作为entailment任务用于奖励模型训练，实现精细分解和定制化。训练后评估其与现有Bradley-Terry模型和主流基准的对比表现，并发布了对齐Qwen3-32B的开源方案。

Result: RLBFF奖励模型在相同数据条件下显著优于Bradley-Terry模型，在RM-Bench、JudgeBench等权威评测中达到并列或领先地位（RM-Bench 86.2%、JudgeBench 81.4%、榜单首位）。

Conclusion: RLBFF通过灵活结合人类反馈和规则验证，实现了个性化和高性能的奖励建模，为大模型对齐提供了更强、开放、经济高效的训练方式。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [153] [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
*Yizhou Wang,Chen Tang,Han Deng,Jiabei Xiao,Jiaqi Liu,Jianyu Wu,Jun Yao,Pengze Li,Encheng Su,Lintao Wang,Guohang Zhuang,Yuchen Ren,Ben Fei,Ming Hu,Xin Chen,Dongzhan Zhou,Junjun He,Xiangyu Yue,Zhenfei Yin,Jiamin Wu,Qihao Zheng,Yuhao Zhou,Huihui Xu,Chenglong Ma,Yan Lu,Wenlong Zhang,Chunfeng Song,Philip Torr,Shixiang Tang,Xinzhu Ma,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 本文提出了一种用于科学推理的基础模型，通过对2060亿token的多样化科学数据进行预训练，并结合指令微调(SFT)、长链思维引导、强化学习等方法，使其能对多种科学任务进行高效推理和表达。该模型在不同学科和任务间具备广泛适应能力，并开源了模型和相关数据代码。


<details>
  <summary>Details</summary>
Motivation: 科学研究涉及多种数据类型和表述方式，传统专业系统往往只能覆盖有限任务且缺乏泛化能力，难以支持跨学科、跨任务的科学推理与表达。因此，亟需一种能处理多样科学表示的通用语言推理模型。

Method: 本工作构建了大规模科学语料库，覆盖科学文本、原始序列及二者配对，通过预训练获得多模态理解能力。进一步利用指令微调（SFT）和包含长链推理的冷启动策略，并通过强化学习增强模型真实、细致的科学推理能力。模型能处理文本与科学格式转换、知识抽取、属性预测与分类、序列生成等多任务。

Result: 模型实现了覆盖多达103项科学流程任务的高性能。对比专业系统，模型指令覆盖面更广、跨领域泛化更好、输出更真实可靠。多学科学习策略提升了迁移能力和下游任务表现。

Conclusion: 该工作展示了通过大规模预训练与精细调优，可建立能够泛化执行多种科学任务的推理模型，为科学研究和实际应用提供了强有力的通用工具。相关成果已全部开源，促进学界进一步研究与应用。

Abstract: We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [154] [Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation](https://arxiv.org/abs/2509.20486)
*Sven Ochs,Philip Schörner,Marc René Zofka,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本论文提出将摄像头的语义分割信息与LiDAR（激光雷达）分割结果相结合，通过将LiDAR点投射到摄像头的语义分割空间，提升自动驾驶系统中基于LiDAR定位的精度和鲁棒性。方法在多种传感器配置下获得验证，并在真实城市环境下进行了大规模测试。


<details>
  <summary>Details</summary>
Motivation: LiDAR数据的语义分割在面对不同类型和配置的传感器时存在较大挑战。由于语义信息对自动驾驶定位系统的准确性和鲁棒性有显著提升作用，如何将多源感知的语义信息有效结合，成为亟需解决的问题。

Method: 方法通过将LiDAR点云映射到摄像头的语义分割空间，融合摄像头（采用Depth-Anything网络）和激光雷达（自适应分割网络）的分割信息，提升LiDAR定位的表现。实验依托CoCar NextGen平台，融合GNSS+RTK技术获取高精度定位基准，并在不同道路环境下（共计55公里）进行测试。

Result: 该方法在包含城市、多车道道路和乡村公路的多样化测试环境下进行了大规模验证，结果表明所提多模态方法能显著提升自动驾驶系统中LiDAR定位的精度和可靠性。

Conclusion: 将摄像头的语义分割结果与LiDAR点云融合，能够提升自动驾驶系统LiDAR定位的性能，是实现复杂真实环境下高精度、多场景导航的有效途径。

Abstract: Semantic segmentation of LiDAR data presents considerable challenges,
particularly when dealing with diverse sensor types and configurations.
However, incorporating semantic information can significantly enhance the
accuracy and robustness of LiDAR-based localization techniques for autonomous
mobile systems. We propose an approach that integrates semantic camera data
with LiDAR segmentation to address this challenge. By projecting LiDAR points
into the semantic segmentation space of the camera, our method enhances the
precision and reliability of the LiDAR-based localization pipeline.
  For validation, we utilize the CoCar NextGen platform from the FZI Research
Center for Information Technology, which offers diverse sensor modalities and
configurations. The sensor setup of CoCar NextGen enables a thorough analysis
of different sensor types. Our evaluation leverages the state-of-the-art
Depth-Anything network for camera image segmentation and an adaptive
segmentation network for LiDAR segmentation. To establish a reliable ground
truth for LiDAR-based localization, we make us of a Global Navigation Satellite
System (GNSS) solution with Real-Time Kinematic corrections (RTK).
Additionally, we conduct an extensive 55 km drive through the city of
Karlsruhe, Germany, covering a variety of environments, including urban areas,
multi-lane roads, and rural highways. This multimodal approach paves the way
for more reliable and precise autonomous navigation systems, particularly in
complex real-world environments.

</details>


### [155] [Revisiting Formal Methods for Autonomous Robots: A Structured Survey](https://arxiv.org/abs/2509.20488)
*Atef Azaiez,David A. Anisi,Marie Farrell,Matt Luckcuck*

Main category: cs.RO

TL;DR: 本文系统性回顾了形式化方法在机器人自主系统中的应用现状与发展趋势，识别出持续和新兴的研究趋势。


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主系统（RAS）中人工智能应用的增加，确保其安全性和可靠性变得尤为重要。形式化方法（FM）作为规约和验证复杂系统的重要手段，其在RAS领域应用的全面梳理尚缺，本文旨在归纳这些研究并发现新趋势。

Method: 作者采用结构化文献综述方法，明确选择数据库、搜索词、过滤规则及协作式评审流程，对已检索到的相关论文进行系统筛选及归类，统计各种FM方法及形式化技术在RAS中的具体应用情况，并分析FM与子符号AI系统结合的演变。

Result: 结果表明，RAS领域内FM相关研究保持部分延续性趋势，同时出现了新变化，特别是在形式化综合（Formal Synthesis）与概率验证（Probabilistic Verification）等方法采用上的显著增长。

Conclusion: 整体上，FM在RAS领域的研究逐步成熟，不仅保留了既有发展态势，也不断吸收创新方法，提升了对新型AI赋能机器人系统的适用性和有效性。

Abstract: This paper presents the initial results from our structured literature review
on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We
describe our structured survey methodology; including database selection and
associated search strings, search filters and collaborative review of
identified papers. We categorise and enumerate the FM approaches and formalisms
that have been used for specification and verification of RAS. We investigate
FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of
how FM is used over time in this field. This work complements a pre-existing
survey in this area and we examine how this research area has matured over
time. Specifically, our survey demonstrates that some trends have persisted as
observed in a previous survey. Additionally, it recognized new trends that were
not considered previously including a noticeable increase in adopting Formal
Synthesis approaches as well as Probabilistic Verification Techniques.

</details>


### [156] [Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/abs/2509.20499)
*Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: 本文提出了一种零样本视觉-语言导航（VLN）方法，通过整合简化的路径点预测器与多模态大语言模型，实现了在复杂连续环境下的高效导航，并在相关数据集上取得了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型与机器人技术的快速发展，如何让具身智能体能够根据视觉与自然语言指令自主演化（VLN）成为重要且具有实际意义的任务，尤其在需要智能体连续感知、理解并规划动作的高级场景中，现有方法仍有很大提升空间。

Method: 作者提出了一种零样本框架：利用抽象障碍物地图上的简化路径点预测器，生成可达路径点，并将其与动态更新的拓扑图和访问记录整合。系统将空间结构与探索历史进行编码，作为prompt输入多模态大语言模型，从而在推理与路径修正中结合局部路径规划与探索策略。

Result: 在R2R-CE与RxR-CE两个VLN基准上，该方法的零样本导航成功率分别达到41%和36%，优于所有现有方法，表现出色。

Conclusion: 所提方法有效促进了VLN零样本导航能力，解决了感知、理解和动作规划的协同难题，同时为多模态大模型在复杂实体世界任务中的应用提供了新的思路。

Abstract: With the rapid progress of foundation models and robotics, vision-language
navigation (VLN) has emerged as a key task for embodied agents with broad
practical applications. We address VLN in continuous environments, a
particularly challenging setting where an agent must jointly interpret natural
language instructions, perceive its surroundings, and plan low-level actions.
We propose a zero-shot framework that integrates a simplified yet effective
waypoint predictor with a multimodal large language model (MLLM). The predictor
operates on an abstract obstacle map, producing linearly reachable waypoints,
which are incorporated into a dynamically updated topological graph with
explicit visitation records. The graph and visitation information are encoded
into the prompt, enabling reasoning over both spatial structure and exploration
history to encourage exploration and equip MLLM with local path planning for
error correction. Extensive experiments on R2R-CE and RxR-CE show that our
method achieves state-of-the-art zero-shot performance, with success rates of
41% and 36%, respectively, outperforming prior state-of-the-art methods.

</details>


### [157] [MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors](https://arxiv.org/abs/2509.20510)
*Petr Trunin,Diana Cafiso,Anderson Brazil Nardin,Trevor Exley,Lucia Beccai*

Main category: cs.RO

TL;DR: 本论文受非洲象鼻末端结构启发，提出了一种集成光学传感器的单体柔性机械手—MELEGROS，实现了结构、驱动与感知的无缝一体化。该机械手采用单一柔性材料与3D打印一体成型，集成气动腔与光波导传感器，具备多功能抓取和灵敏感知能力。


<details>
  <summary>Details</summary>
Motivation: 自然界中的象鼻集成了复杂的结构、驱动和感知能力，是天然的灵巧操作器。但现有软体机械手大多采用多材料或腱驱动结构，导致传感器与本体机械性能不匹配，难以实现高效感知和控制。本研究旨在突破这一限制，开发结构、驱动和传感全一体化的新型机械手。

Method: 设计了一种灵感来源于非洲象鼻的机械手MELEGROS，采用单一柔性树脂，通过连续3D打印工艺一体成型。机械手内集成5个气动腔用于驱动，6根光波导传感器用于力/形变感知。结构为气动格栅状（12.5mm单元），传感器和驱动腔都嵌于同一材料内，无机械性能错配。仿真辅助设计与传感器布置，仅4轮迭代优化出最终原型。

Result: MELEGROS实现了伸长、压缩、弯曲等柔性形态，以及独立的触觉和本体觉信号解耦。该机械手（132g）能举起超过自身两倍重量，完成捏、舀、抓取等生物启发动作，能灵巧抓握葡萄等易碎物。集成光学传感器对触碰、弯曲及腔变形提供独立响应，赋予多功能感知能力。

Conclusion: MELEGROS展示了一种全新软体机器人范式，实现了结构连续、传感与驱动深度融合。该方法避免了多材料集成的力学矛盾，极大简化了加工过程并提升了感知精度，为实现多功能、生物启发操作的未来机器人奠定基础。

Abstract: The elephant trunk exemplifies a natural gripper where structure, actuation,
and sensing are seamlessly integrated. Inspired by the distal morphology of the
African elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired
GRipper with Optical Sensors, emphasizing sensing as an intrinsic,
co-fabricated capability. Unlike multi-material or tendon-based approaches,
MELEGROS directly integrates six optical waveguide sensors and five pneumatic
chambers into a pneumatically actuated lattice structure (12.5 mm cell size)
using a single soft resin and one continuous 3D print. This eliminates
mechanical mismatches between sensors, actuators, and body, reducing model
uncertainty and enabling simulation-guided sensor design and placement. Only
four iterations were required to achieve the final prototype, which features a
continuous structure capable of elongation, compression, and bending while
decoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than
twice its weight, performs bioinspired actions such as pinching, scooping, and
reaching, and delicately grasps fragile items like grapes. The integrated
optical sensors provide distinct responses to touch, bending, and chamber
deformation, enabling multifunctional perception. MELEGROS demonstrates a new
paradigm for soft robotics where fully embedded sensing and continuous
structures inherently support versatile, bioinspired manipulation.

</details>


### [158] [Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation](https://arxiv.org/abs/2509.20516)
*Prasanna Sriganesh,Barath Satheeshkumar,Anushree Sabnis,Matthew Travers*

Main category: cs.RO

TL;DR: 该论文提出了一种用于四足机器人在复杂环境中自主清理障碍的新方法，强调感知与动作的闭环协同，有效提升了机器人推动障碍物时的任务成功率和目标追踪精度。


<details>
  <summary>Details</summary>
Motivation: 在密集复杂环境（如杂物堆积的楼梯）中，机器人需自主决策并与障碍物互动（如用一条腿推动），但此类物理动作会产生新约束，尤其是机器人在推动时障碍物会遮挡传感器，导致状态估计困难。

Method: 作者提出了一种紧密耦合的感知-动作框架，通过机器人自身的本体感受（如腿部接触和位置反馈）进行状态估计，预测被推动物体在被遮挡期间的位置变化，并指导感知系统在交互结束后重新检测对象，实现动作与感知的闭环。此外，系统还能通过推动结果判断物体不可移动。

Result: 在波士顿动力Spot机器人上的实验表明，该方法在楼梯推动障碍物任务中，比传统开环方法表现出更高的任务成功率与追踪精度。

Conclusion: 互动感知闭环框架提高了机器人在复杂场景中清障任务的鲁棒性和效率，实现了准确跟踪和有效重新检测障碍物，对推进机器人自主能力有重要提升。

Abstract: For robots to operate autonomously in densely cluttered environments, they
must reason about and potentially physically interact with obstacles to clear a
path. Safely clearing a path on challenging terrain, such as a cluttered
staircase, requires controlled interaction. For example, a quadrupedal robot
that pushes objects out of the way with one leg while maintaining a stable
stance with its three other legs. However, tightly coupled physical actions,
such as one-legged pushing, create new constraints on the system that can be
difficult to predict at design time. In this work, we present a new method that
addresses one such constraint, wherein the object being pushed by a quadrupedal
robot with one of its legs becomes occluded from the robot's sensors during
manipulation. To address this challenge, we present a tightly coupled
perception-action framework that enables the robot to perceive clutter, reason
about feasible push paths, and execute the clearing maneuver. Our core
contribution is an interaction-aware state estimation loop that uses
proprioceptive feedback regarding foot contact and leg position to predict an
object's displacement during the occlusion. This prediction guides the
perception system to robustly re-detect the object after the interaction,
closing the loop between action and sensing to enable accurate tracking even
after partial pushes. Using this feedback allows the robot to learn from
physical outcomes, reclassifying an object as immovable if a push fails due to
it being too heavy. We present results of implementing our approach on a Boston
Dynamics Spot robot that show our interaction-aware approach achieves higher
task success rates and tracking accuracy in pushing objects on stairs compared
to open-loop baselines.

</details>


### [159] [Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2509.20541)
*Anujith Muraleedharan,Anamika J H*

Main category: cs.RO

TL;DR: 论文提出了一种名为SPARQ的进展感知查询策略，仅在机器人学习停滞或变差时请求人类反馈，大幅减少了对专家反馈的需求，同时保证了学习效果。


<details>
  <summary>Details</summary>
Motivation: 目前大多数人类参与强化学习（HiL-RL）假设可以无限获得人类反馈，然而实际机器人部署时，反馈昂贵且有限，因此需要一种高效利用人类反馈的方法。

Method: SPARQ策略会监控机器人学习进展，仅在学习无明显进步或退步时才请求人类反馈。作者在PyBullet模拟环境下的UR5抓取立方体任务中，对比了无反馈、随机请求反馈、持续请求反馈和SPARQ策略。

Result: SPARQ在反馈预算节省一半的前提下，实现了与持续请求反馈几乎相同的高任务成功率，且比无反馈和随机请求反馈表现更稳定和高效。

Conclusion: 基于进展感知的选择性查询能大大提升人机协作式强化学习在受限人类反馈资源下的效率和可扩展性。

Abstract: Human feedback can greatly accelerate robot learning, but in real-world
settings, such feedback is costly and limited. Existing human-in-the-loop
reinforcement learning (HiL-RL) methods often assume abundant feedback,
limiting their practicality for physical robot deployment. In this work, we
introduce SPARQ, a progress-aware query policy that requests feedback only when
learning stagnates or worsens, thereby reducing unnecessary oracle calls. We
evaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing
against three baselines: no feedback, random querying, and always querying. Our
experiments show that SPARQ achieves near-perfect task success, matching the
performance of always querying while consuming about half the feedback budget.
It also provides more stable and efficient learning than random querying, and
significantly improves over training without feedback. These findings suggest
that selective, progress-based query strategies can make HiL-RL more efficient
and scalable for robots operating under realistic human effort constraints.

</details>


### [160] [GraspFactory: A Large Object-Centric Grasping Dataset](https://arxiv.org/abs/2509.20550)
*Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta*

Main category: cs.RO

TL;DR: 本文提出了一个大规模的6-DoF机械手抓取数据集GraspFactory，以提升抓取模型对新奇物体的泛化能力，特别适用于数据密集型模型的训练。


<details>
  <summary>Details</summary>
Motivation: 工业自动化中，机器人需处理多种新颖物体，但现有抓取模型因数据集规模和多样性受限，难以泛化到大量实际物体。为解决这一问题，需要具备几何多样性的大规模抓取数据集。

Method: 作者构建了GraspFactory数据集，包含1.09亿条针对Franka Panda（14,690种物体）和Robotiq 2F-85（33,710种物体）抓取的6-DoF样本，并进行了仿真与现实环境中的模型训练和测试验证。

Result: 使用GraspFactory子集训练的抓取模型在仿真环境和真实世界中表现出良好的泛化能力，能够较好地适应新颖和多样的物体。数据集和相关工具已公开。

Conclusion: GraspFactory为抓取模型提供了丰富、几何多样的训练数据，有助于推动机器人抓取领域泛化能力的提升和应用落地。

Abstract: Robotic grasping is a crucial task in industrial automation, where robots are
increasingly expected to handle a wide range of objects. However, a significant
challenge arises when robot grasping models trained on limited datasets
encounter novel objects. In real-world environments such as warehouses or
manufacturing plants, the diversity of objects can be vast, and grasping models
need to generalize to this diversity. Training large, generalizable
robot-grasping models requires geometrically diverse datasets. In this paper,
we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps
collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85
grippers (with 33,710 objects). GraspFactory is designed for training
data-intensive models, and we demonstrate the generalization capabilities of
one such model trained on a subset of GraspFactory in both simulated and
real-world settings. The dataset and tools are made available for download at
https://graspfactory.github.io/.

</details>


### [161] [Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles](https://arxiv.org/abs/2509.20593)
*Song Ma,Richard Bucknall,Yuanchang Liu*

Main category: cs.RO

TL;DR: 提出一种结合高保真污染扩散模拟与USV智能路径规划的不确定性感知追踪方法，能高效准确定位海洋污染源。


<details>
  <summary>Details</summary>
Motivation: 海洋污染事件频发，污染源定位对环境应急响应至关重要。现有方法在不确定性处理和自动化方面存在局限，促使该研究提出更高效的源追踪方案。

Method: 结合高保真污染扩散仿真与信息量最大化路径规划，通过ROS处理实时传感器数据，动态更新污染源的概率位置估计，同时量化预测中的不确定性。

Result: 在模拟实验中，面对不同污染源、流场和初始位置，该系统都能高精度、高效率地定位污染源。

Conclusion: 提出的方法可靠高效，可为海洋污染事件提供全自动监测和快速响应技术，推动环境监测无人化发展。

Abstract: This paper proposes an uncertainty-aware marine pollution source tracking
framework for unmanned surface vehicles (USVs). By integrating high-fidelity
marine pollution dispersion simulation with informative path planning
techniques, we demonstrate effective identification of pollution sources in
marine environments. The proposed approach is implemented based on Robot
Operating System (ROS), processing real-time sensor data to update
probabilistic source location estimates. The system progressively refines the
estimation of source location while quantifying uncertainty levels in its
predictions. Experiments conducted in simulated environments with varying
source locations, flow conditions, and starting positions demonstrate the
framework's ability to localise pollution sources with high accuracy. Results
show that the proposed approach achieves reliable source localisation
efficiently. This work contributes to the development of full autonomous
environmental monitoring capabilities essential for rapid response to marine
pollution incidents.

</details>


### [162] [Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation](https://arxiv.org/abs/2509.20623)
*Satyajeet Das,Darren Chiu,Zhehui Huang,Lars Lindemann,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: 本文提出了一种推理时潜在激活编辑（LAE）框架，用于在不修改预训练策略权重或结构的前提下提升多无人机导航的安全性，实验证明能极大减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 目前基于强化学习的多无人机导航虽然进步显著，但仍容易在障碍物密集环境下发生碰撞，通过重训练修正这种安全问题成本高，而且容易破坏已学到的其他能力，因此需要更高效的策略后处理方法。

Method: 提出LAE框架，核心分为两步：1）在线分类器监控网络中间激活，识别潜在的不安全状态；2）激活编辑模块对被标记的激活做调整，引导策略转向更安全的行为。此外，通过训练一个预测碰撞相关激活的潜在世界模型，使策略对风险感知更加敏感，从而提前规避风险。

Result: 仿真与实际Crazyflie无人机实验显示LAE能显著减少碰撞次数，比未编辑前减少近90%，同时大幅提升无碰撞完成任务的轨迹比例，且不影响任务完成率。

Conclusion: LAE作为一种推理时、轻量且无需修改原始策略参数的方法，为已部署机器人的策略后精化提供了新范式，尤其适合资源受限硬件应用，具有广阔应用前景。

Abstract: Reinforcement learning has enabled significant progress in complex domains
such as coordinating and navigating multiple quadrotors. However, even
well-trained policies remain vulnerable to collisions in obstacle-rich
environments. Addressing these infrequent but critical safety failures through
retraining or fine-tuning is costly and risks degrading previously learned
skills. Inspired by activation steering in large language models and latent
editing in computer vision, we introduce a framework for inference-time Latent
Activation Editing (LAE) that refines the behavior of pre-trained policies
without modifying their weights or architecture. The framework operates in two
stages: (i) an online classifier monitors intermediate activations to detect
states associated with undesired behaviors, and (ii) an activation editing
module that selectively modifies flagged activations to shift the policy
towards safer regimes. In this work, we focus on improving safety in
multi-quadrotor navigation. We hypothesize that amplifying a policy's internal
perception of risk can induce safer behaviors. We instantiate this idea through
a latent collision world model trained to predict future pre-collision
activations, thereby prompting earlier and more cautious avoidance responses.
Extensive simulations and real-world Crazyflie experiments demonstrate that LAE
achieves statistically significant reduction in collisions (nearly 90% fewer
cumulative collisions compared to the unedited baseline) and substantially
increases the fraction of collision-free trajectories, while preserving task
completion. More broadly, our results establish LAE as a lightweight paradigm,
feasible on resource-constrained hardware, for post-deployment refinement of
learned robot policies.

</details>


### [163] [Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments](https://arxiv.org/abs/2509.20635)
*Matheus P. Angarola,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 本文提出了一种分层强化学习框架，通过地形专用策略和课程学习，在复杂地形下提升足式机器人在盲态行走时的敏捷性和跟踪表现，在仿真中取得显著优于通用策略的效果。


<details>
  <summary>Details</summary>
Motivation: 足式机器人在不同且复杂地形上需要具备强健敏捷的运动能力，尤其在没有地形感知信息的盲态行走环境下更具挑战，因此亟需提升其适应性和鲁棒性。

Method: 提出了分层强化学习框架，结合针对不同地形的专用运动策略以及课程化训练方法，以提升机器人在各种复杂地形（尤其是低摩擦和不连续地形）上的运动性能。

Result: 在仿真环境下，该方法在混合复杂地形上的任务成功率比通用策略提升最高可达16%，并且在速度目标增加时展现了更低的轨迹跟踪误差，尤其在低摩擦和不连续地形表现突出。

Conclusion: 本文方法比通用策略具有更优秀的适应性和鲁棒性，为足式机器人在未知复杂地形上的盲态运动提供了有效解决方案。

Abstract: Legged robots must exhibit robust and agile locomotion across diverse,
unstructured terrains, a challenge exacerbated under blind locomotion settings
where terrain information is unavailable. This work introduces a hierarchical
reinforcement learning framework that leverages terrain-specialized policies
and curriculum learning to enhance agility and tracking performance in complex
environments. We validated our method on simulation, where our approach
outperforms a generalist policy by up to 16% in success rate and achieves lower
tracking errors as the velocity target increases, particularly on low-friction
and discontinuous terrains, demonstrating superior adaptability and robustness
across mixed-terrain scenarios.

</details>


### [164] [Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation](https://arxiv.org/abs/2509.20646)
*Sun Zhaole,Xiaofeng Mao,Jihong Zhu,Yuanlong Zhang,Robert B. Fisher*

Main category: cs.RO

TL;DR: 本文提出了一种新型多指机械手SLeap Hand，通过吸盘实现更简单、稳定的精细操作，突破仿人手限制，能完成以往需两手操作的任务，并方便数据收集和学习。


<details>
  <summary>Details</summary>
Motivation: 传统仿人手的灵巧操作不仅容易受限于人手本身的能力，还导致数据收集困难和示范不稳定，阻碍了复杂操作任务的进步。

Method: 作者设计了集成吸盘的多指机械手（SLeap Hand），利用吸附代替复杂摩擦力-闭合，把精细操作简化为单点吸附，同时提升了远程操控和模仿演示的稳定性和数据质量。

Result: SLeap Hand能够实现如单手剪纸、手内书写等人手难以完成的操作，且远高于传统仿人手机械手的操作稳定性和任务多样性。

Conclusion: 打破仿人机械手思路，通过吸盘等新型机械结构，不仅拓展了机器手的操作能力，还极大降低了高质量数据收集门槛，为机器手操作学习和实际应用开辟了新路径。

Abstract: Dexterous in-hand manipulation remains a foundational challenge in robotics,
with progress often constrained by the prevailing paradigm of imitating the
human hand. This anthropomorphic approach creates two critical barriers: 1) it
limits robotic capabilities to tasks humans can already perform, and 2) it
makes data collection for learning-based methods exceedingly difficult. Both
challenges are caused by traditional force-closure which requires coordinating
complex, multi-point contacts based on friction, normal force, and gravity to
grasp an object. This makes teleoperated demonstrations unstable and amplifies
the sim-to-real gap for reinforcement learning. In this work, we propose a
paradigm shift: moving away from replicating human mechanics toward the design
of novel robotic embodiments. We introduce the \textbf{S}uction
\textbf{Leap}-Hand (SLeap Hand), a multi-fingered hand featuring integrated
fingertip suction cups that realize a new form of suction-enabled dexterity. By
replacing complex force-closure grasps with stable, single-point adhesion, our
design fundamentally simplifies in-hand teleoperation and facilitates the
collection of high-quality demonstration data. More importantly, this
suction-based embodiment unlocks a new class of dexterous skills that are
difficult or even impossible for the human hand, such as one-handed paper
cutting and in-hand writing. Our work demonstrates that by moving beyond
anthropomorphic constraints, novel embodiments can not only lower the barrier
for collecting robust manipulation data but also enable the stable,
single-handed completion of tasks that would typically require two human hands.
Our webpage is https://sites.google.com/view/sleaphand.

</details>


### [165] [Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills](https://arxiv.org/abs/2509.20653)
*Congkai Shen,Siyuan Yu,Yifan Weng,Haoran Ma,Chen Li,Hiroshi Yasuda,James Dallas,Michael Thompson,John Subosits,Tulga Ersal*

Main category: cs.RO

TL;DR: 本文提出了一种基于触觉共享控制的驾驶训练框架，通过人机合作帮助驾驶者掌握高性能驾驶技能，并在赛道驾驶中取得超越自学和全自动辅助的成效。


<details>
  <summary>Details</summary>
Motivation: 虽然共享控制能提升安全性和性能，但其对复杂、高难度任务下驾驶技能习得的影响尚缺乏研究。以往相关研究大多集中在简单任务或采用其他反馈方式。

Method: 设计了一个“虚拟赛道教练”框架：包括可与人类驾驶者协作的自动驾驶系统，以及根据驾驶者表现渐进式减少辅助的触觉共享控制机制。设置了自学和完全辅助两种基准进行对比实验。

Result: 人类受试实验显示，采用该框架训练的驾驶者在赛车技能和表现一致性方面均优于自学及全自动辅助组。

Conclusion: 基于触觉共享控制的辅助机制能够有效提升驾驶者掌握高难度驾驶技能的效率和水平，对于复杂驾驶训练具有重要应用价值。

Abstract: This study introduces a haptic shared control framework designed to teach
human drivers advanced driving skills. In this context, shared control refers
to a driving mode where the human driver collaborates with an autonomous
driving system to control the steering of a vehicle simultaneously. Advanced
driving skills are those necessary to safely push the vehicle to its handling
limits in high-performance driving such as racing and emergency obstacle
avoidance. Previous research has demonstrated the performance and safety
benefits of shared control schemes using both subjective and objective
evaluations. However, these schemes have not been assessed for their impact on
skill acquisition on complex and demanding tasks. Prior research on long-term
skill acquisition either applies haptic shared control to simple tasks or
employs other feedback methods like visual and auditory aids. To bridge this
gap, this study creates a cyber racing coach framework based on the haptic
shared control paradigm and evaluates its performance in helping human drivers
acquire high-performance driving skills. The framework introduces (1) an
autonomous driving system that is capable of cooperating with humans in a
highly performant driving scenario; and (2) a haptic shared control mechanism
along with a fading scheme to gradually reduce the steering assistance from
autonomy based on the human driver's performance during training. Two
benchmarks are considered: self-learning (no assistance) and full assistance
during training. Results from a human subject study indicate that the proposed
framework helps human drivers develop superior racing skills compared to the
benchmarks, resulting in better performance and consistency.

</details>


### [166] [EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation](https://arxiv.org/abs/2509.20656)
*Junzhe Wang,Jiarui Xie,Pengfei Hao,Zheng Li,Yi Cai*

Main category: cs.RO

TL;DR: 该论文提出了一种集成脑电信号解码、增强现实反馈与机器人抓取的闭环脑机接口系统，实现了更稳定、灵活和高效的人机交互，尤其适用于辅助类机器人场景。实验结果表明，该系统在准确率、效率和用户体验方面均取得了优秀成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的脑机接口-机器人系统面临脑电信号噪声大、不稳定、目标选择受限以及缺乏闭环验证等问题，严重制约了这些技术在现实辅助领域的应用价值。

Method: 本研究提出一个闭环BCI-AR-机器人系统，使用14通道EEG头戴设备进行个性化运动想象校准，利用智能手机上的增强现实界面提供针对方向的反馈以增强操作稳定性，机器人手臂结合决策输出与视觉位姿估计实现自主抓取。所有组件协同，实现无接触操作。

Result: 实验表明，运动想象脑电训练准确率达93.1%，信息传输率平均14.8 bit/min；AR反馈显著提升了持续控制力与最高ITR（21.3 bit/min）；闭环抓取成功率达到97.2%，操作效率高，用户主观控制感强。

Conclusion: 增强现实反馈能显著提高基于脑电的控制稳定性，该框架为实现坚定可靠、无接触的抓取任务提供切实方案，有效推动了辅助类机器人和未来人机交互模式的发展。

Abstract: Reliable brain-computer interface (BCI) control of robots provides an
intuitive and accessible means of human-robot interaction, particularly
valuable for individuals with motor impairments. However, existing BCI-Robot
systems face major limitations: electroencephalography (EEG) signals are noisy
and unstable, target selection is often predefined and inflexible, and most
studies remain restricted to simulation without closed-loop validation. These
issues hinder real-world deployment in assistive scenarios. To address them, we
propose a closed-loop BCI-AR-Robot system that integrates motor imagery
(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic
grasping for zero-touch operation. A 14-channel EEG headset enabled
individualized MI calibration, a smartphone-based AR interface supported
multi-target navigation with direction-congruent feedback to enhance stability,
and the robotic arm combined decision outputs with vision-based pose estimation
for autonomous grasping. Experiments are conducted to validate the framework:
MI training achieved 93.1 percent accuracy with an average information transfer
rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained
control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with
static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2
percent success rate with good efficiency and strong user-reported control.
These results show that AR feedback substantially stabilizes EEG-based control
and that the proposed framework enables robust zero-touch grasping, advancing
assistive robotic applications and future modes of human-robot interaction.

</details>


### [167] [Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks](https://arxiv.org/abs/2509.20674)
*Zeyu Han,Shuocheng Yang,Minghan Zhu,Fang Zhang,Shaobing Xu,Maani Ghaffari,Jianqiang Wang*

Main category: cs.RO

TL;DR: 本文提出Equi-RO网络，一种针对4D毫米波雷达的同变性里程计方法，在恶劣天气下对定位更为鲁棒，实验显示其在现有数据集上的精度优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有里程计方法（如基于LiDAR和相机）在极端天气下性能大幅下降，但毫米波雷达具有全天候观测能力，因此亟需开发高效的毫米波雷达里程计方法提升自动驾驶和机器人定位的鲁棒性。

Method: 该方法首先将雷达多普勒速度信息预处理为图结构中的节点和边的不变特征，并通过分别处理同变特征和不变特征的网络模块进行特征提取与聚合，采用图神经网络以改善稀疏雷达点云中的帧间匹配效率。

Result: 在公开数据集及自采数据集上，Equi-RO方法在平移和旋转精度方面分别较最佳主流算法提升10.7%和20.0%，表现出更强的鲁棒性和准确性。

Conclusion: Equi-RO方法为4D毫米波雷达里程计提供了新颖高效的解决方案，显著提升了在恶劣环境中的定位性能，对自动驾驶和机器人导航具有应用前景。

Abstract: Autonomous vehicles and robots rely on accurate odometry estimation in
GPS-denied environments. While LiDARs and cameras struggle under extreme
weather, 4D mmWave radar emerges as a robust alternative with all-weather
operability and velocity measurement. In this paper, we introduce Equi-RO, an
equivariant network-based framework for 4D radar odometry. Our algorithm
pre-processes Doppler velocity into invariant node and edge features in the
graph, and employs separate networks for equivariant and invariant feature
processing. A graph-based architecture enhances feature aggregation in sparse
radar data, improving inter-frame correspondence. Experiments on the
open-source dataset and self-collected dataset show Equi-RO outperforms
state-of-the-art algorithms in accuracy and robustness. Overall, our method
achieves 10.7% and 20.0% relative improvements in translation and rotation
accuracy, respectively, compared to the best baseline on the open-source
dataset.

</details>


### [168] [Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation](https://arxiv.org/abs/2509.20681)
*Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了一种新的从单张或少量图像快速重建高保真曲面和SDF场的方法，显著提升了效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前隐式表面重建方法（如NeuS等）通常依赖大量多视角图像，并需长时间训练，难以满足快速、低样本任务需求。为了支持机器人场景中的实时障碍规避和路径规划，需要能从少量甚至单张图像高效重建表面。

Method: 提出Fast Image-to-Neural Surface(FINS)，该方法结合多分辨率哈希网格编码器与轻量化几何、颜色预测头，并利用近似的二阶优化方法将训练速度提升到秒级。此外，通过使用预训练的基础大模型估算单张图片隐含的几何信息，实现单图重建。

Result: 实验表明，FINS在表面重建和SDF场估算任务中，较现有方法在收敛速度和精度上均有显著提升，并可扩展至多种基准数据集。其在机器人表面跟随等应用中亦证明了有效性。

Conclusion: FINS实现了基于单张或少量图像的高效率、高精度神经表面重建，推动了机器人感知与三维场景理解的实时化发展，并展现良好泛化和实用价值。

Abstract: Implicit representations have been widely applied in robotics for obstacle
avoidance and path planning. In this paper, we explore the problem of
constructing an implicit distance representation from a single image. Past
methods for implicit surface reconstruction, such as \emph{NeuS} and its
variants generally require a large set of multi-view images as input, and
require long training times. In this work, we propose Fast Image-to-Neural
Surface (FINS), a lightweight framework that can reconstruct high-fidelity
surfaces and SDF fields based on a single or a small set of images. FINS
integrates a multi-resolution hash grid encoder with lightweight geometry and
color heads, making the training via an approximate second-order optimizer
highly efficient and capable of converging within a few seconds. Additionally,
we achieve the construction of a neural surface requiring only a single RGB
image, by leveraging pre-trained foundation models to estimate the geometry
inherent in the image. Our experiments demonstrate that under the same
conditions, our method outperforms state-of-the-art baselines in both
convergence speed and accuracy on surface reconstruction and SDF field
estimation. Moreover, we demonstrate the applicability of FINS for robot
surface following tasks and show its scalability to a variety of benchmark
datasets.

</details>


### [169] [RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks](https://arxiv.org/abs/2509.20688)
*Shouren Mao,Minghao Qin,Wei Dong,Huajian Liu,Yongzhuo Gao*

Main category: cs.RO

TL;DR: 本文提出了一种面向资源的多目标神经架构搜索（NAS）方法RAM-NAS，专注于提升超网训练效果，并兼顾机器人设备的硬件资源限制，实现了模型精度与延迟的平衡，并在多种机器人边缘硬件上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统NAS方法在超网训练和对实际机器人硬件资源的适应性方面存在不足。作者希望解决模型设计过程中忽略硬件资源限制的问题，从而使NAS在机器人等边缘场景下更具实用性和适应性。

Method: 1. 引入子网互蒸馏机制，对所有通过sandwich rule采样的子网进行相互蒸馏；2. 采用Decoupled Knowledge Distillation（DKD）损失增强logits蒸馏性能；3. 在三种机器人边缘硬件上采集数据训练延迟预测器，在搜索阶段精准评估硬件推理延迟，实现精度与延迟的多目标进化搜索。

Result: 提出的RAM-NAS模型在ImageNet上获得76.7%到81.4%的top-1准确率，在下游检测和分割任务中，在三种硬件设备上的推理时间均优于基于MobileNetv3的方法。

Conclusion: RAM-NAS有效结合了资源感知与多目标NAS，显著缩短了机器人边缘硬件上的模型推理时间，同时保持了较高的准确率，为NAS在资源受限的机器人场景下的应用提供了新的解决方案。

Abstract: Neural architecture search (NAS) has shown great promise in automatically
designing lightweight models. However, conventional approaches are insufficient
in training the supernet and pay little attention to actual robot hardware
resources. To meet such challenges, we propose RAM-NAS, a resource-aware
multi-objective NAS method that focuses on improving the supernet pretrain and
resource-awareness on robot hardware devices. We introduce the concept of
subnets mutual distillation, which refers to mutually distilling all subnets
sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge
Distillation (DKD) loss to enhance logits distillation performance. To expedite
the search process with consideration for hardware resources, we used data from
three types of robotic edge hardware to train Latency Surrogate predictors.
These predictors facilitated the estimation of hardware inference latency
during the search phase, enabling a unified multi-objective evolutionary search
to balance model accuracy and latency trade-offs. Our discovered model family,
RAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on
ImageNet. In addition, the resource-aware multi-objective NAS we employ
significantly reduces the model's inference latency on edge hardware for
robots. We conducted experiments on downstream tasks to verify the scalability
of our methods. The inference time for detection and segmentation is reduced on
all three hardware types compared to MobileNetv3-based methods. Our work fills
the gap in NAS for robot hardware resource-aware.

</details>


### [170] [Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking](https://arxiv.org/abs/2509.20689)
*Chathura Semasinghe,Siavash Rezazadeh*

Main category: cs.RO

TL;DR: 本文将强迫振荡基础的步行降阶模型扩展到包含踝关节和足部。新模型模拟踝部动力学，提升了步态表现，并揭示了类似人类的稳定机制。


<details>
  <summary>Details</summary>
Motivation: 现有步行模型大多采用点足假设，未能反映实际人体踝部和足部对步态及稳定性的影响，因此亟需更拟人的建模方法以提升对人类步行控制机制的理解。

Method: 设计了一种仿人踝部动力学范式，并将其融入强迫振荡降阶模型，通过足部落点与踝关节策略的结合，实现多样的步行稳定性分析。

Result: 新模型在引入初始条件大误差时，可通过足部落点和踝关节策略共同保证稳定性；而面对小扰动，仅凭设计的本体感受踝关节控制即可动态稳定，无需依赖足部落点调整。

Conclusion: 该带踝与足的人形步行模型能够模拟出无需依靠足部动作即可自稳定的小扰动抗扰效果，这与人类步行观测结果相符，有助于深入理解仿人步行及其稳定控制原理。

Abstract: This paper extends the forced-oscillation-based reduced-order model of
walking to a model with ankles and feet. A human-inspired paradigm was designed
for the ankle dynamics, which results in improved gait characteristics compared
to the point-foot model. In addition, it was shown that while the proposed
model can stabilize against large errors in initial conditions through
combination of foot placement and ankle strategies, the model is able to
stabilize against small perturbations without relying on the foot placement
control and solely through the designed proprioceptive ankle scheme. This novel
property, which is also observed in humans, can help in better understanding of
anthropomorphic walking and its stabilization mechanisms.

</details>


### [171] [RuN: Residual Policy for Natural Humanoid Locomotion](https://arxiv.org/abs/2509.20696)
*Qingpeng Li,Chengrui Zhu,Yanming Wu,Xin Yuan,Zhen Zhang,Jian Yang,Yong Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的残差学习框架RuN，将人形机器人控制任务分解为运动生成和动力学修正两部分，实现了稳定、自然的步态及步行-跑步平滑过渡，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在实现人形机器人多速率自然行走、跑步及平滑过渡方面存在挑战，因为它们通常要求单一策略同时学习运动模仿、速度跟踪和稳定性维持，导致学习难度大、泛化能力弱。

Method: RuN框架分为两部分：1）使用预训练的条件运动生成器，生成运动先验保证动作自然；2）利用强化学习训练的残差策略，对生成器输出做轻量级动力学修正。通过这一解耦方式，显著简化了学习难度并提升了训练效率。

Result: 在Unitree G1人形机器人仿真和实机实验中，RuN实现了从0到2.5 m/s的大范围速度下，稳定自然的步态及顺畅的步行-跑步过渡。无论是训练效率还是最终性能均超过了当前主流强化学习方法。

Conclusion: RuN框架通过运动生成功能与动力学修正的分离，极大提升了人形机器人在复杂动态下的运动自然性和控制稳定性，为未来机器人动态行为的学习与应用带来新思路。

Abstract: Enabling humanoid robots to achieve natural and dynamic locomotion across a
wide range of speeds, including smooth transitions from walking to running,
presents a significant challenge. Existing deep reinforcement learning methods
typically require the policy to directly track a reference motion, forcing a
single policy to simultaneously learn motion imitation, velocity tracking, and
stability maintenance. To address this, we introduce RuN, a novel decoupled
residual learning framework. RuN decomposes the control task by pairing a
pre-trained Conditional Motion Generator, which provides a kinematically
natural motion prior, with a reinforcement learning policy that learns a
lightweight residual correction to handle dynamical interactions. Experiments
in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN
achieves stable, natural gaits and smooth walk-run transitions across a broad
velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both
training efficiency and final performance.

</details>


### [172] [Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations](https://arxiv.org/abs/2509.20703)
*Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于视频学习示范的机器人操作方法，能够让机器人以模仿人为目标生成可行的抓取姿态和物体轨迹，同时确保安全避障。


<details>
  <summary>Details</summary>
Motivation: 传统基于远程操控或手把手示范的机器人教学方式难以扩展，将人类视频示范用于机器人学习面临机器人结构与关节约束的挑战，因此需要新的方法来缩小人机差异，实现高效模仿。

Method: 作者提出了联合流轨迹优化（JFTO）框架：不直接模仿手部动作，而是以物体为中心，兼顾抓取可行性、演示一致性和避障。通过将流匹配方法扩展到物体位姿空间（SE(3)），实现对物体轨迹的概率建模，并在优化目标中集成抓取相似度、轨迹似然与碰撞惩罚。

Result: 方法在仿真和真实环境下的多种操作任务中进行了验证，展现出优秀的模仿能力和对不同任务的适应性。

Conclusion: JFTO框架能有效提升机器人对基于人类视频示范任务的模仿性能，克服了传统方法存在的人机表示差异和安全执行等难题。

Abstract: Learning from human video demonstrations offers a scalable alternative to
teleoperation or kinesthetic teaching, but poses challenges for robot
manipulators due to embodiment differences and joint feasibility constraints.
We address this problem by proposing the Joint Flow Trajectory Optimization
(JFTO) framework for grasp pose generation and object trajectory imitation
under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than
directly imitating human hand motions, our method treats demonstrations as
object-centric guides, balancing three objectives: (i) selecting a feasible
grasp pose, (ii) generating object trajectories consistent with demonstrated
motions, and (iii) ensuring collision-free execution within robot kinematics.
To capture the multimodal nature of demonstrations, we extend flow matching to
$\SE(3)$ for probabilistic modeling of object trajectories, enabling
density-aware imitation that avoids mode collapse. The resulting optimization
integrates grasp similarity, trajectory likelihood, and collision penalties
into a unified differentiable objective. We validate our approach in both
simulation and real-world experiments across diverse real-world manipulation
tasks.

</details>


### [173] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: 本文提出BIM2RDT框架，将静态BIM模型转化为面向机器人、动态更新的工地数字孪生体，融合三类数据流，实现施工管理的智能化和安全性提升。


<details>
  <summary>Details</summary>
Motivation: 目前建筑业数字化管理受限于BIM与现场实际情况的割裂，且现场数据与施工数字孪生体的自动融合性差，急需新的方法实现BIM与现场数据的实时联通和施工环节的安全管控。

Method: 该框架整合BIM模型的几何和语义信息、物联网的活动数据和机器人采集的空间视觉数据。提出了基于大语言模型推理的Semantic-Gravity ICP算法，实现点云更精准的配准。采用YOLOE目标检测与Shi-Tomasi角点跟踪施工元素，并将工地震动安全事件映射到数字孪生，利用IFC标准提供安全干预，形成机器人与数字孪生的动态反馈机制。

Result: 实验表明，SG-ICP算法在受遮挡环境下点云配准的均方根误差（RMSE）降低64.3%-88.3%，对比标准ICP表现优异。实时震动安全检测能准确触发超标预警，提升ISO 5349-1标准下的合规性。

Conclusion: BIM2RDT框架有效提升了BIM数据与工地现况的集成和现场作业安全水平，对建筑施工数字化和机器人应用具有重大推动作用。

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>


### [174] [Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor](https://arxiv.org/abs/2509.20709)
*Mani Amani,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出了一种将自然语言指令与来自建筑信息模型（BIM）的语义地图融合，用于提升机器人任务规划中的路径安全性与上下文适应性的框架。


<details>
  <summary>Details</summary>
Motivation: 自然语言与BIM为机器人规划提供了丰富的信息，但如何将这两者有效结合，用于更智能、上下文感知的导航和决策，仍是挑战。该论文动机在于克服现有方法在理解和利用自然语言指令上的局限，提升机器人在实际建筑环境中的适应能力与安全性。

Method: 作者提出将大语言模型（LLM）视为传感器，用其输出的危险概率作为伪计数，通过Beta-Bernoulli贝叶斯融合方法，与BIM中障碍物的先验参数结合，动态调整基于势场法的斥力系数。该方法能根据自然语言指令中的情感和上下文自动调整路径代价，为传统/学习型机器人路径规划方法提供灵活增强。

Result: 仿真实验结果显示，本方法在路径的强健性与有效性上，均优于只使用BIM信息或未融合自然语言时的方案，无论在定性还是定量上都有明显提升。

Conclusion: 提出的贝叶斯融合方法能够实现多自然语言指令与BIM信息的高效结合，为建筑场景中的机器人提供了更具上下文感知能力和安全性的路径规划工具，并具备良好的可扩展性和易集成性。

Abstract: Integrating natural language (NL) prompts into robotic mission planning has
attracted significant interest in recent years. In the construction domain,
Building Information Models (BIM) encapsulate rich NL descriptions of the
environment. We present a novel framework that fuses NL directives with
BIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting
the LLM as a sensor: each obstacle's design-time repulsive coefficient is
treated as a Beta(alpha, beta) random variable and LLM-returned danger scores
are incorporated as pseudo-counts to update alpha and beta. The resulting
posterior mean yields a continuous, context-aware repulsive gain that augments
a Euclidean-distance-based potential field for cost heuristics. By adjusting
gains based on sentiment and context inferred from user prompts, our method
guides robots along safer, more context-aware paths. This provides a
numerically stable method that can chain multiple natural commands and prompts
from construction workers and foreman to enable planning while giving
flexibility to be integrated in any learned or classical AI framework.
Simulation results demonstrate that this Beta-Bernoulli fusion yields both
qualitative and quantitative improvements in path robustness and validity.

</details>


### [175] [RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking](https://arxiv.org/abs/2509.20717)
*Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 本论文提出了一种名为RobotDancing的简单且可扩展的框架，能够提升人形机器人在长时间高动态运动跟踪任务中的表现，并实现仿真到实物（sim-to-real）的无缝迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高动态动作追踪中易出现模型与真实机器人之间的不匹配，导致误差积累，这极大限制了人形机器人完成复杂动作的能力。

Method: RobotDancing采用端到端的强化学习方法，利用统一的观测、奖励与超参数配置，直接预测剩余关节目标，以显性修正动力学上的模型与实际偏差。训练过程包含仿真训练、仿真验证和零样本（zero-shot）实物迁移，并在多个机器人硬件平台上进行测试。

Result: RobotDancing在Unitree G1平台，通过LAFAN1舞蹈动作数据集的迁移，实现了多分钟、高能量、诸如跳跃、旋转、侧翻等复杂动作的精准跟踪，并能零样本部署到实体机器人H1/H1-2上，运动跟踪效果优异。

Conclusion: 所提方法实现了鲁棒、精准的复杂动作追踪，显著提升了人形机器人在实际场景中执行高动态动作的能力，为机器人运动控制与仿真到现实迁移提供了一种有效思路。

Abstract: Long-horizon, high-dynamic motion tracking on humanoids remains brittle
because absolute joint commands cannot compensate model-plant mismatch, leading
to error accumulation. We propose RobotDancing, a simple, scalable framework
that predicts residual joint targets to explicitly correct dynamics
discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and
zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)
setup with a unified observation, reward, and hyperparameter configuration. We
evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and
validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy
behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with
high motion tracking quality.

</details>


### [176] [SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning](https://arxiv.org/abs/2509.20739)
*Guoyang Zhao,Yudong Li,Weiqing Qi,Kai Zhang,Bonan Liu,Kai Chen,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种全新的、无需SLAM的视觉-语言导航框架，利用语义推理和轻量级拓扑表示提升四足机器人导航的鲁棒性和智能性。


<details>
  <summary>Details</summary>
Motivation: 传统基于SLAM的方法在四足机器人快速运动、传感器漂移、标定误差等情况下表现脆弱，且缺乏任务驱动的语义推理，限制了机器人探索能力。

Method: 作者提出以视觉-语言融合为核心的语义感知模块，通过场景级和物体级信息结合，实现鲁棒的语义推理。采用语义-概率拓扑地图，结合大语言模型(LLM)进行全局子目标选择和基于视觉的局部避障规划，并与强化学习行走控制器集成，支持多平台部署。

Result: 仿真和现实实验表明，该方法在语义精度、规划质量和导航成功率方面均实现了持续提升。消融实验显示分层感知和细粒度局部规划对性能提升不可或缺。

Conclusion: 本文开创了一种基于视觉-语言、抛弃传统几何建图的导航范式，将机器人探索从以几何为中心转向以语义为核心的决策和规划。

Abstract: Conventional SLAM pipelines for legged robot navigation are fragile under
rapid motion, calibration demands, and sensor drift, while offering limited
semantic reasoning for task-driven exploration. To deal with these issues, we
propose a vision-only, SLAM-free navigation framework that replaces dense
geometry with semantic reasoning and lightweight topological representations. A
hierarchical vision-language perception module fuses scene-level context with
object-level cues for robust semantic inference. And a semantic-probabilistic
topological map supports coarse-to-fine planning: LLM-based global reasoning
for subgoal selection and vision-based local planning for obstacle avoidance.
Integrated with reinforcement-learning locomotion controllers, the framework is
deployable across diverse legged robot platforms. Experiments in simulation and
real-world settings demonstrate consistent improvements in semantic accuracy,
planning quality, and navigation success, while ablation studies further
showcase the necessity of both hierarchical perception and fine local planning.
This work introduces a new paradigm for SLAM-free, vision-language-driven
navigation, shifting robotic exploration from geometry-centric mapping to
semantics-driven decision making.

</details>


### [177] [MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM](https://arxiv.org/abs/2509.20757)
*Yuxuan Zhou,Xingxing Li,Shengyu Li,Zhuohao Yan,Chunxi Xia,Shaoquan Feng*

Main category: cs.RO

TL;DR: 本文提出了一种新型多传感器辅助的视觉SLAM系统（MASt3R-Fusion），结合了基于神经网络的点云回归和惯导、GNSS等多传感器信息，实现了鲁棒和高精度的三维建图及定位。


<details>
  <summary>Details</summary>
Motivation: 传统视觉SLAM在低纹理、尺度不确定和光照等极端环境下表现较差。虽然最新的神经网络点云回归方法提升了建图精度，但忽略了多传感器融合的优势。亟需一种方法将神经网络视觉特性与多传感器信息有效结合，弥补现有方法的不足。

Method: 该方法提出将基于Sim(3)的视觉对齐约束（采用海森矩阵形式）引入通用的、具备尺度的SE(3)因子图中，实现视觉、惯导和GNSS等多信息融合。采用层级因子图架构，支持实时滑窗优化与全局回环优化，保证实时性和全局一致性。

Result: 在公开基准与自采数据集上评估，结果显示在准确性和鲁棒性上均大幅超越现有以视觉为主的多传感器SLAM系统。

Conclusion: 该系统兼顾了深度学习方法与概率传感器融合的优势，提升了视觉SLAM在多种复杂场景下的实用性，为三维建图与定位相关研究与应用提供了新的思路。项目代码将开源，利于学术复现与二次开发。

Abstract: Visual SLAM is a cornerstone technique in robotics, autonomous driving and
extended reality (XR), yet classical systems often struggle with low-texture
environments, scale ambiguity, and degraded performance under challenging
visual conditions. Recent advancements in feed-forward neural network-based
pointmap regression have demonstrated the potential to recover high-fidelity 3D
scene geometry directly from images, leveraging learned spatial priors to
overcome limitations of traditional multi-view geometry methods. However, the
widely validated advantages of probabilistic multi-sensor information fusion
are often discarded in these pipelines. In this work, we propose
MASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly
integrates feed-forward pointmap regression with complementary sensor
information, including inertial measurements and GNSS data. The system
introduces Sim(3)-based visualalignment constraints (in the Hessian form) into
a universal metric-scale SE(3) factor graph for effective information fusion. A
hierarchical factor graph design is developed, which allows both real-time
sliding-window optimization and global optimization with aggressive loop
closures, enabling real-time pose tracking, metric-scale structure perception
and globally consistent mapping. We evaluate our approach on both public
benchmarks and self-collected datasets, demonstrating substantial improvements
in accuracy and robustness over existing visual-centered multi-sensor SLAM
systems. The code will be released open-source to support reproducibility and
further research (https://github.com/GREAT-WHU/MASt3R-Fusion).

</details>


### [178] [Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning](https://arxiv.org/abs/2509.20766)
*Gawon Lee,Daesol Cho,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多任务强化学习（MTRL）探索方法MT-Lévy，在机器人领域显著提升了采样效率和探索能力。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习有助于提升样本利用率和泛化能力，但在机器人应用中受限于高昂的数据采集成本。因此，亟需方法提升MTRL的探索效率。

Method: 提出MT-Lévy方法：结合跨任务行为共享与受Lévy flight启发的时间扩展型探索。利用相关任务已训练策略引导探索，并根据任务完成度动态调整探索强度，以高效覆盖状态空间。

Result: 实验证明，MT-Lévy相较于传统方法，在探索能力和样本效率上均有显著提升。定量和定性分析以及消融实验证实行为共享与自适应探索的协同作用。

Conclusion: MT-Lévy为实际机器人多任务学习提供了更高效的解决方案，极大提高了MTRL方法在实际场景中的实用性。

Abstract: Multi-task reinforcement learning (MTRL) offers a promising approach to
improve sample efficiency and generalization by training agents across multiple
tasks, enabling knowledge sharing between them. However, applying MTRL to
robotics remains challenging due to the high cost of collecting diverse task
data. To address this, we propose MT-L\'evy, a novel exploration strategy that
enhances sample efficiency in MTRL environments by combining behavior sharing
across tasks with temporally extended exploration inspired by L\'evy flight.
MT-L\'evy leverages policies trained on related tasks to guide exploration
towards key states, while dynamically adjusting exploration levels based on
task success ratios. This approach enables more efficient state-space coverage,
even in complex robotics environments. Empirical results demonstrate that
MT-L\'evy significantly improves exploration and sample efficiency, supported
by quantitative and qualitative analyses. Ablation studies further highlight
the contribution of each component, showing that combining behavior sharing
with adaptive exploration strategies can significantly improve the practicality
of MTRL in robotics applications.

</details>


### [179] [SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation](https://arxiv.org/abs/2509.20839)
*Jiaxuan He,Jiamei Ren,Chongshang Yan,Wenjie Song*

Main category: cs.RO

TL;DR: SemSight是一种鸟瞰图预测模型，能够从已知信息推断出未知区域的房间级语义结构，提升了室内导航和探索的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有导航与探索方法多仅关注单一目标或简单的几何占据图，缺乏对室内房间级语义结构推理的能力，这限制了导航的智能化和效率。

Method: 提出SemSight模型，对多层次语义进行贝叶斯鸟瞰预测，采用编码器-解码器神经网络，并引入对未知区域加权的mask约束监督策略，使模型更关注预测未知区域的语义结构。在2,000个室内布局上模拟探索，构建4万个序列观测数据作为训练集。

Result: SemSight能有效完成未知区域的语义地图补全，在关键功能类别的预测上，结构一致性与区域识别准确率均优于无需mask监督的基线方法。此外在闭环导航任务仿真中，显著减少了搜索步数。

Conclusion: SemSight提升了机器人在复杂室内环境中对未知区域的理解与语义预测，有效增强了目标导航与自主探索的效率，为实际应用奠定基础。

Abstract: In target-driven navigation and autonomous exploration, reasonable prediction
of unknown regions is crucial for efficient navigation and environment
understanding. Existing methods mostly focus on single objects or geometric
occupancy maps, lacking the ability to model room-level semantic structures. We
propose SemSight, a probabilistic bird's-eye-view prediction model for
multi-level scene semantics. The model jointly infers structural layouts,
global scene context, and target area distributions, completing semantic maps
of unexplored areas while estimating probability maps for target categories. To
train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout
graphs, constructing a diverse dataset of 40,000 sequential egocentric
observations paired with complete semantic maps. We adopt an encoder-decoder
network as the core architecture and introduce a mask-constrained supervision
strategy. This strategy applies a binary mask of unexplored areas so that
supervision focuses only on unknown regions, forcing the model to infer
semantic structures from the observed context. Experimental results show that
SemSight improves prediction performance for key functional categories in
unexplored regions and outperforms non-mask-supervised approaches on metrics
such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It
also enhances navigation efficiency in closed-loop simulations, reducing the
number of search steps when guiding robots toward target areas.

</details>


### [180] [ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation](https://arxiv.org/abs/2509.20841)
*Dekun Lu,Wei Gao,Kui Jia*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的链式运动定向关键点（CoMOK）方法，用于实现端到端的机器人操作策略，能够泛化和准确地完成多样化任务。


<details>
  <summary>Details</summary>
Motivation: 当前端到端的神经网络在机器人操作上的表现尚不足以大规模实际部署，现有方法在任务泛化、精度和可靠性上有限，亟需更通用和强大的操作策略表达方式。

Method: 作者提出用“运动定向关键点”链作为机器人的动作表示，用神经网络端到端训练，实现对多样化任务的统一表述。该表征方式扩展了标准末端执行器位姿动作空间，对不同形状、尺寸的物体有自然的泛化能力，同时能达到亚厘米级精度，还易于处理多阶段、多模态操作及可变形物体。

Result: 通过大量仿真和实际硬件实验验证，该方法在任务多样性、精度和通用性等方面都体现出有效性，并优于以往的端到端方法。

Conclusion: 所提出的CoMOK策略为提升端到端神经网络在机器人操作领域的实用性和泛化能力提供了新途径，有潜力扩大实际部署范围。

Abstract: End-to-end robot manipulation policies offer significant potential for
enabling embodied agents to understand and interact with the world. Unlike
traditional modular pipelines, end-to-end learning mitigates key limitations
such as information loss between modules and feature misalignment caused by
isolated optimization targets. Despite these advantages, existing end-to-end
neural networks for robotic manipulation--including those based on large
VLM/VLA models--remain insufficiently performant for large-scale practical
deployment. In this paper, we take a step towards an end-to-end manipulation
policy that is generalizable, accurate and reliable. To achieve this goal, we
propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for
robotic manipulation. Our formulation is used as the action representation of a
neural policy, which can be trained in an end-to-end fashion. Such an action
representation is general, as it extends the standard end-effector pose action
representation and supports a diverse set of manipulation tasks in a unified
manner. The oriented keypoint in our method enables natural generalization to
objects with different shapes and sizes, while achieving sub-centimeter
accuracy. Moreover, our formulation can easily handle multi-stage tasks,
multi-modal robot behaviors, and deformable objects. Extensive simulated and
hardware experiments demonstrate the effectiveness of our method.

</details>


### [181] [MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases](https://arxiv.org/abs/2509.20843)
*Ziang Luo,Kangan Qian,Jiahua Wang,Yuechen Luo,Jinyu Miao,Zheng Fu,Yunlong Wang,Sicong Jiang,Zilin Huang,Yifei Hu,Yuhao Yang,Hao Ye,Mengmeng Yang,Xiaojian Dong,Kun Jiang,Diange Yang*

Main category: cs.RO

TL;DR: 本文提出了MTRDrive框架，通过记忆检索与动态工具结合，提升视觉-语言模型在自动驾驶领域的泛化和鲁棒性，实验证明其在多项基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在自动驾驶中易产生幻觉、泛化性差，尤其面对分布外场景时表现不可靠，难以满足实际部署需求。

Method: 提出MTRDrive闭环系统，将基于记忆的经验检索机制与动态工具集成协同，强化模型与环境的互动和推理决策，同时引入了针对复杂施工场景的新基准测试Roadwork-VLM。

Result: 在NAVSIM公开基准上，MTRDrive（3B参数）实现PDMS 88.3、高层规划分数79.8%、规划精度82.6%。在Roadwork-VLM基准上实现zero-shot驾驶分数80.2%，显著优于现有方法。

Conclusion: MTRDrive展现了在自动驾驶领域提升泛化性与决策可靠性的潜力，有望推动VLMs安全可靠落地。

Abstract: Vision-Language Models(VLMs) have demonstrated significant potential for
end-to-end autonomous driving, yet a substantial gap remains between their
current capabilities and the reliability necessary for real-world deployment. A
critical challenge is their fragility, characterized by hallucinations and poor
generalization in out-of-distribution (OOD) scenarios. To bridge this gap, we
introduce MTRDrive, a novel framework that integrates procedural driving
experiences with a dynamic toolkit to enhance generalization and proactive
decision-making.
  MTRDrive addresses these limitations through a closed-loop system that
combines a memory-based experience retrieval mechanism with dynamic toolkits.
This synergy enables the model to interact more effectively with its
environment, improving both reasoning and decision-making capabilities with the
help of our memory-tool synergistic reasoning. Additionally, we introduce a new
benchmark based on complex Roadwork construction scenarios to rigorously
evaluate zero-shot generalization.
  Extensive experiments demonstrate the superior effectiveness of our approach.
On the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an
exceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art
performance bar on high-level planning, with a driving metric score of 79.8\%
and a planning accuracy of 82.6\%. Rigorous zero-shot evaluation on the new
Roadwork-VLM benchmark shows a strong ability to reason robustly in unseen
scenarios, achieving a driving metric score of 80.2\%. These results highlight
MTRDrive's potential to advance autonomous driving toward safer and more
reliable systems.

</details>


### [182] [Efficient Differentiable Contact Model with Long-range Influence](https://arxiv.org/abs/2509.20917)
*Xiaohan Ye,Kui Wu,Zherong Pan,Taku Komura*

Main category: cs.RO

TL;DR: 本文提出了一个适用于可微分物理仿真器的高效接触模型，以解决现有可微分模拟中梯度信息不连续或消失，从而影响下游优化任务的问题。


<details>
  <summary>Details</summary>
Motivation: 随着可微分物理的成熟，其在模型预测控制、机器人设计优化和神经PDE求解等应用中的作用日益重要。但在实际应用中，由于接触模型设计不当，仿真器返回的梯度信息存在突变或消失，影响了基于梯度的方法的收敛性。

Method: 本文分析了导致梯度不良行为的根本原因在于接触模型，并提出了一套接触模型需要满足的性质，用于保证良好的梯度信息。随后设计并实现了一个满足这些性质且计算高效的可微分刚体接触模型。

Result: 实验表明，新模型即使从简单初始化出发，也能发现复杂的、具有丰富接触行为的控制信号，并能完成多种下游运动与操作任务。

Conclusion: 提出的接触模型不仅保证了优化过程中梯度的可用性和稳定性，还兼顾了计算效率，极大提升了可微分物理仿真在控制和机器人等领域的实用性。

Abstract: With the maturation of differentiable physics, its role in various downstream
applications: such as model predictive control, robotic design optimization,
and neural PDE solvers, has become increasingly important. However, the
derivative information provided by differentiable simulators can exhibit abrupt
changes or vanish altogether, impeding the convergence of gradient-based
optimizers. In this work, we demonstrate that such erratic gradient behavior is
closely tied to the design of contact models. We further introduce a set of
properties that a contact model must satisfy to ensure well-behaved gradient
information. Lastly, we present a practical contact model for differentiable
rigid-body simulators that satisfies all of these properties while maintaining
computational efficiency. Our experiments show that, even from simple
initializations, our contact model can discover complex, contact-rich control
signals, enabling the successful execution of a range of downstream locomotion
and manipulation tasks.

</details>


### [183] [Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement](https://arxiv.org/abs/2509.20938)
*Jianbo Zhao,Taiyu Ban,Xiangjie Li,Xingtai Gui,Hangning Zhou,Lei Liu,Hongwei Zhao,Bin Li*

Main category: cs.RO

TL;DR: 本文提出了TISA模块，通过解决自回归模型在自动驾驶端到端规划中的时空错位问题，大幅提升了性能，并结合动力学预测头和多目标后训练，实现了业界最优表现。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在自动驾驶规划中表现强劲，但由于未来行动需依赖于过去感知数据，导致时空视角不一致，限制了其性能。本研究旨在解决这一核心瓶颈，提升自动驾驶模型的实际能力。

Method: 1）提出TISA模块，将初始环境特征投影到一致的自车坐标系，解决未来时刻的时空错位问题；2）采用动力学动作预测头，确保轨迹物理可行性；3）引入基于DPO的多目标后训练，对特定驾驶行为进行细粒度反馈，超越单一模仿目标。

Result: 所提方法在NAVSIM数据集上取得了89.8的PDMS，刷新了自回归模型的行业最佳记录。

Conclusion: TISA模块有效校正时空视角，动力学约束与多目标优化进一步增强模型表现。整体方案为自动驾驶场景下端到端自回归规划树立了新基线，并具有更强泛化与实际可用性。

Abstract: The inherent sequential modeling capabilities of autoregressive models make
them a formidable baseline for end-to-end planning in autonomous driving.
Nevertheless, their performance is constrained by a spatio-temporal
misalignment, as the planner must condition future actions on past sensory
data. This creates an inconsistent worldview, limiting the upper bound of
performance for an otherwise powerful approach. To address this, we propose a
Time-Invariant Spatial Alignment (TISA) module that learns to project initial
environmental features into a consistent ego-centric frame for each future time
step, effectively correcting the agent's worldview without explicit future
scene prediction. In addition, we employ a kinematic action prediction head
(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.
Finally, we introduce a multi-objective post-training stage using Direct
Preference Optimization (DPO) to move beyond pure imitation. Our approach
provides targeted feedback on specific driving behaviors, offering a more
fine-grained learning signal than the single, overall objective used in
standard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM
dataset among autoregressive models. The video document is available at
https://tisa-dpo-e2e.github.io/.

</details>


### [184] [BactoBot: A Low-Cost, Bacteria-Inspired Soft Underwater Robot for Marine Exploration](https://arxiv.org/abs/2509.20964)
*Rubaiyat Tasnim Chowdhury,Nayan Bala,Ronojoy Roy,Tarek Mahmud*

Main category: cs.RO

TL;DR: 本文介绍了一种名为BactoBot的低成本软体水下机器人，其结构和推进方式模拟细菌鞭毛，旨在以温和、安全的方式探索海洋环境。原型设计以12个硅胶软臂和3D打印骨架组成，实验表明其具备基础运动能力，并为后续自主和实际应用打下基础。


<details>
  <summary>Details</summary>
Motivation: 传统的刚性水下机器人在海洋环境中可能对脆弱的生态系统造成损害，因此亟需一种更柔和且安全的水下探索工具，尤其适用于科研和有限资源场景。

Method: 借鉴细菌鞭毛推进的生物原理，设计了一种12臂硅胶软体结构，采用3D打印和食品级硅胶铸造等简易制造工艺。机器人通过改进防水和调节浮力的方法，利用市售微控制器进行控制，并在水池中进行运动测试。

Result: 原型机在封闭水池中成功实现了前进和转向运动，显示了多臂软体设计的有效性及生物运动机理的低成本可实现性。

Conclusion: 该项目验证了低成本软体仿生水下机器人的可行性，为环境友好型、适用于资源受限情景的海洋机器人开发提供了基础，未来可进一步实现自主化升级和野外部署。

Abstract: Traditional rigid underwater vehicles pose risks to delicate marine
ecosystems. This paper presents BactoBot, a low-cost, soft underwater robot
designed for safe and gentle marine exploration. Inspired by bacterial
flagellar propulsion, BactoBot features 12 flexible, silicone-based arms
arranged on a 3D-printed dodecahedral frame. The design provides inherent
compliance, redundancy, and the potential for omnidirectional movement. The
prototype was fabricated using accessible DIY methods, including food-grade
silicone molding, 3D printing, and off-the-shelf microcontrollers.
Waterproofing and buoyancy calibration protocols were developed, and the robot
was successfully tested in a controlled water tank, demonstrating forward
motion and turning. The results validate the feasibility of replicating complex
biological locomotion at low cost. The project lays a foundation for
environmentally conscious robotic tools, particularly for marine science in
resource-constrained settings, and identifies pathways toward autonomous
operation and field deployment.

</details>


### [185] [AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/abs/2509.21006)
*Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种名为AnywhereVLA的模块化移动操作系统，能够根据自然语言指令在未知、多变的室内环境中实现物体的抓取与放置任务。该系统集成了经典SLAM、语义建图、前沿探索等模块，并结合了经过微调的小型VLA操作头，实现了端到端的自主操作。实验表明该系统在消费级硬件上运行流畅，任务成功率为46%。


<details>
  <summary>Details</summary>
Motivation: 让机器人能依据自然语言指令，在实际复杂环境下自主完成抓取与放置任务，是服务机器人广泛应用的关键挑战。现有方法要么依赖大型模型算力消耗大，要么泛化能力弱，难以在实际场景高效可靠地运行。

Method: 提出了AnywhereVLA框架，将用户自然语言解析为结构化任务图，从而驱动SLAM、语义建图和前沿探索，基于可见性与可达性做抓取前位姿规划。物体操作模块采用SmolVLA头部，在特定平台数据上微调以增强抓取与放置的泛化能力。系统整体在嵌入式消费级硬件(Jetson Orin NX、Intel NUC)上实时运行。

Result: 在多房间实验室环境下(包含静态场景和人类日常活动干扰)，AnywhereVLA在嵌入式算力下任务总成功率达到46%。

Conclusion: AnywhereVLA通过融合经典几何导航与微调语言感知操作模块，可以在消费级硬件上实现具有良好泛化能力的自然语言移动操作任务，兼具传统方法的可靠性与新方法的灵活性。

Abstract: We address natural language pick-and-place in unseen, unpredictable indoor
environments with AnywhereVLA, a modular framework for mobile manipulation. A
user text prompt serves as an entry point and is parsed into a structured task
graph that conditions classical SLAM with LiDAR and cameras, metric semantic
mapping, and a task-aware frontier exploration policy. An approach planner then
selects visibility and reachability aware pre grasp base poses. For
interaction, a compact SmolVLA manipulation head is fine tuned on platform pick
and place trajectories for the SO-101 by TheRobotStudio, grounding local visual
context and sub-goals into grasp and place proposals. The full system runs
fully onboard on consumer-level hardware, with Jetson Orin NX for perception
and VLA and an Intel NUC for SLAM, exploration, and control, sustaining
real-time operation. We evaluated AnywhereVLA in a multi-room lab under static
scenes and normal human motion. In this setting, the system achieves a $46\%$
overall task success rate while maintaining throughput on embedded compute. By
combining a classical stack with a fine-tuned VLA manipulation, the system
inherits the reliability of geometry-based navigation with the agility and task
generalization of language-conditioned manipulation.

</details>


### [186] [Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting](https://arxiv.org/abs/2509.21020)
*Abdelaziz Shaarawy,Cansu Erdogan,Rustam Stolkin,Alireza Rastegarpanah*

Main category: cs.RO

TL;DR: 提出了一种用于电动汽车电池多机器人拆解的四层任务与运动规划（TAMP）框架，结合符号任务规划、分配优化与基于学习的运动生成方法，大幅提升了多机器人在复杂环境下的协作效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车电池拆解流程复杂，需多机器人协作，且环境拥挤且动态，要求高效、精确且安全的任务分配与运动规划。现有方法在运动路径、执行效率和碰撞控制等方面存在不足，难以满足实际工业应用需求。

Method: 提出四层TAMP框架：1）符号任务规划，2）基于成本和可达性优化的任务分配，3）TP-GMM学习型运动生成，4）结合YOLOv8和OctoMap提供实时感知与场景建模，并以FCL与MoveIt进行碰撞检测与路径调整。实现了端到端的感知—决策—规划—避障闭环。

Result: 在UR10e双臂机器人平台上拆解电池关键部件（电缆、汇流排、服务插头、单体电池），框架生成的运动比RRTConnect基线更紧凑和安全：末端执行器路径平均缩短63.3%，总任务时间缩短8.1%，单臂运动体积显著减少，重叠空间减小47%。

Conclusion: 所提TAMP框架有效提升了复杂动态环境下多机器人电池拆解任务的自主性、精确性和安全性，优于传统任务与运动规划方法，有望用于实际工业拆解场景。

Abstract: Electric-vehicle (EV) battery disassembly requires precise multi-robot
coordination, short and reliable motions, and robust collision safety in
cluttered, dynamic scenes. We propose a four-layer task-and-motion planning
(TAMP) framework that couples symbolic task planning and cost- and
accessibility-aware allocation with a TP-GMM-guided motion planner learned from
demonstrations. Stereo vision with YOLOv8 provides real-time component
localization, while OctoMap-based 3D mapping and FCL(Flexible Collision
Library) checks in MoveIt unify predictive digital-twin collision checking with
reactive, vision-based avoidance. Validated on two UR10e robots across cable,
busbar, service plug, and three leaf-cell removals, the approach yields
substantially more compact and safer motions than a default RRTConnect baseline
under identical perception and task assignments: average end-effector path
length drops by $-63.3\%$ and makespan by $-8.1\%$; per-arm swept volumes
shrink (R1: $0.583\rightarrow0.139\,\mathrm{m}^3$; R2:
$0.696\rightarrow0.252\,\mathrm{m}^3$), and mutual overlap decreases by $47\%$
($0.064\rightarrow0.034\,\mathrm{m}^3$). These results highlight improved
autonomy, precision, and safety for multi-robot EV battery disassembly in
unstructured, dynamic environments.

</details>


### [187] [KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models](https://arxiv.org/abs/2509.21027)
*Sibo Li,Qianyue Hao,Yu Shang,Yong Li*

Main category: cs.RO

TL;DR: KeyWorld是一种提升机器人世界模型预测效率与物理合理性的框架，通过聚焦关键帧运算与高效中间帧补全，大幅提升了推理速度和结果有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人世界模型在预测未来环境状态时，采用逐帧生成策略，导致了大量冗余运算，并且难以保证生成轨迹的物理合理性，这严重限制了其实用性。问题根源在于模型对帧间变化缺乏关注，忽视了关键语义转变的重要性。

Method: 提出KeyWorld框架，流程为：首先通过简化运动轨迹迭代提取出语义关键帧；然后用DiT模型（扩散Transformer）根据文本任务描述生成这些物理意义明确的关键帧；最后，通过轻量卷积插值器高效补齐中间非关键帧，完整还原视频序列。

Result: 在LIBERO基准测试中，KeyWorld相较传统逐帧生成方法推理速度提升5.68倍，并且在复杂任务中对物理有效性的提升尤为明显。聚焦关键帧使生成视频更符合真实世界运动规律。

Conclusion: KeyWorld为高效、实时机器人控制等需要快速且有效世界模型的场景，提供了务实的解决方案，推动了世界模型在实际中的部署应用。

Abstract: Robotic world models are a promising paradigm for forecasting future
environment states, yet their inference speed and the physical plausibility of
generated trajectories remain critical bottlenecks, limiting their real-world
applications. This stems from the redundancy of the prevailing frame-to-frame
generation approach, where the model conducts costly computation on similar
frames, as well as neglecting the semantic importance of key transitions. To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames. Specifically, KeyWorld first identifies
significant transitions by iteratively simplifying the robot's motion
trajectories, obtaining the ground truth key frames. Then, a DiT model is
trained to reason and generate these physically meaningful key frames from
textual task descriptions. Finally, a lightweight interpolator efficiently
reconstructs the full video by inpainting all intermediate frames. Evaluations
on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$
acceleration compared to the frame-to-frame generation baseline, and focusing
on the motion-aware key frames further contributes to the physical validity of
the generated videos, especially on complex tasks. Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models. Code is
released at https://anonymous.4open.science/r/Keyworld-E43D.

</details>


### [188] [MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation](https://arxiv.org/abs/2509.21045)
*Mahya Ramezani,M. Amin Alandihallaj,Barış Can Yalçın,Miguel Angel Olivares Mendez,Holger Voos*

Main category: cs.RO

TL;DR: 该论文提出了一种将强化学习（RL）与模型预测控制（MPC）集成的自主卫星对接控制方法，有效提升了带部分燃油时对接的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的卫星对接在微重力下由于燃料晃动导致不可预测的扰动力，给系统稳定性带来挑战。因此，亟需能兼顾鲁棒性和燃油高效性的对接控制新方法。

Method: 本文结合了Proximal Policy Optimization (PPO)和Soft Actor-Critic (SAC) 两种RL算法与MPC控制框架，利用MPC的预测能力加速RL训练并提升控制鲁棒性。方案在实验室平面稳定实验和带燃料晃动的6自由度高保真数值仿真中进行验证。

Result: 仿真结果表明，SAC-MPC混合方法在对接精度、成功率及控制能耗方面均优于单独的RL和PPO-MPC方法。

Conclusion: 本研究提升了卫星在轨加注与服务任务的实际可行性，促进了燃油高效、抗扰动的自主对接技术发展。

Abstract: This paper presents an integrated Reinforcement Learning (RL) and Model
Predictive Control (MPC) framework for autonomous satellite docking with a
partially filled fuel tank. Traditional docking control faces challenges due to
fuel sloshing in microgravity, which induces unpredictable forces affecting
stability. To address this, we integrate Proximal Policy Optimization (PPO) and
Soft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive
capabilities to accelerate RL training and improve control robustness. The
proposed approach is validated through Zero-G Lab of SnT experiments for planar
stabilization and high-fidelity numerical simulations for 6-DOF docking with
fuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves
superior docking accuracy, higher success rates, and lower control effort,
outperforming standalone RL and PPO-MPC methods. This study advances
fuel-efficient and disturbance-resilient satellite docking, enhancing the
feasibility of on-orbit refueling and servicing missions.

</details>


### [189] [Normalizing Flows are Capable Visuomotor Policy Learning Models](https://arxiv.org/abs/2509.21073)
*Simon Kristoffersson Lind,Jialong Li,Maj Stenmark,Volker Krüger*

Main category: cs.RO

TL;DR: 本文提出了一种基于Normalizing Flows（归一化流）的新型视觉-运动策略学习模型，用以替代高计算成本且缺乏输出置信度量的扩散模型，实现了更高效且可信的推理过程。实验证明，其性能优于现有的Diffusion Policy，推理速度提升至最高30倍，并验证了架构与训练技巧的有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能学习复杂行为，但推理计算量大且无法量化输出置信度，难以满足通用机器人对效率和可信度的需求。作者认为，模型必须能提供可靠的置信度度量，才能支撑安全可靠的泛化机器人任务。

Method: 提出Normalizing Flows Policy模型，利用Normalizing Flows在保持灵活建模能力的同时，天然输出概率密度函数，从而获得置信度度量，并提升推理效率。模型在四个不同的机器人模拟任务中进行了详实的实验测试，并通过消融实验分析了关键设计和训练技巧对性能的影响。

Result: Normalizing Flows Policy在性能上与Diffusion Policy持平甚至多次优于后者，且样本利用率更高，推理速度最高快至30倍。消融实验证实了多种架构和训练方式提升了模型表现。

Conclusion: Normalizing Flows为策略学习领域提供了高效、可信且性能优异的新方案，较扩散模型在通用机器人中的应用潜力更大。

Abstract: The field of general purpose robotics has recently embraced powerful
probabilistic models, such as diffusion models, to model and learn complex
behaviors. However, these models often come with significant trade-offs, namely
high computational costs for inference and a fundamental inability to quantify
output uncertainty. We argue that a model's trustworthiness, a critical factor
for reliable, general-purpose robotics, is inherently linked to its ability to
provide confidence measures.
  In this work, we introduce Normalizing Flows Policy, a novel visuomotor
policy learning model based on Normalizing Flows. We show that Normalizing
Flows are a natural and powerful alternative to diffusion models, providing
both a statistically sound measure of confidence and a highly efficient
inference process. Through comprehensive experiments across four distinct
simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves
performance comparable to, and often surpassing, Diffusion Policy, and it does
so not only with improved sample efficiency but also with up to 30 times faster
inference. Additionally, our ablation study validates several key architectural
and training techniques that enable Normalizing Flows to perform well in this
domain.

</details>


### [190] [Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection](https://arxiv.org/abs/2509.21085)
*Chenyu Zhao,Jingao Xu,Ciyu Ruan,Haoyang Wang,Shengbo Wang,Jiaqi Li,Jirong Zha,Weijie Hong,Zheng Yang,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.RO

TL;DR: AirTouch系统利用无人机自身的地效现象，通过解析无人机的姿态传感器和飞行指令，实现快速、低成本且高精度环境边缘检测，优于传统视觉方法。


<details>
  <summary>Details</summary>
Motivation: 灾害救援、无人导航等场景需要无人机进行高效、准确的环境边缘检测。现有基于雷达或摄像头的检测方案成本高、计算量大，不利于轻量级无人机应用。研究者希望寻找新的低成本、低能耗且精度高的方法。

Method: 提出AirTouch系统，转变传统飞控对地效的负面看法，将地效变化作为感知新模态。通过理论分析、算法设计与原型实现，利用无人机姿态传感器和飞行指令数据，捕捉地效变化以判断材料边界，实现高效边缘检测。

Result: 实验显示，系统边缘检测的距离误差均值仅0.051米，准确率提高86%，能耗仅43 mW，性能优于传统视觉基方法。

Conclusion: AirTouch系统将地效作为新型感知模态，实现了节能、成本低、检测高效精确的无人机环境边缘检测，填补了视觉方法无法满足的使用场景，具有重要应用前景。

Abstract: Drone-based rapid and accurate environmental edge detection is highly
advantageous for tasks such as disaster relief and autonomous navigation.
Current methods, using radars or cameras, raise deployment costs and burden
lightweight drones with high computational demands. In this paper, we propose
AirTouch, a system that transforms the ground effect from a stability "foe" in
traditional flight control views, into a "friend" for accurate and efficient
edge detection. Our key insight is that analyzing drone basic attitude sensor
readings and flight commands allows us to detect ground effect changes. Such
changes typically indicate the drone flying over a boundary of two materials,
making this information valuable for edge detection. We approach this insight
through theoretical analysis, algorithm design, and implementation, fully
leveraging the ground effect as a new sensing modality without compromising
drone flight stability, thereby achieving accurate and efficient scene edge
detection. We also compare this new sensing modality with vision-based methods
to clarify its exclusive advantages in resource efficiency and detection
capability. Extensive evaluations demonstrate that our system achieves a high
detection accuracy with mean detection distance errors of 0.051m, outperforming
the baseline method performance by 86%. With such detection performance, our
system requires only 43 mW power consumption, contributing to this new sensing
modality for low-cost and highly efficient edge detection.

</details>


### [191] [Cross-Modal Instructions for Robot Motion Generation](https://arxiv.org/abs/2509.21107)
*William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 该论文提出通过跨模态指令（如粗略标注和文本标签），而非传统物理演示，来教授机器人新行为。作者提出了CrossInstruct框架，实现了指令驱动的机器人动作生成，并通过强化学习进一步优化策略，实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统机器人行为学习依赖于人工演示，如遥操作或手把手引导，但这类数据的收集成本高且难以扩展，因此需要更便捷且易扩展的示范方式。

Method: 提出CrossInstruct框架，将带有自由文本标签的标注作为跨模态指令输入大型视觉-语言模型（VLM），模型借助微调小模型，综合多视角信息，生成3D运动轨迹。进一步利用CrossInstruct输出与强化学习结合，学习精细化任务策略。

Result: CrossInstruct在基准仿真任务和真实机器人硬件上进行验证，无需额外微调即可有效生成可执行指令，并为后续通过强化学习优化策略提供了优良初始化。

Conclusion: 跨模态指令（而非物理演示）为机器人行为学习提供有效新范式，CrossInstruct兼具通用性和初始化能力，并能与强化学习协同提升机器人复杂任务的完成能力。

Abstract: Teaching robots novel behaviors typically requires motion demonstrations via
teleoperation or kinaesthetic teaching, that is, physically guiding the robot.
While recent work has explored using human sketches to specify desired
behaviors, data collection remains cumbersome, and demonstration datasets are
difficult to scale. In this paper, we introduce an alternative paradigm,
Learning from Cross-Modal Instructions, where robots are shaped by
demonstrations in the form of rough annotations, which can contain free-form
text labels, and are used in lieu of physical motion. We introduce the
CrossInstruct framework, which integrates cross-modal instructions as examples
into the context input to a foundational vision-language model (VLM). The VLM
then iteratively queries a smaller, fine-tuned model, and synthesizes the
desired motion over multiple 2D views. These are then subsequently fused into a
coherent distribution over 3D motion trajectories in the robot's workspace. By
incorporating the reasoning of the large VLM with a fine-grained pointing
model, CrossInstruct produces executable robot behaviors that generalize beyond
the environment of in the limited set of instruction examples. We then
introduce a downstream reinforcement learning pipeline that leverages
CrossInstruct outputs to efficiently learn policies to complete fine-grained
tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and
real hardware, demonstrating effectiveness without additional fine-tuning and
providing a strong initialization for policies subsequently refined via
reinforcement learning.

</details>


### [192] [Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study](https://arxiv.org/abs/2509.21122)
*Mingjiang Liu,Hailong Huang*

Main category: cs.RO

TL;DR: 本文提出了一种用于无人机平衡球体任务的分层控制框架，并用强化学习方法优化高层决策，在仿真中显著优于PID控制器。作者分析发现，强化学习的优势主要来自对更丰富状态信息的利用。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态任务如无人机平衡球体时，传统PID控制器难以高效应对多变环境，存在调参瓶颈。引入强化学习可探索更优决策，并分析性能提升背后的原因。

Method: 将任务分为高层平衡决策与低层无人机控制两个层次。其中高层采用强化学习策略训练，低层维持传统控制。并与PID控制器在同一框架下对比仿真表现，通过有系统的分析探究性能差异来源。

Result: 实验显示，强化学习策略在平衡任务中表现出较明显的性能优势。进一步分析排除调参和非线性映射的影响，发现性能提升因强化学习更充分利用了丰富的状态观测。

Conclusion: 学习型控制策略的关键在于全面丰富的状态观测输入，提升感知能力有望进一步优化控制效果。强化学习在复杂动态任务中具有实用潜力。

Abstract: This paper addresses a drone ball-balancing task, in which a drone stabilizes
a ball atop a movable beam through cable-based interaction. We propose a
hierarchical control framework that decouples high-level balancing policy from
low-level drone control, and train a reinforcement learning (RL) policy to
handle the high-level decision-making. Simulation results show that the RL
policy achieves superior performance compared to carefully tuned PID
controllers within the same hierarchical structure. Through systematic
comparative analysis, we demonstrate that RL's advantage stems not from
improved parameter tuning or inherent nonlinear mapping capabilities, but from
its ability to effectively utilize richer state observations. These findings
underscore the critical role of comprehensive state representation in
learning-based systems and suggest that enhanced sensing could be instrumental
in improving controller performance.

</details>


### [193] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: 该论文提出了一个为车载GUI设计的高保真基准和交互环境Automotive-ENV，并基于此开发了支持地理信息感知的多模态智能体ASURADA，有效提升了车辆安全相关任务的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态智能体在通用GUI交互方面表现出色，但在汽车系统中的应用几乎未被探索。车载GUI存在注意力受限、安全性高、交互模式复杂等特殊挑战，需要专门的研究和测试环境。

Method: 作者开发了Automotive-ENV，这是第一个专为车载GUI设计的高保真基准测试平台，涵盖了185个参数化任务，包括显式控制、隐式意图理解和安全相关任务，并具备结构化的多模态观测和精确的程序性检查。基于该平台，作者提出了融合GPS地理信息的多模态智能体ASURADA，可根据具体地点和环境动态调整动作。

Result: 实验证明，融合地理信息的ASURADA在安全相关任务中的成功率显著提升，强调了地理和环境语境在智能驾驶中的重要性。

Conclusion: 地理感知的多模态智能体在车载系统中具有巨大潜力，Automotive-ENV及相关工具的开放有望推动安全、适应性更强的车载智能体研究和应用。

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


### [194] [DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps](https://arxiv.org/abs/2509.21145)
*Md Faizal Karim,Vignesh Vembar,Keshab Patra,Gaurav Singh,K Madhava Krishna*

Main category: cs.RO

TL;DR: 提出了一种名为DAGDiff的端到端方法，通过在SE(3)xSE(3)空间内直接生成双臂抓取对，实现了更稳健、可泛化的双臂抓取，并在现实点云与实际机器人系统中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有双臂抓取方法常将任务分解为两个独立的抓取，依赖于区域先验或启发式手段，导致泛化能力差且没有稳定性保证。本研究旨在提出一种既能保证稳定性，又能有效避免碰撞并具备良好泛化能力的双臂抓取方法。

Method: 提出DAGDiff框架，用去噪扩散过程在SE(3)xSE(3)空间中直接生成抓取对。利用分类器信号在生成过程中引导，实现面向几何、稳定性和碰撞的多重约束，使生成的抓取符合物理可行性和力闭合条件。

Result: 通过力闭合分析、碰撞检测和大规模物理仿真实验，DAGDiff在各项指标上均优于既往方法，展现出更高的抓取稳定性和鲁棒性。

Conclusion: DAGDiff不仅提升了双臂抓取的稳定性与泛化能力，还可直接应用于之前未见过的真实物体点云，在异构双臂机器人实物操作中达到可靠抓取效果。

Abstract: Reliable dual-arm grasping is essential for manipulating large and complex
objects but remains a challenging problem due to stability, collision, and
generalization requirements. Prior methods typically decompose the task into
two independent grasp proposals, relying on region priors or heuristics that
limit generalization and provide no principled guarantee of stability. We
propose DAGDiff, an end-to-end framework that directly denoises to grasp pairs
in the SE(3) x SE(3) space. Our key insight is that stability and collision can
be enforced more effectively by guiding the diffusion process with classifier
signals, rather than relying on explicit region detection or object priors. To
this end, DAGDiff integrates geometry-, stability-, and collision-aware
guidance terms that steer the generative process toward grasps that are
physically valid and force-closure compliant. We comprehensively evaluate
DAGDiff through analytical force-closure checks, collision analysis, and
large-scale physics-based simulations, showing consistent improvements over
previous work on these metrics. Finally, we demonstrate that our framework
generates dual-arm grasps directly on real-world point clouds of previously
unseen objects, which are executed on a heterogeneous dual-arm setup where two
manipulators reliably grasp and lift them.

</details>


### [195] [Human-like Navigation in a World Built for Humans](https://arxiv.org/abs/2509.21189)
*Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang*

Main category: cs.RO

TL;DR: 本文提出了ReasonNav，一种结合视觉-语言模型（VLM）推理能力的人类仿生导航系统，显著提升机器人在复杂建筑中的高效导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航系统缺乏阅读标志、询问路径等人类常用导航策略，导致其在大环境中效率低下。

Method: ReasonNav使用模块化设计，将人类导航技能（如阅读标志、询问等）通过视觉-语言模型进行集成，并针对导航地标设计了简洁的输入输出抽象，使VLM专注于语言理解和推理。

Result: 在真实与模拟导航任务中，ReasonNav能够利用高级推理能力，更高效地在大型复杂建筑内完成导航任务。

Conclusion: 整合视觉-语言模型推理的导航系统可有效提升机器人在陌生大型环境中的导航能力，为未来机器人自主导航提供了新思路。

Abstract: When navigating in a man-made environment they haven't visited before--like
an office building--humans employ behaviors such as reading signs and asking
others for directions. These behaviors help humans reach their destinations
efficiently by reducing the need to search through large areas. Existing robot
navigation systems lack the ability to execute such behaviors and are thus
highly inefficient at navigating within large environments. We present
ReasonNav, a modular navigation system which integrates these human-like
navigation skills by leveraging the reasoning capabilities of a vision-language
model (VLM). We design compact input and output abstractions based on
navigation landmarks, allowing the VLM to focus on language understanding and
reasoning. We evaluate ReasonNav on real and simulated navigation tasks and
show that the agent successfully employs higher-order reasoning to navigate
efficiently in large, complex buildings.

</details>


### [196] [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](https://arxiv.org/abs/2509.21210)
*Ali Kafili Gavgani,Amin Talaeizadeh,Aria Alasty,Hossein Nejat Pishkenari,Esmaeil Najafi*

Main category: cs.RO

TL;DR: 本研究针对多旋翼飞行器受控能力有限的问题，提出并验证了多种新型全方位定向配置与控制方法，并重点分析了能耗和控制性能。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼由于动力臂配置，无法独立控制姿态与位置，极大限制了其机动性和应用场景。本研究旨在打破这种受限，实现多旋翼的“全向”控制能力。

Method: 提出了数种带有可控螺旋桨轴角的多旋翼新型配置，并系统推导了动力学模型。利用Simscape Multibody仿真平台进行各类系统建模和验证。设计了滑模控制器（增强扰动鲁棒性）和结合线性/非线性分配器的重力补偿PID控制器（提升计算效率），并实现了节能导向的自定义控制分配算法。

Result: 各新配置及控制器有效应对了强扰动和不确定性。通过仿真对比，不同配置和控制器的能耗（“功耗因子”）差异明显，表明了设计优化空间。还定性分析了不同类型不确定性的控制影响，提出了改进硬件和模型的方向。

Conclusion: 本研究为多旋翼全向控制设计与控制器选择提供了理论依据与工程建议，有助于未来全向无人机的研发和优化，尤其在低能耗和高鲁棒性需求场景下潜力巨大。

Abstract: Conventional multi-rotors are under-actuated systems, hindering them from
independently controlling attitude from position. In this study, we present
several distinct configurations that incorporate additional control inputs for
manipulating the angles of the propeller axes. This addresses the mentioned
limitations, making the systems "omniorientational". We comprehensively derived
detailed dynamic models for all introduced configurations and validated by a
methodology using Simscape Multibody simulations. Two controllers are designed:
a sliding mode controller for robust handling of disturbances and a novel
PID-based controller with gravity compensation integrating linear and
non-linear allocators, designed for computational efficiency. A custom control
allocation strategy is implemented to manage the input-non-affine nature of
these systems, seeking to maximize battery life by minimizing the "Power
Consumption Factor" defined in this study. Moreover, the controllers
effectively managed harsh disturbances and uncertainties. Simulations compare
and analyze the proposed configurations and controllers, majorly considering
their power consumption. Furthermore, we conduct a qualitative comparison to
evaluate the impact of different types of uncertainties on the control system,
highlighting areas for potential model or hardware improvements. The analysis
in this study provides a roadmap for future researchers to design
omniorientational drones based on their design objectives, offering practical
insights into configuration selection and controller design. This research
aligns with the project SAC-1, one of the objectives of Sharif AgRoLab.

</details>


### [197] [SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation](https://arxiv.org/abs/2509.21231)
*Jaehwi Jang,Zhuoheng Wang,Ziyi Zhou,Feiyang Wu,Ye Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的、稳定的末端执行器控制框架(SEEC)，通过模型增强的残差学习有效提升仿人机器人手臂末端器件在动态步行中的鲁棒性与精确性。实验证明其在多种仿真环境及真实机器人上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的方法需依赖精确的动力学建模，难以应对实际中的摩擦、间隙等复杂因素；学习型方法虽然能自适应上述实际因素，但普遍存在过拟合和泛化能力弱的问题，难以直接适配新场景。因此，迫切需要一种兼具精确性与适应性的末端器件控制方法。

Method: 提出了SEEC框架，核心思路是将模型增强与残差学习相结合，通过模型引导下的强化学习训练上半身策略，提升末端执行器对下肢扰动的补偿能力。特设扰动生成器丰富扰动类型，强化泛化能力。

Result: 在多种仿真平台对框架进行验证，并将其迁移至Booster T1仿人机器人。实验结果表明该方法在多项复杂任务中均明显优于对比基线，具备更强的鲁棒性和适应性。

Conclusion: SEEC框架能有效提升仿人机器人末端执行器的稳定性和泛化能力，对多样化和高难度的步态操作任务具备强大支持，验证了理论创新与实际应用价值。

Abstract: Arm end-effector stabilization is essential for humanoid loco-manipulation
tasks, yet it remains challenging due to the high degrees of freedom and
inherent dynamic instability of bipedal robot structures. Previous model-based
controllers achieve precise end-effector control but rely on precise dynamics
modeling and estimation, which often struggle to capture real-world factors
(e.g., friction and backlash) and thus degrade in practice. On the other hand,
learning-based methods can better mitigate these factors via exploration and
domain randomization, and have shown potential in real-world use. However, they
often overfit to training conditions, requiring retraining with the entire
body, and still struggle to adapt to unseen scenarios. To address these
challenges, we propose a novel stable end-effector control (SEEC) framework
with model-enhanced residual learning that learns to achieve precise and robust
end-effector compensation for lower-body induced disturbances through
model-guided reinforcement learning (RL) with a perturbation generator. This
design allows the upper-body policy to achieve accurate end-effector
stabilization as well as adapt to unseen locomotion controllers with no
additional training. We validate our framework in different simulators and
transfer trained policies to the Booster T1 humanoid robot. Experiments
demonstrate that our method consistently outperforms baselines and robustly
handles diverse and demanding loco-manipulation tasks.

</details>


### [198] [FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration](https://arxiv.org/abs/2509.21242)
*Yutong Li,Jieyi Zhang,Wenqiang Xu,Tutian Tang,Cewu Lu*

Main category: cs.RO

TL;DR: FSGlove是一种基于惯性测量单元(IMU)的手部动作捕捉系统，可同时追踪高达48个自由度(DoF)、重建个性化手型，并在手部动作捕捉精度及形状还原方面超越商业产品。


<details>
  <summary>Details</summary>
Motivation: 现有手部MoCap系统在关节高自由度追踪和个性化手型还原方面存在局限，尤其在复杂操作和接触性任务中无法满足精度和多样性需求。

Method: FSGlove通过在每个指关节和手背安装IMU实现高分辨率动作感知。引入DiffHCal校准方法，并结合可微优化的MANO参数模型，实现关节运动、个性化手型及传感器对准误差的统一求解。

Result: 该系统在关节角度误差小于2.7°，在形状重建和接触精度上优于主流商业产品；能够捕捉精细的手部动作如指尖摩擦。对比Nokov光学MoCap，提升了运动与接触的统一追踪能力。

Conclusion: FSGlove作为开源软硬件平台，不仅兼容现有VR及机器人生态，还极大缩小了人类手部灵巧度与机器人模仿之间的差距，在高精度手部动作捕捉领域具有重要应用价值。

Abstract: Accurate hand motion capture (MoCap) is vital for applications in robotics,
virtual reality, and biomechanics, yet existing systems face limitations in
capturing high-degree-of-freedom (DoF) joint kinematics and personalized hand
shape. Commercial gloves offer up to 21 DoFs, which are insufficient for
complex manipulations while neglecting shape variations that are critical for
contact-rich tasks. We present FSGlove, an inertial-based system that
simultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes
via DiffHCal, a novel calibration method. Each finger joint and the dorsum are
equipped with IMUs, enabling high-resolution motion sensing. DiffHCal
integrates with the parametric MANO model through differentiable optimization,
resolving joint kinematics, shape parameters, and sensor misalignment during a
single streamlined calibration. The system achieves state-of-the-art accuracy,
with joint angle errors of less than 2.7 degree, and outperforms commercial
alternatives in shape reconstruction and contact fidelity. FSGlove's
open-source hardware and software design ensures compatibility with current VR
and robotics ecosystems, while its ability to capture subtle motions (e.g.,
fingertip rubbing) bridges the gap between human dexterity and robotic
imitation. Evaluated against Nokov optical MoCap, FSGlove advances hand
tracking by unifying the kinematic and contact fidelity. Hardware design,
software, and more results are available at:
https://sites.google.com/view/fsglove.

</details>


### [199] [RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2509.21243)
*Jiyeon Koo,Taewan Cho,Hyunjoon Kang,Eunseom Pyo,Tae Gyun Oh,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: 本论文提出了一种名为RetoVLA的新型轻量级视觉-语言-动作模型架构，通过重复利用原本被丢弃的Register Tokens来增强空间推理能力，并在7自由度机械臂上显著提升了复杂操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型虽然泛化能力强，但因体积庞大和计算成本高，难以实际部署。而传统的减轻模型方法往往会牺牲空间推理等关键能力，导致效率和性能之间存在权衡。作者希望能同时实现高效和优秀性能。

Method: 作者提出直接复用Register Tokens作为空间特征，将其注入到动作专家模块中，从而在保持模型轻量化的基础上，增强模型的空间推理能力。

Result: 在自制的7自由度机械臂上的实验结果显示，RetoVLA模型在复杂操作任务的成功率上绝对提升了17.1个百分点，表明该方法有效提升了空间推理能力和任务表现。

Conclusion: 被视为无用的Register Tokens实际上蕴含有价值的空间信息，通过合理利用它们，可在保证模型轻量化的情况下显著增强机器人智能，为机器人领域的模型设计提供了新思路。

Abstract: Recent Vision-Language-Action (VLA) models demonstrate remarkable
generalization in robotics but are restricted by their substantial size and
computational cost, limiting real-world deployment. However, conventional
lightweighting methods often sacrifice critical capabilities, particularly
spatial reasoning. This creates a trade-off between efficiency and performance.
To address this challenge, our work reuses Register Tokens, which were
introduced for artifact removal in Vision Transformers but subsequently
discarded. We suppose that these tokens contain essential spatial information
and propose RetoVLA, a novel architecture that reuses them directly by
injecting them into the Action Expert.
  RetoVLA maintains a lightweight structure while leveraging this repurposed
spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness
through a series of comprehensive experiments. On our custom-built 7-DOF robot
arm, the model achieves a 17.1%p absolute improvement in success rates for
complex manipulation tasks. Our results confirm that reusing Register Tokens
directly enhances spatial reasoning, demonstrating that what was previously
discarded as an artifact is in fact a valuable, unexplored resource for robotic
intelligence. A video demonstration is available at:
https://youtu.be/2CseBR-snZg

</details>


### [200] [BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives](https://arxiv.org/abs/2509.21256)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: 本文针对机器人非抓取操作（如推、拨、转动）提出了一套双臂非抓取操作基元（BiNoMaP）及无强化学习（RL）三阶段学习框架，在实际多任务与多类别物体测试中表现出高效性和良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作因涉及丰富接触和难以解析的动力学，尤其是在双臂及无外部支持条件下，始终是机器人领域的难点和研究空白。现有很多方法过度依赖单臂配置和外部稳固结构，且普遍依赖强化学习导致样本效率低和泛化性不足。

Method: （1）提出通用双臂操作配置及一套双臂非抓取基元（BiNoMaP）；（2）不用RL，采用三阶段学习流程：从视频示范自动提取运动轨迹-几何感知后处理优化轨迹-基于物体几何属性参数化基元以实现类别泛化。

Result: 在多种代表性双臂非抓取任务和不同类别物体实验中，验证了方法的高效性、有效性、多样性及优越的泛化能力。

Conclusion: 本文提出的BiNoMaP与RL-free三阶段方法不仅避免了强化学习训练不足的缺点，还能适应不同物体类别和任务，有望提升机器人在实际场景下的操作能力和泛用性。

Abstract: Non-prehensile manipulation, encompassing ungraspable actions such as
pushing, poking, and pivoting, represents a critical yet underexplored domain
in robotics due to its contact-rich and analytically intractable nature. In
this work, we revisit this problem from two novel perspectives. First, we move
beyond the usual single-arm setup and the strong assumption of favorable
external dexterity such as walls, ramps, or edges. Instead, we advocate a
generalizable dual-arm configuration and establish a suite of Bimanual
Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the
prevailing RL-based paradigm and propose a three-stage, RL-free framework to
learn non-prehensile skills. Specifically, we begin by extracting bimanual hand
motion trajectories from video demonstrations. Due to visual inaccuracies and
morphological gaps, these coarse trajectories are difficult to transfer
directly to robotic end-effectors. To address this, we propose a geometry-aware
post-optimization algorithm that refines raw motions into executable
manipulation primitives that conform to specific motion patterns. Beyond
instance-level reproduction, we further enable category-level generalization by
parameterizing the learned primitives with object-relevant geometric
attributes, particularly size, resulting in adaptable and general parameterized
manipulation primitives. We validate BiNoMaP across a range of representative
bimanual tasks and diverse object categories, demonstrating its effectiveness,
efficiency, versatility, and superior generalization capability.

</details>


### [201] [\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)](https://arxiv.org/abs/2509.21264)
*Babak Salamat,Dominik Mattern,Sebastian-Sven Olzem,Gerhard Elsbacher,Christian Seidel,Andrea M. Tonello*

Main category: cs.RO

TL;DR: 本文提出了GMP³多阶段全球路径规划框架，实现了在复杂环境中为无人机生成动态可行的三维轨迹，并通过模块化地面控制软件DroneManager将其应用于实际无人机，验证了其在障碍规避和轨迹平滑性上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机路径规划方法主要集中在欧几里得位置空间，难以同时高效处理无人机的平移与旋转动态，尤其在受限空间中难以生成可行的全局平滑轨迹；此外，多无人机协作和路径全局优化也是一个亟需解决的问题。

Method: 1. 将路径规划空间扩展到Lie群SE(3)，可联合优化位置与姿态轨迹。2. 引入改进Bellman算子结合轨迹先验，提升强化学习策略的收敛效率。3. 采用分布式体制，各智能体在轨迹段上局部优化并通过共识机制协同更新，实现全局约束下的合作。4. 开发DroneManager软件，支持与真实无人机通过MAVLink协议交互，便于实际部署。

Result: 在仿真和室内无人机实验中，GMP³展现了在复杂三维限制环境中的高效障碍规避能力、轨迹平滑性与动态可行性，包括姿态和位置的协同规划效果。

Conclusion: GMP³为无人机在复杂受限三维环境中提供了高效可行的全球路径规划解决方案，并通过实际部署验证了其实用性。该方法促进了运动学约束下多无人机分布式协作与路径全局优化的发展。

Abstract: We propose $\text{GMP}^{3}$, a multiphase global path planning framework that
generates dynamically feasible three-dimensional trajectories for unmanned
aerial vehicles (UAVs) operating in cluttered environments. The framework
extends traditional path planning from Euclidean position spaces to the Lie
group $\mathrm{SE}(3)$, allowing joint learning of translational motion and
rotational dynamics. A modified Bellman-based operator is introduced to support
reinforcement learning (RL) policy updates while leveraging prior trajectory
information for improved convergence. $\text{GMP}^{3}$ is designed as a
distributed framework in which agents influence each other and share policy
information along the trajectory: each agent refines its assigned segment and
shares with its neighbors via a consensus-based scheme, enabling cooperative
policy updates and convergence toward a path shaped globally even under
kinematic constraints. We also propose DroneManager, a modular ground control
software that interfaces the planner with real UAV platforms via the MAVLink
protocol, supporting real-time deployment and feedback. Simulation studies and
indoor flight experiments validate the effectiveness of the proposed method in
constrained 3D environments, demonstrating reliable obstacle avoidance and
smooth, feasible trajectories across both position and orientation. The
open-source implementation is available at
https://github.com/Domattee/DroneManager

</details>


### [202] [Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds](https://arxiv.org/abs/2509.21281)
*Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 本文提出了一种能同时保持动作分类层级结构和时间动态一致性的机器人类人动作生成新方法GPHDM，并在手部抓握动作数据集上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人动作生成方法很少利用人体动作的分层结构（如生物力学中的运动分类法），导致生成的动作缺乏与真实人类动作的结构对应性。本文希望通过结合动作层级结构，提高机器人生成动作的自然度和一致性。

Method: 改进了高斯过程动力学模型（GPDM），将其动力学先验扩展到双曲流形，并引入了动作分类法驱动的归纳偏置。基于新的结构，提出了三种动作生成机制：两种递归概率方法和一种基于pullback-metric测地线的生成方法，使生成动作既符合分类层次结构，也具备物理一致性。

Result: 在手部抓握动作的实验中，GPHDM能够准确编码动作的层次结构和时间动态，并能生成新的、物理合理的动作轨迹。

Conclusion: GPHDM能有效结合分类学结构和动作动力学，为生成更自然、结构一致的机器人类人动作提供了有力工具。

Abstract: Human-like motion generation for robots often draws inspiration from
biomechanical studies, which often categorize complex human motions into
hierarchical taxonomies. While these taxonomies provide rich structural
information about how movements relate to one another, this information is
frequently overlooked in motion generation models, leading to a disconnect
between the generated motions and their underlying hierarchical structure. This
paper introduces the \ac{gphdm}, a novel approach that learns latent
representations preserving both the hierarchical structure of motions and their
temporal dynamics to ensure physical consistency. Our model achieves this by
extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to
the hyperbolic manifold and integrating it with taxonomy-aware inductive
biases. Building on this geometry- and taxonomy-aware frameworks, we propose
three novel mechanisms for generating motions that are both
taxonomically-structured and physically-consistent: two probabilistic recursive
approaches and a method based on pullback-metric geodesics. Experiments on
generating realistic motion sequences on the hand grasping taxonomy show that
the proposed GPHDM faithfully encodes the underlying taxonomy and temporal
dynamics, and generates novel physically-consistent trajectories.

</details>
