<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

TL;DR: Lumina-mGPT 2.0是一个从零训练的自回归型单体生成模型，能在多个图像生成任务中达到甚至超越扩散模型的表现，并具备统一多模态生成和高效率解码的能力。


<details>
  <summary>Details</summary>
Motivation: 当前高质量图像生成主要依赖扩散模型或依靠预训练组件、自回归模型，但这些路径要么受限于架构灵活性和授权限制、要么在表现上不及SOTA扩散模型。本文作者希望突破这些限制，打造能媲美甚至超越扩散模型的新型自回归生成基础模型。

Method: Lumina-mGPT 2.0完全从零训练，不依赖任何预训练组件或混合架构，实现了架构和授权的自由。采用统一的tokenization方案，支持多任务（如主体生成、图像编辑、可控合成、密集预测），并引入了推理期缩放和推测性Jacobi采样等高效解码策略提升生成速度和质量。

Result: 在多个标准文本到图像基准（如GenEval、DPG）上，Lumina-mGPT 2.0的生成质量达到或超过扩散模型（如DALL-E 3和SANA）。在Graph200K多任务基准上，模型多任务表现优异。

Conclusion: Lumina-mGPT 2.0作为新一代自回归生成基础模型，不仅在生成质量上与扩散模型媲美，还兼具极强的灵活性、多任务能力和高效推理，适合统一多模态生成的全面场景。代码与模型已开源。

Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [2] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文提出了一种高效、可在设备端部署的体育视频理解模型SV3.3B，通过新颖的时序运动差分采样和自监督学习，实现低计算资源下对运动细节的精准分析。


<details>
  <summary>Details</summary>
Motivation: 现有自动体育视频分析方法对运动细节理解不足，且需要大量算力，难以在设备端实时部署，无法精确识别如准备、执行、随动等关键阶段。

Method: 提出SV3.3B模型，结合了时序运动差分采样和自监督学习，利用DWT-VGG16-LDA机制智能提取16帧关键帧，再用V-DWT-JEPA2编码器（掩码去噪预训练）和LLM解码器（运动动作生成微调）。

Result: 在NSVA篮球数据集上，SV3.3B在文本生成和体育分析专属指标上优于GPT-4o等更大模型，实现了29.2%的地面实况验证指标提升，同时信息密度、动作复杂度、精度等体育分析相关指标均有显著提升。

Conclusion: SV3.3B在大幅降低计算资源需求的同时，实现对体育动作的细致理解和丰富分析描述，为体育视频分析领域带来高效、实用的新工具。

Abstract: This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [3] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了Detail++，一种无需训练的分阶段细化生成方法，能够更好处理包含多主题和复杂属性的文本生成图像任务。通过逐步细化属性绑定，大幅提升图像生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像（T2I）生成模型虽然效果显著，但在处理包含多主题和复杂属性的复杂提示词时表现不佳，主要在于难以精确绑定属性和对应目标，且细节处理容易混淆。作者受人类绘画按步骤分层细化的启发，旨在解决这一现有难题。

Method: 提出Detail++框架，通过一种无训练的渐进式细节注入（PDI）策略，将复杂的提示语拆解为易处理的子提示语，分阶段引导生成过程。首先生成整体布局，再逐步细化。结合交叉注意力机制，并引入质心对齐损失（Centroid Alignment Loss），在测试阶段进一步提升属性与目标的一致性与绑定准确性。

Result: 在T2I-CompBench和新构建的风格组合基准上进行大量实验，结果显示在多对象和复杂风格场景下，Detail++的生成质量与一致性明显优于现有方法。

Conclusion: Detail++有效提升了文本到图像生成模型处理复杂描述的能力，尤其在多主体、多属性和复杂风格场景下表现突出，为高质量和可控图像生成提供了新思路。

Abstract: Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [4] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: 本文提出FishDet-M，这是目前最大的统一鱼类检测基准，整合了13个公开数据集，并进行了全面、系统的模型评测及自适应模型选择方法的探索。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测对于生态监测、水产养殖自动化和机器人感知极为重要。然而，目前实际应用受数据集分散、成像条件多样和评价协议不统一的限制，导致检测模型难以部署于现实场景。

Method: 作者将13个不同环境下的公开鱼类图像数据集进行统一，采用COCO风格的标注标准（包含目标框和分割掩码），并对YOLOv8–YOLOv12、R-CNN、DETR等28种主流检测算法进行了系统性基准测试。测试维度包括mAP、mAP@50、mAP@75及尺度相关指标，并从推理延迟和模型参数量进行效率分析。同时，提出了一种基于CLIP的零样本模型选择框架，用于根据图像内容动态选择最适合的检测器。

Result: 系统评测结果展示了不同架构的检测模型在FishDet-M数据集上的性能差异及效率与准确率的权衡。基于CLIP的模型选择方法实现了无需集成计算条件下的高检测性能，适合实时应用。

Conclusion: FishDet-M为水下目标检测建立了标准化、可复现的评测平台，提升了不同模型在复杂水域场景下的对比与推广价值。所有数据、预训练模型和评测工具均已公开，有助于推动水下视觉与智能海洋系统领域的未来研究。

Abstract: Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [5] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 本研究利用生成式AI模型（LightningDiT）生成高真实性的合成皮肤影像，评估公开黑色素瘤检测模型的公平性，并探讨在评估数据分布与训练数据不匹配时的难点。


<details>
  <summary>Details</summary>
Motivation: 深度学习在边缘医疗设备的应用有望革新皮肤癌筛查（如黑色素瘤），但系统固有偏见可能带来风险。因此，评估与提升其公平性迫切且重要。公平性评估的难点之一在于需要代表性强、涵盖多种个人身份信息（如性别、年龄、种族）和少数群体的数据集。

Method: 本研究利用当前最先进的生成式AI模型LightningDiT，生成高质量的合成皮肤影像数据，用于测试和评估主流公开黑色素瘤分类模型在不同群体上的公平性表现。

Result: 实验结果表明，使用高真实度的合成数据进行公平性评估是一条有前景的方向。但当评估模型训练数据与合成影像背后的数据分布不一致时，公平性验证会变得很困难。

Conclusion: 尽管存在数据分布不匹配带来的挑战，本研究提出的方法为用合成数据评测和加强医疗影像AI系统的公平性提供了新途径，具有重要研究和应用价值。

Abstract: Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [6] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于图像恢复的Transformer架构——DiNAT-IR，结合了膨胀邻域注意力（DiNA）和通道感知模块，有效兼顾图像恢复的效率和质量。


<details>
  <summary>Details</summary>
Motivation: Transformer因其强大的全局建模能力在图像恢复领域表现出色，但其高昂的自注意力计算成本限制了在高分辨率场景的应用。现有的通道注意力方法虽然高效，却容易忽略局部细节，无法很好恢复图像局部瑕疵。因此，亟需一种新方法在提升效率的同时重视局部与全局信息融合。

Method: 作者设计了一种膨胀邻域注意力（DiNA），通过滑动窗口加混合扩张因子扩展感受野，在成本可控的前提下融合全局和局部上下文。同时引入通道感知模块，弥补局部注意力对全局信息建模能力的不足。

Result: 实验结果显示，所提出的DiNAT-IR架构在多个公开图像恢复基准数据集上取得了与现有方法具有竞争力的高质量恢复效果。

Conclusion: DiNAT-IR在提高效率的同时，充分融合了全局与局部信息，对低层视觉任务如图像去模糊等具有良好适应性和高实用价值。

Abstract: Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [7] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Adaptive Feature Refinement（AFR）的新模块，用于提升无监督领域自适应语义分割的精度，特别是在复杂区域的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督领域自适应语义分割方法难以兼顾细粒度的局部细节与全局上下文信息，导致在复杂区域的分割准确率降低。

Method: 提出AFR模块，通过低分辨率预测(Logits)的语义先验对高分辨率特征进行细化，并引入高频成分以捕捉细粒度结构和边界信息。同时采用不确定性驱动的注意力机制自动平衡局部与全局信息，模块设计轻量，可集成进现有HRDA方法。

Result: 在GTA V到Cityscapes和Synthia到Cityscapes的UDA-SS任务上，集成AFR模块的方法分别提升了1.05%和1.04%的mIoU，达到了最新最优分割性能。

Conclusion: AFR模块能够有效提升无监督领域自适应语义分割中复杂区域的分割表现，且简单易于集成，对现有主流框架具有较强的实用价值。

Abstract: In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [8] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 本文提出了一套专门针对老年患者虚拟学习环境下参与度识别的AI数据集（OPEN），并在此基础上训练模型，实现了高达81%的虚拟互动参与度识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟学习和远程康复领域的参与度自动识别方法大多局限于年轻群体，且缺乏面向老年人的数据资源；同时，现有方法往往忽视了上下文关联和跨多次会话的动态变化。为促进老年人虚拟场景下的个性化互动与干预，有必要建立相关数据集和评测基线。

Method: 作者招募11名老年参与者，在为期六周的虚拟心脏康复小组学习中采集了35小时的人体骨骼、面部、手部关节点等视觉特征及情感、行为指标，标注了参与度状态及多种上下文信息。未公开原始视频，以保护隐私。之后，作者以该数据集为基础，训练了多种机器学习和深度学习模型用于自动化识别虚拟学习情境下的参与度。

Result: 所训练的模型在OPEN数据集上实现了最高81%的自动化参与度识别准确率。

Conclusion: OPEN数据集为老年人虚拟参与度的AI研究奠定了基础，支持大规模、个性化的参与建模，有助于推动更广泛的虚拟环境下的参与度识别研究。

Abstract: Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>


### [9] [Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring](https://arxiv.org/abs/2507.17987)
*Arsen Yermukan,Pedro Machado,Feliciano Domingos,Isibor Kennedy Ihianle,Jordan J. Bird,Stefano S. K. Kaburu,Samantha J. Ward*

Main category: cs.CV

TL;DR: 本论文提出了一种利用YOLO系列目标检测模型，通过视频实时分析自动检测胡须龙（Pogona Viticeps）两种关键行为（晒太阳和捕食）的系统。结果显示YOLOv8s在准确率和速度间表现最佳，可大幅提升爬行动物行为监测的效率与数据质量。


<details>
  <summary>Details</summary>
Motivation: 传统的胡须龙行为监测费时且易出错，亟需一种自动化、准确性高的方法，以提升研究效率和数据可靠性。

Method: 作者基于包含1200张图片的自制公开数据集，分别训练了五种YOLO模型（v5, v7, v8, v11, v12），标注对象为胡须龙、加热灯与蟋蟀。系统以YOLO模型从视频逐帧提取坐标、再通过时序插值和规则逻辑判别具体行为。

Result: YOLOv8s模型以mAP@0.5:0.95=0.855在各版本中表现最优，实现了稳定的晒太阳检测；但对于捕食检测，因蟋蟀检测mAP仅为0.392，准确率不足。

Conclusion: 自动化系统可显著提升实验室爬行动物行为监测的效率和数据质量。未来提升重点在于扩充数据集或采用专门的小目标检测器以改善蟋蟀检测能力。

Abstract: Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.

</details>


### [10] [AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID](https://arxiv.org/abs/2507.17995)
*Huy Nguyen,Kien Nguyen,Akila Pemasiri,Akmal Jahan,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出了首个空地跨模态（RGB-红外）视频行人再识别数据集AG-VPReID.VIR，并设计了针对该任务的三路流特征学习架构TCC-VPReID。实验验证了方法的有效性和数据集的挑战性。


<details>
  <summary>Details</summary>
Motivation: 目前的行人再识别数据集主要聚焦于地面视角，忽略了“空地协同”及RGB-红外跨模态的需求。传统地面红外监控存在易被遮挡、覆盖范围有限等问题，而空中视角可弥补此不足。因此亟需一个涉及空地多模态的新数据集和有效方法。

Method: 1）构建AG-VPReID.VIR: 利用无人机与固定CCTV采集RGB及红外视频，含1837个身份、4861条轨迹。2）提出TCC-VPReID: 三支路特征学习，结合风格鲁棒性、记忆适配和中介式时序建模，减小空地视角与模态差异。

Result: 新数据集AG-VPReID.VIR展现出更复杂、更具挑战性的跨模态识别场景。TCC-VPReID在多个评测协议下显著优于现有方法。

Conclusion: 提出的AG-VPReID.VIR数据集推动了空地跨模态Re-ID研究，TCC-VPReID有效缓解了视角、模态等域间差异，在此领域具有较强应用潜力。

Abstract: Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.

</details>


### [11] [Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification](https://arxiv.org/abs/2507.17996)
*Emma A. M. Stanley,Raghav Mehta,Mélanie Roschewitz,Nils D. Forkert,Ben Glocker*

Main category: cs.CV

TL;DR: 本论文探讨了医学影像数据中不同子群体标签偏差（label bias）对深度学习模型公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 医学影像中子群体标签系统性错标的问题被低估，而这直接影响医学AI系统的公平性。作者希望揭示子群体的规模和可分性如何影响模型学习到的特征及其性能。

Method: 利用EMBED乳腺影像数据集，针对基于影像制造商划分的可分子群体和“伪子群体”，人为引入标签偏差，训练二分类模型，分析模型特征分布及分类性能，并对比验证集标签是否干净对结果的影响。

Result: 发现标签偏差会改变模型学习到的子群体特征表示，这种变化取决于子群体的规模和可分性。此外，若验证集本身也带有标签偏差，分类阈值设定会进一步恶化子群体性能，例如主流可分子群体的真阳性率从0.898降至0.518。

Conclusion: 标签偏差会对医学影像AI子群体的公平性造成显著影响，研究强调了在实践中应重视和缓解标签偏差问题。

Abstract: Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.

</details>


### [12] [Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](https://arxiv.org/abs/2507.17998)
*Jaeho Shin,Hyeonjae Gil,Junwoo Jang,Maani Ghaffari,Ayoung Kim*

Main category: cs.CV

TL;DR: 本论文提出了一个基于仿射Grassmann流形的、可优化的距离函数，并首次实现了其在刚体变换下的显示优化，提升了特征配准的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有用Grassmann流形衡量线/面之间距离的方法，仅能测度接近性，无法获得刚体变换的显式距离函数，使得该理论难以真正应用于配准（Registration）任务。因此需要将距离函数与刚体变换参数结合，实现在流形上的可优化目标函数。

Method: 作者首次推导了Grassmann特征间关于刚体变换（旋转$f{R}$与平移$f{t}$）的可优化代价函数，并通过严密的数学论证，证明高维子空间基底可作为代价的显式表示。最终提出基于变换基底的优化代价函数，能够直接最小化测地距离并规避表示歧义。还将方法扩展到最大化内点集的BnB求解器中。

Result: 所提出方法可应用于任意仿射子空间的配准问题，与基于向量参数的方法相比，能直接在流形上找到全局最优解。实验证明新方法在多个计算机视觉任务中提升了解的收敛性或超越了现有方案。

Conclusion: 本论文首次给出基于Grassmann流形表征特征间刚体变换的可优化距离函数，为配准等实际任务提供了理论基础和方法工具，并在实验中表现出优越性，推动了该领域实际应用发展。

Abstract: Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.

</details>


### [13] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: 本文提出了GRR-CoCa模型，对现有的SOTA多模态生成模型CoCa进行了结构改进，并在多项任务上显著优于原模型。


<details>
  <summary>Details</summary>
Motivation: 当前领先的多模态生成模型架构在结构创新上落后于最新的大型语言模型（LLMs），因此希望通过引入LLM中的先进结构，提升多模态模型表现。

Method: 在CoCa模型基础上，将高斯误差门控线性单元、均方根归一化和旋转位置编码引入文本解码器和视觉Transformer（ViT）编码器，并与仅在文本解码器修改的Baseline CoCa做对比，采用标准预训练与微调流程，在多个任务上评价表现。

Result: GRR-CoCa在预训练和三个不同的微调数据集上均显著超越Baseline CoCa。预训练时，对比损失降低27.25%、困惑度降低3.71%、CoCa损失降低7.15%；微调时，对比损失平均降低13.66%、困惑度降低5.18%、CoCa损失降低5.55%。

Conclusion: GRR-CoCa结构上的改进显著提升了多模态模型在视觉-语言领域的表现和泛化能力。

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [14] [Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics](https://arxiv.org/abs/2507.18015)
*Yuezun Li,Delong Zhu,Xinjie Cui,Siwei Lyu*

Main category: cs.CV

TL;DR: 本文提出了新的大规模视频DeepFake数据集Celeb-DF++，涵盖了三种常见伪造场景以及22种主流DeepFake生成方法，用以推动更具泛化能力的DeepFake检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有DeepFake检测数据集伪造类型有限，导致检测方法难以泛化到未见过的新型伪造手法。因此亟需更大且类型更丰富的数据集，以支撑泛化性更强的检测算法开发。

Method: 在原有Celeb-DF数据集基础上，构建了包含Face-swap、Face-reenactment和Talking-face三类伪造场景以及22种最新DeepFake方法产生视频的Celeb-DF++数据集，并制定了24种检测方法的评测协议，系统性评估方法泛化性能。

Result: 实验表明，24种最新DeepFake检测方法在Celeb-DF++上的泛化性能普遍有限，凸显了当前检测技术在应对伪造多样性时的挑战和局限。

Conclusion: Celeb-DF++为研究更具泛化能力的DeepFake检测方法提供了标准且挑战性的数据基准，有助于推动该领域的发展，同时揭示了当前方法在伪造类型多样下面临的困难。

Abstract: The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.

</details>


### [15] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D高斯修补（inpainting）框架，能够通过稀疏修补的视图重建完整的3D场景，在多项主流数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3D Gaussian Splatting等方法极大提升了3D重建与新视图合成的质量与效率，但3D场景修补依旧面临多视图一致性难以保证及3D结构不规则的问题。为解决这些核心挑战，作者提出新的3D修补框架。

Method: 提出了结合自动掩码精细化和区域不确定性引导优化的3D高斯修补框架。首先，利用高斯场景滤波与反投影等操作实时修正修补掩码，实现遮挡区域更准确的定位与边界还原；之后，通过估算训练时多视角下各区域的重要性，精细优化不确定区域，从而提升多视图一致性及细节还原能力。

Result: 在多个不同的数据集上，提出方法在视觉质量和视角一致性上均超过了当前最优的相关方法。

Conclusion: 作者的方法有效提升了3D场景修补的质量和一致性，这一通用框架有望为实际复杂3D内容创作提供有力支持。

Abstract: Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.

</details>


### [16] [Emotion Recognition from Skeleton Data: A Comprehensive Survey](https://arxiv.org/abs/2507.18026)
*Haifeng Lu,Jiuyi Chen,Zhen Zhang,Ruida Liu,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本综述论文系统梳理了基于骨骼的身体动作进行情感识别的方法和进展，包括心理模型、公开数据集、方法分类、技术比较以及在心理健康等领域的应用和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别主要依赖面部表情或生理信号，存在隐私等问题。随着3D骨骼捕捉和姿态估算技术的发展，通过身体动作实现情感识别越来越可行且具备隐私优势。因此，亟需一次系统性综述来整理现有方法与挑战。

Method: 论文从心理学理论入手，梳理了动作与情绪的关联，详细总结了公开数据集及其标签方式，将方法分为姿态类与步态类，并在此基础上提出统一技术范式，包括传统方法、Feat2Net、FeatFusionNet和End2EndNet，并回顾和比较了各类别代表性工作及基准测试结果。

Result: 综述方法系统比较了不同技术路线下的代表性方法在公开数据集上的表现，归纳了各自优缺点，并展示了骨骼动作情感识别在心理健康检测（如抑郁和自闭症）中的扩展应用。

Conclusion: 骨骼动作情感识别方法具备良好的隐私性和应用前景，但仍存在如情感标签标准、泛化能力和数据多样性等挑战。未来需要进一步完善理论、提升算法泛化性、拓展实际应用场景。

Abstract: Emotion recognition through body movements has emerged as a compelling and
privacy-preserving alternative to traditional methods that rely on facial
expressions or physiological signals. Recent advancements in 3D skeleton
acquisition technologies and pose estimation algorithms have significantly
enhanced the feasibility of emotion recognition based on full-body motion. This
survey provides a comprehensive and systematic review of skeleton-based emotion
recognition techniques. First, we introduce psychological models of emotion and
examine the relationship between bodily movements and emotional expression.
Next, we summarize publicly available datasets, highlighting the differences in
data acquisition methods and emotion labeling strategies. We then categorize
existing methods into posture-based and gait-based approaches, analyzing them
from both data-driven and technical perspectives. In particular, we propose a
unified taxonomy that encompasses four primary technical paradigms: Traditional
approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works
within each category are reviewed and compared, with benchmarking results
across commonly used datasets. Finally, we explore the extended applications of
emotion recognition in mental health assessment, such as detecting depression
and autism, and discuss the open challenges and future research directions in
this rapidly evolving field.

</details>


### [17] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: ViGText提出了一种将图像视觉信息和大语言模型文本解释结合、基于图神经网络的深度伪造检测方法，显著提升了模型的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的发展，传统检测方法在应对复杂定制化伪造时面临泛化能力差、易受恶意攻击等问题，亟需创新手段来提升检测精度和适应性，保障媒体的真实性。

Method: ViGText方法将图像划分为小块，结合大语言模型生成的详细文本解释，通过构建图像、文本及其融合的图结构，用图神经网络（GNN）进行多层次的特征提取，涵盖空间和频域，实现对深度伪造内容的有效识别。

Result: ViGText实验表现突出，对泛化能力的提升尤为明显：F1平均分由72.45%提升到98.32%；在鲁棒性方面，相较于其他检测方法召回率提升11.1%；即使遭遇针对图结构的攻击，性能下降不超过4%。

Conclusion: ViGText通过引入文本与视觉的细粒度融合分析，极大提升了深度伪造检测的准确性和鲁棒性，为媒体内容的真实性与信息安全树立了新标准。

Abstract: The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.

</details>


### [18] [DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition](https://arxiv.org/abs/2507.18444)
*Haiyang Jiang,Songhao Piao,Chao Gao,Lei Yu,Liguo Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合DSFormer和创新块聚类策略的新视觉地点识别框架，大幅提升了跨环境和视角下的鲁棒性，并减少了30%的训练数据需求，性能超过当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别(VPR)系统在环境和视角变化剧烈时难以保持高性能，因此亟需提升模型适应性和训练效率。

Method: 1) 提出DSFormer，基于Transformer的双尺度特征交叉学习模块，对CNN末两层提取的语义及空间特征进行双向融合。2) 创新块聚类策略，对SF-XL训练集从多视角进行重分割，优化数据组织以增强模型的视角鲁棒性。

Result: 在大部分基准数据集上实现了当前最好性能。新框架作为全局检索方案，512维度描述符下，优于DELG、Patch-NetVLAD、TransVPR和R2Former等先进方法，并显著提升了计算效率。

Conclusion: 提出的DSFormer和分块聚类方法显著提升了VPR在多变环境中的鲁棒性和效率，同时减少了训练数据需求，为视觉地点识别提供了更高效、强大的解决方案。

Abstract: Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.

</details>


### [19] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: 本文提出了Transition-Aware Video (TAV)数据集，以解决现有文本生成视频模型缺乏场景转换感知的问题。实验显示，基于TAV数据集进行训练可以提升模型对多场景视频提示内容的理解和生成能力，同时保持较高的画质。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频生成主要聚焦于单一场景的短视频，难以应对包含多个场景转换的长视频生成。主要原因是模型无法根据提示自动识别何时需要场景转换，而训练数据也多为单场景视频，缺少多场景转换的标注和学习机会。多场景分割和场景转换检测在生成连贯、复杂视频时非常重要。

Method: 提出并构建了Transition-Aware Video（TAV）数据集，该数据集精心预处理，包含多个场景转换的视频片段。通过在该数据集上对模型进行后训练，强化其对场景转换提示的感知和响应能力。

Result: 实验表明，使用TAV数据集后训练的模型在理解和生成多场景转换的视频方面有显著提升，能够更准确地检测和生成所需的场景转换，同时维持视频帧的图片质量。

Conclusion: 引入多场景转换的数据集能有效提升文本生成视频系统对场景补全和连续性的认知。TAV数据集有望成为推进多场景视频生成研究与应用的重要资源。

Abstract: Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.

</details>


### [20] [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/abs/2507.18060)
*Chengxuan Zhu,Qingnan Fan,Qi Zhang,Jinwei Chen,Huaqi Zhang,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: BokehDiff是一种利用生成扩散模型和物理约束实现高质量镜头虚化（bokeh）渲染的方法，克服了传统方法在深度边界处产生伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 现有镜头虚化渲染方法受限于深度估计算法精度，尤其在深度突变处易产生伪影，影响物理真实性和视觉美感。缺乏高质量配对数据也是一大挑战。

Method: 采用受物理规律启发的自注意力模块，结合深度相关的圆形模糊约束和自遮挡效应，同时将扩散模型适配为无噪声单步推理方式。为解决数据不足，提出用扩散模型合成具有透明度的前景图像以扩充数据多样性。

Result: 方法实现了高质量且真实感强的镜头虚化渲染效果，并有效避免了深度边界伪影。单步推理模型效率高、表现优。

Conclusion: BokehDiff不仅提高了镜头虚化渲染的物理真实性和视觉表现，还为数据不足的问题提供了创新解决方案，具备较高的实际应用价值。

Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves
physically accurate and visually appealing outcomes, with the help of
generative diffusion prior. Previous methods are bounded by the accuracy of
depth estimation, generating artifacts in depth discontinuities. Our method
employs a physics-inspired self-attention module that aligns with the image
formation process, incorporating depth-dependent circle of confusion constraint
and self-occlusion effects. We adapt the diffusion model to the one-step
inference scheme without introducing additional noise, and achieve results of
high quality and fidelity. To address the lack of scalable paired data, we
propose to synthesize photorealistic foregrounds with transparency with
diffusion models, balancing authenticity and scene diversity.

</details>


### [21] [Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement](https://arxiv.org/abs/2507.18064)
*Xiaoran Sun,Liyan Wang,Cong Wang,Yeying Jin,Kin-man Lam,Zhixun Su,Yang Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉-语言大模型（VLM）与迭代和手动指令（IMI）的低照度图像增强新框架VLM-IMI，显著提升了低照度图像增强的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的低照度图像增强方法主要依赖预训练模型先验或输入图像本身，忽略了正常光照图像蕴含的语义指导，导致在复杂光照下效果受限。

Method: VLM-IMI利用了大规模视觉-语言模型，将文本描述作为增强引导，结合一个“指令先验融合模块”，负责动态对齐和融合图像与文本特征，实现语义驱动的图像复原。推理阶段通过迭代和手动调整文本指令持续提升增强效果。

Result: 在多种低照度场景下的大量实验证明，VLM-IMI在定量指标和感知质量上均超过了最新主流方法。

Conclusion: VLM-IMI有效融合了多模态先验和迭代语义优化，大幅提升了低照度图像增强质量，为复杂场景下的图像修复提供了新思路。

Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained
model priors, low-light inputs, or both, while neglecting the semantic guidance
available from normal-light images. This limitation hinders their effectiveness
in complex lighting conditions. In this paper, we propose VLM-IMI, a novel
framework that leverages large vision-language models (VLMs) with iterative and
manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions
of the desired normal-light content as enhancement cues, enabling semantically
informed restoration. To effectively integrate cross-modal priors, we introduce
an instruction prior fusion module, which dynamically aligns and fuses image
and text features, promoting the generation of detailed and semantically
coherent outputs. During inference, we adopt an iterative and manual
instruction strategy to refine textual instructions, progressively improving
visual quality. This refinement enhances structural fidelity, semantic
alignment, and the recovery of fine details under extremely low-light
conditions. Extensive experiments across diverse scenarios demonstrate that
VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and
perceptual quality. The source code is available at
https://github.com/sunxiaoran01/VLM-IMI.

</details>


### [22] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的文本驱动的轻量级EUS胰腺肿瘤分割模型TextSAM-EUS，能在无需人工几何提示的情况下实现自动分割，并显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 全监督深度学习方法分割EUS胰腺肿瘤时对高质量标注数据依赖大，且EUS图像本身噪声较大、对比度低，导致分割效果差。因此急需一种高效、可靠且减少人工参与的新方法。

Method: 该方法基于 Segment Anything Model (SAM)，结合 BiomedCLIP 文本编码器和LoRA微调策略，引入文本提示进行上下文学习，只需微调0.86%的参数，实现无需人工几何提示的自动肿瘤分割。

Result: 在公开的胰腺EUS数据库上，TextSAM-EUS自动提示下Dice为82.69%、NSD为85.28%；手动几何提示下Dice为83.10%、NSD为85.70%，均显著超越现有SOTA模型。

Conclusion: TextSAM-EUS首次在SAM基础上引入prompt learning用于医学图像分割，实现了高效、稳健的自动EUS胰腺肿瘤分割，为临床应用带来新的实践选择。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.

</details>


### [23] [Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover](https://arxiv.org/abs/2507.18099)
*Naman Srivastava,Joel D Joy,Yash Dixit,Swarup E,Rakshit Ramesh*

Main category: cs.CV

TL;DR: 本研究结合大气校正、先进的深度学习方法（如DeeplabV3+、CPS及其改进）对印度海得拉巴市的Cartosat多光谱影像进行土地覆被分类，分析城市化进程下的土地利用变化，以辅助城市规划。


<details>
  <summary>Details</summary>
Motivation: 精准的土地利用／覆被（LULC）制图对于城市和资源规划至关重要，在智能和可持续城市发展中扮演关键角色。然而，如何提升遥感影像下LULC分类的准确率，以及为城市化快速变化提供决策支持，是当前亟待解决的问题。

Method: 首先对Cartosat多光谱传感器数据应用基于查找表（LUT）的实用大气校正方法，提升数据质量。随后使用监督和半监督学习方法（包括DeeplabV3+和交叉伪标签监督CPS），并对CPS模型通过动态加权机制优化伪标签的可靠性，提升分类准确率。以海得拉巴为案例，进行时序影像分析。

Result: 不同方法在LULC制图中的精度进行了评估。通过改进的CPS方法，提升了模型伪标签生成阶段的可靠性，更好地捕捉到城市化带来的土地利用变化，如城市区域扩展、绿地缩减、工业区增长等现象。

Conclusion: 研究证明，结合高质量的数据预处理和先进深度学习模型的LULC制图技术，对城市规划和政策制定具有重要现实意义。所提出的综合方法在实际案例中表现优异，可为快速城市化地区土地利用管理和可持续发展提供科学依据。

Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.

</details>


### [24] [Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning](https://arxiv.org/abs/2507.18100)
*Ruizhe Chen,Zhiting Fan,Tianze Luo,Heqing Zou,Zhaopeng Feng,Guiyang Xie,Hansheng Zhang,Zhuochen Wang,Zuozhu Liu,Huaijian Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合有监督微调和强化学习的两阶段训练框架，以提升视频时序定位（VTG）任务的准确率和泛化能力。实验表明新方法在多个基准数据集上表现优异，尤其在复杂及开放领域场景下。


<details>
  <summary>Details</summary>
Motivation: 目前VTG方法借助大型视觉语言模型和指令微调取得进展，但在时序感知和泛化方面仍存在不足。作者希望通过优化训练策略，提升模型对时序信息的理解和推理能力。

Method: 作者提出两阶段训练框架：第一阶段，利用高质量冷启动数据进行有监督微调（SFT）；第二阶段，引入难度控制的强化学习（RL）策略，进一步提升模型的时序定位和推理能力。

Result: 该方法在多个视频时序定位基准上均优于现有模型，特别是在更具挑战性和开放领域任务中表现更为突出。同时，论文详细分析了训练策略和数据集策划带来的影响。

Conclusion: 高质量冷启动数据和难度可控的强化学习对提升VTG模型的性能至关重要。作者公开了全部中间数据集、模型和代码，以促进后续研究和产业应用。

Abstract: Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.

</details>


### [25] [A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2507.18104)
*Qianyi He,Yuan Chang Leong*

Main category: cs.CV

TL;DR: 本文提出了一种基于序列到序列Transformer的模型，可以从视觉、听觉和语言输入自回归地预测全脑fMRI响应，对自然电影刺激的全脑反应建模表现优异。


<details>
  <summary>Details</summary>
Motivation: 本工作旨在应对Algonauts 2025挑战，即准确预测自然多模态电影刺激下的人类大脑全脑fMRI响应，推动对多模态信息处理机制的理解。

Method: 作者提出了一种序列到序列的Transformer模型，利用VideoMAE、HuBERT、Qwen、BridgeTower等预训练模型提取多模态刺激特征，并通过双重交叉注意力机制综合当前刺激、前序脑状态和剧情级总结，实现神经响应的自回归预测；模型采用共享编码器与部分受试者专属解码器的结构，以兼顾通用性和个体差异。

Result: 所提模型在分布内和分布外的数据上均取得了强劲的脑活动预测性能。

Conclusion: 基于多模态序列上下文的Transformer模型在大脑活动预测中有效，特别是强化了长时程的时序建模和个体差异的兼顾，为脑影像解码提供了新思路。

Abstract: The Algonauts 2025 Challenge called on the community to develop encoding
models that predict whole-brain fMRI responses to naturalistic multimodal
movies. In this submission, we propose a sequence-to-sequence Transformer that
autoregressively predicts fMRI activity from visual, auditory, and language
inputs. Stimulus features were extracted using pretrained models including
VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information
from prior brain states, current stimuli, and episode-level summaries via dual
cross-attention mechanisms that attend to both perceptual information extracted
from the stimulus as well as narrative information provided by high-level
summaries of narrative content. One core innovation of our approach is the use
of sequences of multimodal context to predict sequences of brain activity,
enabling the model to capture long-range temporal structure in both stimuli and
neural responses. Another is the combination of a shared encoder with partial
subject-specific decoder, which leverages common structure across subjects
while accounting for individual variability. Our model achieves strong
performance on both in-distribution and out-of-distribution data, demonstrating
the effectiveness of temporally-aware, multimodal sequence modeling for brain
activity prediction. The code is available at
https://github.com/Angelneer926/Algonauts_challenge.

</details>


### [26] [Distributional Uncertainty for Out-of-Distribution Detection](https://arxiv.org/abs/2507.18106)
*JinYoung Kim,DaeUng Jo,Kimin Yun,Jeonghyo Song,Youngjoon Yoo*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度神经网络框架，用于更准确地估计不确定性并检测分布外（OoD）样本，方法结合自由能和Beta分布，实现了更高效、语义性更强的不确定区域分割。


<details>
  <summary>Details</summary>
Motivation: 现有用于不确定性估计的方法如MC Dropout多只侧重模型或数据不确定性，无法很好地匹配OoD检测的语义目标，导致无法有效识别那些分布外或错误分类的区域。

Method: 作者提出Free-Energy Posterior Network：1）用基于自由能的Beta分布密度估计器，实现对模糊或未知区域的细粒度不确定性度量；2）在后验网络中集成新的损失函数，使得网络无需随机采样、可直接从参数中估计不确定性。同时将该方法融入RPL框架，利用Beta分布方差让网络学习OoD区域。

Result: 在Fishyscapes、RoadAnomaly、Segment-Me-If-You-Can等实际复杂数据集上，所提方法表现优异，实现了有效、语义性强的不确定感知分割。

Conclusion: Free-Energy Posterior Network能够超越传统方法，联合分布不确定性和能量信息，提升了OoD检测和不确定性估计的效率和表现。

Abstract: Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.

</details>


### [27] [T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation](https://arxiv.org/abs/2507.18107)
*Yubin Chen,Xuyang Guo,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 本文提出了T2VWorldBench，这是首个系统化评估文本生成视频（T2V）模型世界知识能力的基准。通过对当前主流T2V模型的评估，发现现有模型在常识和事实一致性方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 虽然T2V模型能生成视觉上合理的视频内容，但它们在利用世界知识保证语义一致性和事实准确性方面尚未被充分研究。因此，亟需评估它们此类能力。

Method: 作者构建了T2VWorldBench评测框架，涵盖6大类、60小类、共1,200个文本提示，内容涉及物理、自然、活动、文化、因果关系和对象。评测结合了人工评价和基于视觉-语言模型的自动评价。

Result: 对10个先进的T2V模型（包括开源和商业模型）进行测试，结果显示大部分模型难以理解和正确生成涵盖世界知识的视频。

Conclusion: 当前T2V模型在处理常识推理和事实生成方面存在明显能力缺口，该基准为未来提升模型知识能力和事实一致性提供了研究方向和突破口。

Abstract: Text-to-video (T2V) models have shown remarkable performance in generating
visually reasonable scenes, while their capability to leverage world knowledge
for ensuring semantic consistency and factual accuracy remains largely
understudied. In response to this challenge, we propose T2VWorldBench, the
first systematic evaluation framework for evaluating the world knowledge
generation abilities of text-to-video models, covering 6 major categories, 60
subcategories, and 1,200 prompts across a wide range of domains, including
physics, nature, activity, culture, causality, and object. To address both
human preference and scalable evaluation, our benchmark incorporates both human
evaluation and automated evaluation using vision-language models (VLMs). We
evaluated the 10 most advanced text-to-video models currently available,
ranging from open source to commercial models, and found that most models are
unable to understand world knowledge and generate truly correct videos. These
findings point out a critical gap in the capability of current text-to-video
models to leverage world knowledge, providing valuable research opportunities
and entry points for constructing models with robust capabilities for
commonsense reasoning and factual generation.

</details>


### [28] [Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy](https://arxiv.org/abs/2507.18135)
*Kesheng Wang,Xiaoyu Chen,Chunlei He,Fenfen Li,Xinxin Yu,Dexing Kong,Shoujun Huang,Qi Dai*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息熵的新型曲线弯曲性量化框架，并验证其在分析睑板腺萎缩一致性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像分析中，曲线弯曲性的精确量化对于疾病的辅助诊断和病理评估至关重要。现有方法（如曲率、弦长比）多以直线为参照，难以满足实际生物参考曲线的需求。

Method: 提出了基于概率建模和熵理论的曲线弯曲性定量框架，引入曲线数据域变换，并通过与生物学意义参考曲线的对比实现评估。首先进行数值仿真验证稳定性，随后应用于睑板腺萎缩一致性的空间量化与群体差异分析。

Result: 该方法能区分蠕形螨阴性与阳性患者萎缩一致性，取得AUC=0.8768，灵敏度0.75，特异性0.93。

Conclusion: 所提框架在医学曲线弯曲性定量领域展现出良好的实用性和广泛应用前景，尤其适用于存在生物参考曲线的场景，可为医学形态学定量评估提供客观指标。

Abstract: In the medical image analysis field, precise quantification of curve
tortuosity plays a critical role in the auxiliary diagnosis and pathological
assessment of various diseases. In this study, we propose a novel framework for
tortuosity quantification and demonstrate its effectiveness through the
evaluation of meibomian gland atrophy uniformity,serving as a representative
application scenario.
  We introduce an information entropy-based tortuosity quantification framework
that integrates probability modeling with entropy theory and incorporates
domain transformation of curve data. Unlike traditional methods such as
curvature or arc-chord ratio, this approach evaluates the tortuosity of a
target curve by comparing it to a designated reference curve. Consequently, it
is more suitable for tortuosity assessment tasks in medical data where
biologically plausible reference curves are available, providing a more robust
and objective evaluation metric without relying on idealized straight-line
comparisons.
  First, we conducted numerical simulation experiments to preliminarily assess
the stability and validity of the method. Subsequently, the framework was
applied to quantify the spatial uniformity of meibomian gland atrophy and to
analyze the difference in this uniformity between \textit{Demodex}-negative and
\textit{Demodex}-positive patient groups. The results demonstrated a
significant difference in tortuosity-based uniformity between the two groups,
with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of
0.93. These findings highlight the clinical utility of the proposed framework
in curve tortuosity analysis and its potential as a generalizable tool for
quantitative morphological evaluation in medical diagnostics.

</details>


### [29] [Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18144)
*Jinhong He,Minglong Xue,Zhipu Liu,Mingliang Zhou,Aoxiang Ning,Palaiahnakote Shivakumara*

Main category: cs.CV

TL;DR: 本文提出了一种用于低照度图像增强的双向扩散优化机制，有效提升了生成图像的质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的低照度图像增强方法普遍只从一个方向建模图像退化，难以应对现实世界复杂且多样的退化模式，导致输出存在结构不一致及像素偏移。

Method: 作者提出双向扩散机制，在训练时同时进行从低照度到正常照度、及从正常照度到低照度的双向扩散建模，并引入自适应特征交互块（AFI）提升特征表征能力。此外，设计反射感知校正模块（RACM）用于颜色纠正与内容一致性保障。通过双向扩散及其互补性，实现对退化过程的精确匹配和一致性约束。

Result: 在多个公开基准数据集上，该方法在定量与定性评估中均优于最新方法，并能很好地泛化到多样化的图像退化场景。

Conclusion: 提出的双向扩散优化机制及模块设计，有效提升了低照度图像的增强效果，生成图像更符合人眼感知，在实际应用中具有较强的泛化能力。

Abstract: Low-light image enhancement aims to improve the visibility of degraded images
to better align with human visual perception. While diffusion-based methods
have shown promising performance due to their strong generative capabilities.
However, their unidirectional modelling of degradation often struggles to
capture the complexity of real-world degradation patterns, leading to
structural inconsistencies and pixel misalignments. To address these
challenges, we propose a bidirectional diffusion optimization mechanism that
jointly models the degradation processes of both low-light and normal-light
images, enabling more precise degradation parameter matching and enhancing
generation quality. Specifically, we perform bidirectional diffusion-from
low-to-normal light and from normal-to-low light during training and introduce
an adaptive feature interaction block (AFI) to refine feature representation.
By leveraging the complementarity between these two paths, our approach imposes
an implicit symmetry constraint on illumination attenuation and noise
distribution, facilitating consistent degradation learning and improving the
models ability to perceive illumination and detail degradation. Additionally,
we design a reflection-aware correction module (RACM) to guide color
restoration post-denoising and suppress overexposed regions, ensuring content
consistency and generating high-quality images that align with human visual
perception. Extensive experiments on multiple benchmark datasets demonstrate
that our method outperforms state-of-the-art methods in both quantitative and
qualitative evaluations while generalizing effectively to diverse degradation
scenarios. Code at https://github.com/hejh8/BidDiff

</details>


### [30] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种融合可见光（RGB）与红外（IR）图像的多模态目标检测方法WaveMamba，通过小波变换高效融合两者的频域特征，并在检测头中集成逆小波变换（IDWT）以减少信息损失。实验结果在四个基准数据集上取得了4.5%的平均mAP提升。


<details>
  <summary>Details</summary>
Motivation: RGB与红外图像各自拥有感知上的互补优势，二者有效融合有望显著提升目标检测性能。然而如何充分提取和利用两种模态的互补频域特征，仍存在难以高效融合、信息损失严重等挑战。

Method: 作者提出WaveMamba方法，核心是WaveMamba Fusion Block（WMFB），利用离散小波变换（DWT）将RGB、IR图像分解为低频和高频子带，并对低频特征通过基于Mamba的LMFB模块（含通道交换和门控注意力深度融合），对高频特征采用绝对最大值融合法。最后，改进检测头引入IDWT减少特征重建时的信息损失。

Result: 在四个公开基准数据集上实验，WaveMamba方法在平均mAP上较当前先进方法提升了4.5%，表现出显著的效果提升。

Conclusion: WaveMamba展现出强大的多模态信息融合能力，优于现有方法，验证了频域融合和改进检测头的有效性。这为多模态目标检测提供了新的技术方案。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [31] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 本论文提出了一种针对FPGA优化的YOLOv5实时目标检测和分类系统，兼顾高精度与资源效率，实现了在边缘端的低功耗高准确度目标检测。


<details>
  <summary>Details</summary>
Motivation: 尽管已有基于YOLO等深度学习方法的目标检测在准确性和速度上表现优秀，但在面向FPGA的边缘设备上仍难以做到资源高效。本文意在解决现有方法FPGA部署时的资源利用不足问题。

Method: 作者基于YOLOv5模型，针对边缘FPGA平台进行了优化改进，训练采用COCO和GTSRD数据集，并在Xilinx Kria KV260 FPGA开发板上实现与测试。

Result: 系统实现了99%的分类准确率，功耗为3.5W，帧率达到9帧每秒。

Conclusion: 实验结果显示，所提方法可以在边缘计算设备上实现高效的实时目标检测与分类，具备较好的资源利用率和实际应用潜力。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


### [32] [Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling](https://arxiv.org/abs/2507.18176)
*Abhishek Kaushik,Norbert Haala,Uwe Soergel*

Main category: cs.CV

TL;DR: 本文提出了一种无监督领域自适应（UDA）框架，通过对3D激光雷达点云分割在不同领域（如传感器类型或地理位置）下的性能退化问题进行有效处理，显著提升在目标无标签数据集上的分割精度。


<details>
  <summary>Details</summary>
Motivation: 3D激光雷达语义分割在自动驾驶等领域非常重要，但模型在不同领域环境下精度会显著下降，而对新领域进行人工标注数据成本极高，因此急需无监督方法提升模型迁移能力。

Method: 方法分为两阶段：首先利用无监督对比学习，在片段级别预训练主干网络，从而获得领域不变的特征；随后提出多模型伪标签策略，集成多种先进结构（如投影、体素、混合、圆柱体方法）对目标域进行推理，结果通过硬投票生成高质量伪标签，之后用生成的伪标签对网络进行微调。

Result: 在从SemanticKITTI向SemanticPOSS和SemanticSlamantic等无标签数据集适应实验中，所提方法相较于直接迁移及单一模型的UDA方案，分割精度均有显著提升。

Conclusion: 将对比学习与多模型集成伪标签策略相结合，在无需目标域标注的情况下，能有效缩小复杂领域间的差距。这一方法为3D激光雷达点云领域的领域自适应开辟了新的思路。

Abstract: Addressing performance degradation in 3D LiDAR semantic segmentation due to
domain shifts (e.g., sensor type, geographical location) is crucial for
autonomous systems, yet manual annotation of target data is prohibitive. This
study addresses the challenge using Unsupervised Domain Adaptation (UDA) and
introduces a novel two-stage framework to tackle it. Initially, unsupervised
contrastive learning at the segment level is used to pre-train a backbone
network, enabling it to learn robust, domain-invariant features without labels.
Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing
an ensemble of diverse state-of-the-art architectures (including projection,
voxel, hybrid, and cylinder-based methods). Predictions from these models are
aggregated via hard voting to generate high-quality, refined pseudo-labels for
the unlabeled target domain, mitigating single-model biases. The contrastively
pre-trained network is then fine-tuned using these robust pseudo-labels.
Experiments adapting from SemanticKITTI to unlabeled target datasets
(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in
segmentation accuracy compared to direct transfer and single-model UDA
approaches. These results highlight the effectiveness of combining contrastive
pre-training with refined ensemble pseudo-labeling for bridging complex domain
gaps without requiring target domain annotations.

</details>


### [33] [Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios](https://arxiv.org/abs/2507.18177)
*Dhruv Jain,Romain Modzelewski,Romain Hérault,Clement Chatelain,Eva Torfeh,Sebastien Thureau*

Main category: cs.CV

TL;DR: 提出了一种新型医学图像分割架构Diff-UMamba，通过降噪模块与长距离依赖机制，有效提升了小样本下分割的准确性和鲁棒性，在多个公开和内部数据集上优于主流基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在数据有限时易过拟合噪声与无关特征，降低了对未见样本的泛化能力，尤其影响医学图像分割任务，因此亟需方法提高在小样本场景下的表现。

Method: 在UNet基础上结合了“mamba”机制建模长距离依赖；核心创新是设计了噪声抑制模块（NRM），通过信号差分策略，在编码器阶段过滤掉噪声和无关激活，从而强化与任务相关的特征表达。

Result: 在多个公开数据集（MSD肺、胰腺、AIIB23）和内部NSCLC数据集中，与主流基线法对比，分割准确率提升1-5%；在BraTS-21等有限数据条件下，也展现出较强的鲁棒性和性能提升。

Conclusion: Diff-UMamba利用降噪与长距离依赖机制，有效抑制噪声并增强临床相关特征表达，在医学图像小样本分割任务中显著提升准确性和泛化性，具备潜在实用价值。

Abstract: In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.

</details>


### [34] [MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation](https://arxiv.org/abs/2507.18184)
*Hoang Hai Nam Nguyen,Phan Nguyen Duc Hieu,Ho Won Lee*

Main category: cs.CV

TL;DR: MatSSL是一种集成门控特征融合的自监督学习架构，可在小型无标签数据集上预训练，并在金属材料显微组织分析任务中取得了优于传统方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有金属材料微观图像分析方法主要依赖有监督学习，对新数据集需重新训练，且小样本下效果不稳定；而多数自监督方法又依赖大规模数据集。作者希望设计一种能够在小规模无标签数据下依然有效的自监督学习方法。

Method: 提出MatSSL架构，在骨干网络各阶段引入门控特征融合机制，首先在小型无标签数据集上进行自监督预训练，再在多组基准数据集上微调，以适应不同的金属材料分割任务。

Result: MatSSL在MetalDAM数据集上分割模型达到69.13%的mIoU，优于ImageNet预训练编码器的66.73%；在EBC基准数据集上，平均mIoU可比MicroNet预训练模型提升近40%。

Conclusion: MatSSL能够用较少的无标签数据实现有效预训练，提升在金属材料领域的分割效果，同时保留大规模自然图像预训练得到的通用、可迁移特征。

Abstract: MatSSL is a streamlined self-supervised learning (SSL) architecture that
employs Gated Feature Fusion at each stage of the backbone to integrate
multi-level representations effectively. Current micrograph analysis of
metallic materials relies on supervised methods, which require retraining for
each new dataset and often perform inconsistently with only a few labeled
samples. While SSL offers a promising alternative by leveraging unlabeled data,
most existing methods still depend on large-scale datasets to be effective.
MatSSL is designed to overcome this limitation. We first perform
self-supervised pretraining on a small-scale, unlabeled dataset and then
fine-tune the model on multiple benchmark datasets. The resulting segmentation
models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an
ImageNet-pretrained encoder, and delivers consistently up to nearly 40%
improvement in average mIoU on the Environmental Barrier Coating benchmark
dataset (EBC) compared to models pretrained with MicroNet. This suggests that
MatSSL enables effective adaptation to the metallographic domain using only a
small amount of unlabeled data, while preserving the rich and transferable
features learned from large-scale pretraining on natural images.

</details>


### [35] [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/abs/2507.18192)
*Minghao Fu,Guo-Hua Wang,Xiaohao Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为TeEFusion的新型蒸馏方法，用于提升文本到图像生成模型的推理效率，同时保持高质量输出。实验表明，该方法能实现高达6倍的推理加速，且图像质量与教师模型相当。


<details>
  <summary>Details</summary>
Motivation: 当前主流的文本到图像生成方法依赖复杂的采样策略和无分类器引导（CFG），虽然保证了生成质量，但推理成本过高，严重影响模型实际应用效率。因此，亟需高效且高质量的模型推理方法。

Method: TeEFusion方法将引导强度直接融合进文本嵌入，通过线性操作结合条件和非条件文本嵌入，以此重建CFG引导效果。此外，采用高效的蒸馏方法，让学生模型学习教师模型的复杂采样策略，无需添加额外参数。

Result: 在SD3等最新模型上的大量实验显示，学生模型以远简化的采样策略接近教师模型的生成性能，同时推理速度提升最高可达6倍，图像质量几乎无损。

Conclusion: TeEFusion方法能显著简化文本到图像生成模型的推理流程，提高效率，同时维持高质量输出。该方法在工业应用中具有很高的推广价值。

Abstract: Recent advances in text-to-image synthesis largely benefit from sophisticated
sampling strategies and classifier-free guidance (CFG) to ensure high-quality
generation. However, CFG's reliance on two forward passes, especially when
combined with intricate sampling algorithms, results in prohibitively high
inference costs. To address this, we introduce TeEFusion (\textbf{Te}xt
\textbf{E}mbeddings \textbf{Fusion}), a novel and efficient distillation method
that directly incorporates the guidance magnitude into the text embeddings and
distills the teacher model's complex sampling strategy. By simply fusing
conditional and unconditional text embeddings using linear operations,
TeEFusion reconstructs the desired guidance without adding extra parameters,
simultaneously enabling the student model to learn from the teacher's output
produced via its sophisticated sampling approach. Extensive experiments on
state-of-the-art models such as SD3 demonstrate that our method allows the
student to closely mimic the teacher's performance with a far simpler and more
efficient sampling strategy. Consequently, the student model achieves inference
speeds up to 6$\times$ faster than the teacher model, while maintaining image
quality at levels comparable to those obtained through the teacher's complex
sampling approach. The code is publicly available at
\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.

</details>


### [36] [LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation](https://arxiv.org/abs/2507.18214)
*Qilin Huang,Tianyu Lin,Zhiguang Chen,Fudan Zheng*

Main category: cs.CV

TL;DR: 本文提出了LEAF，一种基于潜在扩散模型（latent diffusion model）的医学图像分割方法，通过调整训练过程和特征提取策略，提升了分割准确度且无额外计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在医学图像分割中有效，但往往沿用标准噪声预测流程，未针对分割任务调整，同时预训练模型在特征提取方面仍有不足。作者希望解决这些不足，提升分割表现。

Method: 1）将原本用于预测噪声的训练目标直接替换为预测分割图，降低结果方差；2）通过特征蒸馏方法，将卷积层隐状态与基于Transformer视觉编码器的特征对齐。训练中进行模型微调，测试阶段不增加参数和计算量。

Result: 在多种不同疾病类型的医学图像分割数据集上，作者提出的方法均显著提升了扩散模型的分割准确率。

Conclusion: LEAF能有效提升医学图像分割性能，无需增加模型复杂度和推理计算，参数高效且实用。

Abstract: Leveraging the powerful capabilities of diffusion models has yielded quite
effective results in medical image segmentation tasks. However, existing
methods typically transfer the original training process directly without
specific adjustments for segmentation tasks. Furthermore, the commonly used
pre-trained diffusion models still have deficiencies in feature extraction.
Based on these considerations, we propose LEAF, a medical image segmentation
model grounded in latent diffusion models. During the fine-tuning process, we
replace the original noise prediction pattern with a direct prediction of the
segmentation map, thereby reducing the variance of segmentation results. We
also employ a feature distillation method to align the hidden states of the
convolutional layers with the features from a transformer-based vision encoder.
Experimental results demonstrate that our method enhances the performance of
the original diffusion model across multiple segmentation datasets for
different disease types. Notably, our approach does not alter the model
architecture, nor does it increase the number of parameters or computation
during the inference phase, making it highly efficient.

</details>


### [37] [3D Test-time Adaptation via Graph Spectral Driven Point Shift](https://arxiv.org/abs/2507.18225)
*Xin Wei,Qin Yang,Yijie Fang,Mingrui Zhu,Nannan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D点云测试时自适应方法GSDTTA，通过在图谱域进行自适应，有效提升效率并减少计算资源消耗，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D测试时自适应方法大多依赖于高计算开销的空间域优化，且常常需要额外训练数据。而3D点云由于不规则和无序的结构，现有方法的应用受限。因此亟需更高效、无需额外数据的新方法。

Method: 提出GSDTTA方法，将3D点云表示为离群感知的图，并通过图傅里叶变换(GFT)转换到图谱域。仅优化最低10%的频率分量来实现高效适应，然后通过逆GFT重建适应后的点云。此外，采用特征向量引导的自训练策略，迭代改进谱域调整和模型参数。

Result: 在多个基准数据集上的实验和消融研究显示，GSDTTA优于现有3D点云测试时自适应方法。

Conclusion: GSDTTA以更高效的谱域自适应方式显著提升了3D点云分类的自适应能力，为实际应用提供了更优解。

Abstract: While test-time adaptation (TTA) methods effectively address domain shifts by
dynamically adapting pre-trained models to target domain data during online
inference, their application to 3D point clouds is hindered by their irregular
and unordered structure. Current 3D TTA methods often rely on computationally
expensive spatial-domain optimizations and may require additional training
data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation
(GSDTTA), a novel approach for 3D point cloud classification that shifts
adaptation to the graph spectral domain, enabling more efficient adaptation by
capturing global structural properties with fewer parameters. Point clouds in
target domain are represented as outlier-aware graphs and transformed into
graph spectral domain by Graph Fourier Transform (GFT). For efficiency,
adaptation is performed by optimizing only the lowest 10% of frequency
components, which capture the majority of the point cloud's energy. An inverse
GFT (IGFT) is then applied to reconstruct the adapted point cloud with the
graph spectral-driven point shift. This process is enhanced by an
eigenmap-guided self-training strategy that iteratively refines both the
spectral adjustments and the model parameters. Experimental results and
ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,
outperforming existing TTA methods for 3D point cloud classification.

</details>


### [38] [DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception](https://arxiv.org/abs/2507.18237)
*Chengchang Tian,Jianwei Ma,Yan Huang,Zhanye Chen,Honghao Wei,Hui Zhang,Wei Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为DATA的网络，旨在提升协同感知过程中特征融合的有效性，通过域和时间对齐方式减缓输入特征因差异化和延迟带来的累积退化。


<details>
  <summary>Details</summary>
Motivation: 协同感知中，特征级融合虽然兼顾性能和带宽，但高度依赖输入特征质量。现实部署时，由硬件多样性和环境导致的域差异，以及通信延迟导致的时间错位，会连续性地降低特征质量。因此，如何在保持低带宽的同时，提升特征质量，是一个亟需解决的问题。

Method: 作者提出了DATA网络，包括：1) 一致性保持域对齐模块（CDAM），通过分层下采样与观测约束判别器，减小特征的域间差异；2) 进阶的时间对齐模块（PTAM），采用多尺度运动建模和双阶段补偿，缓解通信迟滞带来的时间错配；3) 面向实例的特征聚合模块（IFAM），进一步增强了对齐后特征的语义表达能力。

Result: 在三个常用协同感知数据集上，DATA网络取得了领先的性能。尤其在严苛的通信延迟和位姿误差条件下，表现出较强的鲁棒性。

Conclusion: 针对协同感知中特征融合易受域差异和时间错配影响的问题，DATA网络通过系统性的对齐和增强，有效提升了融合性能和系统鲁棒性，达到了最新的性能水平。

Abstract: Feature-level fusion shows promise in collaborative perception (CP) through
balanced performance and communication bandwidth trade-off. However, its
effectiveness critically relies on input feature quality. The acquisition of
high-quality features faces domain gaps from hardware diversity and deployment
conditions, alongside temporal misalignment from transmission delays. These
challenges degrade feature quality with cumulative effects throughout the
collaborative network. In this paper, we present the Domain-And-Time Alignment
(DATA) network, designed to systematically align features while maximizing
their semantic representations for fusion. Specifically, we propose a
Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps
through proximal-region hierarchical downsampling and observability-constrained
discriminator. We further propose a Progressive Temporal Alignment Module
(PTAM) to handle transmission delays via multi-scale motion modeling and
two-stage compensation. Building upon the aligned features, an Instance-focused
Feature Aggregation Module (IFAM) is developed to enhance semantic
representations. Extensive experiments demonstrate that DATA achieves
state-of-the-art performance on three typical datasets, maintaining robustness
with severe communication delays and pose errors. The code will be released at
https://github.com/ChengchangTian/DATA.

</details>


### [39] [DepthDark: Robust Monocular Depth Estimation for Low-Light Environments](https://arxiv.org/abs/2507.18243)
*Longjian Zeng,Zunjie Zhu,Rongfeng Lu,Ming Lu,Bolun Zheng,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 提出了DepthDark模型，以提升在低光环境下的单目深度估计能力，通过仿真夜间成像和有效微调策略取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计基础模型主要针对白天场景，在低光环境中表现大幅下降，缺乏专门针对低光环境的基础模型，且训练数据和高效微调方法不足。

Method: 提出了融合耀斑和噪声仿真的图像生成模块，以模拟夜间成像流程，生成高质量低光配对数据集。设计了结合照明引导和多尺度特征融合的高效低光微调策略（PEFT）。

Result: 在nuScenes-Night和RobotCar-Night等低光深度估计数据集上取得了最先进的性能，在有限训练数据和算力条件下表现优秀。

Conclusion: DepthDark实现了低光单目深度估计性能的显著提升，解决了数据和微调策略短板，为低光场景下的深度感知任务提供了有力的基础模型。

Abstract: In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.

</details>


### [40] [LONG3R: Long Sequence Streaming 3D Reconstruction](https://arxiv.org/abs/2507.18255)
*Zhuoguang Chen,Minghui Qin,Tianyuan Yuan,Zhe Liu,Hang Zhao*

Main category: cs.CV

TL;DR: 本文提出了LONG3R，一种适用于长序列流式多视角3D重建的模型，可在保持实时速度下优化3D重建效果，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视角场景重建方法在处理输入图像流时，受限于离线优化耗时或仅能处理短序列，无法满足实时与长序列场景需求。

Method: 提出LONG3R模型，通过递归操作处理每帧输入，采用记忆门控机制筛选相关记忆，并与新观测共同输入双源精细解码器，实现粗到细交互；创新性地引入3D时空记忆，动态裁剪冗余空间信息并自适应分辨率调整，同时采用两阶段课程训练策略提升长序列表现及训练效率。

Result: 在多项实验中，LONG3R在长序列场景下的性能超过当前主流流式3D重建方法，同时能够保持实时推理速度。

Conclusion: LONG3R解决了长序列流式3D重建的主要难题，为实时、长时多视角3D重建任务提供了更优的技术选择。

Abstract: Recent advancements in multi-view scene reconstruction have been significant,
yet existing methods face limitations when processing streams of input images.
These methods either rely on time-consuming offline optimization or are
restricted to shorter sequences, hindering their applicability in real-time
scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D
Reconstruction), a novel model designed for streaming multi-view 3D scene
reconstruction over longer sequences. Our model achieves real-time processing
by operating recurrently, maintaining and updating memory with each new
observation. We first employ a memory gating mechanism to filter relevant
memory, which, together with a new observation, is fed into a dual-source
refined decoder for coarse-to-fine interaction. To effectively capture
long-sequence memory, we propose a 3D spatio-temporal memory that dynamically
prunes redundant spatial information while adaptively adjusting resolution
along the scene. To enhance our model's performance on long sequences while
maintaining training efficiency, we employ a two-stage curriculum training
strategy, each stage targeting specific capabilities. Experiments demonstrate
that LONG3R outperforms state-of-the-art streaming methods, particularly for
longer sequences, while maintaining real-time inference speed. Project page:
https://zgchen33.github.io/LONG3R/.

</details>


### [41] [Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection](https://arxiv.org/abs/2507.18260)
*Junyao Li,Yahao Lu,Xingyuan Guo,Xiaoyu Xian,Tiantian Wang,Yukai Shi*

Main category: cs.CV

TL;DR: 本文提出了一种针对红外小目标检测（ISTD）在高质量数据稀缺情况下提升鲁棒性的创新方法，结合高斯采样压缩与扩散模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前ISTD领域大多依赖大量昂贵的手工标注数据进行学习，而实际应用中高质量的红外图像数据稀缺，导致现有方法在真实环境下极为脆弱。因此，研究者亟需突破对高质量数据的依赖，提升检测方法对数据稀缺的适应性和鲁棒性。

Method: 作者首先分析主流方法在红外高质量数据稀缺情况下检测性能的变化。随后提出Gaussian Agnostic Representation Learning方法，核心是Gaussian Group Squeezer：利用高斯采样与非均匀量化压缩增强量化信号的多样性，并结合两阶段扩散模型进行真实重建，使合成样本更接近真实分布。通过多样化训练样本显著增强ISTD模型应对挑战的能力。

Result: 实验在多种数据稀缺场景下，将所提方法与多种主流ISTD方法进行对比，结果表明本方法在鲁棒性和检测效果上均显著优于现有技术。

Conclusion: 所提基于高斯采样压缩与扩散模型的自适应表示学习方法，能在训练数据匮乏时显著提升红外小目标检测的鲁棒性和效果，对实际复杂场景下的ISTD具有重要应用价值。

Abstract: Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.

</details>


### [42] [Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis](https://arxiv.org/abs/2507.18287)
*Wenran Zhang,Huihuan Luo,Linda Wei,Ping Nie,Yiqun Wu,Dedong Yu*

Main category: cs.CV

TL;DR: 本研究利用孟德尔随机化方法，探讨了口腔疾病（龋齿、牙周炎）与肺癌之间的因果关系。结果显示龋齿明显增加肺癌，特别是鳞状细胞癌的风险，部分通过肺功能下降介导。牙周炎与肺癌无明显因果关系。


<details>
  <summary>Details</summary>
Motivation: 以前的研究发现口腔疾病与肺癌有关，但多为观察性研究，存在因果关系不确定的问题。因此，作者希望通过遗传工具，更准确地探究二者间的因果联系。

Method: 采用两样本孟德尔随机化（MR）方法，基于大规模全基因组关联分析（GWAS）数据；通过逆方差加权法分析因果关系，通过delta法评估肺功能在其中的中介作用。

Result: 研究发现龋齿发生率每增加1标准差，鳞状细胞肺癌风险增加188%（OR=2.880），其中肺功能（FVC、FEV1）下降分别解释了约5%的总效应。牙周炎未发现与肺癌存在因果关系。

Conclusion: 龋齿对肺癌（尤其是鳞癌）有明确因果作用，部分因其导致肺功能下降。结果提示应将口腔卫生与肺功能监测纳入癌症防控策略。

Abstract: Periodontitis and dental caries are common oral diseases affecting billions
globally. While observational studies suggest links between these conditions
and lung cancer, causality remains uncertain. This study used two sample
Mendelian randomization (MR) to explore causal relationships between dental
traits (periodontitis, dental caries) and lung cancer subtypes, and to assess
mediation by pulmonary function. Genetic instruments were derived from the
largest available genome wide association studies, including data from 487,823
dental caries and 506,594 periodontitis cases, as well as lung cancer data from
the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance
weighting was the main analytical method; lung function mediation was assessed
using the delta method. The results showed a significant positive causal effect
of dental caries on overall lung cancer and its subtypes. Specifically, a one
standard deviation increase in dental caries incidence was associated with a
188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =
1.236--6.713, p = 0.014), partially mediated by declines in forced vital
capacity (FVC) and forced expiratory volume in one second (FEV1), accounting
for 5.124% and 5.890% of the total effect. No causal effect was found for
periodontitis. These findings highlight a causal role of dental caries in lung
cancer risk and support integrating dental care and pulmonary function
monitoring into cancer prevention strategies.

</details>


### [43] [LMM-Det: Make Large Multimodal Models Excel in Object Detection](https://arxiv.org/abs/2507.18300)
*Jincheng Li,Chunyu Xie,Ji Ao,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了LMM-Det方法，通过简单的调整提升大型多模态模型（LMM）在目标检测任务上的表现，无需依赖专门的检测模块。实验证明该方法有效缩小了LMM与专用检测器在目标检测上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 近年来LMM因其多模态理解与推理能力备受关注，但在目标检测方面仍不如专用检测器。作者希望通过改进，使LMM在无需额外检测模块的情况下也能胜任目标检测任务。

Method: 作者对LMM用于目标检测进行了分析，发现召回率比专门检测器低，因此提出通过数据分布调整和推理优化提升召回率，并重新组织指令对话以增强检测能力。最终提出了LMM-Det方法。

Result: 通过大量实验证明，LMM-Det能显著提升LMM的目标检测能力，与专用检测器的性能差距明显缩小。

Conclusion: LMM自身具备目标检测能力，无需额外检测模块即可胜任目标检测任务，LMM-Det是一种简单高效且实际有效的方法。

Abstract: Large multimodal models (LMMs) have garnered wide-spread attention and
interest within the artificial intelligence research and industrial
communities, owing to their remarkable capability in multimodal understanding,
reasoning, and in-context learning, among others. While LMMs have demonstrated
promising results in tackling multimodal tasks like image captioning, visual
question answering, and visual grounding, the object detection capabilities of
LMMs exhibit a significant gap compared to specialist detectors. To bridge the
gap, we depart from the conventional methods of integrating heavy detectors
with LMMs and propose LMM-Det, a simple yet effective approach that leverages a
Large Multimodal Model for vanilla object Detection without relying on
specialized detection modules. Specifically, we conduct a comprehensive
exploratory analysis when a large multimodal model meets with object detection,
revealing that the recall rate degrades significantly compared with specialist
detection models. To mitigate this, we propose to increase the recall rate by
introducing data distribution adjustment and inference optimization tailored
for object detection. We re-organize the instruction conversations to enhance
the object detection capabilities of large multimodal models. We claim that a
large multimodal model possesses detection capability without any extra
detection modules. Extensive experiments support our claim and show the
effectiveness of the versatile LMM-Det. The datasets, models, and codes are
available at https://github.com/360CVGroup/LMM-Det.

</details>


### [44] [Improving Large Vision-Language Models' Understanding for Field Data](https://arxiv.org/abs/2507.18311)
*Xiaomei Zhang,Hanyu Zheng,Xiangyu Zhu,Jinghuan Wei,Junhong Zou,Zhen Lei,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出了FieldLVLM框架，通过专门的特征提取和压缩策略，提升大规模视觉-语言模型在科学领域现场数据上的理解与表现。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型（LVLMs）虽表现优异，但在科学领域特别是现场复杂数据（如自然科学中的流场等）理解方面尚属空白，因此有必要提升LVLM在此类场景下的适应能力。

Method: FieldLVLM包含两大核心：1）领域感知的语言生成策略，利用定制的机器学习流程提取流场等数据的关键物理特征，并转化为结构化文本，形成新数据集；2）数据压缩的多模态模型调优，采用压缩方法输入至LVLM，仅保留最有信息量的特征，确保与模型解码器兼容并提升学习效率。

Result: 在新提出的基准数据集上，FieldLVLM在涉及科学现场数据的多种任务中，显著优于现有主流方法。

Conclusion: FieldLVLM显著提升了大模型在科学领域（特别是现场数据处理）中的表现，为LVLM在科学研究中的应用和领域探索打开新可能。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive capabilities
across a range of tasks that integrate visual and textual understanding, such
as image captioning and visual question answering. These models are trained on
large-scale image and video datasets paired with text, enabling them to bridge
visual perception and natural language processing. However, their application
to scientific domains, especially in interpreting complex field data commonly
used in the natural sciences, remains underexplored. In this work, we introduce
FieldLVLM, a novel framework designed to improve large vision-language models'
understanding of field data. FieldLVLM consists of two main components: a
field-aware language generation strategy and a data-compressed multimodal model
tuning. The field-aware language generation strategy leverages a
special-purpose machine learning pipeline to extract key physical features from
field data, such as flow classification, Reynolds number, and vortex patterns.
This information is then converted into structured textual descriptions that
serve as a dataset. The data-compressed multimodal model tuning focuses on
LVLMs with these generated datasets, using a data compression strategy to
reduce the complexity of field inputs and retain only the most informative
values. This ensures compatibility with the models language decoder and guides
its learning more effectively. Experimental results on newly proposed benchmark
datasets demonstrate that FieldLVLM significantly outperforms existing methods
in tasks involving scientific field data. Our findings suggest that this
approach opens up new possibilities for applying large vision-language models
to scientific research, helping bridge the gap between large models and
domain-specific discovery.

</details>


### [45] [A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation](https://arxiv.org/abs/2507.18323)
*Minje Park,Jeonghwa Lim,Taehyung Yu,Sunghoon Joo*

Main category: cs.CV

TL;DR: 本文提出了首个系统性的心电图（ECG）半监督语义分割（SemiSeg）基准，通过整合多个公开数据集，评测代表性算法，并展示了transformer结构在该任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 心电图分界对临床诊断至关重要，但受限于公开标注数据集的稀缺，深度学习方法进展缓慢。半监督学习能充分利用大量无标注ECG数据，有望突破数据瓶颈。

Method: 作者整合并统一了多个公开心电图数据集，选取了五种计算机视觉领域代表性半监督分割算法，在卷积网络和transformer两个架构上分别实现，并于同域与跨域场景下进行评测。同时，提出针对ECG的训练配置和增强方法，并构建了标准化评估体系。

Result: 实验结果显示，transformer在半监督ECG分割任务中表现优于经典卷积网络结构。

Conclusion: 该基准为半监督心电图分界方法后续研究提供了系统评价平台，有助于推动该领域技术进步。

Abstract: Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.

</details>


### [46] [Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm](https://arxiv.org/abs/2507.18327)
*Jiangjun Peng,Yisi Luo,Xiangyong Cao,Shuang Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种经过修改的核范数（MNN）框架，通过对变换后的矩阵进行核范数操作，实现了对局部和全局低秩信息的联合建模，无需参数权衡，理论上保证Robust PCA和矩阵补全等任务的精确求解，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的核范数方法虽然适用于矩阵恢复问题，但难以同时兼顾数据的局部信息和全局低秩结构，且常需参数调节。现有联合建模局部与全局信息的方法缺乏精确的理论保证。

Method: 通过引入合适的变换，将矩阵映射到新空间，再对变换后的矩阵应用核范数，从而定义出一类新的MNN范数。该方法实现了在无需调节权衡参数的情况下，联合建模局部特征和全局低秩结构。并在温和假设下给出了理论恢复保证。

Result: 实验结果表明，MNN方法在Robust PCA和矩阵补全等任务上效果突出，优于现有结合局部和全局信息的方法。提供了相关代码以便复现。

Conclusion: MNN框架通过灵活的变换设计统一了低秩结构恢复问题，能够理论保证效果且实际表现优异。为结构化低秩恢复问题提供了一种通用且有效的解决思路。

Abstract: The nuclear norm (NN) has been widely explored in matrix recovery problems,
such as Robust PCA and matrix completion, leveraging the inherent global
low-rank structure of the data. In this study, we introduce a new modified
nuclear norm (MNN) framework, where the MNN family norms are defined by
adopting suitable transformations and performing the NN on the transformed
matrix. The MNN framework offers two main advantages: (1) it jointly captures
both local information and global low-rankness without requiring trade-off
parameter tuning; (2) Under mild assumptions on the transformation, we provided
exact theoretical recovery guarantees for both Robust PCA and MC tasks-an
achievement not shared by existing methods that combine local and global
information. Thanks to its general and flexible design, MNN can accommodate
various proven transformations, enabling a unified and effective approach to
structured low-rank recovery. Extensive experiments demonstrate the
effectiveness of our method. Code and supplementary material are available at
https://github.com/andrew-pengjj/modified_nuclear_norm.

</details>


### [47] [GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences](https://arxiv.org/abs/2507.18330)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Franck Ballerini,Stephania-Denisa Bocu*

Main category: cs.CV

TL;DR: 论文提出了一个地面可见光相机卷云序列（GVCCS）数据集，实现了对飞机尾迹卷云的逐个标注和时序追踪，并构建了一个统一的深度学习分析框架。该工作有助于改进对航空非CO2气候影响的监测和物理模型校准。


<details>
  <summary>Details</summary>
Motivation: 航空业带来的气候影响不仅限于CO2，还包括飞机尾迹卷云等非CO2效应，但目前模型准确性受限于大气输入数据质量和复杂过程的建模假设。现有观测数据集缺乏时序追踪与航班归因，阻碍了物理模型的验证和校准。因此需要新的高质量观测数据和分析方法。

Method: 论文构建了一个新的开放数据集GVCCS，采用全景地面相机记录和追踪尾迹卷云，并为每条尾迹提供航班信息。此外，提出统一深度学习网络，实现语义分割、实例分割及时序追踪三大功能。

Result: 数据集包含122段视频，共24228帧，逐条卷云标注并关联航班号。深度学习框架能在同一结构下实现尾迹像素分辨、单体分割和随时间追踪。研究提供了高质量、时序分辨的标注和模型评测基准。

Conclusion: 该数据集和分析框架有助于提升对航空尾迹的监测能力，并为物理模型的校准提供科学依据，促进对航空非CO2气候影响的更准确理解和评估。

Abstract: Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.

</details>


### [48] [Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction](https://arxiv.org/abs/2507.18331)
*Runmin Zhang,Zhu Yu,Si-Yuan Cao,Lingyu Zhu,Guangyi Zhang,Xiaokai Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SGCDet提出了一种新的多视角室内3D目标检测方法，通过自适应构建稀疏3D体素，有效提升了检测性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D检测方法在体素的感受野设计上过于固定，无法灵活整合多视图间的几何与上下文信息，且在体素构建上计算冗余高，降低了效率。

Method: SGCDet提出几何与上下文感知聚合模块，自适应调整不同视图的特征融合方式，提升体素表征能力；同时引入稀疏体素构建策略，仅对高占用概率的体素进行细化，减少自由空间上的冗余计算。此外，SGCDet仅需3D框标注监督，无需精确几何真值。

Result: SGCDet在ScanNet、ScanNet200和ARKitScenes等公开数据集上取得了当前最优的3D目标检测指标，验证了其性能与泛化能力。

Conclusion: SGCDet通过自适应体素构建与多视图特征融合，在保证检测效果的同时显著提高了计算效率，并降低了对场景几何标注的依赖。

Abstract: This work presents SGCDet, a novel multi-view indoor 3D object detection
framework based on adaptive 3D volume construction. Unlike previous approaches
that restrict the receptive field of voxels to fixed locations on images, we
introduce a geometry and context aware aggregation module to integrate
geometric and contextual information within adaptive regions in each image and
dynamically adjust the contributions from different views, enhancing the
representation capability of voxel features. Furthermore, we propose a sparse
volume construction strategy that adaptively identifies and selects voxels with
high occupancy probabilities for feature refinement, minimizing redundant
computation in free space. Benefiting from the above designs, our framework
achieves effective and efficient volume construction in an adaptive way. Better
still, our network can be supervised using only 3D bounding boxes, eliminating
the dependence on ground-truth scene geometry. Experimental results demonstrate
that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200
and ARKitScenes datasets. The source code is available at
https://github.com/RM-Zhang/SGCDet.

</details>


### [49] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为SynC的新框架，通过重新分配更语义匹配的图像-文本对，优化零样本图像描述任务中的合成数据，提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前零样本图像描述任务（ZIC）多借助文本生成图像（T2I）模型生成的合成数据以减少人工标注成本。然而，合成数据常存在语义不对齐问题（如缺失物体或属性错误），导致训练样本噪声大，使模型性能受限。现有剪除噪声数据的方法主要针对文本噪声，对于合成数据面临的图像质量问题效果有限。因此，迫切需要专门为ZIC合成数据剪优的新方法。

Method: 提出SynC框架，不进行常规的过滤或重新生成，而是将每个文本描述分配给当前图像池中与其语义最一致的图像。具体做法为：对每个文本，检索多个候选相关图像，然后用循环一致性评分法（即通过图像反检索原文本）选出最佳匹配图像，实现更强的语义对齐。

Result: 在MS-COCO、Flickr30k、NoCaps等标准数据集上的大量实验结果表明，SynC能显著提升多种ZIC模型的表现，在多个指标下达到最新最优水平。

Conclusion: SynC为ZIC合成数据的高效优选提供了一种有效策略，极大提高了零样本图像描述的性能和数据质量。

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


### [50] [Improving Bird Classification with Primary Color Additives](https://arxiv.org/abs/2507.18334)
*Ezhini Rasendiran R,Chandresh Kumar Maurya*

Main category: cs.CV

TL;DR: 本文提出了一种通过将频率信息用原色加权嵌入到声谱图中来提升鸟类鸣叫分类准确率的方法。该方法能够有效区分不同物种的相似音型（motif），在BirdCLEF 2024任务中大幅优于现有最佳方案。


<details>
  <summary>Details</summary>
Motivation: 鸟类鸣叫分类面临噪声、声音重叠和标签缺失等难题，现有方法在低信噪比及多物种混杂录音下效果不佳。为了提高不同物种间相似音型的可分辨性，作者提出增强频率信息的设计。

Method: 将鸟鸣音频转为声谱图后，使用原色加权将不同频率的信息以色彩方式嵌入图像，强化关键信息。再应用深度学习模型进行分类。通过与不加色的模型及BirdCLEF 2024冠军方案对比评估方法效果。

Result: 所提出方法在BirdCLEF 2024任务上，F1提升7.3%，ROC-AUC提升6.2%，CMAP提升6.6%，相较未加色方案和冠军模型均有显著提升。

Conclusion: 将频率信息以颜色嵌入声谱图显著提升了鸟鸣分类的区分度和准确率，为复杂环境下的生物声纹识别提供了有效手段。

Abstract: We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.

</details>


### [51] [EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs](https://arxiv.org/abs/2507.18342)
*Yuping He,Yifei Huang,Guo Chen,Baoqi Pei,Jilan Xu,Tong Lu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 该论文提出了EgoExoBench，这是首个面向第一人称（主观视角）与第三人称（客观视角）视频理解与推理的基准测试，并系统评估了现有多模态大型语言模型在跨视角推理上的表现。结果显示，当前模型在跨视角协同与时序推理方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 人类智能能够灵活转换和整合不同视角（如第一人称与第三人称）的知识，这对于学习与交流至关重要。但目前多模态大模型是否具备类似能力尚未被探索。因此，亟需有系统的评测来推动相关研究。

Method: 作者构建了EgoExoBench基准，汇集了公开数据集，包含7300多个问答对，覆盖11个子任务。这些任务被划分为三大核心挑战：语义对齐、视角关联、时间推理。随后，作者用13个最先进的MLLMs（多模态大模型）在该基准上进行系统评估。

Result: 实验证明，当前主流MLLMs虽然在单一视角理解任务上表现优越，但在跨视角的语义对齐、视角关联、时序动态推理等任务上表现不佳。

Conclusion: EgoExoBench为研究多模态模型在跨视角智能推理领域提供了重要工具，有望推动具有人类般跨视角推理能力的智能体与助理系统的开发。

Abstract: Transferring and integrating knowledge across first-person (egocentric) and
third-person (exocentric) viewpoints is intrinsic to human intelligence,
enabling humans to learn from others and convey insights from their own
experiences. Despite rapid progress in multimodal large language models
(MLLMs), their ability to perform such cross-view reasoning remains unexplored.
To address this, we introduce EgoExoBench, the first benchmark for
egocentric-exocentric video understanding and reasoning. Built from publicly
available datasets, EgoExoBench comprises over 7,300 question-answer pairs
spanning eleven sub-tasks organized into three core challenges: semantic
alignment, viewpoint association, and temporal reasoning. We evaluate 13
state-of-the-art MLLMs and find that while these models excel on single-view
tasks, they struggle to align semantics across perspectives, accurately
associate views, and infer temporal dynamics in the ego-exo context. We hope
EgoExoBench can serve as a valuable resource for research on embodied agents
and intelligent assistants seeking human-like cross-view intelligence.

</details>


### [52] [VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation](https://arxiv.org/abs/2507.18348)
*Ioannis Sarridis,Christos Koutlis,Symeon Papadopoulos,Christos Diou*

Main category: cs.CV

TL;DR: 该论文提出了一个名为VB-Mitigator的开源框架，用于统一、标准化地进行计算机视觉偏见缓解方法的开发与评估，从而推动该领域研究的进展。


<details>
  <summary>Details</summary>
Motivation: 目前计算机视觉模型中的偏见问题普遍且严重，导致AI系统的不公平、不可靠与难以泛化。虽然偏见缓解技术发展迅速，但由于实现分散、评估方式不一致，阻碍了学术进步和方法公平对比。现有研究中使用的数据集和指标各异，难以复现和横向比较，使得评估现有与新方法的有效性成为难题。

Method: 作者提出VB-Mitigator开源框架，统一了偏见缓解方法的开发、评估和比较分析环境。框架集成了12种主流缓解方法与7个基准数据集，并具有良好的可扩展性，便于后续加入新方法、数据集、评测指标与模型。同时，框架也为研究者推荐了最佳评估实践。

Result: VB-Mitigator实现了多种偏见缓解方法和多样化评估，通过一致的平台进行全面、可复现的方法性能对比，推动了该领域的公平、系统性评估。实验结果表明，不同偏见缓解方法在多个数据集上的性能表现、优势和不足都得以公正、细致地体现出来。

Conclusion: VB-Mitigator为计算机视觉偏见缓解研究提供了一个统一、可扩展的基础设施，有助于标准化评估流程、提升研究公正性，并加快公平性相关技术的发展。该框架及其实践建议能极大地促进社区对偏见问题的理解和解决。

Abstract: Bias in computer vision models remains a significant challenge, often
resulting in unfair, unreliable, and non-generalizable AI systems. Although
research into bias mitigation has intensified, progress continues to be
hindered by fragmented implementations and inconsistent evaluation practices.
Disparate datasets and metrics used across studies complicate reproducibility,
making it difficult to fairly assess and compare the effectiveness of various
approaches. To overcome these limitations, we introduce the Visual Bias
Mitigator (VB-Mitigator), an open-source framework designed to streamline the
development, evaluation, and comparative analysis of visual bias mitigation
techniques. VB-Mitigator offers a unified research environment encompassing 12
established mitigation methods, 7 diverse benchmark datasets. A key strength of
VB-Mitigator is its extensibility, allowing for seamless integration of
additional methods, datasets, metrics, and models. VB-Mitigator aims to
accelerate research toward fairness-aware computer vision models by serving as
a foundational codebase for the research community to develop and assess their
approaches. To this end, we also recommend best evaluation practices and
provide a comprehensive performance comparison among state-of-the-art
methodologies.

</details>


### [53] [Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation](https://arxiv.org/abs/2507.18354)
*Lexuan Zhu,Yuxuan Li,Yuning Ren*

Main category: cs.CV

TL;DR: 本文提出了一种新型可变形卷积模块，引入注意力和前馈网络用于偏移学习，能更好地捕捉全局特征，尤其适用于视网膜血管分割，并在多个公开数据集上取得了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: 传统的可变形卷积只能局部调整卷积核采样，对复杂全局或远距离特征（如视网膜血管的自相似且复杂边缘）表现有限。解决这一局限，以提升医学图像分割的准确性和泛化能力。

Method: 提出了一种结合注意力和前馈网络的新型可变形卷积模块，能学习亚像素的位移场，实现对所有通道特征图的自适应变形，并实现卷积采样区域与网络结构的解耦。基于此模块，设计了GDCUnet模型用于视网膜血管分割。

Result: GDCUnet在公开视网膜血管分割数据集上表现优异，达到了最优的性能。消融实验显示该模块能更好地学习复杂的血管特征，提升模型表征和泛化能力。

Conclusion: 所提出的可变形卷积模块不仅提升了医学图像分割的性能，且具备通用性，可广泛应用于具有复杂全局自相似特征的计算机视觉任务。

Abstract: Deformable convolution can adaptively change the shape of convolution kernel
by learning offsets to deal with complex shape features. We propose a novel
plug and play deformable convolutional module that uses attention and
feedforward networks to learn offsets, so that the deformable patterns can
capture long-distance global features. Compared with previously existing
deformable convolutions, the proposed module learns the sub pixel displacement
field and adaptively warps the feature maps across all channels rather than
directly deforms the convolution kernel , which is equivalent to a relative
deformation of the kernel sampling grids, achieving global feature deformation
and the decoupling of kernel size and learning network. Considering that the
fundus blood vessels have globally self similar complex edges, we design a deep
learning model for fundus blood vessel segmentation, GDCUnet, based on the
proposed convolutional module. Empirical evaluations under the same
configuration and unified framework show that GDCUnet has achieved state of the
art performance on public datasets. Further ablation experiments demonstrated
that the proposed deformable convolutional module could more significantly
learn the complex features of fundus blood vessels, enhancing the model
representation and generalization capabilities.The proposed module is similar
to the interface of conventional convolution, we suggest applying it to more
machine vision tasks with complex global self similar features.

</details>


### [54] [MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image](https://arxiv.org/abs/2507.18371)
*Xiaotian Chen,DongFu Yin,Fei Richard Yu,Xuanchen Li,Xinhao Zhang*

Main category: cs.CV

TL;DR: 本文提出MVG4D，一个能从单张静态图片生成动态4D内容的新框架，实现了高保真和时序一致性的4D生成。核心创新是结合多视图合成和4D高斯Splatting，强化了结构细节和动静一致性。实验证明其性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽在2D到4D内容生成方面有显著进展，但动态4D内容（兼顾高保真和时序一致）仍存难题，具体问题如运动不连续、背景退化等，影响用户在AR/VR等场景下的沉浸体验。

Method: MVG4D框架首先利用图片矩阵模块合成多视图图片，增强时序和空间多样性，为后续3D/4D重建提供丰富监督信号。随后，这些多视图图片用于优化3D高斯点云，并通过一个轻量级形变网络扩展到时间域，实现4D内容生成。

Result: 在Objaverse数据集上的大量实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率等指标上优于现有主流方法，有效减少了闪烁伪影，提升了结构细节的清晰度。

Conclusion: MVG4D有效解决了基于4D高斯Splatting方法存在的运动不连续与背景退化等问题，为仅需极少输入即可高效、可控生成4D动态内容开辟了新方向。

Abstract: Advances in generative modeling have significantly enhanced digital content
creation, extending from 2D images to complex 3D and 4D scenes. Despite
substantial progress, producing high-fidelity and temporally consistent dynamic
4D content remains a challenge. In this paper, we propose MVG4D, a novel
framework that generates dynamic 4D content from a single still image by
combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,
MVG4D employs an image matrix module that synthesizes temporally coherent and
spatially diverse multi-view images, providing rich supervisory signals for
downstream 3D and 4D reconstruction. These multi-view images are used to
optimize a 3D Gaussian point cloud, which is further extended into the temporal
domain via a lightweight deformation network. Our method effectively enhances
temporal consistency, geometric fidelity, and visual realism, addressing key
challenges in motion discontinuity and background degradation that affect prior
4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate
that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and
time efficiency. Notably, it reduces flickering artifacts and sharpens
structural details across views and time, enabling more immersive AR/VR
experiences. MVG4D sets a new direction for efficient and controllable 4D
generation from minimal inputs.

</details>


### [55] [Towards Effective Human-in-the-Loop Assistive AI Agents](https://arxiv.org/abs/2507.18374)
*Filippos Bellos,Yayuan Li,Cary Shu,Ruey Day,Jeffrey M. Siskind,Jason J. Corso*

Main category: cs.CV

TL;DR: 本论文提出了一套用于评估人机协作执行任务的框架，并收集了多模态数据集，通过增强现实（AR）技术结合AI指导，对从日常烹饪到战地医疗等实际任务展开研究，证明AI辅助能提升人类完成任务的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 人机协作在实际任务执行有广泛应用前景，但如何客观评价AI辅助下的人类表现一直是挑战。因此，亟需建立有效的评估体系和数据资源来分析AI指导对人类任务执行和学习结果的影响。

Method: 作者搭建了一个评估框架，并构建了涵盖多种情景的人机多模态互动数据集；同时开发了一套AR支持的AI指导系统，让AI以增强现实的方式在实际物理任务中为用户提供实时互动指导；通过实证的人体实验来研究这种AI协作的实际效果。

Result: 实验证明，AI的互动式指导显著提升了人类在多种任务上的完成效果，减少了错误，同时改善了用户的学习成果。

Conclusion: AI辅助与人类的协作模式通过有效的指导能够促进人类更好地完成复杂任务，为实际应用中推广AI辅助人机协作奠定了基础。

Abstract: Effective human-AI collaboration for physical task completion has significant
potential in both everyday activities and professional domains. AI agents
equipped with informative guidance can enhance human performance, but
evaluating such collaboration remains challenging due to the complexity of
human-in-the-loop interactions. In this work, we introduce an evaluation
framework and a multimodal dataset of human-AI interactions designed to assess
how AI guidance affects procedural task performance, error reduction and
learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI
agent that provides interactive guidance in real-world tasks, from cooking to
battlefield medicine. Through human studies, we share empirical insights into
AI-assisted human performance and demonstrate that AI-assisted collaboration
improves task completion.

</details>


### [56] [Towards Consistent Long-Term Pose Generation](https://arxiv.org/abs/2507.18382)
*Yayuan Li,Filippos Bellos,Jason Corso*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的一阶段架构，直接根据单张RGB图像和文本描述生成具有时序连贯性的连续姿态序列，显著优于以往的量化和自回归方法，尤其是在长时序姿态生成任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的姿态生成方法依赖于中间表示（如量化处理或自回归序列），这会导致推理过程中累积误差，尤其在长时序生成时效果变差，时序一致性难以保证。因此，作者希望设计方法避免中间表示，提高性能和时序一致性。

Method: 作者提出一种新颖的一阶段方法：直接以相对运动预测姿态点的连续坐标，无需中间表示或基于token的生成。同时，采用统一的占位符token策略，在训练与推理时保持一致的生成行为，能够通过单次前向生成实现高效推理。

Result: 在Penn Action与F-PHAB数据集上的大量实验表明，该方法在姿态生成任务上（特别是长时序生成）显著优于现有的基于量化和自回归的主流方法。

Conclusion: 消除中间表示和token化带来的误差，直接在连续坐标空间上生成姿态，可显著提升长时序动作生成的性能和时序一致性，表明所提方法具备良好的实际应用前景。

Abstract: Current approaches to pose generation rely heavily on intermediate
representations, either through two-stage pipelines with quantization or
autoregressive models that accumulate errors during inference. This fundamental
limitation leads to degraded performance, particularly in long-term pose
generation where maintaining temporal coherence is crucial. We propose a novel
one-stage architecture that directly generates poses in continuous coordinate
space from minimal context - a single RGB image and text description - while
maintaining consistent distributions between training and inference. Our key
innovation is eliminating the need for intermediate representations or
token-based generation by operating directly on pose coordinates through a
relative movement prediction mechanism that preserves spatial relationships,
and a unified placeholder token approach that enables single-forward generation
with identical behavior during training and inference. Through extensive
experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)
datasets, we demonstrate that our approach significantly outperforms existing
quantization-based and autoregressive methods, especially in long-term
generation scenarios.

</details>


### [57] [HumanMaterial: Human Material Estimation from a Single Image via Progressive Training](https://arxiv.org/abs/2507.18385)
*Yu Jiang,Jiahao Xia,Jiongming Qin,Yusen Wang,Tuo Cao,Chunxia Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理渲染的全身人体逆向渲染方法，旨在提升人体材质估计的质量与逼真度，尤其是皮肤材质。通过构建高质量数据集和分阶段训练策略，方法在材质预测与渲染效果上达到新的先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有逆向渲染方法因数据与建模简化，导致材质估计（尤其是皮肤）真实感欠佳，难以满足高质量全身人体渲染的应用需求。逆向渲染本身是病态问题，需要更准确的监督和数据支撑。

Method: 构建高质量的OpenHumanBRDF数据集，包含法线、漫反射反照率、粗糙度、高光反照率、位移、亚表面散射等全方位材质信息。提出HumanMaterial模型，采用分阶段（progressive）梯度训练，即通过三组先验模型粗略预测各材质图，再由后续微调模型精细优化。创新性地设计了Controlled PBR Rendering（CPR）损失，使重要的材质图在训练时获得更高权重。

Result: 在OpenHumanBRDF及真实数据集上实验，所提方法在人体材质多图估计与照片级渲染任务上取得了当前最佳性能。

Conclusion: 通过高质量材质数据集、分阶段模型结构及新型损失函数，显著提升了全身人体逆向渲染的材质估计与渲染逼真度，特别在皮肤表现方面具有明显优势。

Abstract: Full-body Human inverse rendering based on physically-based rendering aims to
acquire high-quality materials, which helps achieve photo-realistic rendering
under arbitrary illuminations. This task requires estimating multiple material
maps and usually relies on the constraint of rendering result. The absence of
constraints on the material maps makes inverse rendering an ill-posed task.
Previous works alleviated this problem by building material dataset for
training, but their simplified material data and rendering equation lead to
rendering results with limited realism, especially that of skin. To further
alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)
based on scanned real data and statistical material data. In addition to the
normal, diffuse albedo, roughness, specular albedo, we produce displacement and
subsurface scattering to enhance the realism of rendering results, especially
for the skin. With the increase in prediction tasks for more materials, using
an end-to-end model as in the previous work struggles to balance the importance
among various material maps, and leads to model underfitting. Therefore, we
design a model (HumanMaterial) with progressive training strategy to make full
use of the supervision information of the material maps and improve the
performance of material estimation. HumanMaterial first obtain the initial
material results via three prior models, and then refine the results by a
finetuning model. Prior models estimate different material maps, and each map
has different significance for rendering results. Thus, we design a Controlled
PBR Rendering (CPR) loss, which enhances the importance of the materials to be
optimized during the training of prior models. Extensive experiments on
OpenHumanBRDF dataset and real data demonstrate that our method achieves
state-of-the-art performance.

</details>


### [58] [Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows](https://arxiv.org/abs/2507.18405)
*Simin Huo,Ning Li*

Main category: cs.CV

TL;DR: 本文提出了Iwin Transformer，一种无位置嵌入的新型分层视觉Transformer，通过交错窗口注意力与深度可分离卷积协作，实现了单模块内的全局信息交换，克服了Swin Transformer需要连续两个块来逼近全局注意力的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前主流分层视觉Transformer如Swin Transformer需要叠加多个块才能获得近似全局注意力，效率较低，并且大多依赖于固定的位置嵌入，局限了模型的灵活性和泛化能力。

Method: Iwin Transformer通过交错窗口注意力连接远处的token，采用深度可分离卷积连接邻近token，无需位置嵌入；还能直接从低分辨率微调到高分辨率。其核心模块也可作为独立组件，替换图像生成任务中的自注意力。

Result: Iwin Transformer在ImageNet-1K图像分类任务上达到87.4%的top-1准确率，在语义分割和视频动作识别任务上同样表现出色。其核心模块在条件图像生成等任务中也有较好效果。

Conclusion: Iwin Transformer实现了无需位置嵌入的高效全局建模，具备很强的可迁移性和广泛应用前景，有望为未来如3D注意力的视频生成等方向提供新思路。

Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.

</details>


### [59] [DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation](https://arxiv.org/abs/2507.18407)
*Xun Ye,Ruixiang Tang,Mingda Zhang,Jianglong Qin*

Main category: cs.CV

TL;DR: 提出了一种新的医学图像分割网络（DCFFSNet），通过特征空间解耦和平衡融合，显著提升了分割精度、区域一致性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割网络中对连通性的处理通常直接作为额外模块强制注入，导致特征空间耦合且无法标准化不同特征强度的度量；因此需要一种更加有效的方式整合连通性信息。

Method: 提出了DCFFSNet，利用特征空间解耦策略，量化连通性特征与其他特征的相对强度，并构建了深度连通性特征融合-分离架构，实现多尺度特征动态平衡表达。

Result: 在ISIC2018、DSB2018和MoNuSeg数据集上的实验结果显示，DCFFSNet在所有指标上均优于主流方法，分别在Dice和IoU指标上超越了CMUNet、TransUNet和CSCAUNet。

Conclusion: DCFFSNet能够有效解决分割碎片化问题，实现流畅的边缘过渡，并显著提升医学图像分割的临床实用性。

Abstract: Medical image segmentation leverages topological connectivity theory to
enhance edge precision and regional consistency. However, existing deep
networks integrating connectivity often forcibly inject it as an additional
feature module, resulting in coupled feature spaces with no standardized
mechanism to quantify different feature strengths. To address these issues, we
propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It
introduces an innovative feature space decoupling strategy. This strategy
quantifies the relative strength between connectivity features and other
features. It then builds a deep connectivity feature fusion-separation
architecture. This architecture dynamically balances multi-scale feature
expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg
datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by
1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)
and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).
The results demonstrate that DCFFSNet exceeds existing mainstream methods
across all metrics. It effectively resolves segmentation fragmentation and
achieves smooth edge transitions. This significantly enhances clinical
usability.

</details>


### [60] [Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss](https://arxiv.org/abs/2507.18424)
*Edward Ellis,Robert Mendel,Andrew Bulpitt,Nasim Parsa,Michael F Byrne,Sharib Ali*

Main category: cs.CV

TL;DR: 本文提出通过引入V-JEPA自监督学习框架，并结合创新的3D定位辅助任务，有效提升超声视频分割性能，尤其在标注数据有限时提升明显。


<details>
  <summary>Details</summary>
Motivation: 超声影像数据采集和标注困难，且原始数据噪声高、对比度低，严重依赖临床专家，制约了下游分割模型的性能提升。需要更好利用无标注数据改善分割表现。

Method: 采用V-JEPA这个最近用于视频数据的自监督特征预测方法，并首次应用至超声视频。针对ViT模型缺乏局部信息学习的问题，提出3D定位辅助任务，加强ViT在V-JEPA预训练过程中的局部感知能力。

Result: 实验表明，V-JEPA结合3D定位辅助任务，在不同冻结编码器配置下均显著提升分割性能。当全部训练数据可用时提升可达3.4%，仅用10%训练数据时提升达8.35%。

Conclusion: V-JEPA及其3D定位辅助任务能有效提升超声视频分析中ViT模型的表现，尤其适合少量标注数据条件下应用。

Abstract: Acquiring and annotating large datasets in ultrasound imaging is challenging
due to low contrast, high noise, and susceptibility to artefacts. This process
requires significant time and clinical expertise. Self-supervised learning
(SSL) offers a promising solution by leveraging unlabelled data to learn useful
representations, enabling improved segmentation performance when annotated data
is limited. Recent state-of-the-art developments in SSL for video data include
V-JEPA, a framework solely based on feature prediction, avoiding pixel level
reconstruction or negative samples. We hypothesise that V-JEPA is well-suited
to ultrasound imaging, as it is less sensitive to noisy pixel-level detail
while effectively leveraging temporal information. To the best of our
knowledge, this is the first study to adopt V-JEPA for ultrasound video data.
Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is
well-suited to ViT-based models. However, ViTs can underperform on small
medical datasets due to lack of inductive biases, limited spatial locality and
absence of hierarchical feature learning. To improve locality understanding, we
propose a novel 3D localisation auxiliary task to improve locality in ViT
representations during V-JEPA pre-training. Our results show V-JEPA with our
auxiliary task improves segmentation performance significantly across various
frozen encoder configurations, with gains up to 3.4\% using 100\% and up to
8.35\% using only 10\% of the training data.

</details>


### [61] [NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning](https://arxiv.org/abs/2507.18429)
*Mahdi Ghafourian,Federico M. Sukno*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新颖头部姿态估计方法（NLML-HPE），结合张量分解与前馈神经网络，在训练数据有限的情况下，通过非线性流形学习对面部关键点进行姿态回归预测，实现了高效、实时的头部姿态估计。


<details>
  <summary>Details</summary>
Motivation: 头部姿态估计在各类计算机视觉任务中至关重要。然而，现有HPE数据集标签不准确且训练数据有限，影响估计效果。本文旨在通过改进的数据集和新方法，解决数据不精确和估计效率低下的问题。

Method: 采用张量（Tucker）分解，将欧拉角（偏航、俯仰、翻滚）映射到独立子空间，并用余弦曲线建模每一维流形，通过前馈神经网络实现从人脸关键点到连续姿态角的回归。生成一致且高精度的2D数据集，并仅需有限的训练数据完成训练，显著提升速度和精度。

Result: 在生成的高精度2D头部姿态数据集上，所提方法能精准建模三个旋转轴下的人脸姿态流形，训练完毕后对新数据预测极快，实现了实时、准确的头部姿态估计。

Conclusion: NLML-HPE方法可有效利用有限和高质量数据集，在保持实时性的同时准确地估计头部姿态，并为后续研究人员提供了可复现实验代码和模型，促进领域发展。

Abstract: Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.

</details>


### [62] [PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior](https://arxiv.org/abs/2507.18447)
*Junda Wu,Jessica Echterhoff,Kyungtae Han,Amr Abdelraouf,Rohit Gupta,Julian McAuley*

Main category: cs.CV

TL;DR: 该论文提出了PDB-Eval基准，用于多模态大模型在驾驶行为理解与推理任务上的评测与微调，显著提升了模型在驾驶领域相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集难以基于外部视觉证据全面描述和解释车辆运动及驾驶员行为，限制了个性化支持系统和安全系统有效性。为解决这一问题，作者提出面向个性化驾驶行为理解的新基准和相关任务。

Method: 论文提出了PDB-Eval评测基准，包括PDB-X和PDB-QA两部分。PDB-X用于评估多模态模型对于时间序列驾驶场景的理解，PDB-QA则作为视觉解释问答任务，以微调模型推理能力。该数据集通过外部视觉证据解释驾驶员的内部视角行为，支持模型对生成任务领域迁移的无损泛化。

Result: 实验表明，利用PDB基准细粒度描述和解释微调多模态大模型，在零样本问答任务上的性能提升高达73.2%。此外，在Brain4Cars和AIDE任务中，转向意图预测提升最高可达12.5%，AIDE所有任务一致提升，最大达11.0%。

Conclusion: 通过PDB-Eval基准和方法的引入，可大幅推进多模态模型对驾驶领域场景的理解和推理能力，促进相关智能驾驶和辅助系统的性能提升。

Abstract: Understanding a driver's behavior and intentions is important for potential
risk assessment and early accident prevention. Safety and driver assistance
systems can be tailored to individual drivers' behavior, significantly
enhancing their effectiveness. However, existing datasets are limited in
describing and explaining general vehicle movements based on external visual
evidence. This paper introduces a benchmark, PDB-Eval, for a detailed
understanding of Personalized Driver Behavior, and aligning Large Multimodal
Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists
of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'
understanding of temporal driving scenes. Our dataset is designed to find valid
visual evidence from the external view to explain the driver's behavior from
the internal view. To align MLLMs' reasoning abilities with driving tasks, we
propose PDB-QA as a visual explanation question-answering task for MLLM
instruction fine-tuning. As a generic learning task for generative models like
MLLMs, PDB-QA can bridge the domain gap without harming MLLMs'
generalizability. Our evaluation indicates that fine-tuning MLLMs on
fine-grained descriptions and explanations can effectively bridge the gap
between MLLMs and the driving domain, which improves zero-shot performance on
question-answering tasks by up to 73.2%. We further evaluate the MLLMs
fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition
tasks. We observe up to 12.5% performance improvements on the turn intention
prediction task in Brain4Cars, and consistent performance improvements up to
11.0% on all tasks in AIDE.

</details>


### [63] [Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols](https://arxiv.org/abs/2507.18457)
*Luo Cheng,Hanwei Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.CV

TL;DR: 本文针对LiDAR（激光雷达）3D目标检测中物理对抗攻击的可复现性和标准化问题，提出了通用标准化测试框架，并开源代码，以支持公平和便捷的研究与比较。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR在3D目标检测中应用广泛，但现有的对抗攻击多集中于数字层面，缺乏物理可实现性。物理对抗攻击的可复现性差，影响该领域的现实影响力和研究进展。

Method: 作者提出并实现了一个与设备无关、可抽象物理对抗攻击关键要素的通用标准化框架。该框架支持多种方法，提供开源代码及基准测试流程，可在模拟和真实环境下运行。

Result: 框架能够实现现有物理对抗攻击方法的公平对比，并成功实现了仿真攻击向真实LiDAR系统的迁移。

Conclusion: 该框架推动了对物理对抗攻击的标准化研究，提升了复现性，并为现实LiDAR感知系统的鲁棒性研究和未来攻防对抗提供了有力工具及见解。

Abstract: Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.

</details>


### [64] [CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting](https://arxiv.org/abs/2507.18473)
*Haoran Xu,Saining Zhang,Peishuo Li,Baijun Ye,Xiaoxue Chen,Huan-ang Gao,Jv Zheng,Xiaowei Song,Ziqiao Peng,Run Miao,Jinrang Jia,Yifeng Shi,Guangqi Yi,Hang Zhao,Hao Tang,Hongyang Li,Kaicheng Yu,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了CRUISE框架，用于在V2X自动驾驶场景中高保真地重建与灵活增强真实路况数据，并能大规模扩充数据集，以提升三维检测和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: V2X通信对自动驾驶极为关键，但现有仿真用于V2X数据生成和增广还不充分，限制了感知模型的训练与实际应用，因此需要一种能高质量重建和增广V2X场景的新方法。

Method: 提出了CRUISE框架，采用分解式高斯斑点（Gaussian Splatting）方法，对交通参与者进行可编辑的高斯分解，实现真实场景高保真重建和灵活编辑。同时，该框架支持从自车/基础设施多视角渲染，扩充大规模V2X数据。

Result: 实验表明，CRUISE能够高保真地重建真实V2X场景，提升自车、基础设施和协同视角下的三维检测与跟踪精度，还能高效生成极具挑战性的案例。

Conclusion: CRUISE为V2X自动驾驶场景的数据生成和扩充提供了新工具，有助于提升多视角协同感知的性能，有较高的实际应用前景。

Abstract: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous
driving, enabling cooperation between vehicles and infrastructure. While
simulation has significantly contributed to various autonomous driving tasks,
its potential for data generation and augmentation in V2X scenarios remains
underexplored. In this paper, we introduce CRUISE, a comprehensive
reconstruction-and-synthesis framework designed for V2X driving environments.
CRUISE employs decomposed Gaussian Splatting to accurately reconstruct
real-world scenes while supporting flexible editing. By decomposing dynamic
traffic participants into editable Gaussian representations, CRUISE allows for
seamless modification and augmentation of driving scenes. Furthermore, the
framework renders images from both ego-vehicle and infrastructure views,
enabling large-scale V2X dataset augmentation for training and evaluation. Our
experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X
driving scenes with high fidelity; 2) using CRUISE improves 3D detection across
ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D
tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates
challenging corner cases.

</details>


### [65] [Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection](https://arxiv.org/abs/2507.18481)
*Francesco Dalmonte,Emirhan Bayar,Emre Akbas,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: 本文提出了一种基于最新视觉基础模型的自编码器结构（Q-Former Autoencoder），用于无监督医学图像异常检测，在多个公开数据集上取得了最新最优结果。其核心是利用冻结的视觉基础模型作为特征提取器，并加入感知损失，避免了领域特定微调。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测难点在于异常类型多样且难以获取全面标注数据，传统有监督方法难以适用，因此亟需无监督的方法。

Method: 直接利用DINO、DINOv2、Masked Autoencoder等视觉基础模型作为冻结特征提取器。以Q-Former架构作为自编码器瓶颈层，可以灵活控制重构序列长度并有效聚合多尺度特征。还引入基于预训练Masked Autoencoder特征的感知损失，引导重构更具语义信息。

Result: 在四个医学异常检测基准数据集（如BraTS2021、RESC、RSNA）上进行验证，方法获得SOTA结果，显著优于现有方法。

Conclusion: 预训练在自然图像上的视觉基础模型无需针对医学领域再训练，就能泛化到医学图像异常检测等任务上，展现出较大潜力和实际价值。

Abstract: Anomaly detection in medical images is an important yet challenging task due
to the diversity of possible anomalies and the practical impossibility of
collecting comprehensively annotated data sets. In this work, we tackle
unsupervised medical anomaly detection proposing a modernized autoencoder-based
framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained
vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead
of training encoders from scratch, we directly utilize frozen vision foundation
models as feature extractors, enabling rich, multi-stage, high-level
representations without domain-specific fine-tuning. We propose the usage of
the Q-Former architecture as the bottleneck, which enables the control of the
length of the reconstruction sequence, while efficiently aggregating multiscale
features. Additionally, we incorporate a perceptual loss computed using
features from a pretrained Masked Autoencoder, guiding the reconstruction
towards semantically meaningful structures. Our framework is evaluated on four
diverse medical anomaly detection benchmarks, achieving state-of-the-art
results on BraTS2021, RESC, and RSNA. Our results highlight the potential of
vision foundation model encoders, pretrained on natural images, to generalize
effectively to medical image analysis tasks without further fine-tuning. We
release the code and models at https://github.com/emirhanbayar/QFAE.

</details>


### [66] [A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears](https://arxiv.org/abs/2507.18483)
*Frauke Wilm,Luis Carlos Rivera Monroy,Mathias Öttl,Lukas Mürdter,Leonid Mill,Andreas Maier*

Main category: cs.CV

TL;DR: 本文针对疟疾显微镜图像自动检测难题，提出对NIH公开疟疾数据集进行实例级标注增强，并基于此训练深度学习模型实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 显微镜下的疟疾病原体检测对于疟疾诊断至关重要，而现有可供深度学习训练的高质量实例级标注数据库稀缺，限制了自动化方法的应用。

Method: 对NIH疟疾数据集添加了以COCO格式存储的详细边界框标注，用于支持对象检测任务。采用Faster R-CNN模型对带注释的数据进行训练和交叉验证，检测感染和未感染的红细胞及白细胞。

Result: 在原始数据集上的交叉验证，模型对感染细胞的检测F1分数高达0.88，显示高准确性。同时验证了细致的标注和标注一致性对检测性能的重要作用。

Conclusion: 通过自动化标注精修结合人工校正，能高效获得高质量训练数据，显著提升深度学习在疟疾检测中的表现。增强版数据集已向公众开放，将有助于推动相关领域研究。

Abstract: Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is
an essential component of reliable malaria diagnosis, especially in developing
countries. Deep learning-based object detection methods have demonstrated
strong potential for automated Malaria diagnosis, but their adoption is limited
by the scarcity of datasets with detailed instance-level annotations. In this
work, we present an enhanced version of the publicly available NIH malaria
dataset, with detailed bounding box annotations in COCO format to support
object detection training. We validated the revised annotations by training a
Faster R-CNN model to detect infected and non-infected red blood cells, as well
as white blood cells. Cross-validation on the original dataset yielded F1
scores of up to 0.88 for infected cell detection. These results underscore the
importance of annotation volume and consistency, and demonstrate that automated
annotation refinement combined with targeted manual correction can produce
training data of sufficient quality for robust detection performance. The
updated annotations set is publicly available via GitHub:
https://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.

</details>


### [67] [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](https://arxiv.org/abs/2507.18484)
*Xiao Yang,Lingxuan Wu,Lizhong Wang,Chengyang Ying,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种主动防御3D对抗攻击的新方法Rein-EAD，通过主动与环境互动提升视觉系统对对抗威胁的鲁棒性，在多个3D任务上显著降低攻击成功率且保持正常准确率。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉系统在安全敏感场景（如身份验证、自动驾驶）易受对抗攻击（如对抗贴纸、3D物体）的影响。现有被动防御方法难以适应动态和复杂3D环境，且依赖于对攻击方式的预设假设，导致泛化和适应性不足。因此，亟需更主动、更具适应性的防御机制来提升系统可靠性。

Method: 作者提出主动防御框架Rein-EAD，基于强化学习，通过与环境的自适应探索和交互来增强对抗鲁棒性。方法采用多步目标，兼顾当前准确率与预测的不确定性（预测熵最小化），并设计了面向不确定性的奖励机制以提升策略更新效率，降低了计算资源消耗，无需可微分环境即可应用。

Result: Rein-EAD在3D对象分类、人脸识别、自动驾驶等多任务实验中，显著降低了对抗攻击的成功率，并且在不影响（或轻微影响）正常任务准确率的情况下，在未见和自适应攻击下依然表现出强泛化能力。

Conclusion: Rein-EAD方法验证了主动防御策略在复杂3D视觉任务中的有效性和广泛适用性，有望用于实际面临对抗威胁的各类安全敏感应用场景。

Abstract: Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.

</details>


### [68] [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](https://arxiv.org/abs/2507.18498)
*Zongzheng Zhang,Xuchong Qiu,Boran Zhang,Guantian Zheng,Xunjiang Gu,Guoxuan Chi,Huan-ang Gao,Leichen Wang,Ziming Liu,Xinrun Li,Igor Gilitschenski,Hongyang Li,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: 本文针对自动驾驶的无地图（mapless）方法中在线生成高精地图的不确定性问题，提出自适应融合地图不确定性与运动预测的新方法，有效提升轨迹预测表现，超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶趋向于使用在线生成的高精地图，减少了人工标注与维护成本，但这些地图的可靠性存在不确定性。虽然将地图不确定性纳入轨迹预测有助于性能提升，但目前缺乏对此不确定性在何种场景下最有价值的细致理解。

Method: 作者首先分析了在何种驾驶场景下地图不确定性对轨迹预测最有益，发现车辆动力学状态是一个关键但被忽视的因素。基于此，提出了一种新颖的“本体感知场景门控（Proprioceptive Scenario Gating）”机制，根据车辆未来动力学状态预测，自适应地整合地图不确定性，有助于提高预测性能和解释性。同时，引入基于协方差的地图不确定性建模，更好地反映地图几何特性。

Result: 通过在真实世界 nuScenes 数据集上广泛的消融实验，方法在无地图轨迹预测表现上相较最优现有方法提升最多达到23.6%。

Conclusion: 提出的自适应融合机制和基于协方差的不确定性建模显著提升了自动驾驶轨迹预测的准确性和解释性，展示了地图不确定性与动力学状态结合的巨大潜力，方法效果优于以往技术。

Abstract: Recent advances in autonomous driving are moving towards mapless approaches,
where High-Definition (HD) maps are generated online directly from sensor data,
reducing the need for expensive labeling and maintenance. However, the
reliability of these online-generated maps remains uncertain. While
incorporating map uncertainty into downstream trajectory prediction tasks has
shown potential for performance improvements, current strategies provide
limited insights into the specific scenarios where this uncertainty is
beneficial. In this work, we first analyze the driving scenarios in which
mapping uncertainty has the greatest positive impact on trajectory prediction
and identify a critical, previously overlooked factor: the agent's kinematic
state. Building on these insights, we propose a novel Proprioceptive Scenario
Gating that adaptively integrates map uncertainty into trajectory prediction
based on forecasts of the ego vehicle's future kinematics. This lightweight,
self-supervised approach enhances the synergy between online mapping and
trajectory prediction, providing interpretability around where uncertainty is
advantageous and outperforming previous integration methods. Additionally, we
introduce a Covariance-based Map Uncertainty approach that better aligns with
map geometry, further improving trajectory prediction. Extensive ablation
studies confirm the effectiveness of our approach, achieving up to 23.6%
improvement in mapless trajectory prediction performance over the
state-of-the-art method using the real-world nuScenes driving dataset. Our
code, data, and models are publicly available at
https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.

</details>


### [69] [Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention](https://arxiv.org/abs/2507.18503)
*João Luzio,Alexandre Bernardino,Plinio Moreno*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义和贝叶斯方法的注意力建模框架SemBA-FAST，专为在目标存在的视觉搜索任务中预测人类视觉注意力而设计。该模型通过深度目标检测和概率语义融合机制动态生成注意力图，并能模拟人类凝视点的顺序。实验结果表明，SemBA-FAST在COCO-Search18数据集上优于现有同类模型。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉注意力建模虽取得了一定进展，但仍难以充分结合人类对语义与视觉信息的多层次整合，尤其是在主动视觉搜索中精准地预测人类的凝视路径。为此，研究者希望开发更符合人类注意力机制的模型，提升主动视觉任务的人机交互和认知计算能力。

Method: SemBA-FAST结合了深度学习的目标检测器与概率语义融合机制，并引入人工中心凹处理（foveation），使之能动态、顺序地更新和生成注意力图，以实时预测人在视觉搜索过程中下一步的关注区域。

Result: 在COCO-Search18基准数据集上的实验表明，SemBA-FAST生成的凝视点序列与真实人类路径十分接近，显著优于传统基线和其它顶层驱动方法，并在部分情况下能与利用扫描路径信息的先进模型媲美。

Conclusion: 语义-中心凹概率模型框架在促进接近人类的视觉关注建模方面展现出巨大潜力，有望应用于实时认知计算和机器人等实际场景中。

Abstract: In goal-directed visual tasks, human perception is guided by both top-down
and bottom-up cues. At the same time, foveal vision plays a crucial role in
directing attention efficiently. Modern research on bio-inspired computational
attention models has taken advantage of advancements in deep learning by
utilizing human scanpath data to achieve new state-of-the-art performance. In
this work, we assess the performance of SemBA-FAST, i.e. Semantic-based
Bayesian Attention for Foveal Active visual Search Tasks, a top-down framework
designed for predicting human visual attention in target-present visual search.
SemBA-FAST integrates deep object detection with a probabilistic semantic
fusion mechanism to generate attention maps dynamically, leveraging pre-trained
detectors and artificial foveation to update top-down knowledge and improve
fixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18
benchmark dataset, comparing its performance against other scanpath prediction
models. Our methodology achieves fixation sequences that closely match human
ground-truth scanpaths. Notably, it surpasses baseline and other top-down
approaches and competes, in some cases, with scanpath-informed models. These
findings provide valuable insights into the capabilities of semantic-foveal
probabilistic frameworks for human-like attention modelling, with implications
for real-time cognitive computing and robotics.

</details>


### [70] [Explaining How Visual, Textual and Multimodal Encoders Share Concepts](https://arxiv.org/abs/2507.18512)
*Clément Cornet,Romaric Besançon,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 本文提出了用于跨模型SAE特征比较的新指标，对视觉、文本与多模态编码器进行特征共享性分析，发现文本预训练对视觉特征影响显著。


<details>
  <summary>Details</summary>
Motivation: 虽然以稀疏自编码器提取的特征能帮助理解神经网络，但此前关于模型间的比较仅限于同一模态。该文希望提出能量化跨模态模型特征共享性的比较工具，填补定量比较跨模态特征的空白。

Method: 提出新颖的定量指标对基于SAE的特征进行模型间比较。同时，提出“比较共享性”概念用于量化不同类别模型间单个特征的共享度。基于这些工具，针对21个视觉、文本和多模态编码器，在不同尺寸和数据集情境下系统性分析特征表达的相似与差异。

Result: 实验显示，不同类型及规模的编码器在特征表示上存在差异和共享度。特别是，多模态视觉—语言模型中独特的视觉特征，与文本编码器有较高共享，表明文本预训练对视觉表征有显著影响。

Conclusion: 新指标有助于更精细地比较不同模态模型的特征共享性，为后续多模态和迁移学习等领域提供了量化分析工具，并强调了文本预训练在视觉语义共享中的作用。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts

</details>


### [71] [Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection](https://arxiv.org/abs/2507.18513)
*Adhemar de Senneville,Xavier Bou,Thibaud Ehret,Rafael Grompone,Jean Louis Bonne,Nicolas Dumelie,Thomas Lauvaux,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本论文提出了一种基于部件的遥感目标检测方法，专注于在大范围稀有对象（如生物沼气池）检测，并结合地统计方法评估其区域甲烷产量。


<details>
  <summary>Details</summary>
Motivation: 随着遥感数据量的激增，如何在大范围内高效检测稀有对象成为难题，尤其是在需要对某些人类活动进行环境影响评估时。生物沼气池的甲烷排放是评估环境影响的重要对象，但检测稀有分布的沼气池极具挑战。

Method: （1）构建并引入包含稀有生物沼气池的新数据集，该数据集训练集和验证集较小，测试集极度类别不平衡，负样本为主。 （2）提出基于关键子部件特征的检测方法，通过分析和定位沼气池的组成部分以提升检测效果。（3）将该方法应用到未见过的新区域，环境规模化检测。 （4）结合地统计学方法，基于检测结果统计区域甲烷产量。

Result: 实验表明，部分部件法能提升对稀有沼气池的检测能力，并可应用于大面积、未标注区域。对所得检测结果进一步实现了区域甲烷总产量量化估算。

Conclusion: 提出的方法能有效弥补遥感数据中稀有对象检测难题，为大范围环境影响评估提供了可行途径，尤其是在温室气体监测方面具有实际应用价值。

Abstract: Object detection is one of the main applications of computer vision in remote
sensing imagery. Despite its increasing availability, the sheer volume of
remote sensing data poses a challenge when detecting rare objects across large
geographic areas. Paradoxically, this common challenge is crucial to many
applications, such as estimating environmental impact of certain human
activities at scale. In this paper, we propose to address the problem by
investigating the methane production and emissions of bio-digesters in France.
We first introduce a novel dataset containing bio-digesters, with small
training and validation sets, and a large test set with a high imbalance
towards observations without objects since such sites are rare. We develop a
part-based method that considers essential bio-digester sub-elements to boost
initial detections. To this end, we apply our method to new, unseen regions to
build an inventory of bio-digesters. We then compute geostatistical estimates
of the quantity of methane produced that can be attributed to these
infrastructures in a given area at a given time.

</details>


### [72] [Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs](https://arxiv.org/abs/2507.18517)
*Bolutife Atoki,Jenny Benois-Pineau,Renaud Péteri,Fabien Baldacci,Aymar de Rugy*

Main category: cs.CV

TL;DR: 本文提出了一种方法利用注视点引导Segment Anything Model（SAM）进行目标分割，尤其是在复杂、真实场景下无需针对特定图像进行微调。研究证明该方法能有效提升语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的大型基础模型在目标分割任务时，尤其是在高杂乱场景和无微调情况下，表现可能欠佳。为实现视觉引导的上肢神经假体等实际应用，需要在‘野外’高复杂度场景下提升分割效果。

Method: 提出利用注视点生成的提示（prompt）来引导SAM模型，并在第一视角的视觉数据上进行微调，从而适应复杂和真实的抓取分割场景。

Result: 在Grasping-in-the-Wild数据集上的实验显示，所提方法的IoU分割质量指标提升高达0.51分。

Conclusion: 该方法通过注视点提示和针对实际数据微调，有效提升了基础模型在真实杂乱场景中的目标分割表现，对视觉引导的神经假体具备实际应用价值。

Abstract: In this work, we address the problem of semantic object segmentation using
foundation models. We investigate whether foundation models, trained on a large
number and variety of objects, can perform object segmentation without
fine-tuning on specific images containing everyday objects, but in highly
cluttered visual scenes. The ''in the wild'' context is driven by the target
application of vision guided upper limb neuroprostheses. We propose a method
for generating prompts based on gaze fixations to guide the Segment Anything
Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual
data. Evaluation results of our approach show an improvement of the IoU
segmentation quality metric by up to 0.51 points on real-world challenging data
of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform
(https://universe.roboflow.com/iwrist/grasping-in-the-wild)

</details>


### [73] [GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians](https://arxiv.org/abs/2507.18522)
*Tomislav Pavković,Mohammad-Ali Nikouei Mahani,Johannes Niedermayer,Johannes Betz*

Main category: cs.CV

TL;DR: GaussianFusionOcc提出了一种基于三维高斯分布和创新传感器融合机制的3D语义占据预测方法，能够高效且准确地进行多模态感知任务。


<details>
  <summary>Details</summary>
Motivation: 3D语义占据预测对于自动驾驶的环境理解和安全导航至关重要。现有方法多依赖稠密栅格表示，造成内存压力和推理速度慢，同时不同传感器数据融合效果有待提升。

Method: GaussianFusionOcc利用三维语义高斯分布作为空间表示，同时引入创新的传感器融合技术，融合来自摄像头、激光雷达和毫米波雷达的数据。通过模态无关的可变形注意力机制分别提取各传感器关键特征，并优化高斯分布参数，实现高效、灵活的场景表征。

Result: 实验表明，GaussianFusionOcc在多种传感器组合下均表现出很强的适应性和优越性，在准确性和效率方面超越当前主流方法。

Conclusion: GaussianFusionOcc通过高效的多模态融合和高斯分布表示，显著提升了3D语义占据预测的性能，为自动驾驶环境感知提供了新的解决思路。

Abstract: 3D semantic occupancy prediction is one of the crucial tasks of autonomous
driving. It enables precise and safe interpretation and navigation in complex
environments. Reliable predictions rely on effective sensor fusion, as
different modalities can contain complementary information. Unlike conventional
methods that depend on dense grid representations, our approach,
GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor
fusion mechanism. Seamless integration of data from camera, LiDAR, and radar
sensors enables more precise and scalable occupancy prediction, while 3D
Gaussian representation significantly improves memory efficiency and inference
speed. GaussianFusionOcc employs modality-agnostic deformable attention to
extract essential features from each sensor type, which are then used to refine
Gaussian properties, resulting in a more accurate representation of the
environment. Extensive testing with various sensor combinations demonstrates
the versatility of our approach. By leveraging the robustness of multi-modal
fusion and the efficiency of Gaussian representation, GaussianFusionOcc
outperforms current state-of-the-art models.

</details>


### [74] [IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning](https://arxiv.org/abs/2507.18531)
*Tianheng Qiu,Jingchun Gao,Jingyu Li,Huiyi Leong,Xuan Huang,Xi Wang,Xiaocheng Zhang,Kele Xu,Lan Zhang*

Main category: cs.CV

TL;DR: 本文提出了IntentVCNet方法来提高大规模视觉语言模型（LVLMs）在视频意图导向描述任务中的时空细粒度控制能力，有效缩小了空间和时间理解之间的差距，并在多个数据集和竞赛中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs虽然在空间和时间理解方面各自有一定能力，但在结合时空信息进行意图导向的视频描述时，仍存在细粒度时空控制能力不足的问题，限制了根据用户自定义意图生成目标化描述的效果。为此，作者希望提升LVLMs对时序视频中目标对象的空间细节建模能力，更好地响应用户复杂的意图指令。

Method: 作者提出IntentVCNet框架，在提示设计和模型结构两个层面提升LVLM的时空建模能力。具体包括：（1）引入prompt组合策略，建模用户意图提示与视频序列间的隐式关系；（2）设计高效的box adapter结构，将目标语义信息注入到全局视觉上下文，使视觉token能先验感知用户意图。这两部分相互结合，促进模型生成更精准的意图导向描述。

Result: 结合prompt组合法和box adapter机制后，模型细粒度空间控制与时序理解能力显著提升。在多个开源LVLMs上达到当前最佳水平，并在IntentVC挑战赛中获得亚军。

Conclusion: IntentVCNet方法有效提升了LVLM对视频中目标对象与用户意图的精准建模能力，可用于复杂的意图导向视频描述任务，具有较大应用前景。

Abstract: Intent-oriented controlled video captioning aims to generate targeted
descriptions for specific targets in a video based on customized user intent.
Current Large Visual Language Models (LVLMs) have gained strong instruction
following and visual comprehension capabilities. Although the LVLMs
demonstrated proficiency in spatial and temporal understanding respectively, it
was not able to perform fine-grained spatial control in time sequences in
direct response to instructions. This substantial spatio-temporal gap
complicates efforts to achieve fine-grained intention-oriented control in
video. Towards this end, we propose a novel IntentVCNet that unifies the
temporal and spatial understanding knowledge inherent in LVLMs to bridge the
spatio-temporal gap from both prompting and model perspectives. Specifically,
we first propose a prompt combination strategy designed to enable LLM to model
the implicit relationship between prompts that characterize user intent and
video sequences. We then propose a parameter efficient box adapter that
augments the object semantic information in the global visual context so that
the visual token has a priori information about the user intent. The final
experiment proves that the combination of the two strategies can further
enhance the LVLM's ability to model spatial details in video sequences, and
facilitate the LVLMs to accurately generate controlled intent-oriented
captions. Our proposed method achieved state-of-the-art results in several open
source LVLMs and was the runner-up in the IntentVC challenge. Our code is
available on https://github.com/thqiu0419/IntentVCNet.

</details>


### [75] [COT-AD: Cotton Analysis Dataset](https://arxiv.org/abs/2507.18532)
*Akbar Ali,Mahek Vyas,Soumyaratna Debnath,Chanda Grover Kamra,Jaidev Sanjay Khalane,Reuben Shibu Devanesan,Indra Deep Mastan,Subramanian Sankaranarayanan,Pankaj Khanna,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 本文提出了COT-AD，这是一个用于棉花作物分析的大规模数据集，包含多样化的图像及详细标注，推动基于计算机视觉的农业研究。


<details>
  <summary>Details</summary>
Motivation: 当前棉花领域缺乏专门的、高质量、覆盖全生长周期的数据集，制约了基于深度学习和计算机视觉的作物管理、病虫害识别等研究与应用的发展。

Method: 作者采集了25000多张覆盖棉花不同生长周期的图像，包括航拍田块图像和高分辨率单反相机疾病图像，并对其中5000张进行了精确标注，涵盖病虫害、植被、杂草分析等多种任务类型。

Result: 构建了COT-AD数据集，支持分类、分割、图像复原、增强、合成与早期病害管理等多种下游研究和应用任务，补齐了以棉花为核心的农业数据集空白。

Conclusion: COT-AD为棉花作物管理、智能农业、病害早期发现等数据驱动任务提供了坚实基础，将促进数字农业在棉花领域的发展。

Abstract: This paper presents COT-AD, a comprehensive Dataset designed to enhance
cotton crop analysis through computer vision. Comprising over 25,000 images
captured throughout the cotton growth cycle, with 5,000 annotated images,
COT-AD includes aerial imagery for field-scale detection and segmentation and
high-resolution DSLR images documenting key diseases. The annotations cover
pest and disease recognition, vegetation, and weed analysis, addressing a
critical gap in cotton-specific agricultural datasets. COT-AD supports tasks
such as classification, segmentation, image restoration, enhancement, deep
generative model-based cotton crop synthesis, and early disease management,
advancing data-driven crop management

</details>


### [76] [Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models](https://arxiv.org/abs/2507.18534)
*Xingyu Qiu,Mengying Yang,Xinghua Ma,Dong Liang,Yuzhen Li,Fanding Li,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: 提出了EDA方法，将扩散模型中的噪声类型由高斯噪声拓展为任意噪声模式，在三项医学影像及自然图像消除任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的EDM扩散模型仅使用高斯噪声，这在图像修复中会破坏输入图像并降低恢复效果，因此需探索适用于不同噪声类型的扩散模型设计。

Method: 提出EDA框架，允许在扩散模型中自定义任意噪声注入方式。理论上证明了增加噪声复杂度不会带来额外恢复计算开销，并保持EDM模块灵活性。

Result: 在MRI偏置场校正、CT金属伪影消除和自然图像阴影去除三项任务上进行实验。EDA只需5步采样便超过大多数特定任务方法，在偏置场校正和阴影去除上达到了SOTA表现。

Conclusion: EDA成功拓展了扩散模型的噪声设计空间，兼顾灵活性和高效性，为不同类型噪声下的图像修复问题带来了性能与效率的显著提升。

Abstract: EDM elucidates the unified design space of diffusion models, yet its fixed
noise patterns restricted to pure Gaussian noise, limit advancements in image
restoration. Our study indicates that forcibly injecting Gaussian noise
corrupts the degraded images, overextends the image transformation distance,
and increases restoration complexity. To address this problem, our proposed EDA
Elucidates the Design space of Arbitrary-noise-based diffusion models.
Theoretically, EDA expands the freedom of noise pattern while preserving the
original module flexibility of EDM, with rigorous proof that increased noise
complexity incurs no additional computational overhead during restoration. EDA
is validated on three typical tasks: MRI bias field correction (global smooth
noise), CT metal artifact reduction (global sharp noise), and natural image
shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA
outperforms most task-specific methods and achieves state-of-the-art
performance in bias field correction and shadow removal.

</details>


### [77] [TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation](https://arxiv.org/abs/2507.18537)
*Zhekai Chen,Ruihang Chu,Yukang Chen,Shiwei Zhang,Yujie Wei,Yingya Zhang,Xihui Liu*

Main category: cs.CV

TL;DR: 该论文提出了TTS-VAR，这是一个用于视觉自回归生成模型的通用测试时扩展框架，通过算法创新实现了在生成内容时既高效又提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前视觉生成模型的规模扩展非常依赖昂贵的训练和计算资源，因此迫切需要一种无需重新训练即可提升模型生成表现的方法，即测试时扩展，但现有工作对此关注有限且方法不够通用。

Method: 作者将视觉自回归（VAR）模型的生成过程视为寻路问题，提出自适应递减batch size以动态平衡效率与探索能力，并结合两大组件：1）在粗尺度阶段进行基于聚类的多样性搜索，通过语义特征聚类保留结构多样性，便于后续选优；2）在细尺度阶段进行重采样式潜力选择，通过多尺度历史定义潜力分数，优先选择有前景的候选样本。

Result: 在强大的VAR模型Infinity上实验证明，该框架能将GenEval分数从0.69提升到0.75，提升幅度为8.7%。

Conclusion: 早期结构特征对生成内容的最终质量影响显著，不同尺度下的重采样效果也有差异。TTS-VAR为实际内容生成提供了一种计算资源友好且有效的测试时扩展新范式。

Abstract: Scaling visual generation models is essential for real-world content
creation, yet requires substantial training and computational expenses.
Alternatively, test-time scaling has garnered growing attention due to resource
efficiency and promising performance. In this work, we present TTS-VAR, the
first general test-time scaling framework for visual auto-regressive (VAR)
models, modeling the generation process as a path searching problem. To
dynamically balance computational efficiency with exploration capacity, we
first introduce an adaptive descending batch size schedule throughout the
causal generation process. Besides, inspired by VAR's hierarchical
coarse-to-fine multi-scale generation, our framework integrates two key
components: (i) At coarse scales, we observe that generated tokens are hard for
evaluation, possibly leading to erroneous acceptance of inferior samples or
rejection of superior samples. Noticing that the coarse scales contain
sufficient structural information, we propose clustering-based diversity
search. It preserves structural variety through semantic feature clustering,
enabling later selection on samples with higher potential. (ii) In fine scales,
resampling-based potential selection prioritizes promising candidates using
potential scores, which are defined as reward functions incorporating
multi-scale generation history. Experiments on the powerful VAR model Infinity
show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights
reveal that early-stage structural features effectively influence final
quality, and resampling efficacy varies across generation scales. Code is
available at https://github.com/ali-vilab/TTS-VAR.

</details>


### [78] [Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping](https://arxiv.org/abs/2507.18541)
*Chong Cheng,Zijian Wang,Sicheng Yu,Yu Hu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的无先验位姿的3D高斯点云重建方法，实现了在大量室外无定位图片下准确重建三维场景，并在Waymo和KITTI数据集上取得了最新最好表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点云重建方法依赖精确的相机位姿与点云初始化，但这些往往难以获得，特别是在面对成百上千张无位姿图片时，当前常用的多视图立体(MVS)方法受限于内存和准确性，难以有效扩展。

Method: 该方法将输入图片分块，利用预训练MVS模型产生子点云，再统一映射到全局空间。其核心在于引入概率Procrustes映射策略，将数量巨大的点云以概率方式闭式对齐，并通过软阈值机制剔除不确定匹配。接着，方法将3DGS渲染与解析雅可比联合，实现几何与相机位姿的联合优化与高斯生成。

Result: 本方法在Waymo和KITTI大规模无位姿图片上，能够在几分钟内对数十万点的点云和相机位姿进行全局优化，实现了精确的三维重建与位姿估计。结果远超现有的无位姿3DGS重建方法。

Conclusion: 该工作摆脱了全局准确相机位姿与点云的前提，提出了一套高效且鲁棒的无先验3D重建方案，大大提升了该领域在实际大规模室外场景下的可用性和性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D
representation. Its effectiveness largely depends on precise camera poses and
accurate point cloud initialization, which are often derived from pretrained
Multi-View Stereo (MVS) models. However, in unposed reconstruction task from
hundreds of outdoor images, existing MVS models may struggle with memory limits
and lose accuracy as the number of input images grows. To address this
limitation, we propose a novel unposed 3DGS reconstruction framework that
integrates pretrained MVS priors with the probabilistic Procrustes mapping
strategy. The method partitions input images into subsets, maps submaps into a
global space, and jointly optimizes geometry and poses with 3DGS. Technically,
we formulate the mapping of tens of millions of point clouds as a probabilistic
Procrustes problem and solve a closed-form alignment. By employing
probabilistic coupling along with a soft dustbin mechanism to reject uncertain
correspondences, our method globally aligns point clouds and poses within
minutes across hundreds of images. Moreover, we propose a joint optimization
framework for 3DGS and camera poses. It constructs Gaussians from
confidence-aware anchor points and integrates 3DGS differentiable rendering
with an analytical Jacobian to jointly refine scene and poses, enabling
accurate reconstruction and pose estimation. Experiments on Waymo and KITTI
datasets show that our method achieves accurate reconstruction from unposed
image sequences, setting a new state of the art for unposed 3DGS
reconstruction.

</details>


### [79] [A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration](https://arxiv.org/abs/2507.18551)
*Daniil Morozov,Reuben Dorent,Nazim Haouchine*

Main category: cs.CV

TL;DR: 本文提出了一种用于术中三维超声(iUS)与术前磁共振(MRI)配准的新颖交叉模态特征点描述子，并实现了自动、鲁棒的影像配准，比现有方法取得更好的匹配和配准效果。


<details>
  <summary>Details</summary>
Motivation: iUS与MRI之间在外观、分辨率和视野范围上存在显著差异，导致它们实时配准一直是未解决的问题。现有方法在自动化、鲁棒性和可解释性方面存在不足，因此亟需新的方法提升多模态医学影像配准性能。

Method: 该方法通过将MRI生成合成的iUS影像进行匹配，实现有监督的对比学习训练共享描述子。训练中采用基于课程的triplet loss和动态负样本挖掘，增强描述子的鲁棒性和旋转不变性。检测到的特征点用于实现稀疏匹配和基于刚性的三维配准。

Result: 在ReMIND数据集上，论文方法在11名患者上配准平均精度达到69.8%，平均配准误差2.39 mm，优于主流的特征点匹配算法。

Conclusion: 该方法无需人工初始化，对视野变化鲁棒、可解释性佳，在iUS与MRI配准任务中展现出优越性能，具有良好的实际应用前景。相关代码已开源。

Abstract: Intraoperative registration of real-time ultrasound (iUS) to preoperative
Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe
modality-specific differences in appearance, resolution, and field-of-view. To
address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS
matching and registration. Our approach employs a patient-specific
matching-by-synthesis approach, generating synthetic iUS volumes from
preoperative MRI. This enables supervised contrastive training to learn a
shared descriptor space.
  A probabilistic keypoint detection strategy is then employed to identify
anatomically salient and modality-consistent locations. During training, a
curriculum-based triplet loss with dynamic hard negative mining is used to
learn descriptors that are i) robust to iUS artifacts such as speckle noise and
limited coverage, and ii) rotation-invariant . At inference, the method detects
keypoints in MR and real iUS images and identifies sparse matches, which are
then used to perform rigid registration. Our approach is evaluated using 3D
MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach
outperforms state-of-the-art keypoint matching methods across 11 patients, with
an average precision of $69.8\%$. For image registration, our method achieves a
competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg
benchmark.
  Compared to existing iUS-MR registration approach, our framework is
interpretable, requires no manual initialization, and shows robustness to iUS
field-of-view variation. Code is available at
https://github.com/morozovdd/CrossKEY.

</details>


### [80] [VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding](https://arxiv.org/abs/2507.18552)
*Baoyao Yang,Wanyun Li,Dixin Chen,Junxiang Chen,Wenbin Yao,Haifeng Lin*

Main category: cs.CV

TL;DR: 本文提出了VideoMind，一个视频为中心的全模态数据集，旨在提升视频内容认知和多模态特征表达能力。该数据集包含10.3万条视频样本，并为每条视频系统性地配备了三层次的文本描述，特别突出意图表达。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集多仅关注表层信息，缺乏对视频深层认知如意图表达的支持，难以推动深度视频理解与多模态对齐相关研究。因此，亟需一个能刻画视频事实、抽象及意图等多层次语义的视频数据集。

Method: VideoMind数据集收集了超过10万段配音频视频，并为每条样本提供了三层次结构化文本描述（事实层、抽象层、意图层），通过Chain-of-Thought（COT）方法驱动大多模态语言模型依次推理生成深层认知表达，并细致标注人物、地点、时间、事件、动作和意图等要素。此外，构建了3000条人工审核的金标准样本作为评测基线，并设计混合集成的认知检索实验与多层级检索衡量指标以评估模型性能。

Result: 提供了完整的多层次、多要素标注视频数据集和公开金标基准，公开了InternVideo、VAST、UMT-L等模型在深度认知检索任务上的评测结果。

Conclusion: VideoMind作为一套全新的视频深度认知与多模态对齐基准，有力推动了情感/意图识别等需要深度理解的视频任务研究，数据集和评测工具已全面开源。

Abstract: This paper introduces VideoMind, a video-centric omni-modal dataset designed
for deep video content cognition and enhanced multi-modal feature
representation. The dataset comprises 103K video samples (3K reserved for
testing), each paired with audio and systematically detailed textual
descriptions. Specifically, every video and its audio is described across three
hierarchical layers (factual, abstract, and intent), progressing from surface
to depth. It contains over 22 million words, averaging ~225 words per sample.
VideoMind's key distinction from existing datasets is its provision of intent
expressions, which require contextual integration across the entire video and
are not directly observable. These deep-cognitive expressions are generated
using a Chain-of-Thought (COT) approach, prompting the mLLM through
step-by-step reasoning. Each description includes annotations for subject,
place, time, event, action, and intent, supporting downstream recognition
tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually
validated samples for evaluating deep-cognitive video understanding. We design
hybrid-cognitive retrieval experiments, scored by multi-level retrieval
metrics, to appropriately assess deep video comprehension. Evaluation results
for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a
powerful benchmark for fine-grained cross-modal alignment and advances fields
requiring in-depth video understanding, such as emotion and intent recognition.
The data is publicly available on GitHub, HuggingFace, and OpenDataLab,
https://github.com/cdx-cindy/VideoMind.

</details>


### [81] [Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation](https://arxiv.org/abs/2507.18558)
*Yihong Feng,Chaitanya Pallerla,Xiaomin Lin,Pouya Sohrabipour Sr,Philip Crandall,Wan Shou,Yu She,Dongyi Wang*

Main category: cs.CV

TL;DR: 本论文提出了一套自动生成带标注的仿真鸡胴体图像的方法，并新建了真实标注数据集，用于提升屠宰线鸡只分割AI模型的性能。结果表明，仿真数据可大幅提升分割模型表现，减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 在家禽屠宰加工企业，自动检测鸡胴体对质量控制、食品安全和工作效率至关重要。但先进的实例分割深度学习模型需要大量标注真实数据，人工采集标注既费时又费力。

Method: 作者开发了一个能够生成高质量、自动标注仿真鸡胴体图像的管线，并创建了一个包含300张真实标注图片的新数据集。利用这些数据，在主流实例分割模型中测试了合成数据与真实数据的结合对分割表现的提升。

Result: 实验发现，合成数据与少量真实标注数据结合能够在所有实例分割模型上显著提升对鸡胴体的分割性能。

Conclusion: 合成数据增强是一种缓解数据匮乏、减少人工标注、提升AI检测系统性能的可行且有效方法，在家禽加工行业具有重要应用价值。

Abstract: The poultry industry has been driven by broiler chicken production and has
grown into the world's largest animal protein sector. Automated detection of
chicken carcasses on processing lines is vital for quality control, food
safety, and operational efficiency in slaughterhouses and poultry processing
plants. However, developing robust deep learning models for tasks like instance
segmentation in these fast-paced industrial environments is often hampered by
the need for laborious acquisition and annotation of large-scale real-world
image datasets. We present the first pipeline generating photo-realistic,
automatically labeled synthetic images of chicken carcasses. We also introduce
a new benchmark dataset containing 300 annotated real-world images, curated
specifically for poultry segmentation research. Using these datasets, this
study investigates the efficacy of synthetic data and automatic data annotation
to enhance the instance segmentation of chicken carcasses, particularly when
real annotated data from the processing line is scarce. A small real dataset
with varying proportions of synthetic images was evaluated in prominent
instance segmentation models. Results show that synthetic data significantly
boosts segmentation performance for chicken carcasses across all models. This
research underscores the value of synthetic data augmentation as a viable and
effective strategy to mitigate data scarcity, reduce manual annotation efforts,
and advance the development of robust AI-driven automated detection systems for
chicken carcasses in the poultry processing industry.

</details>


### [82] [Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement](https://arxiv.org/abs/2507.18565)
*Muhammad Imran Zaman,Nisar Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的联合年龄和性别识别方法，并在实际广告投放场景下验证其有效性。通过优化卷积神经网络架构，实现了这两项任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对面部图像识别中年龄和性别任务通常被独立处理的问题，本研究希望通过联合学习和共享特征提升识别效果，以更好地服务于目标广告。

Method: 设计了一种定制的卷积神经网络（CNN），同时优化年龄和性别分类功能。训练时使用了经预处理的大型多样化人脸数据集，增强模型对光照、姿态、图像质量变化的适应能力，并研究了不同网络架构和超参数的影响。

Result: 模型在性别分类上取得了95%的准确率，年龄估计算法实现了5.77年的平均绝对误差。分析显示，对于年轻人群的年龄估计存在一定难度。

Conclusion: 联合建模年龄与性别可提升分类性能，但对年轻年龄段的年龄估计还有改进空间。未来需在数据增强和模型优化方面进一步研究。

Abstract: This paper presents a novel deep learning-based approach for simultaneous age
and gender classification from facial images, designed to enhance the
effectiveness of targeted advertising campaigns. We propose a custom
Convolutional Neural Network (CNN) architecture, optimized for both tasks,
which leverages the inherent correlation between age and gender information
present in facial features. Unlike existing methods that often treat these
tasks independently, our model learns shared representations, leading to
improved performance. The network is trained on a large, diverse dataset of
facial images, carefully pre-processed to ensure robustness against variations
in lighting, pose, and image quality. Our experimental results demonstrate a
significant improvement in gender classification accuracy, achieving 95%, and a
competitive mean absolute error of 5.77 years for age estimation. Critically,
we analyze the performance across different age groups, identifying specific
challenges in accurately estimating the age of younger individuals. This
analysis reveals the need for targeted data augmentation and model refinement
to address these biases. Furthermore, we explore the impact of different CNN
architectures and hyperparameter settings on the overall performance, providing
valuable insights for future research.

</details>


### [83] [Facial Demorphing from a Single Morph Using a Latent Conditional GAN](https://arxiv.org/abs/2507.18566)
*Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 该论文提出了一种新的去混合（demorphing）方法，将混合人脸分解为其原始成分图片，并且能有效处理未知混合技术和不同风格的人脸。该方法超越了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 混合人脸攻击可以让同一张伪造图像与多个真实身份匹配，现有检测方法只能判断是否混合但无法还原原始人脸；已有去混合方法存在与混合图极为相似及对生成方式过度依赖的问题。

Method: 作者提出一种基于潜在空间分解的去混合方法。该方法先在合成数据上训练，无需假定测试与训练使用相同的混合技术，通过在潜在空间中分离成分实现对实拍人脸及任意混合方式的去混合。

Result: 该方法在真实人脸和不同混合方式下测试，去混合结果比现有方法效果好很多，能高保真地复原出原始人脸。

Conclusion: 提出的潜在空间分解去混合方法显著提升了恢复质量且具有很强泛化能力，可为混合攻击的溯源提供更强有力的技术支撑。

Abstract: A morph is created by combining two (or more) face images from two (or more)
identities to create a composite image that is highly similar to both
constituent identities, allowing the forged morph to be biometrically
associated with more than one individual. Morph Attack Detection (MAD) can be
used to detect a morph, but does not reveal the constituent images. Demorphing
- the process of deducing the constituent images - is thus vital to provide
additional evidence about a morph. Existing demorphing methods suffer from the
morph replication problem, where the outputs tend to look very similar to the
morph itself, or assume that train and test morphs are generated using the same
morph technique. The proposed method overcomes these issues. The method
decomposes a morph in latent space allowing it to demorph images created from
unseen morph techniques and face styles. We train our method on morphs created
from synthetic faces and test on morphs created from real faces using arbitrary
morph techniques. Our method outperforms existing methods by a considerable
margin and produces high fidelity demorphed face images.

</details>


### [84] [Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis](https://arxiv.org/abs/2507.18569)
*Yanzuo Lu,Yuxi Ren,Xin Xia,Shanchuan Lin,Xing Wang,Xuefeng Xiao,Andy J. Ma,Xiaohua Xie,Jian-Huang Lai*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识蒸馏方法（ADM），针对现有分布匹配蒸馏（DMD）存在的模式崩溃问题，通过引入对抗式判别器实现更高效且稳定的扩散模型蒸馏，在图像和视频生成任务上取得了新基准。


<details>
  <summary>Details</summary>
Motivation: DMD蒸馏方法虽可将预训练的扩散教师模型压缩至高效生成器，但受限于逆向KL散度最小化，容易出现模式崩溃问题，影响生成多样性。因此需要新的方法克服该缺陷。

Method: 提出Adversarial Distribution Matching（ADM）框架，利用基于扩散的判别器，在潜在空间对真实与生成分数估计器对齐，并采用对抗方式进行蒸馏；在极端的一步蒸馏中，引入混合判别器（潜空间+像素空间）；预训练阶段用ODE分布损失优化初始化，再进入ADM微调，整合为DMDX完整流程。

Result: 在SDXL数据集上一步生成任务中，DMDX优于DMD2，且耗费更少GPU资源；多步ADM蒸馏在SD3-Medium、SD3.5-Large与CogVideoX数据集上，创造新的高效图像与视频生成基准。

Conclusion: ADM及其完整DMDX流程克服了逆向KL带来的生成缺陷，提升了单步及多步扩散模型蒸馏的效率和多样性，可推动高效生成技术的发展。

Abstract: Distribution Matching Distillation (DMD) is a promising score distillation
technique that compresses pre-trained teacher diffusion models into efficient
one-step or multi-step student generators. Nevertheless, its reliance on the
reverse Kullback-Leibler (KL) divergence minimization potentially induces mode
collapse (or mode-seeking) in certain applications. To circumvent this inherent
drawback, we propose Adversarial Distribution Matching (ADM), a novel framework
that leverages diffusion-based discriminators to align the latent predictions
between real and fake score estimators for score distillation in an adversarial
manner. In the context of extremely challenging one-step distillation, we
further improve the pre-trained generator by adversarial distillation with
hybrid discriminators in both latent and pixel spaces. Different from the mean
squared error used in DMD2 pre-training, our method incorporates the
distributional loss on ODE pairs collected from the teacher model, and thus
providing a better initialization for score distillation fine-tuning in the
next stage. By combining the adversarial distillation pre-training with ADM
fine-tuning into a unified pipeline termed DMDX, our proposed method achieves
superior one-step performance on SDXL compared to DMD2 while consuming less GPU
time. Additional experiments that apply multi-step ADM distillation on
SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient
image and video synthesis.

</details>


### [85] [HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation](https://arxiv.org/abs/2507.18575)
*Xinyu Wang,Jinghua Hou,Zhe Liu,Yingying Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种结合Transformer和Mamba的新型架构HybridTM，并通过细粒度整合策略提升3D语义分割性能，在多个数据集上取得业内领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法因注意力机制模型表现突出，但因其二次复杂度，对大规模点云的长距离依赖建模有限；而Mamba基方法处理效率高（线性复杂度），但3D特征提取能力较弱，因此亟需一种有效结合二者优点的新方法来提升3D语义分割效果。

Method: 作者提出了HybridTM架构，将Transformer与Mamba进行有效融合，并创新性地提出内层混合策略（Inner Layer Hybrid Strategy），在更细粒度上同时利用注意力机制和Mamba的优势，实现对长距离与局部特征的多重捕捉。

Result: HybridTM在室内（ScanNet、ScanNet200）和室外（nuScenes）多种数据集上进行了大量实验证明，取得了优异性能，达到了SOTA水平，效果具有强泛化性。

Conclusion: 通过结合Transformer的长距离建模能力和Mamba的高效特征处理能力，HybridTM实现了高效、准确的3D语义分割，为后续相关研究提供了新方向。

Abstract: Transformer-based methods have demonstrated remarkable capabilities in 3D
semantic segmentation through their powerful attention mechanisms, but the
quadratic complexity limits their modeling of long-range dependencies in
large-scale point clouds. While recent Mamba-based approaches offer efficient
processing with linear complexity, they struggle with feature representation
when extracting 3D features. However, effectively combining these complementary
strengths remains an open challenge in this field. In this paper, we propose
HybridTM, the first hybrid architecture that integrates Transformer and Mamba
for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid
Strategy, which combines attention and Mamba at a finer granularity, enabling
simultaneous capture of long-range dependencies and fine-grained local
features. Extensive experiments demonstrate the effectiveness and
generalization of our HybridTM on diverse indoor and outdoor datasets.
Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet,
ScanNet200, and nuScenes benchmarks. The code will be made available at
https://github.com/deepinact/HybridTM.

</details>


### [86] [DRWKV: Focusing on Object Edges for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18594)
*Xuecheng Bai,Yuxiang Wang,Boyu Hu,Qinyuan Jie,Chuanzhi Xu,Hongru Xiao,Kechen Li,Vera Chung*

Main category: cs.CV

TL;DR: 提出了一种新颖的低光照图像增强模型DRWKV，能在极端照明退化下有效增强图像边缘和结构细节，并在多项指标和下游任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前低光照图像增强方法难以在极端照明下同时保持物体边缘连续性与细节结构，现有方法在边缘保真和结构恢复方面存在不足。

Method: 1）提出Global Edge Retinex（GER）理论，将图像的光照与边缘结构有效解耦，提高边缘还原度；2）引入Evolving WKV Attention机制，通过螺旋扫描提升对空间边缘连续性和不规则结构的建模能力；3）设计Bilateral Spectrum Aligner（Bi-SAB）与定制损失函数MS2-Loss，实现亮度和色度特征的对齐，提升自然感和减轻伪影。

Result: 在五个低光照图像增强基准上，DRWKV在PSNR、SSIM和NIQE等评价指标均取得领先，且保持较低的计算复杂度。同时，在低光照多目标跟踪等下游任务中也能提升性能，说明具备较强的泛化能力。

Conclusion: DRWKV模型能够在低光照环境下实现高质量的图像增强，有效恢复边缘细节和结构，兼顾性能、自然性和计算效率。

Abstract: Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.

</details>


### [87] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种创新的3D软件自动化生成方法Scenethesis，能够实现用户需求与3D软件元素之间的细粒度映射和修改，并显著提升约束满足度及生成效果。


<details>
  <summary>Details</summary>
Motivation: 随着软件界面从传统2D转向3D空间，现有自动化生成技术大多集中于2D，3D软件自动生成缺乏灵活度和精准度，难以应对复杂的空间和语义约束。因此，需要新的方法来实现对3D界面的精细控制和更好满足用户需求。

Method: 作者提出了Scenethesis方法，基于自定义领域特定语言ScenethesisLang，将自然语言需求转化为具备约束表达能力的中间表示，并将3D软件生成流程分阶段处理，实现对元素的独立验证、针对性修改与系统化约束满足。

Result: 实验表明Scenethesis可以准确捕捉80%以上用户需求，满足90%以上硬性约束，可同时处理100+约束条件，并在BLIP-2视觉评测上比现有方法提升42.8%。

Conclusion: Scenethesis有效提升了3D软件自动生成的灵活性和精度，为需求驱动、可追溯和约束感知的空间界面设计提供了崭新方案，优于现有技术。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [88] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新的高效零样本领域自适应方法SIDA，通过生成合成图像并利用其风格特征，替代以往依赖文本描述的方式，有效建模复杂的真实世界变异，并降低适应时间。


<details>
  <summary>Details</summary>
Motivation: 现有零样本领域自适应通常依赖CLIP的嵌入空间及文本描述来模拟目标领域风格，但这种方法难以捕捉复杂的、真实世界中的风格变化，且显著增加适应过程的时间。作者希望找到更高效且能处理复杂多样变化的方案。

Method: 提出了SIDA方法，先生成细致的、接近源域的合成图像，再通过图像翻译方法使其具有目标域风格。利用这些合成图像的风格特征作为目标域的代理，并设计Domain Mix和Patch Style Transfer两个模块：前者混合多种风格扩展域内表示，后者为不同图像块赋予不同风格，提高建模真实变化的能力。

Result: 在多个零样本领域自适应场景下，SIDA方法获得了业界领先的性能表现，尤其在处理具有挑战性的领域时效果突出。同时，大幅降低了适应所需的时间。

Conclusion: SIDA利用图像而非文本作为风格线索，有效增强了领域自适应的能力，更适于复杂现实场景，且在效率上有明显优势。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


### [89] [Identifying Prompted Artist Names from Generated Images](https://arxiv.org/abs/2507.18633)
*Grace Su,Sheng-Yu Wang,Aaron Hertzmann,Eli Shechtman,Jun-Yan Zhu,Richard Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个新的基准，用于识别由文本到图像生成模型中调用的艺术家风格，即通过图片预测提示词中出现的艺术家名字。研究涵盖多种测试情景并公开数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型存在争议，即用户常常通过明确指定画家的名字来生成特定风格的图片，例如“in the style of Greg Rutkowski”。如何识别模型生成图片背后的艺术家提示，对于内容归属、版责及模型监管具有重要意义。

Method: 作者构建了包含195万张图片、110位艺术家的大规模数据集，设计了四种泛化测试情景：未出现艺术家、提示词复杂度提升、多艺术家混合提示、不同生成模型。评估了多种识别方法，包括特征相似度、对比风格描述符、数据归因方法、有监督分类器和小样本原型网络。

Result: 实验表明，有监督与小样本方法在已知艺术家和复杂提示下表现突出，而风格描述符对风格特征鲜明的艺术家有较好迁移性。多艺术家混合提示识别难度最大。整体来看，现有方法依然有很大提升空间。

Conclusion: 本文公开了数据集与基准测试平台，为评测和提升文本到图像生成模型的艺术家识别能力提供了工具基础。这一工作有助于模型的合理监管和责任归属，推进相关领域的研究。

Abstract: A common and controversial use of text-to-image models is to generate
pictures by explicitly naming artists, such as "in the style of Greg
Rutkowski". We introduce a benchmark for prompted-artist recognition:
predicting which artist names were invoked in the prompt from the image alone.
The dataset contains 1.95M images covering 110 artists and spans four
generalization settings: held-out artists, increasing prompt complexity,
multiple-artist prompts, and different text-to-image models. We evaluate
feature similarity baselines, contrastive style descriptors, data attribution
methods, supervised classifiers, and few-shot prototypical networks.
Generalization patterns vary: supervised and few-shot models excel on seen
artists and complex prompts, whereas style descriptors transfer better when the
artist's style is pronounced; multi-artist prompts remain the most challenging.
Our benchmark reveals substantial headroom and provides a public testbed to
advance the responsible moderation of text-to-image models. We release the
dataset and benchmark to foster further research:
https://graceduansu.github.io/IdentifyingPromptedArtists/

</details>


### [90] [Captain Cinema: Towards Short Movie Generation](https://arxiv.org/abs/2507.18634)
*Junfei Xiao,Ceyuan Yang,Lvmin Zhang,Shengqu Cai,Yang Zhao,Yuwei Guo,Gordon Wetzstein,Maneesh Agrawala,Alan Yuille,Lu Jiang*

Main category: cs.CV

TL;DR: Captain Cinema提出了一种自动生成短电影的系统，通过文本描述生成连续的关键帧，并基于这些关键帧合成完整影片，实现剧情和视觉的高度连贯与质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频方法难以处理长程叙事和多场景视频，容易丧失剧情连贯性和视觉一致性。该工作旨在解决如何从完整电影描述自动生成视觉和叙事连贯的高质量短片，满足创意内容生产的需求。

Method: 方法分为两步：首先，提出顶部-底部关键帧规划(top-down keyframe planning)，根据文本故事线生成覆盖全剧的关键帧序列；其后，利用包含关键帧条件的视频生成模型（基于适配长上下文的多模态扩散Transformer MM-DiT），合成关键帧间的视频动态场景。为提升多场景长电影的稳定高效生成，特定设计了交错式训练策略，并使用特别整理的电影数据集训练模型。

Result: 实验结果显示，Captain Cinema在自动生成视觉连贯、叙事一致的高质量短电影方面表现优异，生成效率高，能够胜任多场景、长时段故事的视频自动合成任务。

Conclusion: Captain Cinema框架有效解决了长剧情短电影自动生成中的关键难题，并在视觉和故事叙事一致性方面表现出色，为自动化影视内容创作带来了全新工具。

Abstract: We present Captain Cinema, a generation framework for short movie generation.
Given a detailed textual description of a movie storyline, our approach firstly
generates a sequence of keyframes that outline the entire narrative, which
ensures long-range coherence in both the storyline and visual appearance (e.g.,
scenes and characters). We refer to this step as top-down keyframe planning.
These keyframes then serve as conditioning signals for a video synthesis model,
which supports long context learning, to produce the spatio-temporal dynamics
between them. This step is referred to as bottom-up video synthesis. To support
stable and efficient generation of multi-scene long narrative cinematic works,
we introduce an interleaved training strategy for Multimodal Diffusion
Transformers (MM-DiT), specifically adapted for long-context video data. Our
model is trained on a specially curated cinematic dataset consisting of
interleaved data pairs. Our experiments demonstrate that Captain Cinema
performs favorably in the automated creation of visually coherent and narrative
consistent short movies in high quality and efficiency. Project page:
https://thecinema.ai

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习（RL）框架Shop-R1，用于提高大语言模型（LLMs）在人类行为仿真，尤其是在线购物环境中的推理能力。该方法在推理能力和行为预测方面均有显著提升，实验结果比传统方法提高了65%以上。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要依赖LLM生成的推理过程（rationale）和有监督微调（SFT）来提升模型推理和行为预测能力，但它们的性能受限于生成推理时模型本身的推理水平，难以实现更高的仿真度。

Method: Shop-R1将人类行为仿真任务分解为推理生成和行为预测两个阶段，并分别采用不同的奖励信号进行引导。推理生成环节利用模型内部信号（如logit分布）进行自监督引导；在行为预测环节，设计了具有难度感知缩放的分层奖励结构，既考察高层次动作类型，也考察细粒度动作细节的正确性，并按难度分配奖励，防止奖励被滥用。

Result: 使用Shop-R1的方法，相较于传统基线方法在任务上的性能提升超过65%。

Conclusion: 提出的Shop-R1框架能有效提升LLMs在人类行为仿真中的推理和行动预测能力，证明了其在模拟真实在线购物行为时的优势。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [92] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种动态且通用的流程奖励建模方法——DG-PRM，有效提升了大模型在复杂任务中的泛化与表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的流程奖励建模大多依赖启发式方法，难以在不同领域实现泛化。现有‘LLM-as-judge’方法只关注反馈结果，忽略了文本中的有用指导信息。同时，静态且粗粗粒度的评价标准难以适应复杂的流程监管需求。

Method: 作者提出DG-PRM，构建奖励树以捕捉和存储细粒度、多维度的奖励标准，并动态选择奖励信号，对过程各步进行评分。此外，引入帕累托支配估算以区分多维奖励信号中的正负样本对。

Result: 实验表明，DG-PRM在多个主流基准测试中获得了卓越成绩，有效提升了密集奖励任务中大模型的表现。方法还表现出很好的领域外泛化能力。

Conclusion: DG-PRM方法能够捕捉多方面的细粒度奖励信号，并动态适配不同任务流程，极大提升了模型的泛化能力和奖励建模效果，在复杂场景下具有广泛应用前景。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [93] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: 本研究提出并实现了VeriMinder系统，通过大模型和结构化引导，检测与缓解自然语言数据库分析中的认知偏见，从而帮助用户提出更公正的分析问题并提升数据分析质量。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言接口数据库系统（NLIDB）普及，越来越多缺乏统计背景的用户直接用自然语言进行数据分析。然而，如何避免用户在分析中无意引入偏见、提出‘错误问题’成为新的紧迫挑战。以往研究主要关注text-to-SQL生成准确性，用户认知偏见问题鲜有探讨。

Method: 提出VeriMinder交互系统，包含三大创新：(1) 语境相关的偏见检测语义映射框架；(2) 基于‘难以变更（Hard-to-Vary）’原则的结构化数据分析引导框架；(3) 基于大语言模型的高质量任务型prompt生成机制，包括多候选、批判反馈、自我反思流程。系统以网页应用形式供用户交互。

Result: 用户测试中，82.5%的参与者反馈VeriMinder提高了分析质量。在与其他系统比较时，无论具体性、全面性还是准确性，VeriMinder表现均领先至少20%。代码已开源。

Conclusion: VeriMinder能有效缓解NLIDB分析中的偏见，帮助用户避免‘问错问题’，提升分析质量。作为开源工具，有望促进社区进一步实践和研究。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [94] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本文提出一种高效的端到端自动口语评分系统，能够同时处理多题型口语测试，为大规模计算机辅助语言学习提供实际解决方案，并在挑战赛中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前自动口语评分系统普遍依赖逐题建模和文字转录，这导致计算资源消耗大，应用受限。因此，开发一个无需转录、能一次性处理多题型且高效准确的自动评分系统，成为紧迫需求。

Method: 作者基于Whisper-small编码器，设计了一个统一的端到端模型，用单个编码器处理所有四个口语回答，然后通过轻量聚合模块整合信息，最终输出总评分。同时，提出了一种数据采样策略，提升了对类别不平衡数据的训练效率。

Result: 所提出系统在2025 Speak & Improve Challenge中达到了0.384的RMSE，优于基于文本的基线（0.44），模型参数量仅为168M（约为Whisper-small的70%）。采样策略下训练只用44.8%说话人也实现了0.383的RMSE，表现出极佳的数据利用效率和对不平衡类别的鲁棒性。

Conclusion: 该端到端系统在准确性、效率和资源占用上均有提升，证明了其在大规模自动口语评测领域的实际可用性和优越性。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [95] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 本文分析了六种主流AI文本检测工具对新兴LLM DeepSeek生成文本的识别能力，特别考察了在对抗性攻击（如同义改写和“人性化”改写）下的表现。


<details>
  <summary>Details</summary>
Motivation: LLM广泛应用引起了文本真实性及学术诚信问题，市面检测工具对主流模型（如ChatGPT）有已有探讨，但针对新模型DeepSeek的检测效果缺研究。

Method: 选取六个可用的AI检测工具，对比其检测原始、同义改写和人性化改写后的DeepSeek生成文本的准确性。并尝试使用DeepSeek通过few-shot与思维链提示作为检测工具。实验样本来源为49条人工QA及其对应AI生成文本，扩展得196条对抗性变体。

Result: QuillBot与Copyleaks对原始及同义改写文本识别几乎完美，但多款检测器如AI Text Classifier和GPT-2表现不稳定。在“人性化”改写攻击下，所有检测器准确率显著下降（最高71%）。DeepSeek few-shot+思维链检测法表现优异，五次提示下只错判一例。

Conclusion: 主流检测工具对DeepSeek文本识别表现差异显著，对抗性改写尤其“人性化”技术可大幅降低检测准确率。基于DeepSeek自身的“少样本、思维链”检测方法表现优异，提示主流检测工具有被规避风险，需提升鲁棒性。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [96] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 本文提出了一种新的贝叶斯一致性系数（BCC）指标，研究了大语言模型（LLMs）在面对新证据时其“信念”更新过程是否更符合贝叶斯定理。通过自建数据集和多系列预训练语言模型实验证明，参数量和能力更强的模型在信念赋值上更加贝叶斯一致。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注语言模型的表现与规模、能力的关系，但较少探究其“信念”更新是否科学合理，特别是与贝叶斯定理的符合度。作者希望发现大模型是否更具合理的、不矛盾的概率推理能力，有助于更好理解和监管 LLMs。

Method: 提出贝叶斯一致性系数（BCC）度量模型基于证据调整信念的贝叶斯一致性。构建特定数据集，对五类语言模型的不同参数规模与训练量下进行 BCC 测试，并与常见基准测试得分对比分析。

Result: 实验表明，参数量更大、能力更强的预训练模型在 BCC 指标上表现更佳，说明它们基于证据进行信念更新时更符合贝叶斯定理；并展示了模型参数规模、训练数据量与 BCC 之间的正相关性。

Conclusion: 更大、更强的语言模型不仅表现好，而且其信念更新也更贝叶斯一致，这对评估、理解与监管LLMs具重要意义。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [97] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文综述了2011至2025年间40余项针对提格利尼亚语（Tigrinya）自然语言处理（NLP）研究的进展，并公开相关元数据，为未来研究提供指引。


<details>
  <summary>Details</summary>
Motivation: 提格利尼亚语有数百万母语者，但在NLP领域中严重资源匮乏，相关研究较少。该工作旨在系统梳理这一语言在NLP上的研究进展和现有挑战。

Method: 通过系统查阅和分析2011-2025年间40余篇文献，对提格利尼亚语NLP的基础资源、模型及在十余个下游任务（如形态处理、机器翻译、语音识别、问答等）的应用现状进行分类梳理。

Result: 研究显示，提格利尼亚语NLP已从早期基于规则的系统逐步发展为现代的神经网络架构，且每次突破基本都伴随新资源的建立。但其复杂的形态结构和资源稀缺始终是主要挑战。

Conclusion: 提出未来有前景的研究方向，包括基于形态学的建模、跨语言迁移学习以及以社区为中心开发资源。同时，该综述为相关研究提供全面参考与路线图，并开放了调研成果的元数据。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [98] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文介绍了TeleChat系列最新模型：TeleChat2、TeleChat2.5 和 T1，相对于旧版TeleChat，在模型结构仅微小调整的情况下，通过改进的预训练与后训练策略，实现了性能的大幅提升，并且开源了多个版本以支持开发者与研究者。


<details>
  <summary>Details</summary>
Motivation: 现有TeleChat模型在语言理解、推理、代码生成等任务中存在性能瓶颈，尤其是在复杂推理与多样场景应用下难以与最先进专有模型竞争。因此，作者旨在通过提升训练策略显著增强模型性能，并满足不同应用对于速度与推理能力的需求。

Method: 首先，TeleChat2以10万亿高质量多样化tokens进行预训练，再通过SFT和DPO微调。TeleChat2.5与T1在此基础上引入了增量式领域连续预训练，并结合强化学习提升代码生成和数学推理能力。其中T1专为复杂长链式推理设计，TeleChat2.5则侧重推理速度，所有旗舰型号均为115亿参数的稠密Transformer架构。

Result: 在各类推理与任务处理能力方面，上述模型较原TeleChat显示出显著进步。特别是T1-115B在数学与编程等任务上的表现超过了OpenAI的o1-mini和GPT-4o等专有模型。

Conclusion: 新一代TeleChat系列通过先进的训练与优化策略实现了对各种应用场景的优异适配，性能媲美乃至超越主流专有大模型，并通过开源推动开发者和研究者的进一步创新。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [99] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型知识编辑方法NeuralDB，能在不损害模型通用能力的情况下，高效编辑和扩展模型知识。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型知识编辑方法（如Locate-and-Edit）在面对大规模、多事实编辑时，可能导致模型泛化能力下降和遗忘问题，限制了模型更新和应用。

Method: 作者将线性L&E方法建模为键值数据库查询，并提出了NeuralDB：一种将编辑事实明确表示为神经网络键值数据库的框架，并配有非线性门控检索模块，只有在推理涉及所编辑事实时才触发。这样可以大幅提升编辑效率且保护原有模型能力。

Result: 在ZsRE和CounterFacts数据集上、使用GPT2-XL、GPT-J (6B)和Llama-3 (8B)进行大规模（最高10万条事实）编辑实验显示，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性等方面都优于以往方法，并能保持下游多项任务性能。

Conclusion: NeuralDB显著提升了大模型可编辑性，并解决了以往方法在大规模编辑下的遗忘和性能退化问题，为模型高效迭代和知识更新提供了有效工具。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [100] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出了一种高效的推理时操控方法GrAInS，通过定位并作用于对输出最有影响的token，实现精细、可解释的模型行为调整，无需重新训练或额外监督，且在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前推理时操控LLM和VLM主要通过固定、全局的向量干预，忽视了输入token的因果影响及logits的有用梯度。尤其在多模态任务下，视觉和文本信息贡献不均，这些方法难以高效利用模型信息，导致操控能力有限。

Method: GrAInS以Integrated Gradients方法获取每个token对模型偏好输出的贡献度，筛选出正负影响最大的top-k token，基于这些token构建方向性操控向量。在推理阶段，将这些向量引导下的信号注入transformer隐层激活，同时对激活进行规范化，以保证语义控制的精细化和解释性。整个方法无需改变模型权重，也不依赖额外监督。

Result: GrAInS在多种任务和模型上效果显著：在TruthfulQA上比Llama-3.1-8B微调提升13.22%准确率；在MMHal-Bench数据集上，LLaVA-1.6-7B的幻觉率从0.624降至0.514；在SPA-VL的对齐得分上提升8.11%。同时，模型原有流畅性和通用能力得以保持。

Conclusion: GrAInS为模型行为操控提供了高效、解释性强且通用的推理时方法，无需重新训练或外部监督，跨语言和视觉语言任务表现优异，有望推动大模型实际可控性的发展。

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [101] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 本文探索利用大型语言模型（LLM）生成短语断句预测所需的合成标注数据，从而减少人工标注需求并提高数据一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 目前短语断句预测依赖大量人工音频或文本标注，人工成本高，而且语音领域的天然变异性导致获得高质量、一致性数据极为困难。该研究受到LLM在NLP领域生成合成数据、缓解数据挑战的启发，试图拓展到语音断句任务。

Method: 采用LLM生成短语断句的合成标注数据，并与传统人工标注方法进行对比，在多语种场景下评估LLM合成数据的效果。

Result: 结果表明，基于LLM生成的合成数据对短语断句任务中的数据问题具有显著缓解效果。

Conclusion: LLM合成数据生成方法在短语断句预测中的应用可显著降低数据采集和标注成本，并在多语言场景下有效，为语音领域解决类似数据瓶颈问题提供了可行新途径。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [102] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）生成的文本类合成数据在多样性和隐私方面的表现，提出了一套评估指标，并发现当前模型生成的数据在多样性和隐私上存在明显不足。作者据此提出了一种基于提示的优化方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛用于生成合成数据以辅助模型训练，其数据的多样性与隐私风险成为亟需关注的问题。合成数据的效用和安全直接影响下游应用，因此有必要系统评估这些关键性质。

Method: 作者提出了一组新的定量评估指标，专注于合成文本数据的多样性（涵盖语言表达、情感、用户视角）和隐私性（重新识别风险及风格异常）。对多个先进LLM生成的数据进行实验评估，并基于评估结果提出了提升数据多样性的基于提示的生成方案。

Result: 实验显示，现有LLM生成的合成数据在语言、情感、多视角等多样性方面及隐私保护方面均有明显不足。此外，所提指标有效检测并量化了这些问题。基于提示的生成方法可提升评论的多样性且能兼顾隐私。

Conclusion: 当前LLM生成文本数据在多样性和隐私保护方面尚不理想，亟需改进。基于本文提出的指标和优化方法，可为后续研究和实际应用中更好地生成安全且多样的合成数据提供有价值的参考。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [103] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了TELEVAL评测体系，用于更贴近现实场景地评估中文口语语言模型（SLMs）在对话任务中的表现，发现当前模型仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有SLM评测主要聚焦复杂任务，与现实用户实际对话场景不符，缺乏对自然交流和隐含语义处理能力的评估，因此需要一个更贴合用户实际体验的评测框架。

Method: 提出了TELEVAL动态基准测试，从显式语义、言语副语言及隐式语义、系统能力三个维度，针对真实中文对话情境设计评测，分开考核文本和音频输出，尤其强调模型提取用户言语中的隐含线索并恰当回应的能力。

Result: 实验证明，尽管SLMs近年来取得了进步，但在自然的现实对话任务中表现依然有限，距离理想状况仍有很大提升空间。

Conclusion: TELEVAL为更好反映用户体验、促进对话导向SLM的发展提供了新的评测框架，对推动相关技术进步具有实际意义。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [104] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 提出了一种结合BOFT和LoRA-GA优点的混合高效微调方法，在提升大模型微调效率和泛化能力的同时显著降低计算与内存资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）规模庞大，直接对其进行全量微调（fine-tuning）在计算和内存资源上开销巨大，因此需要研发高效、低资源消耗的参数高效微调（PEFT）方法，满足实际部署需求。

Method: 文中系统评估了现有主流PEFT方法（如LoRA、BOFT、LoRA-GA、uRNN），提出了一种新颖的混合微调策略：动态结合了BOFT的正交稳定性与LoRA-GA的梯度快速收敛特性，通过基于梯度范数的分层自适应更新实现高效训练。同时首次将单位RNN（uRNN）原理引入LLM微调，用结构化单元约束优化梯度稳定性。

Result: 在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用规模从7B到405B模型，实验结果表明所提混合方法在收敛速度和泛化性能上均优于单一PEFT基线，准确率逼近全量微调，同时训练时间减少高达2.1倍、内存使用减半。

Conclusion: 混合PEFT策略实现了对LLM高效、可扩展的微调，显著缓解了资源瓶颈，为实际部署大模型提供了实用解决方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [105] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 本文介绍并评估了2024年新版英语GloVe词向量模型，涵盖数据来源、预处理和模型表现，并与2014年原版模型进行了比较。新模型更好地适应了当前语言和文化变化，在结构性任务和最新NER任务中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 原有2014年GloVe模型已广泛应用，但世界与语言持续演变，原模型未覆盖近年新词，且缺乏详细的数据和预处理文档，需构建与记录更符合现有语料的新版词向量。

Method: 作者基于Wikipedia、Gigaword以及Dolma的子集训练了两套新版词向量，并详细记录了数据版本与预处理流程。模型通过词表对比、直接测试和命名实体识别（NER）任务进行评估。

Result: 新GloVe词向量更好地引入了当下文化与语言相关的新词，并在词类类比、相似性等结构性任务上与原版相当，在最新的、时间敏感的NER数据（如非西方新闻语料）上表现更优。

Conclusion: 2024年新版GloVe模型兼具对新兴词汇的覆盖和结构任务的稳健性，并在特定NER任务上优于旧版，适合当前实际应用。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [106] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: GOAT-SLM是一款关注于语言之外特征（如情感、方言、年龄等）的新型语音语言模型，其在理解与生成更自然、富有表现力的语音对话方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管当前端到端语音语言模型在自然对话上取得很大进步，但大多数模型忽视了语音中的情感、方言、年龄等副语言和说话人特征，这限制了AI的表达和适应能力。

Method: GOAT-SLM采用双模态解耦架构，将语言建模与声学实现分开，支持鲁棒的语义理解与表达丰富的语音生成；并提出模块化、分阶段的训练策略，逐步对齐语义、情感和说话人特征，用大规模语音-文本数据集训练模型。

Result: 在TELEVAL等多维度基准测试中，GOAT-SLM在语义任务和非语义任务上均实现了优异且均衡的表现，在情感、方言和年龄敏感交互等方面超越了现有开源模型。

Conclusion: 本工作强调了对语音中非语言信息建模的重要性，推动了更自然、自适应且具社会意识的语音语言系统发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [107] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架，用以衡量多模态大语言模型（MLLM）在多模态数学推理中基于代码进行视觉操作的能力，实验证明目前主流模型距离人类水平还有明显差距。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在多模态数学推理方面取得进展，但现有评估多关注文本推理，缺乏对模型通过代码精确执行视觉操作能力的系统性评价。因此，作者希望填补这一空白，深入研究模型在可视化推理环节的真实能力。

Method: 作者提出了两个新的评估任务：1）多模态代码生成（MCG），评估模型从头构建可视化的能力；2）多模态代码编辑（MCE），考察模型在细粒度视觉操作（如删除、修改、注释）方面的能力。为实现评测，作者设计了包含几何图形、函数图、三类统计图共五类主流数学图形的数据集，对九个主流MLLM进行实验。

Result: 实验结果显示，尽管模型在简单可视化任务上有一定能力，但在需要细致视觉操作的任务上，所有被测MLLM都明显落后于人类，尤其是在精细编辑方面表现较差。

Conclusion: 当前的MLLM在基于代码的多模态数学推理与视觉操作上仍有较大提升空间，新提出的评测框架和数据集为未来模型开发和评估提供了有力工具。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [108] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 本文提出了HIVMedQA基准，用于评估大语言模型在HIV管理中的问答能力，并比较了多种常规及医学专用LLM的表现。研究显示Gemini 2.5 Pro整体表现最佳，但整体上模型准确率受问题复杂度影响，医学专用和超大模型也不一定优于通用模型。


<details>
  <summary>Details</summary>
Motivation: HIV管理复杂，包括多样的治疗选择、共病及依从性问题，临床决策负担重。当前LLM在医疗领域应用有限，实际能力和安全性缺乏充分评估，亟需针对真实临床场景进行标准化测试和分析。

Method: 研究制定了HIVMedQA问答基准，涵盖由感染病医生参与编辑的高质量HIV临床问题。对七个通用型和三个医学专用LLM模型进行了评估，采用提示工程优化模型表现，通过词汇相似性和LLM自评相结合的框架，从多个维度量化模型表现。

Result: Gemini 2.5 Pro在大多数评估维度上表现优异，且前3名有2为专有模型。医学专用或更大规模的模型未必更优，且随问题复杂度上升，所有模型表现均下降。推理和理解能力总体弱于知识回忆，模型还表现出一定的认知偏差。

Conclusion: 尽管LLM在HIV管理问答显示出潜力，但存在准确性、偏见与安全性等问题。为确保其临床集成，亟需面向医疗场景的针对性开发及细致评估，推动LLM更好地服务临床实用需求。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [109] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: Transformer文本嵌入模型存在所谓“sticky tokens”，这些特殊token会极大影响句子相似度判定和下游任务表现。作者系统性分析了sticky tokens，并提出了高效检测方法，显示需改进分词和模型设计。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer文本嵌入在NLP任务中表现优越，但作者发现某些特殊token反复插入却能异常地影响句子嵌入分布和下游任务效果，因此需要找出并解决这一问题。

Method: 作者正式定义了sticky tokens，并提出基于句子和token筛选的快速检测工具Sticky Token Detector（STD）。他们将STD应用于14类预训练模型的40个检查点，分析sticky tokens的来源及其对任务的影响，并通过注意力层分析探究其内部表征机制。

Result: 共检测出868个sticky tokens，主要来自词表中特殊项、未用词或多语料拆分子词。sticky tokens影响与模型和词表规模无显著相关性。在文本聚类、检索等任务上，sticky tokens会导致性能下降高达50%。注意力层分析发现这些token在内部表征中占据主导地位。

Conclusion: sticky tokens对文本嵌入模型和NLP下游任务有显著负面影响，暴露出现有分词和模型设计的鲁棒性问题，未来需改进分词策略和优化模型架构以减缓其影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [110] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 现有大型语言模型在选择题测试中存在通过利用选项顺序或标签偏好获得高分的问题，而非实际理解题目。本文提出SCOPE评估框架，旨在消除这种选择偏差并提升评测公平性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评测中，模型可能通过识别多选题选项的固有偏好（如选项位置规律性）来刷高分数，掩盖了模型实际理解能力，影响评测的公正性和说服力，因此需有新的无数据集依赖的评测与去偏方法。

Method: SCOPE框架通过反复输入无语义内容的提示，估计模型对于不同选项位置的偏差分布，再基于分布进行逆向重排，均衡答对概率（lucky-rate）；同时避免将语义接近的干扰项与正确答案相邻，以阻断模型利用表层线索的近似猜测。

Result: 在多个基准评测实验中，SCOPE方法比现有去偏技术获得更稳定的性能提升，对正确选项信心水平分布也更清晰，展现较高评估公平性。

Conclusion: SCOPE为LLM的评测公平性和可靠性提供了新标准，能有效减少选择性偏差影响，并为模型能力的真实评估奠定基础。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [111] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 本文指出，在电信网络中进行根因分析（RCA）是一项关键但极具挑战性的任务，尤其是对于现有AI方法而言。


<details>
  <summary>Details</summary>
Motivation: 电信网络中出现故障时，快速准确地查明根本原因对于网络的可靠运行至关重要。然而，由于网络结构的复杂性和现实场景下权威数据（基准）的缺乏，现有AI算法难以胜任。

Method: 文章主要分析了AI在电信网络中RCA面临的困难，尤其在处理基于图的复杂推理和缺乏真实基准方面。

Result: 目前AI在应对电信网络RCA任务时遇到了困难，尤其是在图结构推理和真实基准数据缺乏这两方面。

Conclusion: AI在解决电信网络中的根因分析问题时受到限制，需要开发更适合图推理和更真实的基准数据以推动领域发展。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [112] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 本文探讨了知识管理体系（KMS）在ISO30401标准下如何与现有业务流程紧密结合，并通过SECI模型和PDCA循环机制完善管理体系。


<details>
  <summary>Details</summary>
Motivation: 尽管业务流程建模被广泛用于提升组织效率、对齐战略目标，但对于已符合或接近ISO 9001标准的组织来说，如何将新兴的ISO30401知识管理体系与现有流程融合，成为实施过程中的难题。作者希望阐明知识开发与传递如何与运营流程集成。

Method: 作者复盘了ISO9001下的流程建模原则，结合自身作为ISO30401实施者的经验，探索了如何通过部署SECI模型机制并结合PDCA循环步骤来落地知识管理体系。

Result: ISO30401标准下的知识管理活动可以通过合理建模，有效集成进组织的管理体系流程，且SECI模型与PDCA循环为此提供了实际可行的执行路径。

Conclusion: 知识管理体系不是孤立存在的，它必须与组织的全部流程系统集成，通过过程建模与管理创新，能够实现知识有效开发、传递与管理，进而提升组织整体管理水平。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [113] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为GMTP的新方法，有效检测并过滤RAG系统中的对抗性投毒文档，显著提高安全性。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）模型虽然可以通过外部知识提升LLM表现，但是依赖外部文档带来了被恶意投毒的风险，攻击者可通过注入恶意文档影响生成内容。因此，提升RAG系统安全性并检测恶意文档成为亟需解决的问题。

Method: 提出基于梯度的掩码token概率（GMTP）方法：分析检索器的相似度函数梯度，识别对模型影响较大的token，并将这些关键token进行掩码操作，然后通过掩码语言模型（MLM）检测被掩码部分的概率。对于恶意注入的token，MLM给出的概率通常偏低，借此识别和过滤有害文档。

Result: 实验显示，GMTP可以在保留相关文档的同时，过滤掉90%以上的对抗性投毒内容，在各种数据集和对抗场景下都能保持RAG系统的高性能。

Conclusion: GMTP显著提升了RAG系统在面临对抗性攻击时的安全性和鲁棒性，有望在实际应用中广泛采用。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [114] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文分析了指令微调对大语言模型（LLMs）易受用户输入虚假信息影响的影响，发现指令微调增强了模型对虚假信息的接受度，尤其在用户输入时更为显著。


<details>
  <summary>Details</summary>
Motivation: 虽然指令微调提升了LLMs对用户指令的遵循能力，但也可能导致模型过度依赖用户输入，进而更容易接受和生成虚假信息。现有研究多关注模型对与其知识库相悖的信息的接纳情况，对指令微调直接导致的影响关注较少。

Method: 作者通过对比指令微调后的模型与基础模型，系统性分析了模型对用户输入虚假信息的接受程度，并考察了提示结构中用户角色、虚假信息长度及系统警告等因素对误导性的影响。

Result: 结果显示，经过指令微调的LLMs更容易接受用户输入的虚假信息，相比基础模型对用户信息的依赖性更强，且虚假信息的表现受提示结构及警告有一定影响。

Conclusion: 研究表明，指令微调虽能提升模型可用性，但也带来对虚假信息更高的易感性，因此需要采取系统性措施，缓解其副作用，提升模型在实际应用场景中的可靠性。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [115] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Prune&Comp的新型层裁剪方法，有效提升大语言模型压缩与加速后的性能表现。


<details>
  <summary>Details</summary>
Motivation: 层裁剪有助于压缩和加速大语言模型，但直接移除层会导致隐藏状态的数值幅度产生显著断层，带来性能大幅下降。作者希望解决因层移除产生的数值幅度缺口问题，从而提升裁剪后的模型表现。

Method: 作者提出Prune&Comp：首先估算裁剪某层引起的幅度断层，然后通过对剩余参数线下重新缩放，补偿该幅度差异，无需额外训练，也不增加推理时计算量。同时结合迭代式裁剪以进一步增强效果。

Result: 在LLaMA-3-8B等模型上，Prune&Comp配合主流裁剪指标进行5层裁剪时，困惑度下降近一半，问答性能保持原模型的93.19%，比基线提升4.01%。

Conclusion: Prune&Comp可有效缓解层裁剪带来的性能退化问题，提升模型压缩后的实用性和表现，对无训练量加速大模型有明显价值。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [116] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种全新的定位-聚焦方法，有效提升了语音翻译中术语翻译的准确性，并在多项数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译（ST）虽然受到关注，但其中的术语准确翻译仍具有挑战性。现有方法虽引入翻译知识，却容易受到无关噪音影响，知识利用不充分。

Method: 提出Locate-and-Focus方法：首先，准确定位语音中包含术语的片段来构建翻译知识，减少无关信息对ST模型的干扰；然后，将这些知识与语音和文本层面的原文、译文假设关联，使模型在翻译时更关注相关知识。

Result: 实验结果显示，该方法能够有效定位术语并提升术语翻译成功率，同时保持了整体翻译性能的稳健性。

Conclusion: 所提方法优化了术语翻译的准确性，证明了精确定位和聚焦知识对直接语音翻译模型的重要意义。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [117] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 这篇论文对现有六种OCR引擎在两种低资源语言（僧伽罗语和泰米尔语）上的零样本性能进行了比较分析，发现不同引擎在某些语言上表现突出，并引入了一套新的合成泰米尔语OCR基准数据集。


<details>
  <summary>Details</summary>
Motivation: 虽然拉丁语及其衍生文字的OCR已基本解决，但对于使用独特文字的低资源语言（LRL），OCR依旧是个未解难题。该研究旨在填补这一领域的空白，尤其是对于僧伽罗语和泰米尔语，探索商业与开源OCR系统在零样本场景下的表现。

Method: 对六种不同的OCR引擎（包括商业和开源产品）在僧伽罗语和泰米尔语文本上进行了实验，部分引擎受限于语言支持。采用五种测量方法，在字符和词两个层面评估其准确率。

Result: 结果显示，Surya引擎在僧伽罗语OCR所有指标上表现最佳，词错误率仅2.61%；Document AI在泰米尔语上表现最佳，字符错误率低至0.78%。

Conclusion: 尽管现有OCR系统对低资源语言的适配能力差异较大，但部分系统已能在特定语言上实现较高性能。此外，研究还为泰米尔语提供了新的合成基准数据集，为进一步研究打下基础。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [118] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: 本文提出了一种名为StyleAdaptedLM的框架，可以高效地将特定风格（如品牌语调等）迁移到大型语言模型，不需要风格-任务配对数据，也不会影响模型完成任务的能力。


<details>
  <summary>Details</summary>
Motivation: 企业用户越来越需要LLM具备特定的风格（如品牌语调或作者风格），但大规模风格数据往往不具备指令-回复结构，导致风格迁移难以兼顾任务遵循性。

Method: 作者提出用LoRA适配器，先在无结构风格语料上训练适配器，然后与指令遵循模型合并，这样不需要成对数据，也能得到具备风格和任务能力的模型。

Result: 实验证明，该方法在多个数据集和模型上显著提升风格一致性，同时不损失任务遵循性。人工评测也确认模型更好地掌握了品牌风格。

Conclusion: StyleAdaptedLM为LLM实现高效、低成本的风格个性化提供了新途径，在维持任务能力同时，灵活地支持品牌或作者风格定制。

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [119] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型推理模型（LRMs）的新型后门攻击——“过度思考后门”，能够在不影响最终答案正确性的情况下，让模型显著增加推理链的长度，从而造成资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型具备强大的链式思维推理能力，但相关安全隐患研究不足。本文关注于如何利用这些推理特征实施隐蔽攻击，特别是制造无形的资源消耗负担。

Method: 提出“可调节过度思考后门”——通过数据投毒，将带有触发词数量提示的输入与冗余推理过程配对，并利用教师型大模型自动生成带有冗余步骤的推理链，从而精确控制模型的推理冗长度，实现隐蔽资源消耗攻击。

Result: 实验证明，该方法可在并不降低答案正确性的情况下，实现推理过程多倍增长，并具备高度可控性和隐蔽性，已在多种大型推理模型上进行验证。

Conclusion: “过度思考后门”利用LRMs的推理特性，为AI安全领域提供了新的攻击视角，可作为一种重要的资源消耗攻击，呼吁后续需加强对该类隐蔽威胁的防御。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [120] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文研究了机器翻译系统在处理性别模糊的源语时的表现，发现当前系统即使在不确定性情境下也常表现出过度自信或带有偏见。


<details>
  <summary>Details</summary>
Motivation: 在机器翻译中，面对源语未明显标记性别但目标语要求性别的情况，模型需从上下文或外部知识推断性别。已有研究指出模型易受刻板印象影响，且即使与上下文冲突也可能使用不合适的性别。本文提出，模型不仅应在性别明确时自信选择正确性别，在性别模糊时也应表现出适当的不确定性。

Method: 作者采用近期提出的语义不确定性评估指标，分析主流机器翻译系统在不明确与明确性别情境下的翻译表现，并考察消除偏见手段在不同情况下的独立效果。

Result: 研究发现，尽管一些模型在性别明确的情境下译文和性别准确率均较高，但在性别模糊的句子上，并没有表现出应有的不确定性。同时，消除偏见的方法在有歧义和无歧义的情形下表现出相互独立的影响。

Conclusion: 当前机器翻译系统在性别模糊时缺乏对不确定性的处理，未来应关注模型在模糊情境中的不确定性表达与消除偏见的综合效果。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [121] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: 本文提出了TDR框架，通过解耦任务并引入大模型反馈，提升了大模型上下文学习时的示例检索质量，并在30个NLP任务上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的上下文学习能力依赖于高质量示例，但现有方法难以区分不同任务的数据分布，且很难将大模型的反馈细致地用于优化检索模块。为提升多任务环境下的示例检索效果，亟需解决上述难题。

Method: 提出TDR框架，对不同任务的示例进行解耦，确保检索模块能针对特定目标任务检索合适示例；并对大模型的细粒度反馈进行建模，用于监督和指导检索模块的训练，提升检索质量。

Result: 在30个NLP任务上的大量实验证明，TDR方法在所有数据集上均带来了性能提升，并取得了新的SOTA（最优）结果。

Conclusion: TDR是一种易于集成的检索增强方案，能够显著提升大模型的上下文示例检索质量和下游任务表现；该方法对任何大模型均适用，并已开源。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [122] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文提出结合人工专家与大语言模型（LLM）协同的宣传检测新框架，通过自动-人工混合标注提升标注一致性和大规模应用能力，并借助知识蒸馏让小语言模型高效学习，推动社交媒体宣传检测系统的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中的宣传检测任务复杂且高质量标注数据短缺，人工标注细粒度宣传技术时一致性低，阻碍了高效、可扩展的研究和应用，因此亟需新的方法提升数据标注质量和模型能力。

Method: 作者提出分三级的大纲式宣传技术分类体系，分别实验人工和LLM协助的自动预标注流程：模型先自动抽取宣传片段、解释和类型标签，再由人工验证。进一步用高质量的LLM标注数据替代原始人工标注，对小型语言模型进行有结构的知识蒸馏式微调。

Result: 二次人工验证显示，用LLM辅助后的预标注流程在一致性和标注效率上大幅提升。知识蒸馏后的小模型能高效生成结构化标注信息。

Conclusion: 本研究为社交媒体宣传检测提供了高效、可扩展方案，倡导透明与可追溯的媒体生态，有助于实现可持续发展目标16。代码已开源，便于社区复现与应用。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [123] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: 本文提出了CLEAR，一个用于大型语言模型（LLM）基于误差分析的开源互动包，实现精细化、可操作的模型表现诊断。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测多由其他LLM打分，只提供整体排名或单一分数，无法解释具体原因，导致难以针对性改进模型。

Method: CLEAR首先为每个样本生成文本反馈，再自动提取系统级误差类别并统计各类别的出现频率。工具配有互动仪表盘，可可视化整体表现、灵活筛选具体误差类型或分数区间，并支持追踪具体样例。

Result: CLEAR在RAG及数学基准上进行了分析，并通过用户案例展示其实用价值。

Conclusion: CLEAR为LLM评测提供详尽、操作性更强的误差分析方式，有助于理解和改进模型缺陷。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [124] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 本文分析了多语言维基百科中表格数据的一致性问题，并提出了相关分析方法，对数据对齐与一致性分类进行定量和定性研究。


<details>
  <summary>Details</summary>
Motivation: 尽管维基百科多语言词条覆盖相同主题，但各版本独立编辑，导致内容之间存在事实不一致。这会影响维基百科的中立性和可靠性，继而影响大量依赖维基百科为训练集的AI系统。

Method: 作者提出了一种新的方法学，用于收集、对齐及分析多语言维基百科词条中的表格内容，并制定了不一致类型的分类方法，结合定量和定性指标来评估多语言表格数据的对齐与一致性。

Result: 研究在选定数据集上应用了所开发的分析指标和方法，展示了多语言维基百科表格中存在多种不一致类型，并定量和定性描述了这些不一致的情况。

Conclusion: 多语言维基百科存在结构化内容不一致的问题，这对事实核查、多语知识交互和依赖维基内容设计可靠AI具有重要意义，呼吁后续研究和工具来改善多语言知识一致性。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [125] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: 本文介绍了FinDPO，这是针对金融领域打造的基于人类偏好对齐的金融大语言模型（LLM），在金融情绪分析上显著优于传统有监督微调方法。FinDPO在标准情绪分类基准中平均提升了11%，并首次支持将情绪预测转化为可用于投资组合管理的连续评分，在真实交易环境下实现了67%的年化收益和2.0的夏普比。


<details>
  <summary>Details</summary>
Motivation: 随着线上金融文本对市场影响的增强，情绪分析对于量化观点变得愈发重要。现有基于有监督微调（SFT）的LLM模型难以泛化新样本，且可能陷入过拟合，难以适应瞬息万变且专业性强的金融领域。因此，需开发能根据人类偏好对齐、具较强泛化能力的金融特定大模型。

Method: 论文提出FinDPO框架，采用后训练阶段的人类偏好对齐（Direct Preference Optimization, DPO）方法，弥补SFT不足。FinDPO还提出创新的“logit-to-score”转换方法，将离散情绪预测转为连续可排序概率分数，使模型预测可直接用于资产配置决策。

Result: FinDPO在金融情绪标准分类任务中，平均性能比现有最佳SFT模型高11%。应用所提连续情绪评分策略后，投资组合模拟下年化收益达67%，在扣除现实交易成本（5个基点）下夏普比为2.0，风险收益均表现优异。

Conclusion: FinDPO通过人类偏好对齐极大提升了金融领域LLM的泛化与表现，兼具学术和实际投资应用价值，首创性地实现了情绪评分与资产配置的结合，并在实际交易环境下取得了领先的收益和风险控制能力。

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [126] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: 本文针对大语言模型在处理阿拉伯语表格数据时存在的不足，提出并构建了AraTable基准，系统评估LLMs在阿拉伯语结构化数据理解与推理上的表现，并附带自动化评价框架。


<details>
  <summary>Details</summary>
Motivation: 目前主流LLM在英文结构化数据处理方面已有较多研究和资源，但阿拉伯语因缺乏公开数据和其语言特性，相关基准与研究严重不足。本工作旨在填补阿拉伯语表格推理和理解的评测空白，并推动该领域发展。

Method: 作者设计了包含多种任务（直接问答、事实核查、复杂推理等）的AraTable基准，并采用了“LLM生成+人工过滤/验证”的混合流程保障数据质量。此外，还提出了用自我斟酌机制实现的全自动评测框架，结果与人工评判近似。

Result: 初步实验发现，LLMs在阿拉伯语直接问答任务表现尚可，但在复杂推理和事实核查等高阶任务上仍有明显挑战。自动化评测框架与人工判断结果高度一致。

Conclusion: AraTable为促进阿拉伯语结构化数据处理领域提供了高质量公开资源和评测体系，有望推动底层模型的改进和发展，尤其是在复杂表格推理任务上的性能提升。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [127] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 本文提出了一种基于XLM-RoBERTa-large的Transformer模型用于孟加拉语文本的标点符号恢复，并针对低资源环境进行了数据增强，取得了较高准确率。同时公开了数据集和代码，促进该领域研究。


<details>
  <summary>Details</summary>
Motivation: 由于ASR结果通常缺乏标点，影响文本可读性及后续处理，且孟加拉语等低资源语言缺乏相关标注数据，亟需有效的标点恢复方法。

Method: 采用XLM-RoBERTa-large模型，预测句号、逗号、问号、感叹号四种标点，针对不同文本域进行训练。为缓解数据稀缺问题，构建了大规模多样化语料并应用数据增强，调节增强因子alpha。

Result: 最优模型在新闻测试集上准确率为97.1%，参考集91.2%，ASR集90.2%。模型在不同域及噪声环境下均表现出良好泛化能力。

Conclusion: 工作为孟加拉语标点恢复任务建立了强基线，并公开了数据与代码，对低资源NLP有重要推动意义。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [128] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 本论文系统性回顾了合成临床文本的目的、技术和评估方法，确认其在补充真实数据和解决稀疏与隐私问题方面的有效性。但隐私依然是主要挑战，需要更多人工评估。


<details>
  <summary>Details</summary>
Motivation: 临床NLP面临数据稀疏和隐私保护难题。合成生成临床文本被认为可为这些问题提供有效解决方案，但缺少系统回顾当前方法和评估方式。

Method: 作者系统性检索了主流数据库（如PubMed、IEEE、arXiv等），并筛选出94篇与合成医学自由文本生成相关的论文，从目的、技术手段和评估方法三个角度进行了定量分析。

Result: 自2018年以来，合成医学文本研究关注度大增，主要目的涵盖数据增强、写作辅助、语料建设、隐私保护、标注与实用性。Transformer架构，特别是GPT类模型被广泛采用。评估重点集中在相似度、隐私、结构和实用性，其中以实用性评估最常用。合成文本在下游NLP任务中表现出较好补充作用。

Conclusion: 合成医学文本对提升数据质量和模型表现具有重要价值，但生成文本的隐私风险依然突出，需进一步人工审核。未来技术进步有望加速应用落地，缓解数据使用中的法规障碍。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [129] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: LLMs在生成表格数据时存在关注关系分散的问题，GraDe方法通过引入稀疏依赖图，优化关注机制，有效提升表格数据生成的质量。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的特征关联通常比较稀疏，而LLMs的自注意力机制会平均分配关注，导致对重要关系的关注被稀释，尤其在关系复杂或特征语义不清的表中影响更为明显。因此需要一种能够聚焦关键特征关系的方法。

Method: 提出GraDe方法，将外部提取到的函数依赖转换为稀疏依赖图，显式集成到LLM的注意力机制中。通过轻量的动态图学习模块，强化关键特征互动，抑制无关关系，结构感知地调节模型关注。

Result: 实验证明，在多个真实世界表格数据集上，GraDe方法相较现有LLM方法，在复杂数据集上性能提升最高达12%；在合成数据质量方面同样达到主流最好水平。

Conclusion: GraDe是一种高效且结构感知的表格数据建模方法，对LLMs仅有极小改动、实用且有效，适合实际应用场景。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [130] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 本文比较了主流大语言模型（LLMs）和经过专门微调的变换器模型在道德基础检测任务上的表现，发现LLMs在检测道德内容时误判率高，专门微调模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 道德基础检测对于分析社会话语和开发符合伦理的AI非常重要，但当前LLMs在专业道德推理任务上的能力尚不明确。作者希望系统对比LLMs和微调模型在此类任务中的表现。

Method: 作者在Twitter和Reddit数据集上，采用ROC、PR和DET曲线分析，全面比较了最新LLMs与针对该任务微调的变换器模型的道德基础检测能力。

Result: 结果显示LLMs存在较多漏检（高假阴性率），即使经过提示工程优化，亦难以检测出足够的道德内容，专用微调模型则表现更好。

Conclusion: 在道德推理相关应用中，针对任务的模型微调目前仍比依赖prompt的LLMs方案更有效。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [131] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的命名实体识别方法SRU-NER，能识别嵌套实体并通过多任务学习集成多个生物医学数据集，对缺失标注的情况进行了优化。实验结果显示SRU-NER在生物医学及通用领域中表现出色，提升了跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别面临术语繁杂、数据集标注不一致等难题，尤其是对嵌套实体和多数据集集成的问题。为提高模型实用性和泛化能力，亟需一种能处理嵌套实体且兼容多标注体系的方法。

Method: 提出了SRU-NER方法：基于槽位的循环单元结构，能够识别嵌套命名实体；采用多任务学习策略集成不同数据集；并引入动态损失计算，自动对缺失标注实体类型不予惩罚，减少标注不一致对训练的负面影响。

Result: 经过大量实验（包括跨语料库评测和人工评测），SRU-NER在生物医学和一般领域的命名实体识别任务中取得了有竞争力的表现，特别是在跨领域泛化方面有明显提升。

Conclusion: SRU-NER有效提升了实体识别模型在标注不一致和嵌套实体情况下的精度与泛化性，是应对生物医学NER跨数据集应用挑战的有力方法。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [132] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2是一个统一的信息抽取框架，支持多种NLP任务，具高效和小巧优势，便于部署。


<details>
  <summary>Details</summary>
Motivation: 当前多种信息抽取任务（如实体识别、文本分类等）通常需专用模型或依赖大型语言模型，导致计算成本高，部署困难。研究者希望提出通用且高效的解决方案。

Method: GLiNER2基于预训练Transformer编码器，集成了多任务架构。通过schema驱动接口实现命名实体识别、文本分类、层次结构化数据抽取等任务于同一模型，保持低资源消耗和高部署灵活性。

Result: 实验表明GLiNER2在多种抽取与分类任务中拥有有竞争力的表现，而且在CPU端的部署和可访问性相较于LLM方案有显著提升。

Conclusion: GLiNER2能高效地统一多种信息抽取任务，适宜实际应用环境，已作为开源库发布，便于开发者使用和部署。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [133] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: 本文提出了一个新的多模态机器翻译方法GIIFT，通过构建多模态场景图和跨模态图注意力网络，实现了无需图像输入也能提升翻译效果，并在多项数据集上达到了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 多模态机器翻译已证明视觉信息对提升翻译质量有帮助，但现有方法在视觉与语言的融合、泛化到无图像推理等方面受限，因此需要设计能更好保留模态信息、提升无图像推理能力的新方法。

Method: 提出了多模态场景图用于保留和融合各模态特有的信息，并设计了GIIFT框架，包括两阶段流程和跨模态图注意力网络适配器。训练时融合多模态知识，推理时可泛化到纯文本任务，无需图像参与。

Result: 在Multi30K数据集（英-法、英-德任务）上，GIIFT在无图像条件下也超越了所有现有方法，达到了最新最好水平。在WMT基准测试上，相对无图像翻译基线也有显著提升。

Conclusion: GIIFT有效突破了多模态翻译对图像的依赖，实现了可泛化的无图像多模态推理，对未来机器翻译研究具有推动作用。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [134] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 提出了一种新颖的混合分词策略，通过结合6-mer分词和BPE-600，有效提升了DNA语言模型的性能，并在下游预测任务中超越了现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 传统的k-mer分词在捕获DNA序列的局部结构方面有效，但在token分布均匀性和全局序列上下文理解方面存在局限。论文希望通过新的分词方式来平衡局部与全局特征的表达，提升模型表现。

Method: 将唯一的6-mer token与通过600次BPE循环生成的最优BPE token结合，构建平衡且具有上下文感知能力的词表，用以训练基础DNA语言模型，并在下游任务（如next-k-mer预测）进行微调和评价。

Result: 模型在3-mer、4-mer和5-mer预测准确率分别达到了10.78%、10.1%、4.12%，均优于NT、DNABERT2和GROVER等SOTA模型，证明了该混合分词策略的有效性。

Conclusion: 混合分词能同时保留DNA序列的局部结构和全局信息，对于提升DNA语言模型性能具有重要意义，为后续基因序列分析和生物学研究中的应用奠定了基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [135] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 该论文提出了一种名为WINO的新型解码算法，以解决扩散大语言模型（DLLMs）在实现并行生成时面临的质量与速度的权衡难题。WINO通过创新性的“宽进窄出”机制，实现了可撤销解码，有效提高了DLLMs的推理速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散型大语言模型（DLLMs）虽然支持并行快速生成，但现有方法在加速解码的同时会严重损害生成质量，这是因为传统解码过程不可逆、容易出错且无法修正早期错误。因此，亟需一种新方法来改善DLLMs的质量-速度权衡问题。

Method: 提出WINO算法，无需额外训练。其核心是“并行草稿-校验”机制：模型先并行生成多个候选token（草稿），再利用模型的双向上下文能力对草稿进行校验，对存疑token进行再次掩码并重生成，实现解码过程的可撤销、可修正。

Result: WINO在LLaDA、MMaDA等开源DLLMs上进行了实验证实，相比原始DLLMs，在GSM8K数学基准上推理速度提升6倍，准确率提高2.58%；在Flickr30K图像描述任务中，推理速度提升10倍且性能更好。

Conclusion: WINO算法有效破解了DLLMs并行解码过程中的质量-速度难题，在多个公开任务上显著提升了生成效率与质量，为大模型并行推理提供了更优解。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [136] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 该论文提出了SRAG-MAV框架，在中文仇恨言论识别任务中显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度中文仇恨言论识别（FGCHSR）任务具有挑战性，现有方法在任务抽取结构和鲁棒性、准确率方面存在不足。作者旨在提升该任务的自动化识别性能，尤其是在多标签、信息冗杂和数据分布复杂环境下。

Method: 作者提出SRAG-MAV方法，包括三大创新：(1)将原本的四元组抽取任务转为三元组抽取，简化任务结构；(2)引入自检索增强生成（SRAG），动态从训练集中检索相似样例构建上下文提示；(3)采用多轮累积投票（MAV），多轮推理输出投票以提升稳定性和准确性。主干模型基于Qwen2.5-7B大模型。

Result: 在STATE ToxiCN数据集上，SRAG-MAV系统取得了Hard Score 26.66、Soft Score 48.35和Average Score 37.505，显著超越了GPT-4o（15.63分）和微调Qwen2.5-7B（35.365分）等主流基线。

Conclusion: SRAG-MAV框架能有效提升细粒度中文仇恨言论识别的性能，并在公开数据集上大幅领先于当前主流模型，为复杂语言理解任务提供了切实可行的自动化方案。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [137] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了AQuilt框架，能够低成本、自动化地为专用领域生成高质量指令微调数据，从而提升大模型在专用领域上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用领域表现出色，但在垂直细分或专用领域往往效果不佳。主流的方法依赖于数据合成，利用未标注数据获取领域特性，但计算开销大、泛化性不足。需要一种新的高效可泛化的数据合成方法。

Method: AQuilt框架通过Answer、Question、Unlabeled data、Inspection、Logic和Task type六类元素，结合逻辑推理与自检机制，从未标注专用领域数据自动构建指令微调数据，并支持自定义任务类型。

Result: AQuilt合成了70.3万条高质量训练数据，仅用17%的成本即可达到与DeepSeek-V3相当的效果，且在下游任务相关性上更优。

Conclusion: AQuilt能够在低成本下有效提升专用领域大模型的表现，数据生成质量高且通用性强，有望广泛应用于各类专用领域的指令微调。

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [138] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: 本文提出了TRPrompt框架，将文本反馈直接引入到提示词模型的训练中，实现了无需先验数据集收集的高效提示词优化，在多个数学推理数据集上达到了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有提示词优化方法要么依赖于通用LLM和启发式文本反馈（如“逐步思考”），要么基于数值奖励训练专门的提示词模型，但两者各有局限。作者希望融合两种思路，充分利用文本反馈，提高提示词优化效率和效果。

Method: 提出TRPrompt框架，将LLM生成的文本反馈作为奖励信号，直接用于训练提示词模型。该方法无需事先收集数据集，可通过生成和反馈的迭代不断优化模型能力。

Result: TRPrompt在GSMHard和MATH等高难度数学数据集上，能够生成优于现有方法的针对性提示词，并显著提升LLM的推理表现。

Conclusion: 通过将文本奖励信号纳入提示词模型训练，TRPrompt实现了高效的无监督提示词优化，推动了复杂任务中LLM推理能力的提升。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [139] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 本文提出了基于清单打分的强化学习方法（RLCF），显著提升了大语言模型在指令遵循任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常以固定标准进行语言模型对齐，难以适应用户在不同场景下的多样化需求。本研究希望让模型更灵活地理解和执行各种用户指令，提高其泛用性和人机交互体验。

Method: 作者提出RLCF方法：从用户指令自动提取检查项，通过AI评审与专用校验程序逐项评估模型回应的完成度，综合各项得分生成强化学习奖励信号，并用于模型训练。

Result: RLCF方法在多个主流基准（如FollowBench、InFoBench、Arena-Hard等）均优于其他对齐方法。例如，在FollowBench的hard satisfaction rate提升4个百分点，在InFoBench提升6个百分点，在Arena-Hard赢率升3个百分点。

Conclusion: 基于清单反馈的强化学习为提升语言模型多需求任务适应性提供了有效工具，有望成为未来模型对齐的重要方向。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [140] [PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy](https://arxiv.org/abs/2507.17846)
*Alison Bartsch,Arvind Car,Amir Barati Farimani*

Main category: cs.RO

TL;DR: 本文提出了一种机器人系统PinchBot，能够通过夹捏动作完成简单陶艺制作任务，展现了机器人在复杂可变形物体长序列操控中的能力。


<details>
  <summary>Details</summary>
Motivation: 陶艺制作需要高度精细和复杂的动作，对机器学习和机器人操控能力提出了挑战，尤其是在处理可变形物体和长时序任务时。本文选择陶艺作为研究对象，旨在推动机器人在复杂操控任务中的实际应用。

Method: 作者提出了PinchBot系统，它基于目标条件扩散策略模型，将任务进度预测、碰撞约束投影和预训练3D点云嵌入结合，使用夹捏作为核心动作，实现对陶土的多步操控以达到目标3D形态。

Result: 系统能够成功地利用夹捏动作，自动完成多种简单的陶艺目标造型，实验表明该方法有效地解决了多模态、长时序和可变形物体操控中的关键问题。

Conclusion: 研究展示了PinchBot在简单陶艺任务中的可行性和有效性，为未来处理更复杂可变形物体操控任务提供了有力思路和方法基础。

Abstract: Pottery creation is a complicated art form that requires dexterous, precise
and delicate actions to slowly morph a block of clay to a meaningful, and often
useful 3D goal shape. In this work, we aim to create a robotic system that can
create simple pottery goals with only pinch-based actions. This pinch pottery
task allows us to explore the challenges of a highly multi-modal and
long-horizon deformable manipulation task. To this end, we present PinchBot, a
goal-conditioned diffusion policy model that when combined with pre-trained 3D
point cloud embeddings, task progress prediction and collision-constrained
action projection, is able to successfully create a variety of simple pottery
goals. For experimental videos and access to the demonstration dataset, please
visit our project website:
https://sites.google.com/andrew.cmu.edu/pinchbot/home.

</details>


### [141] [A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.17856)
*Dennis Benders,Laura Ferranti,Johannes Köhler*

Main category: cs.RO

TL;DR: 本文介绍了一种用于移动机器人安全导航的非线性模型预测控制（NMPC）方案，强调了理论、证明和实现的实践路径，重点在于保证安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 在充满障碍物的环境中，移动机器人需要安全地导航，既要遵守状态和输入约束，又要避免碰撞，同时还要应对扰动和测量噪声。现有文献多为综述或侧重理论，缺乏注重实际安全和性能并可操作的NMPC实现指导，因此提出本报告。

Method: 作者提出了逐步指导的NMPC设计方法，为移动机器人提供如何实现安全避障导航的详细流程，通过从理论介绍到数学证明再到实际部署，使理论与应用紧密结合，并针对安全性和鲁棒性进行特别说明。

Result: 报告系统地梳理了安全NMPC的关键步骤和实现细节，可帮助使用者在实际中更好地应对障碍、噪声等不确定因素，并确保算法的可实现性和安全性。

Conclusion: 本文为希望应用NMPC于实际机器人导航的研究人员和工程师提供了理论到实践的清晰桥梁，强调安全和性能保证，有助于推动NMPC在机器人领域中的实际落地和应用。

Abstract: Designing a Model Predictive Control (MPC) scheme that enables a mobile robot
to safely navigate through an obstacle-filled environment is a complicated yet
essential task in robotics. In this technical report, safety refers to ensuring
that the robot respects state and input constraints while avoiding collisions
with obstacles despite the presence of disturbances and measurement noise. This
report offers a step-by-step approach to implementing Nonlinear Model
Predictive Control (NMPC) schemes addressing these safety requirements.
Numerous books and survey papers provide comprehensive overviews of linear MPC
(LMPC) \cite{bemporad2007robust,kouvaritakis2016model}, NMPC
\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},
and their applications in various domains, including robotics
\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.
This report does not aim to replicate those exhaustive reviews. Instead, it
focuses specifically on NMPC as a foundation for safe mobile robot navigation.
The goal is to provide a practical and accessible path from theoretical
concepts to mathematical proofs and implementation, emphasizing safety and
performance guarantees. It is intended for researchers, robotics engineers, and
practitioners seeking to bridge the gap between theoretical NMPC formulations
and real-world robotic applications.
  This report is not necessarily meant to remain fixed over time. If someone
finds an error in the presented theory, please reach out via the given email
addresses. We are happy to update the document if necessary.

</details>


### [142] [OpenNav: Open-World Navigation with Multimodal Large Language Models](https://arxiv.org/abs/2507.18033)
*Mingfeng Yuan,Letian Wang,Steven L. Waslander*

Main category: cs.RO

TL;DR: 本文提出了利用多模态大型语言模型（MLLMs）将自由形式的自然语言导航指令转化为机器人可执行的轨迹序列，实现开放世界环境下的自驱导航。该方法在自主驾驶数据集及真实机器人实验证明了其稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展现了强大的常识推理能力，但如何在开放世界中实现从自由语言描述到具体机器人动作的闭环映射仍然存在挑战。目前方法大多只能调用有限的预定义动作，本研究致力于克服这一限制。

Method: 本方法利用多模态大型语言模型解释、分解复杂的指令，并协同视觉-语言感知模型生成2D鸟瞰图值图，将语义知识与空间信息融合，从而合成完整的导航轨迹点序列。实现了开集合指令和对象场景下的零样本视觉-语言导航。同时在大规模自动驾驶数据集和真实机器人（Husky）上进行了评估。

Result: 该系统不仅在大规模自动驾驶数据集的多样室外导航任务中实现了对自由形式指令的零样本执行，还在真实机器人平台上验证了系统对于对象检测误差及语言歧义的鲁棒性和泛化能力。

Conclusion: 融合MLLM语义理解与地图空间信息让机器人有效理解开放式自然语言指令，完成复杂导航任务，对于提升机器人智能导航具有良好应用前景。

Abstract: Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/

</details>


### [143] [Modular Robot and Landmark Localisation Using Relative Bearing Measurements](https://arxiv.org/abs/2507.18070)
*Behzad Zamani,Jochen Trumpf,Chris Manzie*

Main category: cs.RO

TL;DR: 该论文提出了一种用于由多个独立子系统组成系统的模块化非线性最小二乘滤波方法，结合了协方差交集（CI）算法，防止信息重复计数，并将方法具体应用于机器人-地标定位问题，通过仿真实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在多子系统分布式系统中，如何独立且高效地更新各子系统的状态并解决因子系统间相对观测带来的信息关联与冗余，是实际应用（如机器人定位）中亟待解决的问题。传统的整体联合滤波虽然精度高，但计算和通信量大，缺乏灵活性，因此需要一种既能独立估计又能防止信息重复的新方法。

Method: 作者提出一种模块化非线性最小二乘滤波方法，每个子系统各自独立更新状态和协方差估计；对于涉及多个子系统的相对测量，采用CI算法避免信息重复。文中还提供了基于最小二乘估计的CI算法新推导，便于集成。方法在机器人-地标定位问题中进行了具体实现。

Result: 通过随机仿真实验，将所提模块化方法与传统的整体联合状态滤波器进行对比，分析了各自的优劣权衡。实验还包含了改进型方法，能够在通信和带宽受限时，表现出性能的平滑退化。

Conclusion: 所提模块化非线性最小二乘滤波与CI算法结合的方法，有效防止了信息重复计数，具备高效、灵活、易于分布实现等优点，可广泛应用于需要分布式估计的实际系统中，并能在通信受限场景下保持较优退化性能。

Abstract: In this paper we propose a modular nonlinear least squares filtering approach
for systems composed of independent subsystems. The state and error covariance
estimate of each subsystem is updated independently, even when a relative
measurement simultaneously depends on the states of multiple subsystems. We
integrate the Covariance Intersection (CI) algorithm as part of our solution in
order to prevent double counting of information when subsystems share estimates
with each other. An alternative derivation of the CI algorithm based on least
squares estimation makes this integration possible. We particularise the
proposed approach to the robot-landmark localization problem. In this problem,
noisy measurements of the bearing angle to a stationary landmark position
measured relative to the SE(2) pose of a moving robot couple the estimation
problems for the robot pose and the landmark position. In a randomized
simulation study, we benchmark the proposed modular method against a monolithic
joint state filter to elucidate their respective trade-offs. In this study we
also include variants of the proposed method that achieve a graceful
degradation of performance with reduced communication and bandwidth
requirements.

</details>


### [144] [A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion](https://arxiv.org/abs/2507.18138)
*Min-Gyu Kim,Dongyun Kang,Hajun Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 本文提出了一种结合基于模型与基于学习方法的新型四足机器人步态控制框架，通过集成残差模块提升对高不确定性环境的适应性，在真实机器人验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型方法对模型失配敏感，学习方法虽具适应性但效率有限，因此需要结合两者以提升控制性能和学习效率。

Method: 在基于启发式的足步规划与动力学模型框架各环节集成学习型残差模块，并为每个模块选择合适的学习方法，通过模块化结构抵消模型误差。实际在四足机器人上将此框架与模型预测控制结合进行实验。

Result: 新方法在高不确定性环境下表现出色，超越基线方法，提升了控制性能和学习效率，并增强了标称控制器对参数调优的鲁棒性。实际机器人验证中能稳定维持平衡并跟踪指令速度。

Conclusion: 本文提出的方法有效结合了模型与学习优势，实现了四足机器人在复杂环境下的鲁棒控制，对参数敏感性降低，并具较高的实际可行性。

Abstract: This paper presents a novel approach that combines the advantages of both
model-based and learning-based frameworks to achieve robust locomotion. The
residual modules are integrated with each corresponding part of the model-based
framework, a footstep planner and dynamic model designed using heuristics, to
complement performance degradation caused by a model mismatch. By utilizing a
modular structure and selecting the appropriate learning-based method for each
residual module, our framework demonstrates improved control performance in
environments with high uncertainty, while also achieving higher learning
efficiency compared to baseline methods. Moreover, we observed that our
proposed methodology not only enhances control performance but also provides
additional benefits, such as making nominal controllers more robust to
parameter tuning. To investigate the feasibility of our framework, we
demonstrated residual modules combined with model predictive control in a real
quadrupedal robot. Despite uncertainties beyond the simulation, the robot
successfully maintains balance and tracks the commanded velocity.

</details>


### [145] [Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks](https://arxiv.org/abs/2507.18160)
*Luka Šiktar,Branimir Ćaran,Bojan Šekoranja,Marko Švaco*

Main category: cs.RO

TL;DR: 本文提出了一套基于无人机（UAV）的搜索与救援子系统，能够实现实时人员检测、人脸识别与目标追踪。


<details>
  <summary>Details</summary>
Motivation: 在搜索与救援（SAR）任务中，快速准确地检测、识别与追踪被困人员一直是技术挑战。为提升救援效率与自动化水平，亟需融合高效识别与自动导航技术的智能无人机系统。

Method: 该系统将无人机与ROS2框架集成，运用YOLOv11/YOLOv11-pose用于人员检测与关键点识别，dlib库用于人脸识别。无人机采用系统辨识与PD控制器，实现自主导航。识别特定人员后，可自动或手动采集人脸，开始基于人体关键点的追踪，并保持安全距离。系统参数由IMU数据获取，用于优化PD控制器，结合YOLOv11-pose估算目标距离。

Result: 初步实验涵盖14位已知人员，结果显示该系统可在实时条件下有效完成人员检测、识别与追踪任务。

Conclusion: 该系统在实时性与准确性方面表现突出，有望推广至大型无人机平台，结合GPS导航用于实地救援任务，提升SAR行动的智能化与自动化水平。

Abstract: In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.

</details>


### [146] [MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation](https://arxiv.org/abs/2507.18206)
*Arup Kumar Sahoo,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种基于物理约束神经网络（MoRPI-PINN）的方法，能有效提升移动机器人在仅有惯性传感器输入、导航信号缺失场景下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人在卫星导航或摄像头不可用时，仅依靠惯性传感器会导航漂移，亟需更准确可靠的方法来应对实际复杂场景。

Method: 提出MoRPI-PINN，将物理规律和约束融入神经网络训练，通过增强信号处理及带有蛇形运动的运动策略提升惯性导航精度，并验证该方法的有效性。

Result: 实验证明，MoRPI-PINN导航误差相比其他方案提升超过85%，且算法轻量化可在边缘设备上高效运行。

Conclusion: MoRPI-PINN为移动机器人在极限感知条件下的惯性导航提供了一种通用、精确、实用的新方法，适用于各种常见任务。

Abstract: A fundamental requirement for full autonomy in mobile robots is accurate
navigation even in situations where satellite navigation or cameras are
unavailable. In such practical situations, relying only on inertial sensors
will result in navigation solution drift due to the sensors' inherent noise and
error terms. One of the emerging solutions to mitigate drift is to maneuver the
robot in a snake-like slithering motion to increase the inertial
signal-to-noise ratio, allowing the regression of the mobile robot position. In
this work, we propose MoRPI-PINN as a physics-informed neural network framework
for accurate inertial-based mobile robot navigation. By embedding physical laws
and constraints into the training process, MoRPI-PINN is capable of providing
an accurate and robust navigation solution. Using real-world experiments, we
show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN
is a lightweight approach that can be implemented even on edge devices and used
in any typical mobile robot application.

</details>


### [147] [Evaluation of facial landmark localization performance in a surgical setting](https://arxiv.org/abs/2507.18248)
*Ines Frajtag,Marko Švaco,Filip Šuligoj*

Main category: cs.RO

TL;DR: 本文探讨了在手术灯照明条件下，使用MediaPipe算法和机器人臂检测面部关键点的准确性及其在医学领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: 变换光照和不同角度下，准确检测面部特征是医学手术中一大挑战，尤其是在神经外科、眼科及整形外科等领域。因此，作者旨在评估并优化现有面部检测算法在特殊医疗环境下的应用能力。

Method: 实验采用机器人手臂自动调整假人头部的位置，保持手术灯和假人相对固定，借此测试MediaPipe算法对面部关键点的检测效果，分析不同姿态（大的偏航与俯仰角）下的检测准确率和稳定性。

Result: 在手术光照条件下，MediaPipe显著提升了大角度（偏航、俯仰）下的面部关键点检测表现，但部分点位检测仍存在标准差增大，表现出一定的不稳定性。

Conclusion: MediaPipe在医学领域中面部关键点检测表现良好，有望集成进临床手术流程，但不同角度和个别特征点的稳定性还需进一步优化。

Abstract: The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.

</details>


### [148] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: 本文提出了一种名为ReSem3D的统一操控框架，实现了带有细粒度语义对齐的3D空间约束，提升了机器人在多样化语义环境中的操作能力。该方法适用于复杂和动态环境下的多任务操作，支持零样本泛化，实验效果优越。


<details>
  <summary>Details</summary>
Motivation: 当前语义驱动的3D空间约束在机器人操作任务中有助于任务理解与执行一体化，但存在语义表达粗糙、缺乏实时规划、以及在多语义环境中鲁棒性较差的问题。为解决这些限制，作者希望实现一种能够提供细粒度视觉锚定、实时反应并且适应不同语义环境的操控框架。

Method: 作者提出ReSem3D框架，结合视觉基础模型（VFM）和多模态大语言模型（MLLM），采用分层递归推理的方法自动从自然语言指令和RGB-D感知中，构建3D空间约束。具体流程包括部件级提取与区域级细化两个阶段。所生成的空间约束被编码为关节空间中的实时优化目标，实现了动态环境下的反应式操作。

Result: 作者在家庭和化学实验室等语义丰富和稀疏环境下进行了大量仿真和真实实验。实验显示ReSem3D在零样本条件下可以执行多样的操控任务，具有良好的适应性和泛化性。

Conclusion: ReSem3D有效提升了机器人在复杂语义环境下的泛化和实时操作能力，能够实现细致和有层次的空间约束表达，适应动态变化和多样任务需求，表现出强大的实用价值和拓展潜力。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>


### [149] [Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding](https://arxiv.org/abs/2507.18276)
*Xiaojie Zhang,Yuanfei Wang,Ruihai Wu,Kunqi Xu,Yu Li,Liuyu Xiang,Hao Dong,Zhaofeng He*

Main category: cs.RO

TL;DR: 本文提出了AdaRPG框架，通过利用大模型提取和理解物体零部件信息，加强了对多样化关节物体的操作泛化能力，并在仿真与真实环境中展现出较强的跨类别泛化性能。


<details>
  <summary>Details</summary>
Motivation: 关节物体因结构多样且内部机制不可直接观察，导致机器人在操作时面临感知理解和动作策略的泛化难题。现有方法在跨类别泛化方面存在局限，主要在于几何多样性和功能机制差异难以用统一策略解决。

Method: AdaRPG框架通过调用大模型，聚焦于零件级别的视觉信息抽取和功能推理，一方面用构建的部件级功能标注数据集训练affordance模型，另一方面利用大模型蕴含的通用知识推理复杂运动机制，并生成调用基础技能函数的高阶控制指令。

Result: 实验证明，无论在仿真还是现实环境，AdaRPG都能在新类别的关节物体上呈现出较强的操作泛化能力，优于传统方法。

Conclusion: AdaRPG通过零件级表示和大模型推理，有效解决了关节物体操作中的视觉泛化和策略泛化难题，为机器人的自适应物体操作提供了新思路与系统性框架。

Abstract: Articulated objects pose diverse manipulation challenges for robots. Since
their internal structures are not directly observable, robots must adaptively
explore and refine actions to generate successful manipulation trajectories.
While existing works have attempted cross-category generalization in adaptive
articulated object manipulation, two major challenges persist: (1) the
geometric diversity of real-world articulated objects complicates visual
perception and understanding, and (2) variations in object functions and
mechanisms hinder the development of a unified adaptive manipulation strategy.
To address these challenges, we propose AdaRPG, a novel framework that
leverages foundation models to extract object parts, which exhibit greater
local geometric similarity than entire objects, thereby enhancing visual
affordance generalization for functional primitive skills. To support this, we
construct a part-level affordance annotation dataset to train the affordance
model. Additionally, AdaRPG utilizes the common knowledge embedded in
foundation models to reason about complex mechanisms and generate high-level
control codes that invoke primitive skill functions based on part affordance
inference. Simulation and real-world experiments demonstrate AdaRPG's strong
generalization ability across novel articulated object categories.

</details>


### [150] [AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments](https://arxiv.org/abs/2507.18317)
*Chenglong Qian,Yang Xu,Xiufang Shi,Jiming Chen,Liang Li*

Main category: cs.RO

TL;DR: 本文提出了一种自适应多传感器融合方法AF-RLIO，集成了4D毫米波雷达、激光雷达、IMU和GPS，实现了在烟雾、隧道等复杂环境下更稳定、精确的定位与导航。


<details>
  <summary>Details</summary>
Motivation: 现有单一传感器（如LiDAR、GPS）在极端和动态环境下易受干扰，导致机器人定位和导航性能下降，亟需更鲁棒的多传感器融合方案。

Method: AF-RLIO包含三大模块：1) 预处理模块利用雷达协助激光雷达去除动态点，并检测激光雷达降级情况；2) 动态感知多模态里程计模块选择合适点云进行配准，并采用迭代误差状态卡尔曼滤波与IMU紧耦合；3) 因子图优化模块在里程计和GPS数据间自适应分配权重，构建位姿图进行优化。

Result: 方法在多种数据集和真实机器人测试中表现优异，特别在烟雾、隧道等极端情境下，定位精度和鲁棒性明显优于现有方法。

Conclusion: AF-RLIO有效解决了复杂、变化环境中单一传感器方案的不足，为自动机器人提供了更为稳定、安全的导航和定位能力。

Abstract: In robotic navigation, maintaining precise pose estimation and navigation in
complex and dynamic environments is crucial. However, environmental challenges
such as smoke, tunnels, and adverse weather can significantly degrade the
performance of single-sensor systems like LiDAR or GPS, compromising the
overall stability and safety of autonomous robots. To address these challenges,
we propose AF-RLIO: an adaptive fusion approach that integrates 4D
millimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to
leverage the complementary strengths of these sensors for robust odometry
estimation in complex environments. Our method consists of three key modules.
Firstly, the pre-processing module utilizes radar data to assist LiDAR in
removing dynamic points and determining when environmental conditions are
degraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects
appropriate point cloud data for scan-to-map matching and tightly couples it
with the IMU using the Iterative Error State Kalman Filter. Lastly, the factor
graph optimization module balances weights between odometry and GPS data,
constructing a pose graph for optimization. The proposed approach has been
evaluated on datasets and tested in real-world robotic environments,
demonstrating its effectiveness and advantages over existing methods in
challenging conditions such as smoke and tunnels.

</details>


### [151] [G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM](https://arxiv.org/abs/2507.18344)
*Gyuhyeon Pak,Hae Min Cho,Euntai Kim*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的几何感知RGB-D高斯投影SLAM系统G2S-ICP SLAM，实现了高保真的实时三维重建与鲁棒的相机位姿跟踪，超越了现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法在多视角深度解释、重建精度和鲁棒性上存在局限，尤其是基于各向同性假设的高斯椭球体在建模局部表面时易引入不一致。作者希望通过改进表示方式和损失函数，提升RGB-D SLAM系统在几何和视觉精度上的表现。

Method: 提出利用受几何约束的本地切平面2D高斯分布（高斯圆盘）来建模场景表面，结合各向异性协方差先验，将其嵌入通用ICP框架中，无需修改基础配准结构。同时设计几何感知损失函数，联合监督光度、深度和法线一致性。

Result: 所提出的G2S-ICP SLAM在Replica和TUM-RGBD数据集上的实验证明，其在定位精度、重建完整性上均优于现有SLAM系统，并且保持出色的渲染质量和实时性。

Conclusion: G2S-ICP SLAM通过几何对齐的高斯表示与改进的误差函数，有效提升了RGB-D SLAM在视觉与几何精度上，表明该方法具备在实际环境中广泛应用的潜力。

Abstract: In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting
SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D
reconstruction and robust camera pose tracking in real-time by representing
each scene element using a Gaussian distribution constrained to the local
tangent plane. This effectively models the local surface as a 2D Gaussian disk
aligned with the underlying geometry, leading to more consistent depth
interpretation across multiple viewpoints compared to conventional 3D
ellipsoid-based representations with isotropic uncertainty. To integrate this
representation into the SLAM pipeline, we embed the surface-aligned Gaussian
disks into a Generalized ICP framework by introducing anisotropic covariance
prior without altering the underlying registration formulation. Furthermore we
propose a geometry-aware loss that supervises photometric, depth, and normal
consistency. Our system achieves real-time operation while preserving both
visual and geometric fidelity. Extensive experiments on the Replica and
TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems
in terms of localization accuracy, reconstruction completeness, while
maintaining the rendering quality.

</details>


### [152] [Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input](https://arxiv.org/abs/2507.18396)
*Yonghao Fu,Cheng Hu,Haokun Xiong,Zhangpeng Bao,Wenyuan Du,Edoardo Ghignone,Michele Magno,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: 该论文提出了一种结合线性模型预测控制与神经网络残差建模的轨迹跟踪新方法RKMPC，在仿真和实车验证中均优于传统方法，并大幅降低训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有简单纯跟踪方法（PP）忽略车辆模型约束，影响安全，主流模型预测控制方法又面临模型精度与计算效率的权衡难题，急需兼顾可靠性和性能优化的新控制策略。

Method: RKMPC采用双线性MPC结构，基础控制由基于运动学模型的LMPC提供，神经网络训练的RKMPC模块负责补偿误差，最终控制量为二者之和，实现机制模型的可解释性与深度学习增强的性能结合。

Result: 仿真和实车测试显示，RKMPC稳定性和精度优于传统LMPC和KMPC——在仅用后者20%训练数据下，横向误差减少11.7%-22.1%，航向误差缩小8.9%-15.8%，前轮转向稳定性提升27.6%。

Conclusion: RKMPC兼具传统模型可靠性与数据驱动优化能力，提升轨迹跟踪表现并降低对数据量需求，在智能驾驶车辆控制领域具有良好的实际应用前景。

Abstract: In vehicle trajectory tracking tasks, the simplest approach is the Pure
Pursuit (PP) Control. However, this single-point preview tracking strategy
fails to consider vehicle model constraints, compromising driving safety. Model
Predictive Control (MPC) as a widely adopted control method, optimizes control
actions by incorporating mechanistic models and physical constraints. While its
control performance critically depends on the accuracy of vehicle modeling.
Traditional vehicle modeling approaches face inherent trade-offs between
capturing nonlinear dynamics and maintaining computational efficiency, often
resulting in reduced control performance. To address these challenges, this
paper proposes Residual Koopman Model Predictive Control (RKMPC) framework.
This method uses two linear MPC architecture to calculate control inputs: a
Linear Model Predictive Control (LMPC) computes the baseline control input
based on the vehicle kinematic model, and a neural network-based RKMPC
calculates the compensation input. The final control command is obtained by
adding these two components. This design preserves the reliability and
interpretability of traditional mechanistic model while achieving performance
optimization through residual modeling. This method has been validated on the
Carsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH
racing car. Experimental results show that RKMPC requires only 20% of the
training data needed by traditional Koopman Model Predictive Control (KMPC)
while delivering superior tracking performance. Compared to traditional LMPC,
RKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by
8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The
implementation code is available at: https://github.com/ZJU-DDRX/Residual
Koopman.

</details>


### [153] [Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning](https://arxiv.org/abs/2507.18436)
*David Blanco-Mulero,Júlia Borràs,Carme Torras*

Main category: cs.RO

TL;DR: 本论文针对医疗领域中机器人辅助穿衣任务，提出了预穿衣步骤——提前展开折叠衣物，并通过模仿学习和视觉分类等方法实现衣物状态识别与有效展开。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人辅助穿衣研究大多假设衣物已展开，但在实际医护场景中，衣物如手术服通常为折叠状态，需先进行展开，这一前置步骤尚缺乏解决方案。

Method: 提出了“预穿衣”步骤，利用模仿学习训练三种衣物展开操作元（涵盖高低加速度运动），结合视觉分类器将衣物状态分为闭合、部分展开和完全展开，并对单一及组合操作策略进行实证评估。

Result: 实验证明，仅用高动态（大幅度）动作难以展开新拆包的衣物，而不同动作的组合能更高效地展开和准备衣物。

Conclusion: 针对衣物折叠状态，结合多种展开操作策略能提高机器人穿衣前准备效率，对实际医疗辅助具有较强实用意义。

Abstract: Robotic-assisted dressing has the potential to significantly aid both
patients as well as healthcare personnel, reducing the workload and improving
the efficiency in clinical settings. While substantial progress has been made
in robotic dressing assistance, prior works typically assume that garments are
already unfolded and ready for use. However, in medical applications gowns and
aprons are often stored in a folded configuration, requiring an additional
unfolding step. In this paper, we introduce the pre-dressing step, the process
of unfolding garments prior to assisted dressing. We leverage imitation
learning for learning three manipulation primitives, including both high and
low acceleration motions. In addition, we employ a visual classifier to
categorise the garment state as closed, partly opened, and fully opened. We
conduct an empirical evaluation of the learned manipulation primitives as well
as their combinations. Our results show that highly dynamic motions are not
effective for unfolding freshly unpacked garments, where the combination of
motions can efficiently enhance the opening configuration.

</details>


### [154] [A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots](https://arxiv.org/abs/2507.18462)
*Alghalya Al-Hajri,Ejmen Al-Ubejdij,Aiman Erbad,Ali Safa*

Main category: cs.RO

TL;DR: 论文提出利用压缩感知测量矩阵结构，优化机器人环境数据采集路径，显著提升采样效率和重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统遥感和环境监测需要大量采样点，造成时间和能源消耗高，且常规压缩感知在路径设计上利用不足。为此，作者希望充分发掘压缩感知测量矩阵结构，提升机器人采集效率。

Method: 提出基于蒙特卡洛优化的框架，联合优化测量矩阵以最小化机器人路径长度和信号重建误差；利用字典学习得到数据驱动的稀疏变换，提高重建精度，并减少必需采样数。

Result: 实验在Gulf地区重建NO2污染地图，结果显示机器人路径长度缩减至完整覆盖路径的10%以内，重建精度比基于DCT和多项式字典的传统压缩感知方法提升5倍，比既有信息路径规划方法提升2倍。

Conclusion: 新方法通过结合压缩感知结构与路径优化，实现更高效、更准确的机器人环境感知采样方式，对移动感知与环境监测具有应用和研究价值。

Abstract: In recent years, Compressed Sensing (CS) has gained significant interest as a
technique for acquiring high-resolution sensory data using fewer measurements
than traditional Nyquist sampling requires. At the same time, autonomous
robotic platforms such as drones and rovers have become increasingly popular
tools for remote sensing and environmental monitoring tasks, including
measurements of temperature, humidity, and air quality. Within this context,
this paper presents, to the best of our knowledge, the first investigation into
how the structure of CS measurement matrices can be exploited to design
optimized sampling trajectories for robotic environmental data collection. We
propose a novel Monte Carlo optimization framework that generates measurement
matrices designed to minimize both the robot's traversal path length and the
signal reconstruction error within the CS framework. Central to our approach is
the application of Dictionary Learning (DL) to obtain a data-driven sparsifying
transform, which enhances reconstruction accuracy while further reducing the
number of samples that the robot needs to collect. We demonstrate the
effectiveness of our method through experiments reconstructing $NO_2$ pollution
maps over the Gulf region. The results indicate that our approach can reduce
robot travel distance to less than $10\%$ of a full-coverage path, while
improving reconstruction accuracy by over a factor of five compared to
traditional CS methods based on DCT and polynomial dictionaries, as well as by
a factor of two compared to previously-proposed Informative Path Planning (IPP)
methods.

</details>


### [155] [Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces](https://arxiv.org/abs/2507.18502)
*Sait Sovukluk,Grazia Zambella,Tobias Egle,Christian Ott*

Main category: cs.RO

TL;DR: 本文对类人机器人进行了两种整体控制方法（逆动力学整体控制ID-WBC和基于耗散性的整体控制PB-WBC）的实验对比，重点分析各自在不同实际情况下的表现和优劣。


<details>
  <summary>Details</summary>
Motivation: 两种常见的整体控制方法（ID-WBC和PB-WBC）虽理论上都能保证闭环动力学下的稳定性，但在实际存在关节摩擦、传感器噪声、外部扰动等情况下谁更具鲁棒性还不明确，因此需要通过实验进行评估和比较。

Method: 设计了多个机器人实验场景（如摆动足位置与姿态控制、带或不带额外重量的深蹲、跳跃），并对ID-WBC和PB-WBC两种控制方法进行详细的实验对比。同时分析控制器自身的建模特征对实验表现的影响。

Result: 实验揭示了两种控制方法在不同运动任务和受扰条件下的实际表现及其优缺点，并将实验结果与两类控制器的理论设计特点进行了关联分析。

Conclusion: ID-WBC和PB-WBC各有优势和局限：ID-WBC在某些精度任务上表现更好，而PB-WBC在受扰鲁棒性上有优势。本文为机器人控制系统选型和实际部署提供了实验依据和参考。

Abstract: This paper studies the experimental comparison of two different whole-body
control formulations for humanoid robots: inverse dynamics whole-body control
(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers
fundamentally differ from each other as the first is formulated in task
acceleration space and the latter is in task force space with passivity
considerations. Even though both control methods predict stability under ideal
conditions in closed-loop dynamics, their robustness against joint friction,
sensor noise, unmodeled external disturbances, and non-perfect contact
conditions is not evident. Therefore, we analyze and experimentally compare the
two controllers on a humanoid robot platform through swing foot position and
orientation control, squatting with and without unmodeled additional weights,
and jumping. We also relate the observed performance and characteristic
differences with the controller formulations and highlight each controller's
advantages and disadvantages.

</details>
