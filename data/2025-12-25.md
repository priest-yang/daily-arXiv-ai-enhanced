<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: 本文提出了VL4Gaze，这是一个专注于视线理解的大规模数据集，首次系统性评估与提升视觉-语言模型（VLM）对人类视线的理解能力，并验证了特定任务监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在众多任务上表现优秀，但缺乏对人类视线理解的系统性研究和基准。本文旨在填补VLM在视线理解（如注意力与意图推断）上的评估与训练空白。

Method: 作者构建了VL4Gaze基准，包括489K自动生成的问答对，涵盖124K图像，并围绕四类任务（视线目标描述、方向描述、视点定位、歧义问题识别）将视线理解作为VQA问题。对商用及开源VLM在不同学习设置下进行全面评测。

Result: 实验证明，现有大模型在缺乏专门监督时，难以准确推理视线语义及空间定位，而在VL4Gaze上训练后，所有任务表现均大幅提升。

Conclusion: 针对视线理解的多任务监督对于提升VLM相关能力至关重要。VL4Gaze的发布将推动AI在社会视觉理解方向的进一步研究。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种面向边缘设备和物联网环境、符合TinyML限制的垃圾检测神经网络搜索框架TrashDets，能在极低资源消耗下达到较好检测精度。


<details>
  <summary>Details</summary>
Motivation: 垃圾检测在现实环境中的应用广泛，为满足在资源受限（如微控制器、边缘设备）下的应用需求，需要设计高效、低资源消耗、可变规模的检测器。现有方法在精度和资源利用率上存在权衡不足。

Method: 作者基于Once-for-All架构，提出ResDets超级网络并采用迭代式进化搜索方法，交替优化骨干网络和颈部/头部。引入种群传递、精度预测器以减少搜索消耗和提升结果稳定性，并生成适用于不同资源预算的检测器家族（TrashDets）。

Result: 在TACO五类垃圾子集上，最大模型TrashDet-l以30.5M参数实现了19.5 mAP50，精度比之前方法提升高达3.6 mAP50且参数更少。整个TrashDet家族参数量覆盖1.2M至30.5M，对应mAP50则为11.4~19.5。在MAX78002微控制器上，TrashDet-ResNet和TrashDet-MBNet两种变体显著超越现有ai87-fpndetector基线，在能耗、延迟、功耗和精度上都有大幅优化。

Conclusion: TrashDets框架能在不同硬件资源下，灵活生成高效能、符合部署需求的垃圾检测网络，显著优于现有TinyML方法，在精度和资源消耗之间达成更优平衡。

Abstract: This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 本文提出了OccuFly，这是第一个基于摄像头的真实无人机空中场景全局语义补全（SSC）基准数据集，旨在克服现有方法在空中场景和LiDAR缺陷带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大多数SSC研究集中在地面场景，且依赖LiDAR传感器，然而在无人机等空中场景下，LiDAR因多种现实因素（法规、重量、点云稀疏等）难以使用，限制了相关应用的发展。

Method: 作者构建了OccuFly数据集，通过摄像头收集多高空、跨季节、多场景（城市、工业、农村）的数据。提出了一套基于摄像头、无需LiDAR的数据生成框架，结合3D重建和2D标注蒙版的自动提升，实现高效的3D语义标注。

Result: 构建了具备22类语义标签的大规模、真实、空中SSC数据集OccuFly，并在上面对多种先进SSC算法进行了基准测试，揭示了高空视角带来的新挑战。

Conclusion: OccuFly为空中3D场景理解提供了首个全面的视觉基准和数据集，推动了无LiDAR条件下的空中语义场景补全研究，对无人机感知和相关下游任务具有重要意义。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 本文提出了NullBUS框架，可以同时利用带有或不带有文本/空间提示的乳腺超声（BUS）图像进行分割，解决了公共数据集缺少可靠元数据的问题，并取得了领先的分割性能。


<details>
  <summary>Details</summary>
Motivation: 目前很多乳腺超声分割方法依赖于文本或空间提示辅助训练，但实际很多公开数据集缺乏完整的提示信息或文本报告，限制了模型的训练数据量和泛化能力。为此，作者希望提出一种既能利用有提示信息的数据，也能利用缺失提示的数据的模型框架，提高分割的适应性和准确性。

Method: 作者设计了NullBUS混合监督多模态分割框架，引入了nullable prompts机制，即通过可学习的null embedding和presence mask标注提示缺失的情况。当文本/空间提示缺失时，模型自动退化为单纯基于图像的判别；有提示时则联合利用多模态信息。

Result: 在三个主流公开BUS数据集上，NullBUS的平均IoU达到0.8568，平均Dice系数为0.9103，展现了在不同提示可用性下均优于现有方法的分割表现。

Conclusion: NullBUS实现在同一框架下统一利用有提示和无提示的数据，显著提升了乳腺超声分割的鲁棒性和性能，适合实际临床中数据情况复杂的环境。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [5] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

TL;DR: 本文提出了一种面向任务的联合优化框架，实现从光学、传感器建模到轻量级语义分割网络的一体化 RAW-to-task 流水线，相较传统技术明显提升了分割性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶视觉管线将相机设计和后续感知任务分离，使用针对人眼优化的光学与 ISP（图像信号处理），导致原始数据中对下游语义有用信息在去马赛克、去噪、量化等环节被丢弃，且模型需适应各类传感器噪声与伪影，影响感知性能。

Method: 该文提出端到端的共设计方法，将手机规模的可学习光学传感模型（包括镜头建模、可学习色滤阵列、泊松-高斯噪声、量化等）与轻量级分割网络联合优化，直接针对下游分割目标进行训练。系统依托 DeepLens，为 KITTI-360 数据集端到端优化分割表现。

Result: 在 KITTI-360 实验中，所提方法在主流语义分割性能指标 mIoU 上整体优于传统分离式流水线，光学建模和 CFA 学习对薄弱目标和低照度类别表现尤为明显。模型仅 1M 参数且运行速度达 28 FPS，适合边缘部署。此外，联合优化传感系统能根据语义自适应采集，显著提升边界锐度与光照/噪声鲁棒性。

Conclusion: 通过端到端联合优化光学、传感和网络结构，显著提升了自动驾驶感知系统的效率、准确性与部署能力，指明感知系统可靠性提升的新方向。

Abstract: Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: CHAMMI-75是一个包含75个不同生物学研究中多通道显微镜图像的开源数据集，用于开发通道自适应的细胞形态分析模型。


<details>
  <summary>Details</summary>
Motivation: 当前的细胞形态分析模型通常只针对单一类型的显微镜图像训练，导致这些模型难以在不同实验或成像通道条件下复用，影响了其在实际生物学问题中的广泛应用。

Method: 作者从公开数据中整理并发布了CHAMMI-75数据集，涵盖多种成像模态和通道设置。通过在该多样性极高的数据集上训练模型，研究通道自适应模型对多通道显微图像的处理能力。

Result: 实验表明，在CHAMMI-75数据集上训练的模型在多通道生物成像任务中的表现优于传统单一数据源训练的模型，尤其受益于数据集在成像模态上的丰富多样性。

Conclusion: CHAMMI-75推动了更通用、适用于多种实验条件的细胞形态分析模型的开发，对未来的生物学研究具有重要意义。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 提出了一种可自适应视觉预处理方法，大幅提升了视觉-语言模型（VLMs）在推理阶段的处理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在多模态推理任务上表现出色，但由于高分辨率视觉输入导致推理延迟和计算成本较高，限制了实际部署。现有方法在预处理阶段采用静态策略，处理简单图像时存在冗余计算。

Method: 设计了一种基于图像内容动态调整输入分辨率与空间覆盖范围的视觉预处理方法。该方法包含内容感知的图像分析、自适应分辨率选择、内容感知裁剪，并与FastVLM无缝集成，无需修改模型架构或重新训练。

Result: 在DocVQA子集上进行测试，发现新方法能够在保持模型性能的情况下，平均每张图片推理时间缩减超50%、整体生成时间降低、视觉token数量降低超55%。

Conclusion: 基于输入内容的自适应预处理是一种高效、轻量级的方法，可显著提升视觉-语言模型的推理效率，适合实际部署。相关代码开源以便复现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: 该论文提出了ALIVE系统，使传统被动的讲座视频转变为支持实时交互和个性化讲解的动态学习体验，全部在本地设备上运行，保障隐私且反应迅速。


<details>
  <summary>Details</summary>
Motivation: 传统讲座视频虽然灵活，但缺乏及时答疑机制，导致学生遇到困惑时往往需要额外查找资料。随着大语言模型和神经虚拟人技术的发展，有机会提升交互式学习体验，但现有方案往往不具备课程内容感知能力、依赖云服务，或未能统一检索与虚拟人讲解，存在隐私、延迟及集成度低等问题。

Method: 作者提出ALIVE系统，核心包括：1）将讲座内容通过ASR转录、LLM优化，结合神经虚拟人合成生成视频讲解；2）基于语义相似性和时间戳对齐实现内容感知的讲座片段检索；3）实现实时多模态交互，允许学生暂停讲座，通过文本或语音提问，获文本或虚拟人讲解的答复。技术层面采用轻量级嵌入模型、FAISS检索和分段虚拟人逐步加载优化延迟和响应。

Result: 系统在完整医学影像课程上测试，结果显示ALIVE检索准确、延迟低、交互体验佳，能够实时提供有针对性的答疑支持。

Conclusion: ALIVE展示了多模态AI结合本地部署与内容感知检索，可以极大提升录播课程的教学效果，为下一代交互式学习环境提供了可扩展的方案。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [9] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 该论文提出了基于3D GPR的地下管线智能检测新框架，通过多视图特征联合分析和深度学习优化，显著提升了检测准确率、鲁棒性及对小尺度目标的识别。


<details>
  <summary>Details</summary>
Motivation: 当前3D地质雷达在地下管线检测中存在多视图特征相关性弱、小尺度目标识别精度低以及复杂场景下鲁棒性不足等问题，亟需高效的智能检测方法提升识别与定位能力。

Method: 1. 构建B/C/D-Scan三视图联合分析，结合FDTD模拟与实测数据交叉验证特征方法。2. 提出DCO-YOLO（集成DySample、CGLU、OutlookAttention到YOLOv11 ）提升小管线边缘特征提取。3. 提出3D-DIoU空间特征匹配算法，辅助多视图自动注释。4. 采用三视图融合提升整体检测效果。

Result: 在真实城市地下管线场景数据上，方法在多管线复杂场景下实现accuracy 96.2%、recall 93.3%、mAP 96.7%，较基线提升2.0%、2.1%、0.9%。消融实验与可视化均验证了特征增强模块的优化效果。

Conclusion: 融合深度学习与3D GPR物理特性，所提出技术框架高效、可靠，能够为地下管线智能识别与定位提供创新的解决方案。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [10] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: 本文提出了NeRV360，一种专为高分辨率360度视频设计的神经隐式表示压缩方法，实现了仅对用户选择视角进行解码，大幅降低内存和提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 传统NeRV方法在高分辨率360视频处理时内存消耗大、解码慢，难以满足实时应用需求。

Method: NeRV360将视口选择与解码过程集成，只解码用户所需的画面区域，并提出时空仿射变换模块以支持按位置和时间条件解码。

Result: 在6K视频实验中，NeRV360内存消耗降低7倍，解码速度提升2.5倍，图像质量优于代表性工作HNeRV。

Conclusion: NeRV360有效解决了高分辨率360视频实时解码的痛点，在保持甚至提升图像质量的同时大幅节省资源，适合实际应用。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [11] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

TL;DR: 该论文针对跨模态船舶重识别问题，提出了一种无需大规模配对数据，通过Vision Foundation Model进行特征注入的新型高效微调方法，取得了SOTA性能且参数量极低。


<details>
  <summary>Details</summary>
Motivation: 跨模态船舶重识别（CMS Re-ID）在全天候海上目标跟踪中至关重要，但受制于不同模态之间巨大的分布差异。主流方法依赖模态对齐及大规模配对数据集，构建成本高且不易扩展。作者希望借助通用视觉模型，避免对大数据集的依赖，提升实际应用价值。

Method: 基于Platonic Representation Hypothesis，作者提出了一种新的高效参数微调（PEFT）策略——域表示注入（DRI）。具体做法是保持视觉基础模型（VFM）完全冻结，通过可学习的偏移编码器提取包含模态和身份信息的域特定表示，并通过调制器在中间特征层自适应转换后，采用加性融合注入到模型中，无需更改预训练权重，仅用极少参数实现跨模态特征适配。

Result: 作者在多个数据集上进行了大量实验，特别是在HOSS-ReID数据集上，分别仅用1.54M和7.05M的可训练参数，实现了57.9%和60.5%的mAP，均达到了目前最优性能。

Conclusion: 所提DRI方法有效解决了跨模态船舶重识别中的模态鸿沟问题，无需大规模配对数据和模型权重调整，仅通过极少参数即可取得SOTA性能，具有极高的实用性和推广价值。

Abstract: Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [12] [DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction](https://arxiv.org/abs/2512.20898)
*Xiao Yu,Zhaojie Fang,Guanyu Zhou,Yin Shen,Huoling Luo,Ye Li,Ahmed Elazab,Xiang Wan,Ruiquan Ge,Changmiao Wang*

Main category: cs.CV

TL;DR: 本文针对肺结节的早期检测与诊断任务，提出了一种双图时空注意力网络（DGSAN），有效融合多模态和多时序信息，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态信息融合方法（如简单拼接、基本注意力机制）效率低，未能充分挖掘多源数据的时空特征，限制了肺结节自动检测的准确性。

Method: 提出Dual-Graph Spatiotemporal Attention Network，包括Global-Local特征编码器、双图构建（分为模态内/之间的图结构）、分层跨模态图融合模块，还新收集并整理了NLST-cmst多模态数据集。

Result: 在NLST-cmst和CSTL衍生数据集上，DGSAN显著提升了肺结节分类准确率，同时具备优异的计算效率，性能优于当前同类方法。

Conclusion: 该方法为肺结节早期检测提供了更高效的多模态融合手段，推动了相关研究的发展，并通过新数据集为领域提供了研究支撑。

Abstract: Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.

</details>


### [13] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了首个用于评估视觉-语言模型（VLM）对压缩图像处理能力的综合性基准测试，并提出通用适配器以提升其在压缩图像上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着VLM的发展及其实际应用场景对传输和存储效率的要求提升，高效图像压缩变得重要。然而，现有VLM主要针对高比特率图像，缺乏对低比特率压缩图像理解能力的评估和提升方法。

Method: 1) 构建涵盖百万级压缩图像、多种主流编解码器、丰富任务类型的评测基准。
2) 系统分析VLM性能下降来源，区分为信息损失和泛化能力不足。
3) 针对泛化不足，提出通用适配器模块，无需针对不同编解码器分别训练。

Result: 构建了大规模基准测试平台，实验证明通用VLM适配器能够在不同编解码器与比特率下，使VLM性能提升10%-30%。

Conclusion: 本文基准和通用适配器为VLM与压缩图像之间的性能鸿沟提供了系统性测评和有效改进思路，对相关领域具有现实和研究价值。

Abstract: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [14] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

TL;DR: 本文提出了一种新的三维视觉定位（3DVG）方法PanoGrounder，将全景多模态表示与预训练2D视觉-语言模型结合，实现了强大的视觉语言推理和优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的3D视觉定位方法依赖有限的有标注3D数据集，泛化性较差，且推理能力有限，不如现有的强大视觉-语言模型。如何充分利用2D视觉-语言模型提升3D视觉定位性能成为关键挑战。

Method: PanoGrounder方法首先将3D场景渲染成包含3D语义和几何信息的全景图像，作为2D-3D的中间表示。然后在每个全景视角上通过预训练2D视觉-语言模型进行文本指引下的目标定位，最后将多个视角下的定位结果融合生成最终3D包围盒。该流程包括三阶段：视角选取、全景目标定位、三维融合。

Result: 该方法在ScanRefer和Nr3D数据集上取得了最新的最优结果，并且在未见过的3D数据集和不同表述方式的文本上表现出优越的泛化能力。

Conclusion: 将全景多模态表示与强大的2D视觉-语言模型结合，可以显著提升3D视觉定位的性能和泛化性，为视觉-语言机器人导航与理解任务提供了新的解决思路。

Abstract: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [15] [Self-supervised Multiplex Consensus Mamba for General Image Fusion](https://arxiv.org/abs/2512.20921)
*Yingying Wang,Rongjin Zhuang,Hui Zheng,Xuanhua He,Ke Cao,Xiaotong Tu,Xinghao Ding*

Main category: cs.CV

TL;DR: 提出了一种通用型图像融合框架SMC-Mamba，无需增加复杂度即可提升多种下游视觉任务表现，实验证明其优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法多为特定任务设计，难以兼顾多类应用且模型复杂度较高，亟需通用、高效的图像融合方案。

Method: SMC-Mamba包含两个核心模块：1）模态无关特征增强（MAFE）用于保留细节并提升全局特征表示，2）多路共识跨模态Mamba（MCCM）实现多专家之间的动态协作与特征整合，并通过跨模态扫描提升模态间信息交互。此外，引入双层自监督对比损失（BSCL），提升高频信息保真及下游性能，无需增加计算量。

Result: 在红外-可见、医学、多焦、多曝光等多种融合及相关下游视觉任务上，SMC-Mamba在定量和定性评价均优于现有主流融合方法。

Conclusion: SMC-Mamba能够有效提升多模态图像融合质量及相关任务表现，是一种高效且通用的融合解决方案。

Abstract: Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.

</details>


### [16] [Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting](https://arxiv.org/abs/2512.20927)
*Yoonwoo Jeong,Cheng Sun,Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: 本文提出了Q-Render算法，提升了3D高维特征的渲染效率和保真度，并结合GS-Net实现了更快更优的3D开放词汇分割。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇分割方法在处理高维特征时，采用编码本或特征压缩，造成信息损失，影响分割质量，因此需要一种高效且高保真的特征渲染方法。

Method: 提出Quantile Rendering (Q-Render)，只稀疏采样对光线影响较大的3D高斯分布，通过减少不必要的计算提升渲染效率。并提出GS-Net，可自适应地预测高斯特征，实现方法的泛化。

Result: 在ScanNet和LeRF等数据集上，提出的方法在分割质量和渲染速度上均优于当前领先方法，在512维特征下可实现约43.7倍的实时渲染加速。

Conclusion: Q-Render与GS-Net结合不仅提升了分割精度，还极大提高了推理速度，为3D开放词汇分割任务提供了高效且可扩展的解决方案。

Abstract: Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

</details>


### [17] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的视觉编程框架TVP，能通过自身经验自动进化工具库，从而显著提升三维空间推理问题的解决能力，并在多个基准测试上取得了最优成绩。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在三维场景空间推理中面临几何计算难题，尽管视觉编程可将问题拆解为工具调用，但现有方法要么依赖固定工具集，要么在解题前盲目推断工具，导致生成的程序和工具利用率不佳。

Method: 提出了Transductive Visual Programming (TVP) 框架：首先利用基础工具解决问题并积累解题实例，然后从中抽象出可复用的高级工具，不断扩展工具库，实现自我进化和迁移。

Result: TVP在Omni3D-Bench数据集上优于GPT-4o 22%，比前一最佳视觉编程系统提升11%；TVP自动生成的工具使用频率为传统诱导方法的5倍，在SpatialScore-Hard基准任务上无需任何额外适配依然表现优异。

Conclusion: 经验驱动的工具自进化是高效解决空间推理任务的有效范式，可构建自适应的视觉编程智能体。相关代码已开源。

Abstract: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [18] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的协同多智能体推理框架用于无视部分补全任务，显著提升了结构完整性和语义一致性，并通过新评测指标MAC-Score进行评价，实验在多个数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无视部分补全任务在推断物体不可见部分时，常遇到语义一致性和结构完整性难以兼顾的问题，现有逐步推理方法存在推理不稳定和误差累积的缺陷。本文旨在解决这些难题，提升补全结果的质量。

Method: 方法上，作者将语义规划和视觉合成明确解耦，采用协同多智能体框架：先由专门的智能体完成语义推理和结构规划，再进行像素级生成。同时，设置自我校正的验证智能体，通过“思维链”机制在语义规划阶段检测并修正分割错误和遮挡物遗漏，另有多样性假设生成器对不可见区生成多种合理语义解释。此外，作者提出新的MAC-Score评价指标，更贴合人类判断地衡量结构与语义补全效果。

Result: 实验显示，本方法在多个主流数据集上，结构完整性和语义一致性均显著优于SOTA方法，MAC-Score与人工评价和真实标签的相关性也更高。

Conclusion: 本文方法通过将语义推理和视觉合成解耦，并引入多智能体推理，大幅提升无视部分补全的质量，并提出更有效的评价体系，为该任务后续研究提供了新方向。

Abstract: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [19] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 提出了一种新方法REM，主要通过建模真实图片分布、而非生成器伪影，实现了在多重退化、跨平台等真实世界场景下的稳健生成内容检测，并且建立了新基准RealChain，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有生成内容检测方法依赖于特定生成器的伪影，但这些伪影在图像多次降质、跨平台传播后会消失，导致检测效果大幅下降。随着生成模型不断进化，需要一种鲁棒性更强的检测方法。

Method: 提出Real-centric Envelope Modeling（REM）范式：通过自重建中加入特征级扰动生成近真实样本，并提出包络估计器以跨领域一致性学习包围真实图像流形的边界；此外，构建了覆盖多种生成器和真实降质模拟的新基准RealChain进行全面评测。

Result: 在八个基准测试中，REM相比最新方法平均提升7.5%；在高度退化场景（RealChain）也保持了极佳泛化表现。

Conclusion: REM为真实世界下的合成内容检测奠定了坚实基础，同时公开了实现代码和RealChain数据集，有望推进相关领域发展。

Abstract: The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>


### [20] [SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking](https://arxiv.org/abs/2512.20975)
*Yujin Noh,Inho Jake Park,Chigon Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种基于地图引导的LLM智能体（SPOT），能够在多摄像头盲区环境中实现车辆的连续跟踪，无需预训练，能弥补传统CCTV车辆跟踪在盲区连接轨迹上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CCTV的车辆跟踪系统由于摄像头之间存在间隙且视野有限，难以实现多个摄像头下同一车辆轨迹的无缝连接，特别在盲区容易导致目标ID切换和轨迹丢失，影响了实时路径预测的可靠性。

Method: 提出将道路结构（关键点）和摄像头布置信息用2D坐标文档化，再通过分块技术和实时问答进行推理。同时结合摄像头FOV、相对位置等将车辆坐标转化为实际世界坐标，根据车辆的行驶方向、速度和驾驶模式，在路口层面进行束搜索，预测车辆在盲区后最有可能出现的下一个摄像头位置。

Result: 在虚拟城市环境（CARLA模拟器）实验中，SPOT方法在盲区也能准确预测车辆下一个出现摄像头位置，比现有方法更有效地保持了车辆轨迹的连续性。

Conclusion: SPOT可无需预训练，在多CCTV环境中有效解决盲区车辆轨迹连接问题，提升实时路径预测的可靠性，优于传统方法。

Abstract: CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.

</details>


### [21] [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976)
*Zeqing Song,Zhongmiao Yan,Junyuan Deng,Songpengcheng Xia,Xiang Mu,Jingyi Xu,Qi Wu,Ling Pei*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的神经LiDAR增量建图方法XGrid-Mapping，结合稀疏与稠密网格优势，大幅提升了大规模环境下的建图质量和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有神经LiDAR建图方法要么过于依赖稠密隐式表示、未充分利用几何结构，要么基于体素的方案难以实现实时性能，制约了大规模自主系统的环境表达和决策能力。因此，亟需兼顾效率与精度的新型建图框架。

Method: 提出XGrid-Mapping混合网格框架，将稀疏网格（提供几何先验与结构指导）和隐式稠密网格（丰富场景表达）结合，并采用VDB结构和子地图分组织方式降低计算量。同时，设计基于蒸馏的重叠区域对齐策略确保子地图间连续性，引入动态剔除模块提升鲁棒性和采样效率。

Result: 大量实验表明，XGrid-Mapping在建图质量与效率上均优于现有的体素引导和主流神经建图方法。

Conclusion: 该方法有效解决了神经LiDAR建图在大规模、增量式场景中的效率与精度难题，为自主系统环境感知和实时决策提供了坚实基础。

Abstract: Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

</details>


### [22] [X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data](https://arxiv.org/abs/2512.20980)
*Xinquan Yang,Jinheng Xie,Yawen Huang,Yuexiang Li,Huimin Huang,Hao Zheng,Xian Wu,Yefeng Zheng,Linlin Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种结合扩散模型与知识引导和递增学习的新颖数据生成管道，有效解决了肺部影像学长尾异常的样本不足难题，并在公开数据集上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 肺部X光图像中长尾类别的异常（即罕见的病变）样本稀少，传统的扩散生成方法难以充分学习这些病变，导致模型对罕见病变的诊断精度低。本文旨在通过创新的数据增强技术，提升长尾类别病变的检测能力。

Method: 1）利用大量正常X光影像训练扩散模型，生成高质量的正常图像；2）在病变数据中对常见病变进行修复（inpaint），仅保留长尾病变，扩充稀有样本；3）结合大语言模型知识引导（LKG）和递进式增量学习（PIL），稳定细化生成过程。

Result: 在MIMIC和CheXpert两个公开肺部数据集上，所提方法的数据增强策略显著提升了长尾类别病变的检出率，整体诊断精度超越现有最新方法，设立了新基准。

Conclusion: 所提出的管道不仅有效缓解了稀有病变样本少的问题，还推动了自动胸部X光诊断系统对罕见异常的识别能力，对AI辅助医学影像分析具有重要应用前景。

Abstract: Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.

</details>


### [23] [PUFM++: Point Cloud Upsampling via Enhanced Flow Matching](https://arxiv.org/abs/2512.20988)
*Zhi-Song Liu,Chenhang He,Roland Maier,Andreas Rupp*

Main category: cs.CV

TL;DR: PUFM++是一种用于点云上采的新型生成模型，通过增强流匹配流程、提出自适应采样调度、引入流形约束与递归接口网络，大幅提升了点云重建的精度与稳定性，已达到当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 现有点云上采方法在几何精度、对噪声与不完整输入鲁棒性以及与下游任务一致性等方面存在不足，亟需更强的生成技术提升稀疏、噪声点云的重建质量。

Method: PUFM++提出了两阶段流匹配机制，首先从稀疏输入到稠密目标学习直接的流，再通过噪声扰动样本进一步优化分布。创新性地设计了数据驱动的自适应采样时间调度器提升采样效率，并在采样时加流形约束保证生成点与目标表面对齐，同时引入递归接口网络加强特征交互。

Result: 在合成数据集与真实点云扫描实验中，PUFM++在视觉效果和定量精度均超过现有方法，达到了点云上采领域最新最优水平。

Conclusion: PUFM++极大改善了点云上采的几何准确性、输入鲁棒性与与下游表面任务的一致性，是点云重建任务的有力新工具，相关代码和模型已开源。

Abstract: Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.

</details>


### [24] [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003)
*Xiangzuo Wu,Chengwei Ren,Jun Zhou,Xiu Li,Yuan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的多视角逆向渲染框架，可以从多张RGB图像中一致地预测几何、材质和光照等属性，解决了现有方法效率低和一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单视角逆向渲染方法未能利用多视角间的联系，导致结果不一致；多视角优化方法虽然考虑了一致性，但计算复杂、不易扩展。论文旨在解决多视角逆向渲染中效率和一致性难以兼得的问题。

Method: 作者设计了一种端到端前馈神经网络框架，输入多帧RGB图像，通过交替注意力机制处理视角内外的信息，直接预测每像素的物理属性（如反照率、金属度、粗糙度、漫反射阴影和法线）。同时提出了基于一致性的微调策略，利用无标签的真实视频提升模型在真实场景下的一致性和鲁棒性。

Result: 实验表明，该方法在多个基准数据集上，材料、法线估计和多视角一致性方面都优于现有方法，并展现出良好的泛化能力，在真实场景中表现出色。

Conclusion: 该方法兼顾了多视角渲染中的一致性、质量和计算效率，具备优异的实际应用潜力，对逆向渲染领域的发展具有推动作用。

Abstract: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

</details>


### [25] [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004)
*Jinghan Li,Yang Jin,Hao Jiang,Yadong Mu,Yang Song,Kun Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的自回归视觉生成预训练框架NExT-Vid，通过掩码下的下一帧预测方法，有效提升了视觉表示学习在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉生成预训练方法多基于BERT风格的掩码建模，并未充分利用视频分析中关键的时序信息，而现有少数自回归视觉预训练方法也存在语义定位不准确和生成质量较低的问题。为提升视觉生成模型的表示能力和泛化性，亟需新方法。

Method: 作者提出NExT-Vid框架，其核心是掩码下的下一帧预测，将图片与视频联合建模。技术上，采用上下文隔离的自回归预测器以解耦语义表示与目标解码，并引入条件流匹配解码器以提升生成质量和多样性。整个框架通过上下文隔离的流匹配预训练获得更强的表示能力。

Result: 大量大规模预训练实验结果表明，NExT-Vid无论在生成质量还是语义表示上均优于以往生成式视觉预训练方法，尤其是在下游分类任务中表现突出。

Conclusion: NExT-Vid为视觉生成预训练提供了一种能有效利用时序信息、提升分类性能的创新框架，有望推动视觉表示学习的进一步发展。

Abstract: Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

</details>


### [26] [Granular-ball Guided Masking: Structure-aware Data Augmentation](https://arxiv.org/abs/2512.21011)
*Shuyin Xia,Fan Chen,Dawei Dai,Meng Yang,Junwei Han,Xinbo Gao,Guoyin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结构感知的数据增强方法Granular-ball Guided Masking (GBGM)，通过层次化、有结构地保留重要语义信息，同时抑制冗余区域，显著提升深度学习模型的鲁棒性和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型依赖大量标注数据，且容易在数据有限或分布变化时过拟合。现有基于掩码的信息丢弃增强方法通常缺乏结构感知，可能会丢弃关键信息。因此，需要一种结构化的数据增强策略来提升模型泛化与鲁棒性。

Method: 提出了Granular-ball Guided Masking (GBGM)，利用Granular-ball Computing (GBC)指导，通过粗到细的分层掩码过程，自适应地保留语义丰富、结构重要的区域，并抑制冗余部分，实现结构感知的数据增强。该方法简单、模型无关，易于集成到CNN和Vision Transformer等模型中。

Result: 在多个基准数据集上广泛实验，GBGM持续提升了分类精度和掩码图像重建表现，优于现有方法，验证了该结构感知增强方法的有效性和广泛适用性。

Conclusion: GBGM为结构感知型数据增强提供了新的范式，能提升深度学习模型在不同场景下的性能，且具有简洁、无模型依赖和高适配性的优点。

Abstract: Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.

</details>


### [27] [FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing](https://arxiv.org/abs/2512.21015)
*Mingshu Cai,Yixuan Li,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出了FluencyVE，一种简单高效的视频编辑方法，通过集成线性时间序列模块Mamba，替换传统的时序注意力机制，实现全局帧级关注并降低计算成本，在多个视频属性编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像扩散模型在图像生成领域取得巨大成功，但将其扩展到视频编辑时仍面临时序不一致与计算开销大的难题，因此亟需一种能够兼顾生成质量与效率的新方法。

Method: 作者提出FluencyVE，将高效的线性时间序列模块Mamba融入基于Stable Diffusion预训练模型的视频编辑框架中，取代了传统的时序注意力层。此外，通过低秩近似方法替换Attention中的query和key权重矩阵，并引入加权平均方法优化训练过程中的注意力分数。

Result: FluencyVE在多个真实世界视频编辑任务上表现优异，能高效编辑视频的属性、主体和场景位置，显著降低了计算成本，同时较好保持了原始文本到图像模型的生成能力。

Conclusion: FluencyVE方法为视频编辑领域提供了一种高效且效果优秀的新范式，解决了针对时序一致性与效率的两大难题，为实际应用带来了更可行的解决方案。

Abstract: Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.

</details>


### [28] [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019)
*Rui-qing Sun,Xingshan Yao,Tian Lan,Hui-Yang Zhao,Jia-Ling Shi,Chen-Hao Cui,Zhijing Wu,Chen Yang,Xian-Ling Mao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D领域视频防御框架，用于防止3D重建口播换脸技术对个人肖像视频的滥用，在保证视频高保真的情况下有效扰乱3D信息，以抵御深度伪造攻击。


<details>
  <summary>Details</summary>
Motivation: 随着3D领域基准口播人脸生成技术的进步，可以实时合成高度还原的个人口播视频，这引发了对个人肖像恶意滥用的隐私担忧。目前缺乏有效且高效的视频级防御框架来防止这些技术的滥用，现有的2D图像防御不仅计算代价高，还容易影响视频质量，并且无法有效扰乱3D信息。

Method: 作者提出了一种新颖、高效的视频防御框架，通过在视频的3D信息获取过程中添加扰动来保护肖像视频，同时保持视频高保真。主要技术包括：（1）一种基于相似性的参数共享机制，提高计算效率；（2）多尺度双域注意力模块，同时优化空间-频率域的扰动。

Result: 大量实验证明，该框架不仅防御能力强，对比最快基线方案加速47倍，并且保持高保真度；对缩放操作和最新的伪造净化攻击也表现出很强鲁棒性。设计选择通过消融实验进一步验证。

Conclusion: 提出的方法能高效保护个人肖像视频免受3D领域口播换脸技术滥用，不仅提升了防御速度和保真度，还能应对多种攻击，适用于实际隐私保护场景。

Abstract: State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

</details>


### [29] [Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model](https://arxiv.org/abs/2512.21032)
*Mingshu Cai,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的新方法，实现从红外（热成像）人脸图像高质量还原到可见光图像，并在身份保持和图像质量上均刷新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 大多数人脸识别模型基于可见光训练，但监控场景下常需识别夜间红外图像，因域差导致识别性能大幅下降。传统特征方法和现有生成式红外-可见光转换方法存在特征丢失、失真等问题，亟需改进。

Method: 提出潜在扩散模型实现红外到可见光高保真转换，加入多属性分类器加强身份特征提取，并提出Self-attn Mamba模块以提升全局跨模态特征建模能力和推理速度。

Result: 在两大红外-可见光人脸数据集上，方法在生成图像质量和身份一致性方面均优于现有技术。

Conclusion: 该方法有效缓解了红外与可见光域间巨大差异，生成图像在身份字形与质量两方面均达业界领先，具实际部署潜力。

Abstract: Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.

</details>


### [30] [Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising](https://arxiv.org/abs/2512.21038)
*Yiwen Shan,Haiyu Zhao,Peng Hu,Xi Peng,Yuanbiao Gou*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自监督去噪方法，能够在去除相关噪声的同时更好地保留图像细节，并能兼容无损超分辨任务，在实际数据上实现了最新表现。


<details>
  <summary>Details</summary>
Motivation: 传统自监督盲点网络（BSN）去噪方法利用像素重排降采样进行噪声解相关，但降采样幅度过大会破坏细节，不足又去不了相关噪声，因此始终难以兼顾细节和去噪效果。

Method: 提出Next-Scale Prediction（NSP）方法，构造跨尺度的训练对：用完全解相关的低分辨率子图像作为输入，预测保有高分辨细节的高分辨目标，从而实现去噪和细节保留的分离。该结构天然支持噪声图像的超分任务，而且无需重新训练。

Result: 在多个真实数据集上，NSP方法实现了最先进的自监督图像去噪效果，显著缓解了细节与去噪之间的冲突。

Conclusion: NSP方法为自监督去噪提供了有效新范式，实现了兼顾噪声消除和细节保留的优异性能，并具备推广为无损图像超分辨的能力。

Abstract: Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.

</details>


### [31] [A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography](https://arxiv.org/abs/2512.21040)
*Jaehong Lee,You Chan No,YoungWoo Kim,Duksu Kim*

Main category: cs.CV

TL;DR: 本文提出KOREATECH-CGH，一个公开的6,000对RGB-D图像与高分辨率全息图数据集，并引入了一种提升大深度范围全息质量的波幅投影后处理方法。实验显示该方法在重建质量上超越现有技术，并验证了数据集对机器学习全息系统开发的实用性。


<details>
  <summary>Details</summary>
Motivation: 机器学习驱动的计算全息技术（ML-CGH）发展迅速，但高质量大规模全息数据集匮乏限制了该领域的进步。为支持技术进一步提升，有必要公开一个高质量、大规模的全息图数据集。

Method: 作者构建了KOREATECH-CGH数据集，包含6,000对RGB-D图像与复数全息图，分辨率从256*256到2048*2048，覆盖从理论极限的大深度范围。为提升高深度全息图质量，提出了波幅投影（amplitude projection）后处理技术，逐层替换全息波场的幅度分量，同时保留相位信息。

Result: 使用所提波幅投影方法，重建全息图在PSNR（27.01dB）和SSIM（0.87）上比最新的轮廓掩膜方法分别提升2.03dB和0.04。KOREATECH-CGH数据集在全息生成和超分辨等多种任务中表现良好，验证了其实用性和广泛适用性。

Conclusion: KOREATECH-CGH为ML-CGH研究提供了高质量数据基础，波幅投影方法能显著提升大深度范围全息图的重建质量。数据集与方法共同促进了下一代机器学习全息系统的开发与评估。

Abstract: Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.

</details>


### [32] [Matrix Completion Via Reweighted Logarithmic Norm Minimization](https://arxiv.org/abs/2512.21050)
*Zhijie Wang,Liangtian He,Qinghua Zhang,Jifei Miao,Liang-Jian Deng,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的对数加权范数作为低秩矩阵补全问题中更有效的非凸替代项，有效缓解传统核范数带来奇异值收缩过度的问题，并通过ADMM方法优化，实验验证优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统低秩矩阵补全问题中用核范数作为替代项，但该方法往往导致解的次优，因为对奇异值收缩过度。提出更好的替代范数以优化补全效果。

Method: 提出一种全新的对数加权范数作为秩函数的非凸逼近，通过交替方向乘子法（ADMM）高效求解优化问题。

Result: 在图像修补实验中，新方法无论在视觉效果还是定量指标上均优于当前主流的低秩矩阵补全方法。

Conclusion: 对数加权范数能更好地逼近秩函数，配合ADMM优化，能够有效改善传统方法的不足，提升补全质量。

Abstract: Low-rank matrix completion (LRMC) has demonstrated remarkable success in a wide range of applications. To address the NP-hard nature of the rank minimization problem, the nuclear norm is commonly used as a convex and computationally tractable surrogate for the rank function. However, this approach often yields suboptimal solutions due to the excessive shrinkage of singular values. In this letter, we propose a novel reweighted logarithmic norm as a more effective nonconvex surrogate, which provides a closer approximation than many existing alternatives. We efficiently solve the resulting optimization problem by employing the alternating direction method of multipliers (ADMM). Experimental results on image inpainting demonstrate that the proposed method achieves superior performance compared to state-of-the-art LRMC approaches, both in terms of visual quality and quantitative metrics.

</details>


### [33] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种利用事件相机的光流引导6DoF物体姿态跟踪方法，相较于现有事件视觉方法在精度和鲁棒性上有明显提升。


<details>
  <summary>Details</summary>
Motivation: 传统相机在物体姿态跟踪中容易受到运动模糊、传感器噪声、遮挡和光照变化等因素影响，限制了其实际应用。事件相机具备高动态范围和低延迟等优势，有望解决这些问题。因此，作者希望开发新方法充分利用事件相机的优势以提高跟踪能力。

Method: 首先，提出2D-3D混合特征提取策略，分别从事件流和目标模型中检测角点和边缘以精确表征物体运动。其次，通过在时空窗口内最大化事件相关概率，搜索角点的光流，并利用光流引导关联角点和边缘。最后，迭代优化使角点与边缘之间的距离最小，实现6DoF姿态的连续跟踪。

Result: 在模拟和真实数据上的实验结果表明，所提方法在准确率和鲁棒性方面均优于现有主流事件视觉方法。

Conclusion: 基于事件相机和光流引导的6DoF物体姿态跟踪方法能有效克服传统相机方法面临的挑战，实现更高性能的姿态估计。

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [34] [DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors](https://arxiv.org/abs/2512.21054)
*Kaustubh Kundu,Hrishav Bakul Barua,Lucy Robertson-Bell,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 该论文提出了DexAvatar，一个可以从单目视频中高精度重建手势和身体动作的框架，显著提升了手语3D姿态估计的质量。


<details>
  <summary>Details</summary>
Motivation: 目前手语生成需大量高精度2D和3D姿态数据。然而，大多数数据集仅含有自动重建的2D关键点，缺乏准确的3D信息；同时，现有从视频估算3D姿态的技术精度不足，容易受遮挡、噪声和模糊影响。本研究旨在解决这一数据与算法瓶颈。

Method: 提出DexAvatar框架，利用学习到的3D手部和身体先验知识，从自然环境下的单目手语视频重建生物力学精确的手部细节和全身动作。框架整合了手和身体的3D姿态建模，实现精细动作还原。

Result: 在SGNify数据集（此任务唯一基准数据集）上，DexAvatar在手部和身体姿态估计方面比当前最佳方法提升了35.11%。

Conclusion: DexAvatar显著提升了基于视频的手语3D姿态估计的精度，为手语生成提供了更高质量的数据基础，推动了手语理解和生成人机交互领域的发展。

Abstract: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.

</details>


### [35] [Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control](https://arxiv.org/abs/2512.21058)
*Minghao Han,YiChen Liu,Yizhou Liu,Zizhi Chen,Jingqun Tang,Xuecheng Wu,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: UniPath是一个全新语义驱动的病理图像生成框架，结合了诊断理解能力和多流可控生成策略，显著提升了病理图像合成的质量和语义控制能力。


<details>
  <summary>Details</summary>
Motivation: 目前病理计算中的生成模型主要模拟像素，而理解模型已具备临床级性能。生成模型发展受限于高质数据稀缺、缺乏精细语义控制、诊断术语异构三大难题。研究动机在于融合高级诊断理解，推动可控且高质量的病理图像生成。

Method: 提出UniPath框架，具备三种信息流：1）原始文本流，2）高级语义流——利用冻结的多模态大模型(MLLM)通过可学习查询获得稳健诊断语义token和诊断属性组合，3）原型流——通过原型库实现形态组分级的可控生成。同时，构建了265万规模的图文数据集和高质量注释的6.8万子集。

Result: UniPath在多层次病理评测体系下实现了SOTA性能，如Patho-FID为80.9（较次优提升51%），其语义控制的精细度达到真实图像的98.7%。

Conclusion: UniPath显著缓解了数据短缺与语义不可控难题，提升了病理图像生成的质量和临床相关性。所有数据与代码将公开，促进学界发展。

Abstract: In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.

</details>


### [36] [Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition](https://arxiv.org/abs/2512.21064)
*Hongsong Wang,Heng Fei,Bingxuan Dai,Jie Gui*

Main category: cs.CV

TL;DR: 提出了一种高效的多模态骨骼动作识别自监督框架，通过解耦与组合策略在提升性能的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 多模态动作理解需要高效整合不同模态信息，现有方法要么计算量大、要么效果不佳，亟需一种兼顾效率和性能的新方法。

Method: 提出了解耦（Decomposition）与组合（Composition）策略。解耦策略将多模态特征分解为独立的单模态特征并对齐真实特征；组合策略则将单模态特征联合，作为自监督信号提升多模态表示学习。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II等数据集上实验证明，该方法在计算效率与模型性能之间取得了优异的权衡。

Conclusion: 方法有效融合了多模态特征并降低计算开销，能够兼顾效率与泛化能力，具备实际应用前景。

Abstract: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.

</details>


### [37] [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078)
*Tianchen Deng,Xun Chen,Ziming Li,Hongming Shen,Danwei Wang,Javier Civera,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种新型的视觉定位（VPR）方法UniPR-3D，首次有效整合多视图信息，并基于VGGT主干结合2D和3D特征聚合，显著提升了VPR在多环境下的泛化能力和性能，超过当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉位置识别方法主要依赖单张图片检索，虽有多视图优势但相关探索不足，尤其在多样环境下泛化能力有限。因此，作者希望构建一种能有效利用多视角且具备良好泛化能力的新架构。

Method: 基于VGGT骨干网络，设计了专门聚合2D与3D特征的模块，将VGGT生成的3D与2D token联合形成描述符。提出了专为2D纹理和3D几何设计的聚合器，同时引入了单帧和多帧聚合机制，并采用可变长度序列检索策略。

Result: UniPR-3D在实验中表现出色，超越了单视图和多视图VPR基线，特别验证了以几何为基础的token（3D信息）在VPR中的效果。

Conclusion: UniPR-3D为多视图VPR领域设立了新标杆，验证了结合2D和3D特征能有效提升在各类环境下的定位性能，相关代码与模型将开源，便于社区进一步研究与应用。

Abstract: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

</details>


### [38] [Hierarchical Modeling Approach to Fast and Accurate Table Recognition](https://arxiv.org/abs/2512.21083)
*Takaya Kawakatsu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多任务学习模型和并行推理算法，用于提升表格识别的准确性和推理速度，在两个大型公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有文档表格识别模型虽然取得了不错的效果，但方法复杂且推理效率低，且模型有效性缺乏充分解释。因此，亟需高效、解释性更强的新方法。

Method: 提出采用非因果注意力机制的新型多任务模型，以全面捕捉表格结构，并引入并行推理算法加速单元格内容识别。同时对模型在公开表格数据集上进行了可视化和统计评估。

Result: 新模型在两个公开数据集上，无论是视觉效果还是统计指标都优于现有方法，表现出更高的识别精度和推理速度。

Conclusion: 提出的方法有效提升了表格识别任务的整体表现，特别是在结构捕捉和推理速度方面具有明显优势。

Abstract: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.

</details>


### [39] [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094)
*Zhe Cao,Tao Wang,Jiaming Wang,Yanghai Wang,Yuanxing Zhang,Jialu Chen,Miao Deng,Jiahao Wang,Yubin Guo,Chenxi Liao,Yize Zhang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了T2AV-Compass基准，用于全面评估文本到音频-视频（T2AV）生成系统，弥补当前评测片面、不足的局限。


<details>
  <summary>Details</summary>
Motivation: 目前T2AV生成任务面临评测碎片化的问题，普遍只关注单一模态指标或过窄的问题集合，无法全面评价模型在跨模态对齐、目标指令响应和高感知真实度下的性能。

Method: 作者提出T2AV-Compass，包括500个多样复杂的任务提示，利用分类法驱动构建流程，确保语义丰富和物理合理性。评测体系采用客观的信号级指标（音频、视频质量和跨模态对齐）及主观的多模态大模型判别协议，实现指令遵循和真实性的多层次评价。

Result: 对11个主流T2AV系统的实验显示，即便是最强的模型在人类级别的真实感和跨模态一致性上依然有较大差距，普遍存在音频真实感不足、细粒度同步和指令遵循失败等问题。

Conclusion: 结果表明现有T2AV生成系统有显著提升空间，同时凸显了T2AV-Compass作为严苛诊断性测试平台在推动领域发展的重要价值。

Abstract: Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

</details>


### [40] [UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters](https://arxiv.org/abs/2512.21095)
*Yongkun Du,Zhineng Chen,Yazhen Xie,Weikang Baiand Hao Feng,Wei Shi,Yuchen Su,Can Huang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出了一种轻量级（仅1亿参数）的统一文本与公式识别模型UniRec-0.1B，能够高效、准确地识别多层级文本与公式内容，在多个评测中超过主流大模型和专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）虽能统一识别文本与公式，但模型规模大、计算负担重，难以应用于需要高效识别的场景。因此亟需开发小巧、快速且功能强大的文档解析模型。

Method: 构建了包含4000万样本的大规模数据集UniRec40M，设计了一种分层监督训练和语义解耦分词器，分别应对文档层次结构和文本-公式语义纠缠两项挑战，最终训练出仅1亿参数的UniRec-0.1B模型。

Result: UniRec-0.1B在中英文、多领域、多层级文档上的自建和公开评测集上均优于主流的大型VLMs与业内文档解析专家模型，同时推理速度提升2-9倍。

Conclusion: UniRec-0.1B在保证高效性的同时，实现了对文本与公式的精准统一识别，有望推动文档解析模型在实际应用中的普及与落地。

Abstract: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.

</details>


### [41] [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104)
*Chao Gong,Dong Li,Yingwei Pan,Jingjing Chen,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出了一种名为FreeInpaint的新方法，无需额外微调即可提升文本引导图像修复的效果，实现更高的文本对齐性和视觉合理性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的文本引导图像修复方法在实现视觉逼真效果的同时，往往难以兼顾用户文本提示的严格对齐和结果的合理性。这一矛盾限制了其在实际应用中的表现，促使作者探索无需微调即可提升两者表现的新方案。

Method: 提出FreeInpaint方法，特点包括：（1）推理时直接对扩散潜在向量（diffusion latents）进行优化，无需额外微调；（2）利用先验引导噪声优化方法，通过优化初始噪声来引导模型关注指定修复区域；（3）设计复合指导目标，在去噪过程中针对图像修复任务优化中间潜在变量，从而增强文本对齐性和视觉合理性。

Result: 通过大量实验，涵盖多种图像修复扩散模型和评价指标，作者验证了FreeInpaint方法在提升文本对齐性和视觉合理性方面的有效性和鲁棒性。

Conclusion: FreeInpaint是一种即插即用、无需微调的文本引导图像修复方法，可大幅提升生成内容与用户文本提示的一致性及修复结果的视觉质量，有望广泛应用于相关领域。

Abstract: Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

</details>


### [42] [MarineEval: Assessing the Marine Intelligence of Vision-Language Models](https://arxiv.org/abs/2512.21126)
*YuK-Kwan Wong,Tuan-An To,Jipeng Zhang,Ziqiang Zheng,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文提出了第一个大规模的海洋领域多模态问答数据集MarineEval，并通过该数据集系统性评估了现有视觉语言模型（VLMs）在专业海洋领域的表现。结果显示，当前VLMs远未达到专业领域专家水平，存在较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多领域取得了突破，但尚不清楚其能否作为专业领域专家准确解答具有高专业性的海洋问题。缺乏海洋领域针对性问答数据集，阻碍了对此类模型专业能力的系统评估。

Method: 作者构建了MarineEval数据集，包含2000组基于图像的问答对，覆盖7大任务维度和20个能力维度，并由海洋专家审核专业性。随后，系统评测了17个主流VLMs在该数据集上的表现，分析模型能力及其局限。

Result: 大规模实验证明，现有VLMs在回答专业海洋领域问题时表现不佳，无法胜任海洋领域专家角色，存在明显短板。

Conclusion: MarineEval填补了海洋多模态领域基准的空白，为后续领域特化VLMs的研究提供了数据基础和评测工具，呼吁未来加强模型在专业领域的能力提升。

Abstract: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/

</details>


### [43] [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135)
*Gaoren Lin,Huangxuan Zhao,Yuan Xiong,Lefei Zhang,Bo Du,Wentao Zhu*

Main category: cs.CV

TL;DR: 本文提出了TGC-Net，一个基于CLIP的参数高效医疗图文分割网络，有效解决了当前CLIP在医学图像分割中的结构细节、复杂语义和领域适配性不足等问题，实验结果在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学图文分割方法通常依赖未对齐的图文编码器，导致需要复杂的多模态信息融合模块，而直接采用CLIP在医学分割中又受限于结构细节保护不足、临床表述能力弱和领域语义不对齐等问题。作者旨在兼顾参数效率与分割效果，提升医疗分割任务中的多模态建模能力。

Method: 作者提出了TGC-Net，包括三大核心组件：（1）语义-结构协同编码器（SSE），在CLIP的ViT结构基础上引入CNN分支，实现多尺度结构细节的增强；（2）领域增强文本编码器（DATE），通过引入大语言模型的医学知识丰富文本语义表示；（3）视觉-语言校准模块（VLCM），优化跨模态特征在统一空间的对齐和语义联系。

Result: 在胸部X光和胸腔CT的五个数据集上，TGC-Net都取得了最新最好的分割性能，并且在参数量显著减少的情况下，在多个具有挑战性的数据集上Dice系数明显提升。

Conclusion: TGC-Net有效融合了医学图像和文本信息，在保证参数效率的同时显著提升了分割性能，各项创新组件为复杂多模态医学分割任务提供了有效解决方案。

Abstract: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

</details>


### [44] [ORCA: Object Recognition and Comprehension for Archiving Marine Species](https://arxiv.org/abs/2512.21150)
*Yuk-Kwan Wong,Haixin Liang,Zeyu Ma,Yiwei Chen,Ziqiang Zheng,Rinaldi Gotama,Pascal Sebastian,Lauren D. Sparks,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文介绍了一个名为ORCA的多模态基准数据集，用于提升海洋视觉理解。数据集包含大量精细标注图像，并在多个视觉任务上对现有方法进行评估，旨在促进海洋生态研究。


<details>
  <summary>Details</summary>
Motivation: 当前海洋视觉理解领域受限于训练数据匮乏及缺乏与计算机视觉任务相结合的系统性问题设定，严重制约了相关模型的应用和海洋生态保护的推进。

Method: 作者构建了ORCA多模态基准，包含14,647张来自478种海洋物种的图片，结合42,217个边界框和22,321份专家验证的实例描述。该数据集详细捕捉了物种间细致的形态学特征，并设置了对象检测、实例描述、视觉定位三项任务，综合评估了18个主流视觉模型的表现。

Result: 实验结果显示，当前模型在物种多样性、形态学重叠、专业领域需求等方面面临显著挑战，现有方法在海洋场景下的适应性有限。

Conclusion: ORCA数据集为海洋视觉理解提供了高质量基准，系统揭示了领域难点和当前模型不足，对推动该领域研究具有重要意义。

Abstract: Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.

</details>


### [45] [A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation](https://arxiv.org/abs/2512.21174)
*Chenghao Xu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出了一种新方法EFR，通过旋转特征空间来提升小样本图像生成的能力，克服了现有一致性约束方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有小样本图像生成方法中，由于严格或宽松的一致性约束，都无法很好地应对源域与目标域的分布差异，尤其在目标域样本极少时更难估计准确分布，导致生成质量下降。

Method: 提出Equivariant Feature Rotation (EFR) 方法，利用参数化李群中的自适应旋转将源域和目标域特征投射到一个等变代理特征空间内，在该空间内进行双层面对齐。旋转矩阵既能保持域内结构信息，又促进了跨域知识迁移。

Result: 在多个常用数据集上实验，EFR方法显著提升了目标域上的生成效果。

Conclusion: EFR有效缓解了小样本生成中的域间分布差异问题，比现有一致性约束方法表现更优，适用于小样本图像生成任务。

Abstract: Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.

</details>


### [46] [Towards Arbitrary Motion Completing via Hierarchical Continuous Representation](https://arxiv.org/abs/2512.21183)
*Chenghao Xu,Guangtao Lyu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文首次提出针对人体动作序列的连续隐式表示方法，实现了在任意帧率下的动作插值、过渡甚至外推。


<details>
  <summary>Details</summary>
Motivation: 人体物理动作具有连续性，而现有技术多局限于固定帧率，难以平滑过渡或灵活处理不同帧率下的动作数据。为提升动作序列的平滑性和时间一致性，亟需新的连续动作表示方法。

Method: 作者提出了一种基于隐式神经表示（INRs）的分层隐式表示框架NAME。该方法包含分层时间编码机制，用于多尺度提取动作序列的时序特征，并在MLP解码器中嵌入基于傅里叶变换的自定义参数化激活函数，从而提升对复杂动作的高精度表达能力。

Result: 在多个基准数据集上的实验结果表明，该方法在动作插值、过渡和外推等任务上具有显著的有效性和鲁棒性。

Conclusion: 提出方法能在任意帧率下对人体动作序列进行高质量的连贯插值与外推，是动作表示领域的一项创新，为动作数据的连续建模和应用拓展了新路径。

Abstract: Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.

</details>


### [47] [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185)
*Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan*

Main category: cs.CV

TL;DR: UltraShape 1.0是一种新型的可扩展3D扩散模型框架，能够高保真地产生3D几何体，采用两阶段（粗到细）生成流程，配套了高质量数据处理管线，并具有较强的几何生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D几何生成面临粗糙失真、细节不够和数据集质量低等问题，难以实现高保真的开放式3D生成，因此需要更优的生成框架和数据处理方法。

Method: 1. 两阶段生成流程：先合成粗略的几何结构（全局），再细化生成高质量几何细节；2. 创新数据处理管线，引入封闭处理和高质量筛选，有效提升公开3D数据集质量；3. 在扩散过程中将空间定位与几何细节合成解耦，通过体素和RoPE编码，精细合成局部几何。训练仅用公开数据。

Result: UltraShape 1.0在数据处理质量和几何生成精度上与现有开源方法相比表现具有竞争力，能有效提升公开3D数据的几何细节并维持高质量生成。

Conclusion: UltraShape 1.0展示了在有限训练资源下，也能通过创新管线和解耦策略实现高质量3D几何生成。代码和模型全部开源，有助于推动3D生成领域研究。

Abstract: In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

</details>


### [48] [VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs](https://arxiv.org/abs/2512.21194)
*Brigitta Malagurski Törtei,Yasser Dahou,Ngoc Dung Huynh,Wamiq Reyaz Para,Phúc H. Lê Khac,Ankit Singh,Sofian Chaybouti,Sanath Narayan*

Main category: cs.CV

TL;DR: 本文介绍了一个新基准VisRes Bench，用于评估视觉-语言模型（VLMs）在视觉推理任务中的真实能力，结果表明当前VLMs在抽象视觉推理上表现有限。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在诸如视觉问答和图像描述等任务上取得重大进展，但尚不清楚它们是真正依赖视觉推理还是仅仅利用语言先验。为此，需要一个去除语言线索、能严格测试视觉推理的评测基准。

Method: 作者提出VisRes Bench，包括三层难度，分别考察感知补全及图像匹配（Level 1）、单一属性规则推断（Level 2）、以及组合属性整合推理（Level 3），并在超过1.9万张图片上系统评测主流VLMs。

Result: 实验结果显示，在含有细微感知扰动的场景下，现有VLMs表现接近随机水平，暴露出它们在感知和关系推理方面的明显不足。

Conclusion: VisRes Bench为多模态领域的视觉抽象推理研究提供了统一框架，推动VLMs未来能超越模式识别，实现更高级的视觉理解。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.

</details>


### [49] [Human Motion Estimation with Everyday Wearables](https://arxiv.org/abs/2512.21209)
*Siqi Zhu,Yixuan Li,Junfu Li,Qi Wu,Zan Wang,Haozhe Ma,Wei Liang*

Main category: cs.CV

TL;DR: 该论文提出了EveryWear系统，利用日常可穿戴设备（手机、智能手表、耳机和带摄像头的智能眼镜）实现无需标定的全身运动捕捉，实验结果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于穿戴设备的人体运动估计方法存在佩戴不便、硬件昂贵和标定繁琐等问题，限制其在日常生活中的普及。

Method: 提出基于普通可穿戴设备（智能手机、手表、耳机和带三摄像头的智能眼镜）的运动捕捉方法，通过多模态teacher-student架构结合视觉信息与惯性信号，无需显式标定。还发布了包含56种日常活动、9小时数据和3D标注的Ego-Elec数据集，采用真实数据训练以消除模拟到真实的差距。

Result: 实验显示所提方法在全身运动估计任务中优于现有基线模型，验证了其实用性和准确性。

Conclusion: EveryWear系统实现了基于日常可穿戴设备的实用化人体全身运动捕捉，无需繁琐标定，并提供了高质量的数据集推动相关研究。

Abstract: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.

</details>


### [50] [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218)
*Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig*

Main category: cs.CV

TL;DR: 本文提出了一种无需显式监督的新方法，训练大规模多模态模型（LMMs）自动发现和利用视觉推理标记，从而提升视觉任务推理能力并取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态模型仍以文本为主进行推理，难以胜任以视觉推理为主的任务。现有辅助监督方法（如用辅助图像、深度图等）成本高、泛化差，任务适应性弱，因此亟需更灵活且高效的方案。

Method: 提出了一种任务无关的新机制，通过引入视觉推理token，使模型无需人工干预便可自动发现并利用与任务相关的视觉信息，实现图像的自适应重编码，且无需额外的显式监督。

Result: 在各类以视觉为中心的任务上，该方法超越了直接微调方法，并取得了SOTA效果，且对于中间抽象难以明确指定的任务也表现出色。同时方法可泛化用于多任务指令微调。

Conclusion: 该方法有效突破了LMMs在视觉推理上的局限性，大幅提升了任务间的泛化能力及视觉信息利用效率，为多模态模型的发展提供了新思路。

Abstract: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>


### [51] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于事件的轻量级图像检索方法，通过结合实体提取与多模态模型，实现了在复杂真实世界数据上的高效准确检索。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述到图像的检索在真实场景应用中受限于查询的模糊性、上下文依赖及语义多样性，同时需保证系统高效可扩展。当前方法难以解决这些实际挑战，因此需要新的策略提升检索性能。

Method: 提出一个两阶段检索流程：第一阶段利用事件中心的实体提取和BM25算法，根据突出的实体进行高效候选过滤；第二阶段采用BEiT-3多模态模型对候选结果做深度语义重排序，综合语义信息提升检索准确性。

Result: 在OpenEvents v1基准上测试，提出方法取得了0.559的平均精度，显著优于先前的基线系统。

Conclusion: 将事件引导的实体过滤与长文本视觉-语言建模结合，可有效提升复杂场景下的图像检索表现，兼顾效率与准确性，具有实际价值。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [52] [SegMo: Segment-aligned Text to 3D Human Motion Generation](https://arxiv.org/abs/2512.21237)
*Bowen Dang,Lin Wu,Xiaohang Yang,Zheng Yuan,Zhixiang Chen*

Main category: cs.CV

TL;DR: 提出了一种新的3D人体动作生成框架SegMo，实现了文本和动作的细粒度对齐，在主流数据集上优于现有方法，还能用于动作检索等任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D动作生成方法只在整体上对齐文本和动作，忽视了两者都可自然拆分为更小、语义明确的片段，无法实现更细致的对应和更高质量的生成。

Method: 提出SegMo框架，包括三部分：1）文本片段提取，将复杂的描述分解为有序的简单语义动作短语；2）动作片段提取，将完整的动作序列分割为与文本对应的动作片段；3）通过对比学习实现文本和动作片段的细粒度对齐。

Result: 在HumanML3D等主流数据集上，SegMo在动作生成任务中TOP1得分达到0.553，优于强基线方法。此外，SegMo还能用于动作定位、动作-文本检索等任务。

Conclusion: SegMo实现了文本和3D动作的高效细粒度一一对齐，提升了生成质量，也支持多种检索任务，显示出对虚拟现实等领域的广泛应用潜力。

Abstract: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.

</details>


### [53] [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252)
*Jiawei Liu,Junqiao Li,Jiangfan Deng,Gen Li,Siyu Zhou,Zetao Fang,Shanshan Lao,Zengde Deng,Jianing Zhu,Tingting Ma,Jiayi Li,Yunqiu Wang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DreaMontage的新框架，可以从不同用户输入生成长时长、无缝、表现力强的一镜到底（one-shot）视频，克服现有视频生成方法在视觉流畅性和时序连贯性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 一镜到底拍摄手法极具电影美感，但实际拍摄成本高、难以操作。现有虚拟视频生成方法多采用简单视频剪辑，无法实现真正无缝衔接。本研究旨在降低一镜到底的实现门槛，为用户提供更自然连贯的长视频生成工具。

Method: 1）在DiT模型中引入轻量的中间条件机制，通过自适应调优获得强大任意帧控制能力；2）构建高质数据集，并用视觉表现微调（SFT）提升生成质量，再用定制DPO增强人物动作和过渡自然性；3）设计段落自回归推理策略（SAR），保证长视频生成的内存高效性与可扩展性。

Result: 实验证明，DreaMontage框架能够生成具有高度视觉表现力与无缝连贯性的长时长一镜到底视频，并具备良好的计算效率，显著优于现有方法。

Conclusion: DreaMontage为用户提供了将零碎视觉片段转化为连贯一镜到底影视作品的创新工具，推动了高质量虚拟长视频生成技术的发展。

Abstract: The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

</details>


### [54] [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264)
*Changwei Wu,Yifei Chen,Yuxin Du,Mingxuan Liu,Jinying Zong,Beining Wu,Jie Dong,Feiwei Qin,Yunkang Cao,Qiyuan Tian*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的MRI脑部异常检测（AD）框架，可以在任意MRI模态可用性的条件下实现鲁棒的异常检测和定位，在真实临床环境下表现出良好的泛化能力和实用性。


<details>
  <summary>Details</summary>
Motivation: 目前脑部MRI异常检测存在标注异常样本稀缺、关键成像模态缺失等难题，现有方法往往依赖于固定模态配置、需多次训练，或不能有效泛化到新模态组合，导致临床应用场景受限。

Method: 该方法融合了双通路DINOv2编码器与特征分布对齐机制，使缺失模态特征与全模态特征在统计意义上对齐，即使模态严重缺失也能稳定推理。同时，设计了Intrinsic Normal Prototypes（INPs）提取器和受INP引导的解码器，只重建正常解剖结构并突出异常，从而增强语义一致性。通过训练阶段的随机模态屏蔽与间接特征补全，提升了模型对所有模态配置的适应能力，无需重复训练。

Result: 该方法在BraTS2018、MU-Glioma-Post和Pretreat-MetsToBrain-Masks等数据集上进行了大量实验，在7种模态组合中，在工业和医学AD基线方法上均表现优越，并展现出更好的泛化能力。

Conclusion: 本文建立了一种对于多模态医学异常检测具有良好扩展性的新范式，尤其适用于现实条件下模态不完整的情况，对MRI脑部异常检测的临床应用具有重要意义。

Abstract: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

</details>


### [55] [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](https://arxiv.org/abs/2512.21268)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Attention-Conditional Diffusion（ACD）的新型视频扩散模型，能更精准地根据外部信号生成视频，显著提升可控性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频合成方法在条件控制方面存在难以精确对齐的问题，无论是分类器自由指导还是分类器约束，都会因间接建模或优化作弊带来可控性瓶颈、对条件响应不佳，甚至出现对抗式伪影。为解决这一核心痛点，作者提出直接监督模型关注条件信号，从机制上提升对条件的响应能力。

Method: ACD通过引入注意力监督机制，将模型的注意力图与外部控制信号（如稀疏3D对象布局）强制对齐。同时，提出了稀疏3D对象布局作为高效的条件信号，结合专用的Layout ControlNet和自动化标注流程，实现大规模、高效的布局融合。

Result: 在多个主流视频生成数据集上的广泛实验显示，ACD模型相较于现有方法，在条件输入的对齐度、时序连贯性和视觉保真度方面均取得更优表现。

Conclusion: ACD为条件可控视频合成建立了新的有效范式，为复杂条件下的视频生成任务提供了更高精度及实际可用性的方法。

Abstract: Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.

</details>


### [56] [GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation](https://arxiv.org/abs/2512.21276)
*Snehal Singh Tomar,Alexandros Graikos,Arjun Krishna,Dimitris Samaras,Klaus Mueller*

Main category: cs.CV

TL;DR: 本论文提出了一种新的生成模型方法，通过先生成低分辨率的序列再对每帧进行高分辨率细化，从而提升了图像序列生成质量与速度，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常将图像序列视为大张量处理，效率低且连贯性有限。作者观察到这一方法存在瓶颈，旨在寻找更有效的图像序列建模方式。

Method: 方法包括两步：1）先用Diffusion Transformer（DiT）生成低分辨率的图像序列，捕获帧间相关性；2）对每一帧独立超分辨率还原，补充细节。整个模型无需架构改动，仅需在格点化（grid images）训练基础上即可推广到序列生成任务。

Result: 该方法在不同数据集上实现了优于SoTA的生成质量和序列连贯性，可以生成任意长度的高保真图像序列，并提升训练和推理效率（推理速度至少提升2倍）。

Conclusion: 分步生成和细化策略能有效替代传统大张量建模，兼具高质量、泛化能力和高效率，且能广泛适用于多种图像序列生成场景，无需额外监督或先验知识。

Abstract: Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

</details>


### [57] [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284)
*Shihao Zou,Jingjing Li,Wei Ji,Jincai Huang,Kai Wang,Guo Dan,Weixin Si,Yi Pan*

Main category: cs.CV

TL;DR: 提出了一种名为SpikeSurgSeg的新型神经网络模型，可以在手术场景下实现高效、低延迟的视频分割，并能够用于实时、资源受限的环境。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习基础模型在手术场景分割中表现优异，但它们对计算资源和能耗的高要求限制了在实际手术等资源有限环境中的应用。当前脑启发的脉冲神经网络（SNN）具有效率优势，但由于手术场景数据稀缺和视频本身稀疏，SNN在该领域表现有限。论文旨在解决这些限制，使高效神经网络更适合手术场景分割。

Method: 提出SpikeSurgSeg脉冲驱动的视频Transformer框架，结合SNN和Transformer优点，并提出手术场景掩码自编码预训练策略，通过分层管状掩码增强时空特征学习。同时设计轻量级的脉冲分割头，提升时间一致性并保持低延迟。

Result: 在EndoVis18和自有SurgBleed数据集上的实验显示，SpikeSurgSeg mIoU与最优秀ANN模型相当，推理延迟至少降低8倍，相较大模型基础款能加速20倍以上，体现极高实时性和效率优势。

Conclusion: SpikeSurgSeg在不依赖高端GPU的情况下，能够有效提升手术场景分割的实时性与效率，有望推动高效、低能耗的手术智能系统落地应用。

Abstract: Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

</details>


### [58] [Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction](https://arxiv.org/abs/2512.21287)
*Suren Bandara*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度信号处理方法，通过对表格掩码进行高斯卷积和统计阈值处理，有效检测表格行列边界，在噪声和低分辨率情况下表现突出，提升了表格结构数据提取的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的表格结构识别方法在低分辨率或噪声图像下难以准确识别行列边界，而变换器（transformer）类方法对噪声不够鲁棒，掩码法虽然鲁棒但存在噪声敏感和分辨率损失等问题。

Method: 将表格行列边界转化为一维信号，对这些信号采用逐步增大的方差高斯卷积进行平滑，再通过统计阈值法抑制噪声，同时保留稳健的边界；最后，将检测到的信号峰值映射回图片坐标，获得准确的分割边界。

Result: 在PubLayNet-1M基准集上，与TableNet+PyTesseract OCR结合使用，新方法将列边界检测的Cell-Aware Segmentation Accuracy（CASA）从67%提升到76%，并对分辨率变化具有良好鲁棒性。

Conclusion: 本文提出的方法能够有效提升表格结构检测的准确性和鲁棒性，为下游的数据分析任务提供优质的结构化表格输出，尤其适合低质和降解扫描文档场景。

Abstract: Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.

</details>


### [59] [AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents](https://arxiv.org/abs/2512.21302)
*Yue Cao,Yingyao Wang,Pi Bu,Jingxuan Xing,Wei Jiang,Zekun Zhu,Junpeng Ma,Sashuai Zhou,Tong Lu,Jun Song,Yu Cheng,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了AndroidLens，一个用于评估移动端GUI智能体的新型测试框架，能够更全面、更精细地反映其在复杂实际任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体评测仅涵盖有限应用、简单任务和粗粒度指标，难以真实反映其自动化复杂、高延迟移动设备任务的能力，因此需要更具挑战性和现实性的评测框架。

Method: 作者构建了AndroidLens评测框架，包含571个平均需26步以上的跨中英文长期任务，覆盖38个真实领域与多种复杂约束场景；框架支持静态评测（保留环境异常、允许多路径）及基于里程碑的动态评测（细致衡量任务进度，提出ATP指标）。

Result: 通过对当前主流模型的评测发现，即使表现最好的模型任务成功率仅12.7%，ATP为50.47%，远未达到实际应用需求。

Conclusion: 研究揭示了移动端GUI智能体在应对真实场景时的关键难题，包括环境异常、动态探索和长期记忆，对推动该领域方法进步具有重要意义。

Abstract: Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.

</details>


### [60] [TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning](https://arxiv.org/abs/2512.21331)
*Varun Belagali,Saarthak Kapse,Pierre Marza,Srijan Das,Zilinghan Li,Sofiène Boutaj,Pushpak Pati,Srikar Yellapragada,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Prateek Prasanna,Stergios Christodoulidis Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了TICON，一个Transformer架构的图像小块上下文表示模型，能够为计算病理学的各种任务生成丰富的上下文嵌入，大幅提升了多种基准测试的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的切片编码器仅对切片图像进行独立建模，难以捕获全局病理信息，不利于后续多样化的任务，并且不同切片编码器对下游任务表现各异，缺乏一个统一和通用的解决方案。

Method: 提出TICON模型，采用Transformer架构，将任意切片级基础模型的嵌入统一并进行上下文补充，通过遮盖建模目标进行预训练，实现多来源切片嵌入的融合和上下文化。

Result: TICON在多个切片级和切片整体级的基准测试中均实现了新SOTA成绩，将丰富的上下文信息引入后明显提升了任务表现。同时，仅用11K张WSI预训练的TICON slide-level模型，超越了过去使用多达35万WSI训练的同类模型。

Conclusion: TICON成功实现了多种切片基础模型嵌入的统一上下文化，显著提升了计算病理学多任务表现，验证了只需较小数据量也能训练出优异的全片模型，有望成为领域通用方案。

Abstract: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.

</details>


### [61] [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333)
*Avilasha Mandal,Chaoning Zhang,Fachrina Dewi Puspitasari,Xudong Wang,Jiaquan Zhang,Caiyan Qin,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种在视频分割任务中，基于文本引导的视觉token裁剪框架，有效提升了Segment Anything Model 2（SAM2）的推理效率，通过在时序传播前减少token数量，降低了显存和计算开销，同时保证分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有的SAM2等模型在视频目标分割中由于对所有视觉token进行传播，导致计算和内存开销巨大，制约了其在实际场景中的应用；因此需要设计高效的token筛选方法减轻资源压力。

Method: 作者提出在视觉编码后、时序传播前，采用融合视觉上下文、文本语义相关性（来自用户描述或自动生成的对象文本）、以及不确定性线索等多信息的轻量级路由机制，为token评分，仅保留最具信息量的部分token参与后续处理。该框架无需改变主干分割架构。

Result: 在多个视频分割权威数据集上，所提方法最多可提升42.50%的推理速度、降低37.41%的GPU内存消耗，且在分割准确率（J和F指标）上与SAM2基线相当。

Conclusion: 基于编码器后的token筛选可以大幅提升Transformer类视频分割模型效率，为模型实际部署和资源有限环境下实时应用提供了有效途径。

Abstract: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

</details>


### [62] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 该论文提出了Streamo：一个实时流式视频大模型（LLM），能作为通用交互助手，支持多种流式视频任务，超越了当前仅聚焦问答或字幕生成的模型。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频模型功能单一，难以满足实际中的多样化流式视频理解与互动需求。作者希望打造一个通用、具备多任务能力的流式视频助手，提升视频AI应用的广度和实用性。

Method: 1）提出Streamo模型，支持实时解说、动作理解、事件描述、时序事件定位、敏感问答等流式任务；2）构建了Streamo-Instruct-465K大规模、多任务、顺应指令的数据集，涵盖丰富的时间上下文；3）通过端到端统一训练，让模型具备跨任务泛化与互动能力。

Result: 实验表明，Streamo在多种流式视频基准任务下表现优异，具备强大的时序推理和互动能力，相较于仅支持线下视频分析的旧模型有显著提升。

Conclusion: Streamo显著缩短了线下视频模型与实时多模态助手的差距，推动了流式视频统一智能理解向更实用方向发展。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


### [63] [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/abs/2512.21337)
*Li-Zhong Szu-Tu,Ting-Lin Wu,Chia-Jui Chang,He Syu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文揭示了视觉-语言模型（VLMs）在知名建筑和普通建筑上的准确率差异，表明模型更依赖记忆而非理解。作者提出了YearGuessr数据集，并使用新方法系统评估这一偏差。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉-语言模型在知名建筑上的预测性能远优于普通建筑，暴露出模型存在的流行度偏差。这种偏差表明VLMs缺乏可泛化的理解能力，依赖训练或记忆库中熟悉的样本。因此，论文动机是系统性定量分析此类偏差并开发开放评测基准。

Method: 提出YearGuessr数据集——拥有来自157个国家、55,546张标注为建成年份（1001-2024年）的建筑图片，以及相关的多模态信息（如GPS和热度浏览量）。将建成年份预测建模为有序回归任务，并设计了流行度感知区间准确率指标来量化模型的偏向。对30+模型（包括自研YearCLIP）进行系统评测。

Result: 评测结果显示，现有VLMs在知名度高、被记忆的建筑物上表现极好，但对不知名的建筑物则表现明显下滑。这一现象在全部模型中普遍存在，突出了底层推理能力的缺陷。

Conclusion: 现有VLMs对于流行对象的记忆严重偏向，缺乏对新颖、普通样本的泛化能力。YearGuessr基准和新评测指标为未来改进模型推理和公平性提供了重要参考。

Abstract: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/

</details>


### [64] [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/abs/2512.21338)
*Haonan Qiu,Shikun Liu,Zijian Zhou,Zhaochong An,Weiming Ren,Zhiheng Liu,Jonas Schult,Sen He,Shoufa Chen,Yuren Cong,Tao Xiang,Ziwei Liu,Juan-Manuel Perez-Rua*

Main category: cs.CV

TL;DR: 该论文提出了HiStream框架，大幅提升高分辨率视频生成的效率，在保证视觉质量的同时加速高达107.5倍。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视频生成在数字媒体和影视领域非常重要，但现有扩散模型在推理阶段复杂度高，速度慢，难以实际应用。作者为解决这一计算瓶颈，提出新的高效算法。

Method: 提出HiStream自回归框架，从空间、时间和步长三个层减少冗余：1) 空间压缩，先低分辨率去噪再高分辨率细化，并利用缓存特征；2) 时间压缩，分块逐个推理并用锚点缓存，确保推理速度稳定；3) 步长压缩，对后续条件块减少去噪步数。主要模型只用前两点，增强模型全部三点。

Result: 在1080p基准测试上，HiStream主模型在视觉质量达到SOTA的情况下，去噪速度提升至基线（Wan2.1）的76.2倍，几乎无质量损失；加速版本HiStream+将加速提升到107.5倍，实现更优速度-质量权衡。

Conclusion: HiStream解决了扩散模型高分辨率视频生成的效率瓶颈，使该技术更实用和可扩展。

Abstract: High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: 本文提出了一种利用稀疏自编码器（SAEs）自动挖掘大语言模型（LLMs）与评测基准中缺口的方法，能够在模型和基准层面发现表现薄弱的具体概念领域。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs评估普遍依赖标准化基准，这些基准只提供整体分数，隐藏了模型在哪些子领域表现较弱（“模型缺口”）以及评测覆盖是否均衡（“基准缺口”）的问题，难以深入指导模型改进与基准优化。

Method: 该方法利用稀疏自编码器提取模型内部表征的概念激活，计算加权的表现分数，实现模型与基准在概念层面的自动分解与分析，无需人工监督。作者将该方法应用于两个开源大模型和十个评测基准，验证其实用性。

Result: 实验表明，被测模型在与奉承行为对立（如礼貌拒绝、设定界限）及安全相关的概念上表现一致较弱。这些自动挖出的模型缺口与既有文献中人工发现的观察一致。同时，也发现多个基准在服从、权威、指令相关概念上覆盖过多，却漏掉其本应测评的核心概念。

Conclusion: 该方法通过模型内部表征实现基于概念的细粒度分析，丰富了以往仅有的整体分数。它可帮助解释评分原因，并为后续模型改进和基准完善提供方向，是传统评测分数的有益补充。代码已开源。

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [66] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 提出了一种整合稀疏注意力机制的扩散模型SA-DiffuSeq，有效提升了长文本生成的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然文本生成质量高，但在长文本生成时计算和内存开销极大，限制了应用场景。作者希望解决扩散模型在长序列建模时的效率和可扩展性问题。

Method: SA-DiffuSeq将稀疏注意力机制引入扩散模型流程中，在扩散过程内有选择地分配注意力资源，显著减少计算复杂度。同时设计了适用于稀疏注意力的新型软吸收态，提升了采样效率与序列重建稳定性。

Result: SA-DiffuSeq在多个基线扩散模型上，训练效率和采样速度均取得显著提升，长序列场景下效果尤为突出。实验验证其在语义连贯性和生成质量方面并无损失，且在科学写作、大规模代码生成、多轮长对话等实际应用场景表现优异。

Conclusion: 结构化稀疏性与扩散模型结合是一条高效而富有表现力的长文本生成新路径，推进大规模长文生成的发展。

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [67] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: 本文提出了TokSuite工具包和基准，用于系统性分析和评估分词器对语言模型性能的影响，并训练了一组只在分词器不同的模型进行对比，揭示各种主流分词器的优势与不足。


<details>
  <summary>Details</summary>
Motivation: 分词器虽然是语言模型中的基础环节，但其对模型性能的影响机制尚不明确，缺乏专注于分词器影响的系统性研究工具。本文动机是通过构建专门的数据集和对照实验填补这一空白。

Method: 作者提出并发布了TokSuite，包括十四个架构、数据、预算和初始化完全一致，仅分词器不同的对照模型，以及一个专门关注现实世界分词扰动对模型影响的新基准数据集。通过这些资源，对各种常见分词器进行系统性实验分析。

Result: 实验证明，不同分词器对于模型在各种受扰动场景下的性能有显著影响，各主流分词器在不同任务上各有所长，TokSuite有效区分了它们的优缺点。

Conclusion: TokSuite为分词器影响研究提供了有力工具和基准，有助于社区更好地理解并选择合适的分词策略，进而改进语言模型的鲁棒性和泛化能力。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [68] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 提出了一种对抗训练框架来增强任务型对话系统中的用户模拟器真实感，特别是在心理健康支持场景下，有效揭示对话系统缺陷，并提升模拟器的多样性及预测效度。


<details>
  <summary>Details</summary>
Motivation: 现有的用户模拟器难以真实复现人类行为，而优质模拟器对于有效暴露对话系统的潜在失败点至关重要。作者希望通过提升用户模拟器的真实感，来更好地支持心理健康对话系统的开发和测试。

Method: 提出了生成器-判别器的对抗训练框架。生成器（用户模拟器）与判别器竞争，通过多轮迭代优化模拟器，使其行为更加贴近真实用户；并将该框架应用于心理健康支持对话机器人。

Result: 经过微调和对抗训练后，模拟器在暴露系统失败上的表现优于零样本基线模型。模拟器能够在不同配置下维持高拟合度和低分布偏差。经过三轮对抗训练，判别器的判别准确率显著下降，显示模型生成行为越来越接近真实用户。

Conclusion: 对抗训练是构建高真实感、有效性和多样性的用户模拟器的有前景方法，可用于心理健康对话系统的高效、准确和低成本评估，有助于产品部署前的快速检测。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [69] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型和人类专家与新手导师在数学辅导场景下的应答表现，发现语言模型在教学质量上接近人类专家，但在具体教学策略上存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注大型语言模型在数学辅导中的表现，但它们的教学行为是否与人类专家吻合尚不清楚。作者希望通过细致对比，揭示语言模型和人类专家在教学策略和语言特征上的异同。

Method: 采用受控实验，设专业导师、新手导师及多个大型语言模型，面对相同数学辅导对话情景进行应答。逐项比较各自的教学策略和语言特征，如复述、追问、词汇多样性、可读性、礼貌性等，并统计分析这些特征与感知教学质量的关联。

Result: 大型语言模型生成的教学应答在教学质量评分上接近人类专家，但其采用的复述和重述等策略明显少于专家导师；同时会生成更长、词汇更丰富也更有礼貌的回答。分析发现，复述、词汇多样性和追问能力有助于提升教学质量评分，而过多强调主体性和礼貌用语可能反而影响评分。

Conclusion: 最新的大型语言模型在感知教学质量上已媲美人类专家，但其教学和用语策略仍明显不同。分析这些差异对评估智能辅导系统极具价值，有助于提升其真实教学效能。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [70] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: 本文探讨了在大模型中高效移除特定知识的信息遗忘问题，比较了模型编辑算法与传统machine unlearning方法的表现。结果发现，在某些场景下模型编辑方法更具优势，但都难以完全避免对模型整体性能的损害。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs参数量大增，现有的machine unlearning方法要么效率低下，要么不能彻底移除目标知识，甚至影响模型保持的重要信息。本研究旨在寻找更高效、效果更好的知识遗忘方法。

Method: 综合评估三种主流模型编辑算法（ROME、IKE和WISE），并为遗忘场景定制了新的编辑目标，对比这些方法与传统Unlearning方法在规定场景下的表现。

Result: 实验显示，针对特定遗忘需求，改进的模型编辑方法在遗忘质量上可超越传统unlearning baseline方法。但依旧会面临难以精确限定遗忘范围、保护整体性能的挑战。

Conclusion: 模型编辑方法为大模型知识遗忘提供了有前景的解决思路，但需进一步提升其对遗忘范围的精准控制，以及减少对模型整体性能的副作用。

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [71] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 本文研究了在大型语言模型中，去偏与保持人口统计识别能力之间的独立性，并通过多任务评测和特征消融方法减少模型偏见。


<details>
  <summary>Details</summary>
Motivation: 大模型常存在与人口统计特征（如性别、种族、教育等）相关的偏见。过去的去偏技术往往会影响模型识别这些特征的能力。作者想要研究去偏与识别能力是否相关，以及能否“外科手术式”只去除偏见而不伤及识别功能。

Method: 作者设计多任务评估，将人口统计信息与姓名、职业、教育关联，并对模型做特征消融实验。分别测试基于归因和基于相关性的两类特征定位与消融策略，以评估去偏后模型的识别表现和偏见变化。

Result: 在Gemma-2-9B模型上，针对性地对归因特征消融可以减少种族和性别职业偏见，但不会降低对姓名的识别准确率；针对教育偏见，相关性消融更有效，但如果消除了教育任务的归因特征会导致“先验坍塌”，增加总体偏见，需维度区分处理。

Conclusion: 模型偏见来源于特定任务机制而非绝对的人口统计标记。推理时的机制干预可实现精准去偏，不牺牲模型核心能力，需要针对不同偏见维度分别设计去偏机制。

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [72] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 本文提出了一种评估大语言模型（LLM）在处理和操作新颖符号时推理能力的实验框架。通过引入具误导性的语义线索，发现LLM在推理中易受表层语义影响，暴露了其在符号抽象和操控能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在涉及人类价值的决策任务中应用增多，了解其是否具备真正的符号推理能力成为关键。研究动机是探索LLM在遇到陌生和误导性符号时，能否超越表层语义处理，展现出符号抽象和灵活变换能力。

Method: 作者重新定义了常见数字和运算符，赋予其新符号，并设计语义误导情境，通过简单计算任务考察4种主流LLM在这些新符号系统下的处理和抽象能力，尤其关注模型能否抵抗与任务逻辑冲突的表层语义线索。

Result: 实验显示，主流LLM在符号操作中极易受到误导性语义线索影响，哪怕任务很简单，模型依然容易根据错误的表层语义做出决策，表现出符号操控能力的显著不足。

Conclusion: 当前LLM在符号推理上依然存在明显弱点，容易依赖表层语义而非抽象机制。即使推理链条表面遵循指令，深层能力依旧受限。该问题在决策等需鲁棒推理的应用中，可能导致严重后果，应警惕高估LLM推理能力。

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [73] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: 本文提出了EssayCBM，一个注重可解释性的作文自动评分框架，能够以高透明度地评估作文并提供细致反馈，同时保持与黑盒模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动作文评分系统通常作为黑盒操作，不利于教师和学生理解评分依据，缺乏细致且透明的反馈机制。为了解决该问题，作者提出了需要对评分点评过程进行透明化和结构化的需求。

Method: EssayCBM通过对写作的八个核心概念（如论点明确性、证据使用等）分别采用专用预测头进行评分，这些概念分数共同构成一个可解释的瓶颈层，然后再用一个轻量级网络根据这些分数计算最终成绩，此外允许教师手动调整分数并实时查看成绩变化。

Result: EssayCBM实现了与传统黑箱自动评分系统相当的整体评分表现，但优势在于能提供具体且可操作的概念层级反馈。此外，系统为教师提供了即时调整和反馈的能力。

Conclusion: EssayCBM在保证自动评分准确性的前提下，大幅提升了评分过程的可解释性和人机交互性，有助于更透明和负责任的教育评估。

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [74] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出了MediEval基准，结合真实电子健康记录(EHR)与统一医学知识库，系统评估大语言模型在医学领域的事实性与语境一致性，并提出了提升模型安全性的改进微调方法CoRFu。


<details>
  <summary>Details</summary>
Motivation: 现有医学领域大模型评估方法存在局限，要么单独考察事实知识，要么只评估病人级的推理，但未同时验证其正确性，难以系统识别模型在实际应用中的重大安全问题。

Method: 作者提出MediEval基准，将MIMIC-IV EHR与UMLS等生物医学词汇知识库关联生成真实患者上下文中的多样医学事实和反事实陈述，采用四象限框架综合考查知识支撑和上下文一致性；并针对发现的风险，提出一种基于DPO对不安全混淆施以非对称惩罚的CoRFu微调方法。

Result: 在MediEval基准上测试发现，当前主流LLM（包括专有、开源和特定领域模型）普遍存在幻觉、事实颠倒等严重错误。CoRFu方法相较基础模型提升了16.4个macro-F1分数，且完全消除事实颠倒错误。

Conclusion: MediEval基准和CoRFu微调方法显著提升了LLM在医学领域可靠性和安全性，为临床大模型的实际应用提供了更有效的评估与提升手段。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [75] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: 本文介绍了Nemotron 3 Nano 30B-A3B，这是一种混合Mamba-Transformer架构并采用专家混合技术的大模型。该模型训练数据规模庞大，并在推理效率、精度和能力均取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 旨在提升大语言模型的推理效率、准确率以及多任务能力，同时突破上下文长度等限制，并提供更高的开放性和便利性。

Method: Nemotron 3 Nano 采用了Mixture-of-Experts（MoE）和Mamba-Transformer混合架构，在25万亿token上进行预训练，其中包括大量新增数据，在预训练后还进行了有监督微调和大规模RL。

Result: 该模型激活参数量不到上代的一半，但精度更高，推理吞吐量比同级别开源模型高3.3倍，还在多个主流基准上表现更优。支持最长100万token的上下文处理，并加强了推理和对话等能力。

Conclusion: Nemotron 3 Nano 30B-A3B在准确率、推理效率、能力和可开放访问等方面取得了重大进展，是比同类模型更具竞争力的大语言模型。

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [76] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 本文研究了在知识库规模大且不断变化的真实检索环境下，现有检索评估指标在缺乏已知召回率时的适用性，并提出了一种新颖且无需真实相关文档总数的检索质量指标。


<details>
  <summary>Details</summary>
Motivation: 针对实际应用中经常无法获知查询对应的所有相关文档数量，导致无法计算召回率这一问题，作者希望评估并改进检索系统在这种设定下的表现指标。

Method: 比较主流检索质量度量方法与基于大模型生成的回答质量之间的相关性，并在多个相关文档较少（2-15）的数据集上进行实验，此外提出了一种不依赖相关文档总数的新检索指标。

Result: 实验结果表明，文中提出的简单检索质量测度在无需相关文档总数信息下表现良好，且部分现有评估策略与LLM回答质量之间的相关性有限。

Conclusion: 针对难以获知真实相关文档总数的检索环境，传统指标存在不足，文中新指标更适用于评估此类环境下的检索系统性能。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [77] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: 本文介绍了Nemotron 3系列大模型（Nano、Super、Ultra），该系列模型具备强大的推理与对话能力，拥有高吞吐量和极长文本处理能力，并计划开源模型及相关训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在推理、多轮对话与工具调用等任务上需求日益增长，同时需要提升运行效率、处理更长上下文及更低成本，Nemotron 3为此而设计。

Method: Nemotron 3采用了混合专家机制的Mamba-Transformer架构，其中Super与Ultra引入NVFP4训练和LatentMoE提升模型质量，并通过MTP层加速生成。全部模型使用多环境强化学习进行后训练，支持复杂推理与工具调用。Nano追求高精度与极致推理成本，Super适应多智能体协作与高负载场景，Ultra则追求极致准确性和推理。

Result: Nano在同类小模型中表现优异且推理成本低。Super适合IT工单自动化等高工作量任务。Ultra实现了最先进的推理和准确性性能。两大全型还具备更快的文本生成速度。

Conclusion: Nemotron 3系列实现了效果、速度和成本的平衡，为多样化AI应用提供支持。Nano已开放发布，Super和Ultra即将开源，相关模型、训练工具和可分发数据也将一并公开。

Abstract: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [78] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 本文系统性地研究了在严格算力约束下，小型语言模型的结构选择与训练预算如何影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型因应用需求广泛而备受关注，但针对小模型在算力有限条件下的最佳结构设计及效率-精度权衡研究较少。作者希望揭示不同结构（如注意力机制、多层网络等）在有限算力下的表现差异。

Method: 研究以线性预测为基础，逐步引入非线性、自注意力、以及多层Transformer结构，在Tiny Shakespeare字符级、PTB与WikiText-2词级任务上进行系统实验。分别用测试负对数似然值、参数数量和训练FLOPs衡量精度与效率。

Result: 实验发现，即便在小模型规模，基于注意力的结构在每FLOP效率上明显优于MLP。过度增加网络层数或上下文长度但未采取合适优化手段时会损害性能。此外，RoPE等在大模型中有效的结构技巧在小模型上未必适用。

Conclusion: 文章强调了小语言模型结构设计时需权衡效率与精度，不可盲目移植大模型的成功技巧。建议小模型优先考虑高效的注意力结构，同时应精心优化深度和上下文长度。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [79] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理蒸馏溯源追踪框架，以分析在测试情境下学生模型是否真实学到了教师模型的推理能力，还是回归到原有模式，并提出一种基于教师引导的数据选择方法，有效提升蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 以往推理蒸馏方法虽然广泛采用大模型生成推理路径训练小模型，但对学生模型能力的来源分析不足，尤其是在新环境下能否维持教师模型行为不明，影响了模型泛化能力的信心。

Method: 提出推理蒸馏溯源追踪（Reasoning Distillation Provenance Tracing）框架，对学生模型每一步生成的动作，分别获得教师模型、原始学生模型和蒸馏后学生模型的预测概率，并通过对比分类，分析每步推理的来源。此外，提出基于教师-学生分歧直接筛选训练数据的新方法。

Result: 实验证明：在测试阶段，蒸馏学生模型能够生成来源于教师模型的动作，并且这与学生模型表现密切相关。此外，基于分歧的数据选择方法，在多种教师-学生配置下均取得优异效果。

Conclusion: 所提溯源追踪框架可有效解释并指导推理蒸馏过程，为领域提供了新工具和见解，对提升推理蒸馏模型泛化能力具有潜力。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [80] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: 提出了FEND框架，实现对神经精神疾病自动检测的多模态、多语言、大跨度评测。


<details>
  <summary>Details</summary>
Motivation: 神经精神疾病（如阿尔茨海默症、抑郁症、自闭症）具有显著的语言与声音异常，可作为早期诊断的生物标志物。但多模态方法存在多语言泛化与缺乏统一评估框架等挑战，需要系统解决。

Method: 提出了FEND框架，结合语音与文本多模态输入，通过13个含英语、汉语、希腊语、法语和荷兰语的数据集，系统评测AD、抑郁与自闭症检测。重点分析多模态融合、模态失衡、跨任务跨语种泛化等问题。

Result: 多模态融合在阿尔兹海默和抑郁检测中表现优异，但在自闭症检测中受数据集异质性影响效果较差。同时发现多模态融合常因模态失衡未能超越最佳单模态模型；跨语种和任务异质情景下性能明显下降。

Conclusion: FEND为神经精神疾病评估提供了统一多模态、多语种基准，促进领域标准化和可复现性。建议未来研究广泛采纳FEND框架。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [81] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang,Hongzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经网络的轻量化框架，用于在大语言模型中进行逐token的幻觉检测，在多个基准数据集上超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在文本生成和知识问答中表现卓越，但易生成幻觉内容，尤其在高风险领域影响应用。现有幻觉检测方法（如基于不确定性、知识检索）存在高置信错误和对知识库依赖强等问题。

Method: 作者提出了一种神经网络框架：在冻结语言模型参数的基础上，利用轻量的MLP探针对高层隐藏状态做非线性建模，并设计多目标联合损失函数以提升检测稳定性和语义辨析力；还通过贝叶斯优化自动搜索最佳插入层。

Result: 在LongFact、HealthBench和TriviaQA等数据集上，所提MLP探针方法在准确率、召回率和低误报下的检测能力上均明显优于现有最佳方法。

Conclusion: MLP探针能够实时高效、精确地在LLM输出中检测幻觉内容，具有较强的实用性和应用前景。

Abstract: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [82] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 本文提出了TriAligner系统，用于多语言和跨语言事实核查中的主张检索任务，显著提升了检索准确率。


<details>
  <summary>Details</summary>
Motivation: 在虚假信息迅速传播的背景下，准确和高效的事实核查变得至关重要。现有方法在多语言环境下的主张检索效果有限，亟需新方法提升跨语言的检索准确率。

Method: 提出TriAligner，一个结合对比学习的双编码器架构，支持多语言和英语的主张对齐。利用大语言模型进行数据预处理和增强，引入难负样本抽样，提升表示学习效果。系统能自适应学习不同来源内容对齐的重要性。

Result: 在单语和跨语种的评测基准上，TriAligner的主张检索准确率和事实核查表现均显著优于现有基线方法。

Conclusion: TriAligner为多语言、跨语言事实核查主张检索任务提供了有效解决方案，有助于提升信息核查效率，应对多语言环境下的虚假信息传播。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [83] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 提出了一种通过增强调蛋白质等生物序列语言模型的表达能力，从而让它们能够应用链式思维(CoT)推理的新方法。


<details>
  <summary>Details</summary>
Motivation: 链式思维(CoT)在自然语言处理中提升了模型推理能力，但该方法难以应用于如蛋白质或RNA这类表达能力有限的非自然语言模型，因为其token空间较小、表达性弱。为此需要探索如何提升生物序列模型的表达能力以实现CoT推理。

Method: 1) 首次定义了语言表达性（language expressiveness）的概念，用于量化特定语言（包括蛋白质语言）通过其token和语法编码信息的能力。2) 引入了“反思预训练”（reflection pretraining），在生物序列模型中让模型能够产生除答案token以外的辅助“思考token”，用于实现中间推理步骤。3) 理论分析和实验分别验证了增强token集提升了生物语言表达性及模型推理能力。

Result: 增强token集使蛋白质模型能够进行中间推理、自我纠正。在下游任务中，使用反思预训练的模型相较于传统预训练模型有显著性能提升。

Conclusion: 通过拓展生物序列语言模型的表达能力，实现了将链式思维推理迁移应用到蛋白质模型，提升了其复杂任务的推理和自我纠正能力。这为非自然语言领域的复杂模型推理提供了新的路径。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [84] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: 本文提出了MedMistake自动化流水线，自动提取和量化大语言模型（LLM）在医患对话中出现的错误，并将其转化为单问单答（single-shot QA）基准数据集。该方法有助于系统地复现和评估LLM在医学任务中的失败案例，目前已发布了包含多主流模型错误的MedMistake数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型被广泛用于医疗领域，但如何系统、自动地收集和复现它们在临床任务中的错误较为困难，多依赖人工。为推动LLM在安全和可靠性方面的改进，亟需建立高质量、自动化、具复现力的医学错误基准测试集。

Method: 提出了MedMistake自动流水线：(1) 利用LLM生成复杂的医患会话数据；(2) 通过两位LLM评审，对对话中的多维指标（如推理质量、安全性、以患者为中心等）进行评估，并识别错误；(3) 将这些错误自动转化为单问单答形式的QA测试样本。随后，邀请医学专家人工审核部分样本，形成权威子集。

Result: 构建了3390个LLM容易出错的QA样本（MedMistake-All），覆盖GPT-5、Gemini 2.5 Pro等前沿模型。人工审核并验证了211个问答（MedMistake-Bench），使用该子集对12大主流LLM模型进行横向评测，结果显示GPT系列、Claude、Grok等模型表现最佳。

Conclusion: MedMistake成功实现了自动提取LLM医学错误及其结构化量化，有效填补了该领域缺乏标准化自动基准的空白，推动了临床领域LLM模型的可解释性与可靠性研究。数据集和基准已开源，为后续研究和模型优化提供重要资源。

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [85] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 该论文探讨在知识蒸馏过程中，仅对部分序列（尤其是CoT部分）进行监督是否能有效提升小模型的推理能力，并提出了一种减少计算资源消耗的新方法。


<details>
  <summary>Details</summary>
Motivation: 大模型推理能力强，但直接蒸馏推理能力到小模型，需要大量数据和计算资源。长序列的蒸馏尤其耗时耗力，亟需探索如何高效分配监督信号。

Method: 作者分析了在蒸馏Prompt、链式思考（CoT）、答案等不同段落的策略，提出仅对包含Prompt和答案信息的CoT段落蒸馏，并设计了序列截断协议，学习不同截断长度下的效果。

Result: 仅使用每个训练序列前50%的token进行训练，在数学基准上能保留94%的完整序列性能，同时训练时间、显存和FLOPs各减少约50%。

Conclusion: 推理知识蒸馏时，应优先关注前期的推理token，这在保证模型效果的同时显著降低计算开销，为模型压缩和部署提供了有效手段。

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [86] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: 论文提出SFTKey，两阶段训练法优化大模型的输出结果部分，能显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在复杂推理任务中常使用Chain-of-Thought（CoT）技术，但传统的监督微调（SFT）导致模型过度关注冗长的推理过程，而忽视最终的答案部分，影响实际性能和评估。

Method: 提出SFTKey，两阶段训练流程：第一阶段采用常规SFT保证输出格式，第二阶段单独微调答案部分（Key portion），提升最终答案的准确度。

Result: 在多个基准任务和不同模型族上测试，SFTKey方法比常规SFT平均提升超5%的准确率，同时仍保持良好的答案格式。

Conclusion: SFTKey有效解决了传统SFT对于推理过程与答案分配关注度不均的问题，通过优化答案相关token，进一步提升了LLM微调的实际效果。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [87] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya,Zehong Wang,Jiazheng Li,Ziming Li,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 本论文提出数据自适应语义细化（DAS）框架，将固定的图神经网络（GNN）与大语言模型（LLM）闭环结合，实现图结构-语义异质性的动态适应，提升了结构主导图的表现。


<details>
  <summary>Details</summary>
Motivation: 不同图数据中预测信号来源异质，有的依赖节点语义、有的依赖结构特征，固定归纳偏置的模型难以适应所有场景。现有方法试图堆叠归纳偏置，但无法根本解决实际多样化图的挑战。

Method: 提出数据自适应语义细化（DAS）框架，将GNN和LLM结合，GNN对LLM输出的语义做隐式监督，不断细化语义，再反哺GNN，实现闭环优化。

Result: 在丰富文本和无文本图数据上验证，DAS在结构主导图上效果显著提升，在语义主导图上效果也具有竞争力。

Conclusion: 数据自适应的语义细化为结构-语义异质场景下的图学习带来新思路，有效提升了模型性能。

Abstract: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


### [88] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文提出使用半监督学习方法提升大语言模型（LLMs）安全性分类器的性能，强调针对性数据增强的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全性分类器依赖大量有标签数据进行训练，但标注数据昂贵、易出错且常含有合成数据，制约了分类器效果。亟需新的方法以更高效、准确地提升安全分类器能力。

Method: 采用半监督学习方法，结合有标签和无标签数据改进安全任务的表现。同时，针对安全分类任务，研究和设计特定的数据增强方式，并与通用增强手段进行对比分析。

Result: 实验表明，半监督学习能有效提升大语言模型安全性分类器的表现。引入任务特定的数据增强显著优于通用数据增强，改善了模型在安全提示和回复检测上的能力。

Conclusion: 半监督学习结合任务导向增强能充分利用有限有标签数据，提高安全分类任务的效果，为今后LLM安全性提升提供了新思路和实践路径。

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [89] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo,Yi Huang,Mukai Li,Shichang Meng,Fengyuan Liu,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 本文提出了ClarifyMT-Bench多轮消歧基准，用于系统性评测大语言模型（LLM）在多轮不明确情景下的澄清行为，并提出ClarifyAgent模型，有效提升了模型的消歧鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多轮对话中往往面临用户表达信息不完整或存在歧义，但现有评测集中于单轮或理想协作用户，无法真实刻画实际多轮场景下LLM的澄清能力。

Method: 作者提出ClarifyMT-Bench，该基准结合五维歧义类型及六种用户行为模拟，通过LLM-人工混合流程，生成6,120组多轮对话数据，涵盖丰富的模糊来源。利用该基准，评测了10种主流LLM，并开发ClarifyAgent，将澄清能力细分为感知、预测、追踪和规划四模块。

Result: 实验证明主流LLM普遍存在‘澄清不足’问题，容易过早给出回答且随着对话深入能力下降。而ClarifyAgent能显著提升模型消歧表现，对各种歧义条件更具鲁棒性。

Conclusion: ClarifyMT-Bench为大语言模型消歧能力研究提供了可复现的评测基础，ClarifyAgent则有效改进了多轮澄清策略，对推动人机多轮对话中消歧问题研究有重要意义。

Abstract: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [90] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文提出了SpidR-Adapt，一种能以极少无标签数据快速适应新语言的语音表示学习方法，显著提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿只需几百小时的语音暴露即可学会新语言的基本单元，而主流自监督语音模型数据需求极大，存在效率差距。作者旨在缩小这一差距，实现更生物学启发、数据高效的语音模型。

Method: 作者将低资源环境下的语音表示学习建模为元学习问题，设计了多任务自适应预训练（MAdaPT）协议，并提出了首阶双层优化算法（FOBLO），以避免高计算成本。同时，通过交替自监督与有监督目标进行稳健的初始化，提升元训练稳定性。

Result: SpidR-Adapt在少于1小时目标语言音频训练后，在音位区分能力（ABX）和口语建模（sWUGGY、sBLIMP、tSC）任务上均取得显著提升，实现了比标准方法高100倍的数据效率，甚至优于同语种下传统模型。

Conclusion: SpidR-Adapt提供了一条与生物启发一致、结构无关、极为高效的数据利用新路径，对未来低数据语音学习具有实际价值。相关代码和模型已开源。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [91] [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)
*Divij Dudeja,Mayukha Pal*

Main category: cs.CL

TL;DR: SMART是一种针对工程手册（EM）文档的高效结构化记忆与推理变换器，提供了比传统Transformer模型更高的准确率和更低的参数规模。


<details>
  <summary>Details</summary>
Motivation: 工程手册内容冗长、密集且结构复杂，传统Transformer模型对其处理为扁平token流，导致记忆低效并产生自信但错误的答案，实际在工程领域易用性差。

Method: SMART通过三个模块联合处理问题：1）利用句法敏感的Tree LSTM从EM句子中提取主谓宾关系事实；2）用紧凑有序的记忆强化神经网络（MANN）以384维向量索引这些事实并标注来源；3）用6层Transformer融合检索到的事实，生成最终答案。此外，SMART支持两种推理路径：一是针对已知文档的快速索引模式，二是针对新文档的自适应检索辅助模式（FAISS Top 20结果搭配有限记忆）。

Result: SMART模型仅用45.51M参数，比GPT-2和BERT都大幅减少参数量，而且准确度提升了21.3%；在实际部署中，相较小型Transformer模型能有效降低幻觉和错误结果。

Conclusion: 结构化的记忆与推理策略让SMART在处理复杂结构化文档时性能远优于常规Transformer模型，既保证结果准确又大大节省计算资源，具有实际落地优势。

Abstract: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.

</details>


### [92] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: 本文提出了一种通用的并行序列生成框架PTP，在一次模型调用中预测多个相关token，减少了自回归解码时的延迟瓶颈，并实现了不损失建模能力的高效生成。


<details>
  <summary>Details</summary>
Motivation: 目前主流语言模型生成依赖自回归方法，导致推理延迟较大；而现有多token预测方法通常假设各token相互独立，表现有限。亟需一种既能提升生成效率又不牺牲建模能力的方法。

Method: PTP（Parallel Token Prediction）框架，让模型在单次Transformer前向中通过融合采样过程联合预测多个相关token。训练方式包括对已有模型的蒸馏和无教师逆自回归训练。理论上证明PTP可以表达任意自回归分布，突破了独立性假设。

Result: 在Vicuna-7B模型和Spec-Bench上进行实验，PTP每步可接受超过四个token，推断效率和性能达到当前最优水平。

Conclusion: PTP框架实现了长序列的高效并行生成，兼顾推理延迟与模型表达能力，为语言模型推理带来新可能。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [93] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 本文发现当前主流视觉-语言模型（VLM）在ARC类推理基准上的表现劣势，主要源自视觉感知能力不足，而非推理能力本身。


<details>
  <summary>Details</summary>
Motivation: ARC和ARC-AGI等推理基准广泛用于评估AI推理能力，但人类易解、机器难解的现象常被归因于AI推理不足。作者质疑这一观点，怀疑视觉感知才是主要短板。

Method: 提出两阶段实验流程，把感知与推理环节拆开：先单独将图片转为自然语言描述（感知阶段），再用描述进行归纳推理（推理阶段），以对比一体化流程并分析失败来源。

Result: 在Mini-ARC、ACRE和Bongard-LOGO三个数据集上对比发现，多数模型失败（约80%）因感知错误导致。两阶段法显著提升表现，说明感知是主导因素。

Conclusion: ARC类基准混淆了感知与推理，当前性能短板多来自感知，实际推理能力被低估。未来应设计区分感知与推理的评测方法，推动更合理的AI评估。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [94] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: C2LLM是一组基于Qwen-2.5-Coder的代码嵌入大模型，通过PMA机制改进序列表示，性能领先同类大小模型，在MTEB-Code排行榜上取得最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）用于代码嵌入时，序列级表示通常依赖于句末（EOS）标记，信息提取不充分，且嵌入维度不灵活。该工作旨在突破这些瓶颈，提升代码嵌入效果和模型适应性。

Method: 采用Qwen-2.5-Coder为主干，新增Pooling by Multihead Attention（PMA）模块，从所有token聚合序列信息，充分利用LLM因果表示能力，自适应嵌入维度。基于300万公开数据训练出0.5B和7B参数规模的模型。

Result: C2LLM在MTEB-Code指标上，显著超过同等规模其他模型，7B版本取得榜首成绩。

Conclusion: PMA机制能够突破LLM序列嵌入常见的信息瓶颈，C2LLM在代码嵌入任务上表现卓越。

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [95] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新指标“去噪熵”来衡量和指导Masked Diffusion Models (MDMs)生成过程的不确定性，通过调整解码顺序来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: MDMs具有非自回归生成的灵活性，但生成结果对解码顺序极其敏感，导致输出不稳定。作者希望揭示并量化这一现象，提升模型在复杂任务上的可靠性和效果。

Method: 作者首次形式化地分析了MDMs解码顺序对生成质量的影响，将输出差异归因于生成路径上的不确定性累积。为此，提出“去噪熵”作为内部可计算指标，并基于此设计了两种改进生成质量的算法：一种是生成后选择（post-hoc selection），另一种是实时引导（real-time guidance）。

Result: 通过在复杂推理、规划、代码生成等基准上实验证明，基于去噪熵引导的方法能明显提升生成质量和准确度。

Conclusion: 去噪熵为理解和调控MDMs生成过程提供了原理性工具，将不确定性从生成中的“短板”转变为引导高质量输出的优势。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search](https://arxiv.org/abs/2512.20711)
*Jan Mikula,Miroslav Kulich*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Milaps的模型化框架，用于优化移动机器人在连续二维环境下的搜索路径，以最小化找到隐藏目标的期望时间，并在大规模数据集上表现出优越的性能和速度。


<details>
  <summary>Details</summary>
Motivation: 目前，在静态二维连续环境下实现全球最优的移动搜索路径优化很难，主要因环境的连续性以及运动和可见性约束带来的目标函数不可计算性。现有方法大多采用离散化简化问题，或者用启发式手段间接逼近目标函数，但均存在性能和速度的权衡问题。因此，亟需一种既能有效优化全球路径又兼顾效率的解决方案。

Method: 论文提出了Milaps框架，以最小延迟问题为核心，通过引入新的辅助目标，结合旅行递送员问题中的最新anytime元启发式算法进行路径规划。这一方法可快速生成初步解，再为感知配置赋予静态权重，最后进行全局成本的元优化。其整体设计使方案既高效又易适应不同场景。

Result: 在一个新构建的大规模数据集上，Milaps在解的质量和计算时间方面都优于当前最先进的方法。实验显示，最优策略能迅速得到可用路径，并有效在不同感知布局下优化搜索效率。

Conclusion: Milaps提供了一种灵活且高效的移动搜索全局路径优化框架，可应用于多种复杂环境，能够在精度和效率上明显优于传统方法，为移动机器人搜索问题带来了新的解决思路。

Abstract: Expected-time mobile search (ETS) is a fundamental robotics task where a mobile sensor navigates an environment to minimize the expected time required to locate a hidden object. Global route optimization for ETS in static 2D continuous environments remains largely underexplored due to the intractability of objective evaluation, stemming from the continuous nature of the environment and the interplay of motion and visibility constraints. Prior work has addressed this through partial discretization, leading to discrete-sensing formulations tackled via utility-greedy heuristics. Others have taken an indirect approach by heuristically approximating the objective using minimum latency problems on fixed graphs, enabling global route optimization via efficient metaheuristics. This paper builds on and significantly extends the latter by introducing Milaps (Minimum latency problems), a model-based solution framework for ETS. Milaps integrates novel auxiliary objectives and adapts a recent anytime metaheuristic for the traveling deliveryman problem, chosen for its strong performance under tight runtime constraints. Evaluations on a novel large-scale dataset demonstrate superior trade-offs between solution quality and runtime compared to state-of-the-art baselines. The best-performing strategy rapidly generates a preliminary solution, assigns static weights to sensing configurations, and optimizes global costs metaheuristically. Additionally, a qualitative study highlights the framework's flexibility across diverse scenarios.

</details>


### [97] [A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets](https://arxiv.org/abs/2512.20769)
*Tanmay P. Patel,Erica L. Tevere,Erik H. Kramer,Rudranarayan M. Mukherjee*

Main category: cs.RO

TL;DR: 本文提出了一个通用的基于视觉的动态目标自主拦截框架，适用于无人机、地面车和空间飞行器三种异构平台，仅需单目相机和目标标志，实现了无需全局信息的本地自主拦截，并在模拟和实体实验中均表现出高效、鲁棒、低误差。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，各类型机器人常常需自主追踪和拦截动态、不配合的目标，现有方法往往依赖特定平台、传感器融合或全局定位信息，而缺乏能适应多种动力学和观测限制的通用、高效拦截方案。

Method: 方法集成了：(1) 基于扩展卡尔曼滤波器的相对位姿估计，能处理观测中断状况；(2) 融合历史信息的动态目标运动预测器；(3) 实时递推地平线规划器，通过求解约束凸优化规划，实现时间最优且动力学可行的拦截路径。整体系统只依赖单目视觉、部分视场和临时测量。

Result: 在无人机着陆、地面车跟随、空间飞行器接近等典型任务中的仿真与实物实验表明，无论目标运动的确定性与随机性如何，算法都可实现高成功率、低拦截误差、实时嵌入式处理。

Conclusion: 该框架具备良好通用性、鲁棒性与计算高效性，为受限观测、无全局定位的异构移动平台自主动态拦截任务提供了高适应性解决方案。

Abstract: This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.

</details>


### [98] [YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion](https://arxiv.org/abs/2512.20847)
*Parag Khanna,Karen Jane Dsouza,Chunyu Wang,Mårten Björkman,Christian Smith*

Main category: cs.RO

TL;DR: 本文提出了YCB-Handovers数据集，包含2771个人与人之间多种重量物体的交接动作数据，旨在推动人机协作领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作中，机器人缺乏对真实人类交接行为，特别是物体重量变化下姿态调整的理解。现有数据集尚未覆盖人类间真实交接中对重量因素的反应，因此需要一个细致、权威的数据集，帮助研究交接动作、重量敏感型动作规划以及直观的准备提示。

Method: 在标准YCB数据集基础上，收集了2771个人与人之间不同重量物体的交接运动数据，涵盖大范围重量物体。分析细致区分标准交接与需特别注意的交接，并考察物体重量对人类取物动作的影响。提供高分辨率动作时序，为后续数据驱动与仿人模型训练提供支持。

Result: 数据集展示了重量对人类交接动作的明显影响，揭示了人类在准备交接时的行为线索和动作变化。分析结果有助于指导机器人在直观感知物体重量和调整动作规划，提升适应性和交互自然性。

Conclusion: YCB-Handovers数据集为机器人重量敏感型动作规划和人机自然交互研究提供坚实的数据基础。该数据集填补了机器人学习人类交接行为时有关重量影响的数据空白，对于提升未来协作机器人智能和泛化能力具有重要意义。

Abstract: This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.

</details>


### [99] [Early warning signals for loss of control](https://arxiv.org/abs/2512.20868)
*Jasper J. van Beers,Marten Scheffer,Prashant Solanki,Ingrid A. van de Leemput,Egbert H. van Nes,Coen C. de Visser*

Main category: cs.RO

TL;DR: 论文提出一种基于动力学复原力指标的系统安全监控方法，无需依赖准确的系统模型，可为工程系统如无人机等提供早期失控预警。


<details>
  <summary>Details</summary>
Motivation: 传统工程控制方法依赖精确模型，但实际系统（如受损或老化）可能与模型有偏差，易导致控制失效且不易提前发现。需发展无需精确模型的监控和预警手段，提升系统对失稳的感知能力。

Method: 作者借鉴复杂系统临界减速（critical slowing down）这一通用现象，开发出基于动力学复原力指标的系统安全监控方法，无需依赖系统细节模型。该方法通过实时监测系统反馈行为中的早期失稳信号，对接近失控的情况进行预警。

Result: 在无人机等工程设备上实证验证了该方法的有效性，能够实时监测接近失稳的迹象，对潜在失控风险提前预警。实验证明动力学指标能作为通用的系统脆弱性信号。

Conclusion: 这项基于动力学复原力指标的安全监控方法泛用性强，有望应用于无人机、自动驾驶、反应堆等各类反馈系统，提升工程系统早期预警和稳健设计能力。

Abstract: Maintaining stability in feedback systems, from aircraft and autonomous robots to biological and physiological systems, relies on monitoring their behavior and continuously adjusting their inputs. Incremental damage can make such control fragile. This tends to go unnoticed until a small perturbation induces instability (i.e. loss of control). Traditional methods in the field of engineering rely on accurate system models to compute a safe set of operating instructions, which become invalid when the, possibly damaged, system diverges from its model. Here we demonstrate that the approach of such a feedback system towards instability can nonetheless be monitored through dynamical indicators of resilience. This holistic system safety monitor does not rely on a system model and is based on the generic phenomenon of critical slowing down, shown to occur in the climate, biology and other complex nonlinear systems approaching criticality. Our findings for engineered devices opens up a wide range of applications involving real-time early warning systems as well as an empirical guidance of resilient system design exploration, or "tinkering". While we demonstrate the validity using drones, the generic nature of the underlying principles suggest that these indicators could apply across a wider class of controlled systems including reactors, aircraft, and self-driving cars.

</details>


### [100] [Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task](https://arxiv.org/abs/2512.20876)
*Kanata Suzuki,Shota Shimizu,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本文探讨了视觉语言模型(VLM)结合低层次机器人运动数据进行视频理解（如运动轨迹信息）的能力，主要用于自动生成机器人任务描述和分割子任务，通过在仿真器实验中验证新方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前主流VLM主要基于图像和语言训练，缺少来自机器人低层次运动的数据，导致其在理解涉及运动信息的视频方面存在难题。随着机器人技术的发展，验证这些模型是否能理解和应用机器人运动信息对于推动机器人模仿学习和智能化水平提升十分关键。

Method: 提出了将机器人运动数据（如关节和末端执行器状态）作为VLM输入的信息增强方法，利用机器人任务中收集的图像描述与轨迹数据，生成多段“场景”描述，再汇总为完整任务描述。同时，通过对图像描述文本嵌入的相似性分析，实现对复杂任务流程的子任务分割。方法在仿真平台上进行实验验证。

Result: 实验结果表明，将低层次运动信息作为输入可提升VLM在机器人视频任务描述生成和子任务分割两个典型任务上的表现。

Conclusion: 结果证实基础模型（如VLM）通过融合运动信息能够更好地理解和应用机器人相关的视觉-语言-运动任务，有助于提升机器人模仿学习和人机交互效率。

Abstract: From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple "scene" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.

</details>


### [101] [Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms](https://arxiv.org/abs/2512.20888)
*Yiding Nie,Dongliang Fan,Jiatai Huang,Chunyu Liu,Jian S. Dai*

Main category: cs.RO

TL;DR: 本文提出了一种基于连续谱滤波原理的新型可拉伸触觉传感器，实现了超高空间和力分辨率，并在实际软体机器人运动追踪中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有可拉伸触觉传感器在空间分辨率、自解耦能力和抗干扰性能等方面存在明显不足，难以满足软体机器人和人机交互等高要求应用场景。

Method: 作者提出并开发了一种基于连续光谱滤波原理的可拉伸触觉传感器，并对其结构、材料及信号读取方式进行了设计，以实现超高空间分辨率和优异的拉伸/弯曲适应性。

Result: 该传感器在拉伸和弯曲条件下，依然保持高度线性的空间响应（0.996），空间分辨率高达7 μm，力分辨率为5 mN，具备优良的抗穿刺和抗切割能力，并通过集成于平面并联机构上实现了高精度轨迹追踪（旋转分辨率0.02°）。

Conclusion: 连续谱滤波原理的可拉伸触觉传感器兼具高分辨率、自解耦和高鲁棒性，适用于软体机器人、医疗器械及人机交互等高性能传感需求场景。

Abstract: Stretchable sensors indicate promising prospects for soft robotics, medical devices, and human-machine interactions due to the high compliance of soft materials. Discrete sensing strategies, including sensor arrays and distributed sensors, are broadly involved in tactile sensors across versatile applications. However, it remains a challenge to achieve high spatial resolution with self-decoupled capacity and insensitivity to other off-axis stimuli for stretchable tactile sensors. Herein, we develop a stretchable tactile sensor based on the proposed continuous spectral-filtering principle, allowing superhigh resolution for applied stimuli. This proposed sensor enables a high-linear spatial response (0.996) even during stretching and bending, and high continuous spatial (7 μm) and force (5 mN) resolutions with design scalability and interaction robustness to survive piercing and cutting. We further demonstrate the sensors' performance by integrating them into a planar parallel mechanism for precise trajectory tracking (rotational resolution: 0.02°) in real time.

</details>


### [102] [Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality](https://arxiv.org/abs/2512.20931)
*Baoshan Song,Matthew Giamou,Penggao Yan,Chunxi Xia,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种针对GNSS参考系的全局最优定向解算器，能在卫星稀缺或信号受损环境下实现可靠的全局对齐，并能验证结果最优性。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS姿态对齐方法在卫星数量少或信号退化环境下效果不佳，且大多依赖于局部优化算法，存在易陷入局部最优和无法验证解优性的缺陷。为提升算法可靠性、适用性，并推动机器人导航技术发展，亟需一种全局可认证（certifiable）的方法。

Method: 将基于伪距或多普勒观测的原始帧对齐问题构建为非凸二次约束二次规划（QCQP）模型，并进一步放松为拉格朗日对偶问题，获得可计算下界。结合松弛紧致性与可观性分析，提出可认证最优性判别标准，通过仿真与真实实验验证方法有效性。

Result: 实验结果表明，所提方法仅需2颗卫星（利用多普勒测量和二维车辆运动）即可得到可认证的最优解，而VOBA或GVINS等主流方法在同样条件下容易失败或收敛到未知局部最优。同时，相关代码及数据已全部开源。

Conclusion: 本文工作首次实现了在GNSS受限环境下的全局最优帧对齐与可认证结果，显著提升了导航鲁棒性和应用适用范围，为机器人GNSS导航领域技术发展提供了优质工具和理论基础。

Abstract: Estimating the absolute orientation of a local system relative to a global navigation satellite system (GNSS) reference often suffers from local minima and high dependency on satellite availability. Existing methods for this alignment task rely on abundant satellites unavailable in GNSS-degraded environments, or use local optimization methods which cannot guarantee the optimality of a solution. This work introduces a globally optimal solver that transforms raw pseudo-range or Doppler measurements into a convexly relaxed problem. The proposed method is certifiable, meaning it can numerically verify the correctness of the result, filling a gap where existing local optimizers fail. We first formulate the original frame alignment problem as a nonconvex quadratically constrained quadratic program (QCQP) problem and relax the QCQP problem to a concave Lagrangian dual problem that provides a lower cost bound for the original problem. Then we perform relaxation tightness and observability analysis to derive criteria for certifiable optimality of the solution. Finally, simulation and real world experiments are conducted to evaluate the proposed method. The experiments show that our method provides certifiably optimal solutions even with only 2 satellites with Doppler measurements and 2D vehicle motion, while the traditional velocity-based VOBA method and the advanced GVINS alignment technique may fail or converge to local optima without notice. To support the development of GNSS-based navigation techniques in robotics, all code and data are open-sourced at https://github.com/Baoshan-Song/Certifiable-Doppler-alignment.

</details>


### [103] [ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2512.20940)
*Shuhao Ye,Sitong Mao,Yuxiang Cui,Xuan Yu,Shichao Zhai,Wen Chen,Shunbo Zhou,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出ETP-R1框架，将大规模数据和强化微调范式应用于基于图的视觉语言导航任务，实现性能新突破。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法虽然在结构化建图和动作空间简化上有优势，但在利用大规模数据及先进训练范式方面落后于基于大型视觉语言模型（LVLMs）的方法，本研究试图弥补此差距。

Method: 首先利用Gemini API构建了一个高质量大规模预训练数据集，包含多样且低幻觉的指令，提供了将语言映射到拓扑路径的丰富监督信号。同时统一R2R和RxR任务的数据进行联合预训练。随后提出了三阶段训练流程，最后阶段率先将封闭回路、在线强化微调和GRPO算法应用于基于图的视觉语言导航模型。

Result: 在R2R-CE和RxR-CE主流基准集上的各项主要指标均取得新SOTA，展现了方法的优越有效性。

Conclusion: ETP-R1框架有效提升了基于图的视觉语言导航模型的能力，缩小了与LVLMs方法在大数据和训练范式利用上的差距，推动了该领域的发展。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.

</details>


### [104] [From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection](https://arxiv.org/abs/2512.20951)
*Jiangen He,Wanqi Zhang,Jessica Barfield*

Main category: cs.RO

TL;DR: 本研究通过两项实验，考察了社会偏见如何影响人在职业情境下对人工智能代理（机器人）的选择，发现职业类型和固有刻板印象会影响对机器人“肤色”的偏好，可能加剧社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人日益融入各类职业，理解社会偏见是否及如何在对人工代理的选择上体现变得非常重要，否则这些技术部署可能会加剧社会不公。

Method: 作者设计了两项大规模实验（共1038名参与者），涉及建筑、医疗、教育及体育4类职业情境，让参与者在不同肤色和类人性特征的人工代理中做选择；并检验参与者种族对选择的影响及先前暴露（对某种族人类专业人士）对偏好的作用。

Result: 医疗和教育场景中，浅肤色代理更受偏好；而建筑和体育场景中，则对深肤色代理的接受度更高。参与者的种族会影响其选择。实验还发现，事先看到该职业中特定种族的“人类专业人士”，会提高后续选择一致肤色机器人代理的可能性。

Conclusion: 对机器人的选择会复制人际间的职业偏见和肤色歧视，职业与肤色刻板印象可直接迁移至对人工代理的评估。这提示未来机器人部署可能无意间延续甚至放大社会不平等。

Abstract: As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.

</details>


### [105] [Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation](https://arxiv.org/abs/2512.20992)
*Tian-Ao Ren,Jorge Garcia,Seongheon Hong,Jared Grinberg,Hojung Choi,Julia Di,Hao Li,Dmitry Grinberg,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: 本文提出了一种集成高分辨视觉触觉成像与六轴力-力矩传感器的紧凑型多模态传感器，实现了机器人在软组织环境下更稳健地检测皮下特征。


<details>
  <summary>Details</summary>
Motivation: 在软组织环境下，传统的力传感信号受环境变量影响大，难以有效识别皮下微小结构，限制了机器人在医疗、理疗等领域的应用。

Method: 作者设计了一种多模态传感器，将高分辨视觉触觉成像与6轴力-力矩传感器集成，对多种皮下肌腱几何形状的硅胶模型进行实验。比较单独力信号、单独触觉成像及两者融合下对皮下结构的检测能力。

Result: 实验结果表明，单独力信号易产生不确定响应，而触觉成像能准确区分存在性、直径、深度、交错和数量等细节。融合力与触觉信号，不仅能增强皮下结构检测的鲁棒性，还能维持机器人操作的安全与稳定。

Conclusion: 高分辨视觉触觉成像与力传感融合能够实现更可靠的机器人触诊，对医疗、理疗等场景具有重要意义。

Abstract: Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.

</details>


### [106] [Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction](https://arxiv.org/abs/2512.21043)
*Cheng-Yu Kuo,Hirofumi Shin,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本论文提出一种基于能量建模的机器人抓取力控制方法，使机器人手通过触觉自适应调整握力，减少物体滑落，且无需外部感知或先验知识。实验证明方法高效有效，适用于不同物体和动态环境。


<details>
  <summary>Details</summary>
Motivation: 传统机器人在不确定物体性质或外部感知不可靠的情况下，难以精准调节抓取力，导致滑落风险高。而人类仅凭触觉就能快速稳定抓握物体。本文旨在让机器人模仿人类，通过触觉主动探索、快速学习，提升力控制能力。

Method: 提出基于物理的虚拟能量容器模型，将物体视为能量载体。通过比较手指动力输入和物体能量保持的差异，构建感知滑动风险的物理信号。基于此抽象，采用模型学习与规划，利用触觉数据实时建模能量动态并优化抓取力。

Result: 实验证明，在仿真与真实硬件平台上，该方法能在数分钟内从零开始学习抓力控制，显著减少滑动现象，并在多种运动与物体组合下延长抓持时间，无需借助外部传感或物体信息。

Conclusion: 文中方法以物理建模为基础，结合模型驱动的实时学习与规划，实现了通用、快速且自主的机器人抓力控制，对实际机器人抓取任务具有重要应用价值。

Abstract: Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.

</details>


### [107] [Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation](https://arxiv.org/abs/2512.21065)
*Zebin Jiang,Tianle Jin,Xiangtong Yao,Alois Knoll,Hu Cao*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于CLIP的语言引导抓取检测方法（LGGD），通过层次化多模态融合提升机器人在复杂环境下根据自然语言指令的抓取能力，在多个数据集和真实机器人平台上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前语言引导型抓取方法多采取浅层融合策略，导致抓取意图对语义理解有限，无法很好地对齐语言与视觉信息。提升多模态深层融合和视觉-语言强关联性成为当前的关键挑战。

Method: 作者提出LGGD方法，采用Coarse-to-Fine范式，将CLIP视觉与文本嵌入逐步注入视觉特征重构过程，实现语义细粒度对齐。同时设计了基于句子级特征的动态卷积头LDCH，使抓取预测能够自适应任务指令。最后增加精细化模块，增强复杂场景中抓取的一致性与鲁棒性。

Result: 在OCID-VLG和Grasp-Anything++等数据集上，LGGD优于现有方法，对新物体和多样化语言指令表现出强泛化性。在实际机器人平台部署下，体现了高精度与实际有效性。

Conclusion: LGGD能够实现更精细、语义对齐的语言指导抓取预测，对真实世界复杂任务有较好适应性，是推进多模态机器人操作的重要进展。代码将公开。

Abstract: Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.

</details>


### [108] [Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning](https://arxiv.org/abs/2512.21085)
*Shlok Deshmukh,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 本文提出了一种通过差分机构连接在四旋翼上的两自由度轻量机械臂，并采用强化学习实现了六自由度末端控制，在复杂操作任务下表现出高精度与强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决空中机械臂因负载与复杂度受限，传统设计在灵活性与实用性方面的瓶颈，作者希望通过极简机械结构与智能控制方法，兼顾平台简化与操控性能。

Method: 作者设计了一种仅有两自由度的机械臂，通过差分结构固定于无人机上，并通过强化学习（PPO算法）训练智能体，使其产生四旋翼加速度、机体姿态及关节目标角度，并辅以非线性动态反演和PID控制器进行执行。所有训练均在仿真中完成，并在实飞中验证。

Result: 飞行实验显示，系统能实现厘米级的位置精度和角度级的姿态精度，对外部干扰（如推或抓重物）表现出良好鲁棒性。

Conclusion: 研究证明结合智能强化学习控制和简化的机械结构，可以实现高性能、强鲁棒性的空中交互操控，有望推动低成本、轻量化无人机操作平台发展。

Abstract: Aerial manipulators, which combine robotic arms with multi-rotor drones, face strict constraints on arm weight and mechanical complexity. In this work, we study a lightweight 2-degree-of-freedom (DoF) arm mounted on a quadrotor via a differential mechanism, capable of full six-DoF end-effector pose control. While the minimal design enables simplicity and reduced payload, it also introduces challenges such as underactuation and sensitivity to external disturbances, including manipulation of heavy loads and pushing tasks. To address these, we employ reinforcement learning, training a Proximal Policy Optimization (PPO) agent in simulation to generate feedforward commands for quadrotor acceleration and body rates, along with joint angle targets. These commands are tracked by an incremental nonlinear dynamic inversion (INDI) attitude controller and a PID joint controller, respectively. Flight experiments demonstrate centimeter-level position accuracy and degree-level orientation precision, with robust performance under external force disturbances. The results highlight the potential of learning-based control strategies for enabling contact-rich aerial manipulation using simple, lightweight platforms.

</details>


### [109] [Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives](https://arxiv.org/abs/2512.21109)
*Chen Liang,Daniel Rakita*

Main category: cs.RO

TL;DR: 本论文提出在MuJoCo MPC系统中用WASP导数替换传统的有限差分（FD）方法，对机器人工具链带来最高2倍的推理提速和更可靠的控制结果。


<details>
  <summary>Details</summary>
Motivation: 现有的MJPC库广泛依赖MuJoCo模拟器及有限差分（FD）进行导数计算，但FD算法在高自由度或复杂场景时推理速度慢，成为实时控制的瓶颈，限制了其在时间敏感任务中的应用。

Method: 作者提出将Web of Affine Spaces（WASP）这种高效的微分近似计算方法融入MJPC作为FD的无缝替代。WASP通过复用前序相关导数计算信息，能更快更稳定地提供新导数，特别契合MPC的连续迭代优化流程。作者在多种机器人任务与动作规划情景下对比评测了WASP与FD、采样方法的表现。

Result: 实验表明WASP方法能在不破坏MJPC兼容性的前提下统一应用于多样任务，在多自由度系统下，为基于导数求解器（如iLQG）带来最高2倍于FD的速度提升；此外，WASP方法在效率和控制可靠性上也优于MJPC的采样式动作规划器。

Conclusion: WASP导数的集成为MJPC带来显著性能提升，同时保持易用性和兼容性，适用于各类机器人模拟控制任务。作者已开源相关实现以促进后续研究与社区应用。

Abstract: MuJoCo is a powerful and efficient physics simulator widely used in robotics. One common way it is applied in practice is through Model Predictive Control (MPC), which uses repeated rollouts of the simulator to optimize future actions and generate responsive control policies in real time. To make this process more accessible, the open source library MuJoCo MPC (MJPC) provides ready-to-use MPC algorithms and implementations built directly on top of the MuJoCo simulator. However, MJPC relies on finite differencing (FD) to compute derivatives through the underlying MuJoCo simulator, which is often a key bottleneck that can make it prohibitively costly for time-sensitive tasks, especially in high-DOF systems or complex scenes. In this paper, we introduce the use of Web of Affine Spaces (WASP) derivatives within MJPC as a drop-in replacement for FD. WASP is a recently developed approach for efficiently computing sequences of accurate derivative approximations. By reusing information from prior, related derivative calculations, WASP accelerates and stabilizes the computation of new derivatives, making it especially well suited for MPC's iterative, fine-grained updates over time. We evaluate WASP across a diverse suite of MJPC tasks spanning multiple robot embodiments. Our results suggest that WASP derivatives are particularly effective in MJPC: it integrates seamlessly across tasks, delivers consistently robust performance, and achieves up to a 2$\mathsf{x}$ speedup compared to an FD backend when used with derivative-based planners, such as iLQG. In addition, WASP-based MPC outperforms MJPC's stochastic sampling-based planners on our evaluation tasks, offering both greater efficiency and reliability. To support adoption and future research, we release an open-source implementation of MJPC with WASP derivatives fully integrated.

</details>


### [110] [SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation](https://arxiv.org/abs/2512.21133)
*Xiaoyu Mo,Jintian Ge,Zifan Wang,Chen Lv,Karl Henrik Johansson*

Main category: cs.RO

TL;DR: 该论文提出了一种高效可扩展的交通场景稀疏图学习框架 SparScene，通过结构感知的稀疏连接替代以往密集或距离阈值建图方法，实现多智能体高效轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体轨迹生成方法使用基于距离或全连接的密集图来捕捉交通参与者之间的交互，带来了大量冗余边和复杂参数化网络，造成训练和推理效率低下，难以扩展到大型复杂场景。

Method: SparScene 基于车道图拓扑，在智能体与道路及智能体之间构建结构感知的稀疏连接，采用轻量级图编码器高效聚合多种交互关系，从而获得紧凑高效的场景表示。

Result: 在 Waymo Open Motion Dataset (WOMD) 的运动预测任务中，SparScene 以极高效率达到具有竞争力的性能：在5ms内为200多个智能体生成轨迹，推理时间54ms可扩展到5000+智能体与17000+车道，显存消耗仅2.9GB。

Conclusion: SparScene 显著提升了交通场景下多智能体轨迹生成的效率和可扩展性，为大规模复杂交通环境的智能体建模和预测提供了有效工具。

Abstract: Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.

</details>


### [111] [Flocking phase transition and threat responses in bio-inspired autonomous drone swarms](https://arxiv.org/abs/2512.21196)
*Matthieu Verdoucq,Dari Trendafilov,Clément Sire,Ramón Escobedo,Guy Theraulaz,Gautier Hattenberger*

Main category: cs.RO

TL;DR: 本论文提出了一种受动物集群启发的无人机三维集群算法，通过仅依赖最小化的局部邻居交互和简单的对齐、吸引机制，实现了不同集群运动模式的可控转换，并在实地和仿真中验证了其对外部干扰的敏感性与自恢复能力。


<details>
  <summary>Details</summary>
Motivation: 动物集群展现出高效的集体协作能力，激发了自动化无人机集群的设计灵感。现有算法通常复杂或依赖全局信息，难以实现高效、本地化的自主协调。本研究旨在探索仅依靠最基础的局部交互规则，是否可实现多样而稳定的无人机集群行为。

Method: 算法设计使得每架无人机只与一小部分有影响力的邻居进行局部对齐和吸引交互。通过系统性调整交互增益，绘制集群行为相图，结合实地无人机集群实验（10架无人机）和校准飞行力学仿真，研究各类集体行为及对外扰敏感性。

Result: 相图显示集群可在集群化(swarming)和队形化(schooling)两种模式间转变，在关键转变区（临界区）时，群体敏感性、序列波动和重组能力均达到峰值。实测当遇入侵者时，无人机能快速完成集体转向、临时扩散，并在数秒内恢复高度对齐。

Conclusion: 极简的局部交互规则即可实现多种无人机集群行为，通过简单调整参数即可高效切换整体稳定性、灵活性和鲁棒性，为自治无人机集群的设计提供了有效方法。

Abstract: Collective motion inspired by animal groups offers powerful design principles for autonomous aerial swarms. We present a bio-inspired 3D flocking algorithm in which each drone interacts only with a minimal set of influential neighbors, relying solely on local alignment and attraction cues. By systematically tuning these two interaction gains, we map a phase diagram revealing sharp transitions between swarming and schooling, as well as a critical region where susceptibility, polarization fluctuations, and reorganization capacity peak. Outdoor experiments with a swarm of ten drones, combined with simulations using a calibrated flight-dynamics model, show that operating near this transition enhances responsiveness to external disturbances. When confronted with an intruder, the swarm performs rapid collective turns, transient expansions, and reliably recovers high alignment within seconds. These results demonstrate that minimal local-interaction rules are sufficient to generate multiple collective phases and that simple gain modulation offers an efficient mechanism to adjust stability, flexibility, and resilience in drone swarms.

</details>


### [112] [Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation](https://arxiv.org/abs/2512.21201)
*Yu He,Da Huang,Zhenyang Liu,Zixiao Gu,Qiang Sun,Guangnan Ye,Yanwei Fu*

Main category: cs.RO

TL;DR: 提出了一个新颖的零样本物体导航方法Schrödinger's Navigator，能在未知、复杂甚至遮挡严重的环境中更高效地找到目标。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航（ZSON）在真实、杂乱和遮挡严重环境中的表现受限，尤其难以处理不可见区域、动态物体及未知风险，导致导航性能不理想。

Method: 受薛定谔“测不准”思想启发，该方法将未观察区域视为多种潜在场景，针对自我视觉输入和三条备选路径，采用路径条件化3D世界模型，分别沿路径“想象”未来观测结果。把这些3D想象观测整合进导航图，并用来更新value map，从而在不依赖全局建图或额外绕行的情况下，为策略提供避开遮挡、规避风险及跟踪动态目标的参考。

Result: 在Go2 四足机器人上、包括强遮挡、未知风险和动态目标等三种复杂场景下进行实验，结果显示该方法在自定位、目标定位和整体成功率上比分别对比实验中的其他强ZSON基线更具优势，尤其是在遮挡严重环境下。

Conclusion: 基于路径条件化3D想象的Schrödinger's Navigator能有效提升零样本物体导航的鲁棒性和成功率，证明了对不可见空间进行主动推理和风险预判的重要性。

Abstract: Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.

</details>


### [113] [Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3](https://arxiv.org/abs/2512.21219)
*Muhtadin,Faris Rafi Pramana,Dion Hayu Fandiantoro,Moh Ismarintan Zazuli,Atar Fuady Babgei*

Main category: cs.RO

TL;DR: 本论文提出了一种无线嵌入式平衡系统，通过自定义脚部单元实现对仿人舞蹈机器人单腿支撑稳定性的实时无线控制。系统有效提升了机器人在不平坦表面上的动态平衡能力，并消除了传统有线传感器的限制。实验验证了高精度传感与100%平衡维持成功率。


<details>
  <summary>Details</summary>
Motivation: 在仿人舞蹈机器人运动中，单腿支撑阶段的稳定性极为关键，但传统有线传感配置不仅限制关节活动，还引入机械噪声，影响性能。因此迫切需要一种既不影响机械自由度又能高精度传感的新型方案。

Method: 设计了一套无线脚部传感单元，包含4个称重传感器和ESP32-C3微控制器采集压力数据，实时估算压力中心（CoP），并通过无线方式发送到主控板。主控通过PID闭环控制调整身体关键关节，实现动态姿态平衡。

Result: 脚部传感单元平均测量误差仅为14.8g，系统在3度倾斜单腿支撑实验中平衡维持成功率为100%，表现出高可靠性与精度。

Conclusion: 无线CoP实时反馈结合闭环控制能显著提升高自由度仿人机器人的动态姿态稳定性，且系统集成灵活，不影响机械动作，具备实际应用前景。

Abstract: Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.

</details>


### [114] [Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot](https://arxiv.org/abs/2512.21226)
*Shuhan Zhang,Tin Lun Lam*

Main category: cs.RO

TL;DR: 本文提出了一种为SnailBot模块化自重构机器人设计和实现的相对定位系统，实现了高效、鲁棒的实时协作定位。


<details>
  <summary>Details</summary>
Motivation: 模块化自重构机器人在协作任务中需要精确的相对定位，现有系统面临准确性和鲁棒性不足的问题，亟需一种可靠的融合方案。

Method: 本系统融合了ArUco标记识别、光流分析和IMU数据处理，采用基于规则的多源信息融合框架，实现机器人之间的鲁棒准确相对定位。

Result: 实验验证显示，该系统能在动态场景下实现实时、可靠的相对定位，展现了良好的实用性和扩展性。

Conclusion: 该融合系统能够满足模块化机器人协同任务对定位精度和鲁棒性的需求，具备大规模应用潜力。

Abstract: This paper presents the design and implementation of a relative localization system for SnailBot, a modular self reconfigurable robot. The system integrates ArUco marker recognition, optical flow analysis, and IMU data processing into a unified fusion framework, enabling robust and accurate relative positioning for collaborative robotic tasks. Experimental validation demonstrates the effectiveness of the system in realtime operation, with a rule based fusion strategy ensuring reliability across dynamic scenarios. The results highlight the potential for scalable deployment in modular robotic systems.

</details>


### [115] [UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer](https://arxiv.org/abs/2512.21233)
*Chi Zhang,Penglin Cai,Haoqi Yuan,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了一种通过人类触觉数据提升机器人灵巧手触觉学习的低成本方法，实现了人机触觉信息对齐与通用策略迁移。


<details>
  <summary>Details</summary>
Motivation: 传统机器人触觉学习依赖大量真实机器人采集数据，代价高且耗时；同时，人类采集数据因人体与机器人结构差异，难以直接迁移至机器人使用。

Method: 作者提出UniTacHand，将人类与机器人手部触觉信号投影到统一的MANO手模型2D表面，使异构触觉数据具备一致空间表示，再通过对比学习对齐人机触觉表征，仅需10分钟配对数据训练。

Result: 本方法支持从人类到机器人灵巧手的零样本策略迁移，并能泛化到训练数据未见过的物体。混合人类与机器人演示学习，能提升策略性能与数据效率。

Conclusion: UniTacHand促进了触觉灵巧手数据的通用、可扩展与高效学习，为未来机器人触觉智能发展提供了新思路。

Abstract: Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.

</details>


### [116] [RoboCade: Gamifying Robot Data Collection](https://arxiv.org/abs/2512.21235)
*Suvir Mirchandani,Mia Tang,Jiafei Duan,Jubayer Ibn Hamid,Michael Cho,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本论文提出一种通过游戏化远程遥操作平台RoboCade，吸引大众参与机器人演示数据采集的方法，以降低数据采集成本并提升数据规模。实验显示，利用该平台收集的数据有助于提升下游任务的机器人策略性能，同时用户体验也显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习严重依赖高质量人类演示数据集，但这类数据采集过程昂贵且耗时，且限制了数据集可扩展性。作者希望通过设计更有趣和包容性强的方法，扩大数据来源，解决扩展难题。

Method: 作者设计了RoboCade平台，通过在界面和任务设计中融入游戏化元素（如进度条、排行榜、奖励徽章等），使大众用户能够远程为机器人采集数据。还提出任务设计原则，确保收集数据与真实下游任务高度相关。

Result: 在三个不同的操作任务上测试该平台，收集演示数据后进行策略训练。实验结果表明，融合这类数据后，机器人在非游戏化目标任务上的成功率提升16-56%。用户体验调查显示，新手用户对游戏化平台的喜爱度提升了24%。

Conclusion: 游戏化数据采集平台不仅能高效且大规模收集有用演示数据，还能极大提升用户参与度和体验，对推动机器人学习有重要意义。

Abstract: Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.

</details>


### [117] [LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation](https://arxiv.org/abs/2512.21243)
*Anatoly O. Onishchenko,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.RO

TL;DR: 本文提出了一种名为LookPlanGraph的新方法，可实现大语言模型驱动下的机器人任务规划，在环境状态变化时动态更新场景图，从而提升机器人执行指令的鲁棒性和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的机器人任务计划方法通常使用预构建的场景图，并假设所有任务相关信息在开始时已知。然而，这些方法无法应对执行过程中环境可能发生的变化，导致实际执行效果受限。

Method: LookPlanGraph方法将场景图分为静态资源和对象先验，在执行计划时，借助视觉语言模型不断分析机器人第一人称相机视角，持续验证已有对象或发现新对象，从而动态地在线更新场景图，让LLM能随时获取环境的最新信息。

Result: 在VirtualHome和OmniGibson两个虚拟仿真环境中，通过改变物体位置进行实验，结果显示LookPlanGraph在任务完成率上优于基于静态场景图的方法。此外，作者还在真实世界环境中进行了实验，验证了方法的实际适用性。

Conclusion: LookPlanGraph有效解决了环境动态变化下场景图信息失效的问题，提升了机器人依指令完成任务的能力。论文还公开了新数据集GraSIF和自动化验证框架，有助于该领域的进一步研究和验证。

Abstract: Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .

</details>


### [118] [Quadrupped-Legged Robot Movement Plan Generation using Large Language Model](https://arxiv.org/abs/2512.21293)
*Muhtadin,Vincentius Gusti Putu A. B. M.,Ahmad Zaini,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLM），支持自然语言操控四足机器人（DeepRobotics Jueying Lite 3）的分布式控制系统，在真实环境中验证了高效、自然的导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统四足机器人控制界面存在门槛高、需专业技术背景的问题，限制了其普及与应用。论文希望通过自然语言接口简化用户操作，让非专业人员也可高效控制机器人。

Method: 作者提出了一种分布式系统：将高层的自然语言指令处理（基于LLM）外包到服务器，机器人本体仅负责执行经多传感器融合（LiDAR、IMU、里程计）后的导航指令，显著降低本地计算负担，并在ROS环境中实现。

Result: 在结构化室内环境下的四类任务场景进行实验，系统在各场景下导航成功率超过90%，表现出高鲁棒性。

Conclusion: 实验结果表明，将LLM规划能力“离线”处理后用于自治四足机器人行之有效，大幅提高了系统易用性与实际应用可能性。

Abstract: Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.

</details>
