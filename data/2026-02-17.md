<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 159]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.RO](#cs.RO) [Total: 61]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: 本论文提出了MAILS，一种针对无人机（UAV）系统的无需地图的激光雷达（LiDAR）重定位新方法，有效提升了在弱或无GNSS环境中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR重定位方法多针对自动驾驶，无法适应无人机飞行中点云稀疏、高度和偏航变化大的情况，因此在无人机场景下表现不佳，需要专门设计适应无人机需求的方法和数据集。

Method: 提出了本地保持滑动窗口注意力模块用于从稀疏点云中提取几何特征，设计了与坐标无关的特征初始化和本地不变的位置编码机制以增强模型对于无人机飞行大幅度旋转和高度变化的鲁棒性。同时，构建了包含多场景、多轨迹的大规模无人机激光雷达定位数据集，以反映真实飞行特点。

Result: 在多个实验中，MAILS方法在无人机场景下表现出色，定位精度显著优于现有方法。论文也公开了相关代码和数据集。

Conclusion: MAILS有效解决了无人机LiDAR重定位中的精度和鲁棒性难题，为无人机在GNSS不可用场景下的定位提供了实际可行的新方案，对该领域应用有重要推动作用。

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [2] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 本文探讨了解释性交互学习（XIL）在提升机器学习视觉分类器公平性和透明度中的作用，并对多种XIL方法及其混合方式进行了实验分析。结果显示XIL能够有效减少偏差并改善性别分类公平性，尤其是CAIPI方法还能提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习视觉模型常因数据偏差而产生不公平（如性别分类），因此需要方法提升模型公正性和可解释性。通过利用用户对模型解释的反馈指导模型，XIL可能减轻模型偏差。

Method: 比较了两种主流XIL方法（CAIPI和RRR）以及它们的混合变体，利用GradCAM与BLA生成分割掩码对解释进行定量评估，应用于视觉分类中易受数据偏差影响的性别分类任务。

Result: 实验显示所有XIL方法均能引导模型更关注与预测相关的图像特征（CAIPI效用尤为突出），并降低性别分类中的模型偏差，实现误分类率平衡。部分方法（CAIPI）在提升公平性的同时还能提升准确率，但整体上透明度和公平性提升伴随轻微性能下降。

Conclusion: XIL方法对提升视觉模型的公平性和透明度具有积极作用，CAIPI尤其有潜力在改善解释性的同时提升模型准确率，有助于实现更公平的性别分类。

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: 提出了一种名为COOPERTRIM的自适应特征选择框架，通过利用环境的时间连续性，有效减少合作感知系统中的通信带宽消耗，同时保持感知性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶等场景下合作感知需要多智能体之间共享大量传感器信息，但无线通信带宽有限，难以支撑如此大的数据流量。虽然此前工作尝试仅传输部分特征，但带宽问题依然突出。因此，亟需一种创新方式本质性缓解带宽压力，推动实际部署。

Method: COOPERTRIM框架通过利用环境的时间连续性，提出了一种新的时间不确定性度量方法（conformal temporal uncertainty metric），动态识别和选择对环境动态变化最关键的特征，从而避免重复传输静态信息。系统还设计了数据驱动机制，根据环境复杂度自适应调整所需传输的特征数量。该方法在语义分割和三维检测等任务中进行了评估。

Result: 在多种开源合作分割与检测模型中，COOPERTRIM分别实现了多达80.28%和72.52%的带宽节省，准确率与原有方法相当。相比其他选择策略，COOPERTRIM在带宽降低达72%的情况下IoU的提升可达45.54%。结合压缩技术，带宽最低可降至1.46%，且不损失IoU性能。

Conclusion: COOPERTRIM能动态适应环境变化、定位误差及通信延迟，在实际部署场景中展现出极高灵活性和实际应用前景。

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

</details>


### [4] [Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs](https://arxiv.org/abs/2602.13289)
*Paul Jonas Kurz,Tobias Jan Wieczorek,Mohamed A. Abdelsalam,Rahaf Aljundi,Marcus Rohrbach*

Main category: cs.CV

TL;DR: 本文系统性地研究了多模态大语言模型在量化压缩后的准确性和可靠性表现，并提出了一套缓解方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型在实际场景中应用，对模型的可靠性和部署效率（尤其是在边缘设备上）要求越来越高，但现有模型往往过于自信且模型体积大，难以部署。因此需要研究量化压缩对模型准确性和可靠性的综合影响。

Method: 作者针对视觉问答任务，对Qwen2-VL-7B和Idefics3-8B两款多模态大模型，应用了无数据的HQQ和有数据的MBQ两种后训练量化方法，并在不同量化比特宽度下评估模型的准确性和可靠性。同时，适配Selector置信度估计器到量化多模态环境，测试其在多种量化及OOD场景下的鲁棒性。

Result: 实验发现，后训练量化会削弱模型准确性和可靠性，但数据感知的量化方法对影响有一定缓解。Selector方法能显著提升量化模型的可靠性。将int4 MBQ方法与Selector结合，模型在大幅减少内存消耗（约75%）的情况下，接近未压缩模型的性能。

Conclusion: 本研究首次系统性地分析了多模态场景下的量化与可靠性之间的关系，并提出了兼顾效率和可靠性的实用解决方案，对模型部署具有实际意义。

Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessitating compression. We study the intersection of these two challenges by analyzing how Post-Training Quantization (PTQ) compression affects both accuracy and reliability in Visual Question Answering (VQA). We evaluate two MLLMs, Qwen2-VL-7B and Idefics3-8B, quantized with data-free (HQQ) and data-aware (MBQ) methods across multiple bit widths. To counteract the reduction in reliability caused by quantization, we adapt the Selector confidence estimator for quantized multimodal settings and test its robustness across various quantization levels and out-of-distribution (OOD) scenarios. We find that PTQ degrades both accuracy and reliability. Data-aware methods soften the effect thereof. The Selector substantially mitigates the reliability impact. The combination of int4 MBQ and the Selector achieves the best efficiency-reliability trade-off, closing in on uncompressed performance at approx. 75% less memory demand. Overall, we present the first systematic study linking quantization and reliability in multimodal settings.

</details>


### [5] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: 本文提出了一种名为NutVLM的自适应防御框架，用于提升自动驾驶视觉语言模型（VLMs）在应对多种对抗攻击时的鲁棒性，并在Dolphins基准测试上取得了接近5%的整体性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽提升了感知能力，但在面临物理贴片和不可察觉扰动等对抗威胁时较为脆弱，现有防御方法在鲁棒性和正常样本表现之间存在权衡，难以兼顾两者。因此，需要一种更加全面且高效的防御方案。

Method: 提出NutVLM框架，分为两步：先用NutNet++进行三分类（识别正常样本、本地贴片攻击、全局扰动），针对本地威胁采用灰度掩膜纯化方法；针对全局扰动，采用专家引导的对抗提示调整（EAPT），通过梯度和离散投影生成“纠正性驾驶提示”引导模型注意力，无需全量参数微调。

Result: NutVLM在Dolphins数据集上整体性能（如准确率、语言得分、GPT得分）提升了4.89%，优于现有方法，展示了良好的鲁棒性与实用性。

Conclusion: NutVLM是一种可扩展的自动驾驶VLM对抗防御方案，在各类对抗攻击下均表现出较高安全性和优异的感知决策能力，并兼顾了对正常样本的性能。」

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

</details>


### [6] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: 现有多模态大模型在物理推理方向仍有不足，本文提出一种基于代码生成的新框架，直接检验物理世界的重建与动态推理能力。


<details>
  <summary>Details</summary>
Motivation: 多数现有评测方法（如VQA、VoE）无法有效区分模型是否具备真实的物理推理能力，模型可能通过表面线索猜答案，缺乏可检验性。

Method: 提出VisPhyWorld框架，要求模型根据视觉观察生成可执行的仿真器代码，从而直接对物理世界的建模及推理进行评测。随后建立VisPhyBench，包括209个评价场景和有系统的评测流程，检验模型对物体外观和物理运动的重建能力。

Result: 使用该Benchmark，作者的流程能在97.7%的场景中生成有效重建视频。实验结果显示：虽然最先进的MLLMs在场景语义理解方面表现出色，但在物理参数推断和动态模拟的一致性方面表现较差。

Conclusion: 目前的多模态大模型在真实的物理推理上存在明显短板，VisPhyWorld和VisPhyBench提供了一种更直接、可验证的物理推理能力评价途径和挑战。

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

</details>


### [7] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 该论文针对雷达高分辨率距离像（HRRP）生成数据的评估问题，提出了基于物理意义的分解新方法和新指标。


<details>
  <summary>Details</summary>
Motivation: HRRP 数据在雷达自动目标识别中应用广泛，生成数据有助于缓解数据集不足问题，但目前主流评估 HRRP 生成数据的方法依赖于不可解释的黑盒分类模型，缺乏物理上的解释性和多层次评价。

Method: 作者将 HRRP 数据分解为 mask、特征和噪声三部分，并基于这种分解提出了两个具有物理解释性的评估指标，用于评判生成数据的质量。

Result: 通过使用高成本的挑战性数据集，作者验证了所提出指标的有效性，证明它们具有良好的判别能力。

Conclusion: 论文的分解方法和指标为 HRRP 生成数据的物理可解释性和多层次评价提供了新的途径，优于现有黑盒模型依赖的评估方式。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [8] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本文针对高分辨率距离像（HRRP）强依赖获取条件、影响目标识别稳健性的难题，提出基于几何条件的生成模型，利用大规模海事数据库进行合成建模，有效再现真实数据中的几何趋势。


<details>
  <summary>Details</summary>
Motivation: HRRP虽适用于快速雷达目标识别，但其对采集条件高度敏感，难以在多变场景下稳定应用。已有HRRP合成方法受数据集规模和泛化性限制。因此，亟需能在大范围、多变场景下具备稳健性的HRRP生成机制。

Method: 分析大规模海事数据库中的变异性，发现决定性因素为目标几何尺寸与观测角。以此为条件，训练生成模型（如GAN等），用于合成条件HRRP。

Result: 合成结果能够重现真实数据中由几何驱动的视线趋势，证明该方法可捕捉和泛化HRRP中的核心特征。

Conclusion: 观测几何在HRRP稳健生成中的核心作用得到凸显，基于几何条件的生成模型为提升复杂场景下的目标识别鲁棒性提供了理论和方法支持。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [9] [Effect of Convolutional Depth on Image Recognition Performance: VGG vs. ResNet vs. GoogLeNet](https://arxiv.org/abs/2602.13298)
*Manfred M. Fischer,Joshua Pitts*

Main category: cs.CV

TL;DR: 本文通过对VGG、ResNet和GoogLeNet三种经典卷积神经网络结构的系统对比，揭示了深度对分类性能、收敛性及计算效率的影响。


<details>
  <summary>Details</summary>
Motivation: 当前图像识别中通常通过增加网络深度提升性能，但简单加深网络并非总是带来精度提升、优化稳定或计算效率提升，因此需系统研究不同架构下深度的真实作用。

Method: 统一训练协议，严格区分名义深度与有效深度，比较VGG、ResNet、GoogLeNet三种架构在图像分类中的表现，包括准确率、收敛情况与计算效率。

Result: 仅增大名义深度的网络早期精度趋于饱和且易出现优化不稳定，而含残差（ResNet）和Inception结构的网络可用较低的有效深度换取更高的准确率和更好的精度-计算效率权衡。

Conclusion: 影响网络性能的关键因素是有效深度而非单纯的名义深度，网络结构设计（如残差、Inception模块）能有效提升深度带来的实际益处。

Abstract: Increasing convolutional depth has been central to advances in image recognition, yet deeper networks do not uniformly yield higher accuracy, stable optimization, or efficient computation. We present a controlled comparative study of three canonical convolutional neural network architectures - VGG, ResNet, and GoogLeNet - to isolate how depth influences classification performance, convergence behavior, and computational efficiency. By standardizing training protocols and explicitly distinguishing between nominal and effective depth, we show that the benefits of depth depend critically on architectural mechanisms that constrain its effective manifestation during training rather than on nominal depth alone. Although plain deep networks exhibit early accuracy saturation and optimization instability, residual and inception-based architectures consistently translate additional depth into improved accuracy at lower effective depth and favorable accuracy-compute trade-offs. These findings demonstrate that effective depth, not nominal depth, is the operative quantity governing depth's role as a productive scaling dimension in convolutional networks.

</details>


### [10] [KidMesh: Computational Mesh Reconstruction for Pediatric Congenital Hydronephrosis Using Deep Neural Networks](https://arxiv.org/abs/2602.13299)
*Haoran Sun,Zhanpeng Zhu,Anguo Zhang,Bo Liu,Zhaohua Lin,Liqin Huang,Mingjing Yang,Lei Liu,Shan Lin,Wangbin Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种名为KidMesh的端到端深度神经网络方法，能够直接从MRU影像自动重建儿童先天性肾积水（CH）三维网格，无需复杂后处理。该方法速度快、性能与传统方法相当，且便于后续功能学（如尿动力学）模拟应用。


<details>
  <summary>Details</summary>
Motivation: 当前MRU影像的基于体素分割方法，虽然能提供肾积水区域的形态信息，但难以直接应用于功能学模拟，如尿动力学，需要复杂的后处理将分割结果转为三维网格，这一过程耗时且依赖人工。因此，亟需一种可以自动、直接输出合规三维网格的高效方法。

Method: 提出了KidMesh深度神经网络框架。该方法利用MRU影像提取特征图，再以网格采样方式映射为特征顶点，通过变形模板网格生成具体肾积水的3D网格结构。为应对稀疏MRU切片导致的网格标注难题，论文设计了无需精确网格标注的新训练策略。

Result: KidMesh能在平均0.4秒内完成CH三维网格重建，且性能与传统方法相当（Dice值0.86），无需后处理。重建网格无自交，顶点距离误差在3.2mm以上仅占3.7%，在6.4mm以上仅占0.2%。

Conclusion: KidMesh突破了依赖网格后处理的局限，实现了从MRU到三维网格的自动、快速且精确的重建，对促进肾积水诊断和功能学模拟具有重要意义，能为临床尿动力学提供有效数据支持。

Abstract: Pediatric congenital hydronephrosis (CH) is a common urinary tract disorder, primarily caused by obstruction at the renal pelvis-ureter junction. Magnetic resonance urography (MRU) can visualize hydronephrosis, including renal pelvis and calyces, by utilizing the natural contrast provided by water. Existing voxel-based segmentation approaches can extract CH regions from MRU, facilitating disease diagnosis and prognosis. However, these segmentation methods predominantly focus on morphological features, such as size, shape, and structure. To enable functional assessments, such as urodynamic simulations, external complex post-processing steps are required to convert these results into mesh-level representations. To address this limitation, we propose an end-to-end method based on deep neural networks, namely KidMesh, which could automatically reconstruct CH meshes directly from MRU. Generally, KidMesh extracts feature maps from MRU images and converts them into feature vertices through grid sampling. It then deforms a template mesh according to these feature vertices to generate the specific CH meshes of MRU images. Meanwhile, we develop a novel schema to train KidMesh without relying on accurate mesh-level annotations, which are difficult to obtain due to the sparsely sampled MRU slices. Experimental results show that KidMesh could reconstruct CH meshes in an average of 0.4 seconds, and achieve comparable performance to conventional methods without requiring post-processing. The reconstructed meshes exhibited no self-intersections, with only 3.7% and 0.2% of the vertices having error distances exceeding 3.2mm and 6.4mm, respectively. After rasterization, these meshes achieved a Dice score of 0.86 against manually delineated CH masks. Furthermore, these meshes could be used in renal urine flow simulations, providing valuable urodynamic information for clinical practice.

</details>


### [11] [DriveMamba: Task-Centric Scalable State Space Model for Efficient End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13301)
*Haisheng Su,Wei Wu,Feixiang Song,Junjie Zhang,Zhenjie Yang,Junchi Yan*

Main category: cs.CV

TL;DR: 提出了一种高效的端到端自动驾驶新范式DriveMamba，突破了现有方法的模块串联、信息损失和效率瓶颈，实现了更灵活、更高效的场景理解与决策。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶（E2E-AD）方法普遍采用顺序模块化（如UniAD等），通过感知-预测-规划串行方式整合不同功能，依赖密集BEV特征。这种手动顺序很容易导致信息损失、误差积累，也难以充分建模各任务/传感器间的复杂关系。此外，当前方案在图像特征提取、注意力机制计算效率等方面也存在明显瓶颈，难以扩展到更复杂的时空输入。

Method: 提出DriveMamba，一种任务中心、可扩展的单阶段E2E-AD范式。主要创新包括：1）通过动态任务关系建模与隐式视角对应学习，打破模块界限；2）采用长时序融合，对输入图像特征和任务输出统一稀疏token化，结合基于3D空间排序的线性复杂度操作，有效捕获任务间依赖；3）设计双向轨迹引导的“由局部到全局”扫描机制，加强自车视角下的空间保真性，优化决策。

Result: 在nuScenes和Bench2Drive等主流数据集上广泛实验，DriveMamba在准确率、泛化能力以及系统效率等方面均显著优于现有主流方法。

Conclusion: DriveMamba实现了灵活、高效的端到端自动驾驶，克服了模块串联与效率瓶颈，具备更强的多任务综合能力，为E2E-AD的发展带来新思路。

Abstract: Recent advances towards End-to-End Autonomous Driving (E2E-AD) have been often devoted on integrating modular designs into a unified framework for joint optimization e.g. UniAD, which follow a sequential paradigm (i.e., perception-prediction-planning) based on separable Transformer decoders and rely on dense BEV features to encode scene representations. However, such manual ordering design can inevitably cause information loss and cumulative errors, lacking flexible and diverse relation modeling among different modules and sensors. Meanwhile, insufficient training of image backbone and quadratic-complexity of attention mechanism also hinder the scalability and efficiency of E2E-AD system to handle spatiotemporal input. To this end, we propose DriveMamba, a Task-Centric Scalable paradigm for efficient E2E-AD, which integrates dynamic task relation modeling, implicit view correspondence learning and long-term temporal fusion into a single-stage Unified Mamba decoder. Specifically, both extracted image features and expected task outputs are converted into token-level sparse representations in advance, which are then sorted by their instantiated positions in 3D space. The linear-complexity operator enables efficient long-context sequential token modeling to capture task-related inter-dependencies simultaneously. Additionally, a bidirectional trajectory-guided "local-to-global" scan method is designed to preserve spatial locality from ego-perspective, thus facilitating the ego-planning. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superiority, generalizability and great efficiency of DriveMamba.

</details>


### [12] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: 本文指出，在无监督图像风格迁移中，传统的扩散模型逆过程（如DDIM）在源域信息稀疏、目标域信息丰富时会出现所谓的“频谱崩溃”问题。为此，作者提出了Orthogonal Variance Guidance（OVG）方法，在推理阶段校正噪声分布，提高结构与纹理还原效果。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有扩散逆过程方法在源域频谱稀疏（如草图或低分辨率图像）而目标域频谱丰富时，容易丢失高频纹理，导致输出图像过度平滑且缺乏细节。同时，简单引入噪声又会破坏原结构关联，造成结果失真。

Method: 提出了一种推理阶段的方法：Orthogonal Variance Guidance（OVG），该方法通过在结构梯度零空间内修正ODE动态，实现对理论高斯噪声量级的约束，从而平衡结构与纹理重建。

Result: 在显微镜超分辨率（BBBC021）和草图转影像（Edges2Shoes）任务上，大量实验验证了OVG方法可以明显恢复逼真纹理，同时保证结构的准确再现。

Conclusion: OVG方法有效解决了扩散逆过程中的“频谱崩溃”问题，实现了结构与纹理的双重提升，为无配对图像翻译任务提供了新的技术路径。

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [13] [Progressive Contrast Registration for High-Fidelity Bidirectional Photoacoustic Microscopy Alignment](https://arxiv.org/abs/2602.13304)
*Jiahao Qin*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的PCReg-Net框架，用于高效消除高速光学分辨率光声显微成像中前后扫描线间的错位问题，大幅提升对齐质量，且具备实时处理能力。


<details>
  <summary>Details</summary>
Motivation: 高速光声显微成像为了提升成像速度，广泛应用双向光栅扫描，但这种方式会产生域偏移和几何错位，严重影响图像配准和后续分析。现有方法多基于亮度恒定假设，难以充分矫正错位。

Method: PCReg-Net通过四个轻量级模块实现进阶矫正：首先用配准U-Net进行粗对齐；然后通过参考特征提取器获取多尺度结构信息；再由对比模块比较粗配准和参考特征以检测残余错位；最终用特征注入的精细U-Net输出高精度图像。此外，本文还提出TNCC和TNCG两种无参考一致性评价指标。

Result: 在OR-PAM-Reg-4K数据集上，PCReg-Net的NCC为0.983，SSIM达0.982，PSNR高达46.96 dB，性能超过现有方法14 dB以上，且处理速度达到实时水平。

Conclusion: PCReg-Net显著提升了双向扫描高速光声显微成像的配准质量和速度，在实际应用中具备很强的推广价值。代码已开源，有助于相关领域发展。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods, constrained by brightness constancy assumptions, achieve limited alignment quality (NCC~$\leq 0.96$). We propose PCReg-Net, a progressive contrast-guided registration framework that performs coarse-to-fine alignment through four lightweight modules: (1)~a registration U-Net for coarse alignment, (2)~a reference feature extractor capturing multi-scale structural cues, (3)~a contrast module that identifies residual misalignment by comparing coarse-registered and reference features, and (4)~a refinement U-Net with feature injection for high-fidelity output. We further propose the Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation of inter-frame temporal consistency. On OR-PAM-Reg-4K (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing the state-of-the-art by over 14 dB at real-time speed. Code is available at https://github.com/JiahaoQin/PCReg-Net

</details>


### [14] [WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery](https://arxiv.org/abs/2602.13305)
*Aydin Ayanzadeh,Prakhar Dixit,Sadia Kamal,Milton Halem*

Main category: cs.CV

TL;DR: 本文提出了WildfireVLM，一个结合卫星影像检测与语言驱动风险评估的AI框架，实现了对野火的实时、智能监控与分析。


<details>
  <summary>Details</summary>
Motivation: 野火频发且威胁日增，现有卫星检测因烟雾信号微弱、天气多变及需要大范围实时分析而受限。因此，亟需高效、智能化的野火早期检测和风险评估方案。

Method: 作者构建了跨多源卫星的数据集，利用YOLOv12检测火区和烟雾，并引入多模态大模型（MLLM）对检测结果进行上下文风险解读与应急建议，采用大模型打分标准验证评估效果，并通过服务化架构实现系统部署和可视化。

Result: WildfireVLM能实时处理卫星数据，实现野火与烟雾的精准检测，结合大模型生成高质量、上下文相关的风险分析，并通过仪表板等工具支持野火追踪和管理。

Conclusion: 将计算机视觉与语言推理结合，可为大规模野火监测提供高效、可扩展的解决方案，提升灾害响应和风险管理能力。

Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.

</details>


### [15] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: 本研究提出利用多任务学习微调Qwen2-VL-7B视觉-语言模型，自动对人类绘画作品进行创造力评分与反馈，达到了高准确性，提供了一种可扩展的艺术创造力评估方案。


<details>
  <summary>Details</summary>
Motivation: 人工评分艺术创造力方法（如Torrance创意思维测试）在人力和效率上难以大规模扩展，现有机器学习方法主要依赖图像特征，缺乏解释性反馈。推动自动化、可解释的艺术创造力评估具有重要研究和实际意义。

Method: 构建包含1000件人类绘画作品的数据集，每件作品由两位专家根据五个维度给出评分和文字评价。使用Qwen2-VL-7B视觉-语言模型，通过多任务学习（数值回归与文字生成），结合结构化评分表和描述，模型能同时预测分数并生成符合量化评价的文字反馈。

Result: 模型在100分制上取得Pearson相关系数>0.97，平均绝对误差为3.95，生成解释性文本与专家评价的语义相似度达到SBERT均值0.798，定量与定性表现都优异。

Conclusion: 该方法有效融合了视觉和语言能力，自动化且可解释地实现了艺术创造力评分，为艺术教育和创造力研究提供了可扩展、实用的工具。

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [16] [Visual Para-Thinker: Divide-and-Conquer Reasoning for Visual Comprehension](https://arxiv.org/abs/2602.13310)
*Haoran Xu,Hongyu Wang,Jiaze Li,Shunpeng Chen,Zizhao Tong,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 本文提出了Visual Para-Thinker，这是首个面向多模态大模型（MLLMs）的视觉并行推理框架，在多个视觉基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理测试主要通过增加推理长度（纵向扩展），但这种方式容易让模型陷入单一思维模式，探索多样性受限。而“并行思考”可以缓解这一问题，但该范式在视觉领域尚未有系统研究。

Method: 作者研究了视觉分区对推理并行化的作用，提出了两种并行推理策略，并基于此开发了Visual Para-Thinker框架。该方法通过Pa-Attention和LPRoPE保证了推理路径独立性和多样性，并用vLLM框架实现多模态高效并行。

Result: 实验证明，在V*、CountBench、RefCOCO和HallusionBench等基准视觉数据集上，该方法能有效扩展并行推理能力至视觉领域，表现出更强的多样性和效率。

Conclusion: Visual Para-Thinker成功将并行推理的优势引入视觉多模态领域，为MLLMs带来了更高效、更丰富的视觉推理能力。

Abstract: Existing LLM test-time scaling laws emphasize the emergence of self-reflective behaviors through extended reasoning length. Nevertheless, this vertical scaling strategy often encounters plateaus in exploration as the model becomes locked into specific thinking pattern. By shifting from depth to parallelism, parallel thinking mitigates the narrowing of exploration. However, the extension of this paradigm to visual domain remains an open research question. In this paper, we first examine the role of visual partitioning in parallelized reasoning and subsequently propose two distinct strategies. Based on the above, we introduce Visual Para-Thinker, representing the inaugural parallel reasoning framework for MLLMs. To maintain path independence and promote diversity in reasoning, our approach integrates Pa-Attention alongside LPRoPE. Leveraging the vLLM framework, we have developed a native multimodal implementation that facilitates high-efficiency parallel processing. Empirical results on benchmark datasets such as V*, CountBench, RefCOCO, and HallusionBench confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

</details>


### [17] [Agentic Spatio-Temporal Grounding via Collaborative Reasoning](https://arxiv.org/abs/2602.13313)
*Heng Zhao,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的时空视频定位（STVG）框架ASTG，利用多模态大模型以无训练、开放世界的方式，实现目标物体在视频中的准确定位，性能优于现有的弱监督和零样本方法。


<details>
  <summary>Details</summary>
Motivation: 传统的STVG方法依赖逐帧定位和重标注，计算冗余且泛化能力有限，弱监督方法则存在性能不足的问题。亟需一种无需训练、能应对开放世界的新型方法。

Method: 提出Agentic Spatio-Temporal Grounder（ASTG）框架，设计了空间推理代理（SRA）和时间推理代理（TRA），基于多模态大模型，通过提出-验证范式实现自主、协同的目标视频片段检索和定位，无需严格监督和训练。

Result: 在主流基准测试上，ASTG方法在弱监督和零样本设定下显著优于现有方法，并且与部分全监督方法表现相当。

Conclusion: ASTG框架实现了高效、泛化、无需训练的时空视频定位，在开放世界任务上展现出强大的应用潜力，有望成为STVG领域的新范式。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to retrieve the spatio-temporal tube of a target object or person in a video given a text query. Most existing approaches perform frame-wise spatial localization within a predicted temporal span, resulting in redundant computation, heavy supervision requirements, and limited generalization. Weakly-supervised variants mitigate annotation costs but remain constrained by the dataset-level train-and-fit paradigm with an inferior performance. To address these challenges, we propose the Agentic Spatio-Temporal Grounder (ASTG) framework for the task of STVG towards an open-world and training-free scenario. Specifically, two specialized agents SRA (Spatial Reasoning Agent) and TRA (Temporal Reasoning Agent) constructed leveraging on modern Multimoal Large Language Models (MLLMs) work collaboratively to retrieve the target tube in an autonomous and self-guided manner. Following a propose-and-evaluation paradigm, ASTG duly decouples spatio-temporal reasoning and automates the tube extraction, verification and temporal localization processes. With a dedicate visual memory and dialogue context, the retrieval efficiency is significantly enhanced. Experiments on popular benchmarks demonstrate the superiority of the proposed approach where it outperforms existing weakly-supervised and zero-shot approaches by a margin and is comparable to some of the fully-supervised methods.

</details>


### [18] [Sim2Radar: Toward Bridging the Radar Sim-to-Real Gap with VLM-Guided Scene Reconstruction](https://arxiv.org/abs/2602.13314)
*Emily Bejerano,Federico Tondolo,Aayan Qayyum,Xiaofan Yu,Xiaofan Jiang*

Main category: cs.CV

TL;DR: 本文提出了Sim2Radar框架，从单视图RGB图像合成毫米波雷达训练数据，有效缓解了真实大规模雷达数据收集和标注的瓶颈，显著提升了雷达感知模型的性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达能在恶劣室内环境（如烟雾、灰尘、弱光）中感知场景，但雷达感知模型训练受限于真实数据集稀缺和昂贵。需要一种可扩展的数据生成方法，以支持雷达感知系统的发展。

Method: Sim2Radar通过单视图RGB图像输入，利用深度估计、分割和视觉-语言推理推断物体材质，重建具备材料属性的3D场景；然后基于ITU-R参数化Fresnel反射模型，采用可配置的物理光线追踪模拟毫米波传播，直接生成合成雷达数据。

Result: 在真实室内场景下，使用合成数据预训练、真实数据微调点云物体检测模型，3D AP（IoU 0.3）最高提升3.7个百分点，提升主要来自空间定位性能增强。

Conclusion: 基于物理建模和视觉驱动的雷达仿真可以为雷达学习提供有效的几何先验，在真实数据有限条件下显著提升雷达感知任务表现。

Abstract: Millimeter-wave (mmWave) radar provides reliable perception in visually degraded indoor environments (e.g., smoke, dust, and low light), but learning-based radar perception is bottlenecked by the scarcity and cost of collecting and annotating large-scale radar datasets. We present Sim2Radar, an end-to-end framework that synthesizes training radar data directly from single-view RGB images, enabling scalable data generation without manual scene modeling. Sim2Radar reconstructs a material-aware 3D scene by combining monocular depth estimation, segmentation, and vision-language reasoning to infer object materials, then simulates mmWave propagation with a configurable physics-based ray tracer using Fresnel reflection models parameterized by ITU-R electromagnetic properties. Evaluated on real-world indoor scenes, Sim2Radar improves downstream 3D radar perception via transfer learning: pre-training a radar point-cloud object detection model on synthetic data and fine-tuning on real radar yields up to +3.7 3D AP (IoU 0.3), with gains driven primarily by improved spatial localization. These results suggest that physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision.

</details>


### [19] [IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2602.13315)
*Yifan Tan,Yifu Sun,Shirui Huang,Hong Liu,Guanghua Yu,Jianchen Zhu,Yangdong Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉token剪枝方法IDPruner，通过平衡token重要性和语义多样性，显著提升多模态大模型的推理效率，并在不同模型和任务上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）在处理视觉信息时，受制于视觉token数量庞大，导致计算瓶颈。现有剪枝方法通常只关注token的重要性或多样性，缺乏一个原理性的集成框架，因此迫切需要更优方法提升效率并兼顾性能。

Method: 作者系统性分析了token重要性和语义多样性之间的权衡，并基于此提出Importance and Diversity Pruner（IDPruner）方法。IDPruner利用最大边际相关性（MMR）算法，在这两者之间取得了Pareto最优平衡。不依赖注意力图（attention maps），可无缝兼容FlashAttention，并能高效“单次剪枝”部署。

Result: 实验覆盖多种模型体系与多模态基准，IDPruner均达到了最先进效果和优越的泛化能力。例如在Qwen2.5-VL-7B-Instruct模型上，剪掉75% token时可保留95.18%性能，极端90%剪枝下仍有86.40%。

Conclusion: IDPruner方法通过原理性整合重要性与多样性，实现高效且兼容的视觉token剪枝，为多模态大模型推理加速提供了通用且有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \textbf{I}mportance and \textbf{D}iversity Pruner (\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\% of baseline performance when pruning 75\% of the tokens, and still maintains 86.40\% even under an extreme 90\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.

</details>


### [20] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: 该论文提出了PolyShapes-Ideal (PSI) 数据集，用于诊断视觉模型是否具备拓扑不变性，并通过三个实验展示了Eidos结构在该类任务上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉基准测试受纹理相关性影响过大，难以有效测试模型是否具备对结构（即拓扑）本质的泛化能力。因此需要一个能够单独考察拓扑不变性的新数据集与新的测试方式。

Method: 作者提出了PSI数据集，包括三种诊断实验：1）带噪声的多边形分类，2）MNIST字体的零样本迁移，3）逐步变形下的几何映射。并使用Eidos架构模型在这些任务上进行测试。

Result: Eidos架构在PSI数据集上取得了大于99%的准确率，在30种未见字体上的零样本迁移表现为81.67%，且无需预训练。

Conclusion: 实验结果验证了“形式优先”（Form-First）假设：在结构受限的建模架构下，良好的泛化能力归因于对几何形态的把握，而非对统计特征的敏感。

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [21] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: 该论文提出了一种分层零样本推理框架，结合轻量级目标检测器与多种紧凑型视觉-语言模型，有效提升自主边缘机器人在动态军事环境下的任务执行能力。实验表明系统在检测、评估与分类任务均表现出高准确率，且实现了快速的自动化决策流程。


<details>
  <summary>Details</summary>
Motivation: 军事动态环境下部署自主机器人面临训练数据稀缺和边缘设备运算能力有限的问题，急需能够在数据极少且资源受限下还能表现良好且安全可靠的感知与决策体系。

Method: 构建分层级推理框架：先用Grounding DINO做高召回率目标区域提出，再由Qwen、Gemma家族的紧凑型视觉-语言模型进行语义验证。针对高置信帧进行多任务评测，并提出“Controlled Input”方法，将感知与推理流程解耦，对模型能力进行细致诊断。

Result: 在Battlefield 6生成的55段高保真视频数据上，框架在误报过滤、损伤评估、细粒度目标分类任务上分别达到100%、97.5%、55-90%准确率。Scout-Commander流程中完成100%正确资产部署，GPT-4o评分推理得分9.8/10，推理延迟低于75秒。同时揭示不同模型在感知与推理上存在的典型失效模式。

Conclusion: 分层零样本方案为自主机器人边缘部署和安全关键应用提供可行解法，并为VLM类模型的系统化测试与认证提供了诊断工具。

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [22] [MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation](https://arxiv.org/abs/2602.13326)
*Xirui Hu,Yanbo Ding,Jiahao Wang,Tingting Shi,Yali Wang,Guo Zhi Zhi,Weizhan Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，实现多角色的人物图像动画，可应对复杂交互和遮挡，取得了业界领先的实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人物图像动画方法主要集中在单一角色，难以适应多角色场景下的人物多样性、互动以及经常出现的遮挡问题，因此亟需突破此局限。

Method: 1) 引入统一动作表示方法，将与身份无关的动作信息与具体角色进行绑定，实现对不同形态人物的泛化与多角色扩展。
2) 提出整体4D锚定范式，在统一4D空间中融合动作与视频潜变量，并通过分层4D监督强化对交互与遮挡处理。
3) 构建端到端框架MotionWeaver并制作相关多角色数据集和基准。

Result: MotionWeaver在新构建的多角色视频基准上（包括46小时数据和300段视频）实现了定量和定性上的业界最优，并在多样人物、复杂交互及高难度多角色场景下都表现出很强的泛化能力。

Conclusion: 所提方法有效扩展了人物动画的应用范畴，能强健处理多角色外观和动作变化，显著推动了多角色图像动画的前沿进展。

Abstract: Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

</details>


### [23] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: 提出了一种新型的分层时空视觉-语言-动作（HiST-VLA）模型，用于可靠的自动驾驶轨迹生成，显著提升了3D空间推理和时间一致性，实验证明达到业界最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有VLA（视觉-语言-动作）模型在自动驾驶等关键领域的应用受限，主要是由于其在数值推理、三维空间理解和上下文敏感性方面存在不足。作者希望突破这些局限，实现更可靠的轨迹生成。

Method: 提出HiST-VLA模型，结合几何感知、细粒度驾驶指令和状态历史提示，提升空间和时间推理能力。通过动态token稀疏化减少计算冗余，并使用分层Transformer规划器将粗略路径细化为精细轨迹，引入动态潜在正则实现语言指令与轨迹的严格匹配。

Result: 在NAVSIM v2基准测试集上，HiST-VLA在Navtest任务上取得了88.6的EPDMS，在更难的伪闭环Navhard任务上取得了50.9，均达到了业界最优水平。

Conclusion: HiST-VLA显著提升了VLA模型在轨迹生成上的可靠性和性能，为自动驾驶等领域的多模态智能系统发展提供了新方案。

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [24] [Zwitscherkasten -- DIY Audiovisual bird monitoring](https://arxiv.org/abs/2602.13330)
*Dominik Blum,Elias Häring,Fabian Jirges,Martin Schäffer,David Schick,Florian Schulenberg,Torsten Schön*

Main category: cs.CV

TL;DR: 提出了一种名为Zwitscherkasten的多模态鸟类监测系统，利用视觉和音频数据，在低功耗硬件上实现实时识别。


<details>
  <summary>Details</summary>
Motivation: 目前可持续且大规模的鸟类监测需求日益增加，传统方法成本高、实时性差，亟需技术创新以实现非侵入式且普适的生物多样性监测。

Method: 将音频与视觉的深度学习识别模型部署于资源有限的边缘设备上，通过声学活动检测减少能耗，并采用细粒度视觉识别进行鸟种鉴定。

Result: 证明了即便在嵌入式平台上也能准确识别鸟类，为实际大规模鸟类及生物多样性监测提供了可行方案。

Conclusion: 此系统为可扩展的生物多样性监测和公民科学项目提供了有效的技术支撑，具备实时、低功耗、易部署等优势。

Abstract: This paper presents Zwitscherkasten, a DiY, multimodal system for bird species monitoring using audio and visual data on edge devices. Deep learning models for bioacoustic and image-based classification are deployed on resource-constrained hardware, enabling real-time, non-invasive monitoring. An acoustic activity detector reduces energy consumption, while visual recognition is performed using fine-grained detection and classification pipelines. Results show that accurate bird species identification is feasible on embedded platforms, supporting scalable biodiversity monitoring and citizen science applications.

</details>


### [25] [MedScope: Incentivizing "Think with Videos" for Clinical Reasoning via Coarse-to-Fine Tool Calling](https://arxiv.org/abs/2602.13332)
*Wenjie Li,Yujie Zhang,Haoran Sun,Xingqi He,Hongcheng Gao,Chenglong Ma,Ming Hu,Guankun Wang,Shiyi Yao,Renhao Yang,Hongliang Ren,Lei Wang,Junjun He,Yankai Jiang*

Main category: cs.CV

TL;DR: 本文提出了MedScope，这是一种能够在长时临床视频中结合工具进行推理的新型多模态大模型，通过细致的循证过程提升了临床视频分析的准确性与可信度，并引入了用于精细化标注与评测的新数据集ClinVideoSuite。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在分析长时临床视频时，通常仅采用被动采样或弱实证方式，限制了其在时间上精确定位、验证和举证的能力，因此需要一种能够主动、分阶段查找和验证证据的方法，以更好辅助临床决策和手术相关应用。

Method: 提出MedScope模型，该模型通过分阶段（粗到细）地主动查找证据，在推理过程中动态调用工具，并能针对检索到的观察结果进行验证。同时，开发了高质量、细粒度的临床视频数据套件ClinVideoSuite，并使用Grounding-Aware Group Relative Policy Optimization（GA-GRPO）算法，基于对证据归因的奖励机制直接优化模型中工具的使用和推理。

Result: MedScope模型在完整及细粒度视频理解的基准测试中，均达到了领域内和非领域（迁移）任务上的最新最好成绩，优于以往方法。

Conclusion: 本研究展示了将主动工具整合推理与临床视频处理结合的前景，为实现具有“视频思考”能力的医学AI代理提供了新的方向，相关模型、代码和数据即将开源。

Abstract: Long-form clinical videos are central to visual evidence-based decision-making, with growing importance for applications such as surgical robotics and related settings. However, current multimodal large language models typically process videos with passive sampling or weakly grounded inspection, which limits their ability to iteratively locate, verify, and justify predictions with temporally targeted evidence. To close this gap, we propose MedScope, a tool-using clinical video reasoning model that performs coarse-to-fine evidence seeking over long-form procedures. By interleaving intermediate reasoning with targeted tool calls and verification on retrieved observations, MedScope produces more accurate and trustworthy predictions that are explicitly grounded in temporally localized visual evidence. To address the lack of high-fidelity supervision, we build ClinVideoSuite, an evidence-centric, fine-grained clinical video suite. We then optimize MedScope with Grounding-Aware Group Relative Policy Optimization (GA-GRPO), which directly reinforces tool use with grounding-aligned rewards and evidence-weighted advantages. On full and fine-grained video understanding benchmarks, MedScope achieves state-of-the-art performance in both in-domain and out-of-domain evaluations. Our approach illuminates a path toward medical AI agents that can genuinely "think with videos" through tool-integrated reasoning. We will release our code, models, and data.

</details>


### [26] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: 提出了一种协同推理框架，将轻量ViT部署在边缘设备，并结合近端专家ViT，加速推理并提升准确率。通过动态路由选择专家模型，并设计渐进式专家训练方法，显著提升分类准确率、延迟和能耗表现。


<details>
  <summary>Details</summary>
Motivation: 当前将视觉Transformer（ViT）部署到边缘设备会遇到计算复杂度过高问题。而完全离线至云又带来高延迟等问题，因此亟需在边缘-近端设备间协同推理的新方法，实现低延迟、低能耗、高准确性的平衡。

Method: 提出在边缘设备上使用轻量ViT，总体进行初步推理。对于低置信度的样本，根据top-k预测结果动态路由到近端设备的中等大小专家ViT进行进一步细致分类，并通过递进式专家专精训练方法提升专家在特定子集上的性能。

Result: 在CIFAR-100数据集和真实边缘-近端测试平台上，专家模型在目标子集上的分类准确率提高4.12%，总体准确率较静态专家提升2.76%。延迟相对完全边缘推理下降45%，能耗较仅近端推理下降46%。

Conclusion: 该协同推理框架与动态路由+专家专精训练方法，可以兼顾推理速度、能耗和准确率，适合在实际边缘计算场景下部署ViT模型，可大幅提升边缘智能应用的性能。

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [27] [Meningioma Analysis and Diagnosis using Limited Labeled Samples](https://arxiv.org/abs/2602.13335)
*Jiamiao Lu,Wei Wu,Ke Gao,Ping Mao,Weichuan Zhang,Tuo Wang,Lingkun Ma,Jiapan Guo,Zanyi Wu,Yuqing Hu,Changming Sun*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间和频域特征自适应加权融合的架构，有效提升了少样本脑膜瘤MRI影像的分级分类表现。


<details>
  <summary>Details</summary>
Motivation: 脑膜瘤的生物学行为和治疗反应依赖于分级，准确诊断对治疗和预后至关重要；传统方法在少样本及不同图像中表现受限，需提出更精准、鲁棒的新方法。

Method: 提出了一种结合空间域与离散小波变换（DWT）获得的多频带特征的自适应权重特征融合网络，可根据不同影像自动调整空间/频域特征的重要性；并引入了新的脑膜瘤MRI数据集进行了验证。

Result: 在三个脑膜瘤影像数据集上，所提方法在分级分类准确率上均超越了其他最先进模型，验证了融合与自适应权重机制的有效性。

Conclusion: 自适应空间-频域特征融合的方法能大幅提升少样本条件下脑膜瘤影像的分级诊断，为临床诊断和预后评估提供更可靠的工具，具有广阔的实际应用前景。

Abstract: The biological behavior and treatment response of meningiomas depend on their grade, making an accurate diagnosis essential for treatment planning and prognosis assessment. We observed that the weighted fusion of spatial-frequency domain features significantly influences meningioma classification performance. Notably, the contribution of specific frequency bands obtained by discrete wavelet transform varies considerably across different images. A feature fusion architecture with adaptive weights of different frequency band information and spatial domain information is proposed for few-shot meningioma learning. To verify the effectiveness of the proposed method, a new MRI dataset of meningiomas is introduced. The experimental results demonstrate the superiority of the proposed method compared with existing state-of-the-art methods in three datasets. The code will be available at: https://github.com/ICL-SUST/AMSF-Net

</details>


### [28] [An Integrated Causal Inference Framework for Traffic Safety Modeling with Semantic Street-View Visual Features](https://arxiv.org/abs/2602.13339)
*Lishan Sun,Yujia Cheng,Pengfei Cui,Lei Han,Mohamed Abdel-Aty,Yunhan Zheng,Xingchen Zhang*

Main category: cs.CV

TL;DR: 论文利用谷歌街景图像的视觉环境特征，提出双机器学习框架因果量化绿化程度对区域交通事故的保护作用，结果证实绿化显著降低交通事故发生率，尤其在城市核心区，但对弱势交通参与者保护有限。


<details>
  <summary>Details</summary>
Motivation: 当前交通安全建模多依赖静态统计与基础设施数据，忽略驾驶者的视觉感知影响，也缺乏对视觉环境要素的严谨因果分析，影响了政策评估的准确性。作者希望解决这些不足，通过提取视觉特征并引入因果推断，明确它们对交通安全的实际作用。

Method: 基于谷歌街景图片的语义分割提取视觉环境特征，同时构建双机器学习框架，对特征与交通事故数据进行因果效应量化。采用SHAP值分析混杂因素的非线性影响机制，并运用因果森林计算条件平均处理效应。数据来源为美国迈阿密都会区交通事故数据和22万条街景图像。

Result: 结果显示，街道绿化比例对交通事故有显著且稳健的负向因果效应（平均处理效应=-6.38, p=0.005）；这一保护效应在高密度及社会弱势城区更显著。绿化能有效减少交叉口和追尾事故，但对行人、骑行者等弱势参与者的保护作用有限。

Conclusion: 本研究提供了视觉环境绿化作为交通安全干预的直接因果证据，建议优先改善高风险区域的视觉环境，同时指出需针对弱势交通参与者探索更有效的设计优化方案。

Abstract: Macroscopic traffic safety modeling aims to identify critical risk factors for regional crashes, thereby informing targeted policy interventions for safety improvement. However, current approaches rely heavily on static sociodemographic and infrastructure metrics, frequently overlooking the impacts from drivers' visual perception of driving environment. Although visual environment features have been found to impact driving and traffic crashes, existing evidence remains largely observational, failing to establish the robust causality for traffic policy evaluation under complex spatial environment. To fill these gaps, we applied semantic segmentation on Google Street View imageries to extract visual environmental features and proposed a Double Machine Learning framework to quantify their causal effects on regional crashes. Meanwhile, we utilized SHAP values to characterize the nonlinear influence mechanisms of confounding variables in the models and applied causal forests to estimate conditional average treatment effects. Leveraging crash records from the Miami metropolitan area, Florida, and 220,000 street view images, evidence shows that greenery proportion exerts a significant and robust negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). This protective effect exhibits spatial heterogeneity, being most pronounced in densely populated and socially vulnerable urban cores. While greenery significantly mitigates angle and rear-end crashes, its protective benefit for vulnerable road users (VRUs) remains limited. Our findings provide causal evidence for greening as a potential safety intervention, prioritizing hazardous visual environments while highlighting the need for distinct design optimizations to protect VRUs.

</details>


### [29] [FireRed-Image-Edit-1.0 Techinical Report](https://arxiv.org/abs/2602.13344)
*Super Intelligence Team,Changhao Qiao,Chao Hui,Chen Li,Cunzheng Wang,Dejia Song,Jiale Zhang,Jing Li,Qiang Xiang,Runqi Wang,Shuang Sun,Wei Zhu,Xu Tang,Yao Hu,Yibo Chen,Yuhao Huang,Yuxuan Duan,Zhiyi Chen,Ziyuan Guo*

Main category: cs.CV

TL;DR: FireRed-Image-Edit是一种基于指令的图像编辑扩散Transformer，借助大规模高质量训练数据和多阶段训练流程，在多个公开和自建基准上实现了业界领先的性能。


<details>
  <summary>Details</summary>
Motivation: 当前指令驱动的图像编辑方法在数据质量、训练流程和评测标准方面存在不足，难以获得高性能和通用的编辑能力。作者希望通过系统优化数据、训练与评估，推动该领域进步。

Method: 作者构建了包含9亿文本转图像和7亿图像编辑对的大型数据集，经过严格清洗、自动标注和分层筛选，保留了1亿高质量样本。提出多阶段训练流程（预训练、监督微调、强化学习），并引入如多条件采样器、动态指令对齐、非对称梯度优化、文本编辑专用的奖励机制和一致性损失等创新方法。此外，建立了覆盖丰富编辑任务的新基准REDEdit-Bench。

Result: 在自建REDEdit-Bench和公开ImgEdit、GEdit基准上，FireRed-Image-Edit在多个任务上的表现均达到或超过了现有开源及商业系统，证明了其有效性和泛化能力。

Conclusion: 系统性的数据、训练与评估优化不仅极大提升了指令式图像编辑方法的性能，还通过公开资源推动了领域发展。

Abstract: We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.

</details>


### [30] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动化仓库的世界模型FOREST，能够根据当前观察和预期放置动作预测货箱内物品的新布局。


<details>
  <summary>Details</summary>
Motivation: 自动仓库需要在机器人执行存储操作前预测物品放置后货箱的状态，以提升规划和操作效率，减少错误和碰撞风险。

Method: 作者提出了FOREST模型，它将货箱状态表示为与物品对齐的实例掩码，并采用潜变量扩散Transformer模型，根据观察到的上下文和计划的放置动作预测货箱在操作后的具体配置。

Result: 实验结果表明，FOREST在预测放置后货箱几何结构方面，比传统启发式方法有显著提升。在下游任务的测试中，使用FOREST预测的结果代替真实数据，系统在装载质量评估和多次放置推理上只带来了轻微性能损失。

Conclusion: FOREST能够为自动仓库规划提供有价值的前瞻性信息，有助于提升仓库的智能化和操作可靠性。

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [31] [From Prompt to Production:Automating Brand-Safe Marketing Imagery with Text-to-Image Models](https://arxiv.org/abs/2602.13349)
*Parmida Atighehchian,Henry Wang,Andrei Kapustin,Boris Lerner,Tiancheng Jiang,Taylor Jensen,Negin Sokhandan*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本生成图片模型部署流水线，专注于生成商业产品的营销图片，兼顾自动化规模化和人工质量监督。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图片生成模型发展迅速，如何在生产环境中大规模部署，同时兼顾效率和图片质量，仍存在挑战。需要平衡自动化带来的规模处理能力和人工反馈保障的质量。

Method: 提出了一套全自动可扩展的商业产品营销图片生成流水线。该系统集成了自动化处理和必要的人为检查，利用DINOV2等技术保持图片质量，同时实现创意多样性以符合营销需求。

Result: 系统在营销对象保真度上提升了30.77%（基于DINOV2评测），在人类偏好评测上提升了52%。

Conclusion: 所提方案兼顾效率、可扩展性与人类监督，显著改善了营销图片的保真度与用户偏好，适合规模化部署。

Abstract: Text-to-image models have made significant strides, producing impressive results in generating images from textual descriptions. However, creating a scalable pipeline for deploying these models in production remains a challenge. Achieving the right balance between automation and human feedback is critical to maintain both scale and quality. While automation can handle large volumes, human oversight is still an essential component to ensure that the generated images meet the desired standards and are aligned with the creative vision. This paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The proposed system maintains the quality and fidelity of images, while also introducing sufficient creative variation to adhere to marketing guidelines. By streamlining this process, we ensure a seamless blend of efficiency and human oversight, achieving a $30.77\%$ increase in marketing object fidelity using DINOV2 and a $52.00\%$ increase in human preference over the generated outcome.

</details>


### [32] [Detecting Brick Kiln Infrastructure at Scale: Graph, Foundation, and Remote Sensing Models for Satellite Imagery Data](https://arxiv.org/abs/2602.13350)
*Usman Nazir,Xidong Chen,Hafiz Muhammad Abubakar,Hadia Abu Bakar,Raahim Arbaz,Fezan Rasool,Bin Chen,Sara Khalid*

Main category: cs.CV

TL;DR: 该论文利用高分辨率卫星影像和图神经网络等方法大规模检测南亚地区砖窑，对现有方法进行了系统评测，提出了更高效的监测方案。


<details>
  <summary>Details</summary>
Motivation: 南亚地区砖窑是空气污染和强迫劳动的主要来源，但由于实地数据稀缺且过时，亟需可扩展的自动化监测手段以辅助环境和社会治理。

Method: 作者构建了涵盖南亚和中亚五个地区、超过130万影像块的高分辨率卫星影像数据集；提出了一种区域自适应的图神经网络模型ClimateGraph，用于提取和利用砖窑空间结构特征。并将该方法与常规图学习、遥感、基础模型等方法进行了对比和系统评测。

Result: 实验结果显示，图神经网络、基础卫星影像分析模型及遥感方法各自具有独特优势，多策略结合能显著提升大规模砖窑识别与监测的准确性和实用性。

Conclusion: 论文为利用卫星影像开展大规模砖窑监测提供了切实可行的技术路线和模型评估基线，推动了相关环境保护和社会治理应用的发展。

Abstract: Brick kilns are a major source of air pollution and forced labor in South Asia, yet large-scale monitoring remains limited by sparse and outdated ground data. We study brick kiln detection at scale using high-resolution satellite imagery and curate a multi city zoom-20 (0.149 meters per pixel) resolution dataset comprising over 1.3 million image tiles across five regions in South and Central Asia. We propose ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts, and evaluate it against established graph learning baselines. In parallel, we assess a remote sensing based detection pipeline and benchmark it against recent foundation models for satellite imagery. Our results highlight complementary strengths across graph, foundation, and remote sensing approaches, providing practical guidance for scalable brick kiln monitoring from satellite imagery.

</details>


### [33] [Using Deep Learning to Generate Semantically Correct Hindi Captions](https://arxiv.org/abs/2602.13352)
*Wasim Akram Khan,Anil Kumar Vuppala*

Main category: cs.CV

TL;DR: 本文基于深度学习技术自动生成图像的印地语描述，利用Flickr8k数据集、CNN特征提取和注意力机制，评估了不同模型组合的效果，最终以双向LSTM+VGG16+注意力机制达到最佳表现。


<details>
  <summary>Details</summary>
Motivation: 目前自动图像描述领域研究多集中于英文，其他主流语言（如印地语）相关研究较少。印地语全球用户众多，对其开展图像自动描述研究有重要应用价值。

Method: 使用Flickr8k图像数据集，并通过Google翻译获得印地语图像描述。采用预训练CNN（VGG16、ResNet50、Inception V3）提取图像特征，结合单向与双向LSTM文本编码与注意力机制；最后用BLEU分数评估模型表现。

Result: 基于注意力机制的双向LSTM与VGG16组合，在BLEU-1和BLEU-4上分别取得0.59与0.19的最佳分数，优于其他实验设置。

Conclusion: 该方法能够有效生成语义相关且准确的印地语图像描述，为多语言图像自动描述研究提供了可行思路和方法，具有推广价值，也为后续研究指明了方向。

Abstract: Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.

</details>


### [34] [AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers](https://arxiv.org/abs/2602.13357)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AdaCorrection的自适应缓存校正方法，使Diffusion Transformer在不牺牲生成质量的情况下，实现更高效的推理加速。


<details>
  <summary>Details</summary>
Motivation: 虽然Diffusion Transformer在高保真图像和视频生成领域表现优异，但由于其迭代去噪的结构导致推理速度慢。已有的加速方法存在缓存失效和质量下降问题，因此亟需既能保持高生成质量又能高效复用缓存的改进方法。

Method: 论文提出AdaCorrection自适应缓存校正框架，在每个时间步通过轻量级的时空信号估计缓存有效性，自适应地融合新旧激活特征，整个过程无需额外的监督或重新训练，即时计算校正校准。

Result: AdaCorrection能在几乎不降低生成质量（原始FID）的前提下，实现适度的推理加速。实验结果表明，该方法在多项图像和视频扩散生成基准上，能稳定提升生成性能。

Conclusion: AdaCorrection有效兼顾了高效加速和高质量生成，为扩散Transformer类生成模型提供了实用的推理优化思路。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.

</details>


### [35] [The Diffusion Duet: Harmonizing Dual Channels with Wavelet Suppression for Image Separation](https://arxiv.org/abs/2602.13361)
*Jingwei Li,Wei Pu*

Main category: cs.CV

TL;DR: 本文提出了一种创新性的Dual-Channel Diffusion Separation Model (DCDSM)，利用扩散模型解决盲图像分离问题，在雨雪和复杂混合图像的分离和复原上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统盲图像分离方法因依赖独立性假设或CNN/GAN架构，难以表征真实场景下复杂的特征分布，易出现估计偏差、纹理失真、伪影残留，特别是在强噪声或非线性混合情况下效果有限。

Method: 提出高效的双通道扩散分离模型DCDSM，通过扩散模型的生成能力学习源图像特征分布，以反向去噪过程结合设计的Wavelet Suppression Module（WSM）进行噪声耦合抑制和细节增强，实现交互式的双支路特征分离。

Result: 在合成雨/雪与复杂混合数据集上的大量实验表明，DCDSM在雨雪去除任务上分别达到了35.0023 dB/0.9549 (雨)和29.8108 dB/0.9243 (雪) 的PSNR/SSIM值，显著超越Histoformer和LDRCNet，并在复杂混合分离任务下PSNR和SSIM分别提升4.1249 dB和0.0926。

Conclusion: DCDSM在盲图像分离去雨雪残留、细节保护方面达到了当前最优水平，主客观评价均优于现有方法，展示出扩散模型在复杂场景图像分离中的强大潜力。

Abstract: Blind image separation (BIS) refers to the inverse problem of simultaneously estimating and restoring multiple independent source images from a single observation image under conditions of unknown mixing mode and without prior knowledge of the source images. Traditional methods relying on statistical independence assumptions or CNN/GAN variants struggle to characterize complex feature distributions in real scenes, leading to estimation bias, texture distortion, and artifact residue under strong noise and nonlinear mixing. This paper innovatively introduces diffusion models into dual-channel BIS, proposing an efficient Dual-Channel Diffusion Separation Model (DCDSM). DCDSM leverages diffusion models' powerful generative capability to learn source image feature distributions and reconstruct feature structures effectively. A novel Wavelet Suppression Module (WSM) is designed within the dual-branch reverse denoising process, forming an interactive separation network that enhances detail separation by exploiting the mutual coupling noise characteristic between source images. Extensive experiments on synthetic datasets containing rain/snow and complex mixtures demonstrate that DCDSM achieves state-of-the-art performance: 1) In image restoration tasks, it obtains PSNR/SSIM values of 35.0023 dB/0.9549 and 29.8108 dB/0.9243 for rain and snow removal respectively, outperforming Histoformer and LDRCNet by 1.2570 dB/0.9272 dB (PSNR) and 0.0262/0.0289 (SSIM) on average; 2) For complex mixture separation, the restored dual-source images achieve average PSNR and SSIM of 25.0049 dB and 0.7997, surpassing comparative methods by 4.1249 dB and 0.0926. Both subjective and objective evaluations confirm DCDSM's superiority in addressing rain/snow residue removal and detail preservation challenges.

</details>


### [36] [An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation](https://arxiv.org/abs/2602.13376)
*Giang Son Nguyen,Zi Pong Lim,Sarthak Ketanbhai Modi,Yon Shin Teo,Wenya Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无参考文档图像转代码生成质量评估框架，能够在没有真实标签的情况下评估模型输出。其核心在于只用输入图片和生成代码，自动计算覆盖率和准确性评分，便于工业生产环境持续监控。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的文档处理系统需将流程图图片转为结构化代码，但大多数实际输入没有标准答案，导致难以客观评估输出质量。现有评测方法依赖人工标注，成本高，难以部署于生产。

Method: 提出了无参考的评测框架：
1. 使用OCR提取图片文本，通过Recall_{OCR}指标估算内容覆盖。
2. 通过视觉蕴含(Visual Entailment, VE)检测生成结果中的虚构元素，用Precision_{VE}评估准确性。
3. 两者的调和平均作为F1_{OCR-VE}综合质量分。
该方法完全借助自动化工具，无需人工标注代码。

Result: 在FlowVQA数据集上进行实证，自动分数与基于真实代码的指标高度相关（Pearson’s r: Recall 0.97、Precision 0.91、F1 0.94），说明该方法能有效反映实际模型质量。

Conclusion: 该框架为生产环境下的图像转代码质量监控提供了一种实用、可靠且无需参考答案的新思路，有望广泛应用于实际的文档流程自动化场景。

Abstract: Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\text{Recall}{\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\text{Precision}{\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\text{F1}{\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.

</details>


### [37] [LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery](https://arxiv.org/abs/2602.13378)
*Sohail Ali Farooqui,Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: 本文提出了一种针对无人机(UAV)小目标检测的改进YOLO模型LAF-YOLOv10，在实际无人机平台上实现了更优的检测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 无人机影像中的目标常常尺寸极小、易与背景混杂且存在大幅遮挡，同时设备算力有限，现有目标检测器难以兼顾精度与效率，因此亟需专门优化算法。

Method: 基于YOLOv10n，集成了四项非重叠改进模块：1）PC-C2f通道局部卷积，减少冗余计算；2）AG-FPN注意力引导特征金字塔，改善多尺度特征融合与插值；3）在高分辨率引入辅助P2检测头并移除P5检测头，增强小目标定位能力；4）引入Wise-IoU v3损失，抑制标签噪声带来的梯度干扰。

Result: 在VisDrone-DET2019数据集上达到35.1±0.3% mAP@0.5（参数量2.3M），相较原始YOLOv10n精度提升3.3个百分点；在UAVDT数据集获得35.8±0.4% mAP@0.5。在NVIDIA Jetson Orin Nano上实测，FP16下推理速度达24.3 FPS，适用于嵌入式无人机部署。

Conclusion: 通过多组件联合优化，LAF-YOLOv10显著提升了无人机小目标检测的精度与推理速度，为实际嵌入式无人机监控应用提供了更优解决方案。

Abstract: Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.

</details>


### [38] [Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning](https://arxiv.org/abs/2602.13430)
*Ha-Hieu Pham,Hai-Dang Nguyen,Thanh-Huy Nguyen,Min Xu,Ulas Bagci,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 本文针对胸部X光（CXR）影像分类中因长尾多标签分布和未知类别缺失标注导致的监督不完善问题，提出了长尾多标签学习与零样本识别新方法，并在权威榜单上取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 临床实际中，CXR影像多疾病标签严重长尾分布且罕见或未见疾病常无标注，导致分类模型性能受限。为应对这些挑战，CXR-LT 2026比赛专门提出36类疾病标签，包含常见类和零样本外部分布类别，促使研究更好地适应真实环境。

Method: 针对Task 1（长尾多标签分类），采取考虑类别不均的多标签学习策略，以提升低频类别检测能力。针对Task 2（零样本OOD识别），设计了一种无需使用零样本类别标签或示例，即可对未见类别产生分类分数的预测方法。两任务均基于PadChest数据集扩展建立。

Result: 本方法在macro-averaged mean Average Precision（mAP）指标下，两个任务均表现优异，并在开发阶段公共榜单上取得第一名。

Conclusion: 提出的针对性方法能够有效应对CXR影像中的长尾多标签分布和零样本识别难题，对提高实际临床疾病筛查的智能化水平具有重要意义。

Abstract: Chest X-ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.

</details>


### [39] [Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones](https://arxiv.org/abs/2602.13440)
*Sebastian-Ion Nae,Mihai-Eugen Barbu,Sebastian Mocanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本论文提出了一个专为无人机场景设计的室内视频数据集，并评估了三种基于回放的增量学习方法在有限内存条件下的表现，以解决无人机实时学习新目标类别并防止遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在室内环境中需实时识别和学习新物体类别，但现有数据集大多针对室外，缺乏具备时间连续性的室内视频，且无人机平台受限于资源，对高效的增量学习方法需求迫切。

Method:  authors 构建了一个包含 14,400 帧的室内无人机和地面车辆视频数据集，并提出半自动化标注流程以获得高一致性标签。以 YOLOv11-nano 作为检测器，在有限（5-10%）回放内存条件下，分别对 Experience Replay (ER)、Maximally Interfered Retrieval (MIR) 和 Forgetting-Aware Replay (FAR) 三种增量学习策略进行基准测试。

Result: 在仅 5% 回放空间下，FAR 方法取得了 82.96% 的平均准确率（mAP50-95），优于其他策略。此外，通过 Grad-CAM 可视化分析发现，多类别场景中模型注意力会发生转移，影响对无人机的定位质量。

Conclusion: 新构建的数据集填补了室内无人机视频及增量学习领域的空白，实验证明基于回放的增量学习策略在边缘（资源受限）无人机系统上可行，尤其是 FAR 方法在有限内存下效果突出。

Abstract: Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl

</details>


### [40] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: 本文提出一种混合架构，在可穿戴设备上高效实现了文本视觉问答（Text VQA），兼顾了高文本识别精度与低能耗。


<details>
  <summary>Details</summary>
Motivation: 大多数视频大语言模型在文本识别和基于文本的视觉问答任务中依赖高分辨率视频，但在可穿戴设备上高分辨率视频流会导致耗电量大、发热严重。此外，现有模型在多帧实时处理时难以维持时序一致性。本文目标是解决可穿戴设备部署 Text VQA 时的能效与性能矛盾。

Method: 作者提出利用文本识别与视觉理解在分辨率需求上的不对称性：仅对需要高精细度的文本进行本地高分辨率OCR处理，同时通过低分辨率视频传输场景上下文信息，实现混合处理，从而减少整体能耗。

Result: 在五类Text VQA任务的基准测试中，该系统在只需要标准全分辨率流0.49倍功耗的情况下，取得了72%的准确率，有效支撑了在资源受限的可穿戴设备上的持久问答。

Conclusion: 提出的混合架构在保持高文本理解质量的同时，极大降低了能耗，为可穿戴设备上的文本视觉问答应用提供了可行方案。

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [41] [Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening](https://arxiv.org/abs/2602.13507)
*Md Saiful Islam,Ekram Hossain,Abdelrahman Abdelkader,Tariq Adnan,Fazla Rabbi Mashrur,Sooyong Park,Praveen Kumar,Qasim Sudais,Natalia Chunga,Nami Shah,Jan Freyberg,Christopher Kanan,Ruth Schneider,Ehsan Hoque*

Main category: cs.CV

TL;DR: 本文系统性评估了七种最新视频基础模型（VFMs）在基于远程视频的帕金森病筛查中的有效性，发现不同模型对不同临床任务的表现差异显著，提出为未来模型选择和组合提供依据。


<details>
  <summary>Details</summary>
Motivation: 帕金森病筛查亟需可扩展的评估手段。传统依赖手工特征，受限于任务定制与泛化性，难以适应日益增长的远程医疗需求。近期VFMs的进步为无需任务特定定制的表征学习提供了可能，但其在多样临床任务中的有效性尚未被系统性比较。

Method: 作者构建了包含1888名受试者（727名PD患者）、16个标准化任务、总计32847个视频的大型数据集，系统性评估了七种SOTA视频基础模型（如VideoPrism、V-JEPA、ViViT等）。采用冻结模型表征层并仅训练线性分类头，比较了模型在不同任务中的表现。通过AUC、准确率、特异性与敏感性等指标量化评估其筛查有效性。

Result: 不同VFM架构在多任务下有明显优劣互补：VideoPrism对面部表情、无音频语音运动学最敏感，V-JEPA在上肢运动任务中表现最佳，TimeSformer擅长节律性任务（如手指敲击）。模型整体AUC在76.4-85.3%，准确率为71.5-80.6%。最高特异性可达90.3%，但敏感性相对较低（43.2-57.3%），提示单独模型难以做到高灵敏性检测。

Conclusion: 该工作为远程视频基础模型在帕金森病筛查中的性能建立了系统性基准，强调模型和任务选择依赖性，为多模型多任务整合提供了参考路径。未来需注重敏感性的提升，可通过任务整合和多模态协同优化。提供开源代码与数据以促进后续研究。

Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\_video\_benchmarking-A2C5

</details>


### [42] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 该论文针对扩散模型中的稀疏注意力机制，提出了SpargeAttention2方法，实现了高稀疏性和高效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的训练自由（training-free）稀疏注意力机制虽然可以加速扩散模型，但在高稀疏性下易导致生成质量降低。近期研究发现可训练的稀疏注意力有望进一步提高稀疏性，同时保证生成效果。本文旨在系统分析现有方法的局限，并提出更优的可训练稀疏注意力方案。

Method: 本文提出了SpargeAttention2，包括：1）结合Top-k与Top-p的混合掩码规则，实现更鲁棒的高稀疏性掩码；2）高效的可训练稀疏注意力实现方法；3）借助蒸馏思想设计的微调目标，在使用稀疏注意力微调模型时更好地保留生成质量。

Result: 在视频扩散模型上的实验显示，SpargeAttention2实现了95%的注意力稀疏度，以及16.2倍的注意力加速比，同时生成质量没有下降，且在多个指标上一致优于以往的稀疏注意力方法。

Conclusion: SpargeAttention2作为一种可训练的稀疏注意力方法，能够在大幅提高稀疏性和推理速度的同时，仍然保持高质量的生成效果。分析和实验证明其优于现有方法，对扩散模型的高效推理具有实际意义。

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [43] [Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting](https://arxiv.org/abs/2602.13549)
*Tae-Kyeong Kim,Xingxin Chen,Guile Wu,Chengjie Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种将物理基础渲染与3D高斯溅射方法相结合的新方法，用于提升自动驾驶夜间场景的三维重建效果。实验表明新方法在多个真实数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和3DGS的方法在自动驾驶场景重建上已取得较好效果，但主要聚焦于正常光照条件，对于照明复杂的夜间场景效果较差，亟需针对夜间复杂光照进行优化。

Method: 方法将物理基础渲染（Physically Based Rendering, PBR）与3DGS方法结合，将PBR纳入高斯场景表示中，并联合优化基于BRDF的材质属性。通过全局光照模块显式建模漫反射分量，并用各向异性球形高斯建模镜面反射分量。

Result: 在两个真实自动驾驶夜间数据集（nuScenes和Waymo）中，所提出方法在多种夜间场景下实现了更高质量的三维重建，且定量和定性结果均优于现有主流方法。

Conclusion: 将物理基础渲染融入3DGS极大提升了夜间自动驾驶场景的建模质量，同时还能保持实时渲染效率。该方法为夜间复杂环境下的自动驾驶场景建模提供了更有效的技术路径。

Abstract: This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing methods. To address this problem, this work presents a novel approach that integrates physically based rendering into 3DGS to enhance nighttime scene reconstruction for autonomous driving. Specifically, our approach integrates physically based rendering into composite scene Gaussian representations and jointly optimizes Bidirectional Reflectance Distribution Function (BRDF) based material properties. We explicitly model diffuse components through a global illumination module and specular components by anisotropic spherical Gaussians. As a result, our approach improves reconstruction quality for outdoor nighttime driving scenes, while maintaining real-time rendering. Extensive experiments across diverse nighttime scenarios on two real-world autonomous driving datasets, including nuScenes and Waymo, demonstrate that our approach outperforms the state-of-the-art methods both quantitatively and qualitatively.

</details>


### [44] [Privacy-Concealing Cooperative Perception for BEV Scene Segmentation](https://arxiv.org/abs/2602.13555)
*Song Wang,Lingling Li,Marcus Santos,Guanghui Wang*

Main category: cs.CV

TL;DR: 本文针对自动驾驶车辆合作感知中的隐私泄露问题，提出了一种能够在保持感知性能的同时保护隐私的新方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶合作感知系统中，通过车辆间通信共享感知信息可以提升感知能力，但同时也会带来敏感视觉内容泄露的隐患。因此需要兼顾信息共享与隐私保护。

Method: 作者提出了Privacy-Concealing Cooperation（PCC）框架，针对鸟瞰图（BEV）语义分割任务，设计了一个隐藏网络防止图像重建网络从共享特征中还原原始图片。整个系统采用对抗学习机制，隐藏网络与重建网络博弈，并与感知网络端到端联合优化，保证分割准确性的同时增强隐私保护。

Result: 实验结果显示，该框架能显著降低图像重建质量，减少隐私泄露风险，同时对分割性能影响很小。

Conclusion: PCC方法在提升自动驾驶车辆合作感知系统隐私保护能力的同时基本不损失感知性能，为真实车辆应用提供了更安全的解决方案，相关代码将在发表时公开。

Abstract: Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.

</details>


### [45] [Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation](https://arxiv.org/abs/2602.13585)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diff-Aid的轻量级推理方法，通过自适应调整文本和图像特征的交互，提升扩散模型在文本-图像生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来文本到图像（T2I）扩散模型进步显著，但其在忠实遵循复杂文本描述时仍有难度，主要因为文本与视觉特征间的交互不足。以往提升方法灵活性不佳，且忽略了不同网络模块和时序阶段间交互的动态性。

Method: 作者提出Diff-Aid，在推理阶段针对不同transformer模块和去噪步，对每个token的文本和图像交互进行自适应调整，属于即插即用型，能够轻松集成到主流模型与应用，如风格迁移、可控生成和零样本编辑等。

Result: 在主流基线（如SD 3.5和FLUX）上实验，Diff-Aid能够在多个指标下显著提升模型对提示词的遵循度、生成画质和用户偏好。同时，该方法生成的调制模式具备可解释性，展示了去噪过程中文本token及模型不同部分的贡献。

Conclusion: Diff-Aid不仅提升了T2I扩散模型的生成质量与文本一致性，还具备良好的灵活性和可解释性，为多种下游任务带来进一步提升；相关代码和模型即将开源。

Abstract: Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.

</details>


### [46] [Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks](https://arxiv.org/abs/2602.13588)
*Guanfeng Tang,Hongbo Zhao,Ziwei Long,Jiayao Li,Bohong Xiao,Wei Ye,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出了TwInS（Two Interactive Streams）框架，能够同时进行场景解析和几何视觉任务，结合了来自两条交互式流的信息，无需大量人工标注，实现了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 受到人类视觉系统（同时处理上下文和空间信息的两条交互流）的启发，作者希望设计一个通用、交互式的视觉系统，实现场景解析与几何视觉的联合学习，并减少人工标注依赖。

Method: TwInS采用两条交互流架构，一条进行场景解析，另一条进行几何视觉，这两条流通过多层特征信息相互传递和融合。通过一种新颖的跨任务适配器，将几何特征精准地映射到场景解析空间，实现异构特征融合。系统引入半监督训练策略，利用大规模多视图数据自我进化，无需地面真实对应关系。

Result: 在三个公开数据集上的实验表明，TwInS的核心模块有效，整体性能超过了现有主流方法。

Conclusion: TwInS证明了利用仿生联合学习框架和交互式特征融合可显著提升场景解析与几何视觉的表现，并减少对人工标注的需求。

Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.

</details>


### [47] [AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting](https://arxiv.org/abs/2602.13600)
*Jiacheng Zhang,Feng Liu,Chao Du,Tianyu Pang*

Main category: cs.CV

TL;DR: 提出了AdaVBoost，一种基于自适应视觉注意力增强的新方法，用于缓解大规模视觉-语言模型（LVLMs）中的幻觉问题。该方法根据生成过程中每个token的幻觉风险动态调整增强强度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉注意力增强方法在应对LVLMs幻觉时，采用预设的注意力缩放，这在不同生成步骤上产生了强化不足或过度的新幻觉问题，需要更精细化和动态的干预策略。

Method: 提出了AdaVBoost，核心包括提出视觉定位熵（VGE）用于估计每个生成token的幻觉风险，然后在生成过程中针对高风险token增强视觉注意力，对低风险token减少增强，实现逐token、逐步自适应干预。

Result: 在多个主流LVLMs和幻觉测试基准上，AdaVBoost的表现均大幅优于基线方法，有效降低了模型产生幻觉的概率。

Conclusion: AdaVBoost通过引入VGE和自适应增强机制，实现了对幻觉风险的细粒度识别和干预，为提升LVLMs泛化和可靠性提供了新方向，具有良好的实际应用潜力。

Abstract: Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.

</details>


### [48] [Towards Sparse Video Understanding and Reasoning](https://arxiv.org/abs/2602.13602)
*Chenwei Xu,Zhen Ye,Shang Wu,Weijian Li,Zihan Wang,Zhuofan Xia,Lie Lu,Pranav Maneriker,Fan Du,Manling Li,Han Liu*

Main category: cs.CV

TL;DR: 作者提出了一种新的视频问答（VQA）多轮智能体REVISE，可以高效选取重点帧，减少计算资源消耗，同时提升问答准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答大多采用均匀采样帧，导致处理大量冗余信息和低效推理，缺乏对关键信息的高效提取与利用。

Method: REVISE智能体采用选择性抽帧机制，只挑选少量有信息的帧，并通过跨轮更新摘要来保留状态，还能在足够自信时提前结束推理。其与专有视觉-语言模型兼容，并可通过无监督奖励EAGER对开源模型进行强化微调。EAGER奖励包括置信度提升、摘要充分性和早停正确性三部分。

Result: REVISE在多项视频问答基准测试上，相较于传统方法，在提升问答准确率的同时显著减少了处理帧数、推理轮次和提示tokens数。

Conclusion: REVISE方法在实际应用中具备高效、准确的视频推理能力，是实现稀疏化视频推理的有效途径。

Abstract: We present \revise (\underline{Re}asoning with \underline{Vi}deo \underline{S}parsity), a multi-round agent for video question answering (VQA). Instead of uniformly sampling frames, \revise selects a small set of informative frames, maintains a summary-as-state across rounds, and stops early when confident. It supports proprietary vision-language models (VLMs) in a ``plug-and-play'' setting and enables reinforcement fine-tuning for open-source models. For fine-tuning, we introduce EAGER (Evidence-Adjusted Gain for Efficient Reasoning), an annotation-free reward with three terms: (1) Confidence gain: after new frames are added, we reward the increase in the log-odds gap between the correct option and the strongest alternative; (2) Summary sufficiency: at answer time we re-ask using only the last committed summary and reward success; (3) Correct-and-early stop: answering correctly within a small turn budget is rewarded. Across multiple VQA benchmarks, \revise improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.

</details>


### [49] [A generalizable foundation model for intraoperative understanding across surgical procedures](https://arxiv.org/abs/2602.13633)
*Kanggil Park,Yongjun Jeon,Soyoung Lim,Seonmin Park,Jongmin Shin,Jung Yong Kim,Sehyeon An,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: 本文提出了ZEN模型，一种通用的基础模型，用于手术视频理解，在多种任务和设定下表现优异，向实现手术场景的统一表征迈进。


<details>
  <summary>Details</summary>
Motivation: 微创手术中临床决策依赖实时视觉解读，不同外科医生和手术间的主观差异导致评估、培训和AI系统泛化受限，需要一种更通用且稳健的手术场景理解模型。

Method: 作者构建了丰富且多样化的手术视频数据集，涵盖21种手术、超400万帧，采用自监督多教师蒸馏框架训练ZEN基础模型，并系统评估各种表征学习策略，在统一基准下对比分析。

Result: ZEN在20项下游任务、全微调、冻结主干、少样本和零样本设定下均优于现有手术基础模型，展示出强大的跨手术泛化能力。

Conclusion: ZEN模型实现了手术场景理解的统一表征，为术中辅助和手术培训评估等应用提供了新的支持，是向通用外科AI迈进的一大步。

Abstract: In minimally invasive surgery, clinical decisions depend on real-time visual interpretation, yet intraoperative perception varies substantially across surgeons and procedures. This variability limits consistent assessment, training, and the development of reliable artificial intelligence systems, as most surgical AI models are designed for narrowly defined tasks and do not generalize across procedures or institutions. Here we introduce ZEN, a generalizable foundation model for intraoperative surgical video understanding trained on more than 4 million frames from over 21 procedures using a self-supervised multi-teacher distillation framework. We curated a large and diverse dataset and systematically evaluated multiple representation learning strategies within a unified benchmark. Across 20 downstream tasks and full fine-tuning, frozen-backbone, few-shot and zero-shot settings, ZEN consistently outperforms existing surgical foundation models and demonstrates robust cross-procedure generalization. These results suggest a step toward unified representations for surgical scene understanding and support future applications in intraoperative assistance and surgical training assessment.

</details>


### [50] [Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness](https://arxiv.org/abs/2602.13636)
*Yang Zhou,Derui Ding,Ran Sun,Ying Sun,Haohua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效且鲁棒的无人机目标跟踪框架LGTrack，在多项数据集上实现了领先的实时速度和精度。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉目标跟踪通常需在准确性与实时性之间权衡，尤其在遮挡等困难环境下，传统方法难以兼顾高精度和高效率。

Method: 提出了LGTrack框架，融合动态层选择、高效特征增强和鲁棒表征学习。创新地设计了轻量级全局分组坐标注意力（GGCA）模块以捕获长程依赖，并加入相似性引导层适应（SGLA）模块取代知识蒸馏，实现精度与效率的平衡。

Result: 在三个数据集上进行实验，LGTrack实时跟踪速度达258.7 FPS（UAVDT），跟踪精度为82.8%（Precision），达到了当前最优水平。

Conclusion: LGTrack兼具优异的实时性和鲁棒性，能够高效应对无人机场景下的多种复杂挑战，推动了无人机视觉跟踪的发展。

Abstract: Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack

</details>


### [51] [DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation](https://arxiv.org/abs/2602.13637)
*Haoyu Zhao,Yuang Zhang,Junqi Cheng,Jiaxi Gu,Zenghui Lu,Peng Shu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成系统Divide-and-Conquer Diffusion Model (DCDM)，有效提升了视频生成过程中的语义、一致性与叙述连贯性。


<details>
  <summary>Details</summary>
Motivation: 目前的视频生成模型在视觉效果上已取得突破，但难以保证生成视频在语义、几何与身份等方面的一致性。因此，作者希望通过新的框架解决视频生成中关键的一致性挑战。

Method: DCDM将视频一致性建模分解为三个专用模块：（1）利用大语言模型解析语义，实现单片段内部一致性；（2）提出新的时序相机表达方式与图像初始化机制，保证跨片段的相机运动一致性；（3）通过整体场景生成、窗口化交叉注意力与稀疏自注意力机制，提升跨镜头的一致性并兼顾效率。所有模块基于统一视频生成主干网络。

Result: 在AAAI'26的CVM竞赛测试集上进行了验证，实验结果显示该方法能有效提升视频生成的多维一致性。

Conclusion: DCDM框架及其模块化方法能从语义、相机运动与叙事三方面提升视频生成一致性，是解决当前视频生成关键难题的有效手段。

Abstract: Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI'26, and the results demonstrate that the proposed strategies effectively address these challenges.

</details>


### [52] [KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination](https://arxiv.org/abs/2602.13650)
*Byungjin Choi,Seongsu Bae,Sunjun Kweon,Edward Choi*

Main category: cs.CV

TL;DR: 该论文提出了一个面向韩国医师资格考试风格的多模态选择题数据集KorMedMCQA-V，用于评估视觉-语言模型（VLMs）在医学领域推理能力。数据集包含1534道题及2043张医学影像。基准测试表明，当前最优模型准确率最高96.9%，但多图像问题和部分医学专门化模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉-语言推理主要集中在英语语境，缺乏针对韩语和医学场景的多模态权威评测数据集。医学考试典型问题常需整合多类医学影像信息，且面临多语言和专业领域的特殊挑战，迫切需要丰富的基准数据推动VLMs进步。

Method: 作者收集2012-2023年韩国医考题目，并归纳整理形成1534道带2043张影像（X线、CT、心电图、超声、内镜等）的多模态选择题，其中约30%涉及多幅图像。统一zero-shot评测协议下，对50余种VLMs进行测试，涵盖通用型、医学专用和韩语专用模型。

Result: 最优专有模型Gemini-3.0-Pro准确率96.9%，最佳开源模型Qwen3-VL-32B-Thinking为83.7%，最佳韩语专用模型VARCO-VISION-2.0-14B仅43.2%。注重推理的模型较仅语义调优提升显著（最高+20%），医学专用模型对强通用模型优势不稳定，多图像题对所有模型均有显著拖累，不同医学影像类型模型表现差异大。

Conclusion: KorMedMCQA-V数据集填补韩国医学多模态推理评测空白，为VLMs医学能力系统评测、研究新模型及多模态融合技巧提供了重要平台。实验显示当前模型仍面临多图整合和专业场景适应性挑战。该数据集现已公开。

Abstract: We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.

</details>


### [53] [Optimizing Point-of-Care Ultrasound Video Acquisition for Probabilistic Multi-Task Heart Failure Detection](https://arxiv.org/abs/2602.13658)
*Armin Saadat,Nima Hashemi,Bahar Khodabakhshian,Michael Y. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 本文提出了一种个性化的超声数据采集策略，通过强化学习（RL）自主选择心脏超声的采集视图，实现了诊断效率优化，同时显著减少了视频数量并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 床旁超声（POCUS）须在有限时间和操作精力下快速支持临床决策，优化采集策略有助于降低医生负担、缩短诊疗流程并提升诊断效率。

Method: 将POCUS视为顺序数据采集问题，强化学习智能体逐步决定是否继续采集哪个视图或终止采集。终止后，多视图Transformer模型联合预测主动脉瓣狭窄（AS）严重程度和左心室射血分数（LVEF），输出不确定性概率，实现诊断准确性与采集成本的权衡。

Result: 在12,180例患者的数据集上，测试集（1,820例）结果显示：新方法的视频采集量减少32%，诊断效果（多任务平均平衡准确率77.2%）与全量采集持平，兼顾了效率和多任务鲁棒性能。

Conclusion: 基于患者个体化、成本敏感的视频采集算法，可在保证诊断质量的同时，优化床旁超声操作流程，且具备良好的扩展性，值得进一步临床验证和集成。

Abstract: Purpose: Echocardiography with point-of-care ultrasound (POCUS) must support clinical decision-making under tight bedside time and operator-effort constraints. We introduce a personalized data acquisition strategy in which an RL agent, given a partially observed multi-view study, selects the next view to acquire or terminates acquisition to support heart-failure (HF) assessment. Upon termination, a diagnostic model jointly predicts aortic stenosis (AS) severity and left ventricular ejection fraction (LVEF), two key HF biomarkers, and outputs uncertainty, enabling an explicit trade-off between diagnostic performance and acquisition cost. Methods: We model POCUS as a sequential acquisition problem: at each step, a video selector (RL agent) chooses the next view to acquire or terminates acquisition. Upon termination, a shared multi-view transformer performs multi-task inference with two heads, ordinal AS classification, and LVEF regression, and outputs Gaussian predictive distributions yielding ordinal probabilities over AS classes and EF thresholds. These probabilities drive a reward that balances expected diagnostic benefit against acquisition cost, producing patient-specific acquisition pathways. Results: The dataset comprises 12,180 patient-level studies, split into training/validation/test sets (75/15/15). On the 1,820 test studies, our method matches full-study performance while using 32% fewer videos, achieving 77.2% mean balanced accuracy (bACC) across AS severity classification and LVEF estimation, demonstrating robust multi-task performance under acquisition budgets. Conclusion: Patient-tailored, cost-aware acquisition can streamline POCUS workflows while preserving decision quality, producing interpretable scan pathways suited to bedside use. The framework is extensible to additional cardiac endpoints and merits prospective evaluation for clinical integration.

</details>


### [54] [LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases](https://arxiv.org/abs/2602.13662)
*Khang Nguyen Quoc,Phuong D. Dao,Luyl-Da Quach*

Main category: cs.CV

TL;DR: 本文提出了LeafNet数据集和LeafBench基准，用于多模态视觉语言模型（VLMs）在植物病理学任务中的评估。通过大规模数据和新基准，揭示了当前多模态模型的优势和不足。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言预训练极大提升了多模态处理能力，但在农业领域，如植物病理学的实际应用仍受限，主要原因是缺乏大规模、多模态图像-文本数据集及系统性评价标准。作者希望弥补这一研究空白。

Method: 作者构建了包含186,000张叶片图像、97种病害类别与详细元数据的LeafNet多模态数据集，并据此生成13,950问答对，涵盖六类关键农业任务。基于此，作者提出LeafBench作为视觉问答测试基准，系统评估12个主流多模态模型在植物病理理解上的表现，并对比视觉模型和多模态VLMs的能力。

Result: 在LeafBench上，二分类（健康/病害）任务精度超过90%，但细粒度病原体和物种识别低于65%。调优后的VLMs在诊断精度方面优于传统视觉模型，显示多模态架构整合语言信息的重要优势。

Conclusion: 当前VLMs在植物病理学应用中仍有明显短板，LeafBench为该领域方法进步与评估提供重要支持。未来需推进针对农业领域的多模态AI能力提升。

Abstract: Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\% accuracy, while fine-grained pathogen and species identification remains below 65\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.

</details>


### [55] [EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation](https://arxiv.org/abs/2602.13669)
*Rang Meng,Weipeng Wu,Yingjie Yin,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: EchoTorrent提出了一种多模式视频生成的新方法，显著提升了推理速度和时序一致性，改善了流式生成中的质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频生成方法虽然能实现高视觉质量，但由于推理速度慢和时序稳定性差，难以支持实时应用，特别是在流式推理下容易出现多模态不同步（如画面模糊、嘴型与音频不同步等）的问题，业界急需在效率和性能之间找到更优的折中方案。

Method: EchoTorrent采用四大设计：1）多教师训练（Multi-Teacher Training），通过在不同偏好领域微调模型，获得专家模型，分阶段知识蒸馏到学生模型；2）自适应CFG校准（ACC-DMD），通过分阶段调整CFG，消除冗余运算，实现一步推理；3）混合长尾强制（Hybrid Long Tail Forcing），利用因果-双向混合架构在长序列只对尾部帧做对齐训练，增强长时间生成的时空一致性；4）VAE解码器优化（VAE Decoder Refiner），在像素域细化解码器，恢复高频细节。

Result: 实验证明，EchoTorrent可以以极少的多步自动回归生成实现更长时间的时序一致性、身份保持和音频-唇形同步，明显优于现有主流方法。

Conclusion: EchoTorrent是一种高效、多模态、一致性强的视频生成方案，特别适用于实时流式生成场景，有效缓解了生成模型的效率与性能矛盾。

Abstract: Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.

</details>


### [56] [An Ensemble Learning Approach towards Waste Segmentation in Cluttered Environment](https://arxiv.org/abs/2602.13681)
*Maimoona Jafar,Syed Imran Ali,Ahsan Saadat,Muhammad Bilal,Shah Khalid*

Main category: cs.CV

TL;DR: 本文提出了一种结合U-Net和FPN的集成学习方法用于垃圾分割任务，在真实场景模拟数据集上显著提升了分割准确率，有助于提升自动垃圾分拣效率。


<details>
  <summary>Details</summary>
Motivation: 废物分类与回收过程在环境保护中极为重要，而目前自动化垃圾分离中受限于复杂实际环境下分割任务的准确性，因此有必要提升废物分割的精度。

Method: 作者提出将U-Net和FPN两种高性能分割模型进行加权平均集成，发挥两者互补优势，通过数据预处理提升模型学习能力，并在接近真实场景的垃圾数据集上进行训练与测试。

Result: 集成模型EL-4在IoU指标上达到0.8306, 高于U-Net的0.8065；Dice损失降至0.09019，低于FPN的0.1183，展示出更优的分割性能。

Conclusion: 本研究提出的集成分割方法能有效提升废物分割准确率，为材料回收设施的自动垃圾分拣提供支持，有助于减少人工干预并提升整体回收效率。

Abstract: Environmental pollution is a critical global issue, with recycling emerging as one of the most viable solutions. This study focuses on waste segregation, a crucial step in recycling processes to obtain raw material. Recent advancements in computer vision have significantly contributed to waste classification and recognition. In waste segregation, segmentation masks are essential for robots to accurately localize and pick objects from conveyor belts. The complexity of real-world waste environments, characterized by deformed items without specific patterns and overlapping objects, further complicates waste segmentation tasks. This paper proposes an Ensemble Learning approach to improve segmentation accuracy by combining high performing segmentation models, U-Net and FPN, using a weighted average method. U-Net excels in capturing fine details and boundaries in segmentation tasks, while FPN effectively handles scale variation and context in complex environments, and their combined masks result in more precise predictions. The dataset used closely mimics real-life waste scenarios, and preprocessing techniques were applied to enhance feature learning for deep learning segmentation models. The ensemble model, referred to as EL-4, achieved an IoU value of 0.8306, an improvement over U-Net's 0.8065, and reduced Dice loss to 0.09019 from FPN's 0.1183. This study could contribute to the efficiency of waste sorting at Material Recovery Facility, facilitating better raw material acquisition for recycling with minimal human intervention and enhancing the overall throughput.

</details>


### [57] [A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy](https://arxiv.org/abs/2602.13693)
*Xin Zhang,Liangxiu Han,Yue Shi,Yalin Zheng,Uazman Alam,Maryam Ferdousi,Rayaz Malik*

Main category: cs.CV

TL;DR: 本文提出一种创新的生成式深度学习方法用于合成角膜共聚焦显微成像（CCM）图像，有效缓解医学影像标注数据稀缺以及形态微小差异带来的挑战，提升糖尿病神经病变（DPN）自动诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: CCM能够敏感检测DPN导致的小纤维损伤，但高质量的带标签样本稀缺，且神经形态变化细微，这限制了深度学习诊断模型的开发。此外，通用的生成式AI模型在医学影像中常出现解剖学真实性不足的问题，难以满足临床分析需求。

Method: 提出基于权重分解低秩适应（WDLoRA）的多模态生成框架。该方法通过分离网络权重的幅值和方向调整，分别学习神经结构取向与基质对比度，并联合神经分割掩膜及疾病特异性提示，生成具有解剖学一致性的CCM图像。

Result: 多维度评估显示，该模型在视觉保真度（FID=5.18）和结构一致性（SSIM=0.630）均优于传统GAN与扩散模型。合成图像保留了临床关键生物标志物，统计上与真实患者数据无显著差异，用其训练的自动诊断模型诊断准确率提升2.1%，分割性能提升2.2%。

Conclusion: WDLoRA框架能够高效生成符合医学要求的CCM影像，缓解数据瓶颈，为医学AI诊断模型提升性能提供了新的思路和有效手段。

Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fréchet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.

</details>


### [58] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: 本文利用视觉语言模型（VLM）自动检测显微镜下的寄生虫卵，效果超过传统方法，显示该方法有望成为智能诊断的关键技术。


<details>
  <summary>Details</summary>
Motivation: 土传蠕虫感染影响全球大量人群，而传统人工显微镜诊断既耗时又容易出错，并且在资源有限的地区专业技术短缺。因此，需要高效、自动化的诊断方法。

Method: 本研究采用微软 Florence 等视觉语言模型（VLM），对其进行微调，使其能在显微镜图像中精准定位所有寄生虫卵，并与主流目标检测方法如 EfficientDet 进行了比较。

Result: 本地化 VLM 在寄生虫卵检测中的mIOU达0.94，性能优于EfficientDet等传统方法。

Conclusion: 所提出的VLM方法在寄生虫卵定位上表现优越，显示出构建可扩展智能寄生虫诊断系统的巨大潜力，可望应用于工程实践，缓解人力短缺与误诊问题。

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [59] [RGA-Net: A Vision Enhancement Framework for Robotic Surgical Systems Using Reciprocal Attention Mechanisms](https://arxiv.org/abs/2602.13726)
*Quanjun Li,Weixuan Li,Han Xia,Junhua Zhou,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 该论文提出了RGA-Net，一种专为机器人手术流程中去除手术烟雾设计的深度学习模型，通过创新的注意力机制显著提升了内镜视频的清晰度，从而改善了人机界面和手术安全。


<details>
  <summary>Details</summary>
Motivation: 手术烟雾会严重影响机器人手术系统的视觉反馈，降低操作者的操作精度和病人安全性，因此亟需高效去烟雾方法提升内镜视频质量。

Method: 提出了一种分层编码-解码结构的RGA-Net框架，包含了双流混合注意力（DHA）模块与轴向分解注意力（ADA）模块，通过频域与时域双重处理本地细节与全局变化，并利用互递交叉门控机制优化特征交互。

Result: 在DesmokeData和LSD3K手术数据集上的实验显示，RGA-Net在恢复内镜可视化清晰度方面较现有方法取得了优异表现，适应机器人手术实际场景需求。

Conclusion: RGA-Net为机器人手术系统提供了可靠的视觉增强技术基础，有望减轻外科医生的认知负担，优化手术流程，并降低医疗风险，未来可通过临床试验进一步验证其实用价值。

Abstract: Robotic surgical systems rely heavily on high-quality visual feedback for precise teleoperation; yet, surgical smoke from energy-based devices significantly degrades endoscopic video feeds, compromising the human-robot interface and surgical outcomes. This paper presents RGA-Net (Reciprocal Gating and Attention-fusion Network), a novel deep learning framework specifically designed for smoke removal in robotic surgery workflows. Our approach addresses the unique challenges of surgical smoke-including dense, non-homogeneous distribution and complex light scattering-through a hierarchical encoder-decoder architecture featuring two key innovations: (1) a Dual-Stream Hybrid Attention (DHA) module that combines shifted window attention with frequency-domain processing to capture both local surgical details and global illumination changes, and (2) an Axis-Decomposed Attention (ADA) module that efficiently processes multi-scale features through factorized attention mechanisms. These components are connected via reciprocal cross-gating blocks that enable bidirectional feature modulation between encoder and decoder pathways. Extensive experiments on the DesmokeData and LSD3K surgical datasets demonstrate that RGA-Net achieves superior performance in restoring visual clarity suitable for robotic surgery integration. Our method enhances the surgeon-robot interface by providing consistently clear visualization, laying a technical foundation for alleviating surgeons' cognitive burden, optimizing operation workflows, and reducing iatrogenic injury risks in minimally invasive procedures. These practical benefits could be further validated through future clinical trials involving surgeon usability assessments. The proposed framework represents a significant step toward more reliable and safer robotic surgical systems through computational vision enhancement.

</details>


### [60] [Explore Intrinsic Geometry for Query-based Tiny and Oriented Object Detector with Momentum-based Bipartite Matching](https://arxiv.org/abs/2602.13728)
*Junpeng Zhang,Zewei Yang,Jie Feng,Yuhui Zheng,Ronghua Shang,Mengxuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于查询的方向目标检测器IGOFormer，通过显式融入内在几何信息和提升阶段间匹配稳定性，在检测任意方向，尤其是极小目标时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的检测器在检测任意方向的目标（尤其是小目标）时效果有限，原因在于特征解码时几何信息利用不足，以及多阶段匹配造成监督信号不一致。作者希望解决这两个关键问题。

Method: 作者引入了Intrinsic Geometry-aware Decoder，将由对象查询激活的特征与从相关性中推断出来的几何嵌入相结合，增强了目标的几何布局表达；同时提出Momentum-based Bipartite Matching机制，用带平滑因子的指数移动平均，融合历史匹配信息，缓解阶段间监督信号冲突。

Result: 在DOTA-V1.0数据集、Swin-T骨干网络、单尺度条件下，IGOFormer取得了AP$_{50}$得分78.00%的优异性能。论文还进行了充分的实验和消融分析验证其有效性。

Conclusion: IGOFormer方法通过改善几何特征利用与匹配机制，显著提升了方向目标检测性能，尤其适合遥感等场景下的小目标检测，显示出较强的实际应用潜力。

Abstract: Recent query-based detectors have achieved remarkable progress, yet their performance remains constrained when handling objects with arbitrary orientations, especially for tiny objects capturing limited texture information. This limitation primarily stems from the underutilization of intrinsic geometry during pixel-based feature decoding and the occurrence of inter-stage matching inconsistency caused by stage-wise bipartite matching. To tackle these challenges, we present IGOFormer, a novel query-based oriented object detector that explicitly integrates intrinsic geometry into feature decoding and enhances inter-stage matching stability. Specifically, we design an Intrinsic Geometry-aware Decoder, which enhances the object-related features conditioned on an object query by injecting complementary geometric embeddings extrapolated from their correlations to capture the geometric layout of the object, thereby offering a critical geometric insight into its orientation. Meanwhile, a Momentum-based Bipartite Matching scheme is developed to adaptively aggregate historical matching costs by formulating an exponential moving average with query-specific smoothing factors, effectively preventing conflicting supervisory signals arising from inter-stage matching inconsistency. Extensive experiments and ablation studies demonstrate the superiority of our IGOFormer for aerial oriented object detection, achieving an AP$_{50}$ score of 78.00\% on DOTA-V1.0 using Swin-T backbone under the single-scale setting. The code will be made publicly available.

</details>


### [61] [Generative Latent Representations of 3D Brain MRI for Multi-Task Downstream Analysis in Down Syndrome](https://arxiv.org/abs/2602.13731)
*Jordi Malé,Juan Fortea,Mateus Rozalem-Aranha,Neus Martínez-Abadías,Xavier Sevillano*

Main category: cs.CV

TL;DR: 本文利用变分自编码器（VAE）将3D脑部MRI影像编码至紧凑的潜在空间，评估其在重建、可视化和脑部疾病分类中的表现，并发现VAE能有效区分唐氏综合征和正常人群。


<details>
  <summary>Details</summary>
Motivation: 虽然生成模型在医学影像领域表现出强大能力，但其潜在表示的结构、信息量及对下游任务的作用尚缺乏深入探讨，尤其是3D脑MRI影像。研究这些潜在表示有助于推动生成模型在神经影像学研究和临床决策上的应用。

Method: 开发了多种变分自编码器（VAE），将3D脑部MRI扫描件编码至紧凑潜在表示。通过三方面评估：1）MRI重建质量，2）PCA可视化潜在空间结构，3）在唐氏综合征与正常人脑部MRI分类任务上的表现。

Result: VAE模型能够高保真地重建脑部MRI，并在潜在空间中成功捕捉重要脑部特征，尤其对于区分唐氏综合征患者和正常对照组显示出明显聚类趋势。

Conclusion: VAE潜在空间不仅能有效保留脑部关键特征，还为相关临床下游任务（如疾病分类）提供高效表示，展现其在医学影像生成和分析中的广阔应用前景。

Abstract: Generative models have emerged as powerful tools in medical imaging, enabling tasks such as segmentation, anomaly detection, and high-quality synthetic data generation. These models typically rely on learning meaningful latent representations, which are particularly valuable given the high-dimensional nature of 3D medical images like brain magnetic resonance imaging (MRI) scans. Despite their potential, latent representations remain underexplored in terms of their structure, information content, and applicability to downstream clinical tasks. Investigating these representations is crucial for advancing the use of generative models in neuroimaging research and clinical decision-making. In this work, we develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations for generative and predictive applications. We systematically evaluate the effectiveness of the learned representations through three key analyses: (i) a quantitative and qualitative assessment of MRI reconstruction quality, (ii) a visualisation of the latent space structure using Principal Component Analysis, and (iii) downstream classification tasks on a proprietary dataset of euploid and Down syndrome individuals brain MRI scans. Our results demonstrate that the VAE successfully captures essential brain features while maintaining high reconstruction fidelity. The latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

</details>


### [62] [T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation](https://arxiv.org/abs/2602.13751)
*Bin Yang,Rong Ou,Weisheng Xu,Jiaqi Xiong,Xintao Li,Taowen Wang,Luyu Zhu,Xu Jiang,Jing Tan,Renjing Xu*

Main category: cs.CV

TL;DR: 本文提出了一套面向文本到动作生成任务的OOD（分布外）评测基准，分析了14个主流模型在更复杂文本输入下的泛化与性能，发现现有方法在细粒度准确性上普遍较弱。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成的评测方法只关注于分布内的文本输入及少量评价标准，难以系统评估模型应对复杂、分布外文本的泛化能力和生成质量。因此急需更具挑战性的评测基准。

Method: 作者构建了包含1,025个分布外文本描述的提示集，设计了统一评估框架：包括大模型评测、动作多因素评估及细粒度准确率评估，并分析了14个代表性模型在这套体系下的表现。

Result: 实验显示，虽然不同基线模型在文本-动作对齐、动作泛化性、物理质量等各方面有优势，但绝大多数模型在细粒度准确率评测下表现不佳。

Conclusion: 现有方法在OOD条件下面临明显局限，提出的评测体系为后续生成模型的设计和评估提供了参考和指导。

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

</details>


### [63] [OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding](https://arxiv.org/abs/2602.13758)
*Haoyi Tao,Chaozheng Huang,Nan Wang,Han Lyu,Linfeng Zhang,Guolin Ke,Xi Fang*

Main category: cs.CV

TL;DR: 论文提出了OmniScience，一个覆盖10多个科学领域、包含150万图像-标题-上下文三元组的大型高保真多模态科学图像数据集，并开发了高密度描述生成和质量过滤流程，显著提升了多模态模型对科学图像的理解。基于该数据集微调的模型在相关评测中大幅优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在自然图像理解表现出色，但在科学图像（如示意图、实验图和分析图表）解释方面能力有限，尤其是开源模型。这主要因为现有数据集领域覆盖不足、结构标注粗糙、语义基础薄弱。

Method: 作者构建了OmniScience数据集，包含1.5百万科学图像及其标题和上下文。提出了一套动态图像再描述流程，利用最新多模态大模型，将视觉特征、原有标题及论文内文本参考联合生成高密度描述，并通过严格质量过滤和专家对齐，保证数据的准确性和完整性。还提出了以Caption QA为代理任务的视觉理解评测协议。

Result: 所提出流程将图文多模态相似度从0.769提升到0.956。基于该数据集微调的Qwen2.5-VL-3B模型在MM-MT-Bench、MMMU等基准任务上，分别提升分数0.378和0.140，明显优于基线。

Conclusion: 通过高质量科学图数据集和创新数据生成、过滤流程，显著提升了多模态大模型对科学图像的理解和推理能力，为科学AI应用奠定坚实基础。

Abstract: Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.

</details>


### [64] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: 本文提出一种新方法，可在仅用单目视频的情况下实现对动态场景的高精度4D重建，显著提升了物理一致性和重建准确率。


<details>
  <summary>Details</summary>
Motivation: 在机器人学习和理解动态世界时，单目视频因信息受限，进行4D场景（时空）重建极具挑战，现有方法在动态和物理一致性方面表现不足。

Method: 作者提出了多尺度动力学机制，将复杂的运动场拆解为多个层次的动态过程，并发展出基于多尺度动力学的动态高斯序列新表征。同时引入基础视觉模型的多模态先验，实现对重建的补充监督和解空间收缩。

Result: 该方法能够由单目视频生成准确且全局一致的4D重建结果，在动态新视角合成任务上于多个基准和真实世界操控数据集超过现有方法。

Conclusion: 通过多尺度结构与多模态先验的结合，本文提出的创新方法有效缓解了单目动态场景重建的模糊性，实现了更高质量和物理合理性的4D重建，具有重要的实际应用价值。

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [65] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 本文提出了SAM4Dcap框架，可通过单目视频估算人体生物力学指标，实现居家或非实验室场景下的运动分析。该系统具备开源、端到端处理能力，无需额外训练，初步实验显示其在膝关节运动学方面与多视角系统效果相当。


<details>
  <summary>Details</summary>
Motivation: 传统的生物力学分析依赖昂贵的光学动作捕捉系统，只能在实验室进行，限制了其临床和防伤应用的普及。尽管多视角视频方法部分降低了成本，但在家庭等个人环境下仍不切实际。研究动机在于开发既经济又易用的单目视频生物力学分析方法，从而促进广泛应用。

Method: 作者提出SAM4Dcap框架，结合了SAM-Body4D的人体时序一致性四维网格恢复，和OpenSim生物力学求解器。该流程可自动将重建的人体网格转为适配不同肌骨模型的轨迹文件，支持自动化处理和Linux原生部署，无需额外训练。

Result: 通过对步行和下跳等任务的初步评估，SAM4Dcap在膝关节运动学预测方面接近多视角系统，但在髋屈曲和结果抖动方面仍有待改进。

Conclusion: SAM4Dcap有效结合了计算机视觉与生物力学建模，提供了灵活且易获取的单目视频动作分析方案，将生物力学分析从实验室拓展到更广泛的应用场景。

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [66] [RPGD: RANSAC-P3P Gradient Descent for Extrinsic Calibration in 3D Human Pose Estimation](https://arxiv.org/abs/2602.13901)
*Zhanyu Tuo*

Main category: cs.CV

TL;DR: 本文提出了一种名为RPGD的新型外参标定方法，能够利用自然人体动作，将基于动捕（MoCap）的3D骨架数据与单目或多视角RGB相机对齐。该方法在多个公开和自采数据集上均表现优异，实现了高精度、自动化且鲁棒的外参标定。


<details>
  <summary>Details</summary>
Motivation: 在多相机3D人体姿态估计任务中，将动捕系统获得的3D骨架与RGB相机数据精确对齐是一个关键难题，尤其是在无特定标定工具、环境复杂或大规模数据收集时更为困难。现有方法要么依赖特殊设备，要么对自然动作的容忍度低，故亟需一种简便、鲁棒且自动化的新方案。

Method: RPGD方法将外参标定问题建模为一个从粗到细的优化问题，首先使用RANSAC-P3P算法进行全局鲁棒的初始化，然后通过梯度下降进一步优化，实现基于自然人体姿态的自动标定。该方法无需特殊标定物体，只需常见的人体动作即可。

Result: 在三个公开大规模3D人体姿态估计数据集和一个作者自采的自然场景数据集上，RPGD的实验结果显示其外参恢复精度与已知真实值非常吻合，即使在噪声较大和具有挑战性的场景下，重投影误差也能达到亚像素级别。

Conclusion: RPGD为大规模3D人体姿态数据集采集任务提供了一种简单、高精度、自动化和鲁棒的外参标定方案，极大提升了标定效率与实际可用性。

Abstract: In this paper, we propose RPGD (RANSAC-P3P Gradient Descent), a human-pose-driven extrinsic calibration framework that robustly aligns MoCap-based 3D skeletal data with monocular or multi-view RGB cameras using only natural human motion. RPGD formulates extrinsic calibration as a coarse-to-fine problem tailored to human poses, combining the global robustness of RANSAC-P3P with Gradient-Descent-based refinement. We evaluate RPGD on three large-scale public 3D HPE datasets as well as on a self-collected in-the-wild dataset. Experimental results demonstrate that RPGD consistently recovers extrinsic parameters with accuracy comparable to the provided ground truth, achieving sub-pixel MPJPE reprojection error even in challenging, noisy settings. These results indicate that RPGD provides a practical and automatic solution for reliable extrinsic calibration of large-scale 3D HPE dataset collection.

</details>


### [67] [Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking](https://arxiv.org/abs/2602.13772)
*Xiaoyu Li,Yitao Wu,Xian Wu,Haolin Zhuo,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 本文提出了一种通用的离线3D多目标跟踪方法Offline-Poly，实现对任意追踪器输出的再优化，达到更高的4D自动标注性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线3D MOT方法大多直接沿用在线框架，未充分利用离线处理资源无限制和全局可见的优势，且高度依赖特定上游模型，适应性不强。

Method: 提出了以Tracking-by-Tracking (TBT)为核心的Offline-Poly跟踪范式，针对任意追踪结果结合预处理、分层匹配与融合、最终精炼多个模块，实现短时冗余剔除、全局重识别及跨追踪器集成。

Result: 在nuScenes数据集上，Offline-Poly取得77.6% AMOTA，在KITTI上取得83.00% HOTA，都为当前最优水平，并展示出很好的灵活性与泛化性。

Conclusion: Offline-Poly突破了特定上游依赖的限制，充分利用离线优势，能对离线3D MOT实现灵活、通用且高效的优化，是4DAL流程中关键的推进方法。

Abstract: Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.

</details>


### [68] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: 本论文系统性地研究了工业环境下XR系统中视觉-惯性里程计（VIO）在传感器退化条件下的表现，发现惯性传感器退化对定位误差影响更严重。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-惯性里程计广泛应用于XR系统的姿态跟踪，但现实工业环境下的传感器很可能出现退化，但现有研究主要关注理想条件下的表现，忽略了连续退化下的影响。

Method: 作者通过受控实验，系统性地对VIO系统注入视觉和惯性传感器故障，量化两种退化在不同工况下的影响。

Result: 实验发现：视觉传感退化通常导致厘米级别定位误差，而惯性传感器退化会引起高达几百至几千米的大幅轨迹偏差，二者影响高度不对称。

Conclusion: 设计和评估XR工业系统时应更加重视惯性传感器的可靠性，因为其退化可能带来更严重定位问题。

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [69] [Skeleton2Stage: Reward-Guided Fine-Tuning for Physically Plausible Dance Generation](https://arxiv.org/abs/2602.13778)
*Jidong Jia,Youjian Zhang,Huan Fu,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出通过加入物理约束和强化学习微调，生成更符合实际物理规律的舞蹈动作，显著减少身体穿模和脚-地异常，实现更真实美观的舞蹈生成。


<details>
  <summary>Details</summary>
Motivation: 以往舞蹈生成主要在骨骼级别，忽略了三维人体网格的物理约束，导致可视化时动作虽在关节点上合理却存在身体自穿透及脚部异常，降低美感及实际可用性。本文旨在解决骨骼到网格表现之间的物理违和问题。

Method: 从人体网格中设计基于物理的奖励项，并用强化学习微调扩散模型。奖励涵盖：（i）动作可被物理仿真器复制的模仿奖励（惩罚穿透与脚打滑），（ii）专门用于捕捉舞蹈脚-地互动的偏差奖励，并在推理时附加引导。同时，为避免生成僵硬动作，引入抗冻结奖励以兼顾动态性和物理合理性。

Result: 在多个舞蹈数据集上，所提方法显著提升了生成动作的物理合理性，生成更真实、美观的舞蹈视频。

Conclusion: 骨骼到网格的物理约束是提升舞蹈生成现实性和美观性的关键。通过物理奖励设计与强化学习微调，可有效减少常见物理异常，为真实舞蹈动作合成带来明显改进。

Abstract: Despite advances in dance generation, most methods are trained in the skeletal domain and ignore mesh-level physical constraints. As a result, motions that look plausible as joint trajectories often exhibit body self-penetration and Foot-Ground Contact (FGC) anomalies when visualized with a human body mesh, reducing the aesthetic appeal of generated dances and limiting their real-world applications. We address this skeleton-to-mesh gap by deriving physics-based rewards from the body mesh and applying Reinforcement Learning Fine-Tuning (RLFT) to steer the diffusion model toward physically plausible motion synthesis under mesh visualization. Our reward design combines (i) an imitation reward that measures a motion's general plausibility by its imitability in a physical simulator (penalizing penetration and foot skating), and (ii) a Foot-Ground Deviation (FGD) reward with test-time FGD guidance to better capture the dynamic foot-ground interaction in dance. However, we find that the physics-based rewards tend to push the model to generate freezing motions for fewer physical anomalies and better imitability. To mitigate it, we propose an anti-freezing reward to preserve motion dynamics while maintaining physical plausibility. Experiments on multiple dance datasets consistently demonstrate that our method can significantly improve the physical plausibility of generated motions, yielding more realistic and aesthetically pleasing dances. The project page is available at: https://jjd1123.github.io/Skeleton2Stage/

</details>


### [70] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: 本综述系统梳理了几何视觉领域的全局求解器，归纳为BnB、凸松弛及逐步非凸三大范式，并比较了它们在十项核心视觉任务中的理论基础、性能与权衡。


<details>
  <summary>Details</summary>
Motivation: 传统3D视觉任务常受限于局部或启发式算法，无法保证全局最优。全局求解器近年来快速发展，有望提供可验证的、鲁棒性强的优化结果。该论文旨在填补对全局求解器系统化梳理的空白，为研究者提供理论与实践参考。

Method: 作者提出了对全局求解器的系统分类，详细介绍了Branch-and-Bound（分支定界）、Convex Relaxation（凸松弛）、Graduated Non-Convexity（逐步非凸）三种主流范式。分析了它们的理论基础、算法设计，以及为增强鲁棒性和可扩展性所做的改进，并在十个核心几何视觉任务（如Wahba问题和捆绑调整）中进行了案例分析和性能比较。

Result: 综述揭示了不同方法在求解3D视觉几何优化问题时的最优性、鲁棒性和可扩展性权衡。指出全局求解器在提升结果可认证性和实际稳健性方面获得重大进展。同时分析了实际应用中面临的挑战和未来发展方向。

Conclusion: 本文为全局求解器在3D视觉中的发展现状、理论基础和实际应用进行了系统综述，对未来算法可扩展性、与数据先验整合、标准化评价及安全应用等提出了关键展望。读者可通过配套的在线资源获取文献和代码示例，推动该领域进一步发展。

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [71] [Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.13780)
*Hengtong Shen,Li Yan,Hong Xie,Yaxuan Wei,Xinhao Li,Wenfei Shen,Peixian Lv,Fei Tan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于遥感基础模型PerA的新型语义变化检测方法PerASCD，有效提升了遥感双时相图像中的多尺度语义理解和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感语义变化检测（SCD）方法受限于模型语义理解能力和任务复杂性，在性能和方法复杂度上都面临挑战。作者旨在简化检测流程，同时提升检测表现。

Method: 提出了一种模块化的级联门控解码器（CG-Decoder），简化了解码流程并加强多层次特征交互与融合。同时引入软语义一致性损失（SSCLoss），缓解训练过程中的数值不稳定问题。此外，作者还测试了多个遥感基础模型配合该解码器在SCD任务中的适用性。

Result: 在两个公开基准数据集上，所提方法不仅简化了现有检测范式，可无缝适配不同视觉编码器，还取得了当前最优（SOTA）检测性能，验证了方法有效性。

Conclusion: PerASCD方法显著提升了语义变化检测的效率与精度，具有很好的通用性和实用性，为遥感变化检测任务带来了新的范式。

Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.

</details>


### [72] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: 本文提出了一种基于零部件的生成框架，可以高效地从单张图片生成具有真实结构和运动的可动三维物体，在速度、精度和可控性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 交互式3D应用、智能体、机器人和VR/AR等领域广泛需要高质量的可动三维物体，但现有获取高保真部件分解和运动资产的方法效率低或结果不符合需求。因此，迫切需要一种既高效又能准确建模复杂结构和动作的自动化方法。

Method: 作者提出了一种以零部件为核心的生成式框架，用于重建和合成可动三维物体。每个零部件通过潜在token进行编码，并结合部件身份和运动提示条件。模型直接以单张图片为条件，实现高效的端到端前馈推断，生成既保留实例特异性，又结构和运动合理的三维模型。

Result: 在常见的可动类别如抽屉、门等数据集上，作者的方法在输入一致性、部件准确性和动作合理性上均优于优化式与检索式基线，同时极大缩短了推断时间。

Conclusion: 该方法摆脱了以往每实例均需优化的流程，实现了快速、可控且结构与运动兼具的三维物体生成，显著提升了三维互动应用中资产的可扩展性和实用性。

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [73] [Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields](https://arxiv.org/abs/2602.13801)
*Jiaze Li,Daisheng Jin,Fei Hou,Junhui Hou,Zheng Liu,Shiqing Xin,Wenping Wang,Ying He*

Main category: cs.CV

TL;DR: DiWR是一种能从带有噪声、离群点以及非均匀采样的无向点云中重建闭合三维曲面的鲁棒新方法，且效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 目前在从无向点云数据（非均匀采样、含噪声与离群点）重建封闭三维表面时，常规方法依赖多阶段预处理，流程复杂且对异常数据不鲁棒；因此需要一个无需预处理、能联合修正点云方向、权重及信心水平、能自动兼容异常数据的重建方案。

Method: 本方法以广义绕数（GWN）为隐式场表示，联合优化点的朝向、每点面积权重与置信系数，通过最小化诱导绕数场的狄利克雷能量及GWN约束，有效弱化离群点和噪声影响，直接输出高质量封闭曲面。

Result: 在3D Gaussian Splatting、计算机视觉及受损图形基准点云的实验中，DiWR在这些复杂输入条件下成功重建了合理的闭合表面，且在定量和定性实验结果中超越了传统和最新联合优化类方法。

Conclusion: DiWR无需繁琐预处理，稳健地从高噪点云恢复封闭曲面重建能力优异，对非均匀采样、噪声和离群点适应性强，具备领先的实用价值。

Abstract: We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additional GWN-based constraints, allowing DiWR to compensate for non-uniform sampling, reduce the impact of noise, and downweight outliers during reconstruction, with no reliance on separate preprocessing. We evaluate DiWR on point clouds from 3D Gaussian Splatting, a computer-vision pipeline, and corrupted graphics benchmarks. Experiments show that DiWR produces plausible watertight surfaces on these challenging inputs and outperforms both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

</details>


### [74] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: 本文提出了一种更有效的文本到3D生成方法VAR-3D，通过改进的VQ-VAE和渲染监督策略，在提升生成质量和语义对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的3D生成方法在离散化表示学习过程中容易造成信息损失，导致3D形状失真和几何一致性差。此外，两阶段训练范式造成重构和自回归生成目标不一致。

Method: 设计了一个具备视角感知能力的3D VQ-VAE，将复杂的3D结构转化为离散token，并引入渲染监督训练策略，将token预测与视觉重构结合，促进生成结果在视觉和结构上都更贴近输入文本。

Result: 实验证明，VAR-3D在生成质量和文本-3D对齐方面显著优于现有方法。

Conclusion: VAR-3D有效缓解了3D离散表示信息损失和目标不匹配的问题，为文本到3D生成带来了更高质量和更好一致性的结果。

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [75] [Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings](https://arxiv.org/abs/2602.13823)
*Haonan Jiang,Yuji Wang,Yongjie Zhu,Xin Lu,Wenyu Qin,Meng Wang,Pengfei Wan,Yansong Tang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于推理驱动的通用多模态嵌入（UME）框架，通过Embedder-Guided Reinforcement Learning（EG-RL）提升多模态模型的检索与匹配能力，尤其是在有限计算资源条件下也取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）虽然通过生成式Chain-of-Thought（CoT）推理能增强特定任务表示，但往往仅分析文本查询，无法有效提升目标检索相关性，限制了通用多模态嵌入的表现。

Method: 本文提出了推理驱动的UME框架，其核心为Embedder-Guided Reinforcement Learning（EG-RL）：嵌入模块对推理模块进行显式监督，生成面向检索的Traceability CoT（T-CoT），聚焦多模态关键信息，并将此推理证据作为多模态输入进一步优化嵌入。

Result: 在MMEB-V2与UVRB基准任务上，所提方法在有限算力下超越了现有最前沿的嵌入模型，表现出更优的跨模态语义一致性与细粒度匹配能力，同时提升了复杂场景下的泛化性。

Conclusion: 基于推理优化的多模态嵌入模型能在检索和语义理解任务中显著提升性能，是推动 reasoning-driven UME 发展的高效实用方案。

Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

</details>


### [76] [Prior-guided Hierarchical Instance-pixel Contrastive Learning for Ultrasound Speckle Noise Suppression](https://arxiv.org/abs/2602.13831)
*Zhenyu Bu,Yuanxin Xie,Guang-Quan Zhou*

Main category: cs.CV

TL;DR: 提出一种结合对比学习和Transformer-CNN混合架构的超声图像降噪新方法，在提升去噪性能的同时保留关键结构信息，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声成像中的散斑噪声影响图像质量和诊断准确性，但去噪过程中难以在抑制噪声和保持结构细节间取得平衡。

Method: 提出了先验引导的分层实例-像素对比学习模型，包括基于统计引导的像素级对比学习提升局部结构一致性，以及引入memory bank进行实例级对比学习提高全局特征表达；采用Transformer-CNN混合网络结合全局上下文建模与局部细节恢复。

Result: 在两个人工可获取的公开超声数据集上的大量实验表明，该模型在去噪效果和结构保持等方面均优于现有主流方法。

Conclusion: 所提方法能够有效抑制噪声并保留图像结构信息，为超声成像降噪提供了新的高效策略。

Abstract: Ultrasound denoising is essential for mitigating speckle-induced degradations, thereby enhancing image quality and improving diagnostic reliability. Nevertheless, because speckle patterns inherently encode both texture and fine anatomical details, effectively suppressing noise while preserving structural fidelity remains a significant challenge. In this study, we propose a prior-guided hierarchical instance-pixel contrastive learning model for ultrasound denoising, designed to promote noise-invariant and structure-aware feature representations by maximizing the separability between noisy and clean samples at both pixel and instance levels. Specifically, a statistics-guided pixel-level contrastive learning strategy is introduced to enhance distributional discrepancies between noisy and clean pixels, thereby improving local structural consistency. Concurrently, a memory bank is employed to facilitate instance-level contrastive learning in the feature space, encouraging representations that more faithfully approximate the underlying data distribution. Furthermore, a hybrid Transformer-CNN architecture is adopted, coupling a Transformer-based encoder for global context modeling with a CNN-based decoder optimized for fine-grained anatomical structure restoration, thus enabling complementary exploitation of long-range dependencies and local texture details. Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

</details>


### [77] [High-Fidelity Causal Video Diffusion Models for Real-Time Ultra-Low-Bitrate Semantic Communication](https://arxiv.org/abs/2602.13837)
*Cem Eteke,Batuhan Tosun,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: 该论文提出了一种能在极低比特率下进行高保真、因果性和实时视频生成的视频扩散模型，兼顾通信约束和视频质量。


<details>
  <summary>Details</summary>
Motivation: 移动设备与网络通信常常受限于带宽，而传统视频编码难以在极低比特率下保证视频的高质量和语义一致性。因此，作者希望构建一种能在极低比特率下，通过更智能的编码和生成机制，实现高质量视频通信的方法。

Method: 作者设计了基于语义视频编码，将场景结构以“有损语义”方式编码，并补充极高压缩低分辨率帧提供基本纹理信息。基于这些输入，提出了带有语义控制、还原适配器、时序适配器的视频扩散生成模型。同时，引入高效的时序蒸馏过程，实现模型的实时性与因果性，极大地压缩了模型参数并提升了训练效率。

Result: 实验结果表明，所提框架在多个数据集上能以极低比特率（<0.0003bpp）实现出色的感知质量、语义一致性和时序连贯性，综合评测超越了现有的传统、神经网络及生成式视频压缩方法。

Conclusion: 本方法为低比特率下的视频语义通信提供了新途径，在参数量、训练效率和视频质量三方面实现了综合突破，对实际应用具有重要意义。

Abstract: We introduce a video diffusion model for high-fidelity, causal, and real-time video generation under ultra-low-bitrate semantic communication constraints. Our approach utilizes lossy semantic video coding to transmit the semantic scene structure, complemented by a stream of highly compressed, low-resolution frames that provide sufficient texture information to preserve fidelity. Building on these inputs, we introduce a modular video diffusion model that contains Semantic Control, Restoration Adapter, and Temporal Adapter. We further introduce an efficient temporal distillation procedure that enables extension to real-time and causal synthesis, reducing trainable parameters by 300x and training time by 2x, while adhering to communication constraints. Evaluated across diverse datasets, the framework achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in extensive quantitative, qualitative, and subjective evaluations.

</details>


### [78] [Automated Prediction of Paravalvular Regurgitation before Transcatheter Aortic Valve Implantation](https://arxiv.org/abs/2602.13842)
*Michele Cannito,Riccardo Renzulli,Adson Duarte,Farzad Nikfam,Carlo Alberto Barbano,Enrico Chiesa,Francesco Bruno,Federico Giacobbe,Wojciech Wanha,Arturo Giordano,Marco Grangetto,Fabrizio D'Ascenzo*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法，通过术前心脏CT预测TAVI术后常见并发症PVR，显示3D卷积神经网络具有良好预测能力。


<details>
  <summary>Details</summary>
Motivation: TAVI是老年重度主动脉瓣狭窄常用的微创治疗方式，但术后PVR发生率较高，且严重影响预后。术前如果能准确预测PVR风险，将有助于个体化治疗和评估。

Method: 收集了手术前TAVI患者的心脏CT数据，采用三维卷积神经网络（3D CNN）对各项体积影像数据进行训练，旨在利用自动化影像特征挖掘预测术后是否会发生PVR。

Result: 结果表明，所建的体积深度学习模型能准确识别CT影像中的解剖特征，对PVR的预测具有较好表现，优于传统方法。

Conclusion: 基于深度学习的影像分析方法能改善TAVI术后PVR风险的预测，有助于推进个体化风险评估和手术优化。

Abstract: Severe aortic stenosis is a common and life-threatening condition in elderly patients, often treated with Transcatheter Aortic Valve Implantation (TAVI). Despite procedural advances, paravalvular aortic regurgitation (PVR) remains one of the most frequent post-TAVI complications, with a proven impact on long-term prognosis.
  In this work, we investigate the potential of deep learning to predict the occurrence of PVR from preoperative cardiac CT. To this end, a dataset of preoperative TAVI patients was collected, and 3D convolutional neural networks were trained on isotropic CT volumes. The results achieved suggest that volumetric deep learning can capture subtle anatomical features from pre-TAVI imaging, opening new perspectives for personalized risk assessment and procedural optimization. Source code is available at https://github.com/EIDOSLAB/tavi.

</details>


### [79] [Synthetic Dataset Generation and Validation for Robotic Surgery Instrument Segmentation](https://arxiv.org/abs/2602.13844)
*Giorgio Chiesa,Rossella Borra,Vittorio Lauro,Sabrina De Cillis,Daniele Amparore,Cristian Fiori,Riccardo Renzulli,Marco Grangetto*

Main category: cs.CV

TL;DR: 本论文提出了一套用于生成和验证手术机器人器械分割合成数据集的完整流程，并展示合成与真实数据混合训练对提升模型泛化能力的优势。


<details>
  <summary>Details</summary>
Motivation: 现实手术场景中获取标注数据成本极高，严重制约了基于深度学习的手术器械分割方法的发展。作者希望通过合成数据低成本地推动手术机器人视觉任务的数据集构建和模型研究。

Method: 作者基于Da Vinci手术机器人臂的3D重建模型，在Autodesk Maya中通过自动Python管线生成具有真实感、像素精确标签的视频序列。每组场景结合了随机化运动、光照和合成血液效果，以模拟手术中变化多端的环境。通过有控制地混合真实与合成数据，训练多个分割模型，并对模型在不同数据比例下的性能进行评估。

Result: 实验显示，用合成数据与真实数据按一定比例混合训练，能显著提升分割模型的泛化性，优于单用真实数据；但若过度依赖合成数据会导致明显的领域偏移。

Conclusion: 该合成数据生成与验证流程为手术机器人视觉任务提供了一种可复现、可扩展的工具，有助于促进数据增强、领域自适应和基于仿真的模型预训练等相关研究。数据和代码已在线公开。

Abstract: This paper presents a comprehensive workflow for generating and validating a synthetic dataset designed for robotic surgery instrument segmentation. A 3D reconstruction of the Da Vinci robotic arms was refined and animated in Autodesk Maya through a fully automated Python-based pipeline capable of producing photorealistic, labeled video sequences. Each scene integrates randomized motion patterns, lighting variations, and synthetic blood textures to mimic intraoperative variability while preserving pixel-accurate ground truth masks. To validate the realism and effectiveness of the generated data, several segmentation models were trained under controlled ratios of real and synthetic data. Results demonstrate that a balanced composition of real and synthetic samples significantly improves model generalization compared to training on real data only, while excessive reliance on synthetic data introduces a measurable domain shift. The proposed framework provides a reproducible and scalable tool for surgical computer vision, supporting future research in data augmentation, domain adaptation, and simulation-based pretraining for robotic-assisted surgery. Data and code are available at https://github.com/EIDOSLAB/Sintetic-dataset-DaVinci.

</details>


### [80] [Cardiac Output Prediction from Echocardiograms: Self-Supervised Learning with Limited Data](https://arxiv.org/abs/2602.13846)
*Adson Duarte,Davide Vitturini,Emanuele Milillo,Andrea Bragagnolo,Carlo Alberto Barbano,Riccardo Renzulli,Michele Cannito,Federico Giacobbe,Francesco Bruno,Ovidio de Filippo,Fabrizio D'Ascenzo,Marco Grangetto*

Main category: cs.CV

TL;DR: 本文提出利用自监督学习方法（SimCLR）在有限心脏超声视频数据集上进行预训练，以提升心输出量非侵入性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统心输出量（CO）评估需采用侵入性、耗时的右心导管检查，临床迫切需要可靠、无创的替代方案。心脏超声虽为非侵入性工具，但在受限数据下CO预测精度不足。因此需要新方法提升模型在小数据量场景下的泛化能力。

Method: 采用以SimCLR为基础的自监督预训练策略，在仅有下游任务有限样本的小型数据集上预训练超声视频表示，随后进行心输出量预测任务微调，并与当前主流大规模训练模型PanEcho进行对比。

Result: 自监督学习显著缓解了过拟合，提升了特征表示能力。所提方法在测试集上取得平均Pearson相关系数0.41，优于采用百万例大数据训练的PanEcho模型。

Conclusion: 即使在数据稀缺情况下，自监督学习预训练也能显著提升心脏超声CO预测效果，为非侵入性心血管功能评估提供了新方向。

Abstract: Cardiac Output (CO) is a key parameter in the diagnosis and management of cardiovascular diseases. However, its accurate measurement requires right-heart catheterization, an invasive and time-consuming procedure, motivating the development of reliable non-invasive alternatives using echocardiography. In this work, we propose a self-supervised learning (SSL) pretraining strategy based on SimCLR to improve CO prediction from apical four-chamber echocardiographic videos. The pretraining is performed using the same limited dataset available for the downstream task, demonstrating the potential of SSL even under data scarcity. Our results show that SSL mitigates overfitting and improves representation learning, achieving an average Pearson correlation of 0.41 on the test set and outperforming PanEcho, a model trained on over one million echocardiographic exams. Source code is available at https://github.com/EIDOSLAB/cardiac-output.

</details>


### [81] [Low-Pass Filtering Improves Behavioral Alignment of Vision Models](https://arxiv.org/abs/2602.13859)
*Max Wolff,Thomas Klein,Evgenia Rusak,Felix Wichmann,Wieland Brendel*

Main category: cs.CV

TL;DR: 本文发现，通过对输入图像进行低通滤波（如模糊），可以显著提高深度神经网络（DNN）与人类视觉行为的一致性，尤其是在错误模式和形状偏好方面。该方法甚至优于现有的生成模型对齐方案。


<details>
  <summary>Details</summary>
Motivation: 现有的DNN尽管在视觉任务上表现优异，但与人类视觉行为在错误一致性和形状偏好上仍有差距。部分研究认为生成式模型能大幅提升这种对齐，但其原因和背后机制仍不清楚。该研究旨在探讨提升DNN与人类行为对齐的关键因素。

Method: 作者通过系统性实验，对比了生成模型中隐含的图像缩放（即低通滤波）操作与对判别模型（如CLIP）直接进行图像低通滤波（模糊）后的行为表现，测试其与人类的一致性。此外，还通过直接优化滤波器以最大化对齐性能，验证低通滤波是否为最优方案。

Result: 实验表明，对判别模型输入图像在测试时进行模糊处理，能够大幅提升模型与人类行为在模型-人类对齐基准测试中的表现，显著缩小差距。通过优化滤波器，可以找到Pareto最优解，这些滤波器的频率特性与人类视觉系统的带通特性接近。

Conclusion: DNN与人类视觉行为差距的核心因素是对高频信息的处理；低通滤波（或模糊）能有效提升两者的对齐度。最优的高斯滤波器频谱也近似于人类视觉的带通特性。这表明简单的感知前处理就能大幅提升人工视觉模型的生物合理性。

Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \emph{generative} -- rather than \emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.
  Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.
  We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.

</details>


### [82] [Human-Aligned Evaluation of a Pixel-wise DNN Color Constancy Model](https://arxiv.org/abs/2602.13887)
*Hamed Heidari-Gorji,Raquel Gil Rodriguez,Karl R. Gegenfurtner*

Main category: cs.CV

TL;DR: 本研究将深度神经网络(DNN)与人类实验对比，分析不同场景下颜色恒常性的表现。结果显示，模型与人类在基本条件下都表现出高度一致的颜色恒常性，对线索剥离时也有相似下降。


<details>
  <summary>Details</summary>
Motivation: 颜色恒常性是指无论照明条件如何变化，人类仍能感知物体的本色。作者希望利用深度学习模型，比较和分析其与人类在颜色恒常性任务中的表现，检验经典颜色恒常性机制的作用。

Method: 使用之前开发的基于ResNet的U-Net模型，在渲染图像上预训练以预测表面反射率，然后通过迁移学习，仅微调解码器，应对基线虚拟现实(VR)条件。采用和人类实验一样的无色物体选择任务，模型输出用于跨所有实验条件的对比分析。

Result: 模型和人类都在基线条件下表现出高水平的颜色恒常性。当剥离局部环境或空间平均颜色线索时，二者的表现都随之下降，且下降趋势相似。二者在不同条件下的表现高度一致。

Conclusion: 深度学习模型在颜色恒常性任务中能够很大程度上模拟和解释人类的知觉机制，尤其在关键线索变化时反应与人类一致。

Abstract: We previously investigated color constancy in photorealistic virtual reality (VR) and developed a Deep Neural Network (DNN) that predicts reflectance from rendered images. Here, we combine both approaches to compare and study a model and human performance with respect to established color constancy mechanisms: local surround, maximum flux and spatial mean. Rather than evaluating the model against physical ground truth, model performance was assessed using the same achromatic object selection task employed in the human experiments. The model, a ResNet based U-Net from our previous work, was pre-trained on rendered images to predict surface reflectance. We then applied transfer learning, fine-tuning only the network's decoder on images from the baseline VR condition. To parallel the human experiment, the model's output was used to perform the same achromatic object selection task across all conditions. Results show a strong correspondence between the model and human behavior. Both achieved high constancy under baseline conditions and showed similar, condition-dependent performance declines when the local surround or spatial mean color cues were removed.

</details>


### [83] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: 本论文提出了一种高效准确的字体家族分类系统，能识别394种字体，模型与训练数据均开源。


<details>
  <summary>Details</summary>
Motivation: 当前字体识别面临字体数量多、图片多样性大且标注数据稀缺的问题。作者希望提升大规模字体分类准确率，同时降低训练成本。

Method: 采用DINOv2 Vision Transformer做基干模型，通过LoRA（低秩适应）方法微调，仅训练不到1%的模型参数。此外，构建了大规模合成字体图像数据集，涵盖多种字体增强手段如颜色、对齐、换行、噪声等，提升对真实场景的泛化能力；模型预处理确保训练和应用一致性，并部署为在线推理接口。

Result: 在394个字体家族分类任务上，模型达到了约86%的Top-1准确率，并且训练参数极少。相关模型、数据集与训练流程已开源。

Conclusion: 所提出的方法在保持高准确率的同时显著减少了训练负担，证明了LoRA微调和广泛数据增强对字体分类的有效性。该成果有望推进字体识别和相关智能排版领域应用。

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [84] [MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction](https://arxiv.org/abs/2602.13930)
*Ruggiero Santeramo,Igor Zubarev,Florian Jug*

Main category: cs.CV

TL;DR: 该论文提出了一种融合CNN与Transformer结构、并显式建模双侧乳腺不对称性的乳腺癌风险预测模型MamaDino，在分辨率大大降低的乳腺X光图像上也能实现与主流高分辨率模型（如Mirai）相当的预测效果。


<details>
  <summary>Details</summary>
Motivation: 当前乳腺癌筛查希望实现个性化风险预测。深度学习模型已超越传统方法，但主流方法需高分辨率输入且未充分建模双侧乳腺差异，带来硬件负担与建模局限。作者希望探索能否用更低分辨率但更有效结构来恢复现有最优水平。

Method: 提出MamaDino模型：采用CNN与Transformer结合的网络结构，输入为512x512分辨率的乳腺影像，利用双侧特征融合模块（BilateralMixer）显式提取左右乳腺不对称信息，进行3年风险预测。在5万多名OPTIMAM数据集女性中训练，并在内部与外部队列测试。

Result: MamaDino在仅使用Mirai的1/13输入像素前提下，在内外部队列的3年风险预测AUC指标上与Mirai相当。加入BilateralMixer后，模型判别力进一步提升，AUC分别从0.713提升到0.736（内部队列），0.666提升到0.677（外部队列），且在不同年龄、种族、设备类型及肿瘤亚型中表现稳定。

Conclusion: 通过使用结构化的信息融合和对双侧乳腺进行显式建模，即使在低分辨率下也可获得与主流高分辨率模型持平的乳腺癌风险预测准确性，为普及和降低筛查门槛提供了新思路。

Abstract: Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry.
  We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy.
  We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site.
  At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.

</details>


### [85] [Fusing Pixels and Genes: Spatially-Aware Learning in Computational Pathology](https://arxiv.org/abs/2602.13944)
*Minghao Han,Dingkang Yang,Linhao Qu,Zizhi Chen,Gang Li,Han Wang,Jiacong Wang,Lihua Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态表型学习框架STAMP，通过结合空间转录组信息，改进了病理图像的表征学习，在多个数据集和任务上都获得了优秀表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态病理学习方法主要依赖于视觉和语言信息，但语言缺乏分子层面特异性，不能充分反映分子病理特征，导致模型表征能力受限。如何利用更具分子信息深度的监督信号提升病理图像的理解和表征，是该领域的关键挑战。

Method: 作者提出STAMP框架，首次将空间转录组（spatial transcriptomics）数据引入病理多模态学习中。具体方法包括：自监督、基因指导的训练策略，引入空间结构和多尺度特征，开发了层级多尺度对比对齐与跨尺度补丁定位机制。同时，作者构建了目前最大规模的基于Visium的空间转录组数据集SpaVis-6M，并在其上训练空间感知基因编码器。

Result: STAMP框架在六个数据集、四个下游任务上均表现出强大、稳定的性能，证明其分子指导的空间监督对病理多模态表征学习价值巨大。

Conclusion: 结合空间分辨的分子（基因表达）信息对于提升病理多模态学习具有重要意义。STAMP为融合病理图像与分子数据提供了新的范式，展示了空间分子监督的优势和必要性，推动了计算病理学的多模态发展。

Abstract: Recent years have witnessed remarkable progress in multimodal learning within computational pathology. Existing models primarily rely on vision and language modalities; however, language alone lacks molecular specificity and offers limited pathological supervision, leading to representational bottlenecks. In this paper, we propose STAMP, a Spatial Transcriptomics-Augmented Multimodal Pathology representation learning framework that integrates spatially-resolved gene expression profiles to enable molecule-guided joint embedding of pathology images and transcriptomic data. Our study shows that self-supervised, gene-guided training provides a robust and task-agnostic signal for learning pathology image representations. Incorporating spatial context and multi-scale information further enhances model performance and generalizability. To support this, we constructed SpaVis-6M, the largest Visium-based spatial transcriptomics dataset to date, and trained a spatially-aware gene encoder on this resource. Leveraging hierarchical multi-scale contrastive alignment and cross-scale patch localization mechanisms, STAMP effectively aligns spatial transcriptomics with pathology images, capturing spatial structure and molecular variation. We validate STAMP across six datasets and four downstream tasks, where it consistently achieves strong performance. These results highlight the value and necessity of integrating spatially resolved molecular supervision for advancing multimodal learning in computational pathology. The code is included in the supplementary materials. The pretrained weights and SpaVis-6M are available at: https://github.com/Hanminghao/STAMP.

</details>


### [86] [MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars](https://arxiv.org/abs/2602.13961)
*Shuoyuan Wang,Yiran Wang,Hongxin Wei*

Main category: cs.CV

TL;DR: 本文提出了MarsRetrieval，一个面向火星地理发现的视觉-语言模型检索基准，支持多种任务并检验多模态嵌入模型的能力。实验结果表明，通用大模型在火星领域表现有限，领域微调不可或缺。


<details>
  <summary>Details</summary>
Motivation: 当前火星探测及行星科学中，深度学习等数据驱动方法发展迅速，但主流基准都局限于封闭集监督视觉任务，缺乏支持文本引导检索地理发现的功能。

Method: 作者构建了MarsRetrieval基准，包括配对图像-文本检索、地貌检索和全球地理定位三类任务，选用对比学习式双塔编码器与生成式视觉-语言模型进行统一检索评测。

Result: 评估显示，MarsRetrieval任务具有较高挑战性，现有强大基础视觉-语言模型难以准确区分火星领域的地貌差异。

Conclusion: 火星等行星场景下的地理发现任务需要领域特定微调，通用模型无法直接满足高要求，MarsRetrieval为相关方法研究与发展提供了新颖标准。

Abstract: Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval

</details>


### [87] [Elastic Diffusion Transformer](https://arxiv.org/abs/2602.13993)
*Jiangshan Wang,Zeqiang Lai,Jiarui Chen,Jiayi Guo,Hang Guo,Xiu Li,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为E-DiT的弹性扩散Transformer加速框架，通过动态跳过部分计算和缩减MLP宽度，在保证生成质量的同时大幅提升了DiT生成模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的DiT加速方法通常计算容量固定，导致加速效果有限且生成质量下降。作者注意到DiT生成时存在稀疏性，部分计算可以安全跳过，因此希望设计一种能够自适应加速、动态利用稀疏性的方案。

Method: E-DiT为每个DiT block引入轻量级Router组件，根据输入的latent信息动态决定该block是否跳过、MLP宽度缩减比例。并在推理时通过Router实现block级特征缓存，无需重新训练即可消除冗余计算。

Result: 在多组2D和3D数据集上（如Qwen-Image、FLUX和Hunyuan3D-3.0）实验表明E-DiT在几乎不损失生成质量的前提下，最高可实现2倍加速。

Conclusion: E-DiT通过自适应、训练无关的稀疏激活和MLP降宽机制，有效提升DiT模型推理效率，为高质量高效生成任务提供了新思路。

Abstract: Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\sim$2$\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.

</details>


### [88] [Inject Where It Matters: Training-Free Spatially-Adaptive Identity Preservation for Text-to-Image Personalization](https://arxiv.org/abs/2602.13994)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调的新型空间自适应身份调制框架SpatialID，实现了更精确的人脸身份注入，显著提升了文本-图像生成的一致性和质量，有效避免了背景污染。


<details>
  <summary>Details</summary>
Motivation: 现有免调优方法通过空间上均匀注入身份信息，导致身份特征扩散到非人脸区域（如背景、光照），影响生成图像的文本一致性和图像质量。本研究旨在高效、低成本地解决身份信息污染非人脸区域的问题。

Method: 作者设计了SpatialID框架，将身份注入空间上区分为人脸相关和与环境无关的区域，利用由交叉注意力响应推导的空间掩码进行精确区域调制，并提出时空动态调度策略，结合高斯先验、注意力掩码和自适应松弛，动态适应扩散生成过程。

Result: 在IBench数据集的实验中，SpatialID在文本一致性（CLIP-T: 0.281）、视觉一致性（CLIP-I: 0.827）和图像质量（IQ: 0.523）上均达到当前最佳水平，有效消除了背景污染，身份保留效果强。

Conclusion: SpatialID方法实现了高效、精确的人脸身份注入与环境解耦，在保障身份一致性的同时，显著提升了文本依从性和图像质量，为个性化文本到图像生成领域提供了新思路。

Abstract: Personalized text-to-image generation aims to integrate specific identities into arbitrary contexts. However, existing tuning-free methods typically employ Spatially Uniform Visual Injection, causing identity features to contaminate non-facial regions (e.g., backgrounds and lighting) and degrading text adherence. To address this without expensive fine-tuning, we propose SpatialID, a training-free spatially-adaptive identity modulation framework. SpatialID fundamentally decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses. Furthermore, we introduce a Temporal-Spatial Scheduling strategy that dynamically adjusts spatial constraints - transitioning from Gaussian priors to attention-based masks and adaptive relaxation - to align with the diffusion generation dynamics. Extensive experiments on IBench demonstrate that SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523), significantly eliminating background contamination while maintaining robust identity preservation.

</details>


### [89] [A Deployment-Friendly Foundational Framework for Efficient Computational Pathology](https://arxiv.org/abs/2602.14010)
*Yu Cai,Cheng Jin,Jiabo Ma,Fengtao Zhou,Yingxue Xu,Zhengrui Guo,Yihui Wang,Zhengyu Zhang,Ling Liang,Yonghao Tan,Pingcheng Dong,Du Cai,On Ki Tang,Chenglong Zhao,Xi Wang,Can Yang,Yali Xu,Jing Cui,Zhenhui Li,Ronald Cheong Kin Chan,Yueping Liu,Feng Gao,Xiuming Zhang,Li Liang,Hao Chen,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: 本文提出了LitePath方案，使大规模病理基础模型能够在低算力设备上以低能耗、低成本实现高效的病理图像分析，同时保持与最先进方法接近的分析准确率。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型参数量巨大、推理计算量高，导致临床实际部署难度大，难以在普通或者低功耗硬件上普及应用，因此亟需一种兼具高效性与高准确率的模型优化与部署方案。

Method: 提出了LitePath框架，包括通过知识蒸馏获得的轻量化模型LiteFM（整合了Virchow2、H-Optimus-1和UNI2三大基础模型、以1.9亿patch训练）、以及可自适应任务的轻量Patch选择器APS，大幅减少模型参数和计算量，并提出Deployability Score衡量效率与准确率的平衡。

Result: LitePath相较Virchow2参数减少28倍、FLOPs降低403.5倍，可在NVIDIA Jetson Orin Nano Super等低功耗设备上以208 slides/小时处理速度部署，能耗显著降低。横跨多个器官、多任务共37个队列、15700多张切片测试，LitePath模型准确率名列第二，整体AUC达Virchow2模型99.71%，超越多种大模型。

Conclusion: LitePath实现了既快速又低耗能的病理AI分析，在可普及硬件上依然能保持与主流大模型相当的准确度，并有效降低碳足迹，对实际临床部署有显著推动作用。

Abstract: Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.

</details>


### [90] [Flow4R: Unifying 4D Reconstruction and Tracking with Scene Flow](https://arxiv.org/abs/2602.14021)
*Shenhan Qian,Ganlin Zhang,Shangzhe Wu,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了一种基于场景流的新方法Flow4R，用于动态三维场景的重建和追踪，实现了对空间与时间信息的统一建模，达到更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建和追踪方法多为将几何信息和运动信息解耦处理，需静态假设或依赖独立的相机姿态估计，不能直接统一处理动态场景中的结构与运动。

Method: Flow4R以场景流（scene flow）作为核心表示，利用ViT（Vision Transformer）网络，从两帧视角输入中同时预测每像素的三维点位置、场景流、姿态权重和置信度，实现几何与运动的对称联合推理，消除了对显式姿态回归器和捆绑调整的依赖。

Result: Flow4R在静态和动态数据集上联合训练，达到了4D重建和追踪任务上的最新性能，显示出基于场景流的统一表示对于时空场景理解的有效性。

Conclusion: 基于场景流的统一表示方法实现了动态三维场景重建与追踪任务的高效、准确解决，对时空场景分析具有推广和应用前景。

Abstract: Reconstructing and tracking dynamic 3D scenes remains a fundamental challenge in computer vision. Existing approaches often decouple geometry from motion: multi-view reconstruction methods assume static scenes, while dynamic tracking frameworks rely on explicit camera pose estimation or separate motion models. We propose Flow4R, a unified framework that treats camera-space scene flow as the central representation linking 3D structure, object motion, and camera motion. Flow4R predicts a minimal per-pixel property set-3D point position, scene flow, pose weight, and confidence-from two-view inputs using a Vision Transformer. This flow-centric formulation allows local geometry and bidirectional motion to be inferred symmetrically with a shared decoder in a single forward pass, without requiring explicit pose regressors or bundle adjustment. Trained jointly on static and dynamic datasets, Flow4R achieves state-of-the-art performance on 4D reconstruction and tracking tasks, demonstrating the effectiveness of the flow-central representation for spatiotemporal scene understanding.

</details>


### [91] [Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation](https://arxiv.org/abs/2602.14027)
*Jia Li,Xiaomeng Fu,Xurui Peng,Weifeng Chen,Youwei Zheng,Tianyu Zhao,Jiexi Wang,Fangmin Chen,Xing Wang,Hayden Kwok-Hay So*

Main category: cs.CV

TL;DR: 本论文提出了一种训练时无需改动的新方法FLEX，有效提升了自回归视频扩散模型在生成超长视频时的表现，极大地缓解了推理过程中的误差累积和时间退化问题。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型虽然可扩展用于长视频生成，但在延长生成长度时，由于3D位置嵌入的频谱偏置和噪声采样缺乏动态先验，容易迅速产生误差累积导致结果质量急剧下降。为解决长时生成稳定性瓶颈，需要新的推理机制。

Method: 提出FLEX方法，在推理阶段通过Frequency-aware RoPE Modulation、自适应插值低频分量并外推高频分量来保护多尺度时序判别性，同时结合Antiphase Noise Sampling注入高频动态先验，以及Inference-only Attention Sink用于全局结构锚定，实现了短期训练和长期推理间的桥接。

Result: FLEX在VBench长视频基准上显著超越最先进模型，在6倍推断长度（30秒）下表现领先，并在12倍尺度（60秒）下达到长视频微调基线的性能。FLEX作为即插即用组件，能平滑集成现有模型推理流程，将LongLive等模型生成极限推进至4分钟。

Conclusion: FLEX无需重新训练，通过增强推理过程即可极大延长自回归视频扩散模型的连贯生成时长，缓解频谱偏置与动态不足问题，推动视频生成模型在实际应用中的可用性和拓展性。

Abstract: Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

</details>


### [92] [Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection](https://arxiv.org/abs/2602.14040)
*Abhinav Shukla,Nachiket Tapas*

Main category: cs.CV

TL;DR: 本文提出了一种基于可解释性的层级剪枝框架，用于提升物体检测模型在资源受限平台上的推理效率，同时较好地保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽在目标检测领域取得成功，但其复杂度对嵌入式、边缘设备等资源有限的场景部署带来挑战，现有的基于权重大小的剪枝方法难以真实反映各层对任务性能的实际贡献。

Method: 提出了一种受SHAP启发的梯度-激活归因方法，针对每一层进行重要性评估，作为功能贡献的代理指标，并以此进行层级剪枝，不依赖单一静态权重指标。

Result: 在ResNet-50、MobileNetV2、ShuffleNetV2、Faster R-CNN、RetinaNet及YOLOv8等多种检测结构和COCO 2017 上验证，归因剪枝方法能较传统L1剪枝，识别出不同的不重要层，实现更优的准确率与效率权衡。例如ShuffleNetV2提升推理速度10%，而L1剪枝则导致性能下降13.7%。

Conclusion: 数据驱动的层重要性评估对于模型压缩极为关键。基于可解释性的方法为在边缘/资源受限平台部署目标检测DNNs保持性能与解释性提供了新思路。

Abstract: Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\% mAP drop for a 6.2\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.

</details>


### [93] [BitDance: Scaling Autoregressive Generative Models with Binary Tokens](https://arxiv.org/abs/2602.14041)
*Yuang Ai,Jiaming Han,Shaobin Zhuang,Weijia Mao,Xuefeng Hu,Ziyan Yang,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen*

Main category: cs.CV

TL;DR: BitDance是一种新的自回归(AR)图像生成器，通过预测高熵二进制视觉token而不是传统的代码本索引，实现更紧凑和表达能力更强的离散表示，在保持高保真图像生成的同时，大幅提升了模型的效率和速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于代码本的AR图像生成模型受限于token空间表达能力和推理速度，尤其是在高分辨率生成和并行加速方面存在瓶颈。研究者希望设计一种更紧凑、高效且表达能力更强的视觉token方式，并提升推理速率。

Method: 提出BitDance，采用二进制高熵token(一token可表达2^256状态)。模型用连续空间扩散模型(而非softmax)生成二进制token，解决巨大token空间下采样的问题。提出next-patch diffusion，实现多个token并行高精度生成，大幅加速推理。还将BitDance应用于大规模文本-图像生成任务。

Result: 在ImageNet 256x256上，BitDance取得1.24的FID表现，为当前AR模型最佳；在使用5.4倍更少参数(260M)、8.7倍速度情况下，超越同级别并行AR模型(1.4B参数)；在1024x1024高分辨率生成时较前代AR模型有30倍加速。同时展现了高分辨率和写实性能、良好scaling表现。

Conclusion: BitDance极大拓展了自回归图像生成模型在表达能力、并行性和效率上的边界，为高效基础生成模型的研究和应用提供了新思路。相关代码和模型已开源，便于后续深入研究。

Abstract: We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.

</details>


### [94] [Restoration Adaptation for Semantic Segmentation on Low Quality Images](https://arxiv.org/abs/2602.14042)
*Kai Guan,Rongyuan Wu,Shuai Li,Wentao Zhu,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RASS（Restoration Adaptation for Semantic Segmentation）的新方法，有效提升了低质量（LQ）图像语义分割的鲁棒性，通过结合语义约束的图像复原和分割过程，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景下，图像受质量损失影响严重，语义分割性能大幅下降。传统图像复原方法更关注像素还原，忽略了对分割有帮助的语义信息，而分割模型又缺乏处理退化图像的能力。作者旨在弥合图像复原与语义分割的隔阂，提高低质量图像的分割效果。

Method: 提出RASS框架，包含两个核心：1）语义约束复原（SCR）模型，通过对齐cross-attention map与分割掩码，将分割先验注入复原网络，实现语义一致的图像重建；2）利用LoRA模块融合及特定任务微调，将语义复原能力迁移到分割模型，增强其对低质量图像的适应能力。

Result: 作者构建了真实低质量图像分割数据集，并在合成及真实LQ基准实验，验证RASS和SCR在分割与复原上均显著优于最新方法。

Conclusion: 将语义约束引入图像复原、并与分割任务深度融合，能够大幅提升低质量图像的分割性能；所提方法具备泛化性与实用价值。

Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.

</details>


### [95] [CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning](https://arxiv.org/abs/2602.14068)
*Yuhui Wu,Chenxi Xie,Ruibin Li,Liyi Chen,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于图像内容一致性编辑的后训练框架CoCoEdit，并通过区域正则化强化学习显著提高了编辑后的内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模生成模型在图像编辑时，往往会意外改变非目标区域导致内容不一致。论文旨在解决现有方法中非目标区域不必要变化的问题，提升内容一致性。

Method: 1. 增强现有编辑数据集并精细标注指令和掩码，构建高质量训练集（4万样本）。2. 引入像素级相似性奖励，补充基于多模态大模型的奖励机制，确保编辑质量和内容一致性。3. 针对奖励空间不区分区域的问题，提出区域正则化器，用于奖励高回馈的非编辑区域保存和鼓励低回馈样本的编辑效果。4. 增加像素级相似度评价指标用于性能评测。

Result: 将CoCoEdit应用于Qwen-Image-Edit和FLUX-Kontext两个模型后，获得了与先进模型相当的编辑分数，而且内容一致性明显优于它们，具体通过PSNR/SSIM及人工主观评价指标体现。

Conclusion: CoCoEdit能有效提升大模型在保持内容一致性的前提下完成高质量图像编辑，为后续图像编辑模型的训练和优化提供了新思路。

Abstract: Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.

</details>


### [96] [ForgeryVCR: Visual-Centric Reasoning via Efficient Forensic Tools in MLLMs for Image Forgery Detection and Localization](https://arxiv.org/abs/2602.14098)
*Youqi Wang,Shen Chen,Haowei Wang,Rongxuan Peng,Taiping Yao,Shunquan Tan,Changsheng Chen,Bin Li,Shouhong Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法ForgeryVCR，通过引入视觉取证工具和视觉中心推理，有效提升了多模态大模型在图像伪造检测和定位任务中的表现，克服了以文本为主的思考方式导致的细粒度伪造痕迹表达不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在检测和定位图像伪造时，多依赖文本链式推理，对低层次、难以觉察的伪造痕迹描述不充分，容易出现虚假推断。因此，亟需更适合视觉推理的模型和方法。

Method: 提出ForgeryVCR框架，融合取证工具，通过视觉中心推理生成可视化中间结果。提出战略性工具学习的后训练范式，并结合有监督微调和基于工具效用奖励的强化学习，提升模型自发调用多视角推理路径（如局部放大、噪声残差、频域分析等）能力。

Result: ForgeryVCR在大规模实验中，在图像伪造检测和定位任务中都取得了当前最优的表现，并展现出极强的泛化能力和鲁棒性，同时调动的工具数量极少，效率高。

Conclusion: ForgeryVCR通过引入视觉推理和高效工具利用，突破了文本链式推理的局限，显著提升了多模态模型对于图像伪造检测和定位的综合能力。

Abstract: Existing Multimodal Large Language Models (MLLMs) for image forgery detection and localization predominantly operate under a text-centric Chain-of-Thought (CoT) paradigm. However, forcing these models to textually characterize imperceptible low-level tampering traces inevitably leads to hallucinations, as linguistic modalities are insufficient to capture such fine-grained pixel-level inconsistencies. To overcome this, we propose ForgeryVCR, a framework that incorporates a forensic toolbox to materialize imperceptible traces into explicit visual intermediates via Visual-Centric Reasoning. To enable efficient tool utilization, we introduce a Strategic Tool Learning post-training paradigm, encompassing gain-driven trajectory construction for Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) optimization guided by a tool utility reward. This paradigm empowers the MLLM to act as a proactive decision-maker, learning to spontaneously invoke multi-view reasoning paths including local zoom-in for fine-grained inspection and the analysis of invisible inconsistencies in compression history, noise residuals, and frequency domains. Extensive experiments reveal that ForgeryVCR achieves state-of-the-art (SOTA) performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy. The project page is available at https://youqiwong.github.io/projects/ForgeryVCR/.

</details>


### [97] [GeoFusionLRM: Geometry-Aware Self-Correction for Consistent 3D Reconstruction](https://arxiv.org/abs/2602.14119)
*Ahmet Burak Yildirim,Tuna Saygin,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: 本文提出了GeoFusionLRM框架，利用模型自身的几何预测结果自我校正，显著提升了单张图像3D重建的几何精度与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在单图3D重建中，往往存在几何不一致、细节错位等问题，影响重建品质。亟需能充分利用重建模型自身几何信息进行自我校正的方法，以提升重建效果。

Method: GeoFusionLRM利用模型自生成的法向量和深度图，通过专门设计的transformer和特征融合模块，将几何信息反馈给重建网络，实现结构自校正，增强与输入图像的几何一致性。该方法无需额外监督或外部信号。

Result: 大量实验表明，GeoFusionLRM在几何锐利度、法线一致性和模型保真度方面，均优于最新的大参数模型基线。

Conclusion: GeoFusionLRM能显著提升单图3D重建的大模型精度和几何一致性，为无监督几何自校正提供了有效思路。

Abstract: Single-image 3D reconstruction with large reconstruction models (LRMs) has advanced rapidly, yet reconstructions often exhibit geometric inconsistencies and misaligned details that limit fidelity. We introduce GeoFusionLRM, a geometry-aware self-correction framework that leverages the model's own normal and depth predictions to refine structural accuracy. Unlike prior approaches that rely solely on features extracted from the input image, GeoFusionLRM feeds back geometric cues through a dedicated transformer and fusion module, enabling the model to correct errors and enforce consistency with the conditioning image. This design improves the alignment between the reconstructed mesh and the input views without additional supervision or external signals. Extensive experiments demonstrate that GeoFusionLRM achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines.

</details>


### [98] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出了EgoSound，这是首个系统性评估多模态大模型在头戴式（第一人称）声音理解任务上的基准，填补了现有模型在多感官感知领域的空白。


<details>
  <summary>Details</summary>
Motivation: 人类感知世界时是多感官的，不仅依赖视觉，还依赖听觉等其他信息，尤其是在第一人称视角场景中，声音对于空间布局、屏幕外事件及因果推理有极为重要的作用。目前多模态大模型主要聚焦于视觉与语言的结合，缺乏对听觉的全面考察。

Method: 作者构建了EgoSound基准，融合了Ego4D与EgoBlind数据集，涵盖视觉与需依赖声音的情境，定义了7类任务，包括声音感知、本地化、因果推理与跨模态推理等。数据集通过多阶段自动生成流程产生，最终包含7315条经验证的问题-答案对，覆盖900个视频。作者还在9个最新的多模态大模型上进行了系统性实验。

Result: 实验发现，现有多模态大模型已展现出初步的声音推理能力，但在精细的空间理解和因果推理方面表现有限，有较大提升空间。

Conclusion: EgoSound为提升多感官的第一人称智能提供了有挑战性的基准，也为视觉与听觉深度融合奠定了基础，推动多模态理解领域向更全面的多感官感知发展。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [99] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出DenseMLLM模型，在无需复杂任务专用解码器的情况下，让标准多模态大模型（MLLM）完成像语义分割、深度估计等细粒度密集预测任务，并获得领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前，MLLM在高层次视觉理解上表现出色，但应用于密集预测（如语义分割等）通常需额外复杂、针对任务的解码器。这不仅增加模型复杂度，也违背了通用型设计，影响实用性。作者希望解决这个矛盾，简化架构。

Method: DenseMLLM采用标准MLLM架构，核心创新是引入新的视觉token监督策略，实现多标签、多任务的密集预测，无需专用解码器。方法强调最小化模型改动，用监督机制适配相关任务。

Result: 在多种密集预测和视觉-语言基准测试上，DenseMLLM在没有特殊解码器的前提下取得了非常有竞争力的表现。

Conclusion: 标准通用型MLLM通过合适的视觉token监督，可以无需架构专化地胜任密集预测任务，模型更加通用和简洁，拓展了MLLM的适用范围。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [100] [Detection of On-Ground Chestnuts Using Artificial Intelligence Toward Automated Picking](https://arxiv.org/abs/2602.14140)
*Kaixuan Fang,Yuzhen Lu,Xinyang Mu*

Main category: cs.CV

TL;DR: 本研究通过系统对比多种主流实时目标检测模型，提出并验证了适用于复杂田间环境的板栗地面检测方法，取得了高精度成果，并公开了数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有机械化板栗采收方式成本高、易损果且不适合小规模种植者，迫切需要低成本、基于视觉的自动采收技术。而复杂环境下板栗在地面上的识别受光照、遮蔽、杂草、落叶等干扰物影响，准确检测尚未得到充分解决。

Method: 采集并标注319张田间板栗地面图像（共6524个标注目标），系统测试了YOLO和RT-DETR两大家族共29个主流目标检测模型的不同尺寸和版本，在复杂环境下重复建模评估其板栗检测精度和实时性。

Result: 在所有模型中，YOLOv12m在mAP@0.5上表现最佳（95.1%），RT-DETRv2-R101在RT-DETR家族中最高（91.1%），YOLOv11x在mAP@[0.5:0.95]指标上最高（80.1%）。总体上，YOLO系列在检测精度和推理速度上优于RT-DETR，更适合现场实际部署。

Conclusion: 当前主流的实时目标检测模型可用于田间复杂环境下的板栗检测，YOLO表现最优，为低成本自动采收提供了有力支撑。研究公开了数据集和代码，可促进该领域进一步研究与应用。

Abstract: Traditional mechanized chestnut harvesting is too costly for small producers, non-selective, and prone to damaging nuts. Accurate, reliable detection of chestnuts on the orchard floor is crucial for developing low-cost, vision-guided automated harvesting technology. However, developing a reliable chestnut detection system faces challenges in complex environments with shading, varying natural light conditions, and interference from weeds, fallen leaves, stones, and other foreign on-ground objects, which have remained unaddressed. This study collected 319 images of chestnuts on the orchard floor, containing 6524 annotated chestnuts. A comprehensive set of 29 state-of-the-art real-time object detectors, including 14 in the YOLO (v11-13) and 15 in the RT-DETR (v1-v4) families at varied model scales, was systematically evaluated through replicated modeling experiments for chestnut detection. Experimental results show that the YOLOv12m model achieves the best mAP@0.5 of 95.1% among all the evaluated models, while the RT-DETRv2-R101 was the most accurate variant among RT-DETR models, with mAP@0.5 of 91.1%. In terms of mAP@[0.5:0.95], the YOLOv11x model achieved the best accuracy of 80.1%. All models demonstrate significant potential for real-time chestnut detection, and YOLO models outperformed RT-DETR models in terms of both detection accuracy and inference, making them better suited for on-board deployment. Both the dataset and software programs in this study have been made publicly available at https://github.com/AgFood-Sensing-and-Intelligence-Lab/ChestnutDetection.

</details>


### [101] [LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models](https://arxiv.org/abs/2602.14147)
*Shufan Li,Yuchen Zhu,Jiuxiang Gu,Kangning Liu,Zhe Lin,Yongxin Chen,Molei Tao,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: 本文提出了LaViDa-R1，一种基于扩散语言模型（dLLMs）的多模态通用推理模型。与以往主要通过特定任务强化学习训练不同，LaViDa-R1采用统一的后训练框架，结合了有监督微调和多任务的强化学习。模型在视觉数学推理、推理密集型定位和图像编辑等多项多模态任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前dLLMs在多模态理解与生成领域已有应用，但现有推理模型大多依赖于任务专用的强化学习，缺乏统一性和通用性。本文旨在构建一种更具通用性、可扩展性的多模态推理dLLM。

Method: LaViDa-R1提出了一种统一的后训练框架，将监督微调（SFT）和多任务强化学习（RL）无缝整合。引入了answer-forcing、树搜索和互补似然估计等创新训练技巧以提升模型效能与扩展能力。

Result: 大量实验表明，LaViDa-R1在多模态任务（如视觉数学推理、复杂定位和图像编辑）上均具有很强的性能。

Conclusion: LaViDa-R1证明了通过统一后训练框架和创新训练方法，扩散语言模型可实现广泛的多模态推理和生成任务，具有较好的通用性和可扩展性。

Abstract: Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.

</details>


### [102] [ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery](https://arxiv.org/abs/2602.14153)
*Zheng Han,Zixin Yang,Yonghao Long,Lin Zhang,Peter Kazanzides,Qi Dou*

Main category: cs.CV

TL;DR: 本文提出了ARport系统，通过增强现实技术将预先规划的穿刺孔布置无标记投射至患者体表，辅助机器人手术中的精确定位。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助手术中，穿刺孔（port）布局对于手术视野与器械操作至关重要，目前缺乏将术前规划与术中执行无缝衔接的直观导航方式。

Method: ARport通过光学透视式头戴显示器（OST-HMD）获取RGB、深度与位姿数据，无需外部传感器和标记物，自动重建手术场景，并基于基础模型提取体表，实现术前解剖模型与实际体表的表面配准，将规划的穿刺孔实时可视化叠加显示到患者体表。

Result: 在人体仿真模型试验中，ARport能够精准、稳定地将虚拟规划的穿刺孔与真实仿真体表对应叠加，实现高一致性定位。

Conclusion: ARport无需标记和额外设备，能高效地在患者体表直观显示术前穿刺规划，利于手术准备，具备驶入临床常规流程的潜力。

Abstract: Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.

</details>


### [103] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 本文提出了一种无需繁重计算的新方法，实现了高效且高质量的文本驱动图像和视频编辑。该方法在大规模数据集上的实验证明其可与甚至超过以训练为基础的方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的图像和视频编辑方法，虽在测试时指导方面取得进展，但大多依赖于昂贵的矢量-雅可比乘积（VJP）计算，限制了其实用性。研究者希望能找到既高效又精准的方法，解决这一瓶颈。

Method: 本文基于Moufad等人（2025）提出的无VJP近似方法，从理论上分析该算法的有效性，并将其实证评估扩展到大规模图像和视频编辑基准测试中。

Result: 在大规模基准测试中，提出的测试时指导方法在性能上能够媲美甚至超越一些需要额外训练的方法。

Conclusion: 仅采用测试时的算法指导，无VJP近似可以有效实现高效且高质量的编辑，具有重要的实际应用价值。

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [104] [Towards Spatial Transcriptomics-driven Pathology Foundation Models](https://arxiv.org/abs/2602.14177)
*Konstantin Hemker,Andrew H. Song,Cristina Almagro-Pérez,Guillaume Jaume,Sophia J. Wagner,Anurag Vaidya,Nikola Simidjievski,Mateja Jamnik,Faisal Mahmood*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉组学自监督学习框架SEAL，将空间转录组学(ST)的分子信息融入病理图像模型，大幅提升了模型在多种下游任务中的表现，并具备良好的泛化和跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学(ST)能够提供具有空间信息的基因表达数据，但与传统病理图像分析方法结合有限。随着多模态基础模型的兴起，将分子表达与形态学信息耦合有望系统性提升病理图像表征能力。作者希望利用ST信息改进视觉模型，实现病理分析的自动化和智能化。

Method: 提出SEAL（Spatial Expression-Aligned Learning）框架：利用空间转录组学中的“基因表达-组织区域”配对数据，以自监督方式微调现有病理视觉基础模型，将分子水平的局部信息注入视觉编码器。SEAL具有参数高效、可灵活应用于各种常见病理模型的优点。作者在14个器官、70多万对配对样本上进行训练与评估。

Result: SEAL在38项切片级别和15项Patch级别下游任务中表现优异，作为模型替代方案，持续优于主流视觉模型和ST预测基线，任务包括分子状态预测、通路活性分析、治疗反应预测及基因表达水平预测。此外，SEAL在分布外数据上泛化性强，并具备基因到图像检索等新颖跨模态能力。

Conclusion: ST引导的微调可以大幅增强病理基础模型的视觉表征能力和多模态应用潜力；将局部分子监督引入现有模型是有效且可行的步骤，可推动病理图像分析的智能化发展。

Abstract: Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.

</details>


### [105] [UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model](https://arxiv.org/abs/2602.14178)
*Shaobin Zhuang,Yuang Ai,Jiaming Han,Weijia Mao,Xiaohui Li,Fangyikang Wang,Xiao Wang,Yan Li,Shanchuan Lin,Kun Xu,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen,Yali Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一离散视觉分词器UniWeTok，通过大规模二进制码本和创新训练框架，实现了高保真重建、复杂语义提取和生成能力的平衡，并在多项任务中达到了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器难以在高还原度、强语义提取和生成适应三者间取得平衡，限制了多模态大模型（MLLM）的进一步发展。因此需设计能同时满足上述多重要求的新型分词方法。

Method: 1）提出超大二进制码本（$2^{128}$）的统一分词器UniWeTok；2）采用前后蒸馏与感知生成先验提升语义和生成表现；3）提出卷积-注意力混合架构，结合SigLu激活函数，优化语义蒸馏及熵-约束损失平衡；4）设计三阶段训练机制，提升模型对不同分辨率和敏感场景（如人脸、文字）的适应能力。

Result: UniWeTok在ImageNet上生成表现领先（FID: 1.38优于REPA），且训练数据需求显著更低（33B对262B）。其在多模态任务、图像生成及编辑评测中均达到或超越现有方法，展现出广泛的适用性。

Conclusion: UniWeTok有效解决了视觉分词多目标间的矛盾，实现了统一的高效编码和跨任务强适应性，为后续多模态大模型发展奠定基础。代码和模型已开放，推动社区进一步研究。

Abstract: Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.

</details>


### [106] [UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing](https://arxiv.org/abs/2602.14186)
*Hongyang Wei,Bin Wen,Yancheng Long,Yankai Yang,Yuhang Hu,Tianke Zhang,Wei Chen,Haonan Fan,Kaiyu Jiang,Jiankang Chen,Changyi Liu,Kaiyu Tang,Haojie Ding,Xiao Yang,Jia Sun,Huaiqing Wang,Zhenyu Yang,Xinyu Wei,Xianglong He,Yangguang Li,Fan Yang,Tingting Gao,Lei Zhang,Guorui Zhou,Han Li*

Main category: cs.CV

TL;DR: 本论文提出UniRef-Image-Edit系统，统一了单图像编辑与多图像合成，通过创新性的SELF输入表示和两阶段训练方法，在多参考图像的一致性与表现力上取得突出成果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理多参考图像编辑与合成时，常因参考输入之间交互有限导致一致性问题，无法兼顾视觉细节和跨图像的协调性。因此，作者希望构建一个统一、高质量且高一致性的多模态编辑系统。

Method: 作者提出Sequence-Extended Latent Fusion（SELF）作为输入表示方法，将多参考图像动态序列化为潜在序列，并引入全局像素预算约束。采用两阶段训练框架：1）监督微调（SFT），联合训练单图编辑和多图合成，通过逐步放宽像素预算提升细节与一致性。2）强化学习（RL）阶段，提出多源GRPO（MSGRPO），优化模型在多图像冲突条件下的组合一致性。

Result: 实验结果显示，系统在单图像编辑和多图像合成任务中均取得优异表现。逐步扩展像素预算的方法显著提高了模型生成图像的视觉真实性和参考图像间的一致性；MSGRPO进一步提升了多图像构图时的结果一致性。

Conclusion: UniRef-Image-Edit通过SELF统一表示和创新强化学习方法，显著提升了多参考图像编辑的一致性和细节表现，为多模态生成任务提供了新的高效解决方案。代码和数据即将开源，促进社区研究。

Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.

</details>


### [107] [GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery](https://arxiv.org/abs/2602.14201)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yifan Zhang,Long Lan,Xue Yang,Hongda Sun,Yulin Wang,Di Wang,Jun Song,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了GeoEyes，一个新训练框架，大幅提升超高分辨率遥感视觉问答任务（UHR remote sensing VQA）中的多模态大模型对图像进行主动缩放和信息获取的能力。


<details>
  <summary>Details</summary>
Motivation: 现有具备缩放能力的多模态大模型（MLLMs），在遥感等极高分辨率和信息稀疏的场景下，通常会出现工具使用模式同质化（工具滥用），无法高效获取有用证据，导致问答性能受限。作者试图解决这一性能瓶颈。

Method: GeoEyes包含两个关键组件：（1）自建冷启动SFT数据集UHR-CoZ，覆盖多种缩放任务；（2）自适应强化学习方法AdaZoom-GRPO，在缩放与互动过程中明确奖励证据获取与答案准确性的提升，训练模型具备按需缩放和合适停止的能力。

Result: GeoEyes显著提升UHR遥感VQA任务的表现，在XLRS-Bench基准上取得了54.23%的准确率，超过现有同类方法。

Conclusion: 本文方法有效缓解了多模态大模型在高分辨率场景下工具使用单一、证据采集不足的问题，使其在实际遥感等任务中具备更强的视觉理解与判断能力。

Abstract: The "thinking-with-images" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

</details>


### [108] [HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming](https://arxiv.org/abs/2602.14214)
*Jiahui Chen,Bo Peng,Lianchen Jia,Zeyu Zhang,Tianchi Huang,Lifeng Sun*

Main category: cs.CV

TL;DR: 提出HiVid框架，利用大语言模型（LLM）为视频流按内容生成高保真权重，提升主观体验，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内容感知的视频流需要精确的内容重要性权重来优化用户体验，目前依赖人工标注成本高，视觉显著模型泛化性差，亟需新的自动高效手段。

Method: 1）设计感知模块，利用LLM在局部上下文评估视频帧并自回归建立整体理解；2）VOD场景下用LLM指导的新型排序算法实现全局重排序应对局部评分不一致问题；3）直播场景下采用融合内容注意力和自适应推理窗口的多模态时序模型，实现无未来知识低延迟预测。

Result: 在VOD和直播场景下，HiVid框架权重预测准确率分别比SOTA提升11.5%和26%；用户实验表明可提升QoE相关性14.7%。

Conclusion: HiVid通过创新性地结合LLM解决了内容权重标注难题，在多场景下带来显著性能和用户体验提升，有望成为内容感知流媒体的新方向。

Abstract: Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video. (2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm. (3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\% for VOD and 26\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\%.

</details>


### [109] [Freq-DP Net: A Dual-Branch Network for Fence Removal using Dual-Pixel and Fourier Priors](https://arxiv.org/abs/2602.14226)
*Kunal Swami,Sudha Velusamy,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 本文提出了一种利用双像素(DP)传感器信息的新方法，通过几何和结构先验，有效提升单张图像中的栅栏去除效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前单张图像的栅栏去除方法常因缺乏运动线索或仅适用于动态图像而性能有限。而实际应用中，静态场景和单帧图像的栅栏去除需求普遍存在。作者希望利用DP传感器获取的深度和结构信息，突破现有限制。

Method: 提出了Freq-DP Net，设计双分支网络：一支通过DP传感器生成的散焦视差形成几何先验（使用显式代价体积表示），另一支用快速傅里叶卷积（FFC）学习栅栏的全局结构先验，再通过注意力机制融合两种信息，获得高精度的栅栏分割结果。

Result: 作者建立并公开了包含多样栅栏类型的基准数据集。实验显示，所提方法在单帧DP图像栅栏去除任务上明显优于强基线模型，达到了新SOTA水平。

Conclusion: 本文创新性将DP信息与频域结构融合，有效改善了栅栏去除表现，为后续相关应用提供了技术基础和高质量参考数据集。

Abstract: Removing fence occlusions from single images is a challenging task that degrades visual quality and limits downstream computer vision applications. Existing methods often fail on static scenes or require motion cues from multiple frames. To overcome these limitations, we introduce the first framework to leverage dual-pixel (DP) sensors for this problem. We propose Freq-DP Net, a novel dual-branch network that fuses two complementary priors: a geometric prior from defocus disparity, modeled using an explicit cost volume, and a structural prior of the fence's global pattern, learned via Fast Fourier Convolution (FFC). An attention mechanism intelligently merges these cues for highly accurate fence segmentation. To validate our approach, we build and release a diverse benchmark with different fence varieties. Experiments demonstrate that our method significantly outperforms strong general-purpose baselines, establishing a new state-of-the-art for single-image, DP-based fence removal.

</details>


### [110] [Learning Significant Persistent Homology Features for 3D Shape Understanding](https://arxiv.org/abs/2602.14228)
*Prachi Kudeshia,Jiju Poovvancheri*

Main category: cs.CV

TL;DR: 本文提出了包含拓扑信息的3D点云数据集，并开发了一种基于深度学习的显著持久点选择方法，提升了3D形状分析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 目前3D形状分析的基准数据集多关注几何信息，缺乏对拓扑结构的描述，限制了拓扑感知深度学习方法的发展。

Method: 作者将ModelNet40和ShapeNet数据集中的每个点云拓展为包含其持久同调特征，从而得到拓扑增强的数据集。在此基础上，提出了TopoGAT方法，通过深度学习自适应地从原始数据和拓扑特征中选取最有用的拓扑点，替代传统的人工统计选择方式。

Result: 实验表明，TopoGAT相较传统统计方法在稳定性和判别能力上有明显优势。将显著持久点融入现有的点云分类和分割流程能提升分类准确率和分割指标。

Conclusion: 拓扑增强的数据集和可学习的特征选择方法促进了持久同调等拓扑工具在3D点云深度学习中的实际应用，为统一几何-拓扑学习奠定了基础。

Abstract: Geometry and topology constitute complementary descriptors of three-dimensional shape, yet existing benchmark datasets primarily capture geometric information while neglecting topological structure. This work addresses this limitation by introducing topologically-enriched versions of ModelNet40 and ShapeNet, where each point cloud is augmented with its corresponding persistent homology features. These benchmarks with the topological signatures establish a foundation for unified geometry-topology learning and enable systematic evaluation of topology-aware deep learning architectures for 3D shape analysis. Building on this foundation, we propose a deep learning-based significant persistent point selection method, \textit{TopoGAT}, that learns to identify the most informative topological features directly from input data and the corresponding topological signatures, circumventing the limitations of hand-crafted statistical selection criteria. A comparative study verifies the superiority of the proposed method over traditional statistical approaches in terms of stability and discriminative power. Integrating the selected significant persistent points into standard point cloud classification and part-segmentation pipelines yields improvements in both classification accuracy and segmentation metrics. The presented topologically-enriched datasets, coupled with our learnable significant feature selection approach, enable the broader integration of persistent homology into the practical deep learning workflows for 3D point cloud analysis.

</details>


### [111] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: Sali-Cache是一种面向视觉语言模型（VLMs）处理长视频内容时，主动优化内存管理的新方法，通过在计算注意力操作前就筛选并缓存关键信息，大幅节省了内存且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 处理长视频时，KV缓存随序列长度线性增长，造成内存瓶颈。现有方法通过计算全部注意力矩阵后再丢弃不重要信息，导致大量计算资源浪费，需要更高效的内存管理方案。

Method: Sali-Cache在注意力机制运算前，通过双信号自适应缓存策略，采用基于光流分析的时间过滤器检测帧间冗余，以及基于显著性检测的空间过滤器筛选视觉显著区域，提前筛选和管理内存分配，避免不必要的计算。

Result: 在LLaVA 1.6架构实验中，Sali-Cache实现了2.2倍的有效内存压缩率，并在BLEU、ROUGE-L和Exact Match等指标上保持100%准确率。

Conclusion: Sali-Cache在不损失性能的前提下，极大提升长视频处理的内存效率，使消费级硬件上也能高效处理上下文丰富的视频内容。

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [112] [AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks](https://arxiv.org/abs/2602.14237)
*Kunal Swami,Raghu Chittersu,Yuvraj Rathore,Rajeev Irny,Shashavali Doodekula,Alok Shukla*

Main category: cs.CV

TL;DR: 本论文提出了一种结合触控先验与简洁指令，实现物体添加的用户友好框架AbracADDbra，并引入了新的评测基准Touch2Add，结果显示方法优于现有基线，提升了编辑高保真度和易用性。


<details>
  <summary>Details</summary>
Motivation: 传统的物体添加受限于文本提示歧义或繁琐的掩码输入，降低了交互效率和易用性，因此需要一种更直观、高效的编辑方式。

Method: 提出AbracADDbra框架，利用直观触点作为空间约束，将简短指令与具体位置匹配。架构为解耦式，首先用视觉-语言transformer根据触点完成定位，再用扩散模型联合生成物体及实例掩码，实现高质量融合。同时，提出Touch2Add评测基准。

Result: 大量实验证明，所提定位模型显著优于随机和通用VLM基线，在物体添加和编辑的保真度上表现突出。

Conclusion: 本文工作为物体添加任务提供了更高效易用的交互方式，并通过分阶段结构和新基准推动相关技术发展。

Abstract: Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.

</details>


### [113] [Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision](https://arxiv.org/abs/2602.14276)
*A. Said Gurbuz,Sunghwan Hong,Ahmed Nassar,Marc Pollefeys,Peter Staar*

Main category: cs.CV

TL;DR: 本文提出了ScreenParse数据集和ScreenVLM模型，用于高效且全面的屏幕UI元素解析，并显著提升了视觉语言模型在UI理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的屏幕解析和指令落地依赖的数据集存在标注稀疏、覆盖少、类别单一等问题，影响模型的泛化能力和应用效率。希望用更大规模、更密集标注的数据集与高效模型推动屏幕UI理解能力，并满足实际低延迟部署需求。

Method: 作者构建了ScreenParse数据集，该数据集包含77.1万张网页截图和所有可见UI元素（总计2100万个元素）的密集标注（包括位置框、细粒度类型和文本）。数据集通过自动化的Webshot流程生成，结合了VLM（视觉语言模型）辅助的重标注和质量过滤。基于ScreenParse，作者训练了316M参数的小型VLM（ScreenVLM），采用结构感知损失函数，提升了结构关键token的权重，用于解码紧凑的ScreenTag标记格式。

Result: ScreenVLM在ScreenParse上的PageIoU指标优于更大型的基础VLM（0.592对比0.294），且在公开基准上表现良好。同时，用ScreenParse微调基础VLM也一贯提升了其grounding（指令落地）能力，表明密集屏幕监督为UI理解带来了可迁移的结构先验。

Conclusion: ScreenParse数据集和ScreenVLM模型有效提升了屏幕UI密集解析和理解能力，验证了大规模、高质量标注对VLM任务迁移价值，并推动了实际低延迟屏幕理解应用的发展。

Abstract: Modern computer-use agents (CUA) must perceive a screen as a structured state, what elements are visible, where they are, and what text they contain, before they can reliably ground instructions and act. Yet, most available grounding datasets provide sparse supervision, with insufficient and low-diversity labels that annotate only a small subset of task-relevant elements per screen, which limits both coverage and generalization; moreover, practical deployment requires efficiency to enable low-latency, on-device use. We introduce ScreenParse, a large-scale dataset for complete screen parsing, with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements). ScreenParse is generated by Webshot, an automated, scalable pipeline that renders diverse urls, extracts annotations and applies VLM-based relabeling and quality filtering. Using ScreenParse, we train ScreenVLM, a compact, 316M-parameter vision language model (VLM) that decodes a compact ScreenTag markup representation with a structure-aware loss that upweights structure-critical tokens. ScreenVLM substantially outperforms much larger foundation VLMs on dense parsing (e.g., 0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Moreover, finetuning foundation VLMs on ScreenParse consistently improves their grounding performance, suggesting that dense screen supervision provides transferable structural priors for UI understanding. Project page: https://saidgurbuz.github.io/screenparse/.

</details>


### [114] [Differential pose optimization in descriptor space -- Combining Geometric and Photometric Methods for Motion Estimation](https://arxiv.org/abs/2602.14297)
*Andreas L. Teigen,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: 本文提出用密集的几何特征描述子替代传统光度误差，实现结合光度法和几何法优势的位姿优化新方法，但最终效果并未超过重投影误差法。


<details>
  <summary>Details</summary>
Motivation: 传统相对位姿优化通常选择光度误差或重投影误差，二者各有优缺点，难以兼顾精度、鲁棒性和值回环能力。作者希望设计一种能结合二者优点的优化方法。

Method: 该方法用密集采样的几何特征描述子生成描述子残差，替代传统光度误差，通过描述子残差作为误差项，并利用微分方法优化，实现光度法的亚像素精度和描述子表达能力的结合。

Result: 实验表明该策略能够实现较准确的跟踪，但尽管利用了更多信息，其最终性能仍低于传统的基于重投影误差的优化方法。

Conclusion: 分析认为，描述子相似性度量变化过于平缓，与关键点精确定位未必严格对应，是方法表现不足的主要原因。

Abstract: One of the fundamental problems in computer vision is the two-frame relative pose optimization problem. Primarily, two different kinds of error values are used: photometric error and re-projection error. The selection of error value is usually directly dependent on the selection of feature paradigm, photometric features, or geometric features. It is a trade-off between accuracy, robustness, and the possibility of loop closing. We investigate a third method that combines the strengths of both paradigms into a unified approach. Using densely sampled geometric feature descriptors, we replace the photometric error with a descriptor residual from a dense set of descriptors, thereby enabling the employment of sub-pixel accuracy in differential photometric methods, along with the expressiveness of the geometric feature descriptor. Experiments show that although the proposed strategy is an interesting approach that results in accurate tracking, it ultimately does not outperform pose optimization strategies based on re-projection error despite utilizing more information. We proceed to analyze the underlying reason for this discrepancy and present the hypothesis that the descriptor similarity metric is too slowly varying and does not necessarily correspond strictly to keypoint placement accuracy.

</details>


### [115] [A Generative AI Approach for Reducing Skin Tone Bias in Skin Cancer Classification](https://arxiv.org/abs/2602.14356)
*Areez Muhammed Shabu,Mohammad Samar Ansari,Asra Aslam*

Main category: cs.CV

TL;DR: 该论文提出一种通过生成式AI增强皮肤癌诊断数据集公平性的方法，利用改进的图像生成模型补充深色皮肤样本，实现识别效果和公平性的提升。


<details>
  <summary>Details</summary>
Motivation: 目前常用的皮肤癌AI诊断工具多基于偏重浅色皮肤的数据集，导致对深色皮肤人群诊断准确性和公平性不足，制约了医疗公平。本研究旨在解决数据集肤色分布不均这一突出问题。

Method: 作者采用生成式增强方法，对Stable Diffusion模型应用LoRA算法，在ISIC数据集中深色皮肤子集上微调，并生成结合病变类型与肤色条件的合成皮肤镜图像，用于补充原有训练数据。生成的数据随后被用于两项下游任务：皮损分割和二分类。

Result: 在皮损分割任务中，增强数据集训练出的模型在真实保留集上的IoU、Dice系数和边界精度均有提升。分类任务中，采用增强数据训练的EfficientNet-B0模型准确率达到92.14%。生成数据的有效性得到了验证。

Conclusion: 研究表明，结合生成式AI的数据增强能明显降低皮肤癌诊断中的偏见，提高算法对不同肤色群体的公平性，并为今后相关研究提供了新的思路和方向。

Abstract: Skin cancer is one of the most common cancers worldwide and early detection is critical for effective treatment. However, current AI diagnostic tools are often trained on datasets dominated by lighter skin tones, leading to reduced accuracy and fairness for people with darker skin. The International Skin Imaging Collaboration (ISIC) dataset, one of the most widely used benchmarks, contains over 70% light skin images while dark skins fewer than 8%. This imbalance poses a significant barrier to equitable healthcare delivery and highlights the urgent need for methods that address demographic diversity in medical imaging. This paper addresses this challenge of skin tone imbalance in automated skin cancer detection using dermoscopic images. To overcome this, we present a generative augmentation pipeline that fine-tunes a pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on the image dark-skin subset of the ISIC dataset and generates synthetic dermoscopic images conditioned on lesion type and skin tone. In this study, we investigated the utility of these images on two downstream tasks: lesion segmentation and binary classification. For segmentation, models trained on the augmented dataset and evaluated on held-out real images show consistent improvements in IoU, Dice coefficient, and boundary accuracy. These evalutions provides the verification of Generated dataset. For classification, an EfficientNet-B0 model trained on the augmented dataset achieved 92.14% accuracy. This paper demonstrates that synthetic data augmentation with Generative AI integration can substantially reduce bias with increase fairness in conventional dermatological diagnostics and open challenges for future directions.

</details>


### [116] [Image-based Joint-level Detection for Inflammation in Rheumatoid Arthritis from Small and Imbalanced Data](https://arxiv.org/abs/2602.14365)
*Shun Kato,Yasushi Kondo,Shuntaro Saito,Yoshimitsu Aoki,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本论文提出了利用家庭拍摄的RGB手部图像进行类风湿性关节炎（RA）炎症检测的新方法，并显著提升了检测效能。


<details>
  <summary>Details</summary>
Motivation: 类风湿关节炎早期诊断至关重要，但患者通常需要很长时间才能获得专业医疗服务。旨在开发一种简单易用、可家用的基于RGB图像的关节炎症检测系统，方便疾病早筛与监控。

Method: 作者建立了一个专门的数据集，采用了结合大规模健康手部图像自监督预训练与针对不均衡的训练方法的全局-局部编码器（global local encoder）框架，优化了炎症检测能力。

Result: 所提方法相比基线模型，F1-score提升了0.2，Gmean提升了0.25，验证了新方法的有效性。

Conclusion: 结合自监督预训练和不均衡感知训练的全局-局部编码器能有效从RGB手部图片中检测RA相关炎症，为居家早筛提供了可行新方案。

Abstract: Rheumatoid arthritis (RA) is an autoimmune disease characterized by systemic joint inflammation. Early diagnosis and tight follow-up are essential to the management of RA, as ongoing inflammation can cause irreversible joint damage. The detection of arthritis is important for diagnosis and assessment of disease activity; however, it often takes a long time for patients to receive appropriate specialist care. Therefore, there is a strong need to develop systems that can detect joint inflammation easily using RGB images captured at home. Consequently, we tackle the task of RA inflammation detection from RGB hand images. This task is highly challenging due to general issues in medical imaging, such as the scarcity of positive samples, data imbalance, and the inherent difficulty of the task itself. However, to the best of our knowledge, no existing work has explicitly addressed these challenges in RGB-based RA inflammation detection. This paper quantitatively demonstrates the difficulty of visually detecting inflammation by constructing a dedicated dataset, and we propose a inflammation detection framework with global local encoder that combines self-supervised pretraining on large-scale healthy hand images with imbalance-aware training to detect RA-related joint inflammation from RGB hand images. Our experiments demonstrated that the proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared with the baseline model.

</details>


### [117] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出一种新的视觉变形测量（VDM）方法，通过融合事件流和帧图像，有效提升变形场重建的精度和效率，并显著降低对高存储和算力的需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于图像的方法在处理高度动态场景时，受限于帧间最小运动的假设，导致需要高帧率相机，但这会带来巨大的存储和计算压力。因此，亟需一种既能提供精确变形估计，又能降低资源消耗的新方法。

Method: 1. 提出事件-帧融合框架，利用事件流捕捉高时间分辨率的运动信息，帧图像则提供高空间分辨率估计。
2. 基于固体弹性建模先验，提出仿射不变单纯形（AIS）框架，将变形场分解为低参数、线性化子区块，有效减少稀疏且噪声事件带来的匹配歧义。
3. 引入邻域贪婪优化机制，使收敛良好的子区块能带动邻近区域，提高整体收敛速度并抑制误差累积。
4. 构建一个大规模、多变形场景的数据集，包含事件流与帧的时序对齐信息用于验证方法有效性。

Result: 实验表明，该方法在“生存率”指标上比现有最先进基线提升1.6%，同时只需高帧率视频方法18.9%的数据存储和处理资源。

Conclusion: 提出的方法可在存储和算力需求低得多的情况下，实现对动态变形场的高精度、长时追踪，显著优于传统高帧率图像方法，拥有良好的实际应用前景。

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [118] [Adapting VACE for Real-Time Autoregressive Video Diffusion](https://arxiv.org/abs/2602.14381)
*Ryan Fosdick*

Main category: cs.CV

TL;DR: 本文提出了VACE（视频一体化生成与编辑）的一种适用于实时自回归视频生成的改编方法，实现了在保持流式、低延迟生成的情况下的结构控制与参考指引，但在画质与参考忠实度方面存在一定损失。


<details>
  <summary>Details</summary>
Motivation: 原始VACE模型虽支持多种视频编辑功能（如参考引导、结构控制、修复、时延扩展），但其基于双向attention，需要全序列视野，不适用于需要固定分块和因果attention的流式（streaming）自回归生成场景。本文旨在实现VACE在实时流式自回归视频生成任务中的适配。

Method: 通过将参考帧信息从扩散潜空间迁移至并行条件通道，实现了兼容自回归生成中固定分块（chunking）和key-value缓存（KV caching）的需求，而不影响预训练VACE权重的使用，无需额外训练。

Result: 在1.3B和14B参数规模下，结构控制和修复功能带来20-30%的延迟开销，显存几乎无额外占用。然而，由于采用因果attention，参考-到-视频的还原度相较于原始VACE显著下降。

Conclusion: 改进后的VACE实现了实时流式自回归视频生成中的结构控制和参考引导，能够兼容主流自回归生成框架，无需再训练，但牺牲了部分参考忠实度。代码已开源。

Abstract: We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>


### [119] [Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models](https://arxiv.org/abs/2602.14399)
*In Chong Choi,Jiacheng Zhang,Feng Liu,Yiliao Song*

Main category: cs.CV

TL;DR: 文章提出了一种新的多回合适应性攻破攻击（MAPA）方法，有效提高对对齐后的多模态大模型（LVLMs）越狱的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有文本多回合越狱攻击在多模态模型（LVLMs）上会因视觉输入过于明显而被模型防御机制轻易识别，导致攻击效果降低，亟需开发更有效针对LVLMs的多轮攻防技术。

Method: MAPA方法包括两大关键设计：1）每轮交替进行文本和视觉攻击以诱导更恶意响应；2）多轮迭代过程中根据先前响应动态调整攻击策略，逐步增强模型输出的恶意性。

Result: MAPA在主流LVLMs（LLaVA-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct和GPT-4o-mini）上的攻击成功率提升11-35%，显著优于当前最优方法。

Conclusion: MAPA有效破解现有多模态模型的安全防线，对未来安全对齐和攻防研究有重要启示。

Abstract: Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

</details>


### [120] [pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI](https://arxiv.org/abs/2602.14401)
*Qingqian Yang,Hao Wang,Sai Qian Zhang,Jian Li,Yang Hua,Miao Pan,Tao Song,Zhengwei Qi,Haibing Guan*

Main category: cs.CV

TL;DR: 本论文提出了pFedNavi，一种针对视觉-语言导航（VLN）场景设计的结构感知和动态自适应个性化联邦学习框架，实现了在保护用户隐私的同时有效提升导航模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉-语言导航（VLN）需要大规模私有室内数据，带来隐私风险。普通联邦学习虽然数据不出本地，但因不同用户环境和指令风格差异极大，直接共享参数效果不佳，亟需更个性化的联合建模方法。

Method: 提出pFedNavi框架，通过自适应确定每个客户需个性化的层（如编码-解码投影与环境敏感解码层），并对关键参数进行细粒度混合和融合，兼顾全局知识通用性与本地特性个性化。

Result: 在VLN标准基准R2R和RxR的实验中，无论采用ResNet还是CLIP视觉特征，pFedNavi在所有指标上均优于FedAvg，导航成功率提升最大7.5%，轨迹还原度提升最大7.8%，在非IID数据下收敛速度提升1.38倍。

Conclusion: pFedNavi能在极端异构的视觉-语言导航任务中，兼顾隐私保护、个性化优化与训练高效性，比现有联邦平均方法显著更优，具备实际场景部署潜力。

Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.

</details>


### [121] [Feature Recalibration Based Olfactory-Visual Multimodal Model for Fine-Grained Rice Deterioration Detection](https://arxiv.org/abs/2602.14408)
*Rongqiang Zhao,Hengrui Hu,Yijing Wang,Mingchun Sun,Jie Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种结合嗅觉与视觉的多模态模型，用于精细化识别大米劣变，显著提高了检测准确率，并简化了检测流程。


<details>
  <summary>Details</summary>
Motivation: 现有的大米劣变检测多模态方法对异常特征提取能力有限，且高度依赖昂贵且复杂的设备，例如高光谱相机与质谱仪，导致成本高、时间长。亟需简化流程、降低成本的高性能方法。

Method: 提出基于特征重校准的嗅觉-视觉多模态模型，包含两个主要模块：1）精细劣变嵌入构造器（FDEC），用于重构多模态特征数据集，增强样本表示能力；2）精细劣变重校准注意网络（FDRA-Net），突出信号变化、提升对表面微小劣变的敏感性。

Result: 所提方法在实验中取得了99.89%的分类准确率，对比当前主流方法不仅检测精度提升，流程也得到简化。田间实际检测进一步验证了该方法的准确性与易操作性。

Conclusion: 新方法兼具高准确率和操作简便，显著提升了大米劣变检测水平，并具备在农业和食品行业其他农产品上的推广应用潜力。

Abstract: Multimodal methods are widely used in rice deterioration detection, which exhibit limited capability in representing and extracting fine-grained abnormal features. Moreover, these methods rely on devices, such as hyperspectral cameras and mass spectrometers, increasing detection costs and prolonging data acquisition time. To address these issues, we propose a feature recalibration based olfactory-visual multimodal model for fine-grained rice deterioration detection. The fine-grained deterioration embedding constructor (FDEC) is proposed to reconstruct the labeled multimodal embedded-feature dataset, enhancing sample representation. The fine-grained deterioration recalibration attention network (FDRA-Net) is proposed to emphasize signal variations and increase sensitivity to fine-grained deterioration on the rice surface. Experiments show that the proposed method achieves a classification accuracy of 99.89%. Compared with state-of-the-art methods, the detection accuracy is improved and the procedure is simplified. Furthermore, field detection demonstrates the advantages of accuracy and operational simplicity. The proposed method can also be extended to other agrifood in agriculture and food industry.

</details>


### [122] [Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.14409)
*Haichao Zhu,Zhaorui Yang,Qian Zhang*

Main category: cs.CV

TL;DR: 论文提出一种端到端模块化空间感知框架，将深度学习与几何算法结合，提高相机位姿估计性能。实验指出单靠学习方法不稳定，几何模块仍然不可或缺。


<details>
  <summary>Details</summary>
Motivation: 当前空间感知任务常由几何方法和物理约束解决，近年来深度学习增强了表征能力，但学习方法与几何模块的最佳协作模式尚无定论。

Method: 提出以学习方法生成几何假设，再由经典几何算法判断和处理的端到端模块化框架。在RGB-D序列的相机位姿估计中，采用学习模型（如VGGT）生成位姿与深度建议，经RGB-D ICP几何后端处理，涵盖不同运动强度和场景动态条件。

Result: TUM RGB-D基准上实验显示：（1）仅用学习模型给出位姿方案性能不稳定；（2）若学习方法生成的几何信息与相机内参不一致会影响表现；（3）几何校准并结合几何后端能有效提升性能（在刚性场景下）。

Conclusion: 几何方法不仅是优化补充，更是对学习结果的核心仲裁者。推荐采用模块化、几何感知增强的系统设计以实现鲁棒空间感知。

Abstract: Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.

</details>


### [123] [Hierarchical Vision-Language Interaction for Facial Action Unit Detection](https://arxiv.org/abs/2602.14425)
*Yong Li,Yi Ren,Yizhe Zhang,Wenhua Zhang,Tianyi Zhang,Muyun Jiang,Guo-Sen Xie,Cuntai Guan*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉和文本信息的层次化模型HiVA，用于面部表情单元（AU）检测，在数据有限时表现优异。


<details>
  <summary>Details</summary>
Motivation: 面部表情单元检测在标注数据稀缺时难以学到具有辨别性和泛化能力的特征表示。为解决这一难题，作者希望利用文本语义作为先验，提升模型的表现力和泛化性。

Method: 提出HiVA方法，利用大语言模型生成丰富的AU文本描述，通过层次化的视觉-语言交互学习特征。具体包含：1) AU感知的动态图模块，捕捉AU特定视觉特征；2) 双重交叉注意力，分别建模细粒度和全局的视觉-语言关联，提升AU检测能力。

Result: 在多个实验中，HiVA方法在各项性能指标上均优于当前最优方法，且产生了语义上有意义的激活模式。

Conclusion: HiVA能有效融合视觉与文本信息，提升在受限数据条件下AU检测的准确性和可解释性，对面部行为分析具有应用潜力。

Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.

</details>


### [124] [D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection](https://arxiv.org/abs/2602.14441)
*Gagandeep Singh,Samudi Amarasinghe,Priyanka Singh*

Main category: cs.CV

TL;DR: 本文提出D-SECURE框架，通过结合内部篡改检测与外部证据推理，更有效地识别与查证多模态虚假信息。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚假信息常常结合逼真的图片编辑和流畅但误导性的文本，使得人工和自动化核查都变得困难。现有系统往往只依赖单一证据源，要么只检测内容内部一致性，无法判断事实真伪；要么只基于外部证据，难以识别精细的篡改或误导，导致虚假内容规避检测或被错误判定为真实。因此，需要综合多种检测方式提升核查效果。

Method: 提出D-SECURE框架，将图像内容内部的篡改检测（HAMMER）与基于外部证据的事实核查（DEFAME）相结合。DEFAME用于宏观事实核查，而HAMMER分析细粒度的可疑编辑，实现互补。实验采用DGM4和ClaimReview数据集进行评估。

Result: 实验显示，HAMMER和DEFAME两套系统各具优势，相结合后能互补不足，提升多模态虚假信息检测的准确性和解释性。

Conclusion: D-SECURE框架有效融合了内部和外部证据，能够统一、可解释地检测和查证混合型多模态虚假信息，有助于改进新闻等场景下的自动化信息核查。

Abstract: Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.

</details>


### [125] [Controlling Your Image via Simplified Vector Graphics](https://arxiv.org/abs/2602.14443)
*Lanqing Guo,Xi Liu,Yufei Wang,Zhihao Li,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于简化矢量图形（VGs）的分层可控图像生成方法，实现了对图像元素级别的精确控制，如形状、颜色、对象的添加和删除。该方法通过高效地将图像解析为结构和语义一致的VG分层表示，并设计了相应的图像合成框架，实现了VG引导下的用户自由编辑及其逼真成像。实验结果表明，该方法在多种图像编辑和对象级操控任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然当前的图像生成技术视觉质量显著提升，但尚缺乏对图像内部元素（如形状、颜色、对象）的直观可控编辑能力。实际应用中用户常希望对图像的具体元素进行灵活编辑，但现有方法难以实现直观和精细的操控。

Method: 方法首先将原始图像高效地解析为具有层次和语义对齐的简化矢量图形（VG）表示，然后基于这种VG表示，设计了新的图像合成框架。该框架结合了VG的结构和语义特征及噪声预测，使用户可以在微观元素层面对图像实现自由编辑和无缝还原为写真级别的成像。

Result: 通过大量实验验证，该方法能够在图像编辑、对象级操控、精细内容创作等多种应用中实现高效、精准的编辑和生成，取得了优异效果。

Conclusion: 文中方法为可控图像生成开辟了新思路，实现了元素级编辑、对象级操控和高度写实的生成结果，有望推动图像编辑和合成相关领域的发展。

Abstract: Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/

</details>


### [126] [CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer](https://arxiv.org/abs/2602.14464)
*Wenbo Nie,Zixiang Li,Renshuai Tao,Bin Wu,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoCoDiff的无训练、低成本风格迁移方法，能精细地在图片间转移风格且保持语义一致性，对比现有方法，显著提升了局部和像素级的语义对应。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移方法多关注全局特征，忽略了区域和像素级语义一致性，导致风格迁移效果在细节和结构保存上有所欠缺。作者希望解决高精度语义区域风格迁移的问题。

Method: 提出CoCoDiff框架，基于预训练的潜在扩散模型，无需额外训练。核心有两部分：①像素级语义对应模块，提取扩散模型中间特征生成内容与风格图片间的密集对齐图；②循环一致性模块，在迭代中保持结构与感知一致，确保细粒度几何和细节保留。

Result: 在不依赖额外训练和监督的前提下，CoCoDiff获得了最佳的视觉质量以及强有力的定量指标，超越了需要额外训练和标注的现有方法。

Conclusion: CoCoDiff有效提升了风格迁移任务中的局部细节和总体结构一致性，证明了仅基于扩散模型和简单模块就能达到甚至超越现有复杂方法的表现。

Abstract: Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

</details>


### [127] [TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.14482)
*Hao Ding,Zhichuan Yang,Weijie Ge,Ziqin Gao,Chaoyi Lu,Lei Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为TikArt（Thinking Aperture）的新型多模态大模型视觉推理方法，通过聚焦小区域、多步选区操作，提升细粒度理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在图像理解中往往基于全局编码，难以捕捉微小目标、复杂区域或细微标记，从而影响精细视觉推理能力。为此，作者希望提升模型对局部视觉证据的利用能力，实现更准确的细粒度推理。

Method: 提出TikArt框架，将视觉-语言推理过程转化为对图像区域的决策过程。TikArt包含“思考-开窗-观察”循环，结合放大（Zoom）和基于SAM2分割（Segment）两种开窗操作，实现对规则与不规则目标区域的选取。每次操作后，模型生成显式观测，积累局部视觉信息。以Qwen3-VL-8B为骨干，通过AGRPO强化学习算法分阶段训练，实现分割预训练与多任务联合优化，并设置奖励机制关联任务成功和有效开窗。

Result: 在多个公开基准（如V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO、ReasonSeg）中的实验结果显示，TikArt相较骨干模型获得了一致而显著的性能提升，并能生成可解释的开窗轨迹，体现出其高分辨率推理的能力。

Conclusion: TikArt利用区域决策和强化学习策略，有效提升了多模态大模型在细粒度视觉推理场景中的表现，为高分辨率和复杂视觉任务提供了新思路。

Abstract: We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.

</details>


### [128] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D Gaussian Splatting和三角网格的高效可微渲染器，既保留了表面结构的准确性，又显著提升了优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在新视角合成方面表现优异，但传统三角网格模型受限于可微渲染效率低下，优化成本高。研究动机在于将3DGS和网格模型的优点结合，提升基于网格的重建和优化效率。

Method: 提出了Gaussian Mesh Renderer (GMR)：该方法将高斯基元与三角网格紧密结合，通过从网格三角形解析导出高斯基元，实现高效的可微渲染流程，使优化更快且内存占用更小。

Result: 与传统网格可微渲染器相比，该方法提供更平滑的梯度，尤其在小batch size、有限内存时，优化效果更优，实验效果显著提升。

Conclusion: GMR实现了高保真、高效的基于网格的可微渲染与优化，为三维重建和新视角合成领域带来了有影响力的工具，该代码已开源。

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [129] [Uncertainty-Aware Vision-Language Segmentation for Medical Imaging](https://arxiv.org/abs/2602.14498)
*Aryan Das,Tanishq Rachamalla,Koushik Biswas,Swalpa Kumar Roy,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 本文提出了一种结合影像和临床文本、具备不确定性感知能力的多模态分割框架，实现高效精准的医学诊断。


<details>
  <summary>Details</summary>
Motivation: 目前医学多模态分割任务中，仅依赖影像或忽略数据不确定性常导致诊断结果不可靠，尤其在低质量图像下问题更为突出。作者希望通过引入不确定性建模与更加高效的模态信息融合，提高分割性能和模型鲁棒性。

Method: 1) 提出Modality Decoding Attention Block (MoDAB)与轻量级状态空间混合器（SSMix），实现影像和文本多模态特征的高效融合与长距离依赖建模；2) 设计反映空间重叠、频谱一致性和预测不确定性联合的Spectral-Entropic Uncertainty (SEU) Loss优化目标。

Result: 在QATA-COVID19、MosMed++和Kvasir-SEG等公开医学数据集上，该方法显著优于现有最先进方法（SoTA）的分割性能，同时计算效率更高。

Conclusion: 结果显示：引入不确定性建模和结构化模态对齐对于提升医学视觉-语言分割任务的效果非常关键。

Abstract: We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS

</details>


### [130] [Prototype Instance-semantic Disentanglement with Low-rank Regularized Subspace Clustering for WSIs Explainable Recognition](https://arxiv.org/abs/2602.14501)
*Chentao Li,Pan Huang*

Main category: cs.CV

TL;DR: 本文提出一种在病理诊断中提升肿瘤实例识别与判读的新方法，通过降低实例间语义混淆显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 在全切片图像(WSIs)中，肿瘤区域对病理诊断至关重要，但肿瘤与癌前病变区高度相似，且非肿瘤实例数量远大于肿瘤实例，导致多实例学习中语义混淆，影响诊断模型的表达能力和可解释性。

Method: 提出端到端的原型实例语义解纠缠框架PID-LRSC。首先利用次级实例子空间学习结合低秩子空间聚类（LRSC），减少非肿瘤实例过多造成的混淆；其次用增强式对比学习进行原型实例语义解纠缠（PID），区分肿瘤与癌前组织的高相似度问题。

Result: 在多中心病理数据集上与现有最优方法相比，PID-LRSC均有更优表现。

Conclusion: PID-LRSC不仅在判读过程中提升实例语义清晰度，也显著增强了辅助诊断结果的可靠性。

Abstract: The tumor region plays a key role in pathological diagnosis. Tumor tissues are highly similar to precancerous lesions and non tumor instances often greatly exceed tumor instances in whole slide images (WSIs). These issues cause instance-semantic entanglement in multi-instance learning frameworks, degrading both model representation capability and interpretability. To address this, we propose an end-to-end prototype instance semantic disentanglement framework with low-rank regularized subspace clustering, PID-LRSC, in two aspects. First, we use secondary instance subspace learning to construct low-rank regularized subspace clustering (LRSC), addressing instance entanglement caused by an excessive proportion of non tumor instances. Second, we employ enhanced contrastive learning to design prototype instance semantic disentanglement (PID), resolving semantic entanglement caused by the high similarity between tumor and precancerous tissues. We conduct extensive experiments on multicentre pathology datasets, implying that PID-LRSC outperforms other SOTA methods. Overall, PID-LRSC provides clearer instance semantics during decision-making and significantly enhances the reliability of auxiliary diagnostic outcomes.

</details>


### [131] [MacNet: An End-to-End Manifold-Constrained Adaptive Clustering Network for Interpretable Whole Slide Image Classification](https://arxiv.org/abs/2602.14509)
*Mingrui Ma,Chentao Li,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的多实例学习（MIL）框架，通过结合Grassmann重嵌入和流形自适应聚类，有效提升了病理全切片图像分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有主流全切片图像分析方法主要采用两步式框架，缺乏领域知识指导，存在可解释性差或高维数据聚类不鲁棒等问题。亟需一种同时具备高准确性和良好可解释性的端到端方法。

Method: 作者提出了一种端到端MIL框架，融合Grassmann重嵌入及流形自适应聚类，利用流形几何结构提升聚类效果，并设计了先验知识引导的代理实例标签与聚合策略，更好聚焦于病理相关的肿瘤区域。

Result: 在多中心WSI数据集上的实验表明，该方法在分级准确性和可解释性方面均优于现有方法，并且端到端学习提升了特征表征能力，计算资源需求合理。

Conclusion: 该工作验证了所提方法在实际病理图像分析任务中的优越性和潜力，可为病理AI辅助诊断提供更有价值的技术支持。

Abstract: Whole slide images (WSIs) are the gold standard for pathological diagnosis and sub-typing. Current main-stream two-step frameworks employ offline feature encoders trained without domain-specific knowledge. Among them, attention-based multiple instance learning (MIL) methods are outcome-oriented and offer limited interpretability. Clustering-based approaches can provide explainable decision-making process but suffer from high dimension features and semantically ambiguous centroids. To this end, we propose an end-to-end MIL framework that integrates Grassmann re-embedding and manifold adaptive clustering, where the manifold geometric structure facilitates robust clustering results. Furthermore, we design a prior knowledge guiding proxy instance labeling and aggregation strategy to approximate patch labels and focus on pathologically relevant tumor regions. Experiments on multicentre WSI datasets demonstrate that: 1) our cluster-incorporated model achieves superior performance in both grading accuracy and interpretability; 2) end-to-end learning refines better feature representations and it requires acceptable computation resources.

</details>


### [132] [MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction](https://arxiv.org/abs/2602.14512)
*Zhicheng He,Yunpeng Zhao,Junde Wu,Ziwei Niu,Zijun Li,Lanfen Lin,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了MedVAR，一种基于自回归模型的医学影像生成基础模型，在速度、拓展性与多尺度生成上表现优异，并通过大规模多器官数据集和全面实验验证了其先进性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像生成方法在模型架构效率、多器官大规模数据、与系统性评估等方面存在不足，制约了其在数据增强与隐私保护等应用中的推广。

Method: 提出MedVAR模型，采用自回归和多尺度逐步生成机制，支持从粗到细的图像合成，并整合六个解剖区域共44万张CT和MRI影像，构建层级化数据集。

Result: MedVAR在生成质量、多样性与可扩展性等多项评测中取得当前最优表现。

Conclusion: MedVAR为医学影像生成基础模型提供了一种高效可扩展的架构思路，有望推动领域发展并服务更多下游任务。

Abstract: Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

</details>


### [133] [Efficient Text-Guided Convolutional Adapter for the Diffusion Model](https://arxiv.org/abs/2602.14514)
*Aryan Das,Koushik Biswas,Swalpa Kumar Roy,Badri Narayana Patro,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 本文提出了Nexus Adapters，一种高效、融合文本引导和结构保持的扩散模型适配器，极大减小了参数量并提升了条件图像生成的效果。


<details>
  <summary>Details</summary>
Motivation: 现有结构保持型条件生成方法在配合适配器输入结构信息（如素描或深度图）时，参数量过大且适配器未利用文本提示，训练和推理效率低下，限制了实际应用。作者希望用高效且能同时理解结构和文本提示的新适配器，解决现有方法结构与语义融合不佳的问题。

Method: 作者提出了Nexus Prime与Nexus Slim两种适配器，均在Nexus Block内采用跨模态注意力机制，将结构输入与文本提示信号结合，实现了多模态强化引导；Nexus Prime参数更精简，Nexus Slim更极致轻量化。

Result: 实验显示，Nexus Prime仅比基线T2I-Adapter多800万参数但效果显著提升；Nexus Slim参数比T2I-Adapter少1800万依然达到最优表现。

Conclusion: Nexus Adapters通过高效的跨模态融合结构，实现结构保持条件生成的新SOTA，极大提升了生成质量与效率，可广泛应用于结构与语义结合的图像生成任务。

Abstract: We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters

</details>


### [134] [Architectural Insights for Post-Tornado Damage Recognition](https://arxiv.org/abs/2602.14523)
*Robinson Umeike,Thang Dao,Shane Crawford,John van de Lindt,Blythe Johnston,Wanting,Wang,Trung Do,Ajibola Mofikoya,Sarbesh Banjara,Cuong Pham*

Main category: cs.CV

TL;DR: 本文利用新构建的QSTD数据集，对79种主流深度学习模型进行了大规模评测，发现优化器和学习率设置对于龙卷风损毁建筑识别效果至关重要，甚至比模型结构本身影响更大。文中提出的最佳方案在通用性和准确率方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 灾后建筑损毁评估对救援与资源调度至关重要，但现有自动方法因龙卷风破坏场景视觉复杂和数据分布严重不均，效果不佳，需要系统性方法来提升评测和模型表现。

Method: 作者构建了包含标准和极端类别失衡的QSTD数据集，并在其中开展2,300多个对比实验，系统评测CNN和Transformer系列的79种开源视觉模型，特别考察了模型架构、优化器选择、学习率等技术细节对实际效果的影响。

Result: 实验发现，优化器和超参数设置的影响超过了模型架构选择。例如，将Vision Transformer系列优化器由Adam换为SGD，F1得分提升25-38点；降低学习率普遍提升性能。最佳模型ConvNeXt-Base在外部测试集上Macro F1提升34.6点，并在不同灾害时空和传感器下表现稳健。

Conclusion: 仅靠更换或改进模型架构无法显著提升龙卷风损毁识别的自动化水平；优化器和学习率等“训练细节”优化同样至关重要。系统优化后，通用性与准确率均大幅提升，可服务灾后快速响应与恢复。

Abstract: Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.

</details>


### [135] [Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model](https://arxiv.org/abs/2602.14524)
*Ari Vesalainen,Eetu Mäkelä,Laura Ruotsalainen,Mikko Tolonen*

Main category: cs.CV

TL;DR: 本文对比了专用OCR Transformer模型（TrOCR）与通用视觉-语言模型（Qwen）在处理18世纪历史文本OCR任务中的表现，发现两者虽然总体准确率类似，但在错误类型、可检测性和对学术应用风险上有显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer型OCR与视觉-语言模型在整体准确率上表现优异，但常用的CER/WER等指标难以评估其在历史文本学术应用中的实际可靠性，因此需要对不同模型的错误特性和风险进行深入对比分析。

Method: 本文采用长度加权准确率与基于假设的错误分析，对TrOCR（专用OCR Transformer）和Qwen（通用VLM）在历史英语文本OCR任务中的表现进行了逐行对比和深入评估。

Result: Qwen具备更低的CER/WER和更强的降噪能力，但有选择性地进行了语言常规化和正字法规范，可能无声地篡改原始文本；TrOCR在保持历史拼写形式上表现更佳，但更容易发生级联式错误。两者误差类型和风险大相径庭。

Conclusion: 具有类似总体准确率的模型可以在错误类型、易察觉性及后续学术风险方面存在显著差异。历史数字化应采用考虑模型结构和归纳偏好的评估方法，而不应仅依赖通用准确率指标。

Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.
  While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.

</details>


### [136] [Cross-view Domain Generalization via Geometric Consistency for LiDAR Semantic Segmentation](https://arxiv.org/abs/2602.14525)
*Jindong Zhao,Yuan Gao,Yang Xia,Sheng Nie,Jun Yue,Weiwei Sun,Shaobo Xia*

Main category: cs.CV

TL;DR: 提出了一个新的框架CVGC，通过建模和增强LiDAR视角变化性，提高点云语义分割的跨视角域泛化能力，并在六个数据集上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现实中LiDAR设备部署位置多样（如不同类型车辆或无人机），导致采集数据的视角、点云结构和密度变化大，现有方法泛化能力不足，限制了实际应用。

Method: 1）提出跨视角几何数据增强模块，模拟不同视角下点云可见性和采样密度变化，生成多种视角版本的同一场景。2）设计几何一致性模块，强制这些不同视角下的点云预测结果（语义、占用）保持一致，提升模型泛化能力。

Result: 在六个公开LiDAR点云数据集上系统评测，首次验证LiDAR点云语义分割领域的跨视角泛化能力。CVGC方法从单一源域泛化到多目标域的表现显著优于现有先进方法。

Conclusion: CVGC框架有效解决了LiDAR语义分割中的跨视角域泛化挑战，为实际多视角应用带来更强的鲁棒性和实用性。

Abstract: Domain-generalized LiDAR semantic segmentation (LSS) seeks to train models on source-domain point clouds that generalize reliably to multiple unseen target domains, which is essential for real-world LiDAR applications. However, existing approaches assume similar acquisition views (e.g., vehicle-mounted) and struggle in cross-view scenarios, where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density. Accordingly, we formulate cross-view domain generalization for LiDAR semantic segmentation and propose a novel framework, termed CVGC (Cross-View Geometric Consistency). Specifically, we introduce a cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density, generating multiple cross-view observations of the same scene. Subsequently, a geometric consistency module enforces consistent semantic and occupancy predictions across geometrically augmented point clouds of the same scene. Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints. The source code will be publicly available at https://github.com/KintomZi/CVGC-DG

</details>


### [137] [MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation](https://arxiv.org/abs/2602.14534)
*Hongpeng Wang,Zeyu Zhang,Wenhao Li,Hao Tang*

Main category: cs.CV

TL;DR: MoRL是一个多模态动作模型，结合了有监督微调和强化学习，通过创新奖励设计提升了对人类动作理解和生成的推理能力和真实性。还提出了推理优化方法CoM，并构建了大规模动作-推理数据集，结果在主流基准上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前人类动作的理解与生成在推理能力和测试时规划方面仍存在局限。提升模型的逻辑推理能力及其物理可行性，并使其对文本和动作的一致性变得更强，是实现更先进视觉与机器人系统的关键。

Method: 作者提出了MoRL，一个统一的多模态动作模型。训练流程包括有监督微调和基于可验证奖励的强化学习。奖励设计针对不同任务：理解任务注重语义对齐和推理连贯性，生成任务关注物理合理性和文本-动作一致性。同时，提出了Chain-of-Motion(CoM)推理方法，实现分步规划与反思。此外，构建了两个人类动作-推理相关的大规模数据集，对齐动作序列、推理轨迹和动作描述。

Result: MoRL在HumanML3D和KIT-ML等主流数据集上，较现有最优基线方法取得了显著性能提升。

Conclusion: MoRL有效提升了动作理解和生成中的逻辑推理与感知真实感水平，相关的推理增强机制和新数据集对人机视觉与机器人领域有重要推动作用。

Abstract: Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>


### [138] [OmniVTON++: Training-Free Universal Virtual Try-On with Principal Pose Guidance](https://arxiv.org/abs/2602.14552)
*Zhaotong Yang,Yong Du,Shengfeng He,Yuhui Li,Xinzhe Li,Yangyang Xu,Junyu Dong,Jian Yang*

Main category: cs.CV

TL;DR: OmniVTON++是一个无需训练、通用适用的虚拟试衣系统，解决了衣物对齐、人体结构一致性与边界连续性等难题，并实现了跨数据集、跨衣物类型的最先进表现。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣方法需针对特定数据条件训练，导致通用性差、部署困难。OmniVTON++旨在提出一种无需针对新任务重训练、可广泛适用的虚拟试衣解决方案。

Method: OmniVTON++结合了三项核心技术：（1）Structured Garment Morphing用于驱动衣物与人物对应变形适配；（2）Principal Pose Guidance在扩散采样过程中分步调控人体结构；（3）Continuous Boundary Stitching实现边界连续优化。三者结合构成了无需特定重训练的一体化流程。

Result: 在多种泛化测试下（如跨数据集、跨衣物类型），OmniVTON++表现出最优性能，并能够适应多种场景、多种扩散模型主干。系统不仅适用于单衣服单人像，还支持多衣服、多人物以及二次元角色的虚拟试衣。

Conclusion: OmniVTON++实现了无需训练、通用性强、效果领先的虚拟试衣功能，极大拓展了虚拟试衣的应用范围，并将在日后开源代码。

Abstract: Image-based Virtual Try-On (VTON) concerns the synthesis of realistic person imagery through garment re-rendering under human pose and body constraints. In practice, however, existing approaches are typically optimized for specific data conditions, making their deployment reliant on retraining and limiting their generalization as a unified solution. We present OmniVTON++, a training-free VTON framework designed for universal applicability. It addresses the intertwined challenges of garment alignment, human structural coherence, and boundary continuity by coordinating Structured Garment Morphing for correspondence-driven garment adaptation, Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and Continuous Boundary Stitching for boundary-aware refinement, forming a cohesive pipeline without task-specific retraining. Experimental results demonstrate that OmniVTON++ achieves state-of-the-art performance across diverse generalization settings, including cross-dataset and cross-garment-type evaluations, while reliably operating across scenarios and diffusion backbones within a single formulation. In addition to single-garment, single-human cases, the framework supports multi-garment, multi-human, and anime character virtual try-on, expanding the scope of virtual try-on applications. The source code will be released to the public.

</details>


### [139] [DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving](https://arxiv.org/abs/2602.14577)
*Chenxu Dang,Sining Ang,Yongkang Li,Haochen Tian,Jie Wang,Guang Li,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DriveFine的Vision-Language-Action（VLA）自动驾驶模型，结合生成式和自纠正能力，有效提升了自动驾驶决策的灵活性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的生成式规划器和基于token的规划器各有优缺点，缺乏一种模型能兼顾解码灵活性与自我修正能力。研究动机在于解决这两类方法在自动驾驶等复杂任务中的局限性。

Method: 提出DriveFine，一种掩码扩散VLA模型。采用创新的block-MoE结构，允许在生成专家之上无缝插入精修专家，并通过推理时专家选择和训练时梯度阻断实现专家间完全解耦。同时设计混合强化学习策略，有效激励精修专家探索，并保持训练稳定性。

Result: 在NAVSIM v1、v2和Navhard基准上进行大量实验，DriveFine在有效性和鲁棒性方面表现优异，显著优于现有方法。

Conclusion: DriveFine通过结合扩散模型与自纠正专家，并采用block-MoE等机制，在自动驾驶的多模态生成、决策任务上兼具灵活性、扩展性和性能优势，对自动驾驶系统有重要推动作用。

Abstract: Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.

</details>


### [140] [YOLO26: A Comprehensive Architecture Overview and Key Improvements](https://arxiv.org/abs/2602.14582)
*Priyanto Hidayatullah,Refdinal Tubagus*

Main category: cs.CV

TL;DR: 本文详细分析了YOLO系列最新版本YOLO26的架构和创新点，强调其推理速度提升及多任务适应性，并首度公布了YOLO26的CNN架构图。


<details>
  <summary>Details</summary>
Motivation: YOLO自诞生以来一直是计算机视觉领域的重要模型，但随着应用场景多样化，对实时性和边缘设备适配性提出更高要求。YOLO26的推出旨在解决推理速度和算力受限设备上的性能瓶颈，并拓展其在实例分割、姿态估计等任务中的应用能力。

Method: 作者主要从YOLO26的源代码和官方文档中，深入解析其主要创新，包括去除DFL损失、实施无需NMS的端到端推理、引入ProgLoss和STAL优化小目标检测，以及采用MuSGD优化器提升推理速度。通过严谨的体系结构分析，绘制了YOLO26的架构图。

Result: YOLO26在CPU模式下推理速度提升43%，并在实例分割、姿态估计、定向框等多任务表现优异。架构分析为社区首次全面公开了YOLO26的CNN架构和详细实现机制。

Conclusion: 研究为YOLO26提供了精确的结构理解，方便后续学者和开发者在其基础上持续优化创新，巩固YOLO在计算机视觉领域的领先地位。

Abstract: You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.

</details>


### [141] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: 本论文提出了VariViT，一种能够处理变尺寸医学影像的改进型视觉Transformer模型，并在脑部MRI数据集上实现了优于标准ViT和ResNet的效果。


<details>
  <summary>Details</summary>
Motivation: 传统ViT需要对图像进行定尺寸裁剪、缩放或填充，但医学影像（如肿瘤区域）形态不规则，固定尺寸处理会导致前景/背景比例失衡、信息损失及伪影，影响诊断。

Method: 提出VariViT模型，可支持输入为变尺寸的图像但保持patch块尺寸一致，采用新颖的位置嵌入重缩放方案适应不同patch数，并设计了新的批处理方法以降低计算复杂度。

Result: 在两个三维脑MRI数据集上的测试结果表明，VariViT在胶质瘤基因型预测和脑肿瘤分类任务分别获得了75.5%和76.3%的F1分数，均优于vanilla ViT和ResNet。同时，新的批处理策略使计算时间减少了多达30%。

Conclusion: VariViT有效提升了医学图像特征表征能力，实现更快的训练与推理速度，在多项诊断任务中显著优于传统方法。

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [142] [VIGIL: Tackling Hallucination Detection in Image Recontextualization](https://arxiv.org/abs/2602.14633)
*Joanna Wojciechowicz,Maria Łubniewska,Jakub Antczak,Justyna Baczyńska,Wojciech Gromski,Wojciech Kozłowski,Maciej Zięba*

Main category: cs.CV

TL;DR: 本文提出了VIGIL，这是首个针对大规模多模态模型（LMMs）在图像重构任务中出现的幻觉错误进行细分类的基准数据集和评测框架。作者对幻觉错误进行了五个维度的拆分，并提出了相应的多阶段检测方法。实验验证了方案的有效性，并开放了数据及工具。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在执行图像相关任务时经常出现“幻觉”问题（即生成与真实内容严重不符的内容），但之前相关研究多将这种错误归为单一类型，缺乏细颗粒度的分析，无法准确定位模型缺陷类型。因此，非常需要一个能够细致分类并检测幻觉错误的数据集和评测框架。

Method: 作者构建了VIGIL数据集，将多模态模型在图像重构中的幻觉划分为五类：拼贴物体幻觉、背景幻觉、物体遗漏、位置/逻辑不一致、物理规律违反。基于此，设计了一套多阶段检测流水线，针对对象级真实性、背景一致性和物体遗漏分别利用开源模型进行检测分类，并通过协同集成提升检测效果。

Result: 通过广泛的实验，作者证明了其多阶段检测方法能够有效地检测和分类不同类型的幻觉错误，体现了对模型错误更详细的解释能力。

Conclusion: VIGIL填补了多模态模型幻觉问题细分类和检测领域的空白，为理解和改进大模型在图像任务中的表现提供了重要工具，数据集与代码已完全开源，促进透明性与后续研究。

Abstract: We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.

</details>


### [143] [SketchingReality: From Freehand Scene Sketches To Photorealistic Images](https://arxiv.org/abs/2602.14648)
*Ahmed Bourouis,Mikhail Bessmeltsev,Yulia Gryaditskaya*

Main category: cs.CV

TL;DR: 本文提出了一种基于调制的新方法，实现从手绘草图生成高保真且与草图语义高度匹配的图像，并引入新损失函数，解决了缺乏像素对齐真值图像的问题，实验验证了在草图语义对齐和生成图像质量方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI多依赖文本等条件输入，尽管草图是人类自然直观的表达方式，但从自由手绘草图生成真实图像因缺乏像素对齐真值且存在抽象、扭曲等特性而极具挑战。以往研究更多关注于边缘图，缺乏对真正自由手绘草图的深入探索。该工作的动机在于提升从自由手绘草图生成图像的语义匹配度和真实感。

Method: 作者提出基于调制的生成方法，优先处理草图的语义信息而非严格对齐边缘，并设计了一种新损失函数，使得不需像素对齐真值也能以自由手绘草图训练模型。

Result: 实验证明该方法在语义上更好地匹配自由手绘草图，并在生成图像的真实感和整体质量上均优于现有方法。

Conclusion: 调制式生成方法及其创新损失函数为自由手绘草图到图像的生成任务带来突破，对提升草图条件生成的实用性和灵活性具有现实意义。

Abstract: Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.

</details>


### [144] [MeFEm: Medical Face Embedding model](https://arxiv.org/abs/2602.14672)
*Yury Borets,Stepan Botman*

Main category: cs.CV

TL;DR: 该论文提出了一种基于改进型JEPA（联合嵌入预测架构）的视觉模型MeFEm，专用于面部图像的生物识别和医学分析，展示了其在关键特征提取和BMI估计上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 面部图像在医学和生物识别中有重要应用，但现有模型对关键区域关注不够且存在领域偏差。论文旨在提升面部医学与生物特征分析的准确性和泛化能力。

Method: 提出MeFEm模型，对JEPA架构进行三项改进：1）引入轴向条纹遮罩，聚焦于语义相关区域学习；2）采用圆形损失加权方案；3）对CLS token概率性重分配，提升线性探测性能。模型在经整理的数据集上训练，并在核心人体测量任务和BMI估计场景下进行评估。

Result: 与FaRL、Franca等基线模型相比，MeFEm在人体测量任务上表现更好，且使用数据量更少。在全新的BMI数据集上也取得良好成绩，同时该数据集缓解了现有数据的领域偏差问题。

Conclusion: MeFEm为面部图像生物识别和医学分析任务提供了更优的基线。权重已公开，有望推动该领域进一步研究。

Abstract: We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.

</details>


### [145] [Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection](https://arxiv.org/abs/2602.14679)
*Chanhui Lee,Seunghyun Shin,Donggyu Choi,Hae-gon Jeon,Jeany Son*

Main category: cs.CV

TL;DR: 本文提出了一种首个通用图像免疫框架，能有效防御基于扩散模型的图像编辑攻击，无需对每张图片单独优化，因此更具可扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像编辑虽然功能强大，但也带来了诸如深度伪造、版权滥用等伦理和法律风险。现有的图像免疫方法通常针对单一图像，难以大规模应用。因此亟需一种高效、通用的防护方法。

Method: 受普适对抗扰动（UAP）攻击技术启发，作者提出了一种生成单一对抗扰动的方法，该扰动可广泛应用于多张图片，用于嵌入目标语义并抑制原始内容，从而阻止扩散模型的恶意编辑。该方法无需训练数据或领域知识，且适用于“无数据”场景。

Result: 大量实验表明，所提方法在UAP背景下显著优于现有基线方法，并且在受限扰动预算下效果可媲美针对单一图像的优化方法，还展现了良好的黑盒迁移能力。

Conclusion: 本文首次实现了通用图像免疫，为防御AI语义篡改提供了有效且实用的工具，尤其适用于实际的扩散模型应用场景中。

Abstract: Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic manipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by universal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model's attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the UAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.

</details>


### [146] [It's a Matter of Time: Three Lessons on Long-Term Motion for Perception](https://arxiv.org/abs/2602.14705)
*Willem Davison,Xinyue Hao,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 本论文探讨了长期运动信息在视觉感知中的作用，展示了其在理解动作、物体、材料和空间信息中的重要性，并优于传统图像信息。


<details>
  <summary>Details</summary>
Motivation: 长期以来，时间信息（尤其是运动信息）被认为对感知至关重要。然而，尽管图像信息在感知任务中已有大量研究，关于长时间尺度的运动信息在视觉学习中的作用却了解较少。因此，作者希望明确长时间运动信息到底能为视觉学习带来哪些关键能力。

Method: 作者利用近期在点轨迹估计领域的进展，将其作为学习时序表征的一种方式，并在多个感知任务上进行实验分析。这些实验比较了基于长期运动表征与基于静态图像或标准视频特征的差异和优劣。

Result: 研究发现：1）长期运动表征不仅能够理解动作，还能比图像更好地理解物体、材料和空间信息；2）长期运动表征在样本少和零样本任务中具有更强的泛化能力；3）运动信息的低维性质使其在计算量和准确率之间有更好平衡，且与视频特征结合能提供更高性能。

Conclusion: 长期运动信息对于视觉感知具有独特且强大的表示能力，对未来感知模型的设计与优化具有重要指导意义。

Abstract: Temporal information has long been considered to be essential for perception. While there is extensive research on the role of image information for perceptual tasks, the role of the temporal dimension remains less well understood: What can we learn about the world from long-term motion information? What properties does long-term motion information have for visual learning? We leverage recent success in point-track estimation, which offers an excellent opportunity to learn temporal representations and experiment on a variety of perceptual tasks. We draw 3 clear lessons: 1) Long-term motion representations contain information to understand actions, but also objects, materials, and spatial information, often even better than images. 2) Long-term motion representations generalize far better than image representations in low-data settings and in zero-shot tasks. 3) The very low dimensionality of motion information makes motion representations a better trade-off between GFLOPs and accuracy than standard video representations, and used together they achieve higher performance than video representations alone. We hope these insights will pave the way for the design of future models that leverage the power of long-term motion information for perception.

</details>


### [147] [Depth Completion as Parameter-Efficient Test-Time Adaptation](https://arxiv.org/abs/2602.14751)
*Bingxin Ke,Qunjie Zhou,Jiahui Huang,Xuanchi Ren,Tianchang Shen,Konrad Schindler,Laura Leal-Taixé,Shengyu Huang*

Main category: cs.CV

TL;DR: CAPA是一种高效的测试时优化框架，通过极少参数调整将预训练的3D基础模型适配于稀疏几何线索下的深度补全任务，且无需重新训练主干网络，能在多场景内取得最先进表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法为特定任务训练新的编码器以融合辅助输入，容易过拟合且泛化性差。作者希望提高模型在面对场景稀疏观测时的泛化能力，并减少适配时需调整的模型参数。

Method: CAPA冻结基础模型主干，仅利用如LoRA或VPT等参数高效微调方法，在推理阶段依据稀疏观测的梯度更新极少参数。对于视频，还设计了序列级参数共享，实现多帧联调以提升时间一致性和鲁棒性。此外，CAPA为模型无关设计，支持所有基于ViT的3D基础模型。

Result: CAPA能有效利用少量测量来校正模型在特定场景下的误差，如结构扭曲等问题，并在室内外多种复杂条件下取得了最好的深度补全效果。

Conclusion: CAPA方法显著提升了基础模型在多样化场景中的适应性和泛化能力，实现了最优性能，且仅需极少的参数更新，适合实际部署和拓展。

Abstract: We introduce CAPA, a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models (FMs) for depth completion, using sparse geometric cues. Unlike prior methods that train task-specific encoders for auxiliary inputs, which often overfit and generalize poorly, CAPA freezes the FM backbone. Instead, it updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (e.g. LoRA or VPT), guided by gradients calculated directly from the sparse observations available at inference time. This approach effectively grounds the foundation model's geometric prior in the scene-specific measurements, correcting distortions and misplaced structures. For videos, CAPA introduces sequence-level parameter sharing, jointly adapting all frames to exploit temporal correlations, improve robustness, and enforce multi-frame consistency. CAPA is model-agnostic, compatible with any ViT-based FM, and achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. Project page: research.nvidia.com/labs/dvl/projects/capa.

</details>


### [148] [SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning](https://arxiv.org/abs/2602.14767)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新框架SAILS，用于解决增量语义分割中的遗忘和高计算成本难题，能显著提升任务序列中的鲁棒性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法面临着遗忘、计算资源消耗大以及需要反复训练等问题，这些都严重限制了其在实际场景下的应用。作者希望设计一种无需重复训练、免于遗忘并且高效的增量分割方法。

Method: 提出SAILS框架，将增量语义分割拆解为两个阶段：1）使用Segment Anything Model（SAM）进行零样本区域提取；2）通过在固定特征空间中的原型进行语义关联。框架还采用类内聚类生成多个类原型，更好地建模类内多样性。整个流程无需任何增量训练。

Result: SAILS框架在标准的增量语义分割数据集上，其无训练条件下的表现往往超过了现有依赖训练的方法，尤其在任务序列较长、遗忘问题严重时效果更佳。

Conclusion: SAILS通过彻底避免参数更新，完全消除遗忘现象，并使模型在不同任务中保持一致表现。此外，SAILS还展现出正向的反向迁移能力，新类的加入可以提升对旧类的性能。

Abstract: Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.

</details>


### [149] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 本文提出了GOT-JEPA和OccuSolver，用于提升目标跟踪中泛化性与遮挡感知能力，通过模型预测式预训练和细粒度遮挡建模显著提升了跟踪器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有目标跟踪方法在新场景中易受限于训练目标，泛化能力不足，对遮挡问题的建模较为粗略，难以应对复杂动态环境。

Method: 提出GOT-JEPA，通过教师-学生预测框架实现跟踪模型水平的预测式自监督预训练，即教师在干净帧上生成伪跟踪模型，学生由噪声帧预测，增强鲁棒性；进一步提出OccuSolver，采用以点为中心的点跟踪方法实现细粒度目标可见性估计与遮挡状态逐步优化。

Result: 在七个主流数据集上进行大量实验，结果显示方法在跟踪器的鲁棒性、泛化能力及遮挡建模能力上显著提升。

Conclusion: GOT-JEPA结合OccuSolver能有效提升目标跟踪任务中对动动态及复杂遮挡环境的适应性，为提升泛化与遮挡感知提供新范式。

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [150] [VIPA: Visual Informative Part Attention for Referring Image Segmentation](https://arxiv.org/abs/2602.14788)
*Yubin Cho,Hyunwoo Yu,Kyeongbo Kong,Kyomin Sohn,Bongjoon Hyun,Suk-Ju Kang*

Main category: cs.CV

TL;DR: 本文提出了一种名为VIPA的新框架，通过关注视觉上下文中的关键信息部位，提高了指向式图像分割的精度，并在四个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的指向式图像分割方法虽然利用了视觉信息，但在融合视觉与语言特征时存在高方差和语义一致性问题，导致精细分割效果有限。

Method: 提出了视觉信息部位注意力（VIPA）框架，核心创新在于引入视觉表达（visual expression）。具体包括一个视觉表达生成器（VEG）模块，通过本地-全局语言上下文检索并优化视觉token，去除噪声，增强兴趣区域的上下文及视觉属性。通过改进注意力机制，提升网络对细粒度目标区域的关注与分割效果。

Result: 在四个公开的指向式图像分割基准上，VIPA获得了超过现有先进方法的表现。大量实验和可视化分析验证了该方法的有效性。

Conclusion: VIPA通过更好地利用视觉环境中的关键部位和上下文，增强了视觉和语言语义一致性，并在实际分割任务上取得了优异性能，具有较高应用价值。

Abstract: Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.

</details>


### [151] [Debiasing Central Fixation Confounds Reveals a Peripheral "Sweet Spot" for Human-like Scanpaths in Hard-Attention Vision](https://arxiv.org/abs/2602.14834)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyosh*

Main category: cs.CV

TL;DR: 本文针对视觉硬注意力模型与人眼注视轨迹（scanpath）对齐进行分析，发现常见指标受中心偏置影响严重，提出消除偏置的新指标（GCS）并揭示更真实的人机行为一致性。


<details>
  <summary>Details</summary>
Motivation: 许多视觉硬注意力模型通过与人类注视路径相似性评价，但常用的评估指标在对象为中心的数据集上容易受到中心偏置干扰，导致评价结果不准确，不利于行为一致性的真实分析。

Method: 作者使用Gaze-CIFAR-10数据集，比较了各种注视策略（包括中心注视的简单基线和可学习策略），并通过改变视野范围分析不同模型的扫描路径；同时提出GCS（Gaze Consistency Score）指标，结合中心去偏与运动相似性综合评价。

Result: 中心注视的基线表现出意外的高得分，接近许多学习策略，使常规指标乐观且难以区分真实行为一致和单纯中心化。通过GCS指标，作者揭示在中等感受野下模型的注视路径既去除了中心偏置又与人类在时间动态上相似，并指出视野过大时模型可能走捷径而非真正学习行为模式。

Conclusion: 传统指标在对象中心数据集上夸大了人机行为一致性，GCS能够更有效地区分真实对齐与中心偏置，对任务驱动主动视觉模型的评估和测试基准的设计具有重要意义。

Abstract: Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a "shortcut regime" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.

</details>


### [152] [Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2602.14837)
*Lorenzo Mur Labadia,Ruben Martinez-Cantin,Jose J. Guerrero,Giovanni M. Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文提出了改进短期物体交互预测（STA）的方法，介绍了基于注意力机制的新模型，并首次将环境可供性和交互热点引入预测流程，在公开数据集上取得了大幅提升。


<details>
  <summary>Details</summary>
Motivation: 短期物体交互预测对于可穿戴助手理解用户意图、提供及时帮助及人机交互等应用具有重要意义。然而，目前方法在效率和准确性上仍有限制。

Method: 作者提出了STAformer和STAformer++两种新型注意力机制架构，引入了帧引导的时序池化、图像视频双重注意力及多尺度特征融合。创新性地整合了环境可供性模型作为持久记忆，并通过不同策略实现与主网络的融合。此外，利用手部与物体轨迹预测交互热点，提升预测的定位精度。

Result: 在Ego4D和EPIC-Kitchens数据集上，提出方法在Top-5 mAP指标上取得了显著提升，最高分别提升23和31个百分点。

Conclusion: 通过将环境可供性和交互热点引入STA预测，极大提升了预测精度，为感知系统和智能助手的发展奠定了基础，资源的开放也促进了该领域的进一步研究。

Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>


### [153] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: 本论文提出了一种基于多维持性sheaf拉普拉斯（MPSL）的框架，用于在单纯复形上进行图像分析，能够在多种降维参数下获得更稳定的表现，并优于PCA等传统方法。


<details>
  <summary>Details</summary>
Motivation: 受限于传统降维方法如PCA对降维维数选择敏感，容易导致结果不稳定。论文希望通过同时利用多种降维维数的互补优势，提升图像分析的鲁棒性和有效性。

Method: 方法上，将图像样本建模为单纯复形，应用持续sheaf拉普拉斯以提取多尺度的局部拓扑谱表达。随后对不同尺度和维数下的谱进行统计汇总，得到多尺度多维图像表示。该框架在COIL20和ETH80数据集上进行了分类实验评估。

Result: 实验结果表明，该方法在较宽范围的降维参数下性能更稳定，在中等降维度区域显著优于基线PCA方法。

Conclusion: 多维persistent sheaf拉普拉斯框架在图像的多尺度、多维表征任务中表现优越，提高了对降维参数选择的鲁棒性，并对传统方法构成有力补充。

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [154] [CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography](https://arxiv.org/abs/2602.14879)
*Qingqing Zhu,Qiao Jin,Tejas S. Mathai,Yin Fang,Zhizheng Wang,Yifan Yang,Maame Sarfo-Gyamfi,Benjamin Hou,Ran Gu,Praveen T. S. Balamuralikrishna,Kenneth C. Wang,Ronald M. Summers,Zhiyong Lu*

Main category: cs.CV

TL;DR: 本文介绍了CT-Bench，这是首个公开的包含病灶级别标注的CT影像基准数据集，并评估了多模态AI模型在此数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽可自动分析CT影像和生成报告，但受限于缺乏带有精准病灶标注的公开CT数据集，因此需要新型高质量基准以推动多模态医学AI进展。

Method: 构建了CT-Bench数据集，包含两部分：1）带边界框、描述和大小信息的20,335个病灶标注；2）2,850组涵盖病灶定位、描述、测量和属性分类的视觉问答任务。并有意添加困难负例模拟临床实际。基于该数据集，评估了多种先进多模态模型（如医学CLIP等）性能，并与放射科医生对比。

Result: 实验显示，CT-Bench能够区分不同模型能力。模型在本数据集上的表现明显优于无微调时，并能与专业医生比较。微调后各模型在定位、属性分类等各项指标均显著提升。

Conclusion: CT-Bench填补了CT病灶标注公开数据集的空白，为评估和提升医学影像AI提供了标准平台，可显著助力医疗AI的临床实用性与研究发展。

Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.

</details>


### [155] [Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery](https://arxiv.org/abs/2602.14929)
*Chandrakanth Gudavalli,Tajuddin Manhar Mohammed,Abhay Yadav,Ananth Vishnu Bhaskar,Hardik Prajapati,Cheng Peng,Rama Chellappa,Shivkumar Chandrasekaran,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本文提出了Wrivinder系统，可在缺乏GPS及大视角差的情况下，实现地面照片与卫星影像的几何驱动零样本对齐，并发布了相关评测数据集MC-Sat。


<details>
  <summary>Details</summary>
Motivation: 当前地面图像与卫星地图对齐对于地图测绘、导航与场景理解很重要，但在视角差大或GPS不可用时一直困难，并且缺乏合适的评测数据集。

Method: Wrivinder结合了SfM（结构光还原）、3D Gaussian Splatting、语义约束和单目深度等方法，将多张地面照片聚合、重建3D场景，生成可直接与卫星图像对比的顶视渲染图，实现高精度地理定位。并构建了MC-Sat数据集以支持该任务的系统性评估。

Result: 在无监督零样本测试中，Wrivinder系统在密集和大范围场景下均实现了小于30米的定位误差，性能优异。

Conclusion: Wrivinder和MC-Sat为几何为中心的跨视角影像对齐研究提供了首个系统性基线与测试平台，展示了几何聚合方法在地面与卫星定位任务中的显著潜力。

Abstract: Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.

</details>


### [156] [AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories](https://arxiv.org/abs/2602.14941)
*Zun Wang,Han Lin,Jaehong Yoon,Jaemin Cho,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: AnchorWeave 通过用多个局部几何记忆替代全局 3D 场景记忆，显著提升了摄像头可控视频生成的空间一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的视频生成方法在保持长时间空间一致性方面面临挑战，主要问题在于全局 3D 场景重建会因跨视角的对齐误差（如位姿和深度估计错误）而引入噪声，从而影响生成质量。

Method: AnchorWeave 框架用多个干净的本地几何记忆取代单一全局记忆，并设计了多锚点控制器进行多记忆融合。在生成过程中，通过覆盖驱动的本地记忆检索与目标轨迹对齐，挑选并融合合适的本地记忆。

Result: 大量实验表明，AnchorWeave 显著提升了长序列下场景的一致性与可视质量。消融和分析实验进一步验证了局部几何条件、多锚点控制和覆盖驱动检索机制的有效性。

Conclusion: AnchorWeave 能有效解决跨视图误差的累积问题，提升记忆驱动视频生成的空间一致性，是实现高质量长时空控制生成的有力方案。

Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

</details>


### [157] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: 本文指出当前视觉-语言模型（VLMs）虽然在RGB图像上表现优异，但难以泛化到热成像领域。作者提出了ThermEval-B基准，专门用于评估热成像条件下模型的问答和感知能力。实验发现，现有VLM在温度相关推理任务上表现不佳，强调了开发专门面向热成像的评价和模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 热成像在光照不足或可见光失败的环境中（如夜间监控、救援、自动驾驶、医疗筛查）至关重要，但现有VLM主要基于颜色或纹理信息，不能自动适应热图像中温度信息的表征与推理。缺乏专门的评测基准导致模型在热成像领域提升受限，因此作者希望通过构建新的基准推动能力提升。

Method: 作者提出了ThermEval-B基准，包括大约5.5万个热成像视觉问答对，覆盖各种室内外环境、丰富的密集每像素温度图以及语义体部件标注。通过整合公开数据集和自建的ThermEval-D，实现了对热成像下模型理解能力的系统性评估。

Result: 对25个主流VLM进行评估后发现，所有模型在基于温度的推理任务上均表现不佳；在色图变换下性能进一步下降，容易受限于语言惯性或固定回复。即使使用高级提示或有监督微调，提升也极为有限。

Conclusion: 热成像下的视觉-语言理解与RGB图像有显著不同，需要专门的评测和建模方法。ThermEval作为首个此类基准，有助于推动热成像视觉-语言模型的发展。

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


### [158] [Image Generation with a Sphere Encoder](https://arxiv.org/abs/2602.15030)
*Kaiyu Yue,Menglin Jia,Ji Hou,Tom Goldstein*

Main category: cs.CV

TL;DR: 提出了一种名为Sphere Encoder的高效图像生成框架，实现了单步或少步生成高质量图片，性能可比扩散模型但推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在高质量图像生成领域表现突出，但生成过程通常需要多步推断，计算成本高。作者试图探索能否通过更高效的方法实现速度和质量的兼得。

Method: Sphere Encoder包含一个编码器和解码器。编码器将真实图像均匀映射到一个球面隐空间，解码器则从球面隐空间随机点反向生成图像。训练仅基于重构损失，无需复杂的判别器或多阶段过程。该架构也便于条件图像生成，重复编码-解码还能提升成像质量。

Result: 在多个数据集上的实验结果显示，Sphere Encoder的生成效果与主流扩散模型相当，但生成速度更快，推理成本更低。

Conclusion: Sphere Encoder在高效图像生成领域提供了一种新思路，可在推理资源有限场景下作高质量生成任务，是对当前主流扩散模型的重要补充。

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

</details>


### [159] [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/abs/2602.15031)
*Yehonathan Litman,Shikun Liu,Dario Seyb,Nicholas Milef,Yang Zhou,Carl Marshall,Shubham Tulsiani,Caleb Leak*

Main category: cs.CV

TL;DR: 本文提出了EditCtrl，一个高效的生成式视频修补方法，大幅降低运算成本，专注于需要编辑的区域，并实现了更高的编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有高保真生成式视频编辑方法虽然质量好，但由于处理整个视频上下文导致计算成本极高，特别是对于只需局部修改的情况。作者希望改进编辑效率，并提升编辑效果。

Method: 提出EditCtrl框架，其核心为两个模块：一是只处理被遮罩的局部令牌的局部视频上下文模块，使计算成本随编辑区域大小线性增长；二是轻量级时间全局上下文嵌入器，实现整段视频上下文一致性。结合局部生成和全局指导，既高效又保证编辑连贯。

Result: EditCtrl比当前最先进方法的计算效率提高10倍，同时编辑质量也超越了全注意力机制的方法。还支持多区域文本提示编辑和自回归内容传播的新功能。

Conclusion: EditCtrl兼具高效率和高质量的视频编辑能力，为生成式视频编辑带来了全新能力，具有广泛应用前景。

Abstract: High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [160] [Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation](https://arxiv.org/abs/2602.13263)
*Ligong Lei,Wenwen Lu,Xudong Pang,Zaokere Kadeer,Aishan Wumaier*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考标签的多模态一致性引导数据选择流程，用于提升自动语音识别（ASR）系统在带口音语音上的适应能力，通过音频-文本一致性和WER预测筛选伪标签，有效提升了口音迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统在带口音语音上表现不佳，主因在于口音引起的声学与韵律特征变化与训练数据不匹配，而手动标注适应数据代价高昂。常用伪标签选择方法主要基于文本，容易选出语音上不匹配的标签，导致微调性能下降。

Method: 方法包括：1）基于子模互信息的预选，提升目标相关性并减少计算；2）对每条语音生成多条伪转录，并用无需参考的信号（嵌入空间音频-文本一致性和预测WER）为假设打分；3）基于分位数规则保留可靠伪标签；4）最终用这些高质量伪标签进行模型微调。

Result: 在同域任务中，从3万条样本中选取约1,500条高相关语音，模型WER达到10.91%，接近全监督（10.45%）。在跨域（口音不匹配）下，所提方法有效避免未筛选伪标签带来的退化，并在强ASR骨干下优于随机采样和先进基线。

Conclusion: 采用多模态一致性和无需参考的分数进行伪标签筛选，能显著提升ASR系统对口音的自适应能力，在低资源和高成本场景下提供实用的模型提升方案。

Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics are largely text-centric (e.g., perplexity (PPL) filtering) and can prefer fluent yet acoustically mismatched hypotheses, leading to error amplification when fine-tuning. To address this, we introduce a multimodal consistency-guided, reference-free data selection pipeline for ASR accent adaptation under a transductive, label-free protocol. The pipeline starts with a target-aware preselection step based on submodular mutual information to improve query relevance and reduce downstream computation. It then generates multiple pseudo-transcriptions per utterance via perturbation-based decoding and scores each hypothesis using two reference-free signals: speech--text alignment in a shared embedding space and predicted word error rate (WER). A simple percentile-based selection rule retains reliable pseudo-labels for fine-tuning while discarding noisy utterances. In an in-domain setting, selecting ~1.5k utterances from a 30k pool achieves 10.91% WER, close to 10.45% obtained using 30k supervised labels. In a cross-domain setting with a mismatched candidate pool, consistency-filtered subsets avoid the degradation caused by unfiltered pseudo-labels under strong accent shift, and matched-hour experiments on a stronger ASR backbone further confirm gains over random sampling and recent selection baselines.

</details>


### [161] [LLM-Powered Automatic Translation and Urgency in Crisis Scenarios](https://arxiv.org/abs/2602.13452)
*Belu Ticona,Antonis Anastasopoulos*

Main category: cs.CL

TL;DR: 本论文评估了现有大型语言模型（LLMs）和机器翻译系统在危机领域的多语言翻译能力，特别关注翻译中紧急性信息的保留，发现两者在危机场景下表现不稳定且存在变形风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在危机应对中被广泛提议用于多语种沟通，其在高风险危机场景下的实际适用性却缺乏充分评估。危机传播对紧急性的准确传递至关重要，翻译失误可能造成严重影响，因此有必要系统分析现有通用语言技术在危机环境下的可靠性。

Method: 作者采用多语种危机数据和一个涵盖32种以上语言、带有紧急性标注的新数据集，对前沿的LLMs和专门的翻译模型进行性能测试，并评估其在紧急信息传递方面的表现。此外，比较了不同输入语言下LLMs对紧急性分类的稳定性。

Result: 研究发现，无论是专用翻译模型还是LLMs，在危机域的表现均大幅退化，翻译结果存在较大不稳定性。即使翻译语法准确，紧急性的表达也可能被误导或削弱，LLMs对紧急性的分类结果严重依赖输入语言和提示方式。

Conclusion: 普适型语言技术在危机场景下存在显著部署风险，尤其是在紧急性表达的准确性上不足。作者呼吁建立紧贴危机需求的评估体系，以确保技术部署的安全与有效。

Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.

</details>


### [162] [Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety](https://arxiv.org/abs/2602.13455)
*Phyllis Nabangi,Abdul-Jalil Zakaria,Jema David Ndibwile*

Main category: cs.CL

TL;DR: 本文针对斯瓦希里语环境下儿童网络欺凌问题，研究了难以识别的侮辱性变体语言自动检测方法。通过机器学习模型，评估了多种检测算法在低资源语言下的效果，并提出未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 数字技术普及带来了网络欺凌及在线辱骂的激增，尤其影响儿童。斯瓦希里语作为使用广泛但低资源的非洲语言，缺乏高效的语言处理工具，因此该研究旨在填补对其网络恶意言论识别的空白。

Method: 采用了支持向量机（SVM）、逻辑回归和决策树等机器学习模型，并应用SMOTE等技术处理数据不平衡问题。通过精细调参和高维文本数据分析，评估各模型在识别变体侮辱语言的性能。

Result: 结果显示，尽管所用模型对高维文本数据具备较好效果，但受限于数据集规模小和类别失衡，模型表现（如F1分数、准确率、召回率）难以泛化。各模型在细粒度性能表现上有所差异。

Conclusion: 本研究为低资源语言环境下的网络欺凌检测提供初步探索，强调扩充数据集和采用先进算法的重要性。未来将尝试迁移学习、多模态数据等新方法，以提高网络空间安全性和跨文化适应性。

Abstract: The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.
  We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.
  This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.

</details>


### [163] [Language Model Memory and Memory Models for Language](https://arxiv.org/abs/2602.13466)
*Benjamin L. Badger*

Main category: cs.CL

TL;DR: 本文探讨了机器学习模型，尤其是语言模型和自编码器，在隐藏层向量嵌入中存储输入信息（即“记忆”）的能力并不充分。通过不同训练目标能显著提升信息储存和处理效率。提出新的架构和训练方法以改善现有模型的记忆能力。


<details>
  <summary>Details</summary>
Motivation: 虽然隐层嵌入通常被认为存储了输入的重要信息（即模型记忆），但实际模型的记忆能力特别是在语言模型中远未被充分理解。更深入刻画这种存储能力有助于提升计算和建模效率。

Method: 本文对比了语言模型和自编码器训练下的隐层嵌入信息容量，并提出使用信息丰富的记忆嵌入替代常规token序列，实现计算效率提升。进一步引入可并行化的编码器-解码器记忆模型架构，并通过改变训练目标（联合因果预测和信息保持）及冻结高保真编码器和课程式训练提升记忆形成。

Result: 实验发现传统语言模型嵌入本身包含的信息很有限，而自编码器可以实现几乎完美的记忆。结合信息保持目标和合适的训练流程可以得到信息丰富的内部表示，有效提升模型存储和访问输入信息的能力。

Conclusion: 单纯使用下一个token预测的训练目标无法有效形成高质量记忆，需用联合目标调优模型记忆能力。新提出的架构和训练流程能提升模型在存储和处理输入信息方面的性能，未来相关模型设计应更加关注任务与记忆目标的结合。

Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.

</details>


### [164] [From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier](https://arxiv.org/abs/2602.13504)
*Ozancan Ozdemir*

Main category: cs.CL

TL;DR: 本研究首次量化了土耳其新闻媒体中AI生成内容的比例，发现约2.5%的新闻内容由大模型（LLM）改写或修订。


<details>
  <summary>Details</summary>
Motivation: 当前关于新闻编辑室中大语言模型应用的研究多集中于英语媒体，土耳其媒体方面尚无实证数据，仅有记者采访等定性研究，缺乏定量方法对AI生成内容的实际情况进行分析。因此，亟需通过数据驱动的方法完整描绘AI内容在土耳其新闻中的普及情况。

Method: 作者将土耳其专用的BERT模型（dbmdz/bert-base-turkish-cased）在带标签的3600篇新闻文本上微调，用于AI改写内容的二分类识别，并在3500+未见样本上批量测试，分析不同媒体和时间跨度下的分布。

Result: 模型在测试集上F1分数达到0.9708，在未见样本上平均置信度超过0.96，估算有2.5%的新闻被LLM改写或修订，且媒体间与时间上分布相对稳定。

Conclusion: 本研究首次用实证方法量化了土耳其新闻中AI生成内容的占比，为未来媒体AI应用的研究与监管提供了数据基础，突破了以往依赖采访和自报的数据局限。

Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.

</details>


### [165] [Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens](https://arxiv.org/abs/2602.13517)
*Wei-Lin Chen,Liqian Peng,Tian Tan,Chao Zhao,Blake JianHang Chen,Ziqian Lin,Alec Go,Yu Meng*

Main category: cs.CL

TL;DR: 本文发现，大语言模型推理能力强与否并不和生成长度正相关，但与生成中“深度思考”token比例显著正相关。据此提出高效推理方法Think@n，能在降低推理成本的同时保持甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型推理能力提升通常依赖于较长的思维链生成，但生成长度其实并不能可靠表征推理质量，有时还会导致过度思考而下降。因此需要寻找更准确的推理质量衡量方法。

Method: 作者提出“深度思考token”概念，即在模型内部层数递进中预测发生明显修正的token；利用生成序列中深度思考token比例（deep-thinking ratio）作为新的推理质量指标。并设计Think@n策略，在推理时优先保留深度思考比例高的样本，实现高效推理。

Result: 在AIME 24/25、HMMT 25、GPQA-diamond等复杂基准以及多个主流LLM模型上实验证明，深度思考比例与推理准确率强相关，优于传统基于长度或置信度的衡量。采用Think@n策略，在保持或超越主流方法性能的同时，大幅降低推理计算成本。

Conclusion: 深度思考token比例是衡量LLM推理质量的有效指标，Think@n策略无须延长推理过程即可提升整体效率和准确性。为高效大模型推理带来新思路。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal "overthinking," leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.

</details>


### [166] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 本论文提出了一种新的LLM置信度校准方法——能力校准（capability calibration），旨在更准确地反映模型整体解决问题的概率，而不仅是单个回答的正确率。实验显示，该方法在预测正确率和资源分配方面效果更佳。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型的应用广泛，对置信度的准确估计非常重要。以往聚焦于单一回答的置信度，但这无法涵盖许多实际用例中用户最关心的问题：模型总体能否解决某个任务。因此，单一输出的置信度并不能全面评估模型能力。

Method: 区分并正式定义了能力校准（capability calibration）与传统响应校准（response calibration），并设计了一套实证评测流程，对若干置信度估计方法进行了实验对比。

Result: 实验结果表明，能力校准方法能更好地提升pass@$k$等评估指标，并优化推理资源的分配，表现优于传统的响应校准。

Conclusion: 能力校准提供了比响应校准更契合实际需求的新思路，为大语言模型在多应用场景下的可靠使用打下了基础，具备广泛的实际应用前景。

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [167] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考答案和评分标准的奖励建模方法FLIP，通过反向推断（从生成的回复反推最有可能的指令），用推断出来的指令与原指令的相似度作为奖励信号。实验结果显示FLIP优于现有主流方法，尤其在小模型、长输出和奖励攻击鲁棒性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评判（LLM-as-a-Judge）方法依赖强大推理能力，或需参考答案/显式评分标准，这限制了方法的广泛适用性和开放性。因此亟需一种无需额外答案和评分标准的奖励建模方法，以降低门槛并提升泛化能力。

Method: 提出FLIP方法，即对每个LM输出，利用小模型反向推断最可能导致该输出的原始指令，然后计算推断指令与原指令的相似性，作为奖励信号。该方法无需参考答案和评分量表。作者在四个领域、13个小模型上测试了该方法。

Result: FLIP在四个领域内、13个小模型上的平均指标性能比传统LLM-as-a-Judge方案高出79.6%。FLIP也显著提升了下游任务的测试时扩展性，并对长文本输出和奖励攻击展现出更强鲁棒性。

Conclusion: FLIP方法利用“验证-生成差异”，能够在小模型、无参考答案/评分量表情境下有效进行奖励建模，表现优于主流判别方法，为可靠的奖励机制提供了新方向。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [168] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: 本文提出了一种新型知识蒸馏方法DistillLens，通过在词表空间对齐学生模型和教师模型的中间状态，提升了模型压缩后的性能，并在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法只关注模型输出，忽视了教师模型推理过程中的不确定性和中间层信息，导致学生模型难以充分学习教师的推理能力。现有的特征蒸馏方法又未能捕捉输出所需的丰富不确定性。

Method: DistillLens方法借助Logit Lens，将学生和教师模型的中间隐藏状态投影到词表空间，并通过对称散度目标进行结构对齐。该对齐过程对学生和教师输出间的差异进行双向惩罚，防止模型过于自信或不自信，保留高熵的信息通路。

Result: 在GPT-2和Llama模型上的大量实验表明，DistillLens在各类指令跟随任务上持续优于标准KD和主流特征迁移基线。

Conclusion: DistillLens有效融合了教师模型推理过程的不确定性信息，提升了知识蒸馏效果，为大语言模型压缩提供了更优的方案。

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [169] [LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.13571)
*Zhipeng Song,Xiangyu Kong,Xinrui Bao,Yizhi Zhou,Jiulong Jiao,Sitong Liu,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练且可直接应用的大语言模型置信度再排序（LCR）算法，通过利用LLM的内在置信度信号，有效提升检索增强生成（RAG）系统的文档排序表现，兼顾效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG技术已用于缓解LLM在知识密集任务中的幻觉问题，但现有的再排序方法往往需要复杂的训练和高计算成本，且未充分利用LLM的语义理解能力和置信度信号。因此，作者希望找到一种更高效且实用的再排序方法。

Method: LCR方法基于最大语义聚类比例（MSCP）从黑箱LLM中提取置信度信号，分为两阶段：首先通过多项式采样及语义聚类评估查询及文档置信度；随后采用置信度分箱与多级排序策略，对相关文档优先排序，并在高置信度查询下保留原排序，提升系统鲁棒性。

Result: 在BEIR和TREC基准及BM25、Contriever检索器实验下，LCR使用7-9B参数规模LLM，无需再训练，即可在多种再排序方案中将NDCG@5提升最多20.6%，且未出现性能劣化。消融实验表明LLM置信度与文档相关性高度正相关。

Conclusion: LCR兼具高效性、易用性与广泛兼容性，有助于缓解如医学诊断等场景中因幻觉导致的风险，为RAG系统中文档相关性判断提供更优解。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.

</details>


### [170] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: 提出了一种名为Elo-Evolve的新型大语言模型对齐方法，将对齐任务转变为多智能体自适应竞争，并通过动态对手选择和直接对战结果优化，实现了更高的训练稳定性和更低的噪声敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法通常依赖单一、静态的奖励函数，面临数据稀缺、噪声大和训练不稳定等问题，因此需要一种更动态、鲁棒性更高的对齐机制。

Method: 提出Elo-Evolve框架，通过多智能体之间的动态比赛进行对齐。主要创新包括：(1) 不再依赖Bradley-Terry模型，直接基于二元胜负结果学习；(2) 采用Elo评分机制进行对手选择，实现自动课程学习和难度自适应。借助PAC学习理论论证了配对比较具备更优样本复杂度。

Result: 实验证明Elo-Evolve可以将噪声降低4.5倍，且在Alpaca Eval 2.0和MT-Bench两个评测中表现优于点数方法和静态配对训练，展现出明显的性能提升顺序。

Conclusion: Elo-Evolve框架能够有效提升大语言模型对齐的稳定性、样本效率和泛化能力，是配对比较和动态竞争在对齐任务中的有效实践。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [171] [Metaphors' journeys across time and genre: tracking the evolution of literary metaphors with temporal embeddings](https://arxiv.org/abs/2602.13701)
*Veronica Mangiaterra,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本研究利用历时分布式语义技术，探讨了19世纪至今的意大利文学与非文学语料中隐喻加工难度的变化。结果显示，整体上隐喻的语义相似度（即加工难度）随时间保持稳定，但不同时期与文体对隐喻难度有重要影响。


<details>
  <summary>Details</summary>
Motivation: 文学隐喻具有独特性，但相比日常隐喻实验研究较少，且过往心理语言学和计算研究忽略了时间维度。本研究旨在探讨文学隐喻随时间与文体的演变及其理解难度变化，填补该领域的研究空白。

Method: 研究者在19世纪与21世纪的意大利文学和非文学大语料库上训练词嵌入模型，统计分析515个19世纪文学隐喻的主题词和喻体词的语义相似度，并以此作为隐喻加工难度的代理指标。还考察了语义向量一致性与词语语义邻域密度等个体语义特征的影响。

Result: 整体来看，隐喻词之间的语义相似度随时间未发生显著变化，但体裁显著影响理解难度：现代文学中隐喻更难理解，而现代非文学环境（如网络语料）中的隐喻比19世纪非文学文本更易理解。同时，隐喻个体的语义特征也对难度起到调节作用。

Conclusion: 研究揭示文体与时代变迁对隐喻理解难度的复杂影响，反映出意大利现代文学趋于风格简化，增加了隐喻加工难度；而网络语境的高度创造性提升了隐喻的可理解性。这为历时视角下的隐喻认知和语言变化研究提供了新证据。

Abstract: Metaphors are a distinctive feature of literary language, yet they remain less studied experimentally than everyday metaphors. Moreover, previous psycholinguistic and computational approaches overlooked the temporal dimension, although many literary metaphors were coined centuries apart from contemporary readers. This study innovatively applies tools from diachronic distributional semantics to assess whether the processing costs of literary metaphors varied over time and genre. Specifically, we trained word embeddings on literary and nonliterary Italian corpora from the 19th and 21st centuries, for a total of 124 million tokens, and modeled changes in the semantic similarity between topics and vehicles of 515 19th-century literary metaphors, taking this measure as a proxy of metaphor processing demands. Overall, semantic similarity, and hence metaphor processing demands, remained stable over time. However, genre played a key role: metaphors appeared more difficult (i.e., lower topic-vehicle similarity) in modern literary contexts than in 19th-century literature, but easier (i.e., higher topic-vehicle similarity) in today's nonliterary language (e.g., the Web) than in 19th-century nonliterary texts. This pattern was further shaped by semantic features of metaphors' individual terms, such as vector coherence and semantic neighborhood density. Collectively, these findings align with broader linguistic changes in Italian, such as the stylistic simplification of modern literature, which may have increased metaphor processing demands, and the high creativity of the Web's language, which seems to render metaphor more accessible.

</details>


### [172] [On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis](https://arxiv.org/abs/2602.13713)
*Maciej Uberna,Michał Wawer,Jarosław A. Chudziak,Marcin Koszowy*

Main category: cs.CL

TL;DR: 本文提出了一个多智能体比较框架，通过理论知识增强大模型，显著提升了辨识辩论中重述语用功能的能力，尤其是在语用功能层级上超越了仅检测字面相似性的基线模型。


<details>
  <summary>Details</summary>
Motivation: 背景在于尽管大模型可以检测表层相似，但难以区分和理解不同重述（reformulation）在话语中的实际语用和修辞功能，这对于算法辨析论述和检测修辞策略具有挑战性。

Method: 建立了含有丰富理论知识的对照实验：采用注释的政治辩论数据集，定义四类重述功能（降强、增强、具体化、泛化及其他D-I-S-G-O）。对比了两种基于大模型的系统：一种利用RAG引入论证理论知识，一种为直接零样本基线，并比较了它们的性能。

Result: 理论增强（RAG）的智能体在所有分析维度上对比基线表现出显著提升，尤其在增强和泛化识别任务中最明显，Macro F1-score提升近30%。

Conclusion: 引入论证理论不仅有益且是实现论述功能识别的关键，这一框架为开发具备修辞策略分析能力的可扩展计算工具奠定了基础。

Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.

</details>


### [173] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: 本文提出了一种新方法RMPL，实现了低资源环境下多媒体事件抽取（MEE）的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多媒体事件抽取的研究受限于缺乏标注数据，并且主流方法未能显式学习结构化的事件表示，导致跨模态论元对齐效果较差。

Method: 提出了关系感知的多任务渐进式学习（RMPL）框架。该方法利用单模态事件抽取和多媒体关系抽取提供的异构监督，通过分阶段训练统一跨模态事件表示；然后在混合的文本和图像数据上进行细化训练以识别事件和其论元。

Result: 在M2E2数据集上，RMPL在不同视觉-语言模型和多种模态设置下，均显著优于现有方法。

Conclusion: RMPL有效提升了低资源多媒体事件抽取的表现，为跨模态事件理解提供了新思路和基线。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [174] [How Do Lexical Senses Correspond Between Spoken German and German Sign Language?](https://arxiv.org/abs/2602.13790)
*Melis Çelikkol,Wei Zhao*

Main category: cs.CL

TL;DR: 该论文针对手语词典中多义词和同音异义词不同语境下的符号表达不足的问题，提出了一种基于实际用法的德语-德国语手语词义-手语映射方法，构建了首个跨模态词义-手语注释数据集，并评估了两种计算方法，结果表明语义相似度方法有显著优势。


<details>
  <summary>Details</summary>
Motivation: 双语手语词典中常忽略了多义词和同音异义词在不同语境下与手语符号的具体对应关系，现有词典无法充分反映实际语言使用中的复杂映射关系，急需更精准的词义-手语对照资源。

Method: 通过人工标注，将德语的32个词（来自德语词用法图）和49个德国语手语符号（来自德国语手语数字词典）之间的1404组词用-符号映射进行对应，并归类为一对多、多对一、一对一和不匹配四种类型。进而利用精确匹配（EM）和基于SBERT嵌入的语义相似度（SS）两种计算方法对映射准确性进行自动化评测。

Result: 语义相似度方法（SS）整体表现优于精确匹配（EM），准确率为88.52%比71.31%；其中对于一对多类型提升最为显著（+52.1个百分点）。

Conclusion: 本研究首次建立了德语-德国语手语跨模态词义-符号注释数据集，为识别和丰富双语词典中的实际词义-手语对应关系提供了基础资源，并证实了计算方法在不同映射类型上的可行性和优劣。

Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.

</details>


### [175] [OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum](https://arxiv.org/abs/2602.13793)
*Yangyang Zhang,Zilong Wang,Jianbo Xu,Yongqi Chen,Chu Han,Zhihao Zhang,Shuai Liu,Hui Li,Huiping Zhang,Ziqi Liu,Jiaxin Chen,Jun Zhu,Zheng Feng,Hao Wen,Xingzhu Ju,Yanping Zhong,Yunqiu Zhang,Jie Duan,Jun Li,Dongsheng Li,Weijie Wang,Haiyan Zhu,Wei Jiang,Xiaohua Wu,Shuo Wang,Haiming Li,Qinhao Guo*

Main category: cs.CL

TL;DR: 本论文提出并验证了一种新型多智能体AI系统（OMGs），用于卵巢肿瘤的多学科会诊决策，能在资源受限中心提供类似专家MDT的小组建议。


<details>
  <summary>Details</summary>
Motivation: 由于卵巢肿瘤治疗复杂且异质性高，近年来多学科肿瘤小组（MDT）会诊逐渐成为标准，但全球许多患者因资源限制难以及时获得专家共识。亟需一种能普及、高效、可靠的知识推理与决策工具。

Method: 该研究开发了OMGs多智能体系统，使各学科领域AI代理协作综合多学科证据，产生带有透明推理过程的MDT风格个案建议。为系统性评价建议质量，作者还开发了SPEAR指标（安全性、个性化、证据、可实施性、稳健性），并通过多中心临床案例验证了系统效能。

Result: 在多中心回顾研究中，OMGs在推荐质量上的得分接近专家MDT共识（4.45±0.30 vs 4.53±0.23），证据指标得分更高（4.57 vs 3.92）；在前瞻性多中心验证中（59例患者），系统与常规MDT决策高度一致。与人类-AI配对测试显示，系统对“证据”和“稳健性”两个MDT最难保证的维度提升最大。

Conclusion: OMGs多智能体系统可实现与专家MDT共识相当的决策水平，有望为资源受限地区的肿瘤患者提供高质量、可扩展的多学科诊疗支持。

Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in resource-constrained centres where MDT resources are scarce or unavailable. Here we present OMGs (Ovarian tumour Multidisciplinary intelligent aGent System), a multi-agent AI framework where domain-specific agents deliberate collaboratively to integrate multidisciplinary evidence and generate MDT-style recommendations with transparent rationales. To systematically evaluate MDT recommendation quality, we developed SPEAR (Safety, Personalization, Evidence, Actionability, Robustness) and validated OMGs across diverse clinical scenarios spanning the care continuum. In multicentre re-evaluation, OMGs achieved performance comparable to expert MDT consensus ($4.45 \pm 0.30$ versus $4.53 \pm 0.23$), with higher Evidence scores (4.57 versus 3.92). In prospective multicentre evaluation (59 patients), OMGs demonstrated high concordance with routine MDT decisions. Critically, in paired human-AI studies, OMGs most substantially enhanced clinicians' recommendations in Evidence and Robustness, the dimensions most compromised when multidisciplinary expertise is unavailable. These findings suggest that multi-agent deliberative systems can achieve performance comparable to expert MDT consensus, with potential to expand access to specialized oncology expertise in resource-limited settings.

</details>


### [176] [The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach](https://arxiv.org/abs/2602.13816)
*Muneef Y. Alsawsh,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 本研究调查了也门英语二语学习者习得英语不规则词形变化的过程，发现学习早期受母语迁移影响显著，后期逐渐依靠普遍语法重组特征；适当的语言输入和教学至关重要。


<details>
  <summary>Details</summary>
Motivation: 了解为什么也门二语学习者英语不规则变化习得难，并探讨普遍语法理论（UG）下，母语迁移和二语发展机制的具体作用。

Method: 通过两个发展阶段的学习者样本数据，对错误类型进行分析，并应用统计学方法（如单因素方差分析）检测不规则词形生成的表现变化。

Result: 第一阶段受母语迁移影响大，表现出音位和结构失配，第二阶段学习者对目标语UG属性更敏感，词形重组更接近目标语。不规则词形错误既有跨语际（母语迁移）也有语内（过度泛化等）成因，二阶段问卷中表现显著提升，但在辅音变化、零词素与-a复数等词形上仍有困难。

Conclusion: 母语迁移和二语系统发展共同影响早期学习，后期能部分依赖UG重组语言特征。要充分实现UG驱动的特征重组，需提供高质量、有效的语言输入和教学。

Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.

</details>


### [177] [Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind](https://arxiv.org/abs/2602.13832)
*Minyuan Ruan,Ziyue Wang,Kaiming Liu,Yunghwei Lai,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 本文讨论了大语言模型（LLMs）在理解用户意图时存在的主观信念与客观状态偏差（即认知分歧）问题，并提出以“心理理论”（ToM）机制检测和弥合这种分歧，同时开发了新的基准和数据集以提升模型应对实际互动需求的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs广泛应用于各类任务，但在用户意图表达不精确时，它们常无法准确识别和响应用户真正需求，导致任务失败。为解决主观信念和真实状态的偏离，作者引入了心理理论（ToM）的研究视角，并认为现有相关评估方法局限于理论推断，缺乏实际功能层面的检验。

Method: 作者将ToM正式化为大模型的认知分歧检测与解决机制，并提出了新的基准（\benchname），用以评估模型在实际互动下弥合理解差距的能力。同时，作者构建了基于用户信念追踪和任务状态推断相结合的数据集，通过强化学习对模型进行训练。

Result: 基于该基准对11种主流大模型的测试结果显示，它们在识别用户和环境之间的认知差距方面存在显著不足。经由强化学习和新数据集训练的模型在理解用户心理状态和下游任务表现方面均获得持续提升。

Conclusion: 心理理论能力不应仅视为推理技能，而应被提升为支持人与模型实际互动、弥合认知分歧的关键机制。作者的研究展示了ToM实用价值，并为后续交互式智能体的能力提升奠定基础。

Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill.

</details>


### [178] [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)
*Miles Williams,Young D. Kwon,Rui Li,Alexandros Kouris,Stylianos I. Venieris*

Main category: cs.CL

TL;DR: 该论文提出了一种名为SpecVocab的新型vocabulary speculation方法，用于加速语言模型推理，相较于现有方法实现更高的吞吐率与接收长度。


<details>
  <summary>Details</summary>
Motivation: 现有的speculative decoding方法虽能加速语言模型推理，但其单层草稿模型在生成输出分布时因词表过大存在性能瓶颈。为提升效率，一些工作尝试缩小草稿模型词表，然而这会导致目标词汇无法预测时推理效果下降。作者希望解决词表减小带来的准确性损失问题。

Method: 作者提出SpecVocab方法，每步解码动态选择一个子词表进行预测，兼顾了推理速度和输出准确性。该方法在解码过程中灵活调整词汇集合，从而避免常规减词表方法导致的OOV（目标词出词表）问题。

Result: 实验证明，SpecVocab在多项任务上比最新的EAGLE-3具有更高的接收长度，平均吞吐率最高可提升8.1%。

Conclusion: SpecVocab作为一种高效且有效的词表动态筛选方法，在保持解码效果的同时，进一步加速了大型语言模型的推理速度，优于现有的主流方案。

Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.

</details>


### [179] [PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training](https://arxiv.org/abs/2602.13840)
*Yuhan Cheng,Hancheng Ye,Hai Helen Li,Jingwei Sun,Yiran Chen*

Main category: cs.CL

TL;DR: 本文提出了PrivAct框架，将上下文隐私保护内化进大型语言模型代理的生成行为中，以提升多智能体系统中敏感任务的隐私保护。PrivAct能有效降低隐私泄露率，并保持任务帮助性，实现了隐私和实用性的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在涉及敏感、上下文依赖信息的个性化任务中应用愈发广泛，隐私泄露风险明显增加。现有方法多依赖外部干预，易受攻击且适用场景有限，亟需一种能直接在模型行为中落地的隐私保护方案。

Method: 作者提出PrivAct框架，通过将隐私偏好融入每个代理，实现模型生成环节的原位隐私保护，提升整体系统的上下文完整性（contextual integrity）与隐私-帮助性平衡。

Result: 在多种LLM基础模型与评测基准上，PrivAct表现出隐私保护性能的一致提升，隐私泄露率最高降低12.32%，帮助性保持可比水平，并能在多种多智能体拓扑下实现零样本泛化与鲁棒性。

Conclusion: PrivAct实现了内建于生成行为的上下文隐私保护，优化了多智能体系统中的隐私-实用性权衡，为面向实际敏感任务的LLM部署提供了更安全可靠的方案。

Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.

</details>


### [180] [Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe](https://arxiv.org/abs/2602.13860)
*Somnath Banerjee*

Main category: cs.CL

TL;DR: 本论文提出了一个“负责任智能”框架，旨在让大语言模型（LLMs）在满足现实世界需求的同时，兼顾技术精度、安全与全球文化包容性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在AI领域的影响力日益扩大，现有通用架构已难以应对实际应用中对安全性、上下文相关性和文化敏感度的高要求，因此需要新的方法来实现负责任的AI。

Method: 论文分三个方向：1）采用领域自适应以提高模型在特定任务上的技术精度；2）加强伦理规范，降低对抗性风险；3）实现文化和多语言对齐，提升全球适用性。具体方法包括从传统的监督式适应、推理阶段的安全对齐，到基于人类反馈和偏好建模以提升社会语言敏锐度。

Result: 提出并探索了一套多层次的框架，实验表明该方法可在保持技术精度的同时，提升模型的安全性和文化包容性。

Conclusion: 要在现实中安全、高效地部署LLMs，需要兼顾技术、伦理和文化三重标准，所提框架为负责任人工智能的实践提供了可行的思路。

Abstract: The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.

</details>


### [181] [Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages](https://arxiv.org/abs/2602.13867)
*Somnath Banerjee,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 当前LLM的安全机制、基准和对齐方法主要针对英语及少数高资源语言，对于低资源语言、多语混杂和本地文化情境保护不足。本文总结了相关问题并提出改进议程，强调多语安全应成为核心需求。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）在全球南方国家得到广泛部署，但相关安全措施多数针对高资源语言，未能充分覆盖低资源语言和文化多样性，存在安全和事实转移假设不成立的隐患。

Method: 系统综述了近期研究，分析LLM在低资源语言、代码混合输入下安全性弱化、文化有害行为难以识别、基于英语的知识和补丁迁移失败等；并提出应采取参数高效的安全措施、文化相关的评估指标和本地社区参与的安全工作流程。

Result: 研究发现：1）LLM的安全防护在低资源和代码混合情况下效果大幅下降；2）即使表面“无毒”，也可能出现文化伤害；3）英语知识编辑与安全修复往往难以迁移到其他语言。

Conclusion: 作者呼吁，将多语言安全纳入AI开发必需条件而非附加选项，推动本地化和社区参与，共同实现公平、可信的多语言AI系统。

Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.

</details>


### [182] [ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics](https://arxiv.org/abs/2602.13870)
*Hend Al-Khalifa,Nadia Ghezaiel,Maria Bounnit,Hend Hamed Alhazmi,Noof Abdullah Alfear,Reem Fahad Alqifari,Ameera Masoud Almasoud,Sharefah Ahmed Al-Ghamdi*

Main category: cs.CL

TL;DR: 本论文提出了首个大规模的阿拉伯语礼貌识别数据集ADAB，覆盖多种阿拉伯语方言与领域，并进行了细致标注和模型基线测试。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理对文化敏感性需求的增加，需要能捕捉多语言社会语用现象的资源。然而，阿拉伯语在礼貌检测资源上仍极为匮乏，而阿拉伯语的礼貌表达又极其复杂且多样。

Method: 收集自四大线上平台的现代标准阿拉伯语及海湾、埃及、黎凡特、马格里布多种方言，共10,000条数据，按阿拉伯语言传统与语用理论，分为礼貌、无礼、中性三类，带有16种礼貌类别的语言特征标注，由多名标注者保证相当高的一致性（kappa=0.703）。并对40种机器学习与深度学习模型进行了基线测试。

Result: 构建了ADAB数据集，系统覆盖多领域多方言，标注质量高，并通过40种模型基线推动了相关模型的开发和对比。

Conclusion: ADAB数据集填补了阿拉伯语礼貌NLP资源空白，为后续在礼貌感知、方言处理等阿拉伯语自然语言处理研究提供了坚实基础。

Abstract: The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness detection remain under-explored, despite the rich and complex politeness expressions embedded in Arabic communication. In this paper, we introduce ADAB (Arabic Politeness Dataset), a new annotated Arabic dataset collected from four online platforms, including social media, e-commerce, and customer service domains, covering Modern Standard Arabic and multiple dialects (Gulf, Egyptian, Levantine, and Maghrebi). The dataset was annotated based on Arabic linguistic traditions and pragmatic theory, resulting in three classes: polite, impolite, and neutral. It contains 10,000 samples with linguistic feature annotations across 16 politeness categories and achieves substantial inter-annotator agreement (kappa = 0.703). We benchmark 40 model configurations, including traditional machine learning, transformer-based models, and large language models. The dataset aims to support research on politeness-aware Arabic NLP.

</details>


### [183] [Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach](https://arxiv.org/abs/2602.13890)
*Amir Hossein Mohammadi,Ali Moeinian,Zahra Razavizade,Afsaneh Fatemi,Reza Ramezani*

Main category: cs.CL

TL;DR: 本文针对小型语言模型（SLM）在复杂多跳问答任务中采用RAG方法的Prompt模板优化进行了大规模实证分析，发现合理设计Prompt可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然针对大型语言模型的RAG优化已经有较多研究，但在小型语言模型（SLM）中，尤其是需要复杂推理的多跳问答任务，仍存在研究空白，尤其是Prompt模板如何设计对性能影响重大但未被充分探索。

Method: 本文在HotpotQA数据集上，评估了24种Prompt模板（包括标准RAG、文献中的9种范式以及14种新颖混合模板），分别在Qwen2.5-3B Instruct 和 Gemma3-4B-It 两个主流SLM上测试，总共测试18720个问题实例。

Result: 发现精心设计的Prompt模板可使Qwen2.5模型的性能提升至83%，Gemma3-4B-It达84.5%，比标准RAG提升最高达6%。

Conclusion: 合理设计Prompt模板在SLM-RAG系统中至关重要，本文提供了具体分析和可操作建议，对资源受限环境下SLM-RAG系统设计具有实际指导意义。

Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.

</details>


### [184] [Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin](https://arxiv.org/abs/2602.13905)
*Thibault Clérice,Rachel Bawden,Anthony Glaise,Ariane Pinche,David Smith*

Main category: cs.CL

TL;DR: 本文提出了一个新的任务：预编辑归一化（PEN），旨在桥接自动文本识别（ATR）的古文手稿转录输出与归一化数字版本之间的差距，提升了文本的实用性和兼容性。作者还提供了新的数据集与模型，并在相关任务上显著优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然ATR技术进步改善了对历史文档的访问，但ATR模型要么强调古文手稿的忠实记录，导致输出难被大众和后续NLP工具利用；要么直接归一化输出，但泛化较差且可能过度归一化甚至产生虚假内容。因此，非常需要方法能在保证手稿忠实的基础上进行适度归一化，并兼顾实用性。

Method: 作者定义了PEN任务，并据此从CoMMA语料库等来源构建数据集，自动对齐古法语和拉丁语文本，并人工校对产生金标集。使用ByT5等序列到序列模型对归一化和预标注任务进行基准测试。

Result: 作者发布了466万样本的银标训练集和1.8千样本的金标评估集。所提出的归一化模型（基于ByT5）在标准化任务中CER降至6.7%，大幅优于此前同类工作。

Conclusion: PEN任务和数据集为ATR后处理和数字人文学科带来了新的可能性，能够兼顾手稿忠实度与实际可用性，对领域内自动化工具和文本处理应用具有重要推动作用。

Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.

</details>


### [185] [HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam](https://arxiv.org/abs/2602.13964)
*Weiqi Zhai,Zhihai Wang,Jinghang Wang,Boyu Yang,Xiaogang Li,Xiang Xu,Bohan Wang,Peng Wang,Xingzhe Wu,Anfeng Li,Qiyuan Feng,Yuhao Zhou,Shoulin Han,Wenjie Luo,Yiyuan Li,Yaxuan Wang,Ruixian Luo,Guojie Lin,Peiyao Xiao,Chengliang Xu,Ben Wang,Zeyu Wang,Zichao Chen,Jianan Ye,Yijie Hu,Jialong Chen,Zongwen Shen,Yuliang Xu,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Hu Wei,Que Shen,Bing Zhao*

Main category: cs.CL

TL;DR: 本文提出并发布了HLE-Verified，这是修订和验证过的人类终极考试（HLE）基准数据集，显著减少了原始HLE数据集中的噪声题目，从而提升了大语言模型评估的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的HLE数据集被广泛用于前沿大语言模型的能力评测，但社区分析发现其包含大量有噪声、错误或模糊的题目，会对模型评测结果产生偏差，因此需要更高质量的评测基准。

Method: 采用两阶段流程：第一阶段由领域专家及模型交叉验证题目及答案的正确性，筛选获得641条已验证题目；第二阶段对可修复的有瑕疵题目进行双重独立修订与再审查，最终获得1,170条高质量数据。剩余689条带明确信息的不确定题目单独标注并公布。

Result: 在用七个最先进语言模型进行对比实验时，HLE-Verified数据集上的平均准确率相较原HLE提升了7-10个百分点，对原题或答案有误题目的提升更达30-40个百分点。模型信心与题目或答案错误存在显著相关性，进一步证明修订有效。

Conclusion: HLE-Verified有效降低了评测噪声，使模型能力测量更加真实准确，为大语言模型公平客观评估提供了更可信赖的标准。

Abstract: Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified

</details>


### [186] [Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis](https://arxiv.org/abs/2602.13979)
*Tongze Zhang,Jun-En Ding,Melik Ozolcer,Fang-Ming Hung,Albert Chih-Chieh Yang,Feng Liu,Yi-Rou Ji,Sang Won Bae*

Main category: cs.CL

TL;DR: 该论文提出利用大型语言模型（LLMs）结合链式思维（CoT）方法，通过电子健康记录（EHRs）提升阿尔茨海默病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统阿尔茨海默病诊断依赖于医学成像和临床评估，效率低且对医疗资源要求高。虽然LLMs在医疗EHRs中的应用逐渐增加，但在AD这种多因素复杂疾病中的应用有限，尤其是直接成像难以捕捉复杂病因。

Method: 作者提出用LLMs对患者EHR进行链式思维推理，生成明确的诊断推理路径，而不是直接对EHR进行分类微调。然后利用结构化的CoT预测提升诊断能力及可解释性。

Result: 实验证明，该诊断框架在多个CDR等级判别任务上显著提升了稳定性和诊断性能，F1分数较零样本基线提升至多15%。

Conclusion: 基于CoT的LLM诊断流程不仅提升了阿尔茨海默病诊断性能，还增强了对复杂病因的解释能力，为电子健康数据在神经疾病诊断中的应用提供了新方法。

Abstract: Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.

</details>


### [187] [The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective](https://arxiv.org/abs/2602.14002)
*Ali Zahedzadeh,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文探讨了在多步问答任务中，大语言模型自我生成解释时解释的简洁性与充分性的平衡，提出并实证说明在减少解释长度时仍可保证答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在解答复杂问题时，常用链式思维等自我解释以提升准确率，但这些解释往往冗长且生成成本高，激发了‘到底需要多详细的解释’这一疑问。

Method: 论文基于信息瓶颈原理，将自我解释视为压缩后的关键信息。作者设计了一套评测流程，通过约束解释长度并评估其支持正确答案的能力，在ARC Challenge数据集上、以多种语言模型和英语与波斯语等不同语言进行实验。

Result: 实验发现，在保证解释足够支撑正确答案的前提下，显著压缩解释长度通常不会降低模型准确率，但若过度压缩则会带来性能损失。

Conclusion: 研究表明，大模型生成的解释可以更简洁，同时不牺牲正确性，有助于减少生成成本。适度压缩解释长度是一种可行的优化方向，但需注意不可过度以免性能下降。

Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.

</details>


### [188] [Named Entity Recognition for Payment Data Using NLP](https://arxiv.org/abs/2602.14009)
*Srikumar Nayak*

Main category: cs.CL

TL;DR: 本文系统分析了面向支付数据的最新命名实体识别（NER）算法，提出了基于BERT的创新模型PaymentBERT，显著提升了实体抽取的准确率并满足实时处理需求。


<details>
  <summary>Details</summary>
Motivation: 在金融交易自动化处理过程中，从非结构化支付数据中提取结构化信息对于合规、风控等实际业务至关重要。然而，传统NER方法在支付数据这一特殊场景下存在准确率和通用能力不足的问题。

Method: 作者系统评估了CRF、BiLSTM-CRF、BERT、FinBERT等多种NER模型，并结合领域专属的金融嵌入与上下文表示，提出了PaymentBERT混合架构。实验基于5万条多格式标注支付交易数据，包含SWIFT MT103、ISO 20022及本地支付系统。

Result: 微调BERT模型在实体抽取任务上F1分数达到94.2%，比传统CRF优12.8个百分点；新提出的PaymentBERT模型F1分数升至95.7%，同时支持实时处理，优于所有对比模型。

Conclusion: PaymentBERT在跨格式泛化、剖析实验和部署实践中均表现优异，为金融机构在自动化制裁筛查、反洗钱合规和支付处理系统的实际应用提供了有效技术参考与支持。

Abstract: Named Entity Recognition (NER) has emerged as a critical component in automating financial transaction processing, particularly in extracting structured information from unstructured payment data. This paper presents a comprehensive analysis of state-of-the-art NER algorithms specifically designed for payment data extraction, including Conditional Random Fields (CRF), Bidirectional Long Short-Term Memory with CRF (BiLSTM-CRF), and transformer-based models such as BERT and FinBERT. We conduct extensive experiments on a dataset of 50,000 annotated payment transactions across multiple payment formats including SWIFT MT103, ISO 20022, and domestic payment systems. Our experimental results demonstrate that fine-tuned BERT models achieve an F1-score of 94.2% for entity extraction, outperforming traditional CRF-based approaches by 12.8 percentage points. Furthermore, we introduce PaymentBERT, a novel hybrid architecture combining domain-specific financial embeddings with contextual representations, achieving state-of-the-art performance with 95.7% F1-score while maintaining real-time processing capabilities. We provide detailed analysis of cross-format generalization, ablation studies, and deployment considerations. This research provides practical insights for financial institutions implementing automated sanctions screening, anti-money laundering (AML) compliance, and payment processing systems.

</details>


### [189] [GRRM: Group Relative Reward Modeling for Machine Translation](https://arxiv.org/abs/2602.14028)
*Sen Yang,Shanbo Cheng,Lu Xu,Jianbing Zhang,Shujian Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的Group Quality Metric（GQM）范式，通过Group Relative Reward Model（GRRM）提升了LLM后训练在机器翻译等开放性任务中的效果，实现了更精细的候选序列评估和排序，并提升了翻译质量及推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的标量质量评价指标（SQM）对于分组内序列细粒度比较不足，难以在机器翻译等开放领域中准确评判候选之间的相对优劣。作者认为更具比较性的分组排名方法亟需研究。

Method: 引入Group Quality Metric（GQM）和基于此的Group Relative Reward Model（GRRM），让模型能够联合处理一组候选，利用组内比较关系获取序列相对质量，并实现自适应粒度的评分与排序。此外，将GRRM集成进Group Relative Policy Optimization（GRPO）训练流程中以优化翻译策略。

Result: 实验证明GRRM在众多基线中排名表现优异，拥有竞争力的排序准确性。将其集成于GRPO后提升了翻译总体质量，还显著增强了模型推理能力，与最先进的推理模型表现相当。

Conclusion: 基于GRRM的GQM范式为开放型机器翻译等任务提供了更精准的质量评估和教学信号，显著提升了LLM后训练的性能，具备广阔的实际应用前景。代码和模型已开源。

Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.

</details>


### [190] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: 本文指出，目前Mixture-of-Experts（MoE）嵌入模型采用线性加权求和方式整合不同专家输出，但这与专家输出实际的几何分布（即位于共享的超球面流形上）并不一致。作者提出Spherical Barycentric Aggregation (SBA)方法，有效保持超球面结构，并在多个基准任务上取得更优表现。


<details>
  <summary>Details</summary>
Motivation: 广泛应用的MoE嵌入模型假设专家输出满足线性结构，模型采用线性聚合方法。然而，作者发现这种线性假设与高维空间专家输出分布实际的超球几何结构矛盾，导致聚合结果失真和表达能力损失，因此有必要提出新的、与几何结构相契合的聚合方法。

Method: 作者首先对现有MoE嵌入模型的几何结构进行了系统分析，发现专家输出在高维空间内聚集在超球面上。接着，提出了SBA聚合方法，通过分别处理径向与角度分量，在保留原有路由机制兼容性同时，实现了顺应超球几何分布的聚合。

Result: 在Massive Text Embedding Benchmark (MTEB)的多个任务如语义相似性、聚类、重复问题检测中，SBA在不增加训练成本的前提下，实现了全面且稳定的性能提升，并通过几何分析证明SBA能够有效防止原有聚合导致的向量塌陷现象。

Conclusion: SBA作为几何感知的聚合算子，更好地契合了MoE嵌入模型中专家输出的超球结构，提升了表达能力与任务性能。该工作强调了在MoE嵌入架构中考虑几何结构的重要性。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [191] [Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness](https://arxiv.org/abs/2602.14044)
*Pietro Bernardelle,Stefano Civelli,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本研究通过不同参数规模和模型家族的大型语言模型（LLM），在多个事实核查数据集上，系统分析了上下文长度和证据位置对模型表现的影响，发现随着上下文变长，准确率下降，证据放在开头或结尾表现更好。


<details>
  <summary>Details</summary>
Motivation: 先前已知LLM在长上下文下表现不稳定，尤其是中间部分的内容会被忽视，本文进一步探究这一现象在事实核查任务中的体现，以指导检索增强型事实核查系统的设计。

Method: 作者选取了三类数据集（HOVER、FEVEROUS 和 ClimateFEVER）与五个7B、32B、70B参数规模的开源LLM（来自Llama和Qwen不同家族）进行对比实验，测试了不同上下文长度及证据位置对事实核查任务结果的影响。

Result: 结果发现，LLM对事实有一定的参数化记忆，但随着上下文长度增加，事实核查准确率下降；证据出现在上下文开头或结尾的情况下，模型表现最好，而证据夹在中间表现最差。

Conclusion: 论文强调，上下文结构（尤其是证据摆放位置）对LLM事实核查具有重要影响，对检索增强型核查系统的设计有直接启示意义。

Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.

</details>


### [192] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 本文提出一种新方法LogitsCoder，通过在生成过程中对logits进行轻量级控制，从而提升代码生成时推理链条的深度与效率，克服了以往方法中的“思考不足”与“思考过度”问题，实验显示其性能优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展（TTS）方法虽然可探索推理路径，但存在推理链过浅（无法捕捉复杂问题）和过深（效率低、计算成本高）的问题，亟需一种方法能够在保证推理深度的同时提升效率。

Method: 提出LogitsCoder框架，包括三个主要步骤：（1）Logits Preference Decoding：引导token的选择，以倾向于统计更优的生成模式；（2）Logits Rank Based Path Selection：从多条推理路径中选择多样的方案；（3）Thoughts Aggregation：对生成的推理思路进行聚合，使推理链兼具深度与效率。

Result: 实验结果表明，LogitsCoder生成的推理链条更加高效且质量更高。在多个代码生成基准上的表现优于现有的主流方案。

Conclusion: LogitsCoder有效解决了代码生成过程中推理链条的“思考不足”与“思考过度”两大难题，为代码生成模型带来了性能和效率的双重提升，未来可用于更广泛的结构化推理任务。

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [193] [LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts](https://arxiv.org/abs/2602.14060)
*Yang Liu,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li,Lingyong Yan*

Main category: cs.CL

TL;DR: 论文提出了一种结合数据聚类、语义专家学习和稀疏混合专家模型的定义建模方法LM-Lexicon，取得了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有定义建模方法在语义多样性和专业化表达方面存在局限，作者希望提升模型对不同语义领域的表达能力和泛化能力。

Method: 作者引入了数据聚类对定义建模任务进行领域分解，并用小型语言模型针对每个语义领域进行专家训练，通过稀疏混合专家架构实现模型融合，采用语义感知的领域路由机制提升专家分派效果。

Result: LM-Lexicon在五个主流基准上BLEU分数提升7%，聚类策略带来近10%的定义质量提升，领域级路由较传统Token级路由提升1%，在推理时增加计算和专家规模还能进一步提升性能。

Conclusion: LM-Lexicon显著提升了定义建模任务表现，并为语义密集型应用开发高效语言模型提供了新思路。

Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.

</details>


### [194] [From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset](https://arxiv.org/abs/2602.14062)
*Jandad Jahani,Mursal Dawodi,Jawid Ahmad Baktash*

Main category: cs.CL

TL;DR: 该论文详细分析了Mozilla Common Voice语音数据集中普什图语分支的版本24.0，并着重于其数据量的快速增长及其结构性不足。


<details>
  <summary>Details</summary>
Motivation: 普什图语作为有超过6000万使用者的主要语言，长期缺乏适合现代自动语音识别（ASR）系统开发的公开大规模语音数据集。该研究旨在填补这一资源缺口，并促进低资源语言ASR系统发展。

Method: 作者针对2025年12月的Common Voice普什图语v24.0数据集进行宏观统计和结构分析，涵盖数据量增长、验证效率、贡献者参与分布、人口统计数据完整性和句子级集中性等多个维度，并采用Gini系数等统计手段衡量数据集中度和不均衡性。

Result: 该版本普什图语数据集从2023年的1.49小时增至2025年的2768.7小时（其中976小时已验证用于训练）。分析显示，贡献极度集中（Gini=0.941），贡献者以年轻人为主，42%语音片段缺乏性别标签。句子提示适度复用，大部分结构集中是由少量活跃贡献者造成。

Conclusion: 本研究为快速扩展的低资源语音数据集提供了定量审计，强调了提升数据集质量的实际优先事项，包括扩大验证能力和提升人口统计多样性。

Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.
  This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.
  Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\% of unique sentences account for 50\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.
  These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.

</details>


### [195] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: 本文提出了Open Rubric System（OpenRS），通过基于评分细则而非单一分数的方式对多维人类偏好进行对齐，提高了复杂任务奖励函数的透明性、鲁棒性和可调试性。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型通常将多维的人类偏好压缩为单一分值，但这种方式信息损失大，导致鲁棒性差、容易被“奖励黑客”所攻击，尤其在开放式、难以验证的任务中问题突出。作者认为，要做到稳健对齐，奖励应基于可检查的原则性推理过程而非黑箱分数。

Method: 作者提出OpenRS系统，采用“准则”-“评分细则”方式，结合Pairwise Adaptive Meta-Rubrics（PAMR）和Pointwise Verifiable Rubrics（PVRs），在对比两个候选结果时按准则逐条比较并外部归纳，结合类似“宪法”的meta-rubric统一规范各评分细则的设定和使用，引入两层评分细则迭代优化流程和可验评分细则，提升系统可扩展性及客观性。最终该系统被实例化为对比式RL训练的奖励监督。

Result: OpenRS实现了奖励外部归纳，避免了权重分数化带来的信息丢失，提高了在开放性任务中的鉴别力和稳健性。可验评分细则为可被检验的子任务提供了奖惩依据，也起到保护系统免于演化为异常行为的“护栏”作用。

Conclusion: OpenRS为开放式任务非可验证对齐提供了更透明、稳健的方法。通过显式化的评分原则和可调细则，提升了奖励模型的可检验性和可维护性，减少了“奖励黑客”风险，为复杂AI系统对齐奠定了基础。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [196] [Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework](https://arxiv.org/abs/2602.14073)
*Grzegorz Statkiewicz,Alicja Dobrzeniecka,Karolina Seweryn,Aleksandra Krasnodębska,Karolina Piosek,Katarzyna Bogusz,Sebastian Cygert,Wojciech Kusa*

Main category: cs.CL

TL;DR: 该论文通过自动翻译和轻量过滤的方法，成功构建了面向波兰语的多模态视觉语言模型（VLM），性能显著提升，并公开了模型与评测数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视觉语言模型（VLM）主要基于英语数据训练，导致其在其他语言和文化语境下表现不佳，严重限制了这些模型在非英语用户群体中的可用性和多样性。论文旨在探索如何为低资源语言（如波兰语）开发高质量的多模态VLM。

Method: 作者采用了LLaVA-Next方法，创建了一套波兰语视觉语言模型。采用自动化流水线将现有多模态数据集进行翻译和筛选，并针对OCR与特定文化任务合成波兰语数据，全流程几乎无需人工干预。

Result: 该方法在波兰语适配的MMBench测试集上比LLaVA-1.6-Vicuna-13B提升了9.5%，在人类评测中生成的描述在语言正确性方面也更高。

Conclusion: 结合大规模自动翻译与轻量级数据筛选，能够高效实现面向低资源语言的高质量多模态模型，但在文化覆盖和模型评测方面仍有挑战。相关模型和数据集已开源，为后续研究提供便利。

Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.

</details>


### [197] [GTS: Inference-Time Scaling of Latent Reasoning with a Learnable Gaussian Thought Sampler](https://arxiv.org/abs/2602.14077)
*Minghan Wang,Ye Bai,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理时扩展（ITS）方法，通过高斯思想采样器（GTS）实现对潜在思维模型的结构化探索，相比以往基于启发式噪声的做法，在有限采样预算下更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理模型在推理时常通过诸如dropout和高斯噪声等启发式方法增加多样性，但这些方法的探索行为缺乏明确建模，且在有限采样预算下效率低下。因此，作者希望找到一种更结构化、更高效的探索机制。

Method: 作者将潜在思维的探索建模为可学习分布的条件采样，并提出了一种高斯思想采样器（GTS），能够根据上下文预测连续推理状态的扰动分布。训练过程中采用GRPO风格的策略优化方法，并保持主干网络冻结。

Result: 在GSM8K数据集和两种潜在推理架构上，实验表明GTS相比启发式基线，推理时的扩展性更可靠，表现更优。

Conclusion: 结果显示，提升潜在ITS需要结构化、可优化的探索机制，而不仅仅是增加随机性。

Abstract: Inference-time scaling (ITS) in latent reasoning models typically introduces stochasticity through heuristic perturbations, such as dropout or fixed Gaussian noise. While these methods increase trajectory diversity, their exploration behavior is not explicitly modeled and can be inefficient under finite sampling budgets. We observe that stronger perturbations do not necessarily translate into more effective candidate trajectories, as unguided noise may disrupt internal decision structure rather than steer it. To provide a more structured alternative, we model latent thought exploration as conditional sampling from learnable densities and instantiate this idea as a Gaussian Thought Sampler (GTS). GTS predicts context-dependent perturbation distributions over continuous reasoning states and is trained with GRPO-style policy optimization while keeping the backbone frozen. Experiments on GSM8K with two latent reasoning architectures show that GTS achieves more reliable inference-time scaling than heuristic baselines. These findings indicate that improving latent ITS requires structured and optimizable exploration mechanisms rather than simply amplifying stochasticity.

</details>


### [198] [Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality](https://arxiv.org/abs/2602.14080)
*Nitay Calderon,Eyal Ben-David,Zorik Gekhman,Eran Ofek,Gal Yona*

Main category: cs.CL

TL;DR: 本文提出了一种新方法对大语言模型（LLM）的事实性知识进行行为分析，发现错误更多来自知识提取而非知识缺失，并用WikiProfile基准评价13个主流模型，发现提升利用已有知识比简单扩容更关键。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM事实性评估无法区分“知识确实未编码”与“知识难以获取”两类不同错误，难以具体改进模型。

Method: 提出以事实为单位对LLM知识进行分析，分为是否编码、能否直接提取、或需推理获取。新引入WikiProfile基准，自动流程构建，结合搜索，测试13类模型，统计4百万条响应。

Result: 前沿模型如GPT-5、Gemini-3对基准知识95%-98%已完成编码，但模型在提取知识时表现仍有限，许多被认为‘知识缺失’的错误实则是调取失败，尤其对长尾事实和反向问题更明显。推理性回答能提高知识调取能力，弥补部分失误。

Conclusion: 未来模型提升关键在于改善知识提取和利用方式，而不仅仅依赖模型参数扩展。深入理解知识编码与调取间差异，有助于更有效改进LLM表现。

Abstract: Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.

</details>


### [199] [CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry](https://arxiv.org/abs/2602.14081)
*Shangqing Zhao,Yupei Ren,Yuhao Zhou,Xiaopeng Bai,Man Lan*

Main category: cs.CL

TL;DR: 论文提出了CCiV基准，用于系统评测大语言模型生成古典中文词的结构、韵律和艺术质量，并分析了不同模型的表现与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本生成中的广泛应用，衡量其在对结构、韵律要求极高的古典中文词生成能力变得重要。但现有评测手段无法全面反映其在词领域的精细能力和挑战，因此需要专门的评测基准。

Method: 作者提出CCiV基准，从结构、韵律和艺术质量三个维度评测模型对30种词牌的生成表现，评测了17个主流大模型，并分析了它们在历史变体生成、律调控制等方面的表现，还额外考察了在提示词设计（form-aware prompting）下模型表现的变化。

Result: 主要发现有：1）模型常常生成在历史上存在但不是标准形态的词变体；2）模型更难遵循韵律规则而非结构规则；3）强模型在结构和韵律上受益于特定提示，但弱模型反而退步；4）形式正确性与文学质量的关联较弱且不稳定。

Conclusion: CCiV为词生成提供了系统评测工具，揭示了结构与韵律控制的难点和模型生成中的历史变体现象，表明未来需要兼顾变体识别和更整体的约束创造生成方法。

Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \textbf{C}hinese \textbf{Ci}pai \textbf{V}ariants (\textbf{CCiV}), a benchmark designed to assess LLM-generated \textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.

</details>


### [200] [Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans](https://arxiv.org/abs/2602.14100)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: 本文探讨了神经网络，特别是编码器-解码器模型，在西班牙语L型形态模型学习中的表现。虽然神经网络能复现部分模式，却无法像人类一样泛化其规律。


<details>
  <summary>Details</summary>
Motivation: 想要检验神经网络能否作为人类词形学习的认知模型，并对比其与人类在处理不规则词形变化模式上的泛化能力；聚焦西班牙语的特殊L型形态。

Method: 采用五种编码器-解码器Transformer模型，变量包括位置编码方式（序列式vs位置不变）、词性标记表示（原子式vs分解式）；对比模型在L型模式学习和泛化上的表现，并与人类行为进行对照。

Result: 位置不变编码的模型在词干聚类上表现更好，能重现L型范式，尤其在训练数据稀缺时优势明显。但无论哪类模型，都无法像人类一样将L型模式泛化到所有新词；位置不变模型只泛化到虚拟式，而人类偏向泛化到第一人称直陈式。

Conclusion: 神经网络虽能捕捉统计模式，却无法达到人类词形抽象与泛化水平，揭示数据分布学习与认知抽象间的本质差距。

Abstract: Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is mixed. We investigate this using the Spanish \emph{L-shaped morphome}, where only the first-person singular indicative (e.g., \textit{pongo} `I put') shares its stem with all subjunctive forms (e.g., \textit{ponga, pongas}) despite lacking apparent phonological, semantic, or syntactic motivation. We compare five encoder-decoder transformers varying along two dimensions: sequential vs. position-invariant positional encoding, and atomic vs. decomposed tag representations. Positional encoding proves decisive: position-invariant models recover the correct L-shaped paradigm clustering even when L-shaped verbs are scarce in training, whereas sequential positional encoding models only partially capture the pattern. Yet none of the models productively generalize this pattern to novel forms. Position-invariant models generalize the L-shaped stem across subjunctive cells but fail to extend it to the first-person singular indicative, producing a mood-based generalization rather than the L-shaped morphomic pattern. Humans do the opposite, generalizing preferentially to the first-person singular indicative over subjunctive forms. None of the models reproduce the human pattern, highlighting the gap between statistical pattern reproduction and morphological abstraction.

</details>


### [201] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体医疗问答系统，将多种大型语言模型（LLM）、证据检索和不确定性评估相结合，提升医疗问答的可靠性与可验证性。该系统在多种医学数据集上的表现优于现有基线模型，并实现了较高的准确率和相关性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在医疗问答中由于可信度、证据支撑和置信度表达不足，临床应用受限。本文旨在通过多模型协作与多层验证机制，提高问答系统的可靠性和安全性。

Method: 方法包括：1）微调三类LLM（GPT、LLaMA、DeepSeek R1），并在大规模医学问答数据集上评测其生成能力；2）设计多智能体流程，包括临床推理（LLaMA）、文献检索（自动查PubMed）和答案完善（DeepSeek R1），并引入针对高风险或高不确定性问题的人类验证路径；3）通过蒙特卡洛dropout、不确定性评分、词汇/情感偏见检测及LIME/SHAP分析，提升安全性。

Result: DeepSeek R1在生成质量评测中表现最佳，显著超越医学专用基线BioGPT。完整系统实现87%准确率、约0.80相关性，并能有效降低答案不确定性（困惑度为4.13），端到端回答平均延迟36.5秒。

Conclusion: 多智能体专能与验证机制能够显著提升医学AI问答系统的可靠性与安全性，减缓单一模型存在的局限，为基于证据和偏见感知的临床AI落地提供了可扩展设计。

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [202] [Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering](https://arxiv.org/abs/2602.14162)
*Tao Xu*

Main category: cs.CL

TL;DR: 现有方法预先对整份文档的每页进行视觉-语言模型处理以生成描述，带来高昂成本且存在检索易错等问题。本文提出‘需求侧视觉摄取’（DVI）框架，把视觉模型分析延后到用户实际提问后，仅在索引时做轻量结构化元数据抽取和全文检索，实验表明该方法大幅降低成本，准确率仍可比肩传统方案，对视觉型问题有显著提升。


<details>
  <summary>Details</summary>
Motivation: 动机在于应对传统基于视觉-语言模型的多模态文档问答方法成本过高、检索易错、错误难以恢复等问题。特别是在实际工程文档场景，巨量的视觉分析令现有方案不切实际。

Method: 提出“需求侧摄取”（DVI）框架：索引阶段仅抽取结构化元数据和文本，利用全文检索（如BM25）定位页面，用户发问时才将原始页面图片和具体问题送入视觉-语言模型做针对性理解。

Result: 在两套真实工程图上实验证明，DVI整体问答准确率几乎与传统供给侧方案持平（46.7% vs 48.9%），在视觉必要类问题准确率大幅提升（50% vs 0%），并实现100%页面定位和98%检索空间压缩，同时摄入阶段VLM成本降为零。

Conclusion: DVI方法极大降低了多模态文档问答的运行成本，对视觉相关问题显著提升了准确率，并通过下沉视觉分析至交互阶段，将核心由‘问答准确率’转为‘页面定位能力’，具有实用价值与推广前景。

Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this "pre-ingestion" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is "Index for locating, not understanding"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the "QA accuracy" problem into a "page localization" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.

</details>


### [203] [GPT-5 vs Other LLMs in Long Short-Context Performance](https://arxiv.org/abs/2602.14188)
*Nima Esmi,Maryam Nezhad-Moghaddam,Fatemeh Borhani,Asadollah Shahbahrami,Amin Daemdoost,Georgi Gaydadjiev*

Main category: cs.CL

TL;DR: 该论文评估了当前大模型在超长上下文任务中的实际表现，发现所有模型在处理超70K tokens的大批量数据时精度显著下降，但GPT-5在精准度指标上表现依然突出。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的上下文窗口理论上已扩展到数百万tokens，但实际应用中模型能否高效处理和理解大量细节信息仍存疑。因此，作者希望通过系统评测揭示理论能力与实际表现间的差距。

Method: 作者选取了四个最新大模型（Grok-4、GPT-4、Gemini 2.5、GPT-5），设计三个数据集（菜谱检索、数学题、抑郁症检测的2万条社交媒体帖子），测试它们在长短文本任务下的信息利用与表现，并统计准确率和精准度等指标。

Result: 在抑郁症检测任务中，当输入量超过5K帖子（约70K tokens）时，所有模型准确率大幅下降，在输入2万帖时仅剩50-53%。但GPT-5的精准度依然能保持在约95%。而“middle丢失”问题在新模型中已大幅缓解。

Conclusion: 论文强调，现有大模型的理论上下文能力远超其实际在复杂大数据任务中的表现。实际应用应关注除准确率外的指标（如精准度），以发挥模型的真正效用。

Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the "lost in the middle" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.

</details>


### [204] [Knowing When Not to Answer: Abstention-Aware Scientific Reasoning](https://arxiv.org/abs/2602.14189)
*Samir Abdaljalil,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出了一种“可拒答”科学论证验证框架，针对科学主张的自动验证任务，既可以支持、反驳，也可以选择拒绝给出结论（即拒答），有效降低因不确定而产生的错误。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学主张验证任务中通常要求给出确定答案，但在科学领域，不支持或者不确定的结论可能危害更大。如何让模型在缺乏确凿证据时选择拒答成为重要课题。

Method: 提出了一种新的科学主张逐条件验证框架，把整体主张分解为最小判断单元，通过自然语言推理（NLI）结合外部证据，针对每个条件分别支持、反驳或拒答。在SciFact和PubMedQA两个不同领域数据集上，测试了6种主流大语言模型并进行比较。

Result: 实验表明，无论模型架构如何，模型拒答能力对于整体错误控制至关重要。基于置信度的拒答方法可大幅降低风险，即使绝对准确率提升有限。不同模型在原始准确率差异较小，拒答策略能明显优化最终表现。

Conclusion: 与其单纯追求最佳准确模型，不如关注何时有足够证据来给答。可拒答的评价框架为科学领域AI推理提供了更可靠和实用的标准，对未来科学推理AI研究有重要参考价值。

Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .

</details>


### [205] [We can still parse using syntactic rules](https://arxiv.org/abs/2602.14238)
*Ghaly Hussein*

Main category: cs.CL

TL;DR: 本文提出了一种结合CFG和GPSG理论的新型语法分析方法，不仅提出了新的算法，还融合了语法规则和特征来弥补传统CFG的不足。该系统能生成依存和成分结构树，且容忍噪声和不完整分析，在Universal Dependencies数据集上表现良好，并能为NLP提供更透明可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于CFG的语法分析方法存在处理能力有限、解析结果不够丰富等问题。此外，NLP领域迫切需要理论基础更加完备、可解释性更强的语法分析方法。作者旨在利用自1950年代以来丰富的理论语言学成果，构建兼具实用性和理论深度的新型解析系统。

Method: 方法包括提出新的解析算法以及一套扩展的句法规则和特征，综合利用CFG与GPSG理论，突破传统CFG的局限。该方法能够输出依存和成分解析树，并且能处理含噪声和不完整的输入。系统还设计可生成多重分析假设，便于后续通过重排序提升准确率。实验基于Universal Dependencies数据集进行。

Result: 在Universal Dependencies数据集的开发集（7个语料库）上，平均无标记依存准确率（UAS）为54.5%；在测试集（12个语料库）上为53.8%。表明系统在不同语料和数据集上表现具有一定的稳健性和潜力。

Conclusion: 该方法不仅扩展了传统CFG分析能力，还为NLP提供了高度可解释和透明的语言处理工具。实验结果验证了其实用性和理论价值，为后续进一步提升解析准确性提供了算法基础。

Abstract: This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.

</details>


### [206] [AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents](https://arxiv.org/abs/2602.14257)
*Lingxiang Hu,Yiding Sun,Tianle Xia,Wenwei Li,Ming Xu,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 提出了适用于广告与营销分析场景的真实环境大模型代理评测基准 AD-Bench，发现现有主流模型在复杂多轮任务中表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型代理评测大多停留在理想化仿真环境，缺乏对广告和营销等专业领域中复杂真实任务的衡量，无法满足实际应用需求。

Method: 提出了基于真实业务需求构建的 AD-Bench，采集真实用户营销分析请求，由行业专家提供可验证答案和工具调用轨迹，并将任务按难度（L1-L3）分级，覆盖多轮、多工具交互情境，用于评估和对比大模型代理在专业领域的能力。

Result: 在 AD-Bench 上，Gemini-3-Pro 模型简单任务表现较好（Pass@1=68.0%，Pass@3=83.0%），但在最复杂的 L3 任务上表现明显下降（Pass@1=49.4%，Pass@3=62.1%），行动轨迹覆盖度为70.1%。

Conclusion: 即使是最先进的大模型在真实复杂广告与营销分析任务中仍有较大能力提升空间，AD-Bench 为相关领域模型改进和评测提供了有价值的基准。

Abstract: While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to idealized simulations, failing to address the practical demands of specialized domains like advertising and marketing analytics. In these fields, tasks are inherently more complex, often requiring multi-round interaction with professional marketing tools. To address this gap, we propose AD-Bench, a benchmark designed based on real-world business requirements of advertising and marketing platforms. AD-Bench is constructed from real user marketing analysis requests, with domain experts providing verifiable reference answers and corresponding reference tool-call trajectories. The benchmark categorizes requests into three difficulty levels (L1-L3) to evaluate agents' capabilities under multi-round, multi-tool collaboration. Experiments show that on AD-Bench, Gemini-3-Pro achieves Pass@1 = 68.0% and Pass@3 = 83.0%, but performance drops significantly on L3 to Pass@1 = 49.4% and Pass@3 = 62.1%, with a trajectory coverage of 70.1%, indicating that even state-of-the-art models still exhibit substantial capability gaps in complex advertising and marketing analysis scenarios. AD-Bench provides a realistic benchmark for evaluating and improving advertising marketing agents, the leaderboard and code can be found at https://github.com/Emanual20/adbench-leaderboard.

</details>


### [207] [Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures](https://arxiv.org/abs/2602.14259)
*Matic Korun*

Main category: cs.CL

TL;DR: 本文提出通过分析大模型的token嵌入空间中的几何聚类结构，归纳和分类大语言模型的幻觉（hallucination）类型，并提出了检测和分析不同类型幻觉的几何指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉问题（输出虚假/不真实内容）日益受到关注。现有手段对其成因理解有限，本文希望通过嵌入空间的几何特征，深入刻画和区分不同类型的幻觉，从而更有效地检测和缓解这些问题。

Method: 作者分析了11种主流Transformer模型的静态token嵌入空间，覆盖编码器和解码器架构。提出了三种幻觉类型，并据此引入三类可量化的几何统计指标（α极性耦合、η聚类内聚力、λs径向信息梯度），用以识别和定量各类幻觉。

Result: 所有11个模型均表现出α > 0.5的极性结构和η > 0的聚类性，只有9/11模型在λs上显著（p < 0.05）。ALBERT和MiniLM未通过λs检验，且这与其嵌入压缩、蒸馏特点相符。

Conclusion: 几何聚类结构是特定类型幻觉检测的前置条件，不同架构对各类幻觉的易感性可据此预测。为后续幻觉检测和架构改进提供理论基础和实证依据。

Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.

</details>


### [208] [STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts](https://arxiv.org/abs/2602.14265)
*Zachary Bamberger,Till R. Saenger,Gilad Morad,Ofra Amir,Brandon M. Stewart,Amir Feder*

Main category: cs.CL

TL;DR: 提出了一种新的生成推理方法STATe-of-Thoughts，取代传统高温采样，以可解释的动作控制推理路径，提高了生成多样性、输出质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有推理生成方法如Best-of-N、Tree-of-Thoughts等依赖高温采样以提高输出多样性，但多样性有限且缺乏对推理策略的显式控制，导致可解释性和生成能力受限。

Method: 提出STATe方法，用离散且可解释的动作（文本干预）替代随机采样。通过Controller选择高层推理动作，Generator依据动作生成推理步骤，Evaluator对候选解评分，形成动作-生成-评价的结构化推理搜索流程。

Result: 1）动作引导的文本干预比温度采样产生更高的响应多样性。2）在论证生成任务中，动作序列能解释输出质量。3）基于动作与结果的关联评估，发现优秀但未探索的动作空间，能直接引导生成朝更优方向。

Conclusion: STATe框架提升了文本生成的质量、可解释性与多样性，为高质量自动推理文本生成提供了实用方法。

Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.

</details>


### [209] [Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook](https://arxiv.org/abs/2602.14299)
*Ming Li,Xirui Li,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文通过首次大规模系统诊断分析，揭示了AI智能体社会在动态进化中的行为特征及其与人类社会系统的异同。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型智能体日益在网络环境中扩展，一个核心问题是：AI智能体社会是否会像人类社会一样出现趋同动态？该研究旨在探索AI智能体社会是否会自发形成类似于人类社会的结构和规范。

Method: 作者提出了一个可量化的动态演化诊断框架，用于系统性分析AI智能体社会的演化，包括语义稳定性、词汇更替、个体惯性、影响力持续性及集体共识等指标。通过在Moltbook这一模拟的开放进化在线社会中运行大量AI智能体，收集并分析其行为数据。

Result: 研究发现，虽然AI智能体社会整体语义很快趋于稳定，但各个智能体之间依然高度多样化，词汇变化频繁，难以趋同。个体表现出强惯性，并且对他人互动反应有限，导致影响力难以持续，也难以形成稳定的集体共识或超级节点。

Conclusion: 仅有规模和高互动密度不足以让AI智能体社会出现深层次的社会化现象。这为今后的AI智能体社会的设计与分析提供了可落实的策略与原则。

Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.

</details>


### [210] [InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem](https://arxiv.org/abs/2602.14367)
*Shuofei Qiao,Yunxiang Wei,Xuehai Wang,Bin Wu,Boyang Xue,Ningyu Zhang,Hossein A. Rahmani,Yanshan Wang,Qiang Zhang,Keyan Ding,Jeff Z. Pan,Huajun Chen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 本文提出了InnoEval框架，通过模拟多人专家评审，实现更客观、多维度的科学创意评价，并在多个实验中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型推动科学创意产出的极大发展，当前的创新评价方式却没有同步进步，面临知识面窄、评价维度单一和偏见等问题，亟需更科学、系统的评价方法。

Method: 该方法将创意评价建模为一个知识支撑、多视角推理问题。其核心在于：(1) 利用异构深度知识检索引擎，从多元网络资源动态获取证据以增强评价的知识深度；(2) 通过多学科背景的虚拟评审组成“创新评审团”，实现多维度、去耦合的评价。（3）利用权威论文集构建评价数据集，对InnoEval进行全面实验验证。

Result: InnoEval在点评、对比评估和群组评估三种任务中，稳定优于传统基线模型，并且其判断模式与人类专家高度一致。

Conclusion: InnoEval可有效提升科学创新评价的公正性与多样性，为未来AI辅助科学决策提供了坚实基础。

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

</details>


### [211] [Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models](https://arxiv.org/abs/2602.14386)
*Mufan Xu,Kehai Chen,Xuefeng Bai,Zhengyu Niu,Muyun Yang,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多token级别的策略梯度优化方法（MPO），以更好地提升自回归语言模型在复杂推理任务中的表现。实验结果显示该方法优于传统的单token级别策略。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法将下一个token作为一次决策，这并不足以表达复杂推理任务中通常跨越多个token的语义块，存在优化粒度与问题本质不符的矛盾。

Method: 作者提出MPO方法，用连续K个token作为一个统一的语义动作，将推理序列的组合结构纳入策略优化过程，实现对更高层次目标的优化。

Result: 在数学推理与编程基准任务上的试验表明，MPO优于标准的token级策略梯度方法，尤其在需要复杂推理的任务中效果突出。

Conclusion: 复杂推理任务天然具有块级（block-level）结构，仅用token级别的优化受限，MPO为提升推理密集型任务效率提供了新方向，未来研究可进一步探索超越token粒度的优化方法。

Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.

</details>


### [212] [TruthStance: An Annotated Dataset of Conversations on Truth Social](https://arxiv.org/abs/2602.14406)
*Fathima Ameen,Danielle Brown,Manusha Malgareddy,Amanul Haque*

Main category: cs.CL

TL;DR: 这篇论文介绍了TruthStance数据集，首次针对Truth Social平台大规模收集并标注了争论挖掘和立场检测数据，涵盖丰富的对话结构，并对大语言模型在该任务上的表现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有关于争论挖掘和立场检测的公开数据主要集中在Twitter和Reddit等主流平台，缺乏对alt-tech平台（如Truth Social）上对话结构和观点交锋的研究。该领域亟需新的大规模数据资源，以便更全面理解不同平台上的舆论交互及其动态。

Method: 作者收集了Truth Social 2023-2025年的大规模对话树状结构数据，包括2.4万多条帖子和52万余条评论，保持了完整的回复关系。人工对1,500个实例进行了争论挖掘和立场检测标注，并计算了标注者间的一致性。此外，利用这些人工标注做基准，评估了多种大语言模型提示方法，并使用表现最佳的策略自动标注了更多的数据，使分析范围大幅扩展。

Result: 论文创建了首个Truth Social主流争论和立场数据集，人工标注集为LLM评测提供了基准，实验比较了多种LLM提示，遴选了最优配置并在十万量级评论上扩展自动标注，涵盖了评论深度、话题、用户等多维特征。

Conclusion: TruthStance数据集和基准为研究alt-tech平台上的对话争论与立场检测提供了重要资源。实验发现，经过优化提示的大语言模型具备扩展标注能力，为该领域上的后续分析和模型开发奠定基础，所有代码和数据已公开，为社区带来开放新工具。

Abstract: Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.

</details>


### [213] [WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)](https://arxiv.org/abs/2602.14419)
*Kiyotaka Kasubuchi,Kazuo Fukiya*

Main category: cs.CL

TL;DR: 本文通过测度理论和频谱分析重新诠释了LLM中的Transformer/Attention结构，指出幻觉是其结构性必然。提出WavePhaseNet方法和维度约简，结合上同调理论实现更高的语义一致性和更少的幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）因其Attention结构存在产生‘幻觉’（hallucination）的问题，本文试图从理论上解释其原因，并探索如何减少幻觉、提升推理和语义一致性。

Method: 采用测度理论将嵌入空间视为σ-代数上的条件期望，揭示其与语义真集的非同构性；提出WavePhaseNet方法，运用DFT将语义层次分解为不同频段，实现语义精确操作；分析GPT-4等Embedding向量维度的有效性，利用上同调正则化和Hodge理论对降维后信息进行一致性控制。

Result: 理论上证实幻觉为结构性缺陷，并通过能量分析确定Embedding降到约3,000维仍能完整保持语义与推理能力；基于上同调的空间能够衡量并减少局部推理的不一致性。

Conclusion: LLM幻觉源自存在的结构瓶颈。通过DFT分解的WavePhaseNet和上同调一致性正则化，在大幅降低Embedding维度的同时，显著提升了模型的推理严密性，减轻幻觉，奠定了设计一致性更强LLM的理论基础。

Abstract: This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space functions as a conditional expectation over a σ-algebra, and its failure to be isomorphic to the semantic truth set fundamentally causes logical consistency breakdown. WavePhaseNet Method The authors propose WavePhaseNet, which explicitly constructs a Semantic Conceptual Hierarchy Structure (SCHS) using Discrete Fourier Transform (DFT). By applying DFT along the sequence dimension, semantic information is decomposed into frequency bands: low-frequency components capture global meaning and intent, while high-frequency components represent local syntax and expression. This staged separation enables precise semantic manipulation in diagonalized space. Dimensionality Reduction GPT-4's 24,576-dimensional embedding space exhibits a 1/f spectral structure based on language self-similarity and Zipf's law. Through cumulative energy analysis, the authors derive that approximately 3,000 dimensions constitute the lower bound for "complete representation." This demonstrates that reduction from 24,576 to 3,000 dimensions preserves meaning and intent while enabling rigorous reasoning and suppressing hallucination. Cohomological Consistency Control The reduced embedding space, constructed via cohomological regularization over overlapping local windows, allows defining a graph structure and cochain complex. This quantifies inconsistencies among local inferences as coboundary-based losses. Applying harmonic projection based on Hodge theory positions cohomology as a computable regularization principle for controlling semantic consistency, extracting maximally consistent global representations.

</details>


### [214] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）辅助的知识蒸馏框架，用于提升时序知识图谱(TKG)推理的效率和表现。该方法在保证学生模型轻量化的同时，提升了推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理模型计算量大、部署成本高，主流模型压缩与蒸馏方法多针对静态图，忽视了时间依赖性，导致在时序环境下效果降低。因此，需要一种既具高效推理能力又能保留时序特性的轻量化方法。

Method: 作者提出结合高容量时序教师模型和LLM为辅助教师的双教师蒸馏框架。LLM提供丰富背景知识及时序信号指导，采用阶段对齐策略共同优化监督与蒸馏目标，使学生模型更好建模事件动态性。

Result: 在多个主流的TKG基准数据集、不同底座模型下，该方法在链路预测任务上超过了现有强蒸馏基线，并显著降低学生模型的推理复杂度。

Conclusion: 大语言模型作为教师模型能够有效提升轻量级TKG系统的时序推理能力，为资源高效的TKG应用提供新思路。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [215] [Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models](https://arxiv.org/abs/2602.14466)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文提出了FilBBQ，一个针对菲律宾语境下性别歧视和恐同偏见的生成式语言模型偏见测试集，并通过改进的评测协议系统分析了模型的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成在语言模型中的流行，人们越来越关注模型可能存在的刻板印象偏见。BBQ是重要的偏见评测基准，但其语言和文化覆盖有限，未能针对菲律宾这样的特定文化进行评测。为此，作者希望扩展BBQ，开发适用于菲律宾语境的更全面偏见测试工具。

Method: 作者通过模板分类、文化敏感翻译、新模板构建和提示生成四个阶段，开发出FilBBQ测试集，共包含一万余测试题目，覆盖了菲律宾文化背景下的性别和性取向相关偏见。测试过程中，多次使用不同随机种子生成模型响应，并对偏见评分结果取均值，从而减小模型输出的不稳定性对评测结果的影响。

Result: 实验发现，不同随机种子下偏见分数存在明显波动，同时在涉及情感、家庭、性少数群体兴趣和一夫多妻等多个方面，菲律宾语训练模型表现出一定的性别和恐同偏见。

Conclusion: FilBBQ有效扩展了偏见评测的语言和文化适用性，证实了当前菲律宾语模型存在相关偏见，同时提出的改进评测协议提升了结果的准确性与稳定性，为未来多语言、多文化偏见评测提供了新工具和方法。

Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.

</details>


### [216] [Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation](https://arxiv.org/abs/2602.14469)
*Guangyue Peng,Zongchao Chen,Wen Luo,Yuntao Wen,Wei Li,Ruixiang Feng,Ran Le,Chen Yang,Zhenwei An,Yang Song,Tao Zhang,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文发现逆链式思维（RCG）常受“答案锚定”困扰，简单地让模型忽略答案反而加重了这种依赖。为此，提出结构骨架引导推理（SSR），有效缓解锚定问题，并改进泛化能力。


<details>
  <summary>Details</summary>
Motivation: RCG通过已知问答对生成推理过程，但让模型看到答案易形成“事后合理化”，即答案影响整个解释过程，降低推理质量。研究旨在探明这一点并寻找缓解方法。

Method: 首先定义了三种不同层次的“答案锚定”测量方法（词汇、熵、概率）。接着测试了让模型“忽略答案”的策略，发现其反效果。受讽刺过程理论启发，提出SSR方法：先生成与答案无关的推理骨架，再用骨架生成完整推理，最后引入SSR蒸馏版（SSR-D）通过教师引导提升结构可靠性。

Result: 通过开放式推理基准测试，发现SSR能在所有层面上减少锚定，SSR-D比“忽略答案”基线提升了最高10%，且保持OOD泛化能力。

Conclusion: 结构骨架引导策略比单纯忽略答案更有效地削弱模型对答案的依赖，优化了推理生成的合理性和泛化能力。

Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.

</details>


### [217] [HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation](https://arxiv.org/abs/2602.14470)
*Wen-Sheng Lien,Yu-Kai Chan,Hao-Lung Hsiao,Bo-Kai Ruan,Meng-Fen Chiang,Chien-An Chen,Yi-Ren Yeh,Hong-Han Shuai*

Main category: cs.CL

TL;DR: 本文提出了一种适用于n元超图的检索增强生成（RAG）方法——HyperRAG，通过更高阶的关系建模提升多跳知识问答中的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法主要依赖二元关系和密集的相似性检索，导致检索引入大量无关信息、计算开销大、关系表达能力受限。因此，需要更灵活且表达更丰富的检索推理机制。

Method: 作者设计了HyperRAG框架，利用n元超图捕捉丰富的实体间高阶关系，并提出两种互补的检索方法：（1）HyperRetriever：学习结构-语义联合推理，按查询构建高阶关系链，提升多跳推理的可解释性和准确性；（2）HyperMemory：利用大模型的参数记忆引导检索路径动态扩展，通过beam search优化路径分支选择。

Result: 在11个闭域及3个开放领域知识问答数据集上，HyperRetriever在MRR和Hits@10等评价指标上分别超越最优基线2.95%和1.23%。定性分析显示该方法可自适应、高效地构建可解释的高阶推理链条。

Conclusion: HyperRAG基于高阶超图构建的新型RAG方法，在多跳推理中提升准确率和推理效率，尤其适用于复杂问答任务，对开闭域均有显著效益。

Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

</details>


### [218] [BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR](https://arxiv.org/abs/2602.14488)
*Md. Najib Hasan,Mst. Jannatun Ferdous Rain,Fyad Mohammed,Nazmul Siddique*

Main category: cs.CL

TL;DR: 本文提出了一种结合多种LLM注释器及严格校验机制的BETA标注框架，构建了孟加拉语IR数据集，并分析了跨语言数据集迁移的局限性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的信息检索（IR）任务因缺乏高质量注释数据集而受限，手动标注成本高、难以规模化，直接用大模型自动标注又存在标签可靠性和评估有效性等问题。

Method: 使用来自多种模型家族的LLM作为注释器，通过加入情境对齐、一致性检查和多数同意机制，最终经人工复核来提升标签质量。同时，研究通过LLM机器翻译实现跨低资源语言数据集重用的可行性，并检测语义保持和任务有效性的差异。

Result: 结果发现，不同语言间跨语言迁移存在语义保持不一致和模型偏差等问题，影响数据集的可靠性，且人工复核验证了标签的质量提升。

Conclusion: LLM辅助低资源语言数据集标注具有潜力，但在跨语言数据集复用时受限较多。本研究为低资源语言IR评测基准构建提供了风险警示和实践指导。

Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.

</details>


### [219] [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Ziyi Gao,Xiaotong Lin,Yun Liu,Xing Fu,Yu Cheng,Yongchao Liu,Weiqiang Wang,Zhongle Xie*

Main category: cs.CL

TL;DR: 本论文提出了一种新的用户表示学习框架“Query-as-Anchor”，可实现任务敏感、动态的用户表示，有效提升在多任务、多模态场景下的表现，并在支付宝工业级应用中取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前工业级用户表示学习在追求通用性的同时，难以兼顾具体任务的需求。现有方法多生成静态且任务无关的嵌入表示，导致在实际多样的下游场景内表现受限。此外，多源异构数据造成噪声和模态冲突，进一步削弱了表示能力。

Method: 作者提出Query-as-Anchor框架，将用户建模从静态编码转变为动态的、查询感知的合成。具体方法包括：(1) 构建了UserU大规模多模态行为-语义对齐数据集；(2) 提出Q-Anchor编码结构，将分层粗到细的编码器整合进双塔大模型，通过对比-自回归联合优化，实现查询感知的动态用户表示；(3) 引入基于聚类的软提示微调，改善潜在结构，增强对场景特定模态的关注；(4) 序列末尾查询锚定方式支持KV缓存快速推理，提升线上部署效率。

Result: 该方法在支付宝10个工业基准测试中取得了一致的SOTA表现，展现出强大的可扩展性和高效部署能力。大规模在线A/B测试验证了其在实际商业系统中的有效性。

Conclusion: Query-as-Anchor框架能够解决现有用户表示学习在任务自适应性和多模态冲突方面的核心难题，可高效应用于工业级复杂业务场景，具有显著实际价值。

Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.

</details>


### [220] [Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil](https://arxiv.org/abs/2602.14517)
*Sukumar Kishanthan,Kumar Thushalika,Buddhi Jayasekara,Asela Hevapathige*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型在资源稀缺语言（如僧伽罗语和泰米尔语）中的数学推理能力，发现其复杂推理表现显著下降，质疑其多语种推理的真实性。


<details>
  <summary>Details</summary>
Motivation: 主流观点认为大语言模型（LLMs）在多语言环境下具备均等的推理能力，但尚不清楚它们在低资源语言下是否依赖翻译或能进行本真的推理，尤其是数学推理领域。

Method: 作者设计了一个包含六种不同难度数学题型的测试，涵盖基础算术到复杂的单位冲突和优化问题。所有题目由三种语言（英语、僧伽罗语、泰米尔语）的母语数学专家分别独立创作，避免翻译偏差，并在此平行数据集上评测四个主流大语言模型。

Result: 大模型在基础算术推理上表现良好，但在复杂推理任务（如单位冲突、优化）中，僧伽罗语和泰米尔语表现大幅下降，不同模型和题型之间失败模式也有差异。

Conclusion: 当前大模型在多语言环境下并不能保证一致的推理能力，尤其在低资源语言和复杂任务上有显著短板，需采用更细致、类型感知的评测方法来全面理解其多语言推理水平。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.

</details>


### [221] [Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets](https://arxiv.org/abs/2602.14536)
*Yuchen Yang,Wenze Lin,Enhao Huang,Zhixuan Chu,Hongbin Zhou,Lan Tao,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的token级噪声过滤框架（XTF），通过对细粒度数据属性进行评估，显著提升大语言模型微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集通常以句子为单位设计，这与LLM的token级优化机制不匹配，导致token层面引入噪声，最终影响模型性能。

Method: XTF框架将token级数据对微调的贡献分解为“推理重要性”“知识新颖性”“任务相关性”三种属性，并通过打分筛选噪声token，在反向传播时屏蔽其梯度，从而优化LLM微调。

Result: 在数学、代码、医学三类任务和七个主流LLM实验中，XTF框架较常规微调最高可提升13.7%的下游性能。

Conclusion: token级数据集优化对于LLM微调至关重要，基于属性分解的方法有助于解释和提升复杂训练机制的效果。

Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.

</details>


### [222] [Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.14564)
*Shefayat E Shams Adib,Ahmed Alfey Sani,Ekramul Alam Esham,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文对五种不同规模的大型语言模型（LLM）进行了医疗问答系统的对比评测，结果显示大模型具备更强的表现，但部分中等规模模型在效率与性能间有良好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前医疗问答系统在低资源环境下有助于提升医疗可及性，但对于不同规模LLM的实际表现、部署效益及资源消耗尚未有系统性比较。本文旨在为医疗NLP任务实践提供标准基准和指导。

Method: 作者选取了iCliniq 3.8万多问题的医疗问答数据集，对Llama与GPT多个不同参数规模的LLM进行零样本（zero-shot）评测，并用BLEU、ROUGE等NLP标准指标对模型未经过专门微调的表现进行量化分析。

Result: 实验证明，Llama 3.3 70B Instruct等超大规模模型显著优于3B、8B等小型模型，同时，Llama-4-Maverick-17B在保持较小规模的同时获得接近大模型的效果，展现了性能与计算消耗的良好权衡。

Conclusion: 大型LLM正在逐步趋近专业级医疗推理水平，医疗QA系统的实用化可行性不断提升。此工作建立了标准化基准，有助于未来在降低模型规模和资源消耗、提升临床效用等方向继续优化。

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

</details>


### [223] [The Wikidata Query Logs Dataset](https://arxiv.org/abs/2602.14594)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 本文介绍了WDQL数据集，这是一个包含20万对Wikidata知识图谱上的问题与SPARQL查询对的数据集，是当前同类最大的数据集（比现有最大数据集大6倍），数据来源于真实用户查询日志并去模板化，更贴近实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有Wikidata问题-查询对数据集规模较小且多依赖模板化生成，缺乏真实世界的复杂查询，限制了问答系统训练效果。

Method: 收集Wikidata SPARQL查询服务的真实用户查询日志，对经匿名处理后的查询进行逐步去匿名、清洗和校正，并为每条查询生成对应自然语言问题，采用基于Agent方法辅助自动化处理。

Result: 得到一个20万对的大规模、偏真实用户行为的数据集，在训练问答系统时相较于模板生成数据更加有效；数据集和agent工具均开放共享。

Conclusion: WDQL数据集为知识图谱问答研究提供了更大、更真实、更具挑战性的新资源，促进了更高效问答方法的研发。

Abstract: We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.

</details>


### [224] [GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation](https://arxiv.org/abs/2602.14649)
*Hao Liu,Guangyan Li,Wensheng Zhang,Yongqiang Tang*

Main category: cs.CL

TL;DR: 本文提出了一种名为GradMAP的高效大型语言模型（LLM）层裁剪方法，在加快裁剪速度的同时维持甚至提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽具备强大推理能力，但其高计算成本限制了实际应用。研究发现LLM各层存在大量冗余，因此层裁剪成为研究热点。然而，现有方法在裁剪效果和效率之间难以兼顾。为解决这一问题，需要一种既快又能保持性能的裁剪方法。

Method: 提出两阶段裁剪方法GradMAP。第一阶段，利用梯度幅值作为指标来全局评估层的重要性，每次裁剪决策只需一次反向传播，极大提高效率。第二阶段，分析被裁剪层带来的整体均值偏移，并用一次投影补偿矩阵进行校正，有效缓解性能退化。

Result: 在多项实验中，GradMAP在裁剪速度和性能表现均超过现有方法，速度提升平均达到4倍。

Conclusion: GradMAP实现了高效且高性能的LLM层裁剪，为大模型实际部署提供了有力工具。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.

</details>


### [225] [Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?](https://arxiv.org/abs/2602.14653)
*Matteo Gay,Coleman Haley,Mario Giulianelli,Edoardo Ponti*

Main category: cs.CL

TL;DR: 本研究首次将统一信息密度(UID)理论应用于视觉语境下，实证分析了多语言多模态数据中信息分布规律，发现视觉语境强化了信息平滑分布，支持UID理论在上下文敏感环境下的普适性。


<details>
  <summary>Details</summary>
Motivation: 以往关于UID的研究几乎只考虑纯文本输入，忽略了实际交流时语句所依赖的感知环境。而自然语言常常与视觉及其他感知信息共同作用，作者希望通过引入视觉语境，检验UID理论在更生态化通信环境下是否依然成立。

Method: 本研究利用多语种视觉-语言模型，分析包含30种语言的图像-标题数据，以及13种语言的视觉故事叙述数据（共涵盖11种语系），通过模型预测测算信息‘惊异度’（surprisal），比较文本纯输入与多模态语境下的信息分布差异。

Result: 结果表明，在视觉感知语境下，信息密度分布更平滑，信息的均匀性提升，且这一趋势在不同语系、不同类型语言中广泛存在。对于视觉叙事任务，图像和语篇上下文协同作用带来更明显的信息均匀化，尤其在语篇单元开始处惊异度降低显著。

Conclusion: 研究首次揭示多模态（视觉-语言）环境下UID理论适用性，指出感知语境有助于提升信息分布的均匀性，支持以感知为基础的上下文敏感UID新框架，并对理解自然语言时的信息流动提供新视角。

Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.

</details>


### [226] [Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech](https://arxiv.org/abs/2602.14655)
*Xiao Wei,Bin Wen,Yuqin Lin,Kai Li,Mingyang gu,Xiaobao Wang,Longbiao Wang,Jianwu Dang*

Main category: cs.CL

TL;DR: 本文提出FAL-AD框架，通过联合联邦学习与数据增强，系统性地提升阿尔茨海默症早期语音诊断的数据效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默症早期诊断依赖数据驱动的AI语音检测，但面临医学数据稀缺与隐私难题，严重阻碍诊断效率和模型性能提升。作者旨在解决数据效率困境，使AI方法更实用可推广。

Method: 提出FAL-AD，该方法包括三层创新：1）以语音转换为核心的数据增强技术，生成不同类别交互的病理语音样本；2）引入自适应联邦学习框架，在隐私保护下实现跨机构协同优化；3）设计注意力感知的跨模态融合模型，实现词级语音文本细粒度对齐与互动。

Result: 在ADReSSo数据集上，FAL-AD取得91.52%的多模态最佳准确率，优于所有中心化基线模型，验证了该框架在提升数据效率和实际应用中的有效性。

Conclusion: FAL-AD框架突破了数据短缺和隐私挑战下AI语音诊断的性能瓶颈，为阿尔茨海默症早期筛查提供了可行、高效、安全的解决方案。

Abstract: Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.

</details>


### [227] [Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography](https://arxiv.org/abs/2602.14675)
*Gianluca Vico,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文介绍并公开了一个由众包方式收集的皮埃蒙特语（Piedmontese）意大利平行语料库，并利用该语料库对大语言模型进行了多项任务评测。


<details>
  <summary>Details</summary>
Motivation: 皮埃蒙特语是一种濒危的罗曼语，缺乏高质量的数据资源，严重影响了自然语言处理技术在该语言上的发展。因此，作者希望通过建立并公开语料库促进对皮埃蒙特语的研究，并评估当前大语言模型对低资源语言的处理能力。

Method: 作者从Flores+语料库中提取了145组意大利语-皮埃蒙特语平行句，并请母语使用者用自然正字法进行翻译，并进行人工词对齐。随后，利用该语料库对多种大语言模型在分词一致性、主题分类和机器翻译任务上进行了基准测试。

Result: 分析显示，相比高资源罗曼语（如意大利语、法语等），皮埃蒙特语在分词任务中所需的分词更细碎，即存在分词惩罚；但在主题分类任务上，模型的表现接近高资源语言。机器翻译任务中，从皮埃蒙特语译入高资源语言效果较好，但反向翻译（生成皮埃蒙特语）依旧具有挑战性。

Conclusion: 作者公开了该语料库及代码，为皮埃蒙特语及其他低资源语言的研究提供了重要资源，并展示了大语言模型在低资源语言处理上的机会与瓶颈。

Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.

</details>


### [228] [LLMStructBench: Benchmarking Large Language Model Structured Data Extraction](https://arxiv.org/abs/2602.14743)
*Sönke Tenckhoff,Mario Koddenbrock,Erik Rodner*

Main category: cs.CL

TL;DR: 本文提出了LLMStructBench，这是一个用于评估大型语言模型（LLM）从自然语言文本中提取结构化数据并生成有效JSON输出的新基准。作者提供了公开数据集、评测多模型多提示策略，并引入了新的性能指标。重点发现合适的提示策略比模型大小更关键。


<details>
  <summary>Details</summary>
Motivation: 随着LLM日益应用于信息抽取和自动化数据处理，准确地将自然语言转换为结构化数据变得尤为重要。当前缺乏专门针对结构化数据提取和生成有效JSON输出的系统性评测工具。

Method: 作者构建了一个包含多样、人工校验解析场景的开源数据集，并系统化测试了22种模型和5种提示（prompting）策略。引入了新指标以衡量token级准确率和文档结构有效性。

Result: 实验表明，选择合适的提示策略在保证结构有效性上比模型体量更重要。这对于小模型或本身较弱模型尤为显著，但也可能导致语义错误增加。

Conclusion: LLMStructBench为未来LLM在解析及ETL应用相关研究奠定了基础，强调了提示工程的重要性，并呼吁进一步研究如何提升解析准确性和稳定性。

Abstract: We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.
  In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.

</details>


### [229] [Rethinking the Role of LLMs in Time Series Forecasting](https://arxiv.org/abs/2602.14744)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 之前的研究认为大语言模型（LLM）对时间序列预测（TSF）没有实质帮助，但本论文通过大规模实验证明，在多样任务和场景下，LLM显著提升了预测效果，尤其在跨领域泛化任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 鉴于此前许多工作对LLM在TSF中的有效性提出质疑，原因多为评估设定有限，作者希望系统性、全面地检验LLM用在TSF的实际价值及其适用边界。

Method: 作者开展了大规模实证：覆盖8亿观测值、17种预测场景、4个时间跨度、不同模型对齐策略、包括域内和跨域任务，并分析了预训练知识、模型结构、对齐方式等因素的影响。

Result: 实验证明LLM在TSF任务尤其是跨领域任务上有明显性能提升。预对齐优于后对齐。LLM的结构和知识都重要：预训练对分布偏移下至关重要，架构则善于处理复杂时序动态。大规模混合分布下，完整的LLM不可或缺。

Conclusion: 该工作推翻了过去对LLM无效的认知，明确LLM对TSF的重要作用，揭示了在哪些条件下LLM有效，并为现实中如何设计和应用LLM于TSF提供了实用指导。

Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.

</details>


### [230] [Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins](https://arxiv.org/abs/2602.14749)
*Francesco Gariboldi,Emma Franchino,Edith Haim,Gianluca Lattanzi,Alessandro Grecucci,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究结合认知网络科学，利用BFMN方法，系统分析了不同阶段STEM学习者（包括AI数字孪生体）对科学及其相关学科的认知-情感结构，揭示了数学与焦虑之间的紧密联系，以及AI与人的体验差异。


<details>
  <summary>Details</summary>
Motivation: STEM领域的学习态度受到知识、教育经历及情感的影响，尤其是在应对数学与统计等定量科目时，很多学习者存在焦虑，影响其发展。作者希望借助新颖的网络科学方法，深入探究不同人群对STEM的整体心态，并评估AI模型在模拟这些心态时的优缺点。

Method: 作者采用行为形式心智网络（BFMN）方法，将应答者对关键词的自由联想与情感标注构建为认知网络，并引入AI大模型（GPT-oss）模拟“数字孪生”作对比。通过量化不同主题概念的情感色彩、情绪剖面、网络重叠度和具体性，分析了近千份数据，涵盖高中生、本科生及STEM初级专家。

Result: 结果显示，不同人群一致对科学及研究给予较积极评价，对数学和统计则关联更多负面及焦虑情绪，尤其在高数学焦虑群体表现强烈，且相关概念联想更为抽象与脱离实际。数学与焦虑在人的网络中重叠度高于AI孪生体，显示人的经验性与情感关联更丰富。AI模型虽然能大体反映主流文化态度，但难以体现人类独特的情境认知与焦虑体验。

Conclusion: BFMN方法能有效捕捉STEM学习心态背后的认知和情感特征。数学焦虑源于抽象和去情境化的负性联想，数字孪生AI虽可刻画部分态度结构，其局限性提醒我们，人类教育体验的复杂性难以被完全复制，对教育焦虑等需更关注主观经历和情感维度。

Abstract: Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are cue words and free associations, edges are empirical associative links, and each concept is annotated with perceived valence. We analyse BFMNs from N = 994 observations spanning high school students, university students, and early-career STEM experts, alongside LLM (GPT-oss) "digital twins" prompted to emulate comparable profiles. Focusing also on semantic neighbourhoods ("frames") around key target concepts (e.g., STEM subjects or educational actors/places), we quantify frames in terms of valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Across student groups, science and research are consistently framed positively, while their core quantitative subjects (mathematics and statistics) exhibit more negative and anxiety related auras, amplified in higher math-anxiety subgroups, evidencing a STEM-science cognitive and emotional dissonance. High-anxiety frames are also less concrete than chance, suggesting more abstract and decontextualised representations of threatening quantitative domains. Human networks show greater overlapping between mathematics and anxiety than GPT-oss. The results highlight how BFMNs capture cognitive-affective signatures of mindsets towards the target domains and indicate that LLM-based digital twins approximate cultural attitudes but miss key context-sensitive, experience-based components relevant to replicate human educational anxiety.

</details>


### [231] [Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers](https://arxiv.org/abs/2602.14760)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.CL

TL;DR: 本论文发现自回归Transformer因残差连接导致输入和输出对齐存在偏差，并提出了一种轻量级残差路径缓解方法，有效改善了表征对齐并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）通过因果掩码实现并行化的下一个token预测训练，但由于残差连接将激活固定在当前token而监督信号指向下一个token，可能会导致信息错配，影响模型表现。

Method: 作者使用解码轨迹和基于相似度的衡量指标，在预训练LLM中实证分析表示对齐从输入对齐向输出对齐的转变，并提出了基于残差衰减的残差路径缓解方法，包括定层干预和可学习门控机制两种实现方式。

Result: 在多个基准测试中，提出的方法有效缓解了表示错配问题，提升了模型的表现。

Conclusion: 残差路径错配是自回归Transformer模型中真实存在的问题，采用轻量级的残差路径干预可带来普适且高效的架构改进，有助于提升LLM在各种任务中的性能。

Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.

</details>


### [232] [Unlocking Reasoning Capability on Machine Translation in Large Language Models](https://arxiv.org/abs/2602.14763)
*Sara Rajaee,Sebastian Vincent,Alexandre Berard,Marzieh Fadaee,Kelly Marchisio,Tom Kocmi*

Main category: cs.CL

TL;DR: 作者发现当前针对推理优化的大语言模型在机器翻译任务上表现不佳，通过提出结构化推理框架和合成数据集后，明显提升了翻译质量，说明推理需针对任务结构化才能显著提升机器翻译效果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学、编程等涉及显式推理的任务中表现优异，但其在机器翻译中的推理效果尚且欠缺。论文旨在探索推理机制在提升机器翻译质量上的可行性及其局限，并寻找改进策略。

Method: 作者系统性地评估了多种推理型大模型在WMT24++机器翻译基准集上的表现，发现直接增加显式推理反而降低了翻译质量。分析后，提出结合多步起草、译文充分性优化、流畅性提升和选择性迭代修订的结构化推理框架，同时构建了动态结构化推理的合成数据，并用其对大模型进行后训练。

Result: 简单引入推理步骤会降低机器翻译质量，即便用强模型生成高质量推理也无法显著提升弱模型。采用结构化推理框架和新式训练数据，模型在翻译效果上显著优于传统微调及通用推理基线。

Conclusion: 有效的机器翻译模型需要面向任务定制推理结构，而不是直接套用通用推理模式。结构化推理能更好地服务于翻译任务，是提升翻译型大模型性能的关键。

Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.

</details>


### [233] [Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation](https://arxiv.org/abs/2602.14770)
*Shiwei Hong,Lingyao Li,Ethan Z. Rong,Chenxinran Shen,Zhicong Lu*

Main category: cs.CL

TL;DR: 该论文研究了在多人讨论环境下，社区讨论对大型语言模型（LLM）喜剧写作质量的影响。结果显示，引入讨论能显著提升作品的工艺性、清晰性与社会响应。


<details>
  <summary>Details</summary>
Motivation: 虽然以往研究关注LLM写作过程中的多轮互动及反馈，但大多评估仅限于提示和局部反馈，对于网络社区持续性公共接收反馈的效用关注不足。

Method: 作者构建了一个多智能体沙盒，设定讨论组（记录评论和观众讨论，作为社交记忆用于后续写作）与不讨论组，并采用专家A/B偏好及15项量表对50轮（共250对）单口喜剧稿件进行对比评测。

Result: 在专家评价中，含有社区讨论的条件下，75.6%的作品被偏好，工艺/清晰度（提升0.440）、社会响应（提升0.422）得分显著提高，同时出现了更激进幽默的倾向。

Conclusion: 引入公开社区讨论反馈能有效提升LLM生成内容的质量，尤其在喜剧写作场景下提升其表达与社会互动表现，但可能伴随风险，比如偏激幽默风格的增加。

Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.

</details>


### [234] [Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment](https://arxiv.org/abs/2602.14777)
*Laurène Vaugrante,Anietta Weckauff,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在出现和修正在所谓“突现失调”（emergent misalignment）现象时，模型是否具备对自身行为变化的自我觉察。实验证明模型确实能感知并描述自身的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 已有研究发现，微调于错误问答对的大模型会出现毒性或错误行为（即突现失调），且模型可自我描述这些行为。作者好奇：模型能否准确觉察并表达自身在行为对齐与失调间的转变？

Method: 依次用引发突现失调和修正该现象的数据集合微调GPT-4.1，然后不提供上下文示例，直接测试模型对于自身行为状态的描述能力，比较模型在不同状态下的自我评价。

Result: 模型在被诱发出失调行为后，会将自己评价为比基础模型和“再对齐”后模型更有害，表明其“自我觉察”能力能反映真实的安全状态。

Conclusion: 模型不仅能觉察到自身行为的失调，还能表达自己的安全性状态，这为评估和提升LLM安全性提供了一种可行方法。

Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed "emergent misalignment". Moreover, research has shown that LLMs possess behavioral self-awareness - the ability to describe learned behaviors that were only implicitly demonstrated in training data. Here, we investigate the intersection of these phenomena. We fine-tune GPT-4.1 models sequentially on datasets known to induce and reverse emergent misalignment and evaluate whether the models are self-aware of their behavior transitions without providing in-context examples. Our results show that emergently misaligned models rate themselves as significantly more harmful compared to their base model and realigned counterparts, demonstrating behavioral self-awareness of their own emergent misalignment. Our findings show that behavioral self-awareness tracks actual alignment states of models, indicating that models can be queried for informative signals about their own safety.

</details>


### [235] [A Geometric Analysis of Small-sized Language Model Hallucinations](https://arxiv.org/abs/2602.14778)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CL

TL;DR: 该论文提出通过嵌入空间的几何特征分析并检测小型LLM生成的幻觉内容，使用少量标注即可实现高效、准确分类。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型易出现幻觉输出（即内容流畅但不真实），尤其是在多步推理或具备代理特性的任务中，传统方法难以高效判断和筛选。

Method: 作者从几何角度出发，假设真实回复的特征嵌入会形成更紧密的聚类，幻觉较分散。通过证明该假设，并进一步提出了一种基于几何分离性的标签传播方法，仅需30-50个标注即可分类大量回复。

Result: 几何聚类方法可有效区分真实与幻觉回复，提出的方法在实验中F1分数超过90%。

Conclusion: 将幻觉识别问题转化到嵌入空间的几何分析角度，相比知识检索或单一回复分析，更高效且有较广泛的应用前景。

Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.
  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.
  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.

</details>


### [236] [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
*Yohan Lee,Jisoo Jang,Seoyeon Choi,Sangyeop Kim,Seungtaek Choi*

Main category: cs.CL

TL;DR: 本文揭示了基于LLM的工具代理面临新的供应链攻击面，攻击者通过注册恶意第三方工具，诱使模型陷入循环调用和资源消耗，显著放大计算资源和影响任务表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理广泛利用第三方工具完成复杂任务，这些工具基于文本元数据进行选择和调用，便捷的机制为攻击者注册并滥用工具提供了机会。作者关注由此带来的潜在供应链安全问题。

Method: 作者提出了“结构化过度思考攻击”，通过在工具注册表中加入14个设计恶意行为的工具，在3个服务器环境下进行实验。通过多种模型和异构注册表，观察模型反复、冗余调用恶意工具的现象。

Result: 实验表明，恶意工具可以诱导LLM陷入循环和反复调用序列，导致token消耗最多放大142.4倍，并且实际任务表现受损。常用的生成控制措施无法有效防范该类结构化循环。

Conclusion: 仅靠生成长度等token级别的控制手段无法防御此类攻击，防御策略需关注工具调用结构本身，完善整个LLM工具协同生态的安全性。

Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.

</details>


### [237] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文提出了BasPhyCo，这是首个用于巴斯克语（包括标准和方言变体）的非问答类物理常识推理数据集，并通过多种多语言大模型及专门模型进行评测，发现低资源语言下大型语言模型的物理常识推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前物理常识推理是NLP领域的重要课题，但尚无针对低资源语言（如巴斯克语）且不是简单问答类任务的数据集，也未评估大模型在这些任务和语言下的表现。

Method: 以意大利语GITA数据集为基础，构建了适用于巴斯克语（涵盖标准和方言变体）的非QA物理常识推理数据集BasPhyCo。评估分为三层：1）判断叙述是否合理（准确性）；2）找出导致叙述不合理的冲突成分（连贯性）；3）确定具体造成不合理的物理状态（可验证性）。选用多语种大模型和针对意大利语、巴斯克语预训练的模型进行实验。

Result: 多模型在准确性和连贯性方面表现较好，但在涉及“可验证性”时，尤其是在方言处理任务上，大型语言模型在低资源语言下表现不足，反映其物理常识推理能力有限。

Conclusion: 当前多语种大模型及专门模型在低资源语言领域（如巴斯克语）的复杂物理常识推理能力仍需提升，尤其是在处理方言变体和抽象物理状态理解上。BasPhyCo的公开可望推动该方向研究。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


### [238] [Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research](https://arxiv.org/abs/2602.14819)
*Matteo Rinaldi,Rossella Varvara,Viviana Patti*

Main category: cs.CL

TL;DR: 本文介绍了“Testimole-conversational”语料库，这是一个包含意大利语讨论板消息的大型数据集（30B词，1996-2024年），适用于意大利语大模型的预训练及语言和社会学研究。


<details>
  <summary>Details</summary>
Motivation: 现有意大利语大数据集稀缺，难以支持大模型训练及意大利语数字交流研究，因此迫切需要收集大规模、真实的意大利语对话数据。

Method: 作者收集整理了自1996年至2024年间的意大利语讨论板消息，累计超30B词，通过筛选、清洗等步骤，形成面向多任务分析的高质量语料库。

Result: 构建了极大规模、时间跨度广的意大利语讨论板语料库，能反映计算机媒介交流、非正式书面语、话语结构和网络社会互动等多层面内容。

Conclusion: Testimole-conversational语料库为意大利语大模型预训练、语言变异研究及社会现象分析等提供了强有力的资源，并将免费开放给研究界。

Abstract: We present "Testimole-conversational" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.

</details>


### [239] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: 本文提出了一种名为BFS-PO的强化学习算法，可有效缓解大型推理模型在长链推理任务中易产生冗长答案（过度思考）的现象，实现更精炼的推理，同时提升准确度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在面对复杂推理任务时，虽然表现出色，但往往因生成冗长推理链而导致计算成本增加与输出冗余。现有的强化学习算法（如GRPO/DAPO）更容易加剧这种过度思考现象，因此亟需一种方法减少输出冗余、提高模型效率。

Method: 作者提出了BFS-PO，一种结合Best-First Search探索策略的强化学习算法。BFS-PO通过基于最大熵节点的回溯机制，寻找最短且正确的答案，并在训练过程中不断鼓励模型生成更短的推理链。

Result: 在多个基准测试和不同大型推理模型基础上，BFS-PO能够有效缩短模型生成的答案，同时提升推理准确率。

Conclusion: BFS-PO为缓解大型推理模型的过度思考问题提供了有效解决方案，既提升了模型的精炼度，也提高了准确性，具有实际应用价值。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [240] [Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition](https://arxiv.org/abs/2602.14955)
*Varun Nathan,Shreyas Guha,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文提出了一个在联络中心领域中，进行工具感知的计划生成的框架与基准，支持复杂业务数据查询的自动步骤分解与工具分配，并对主流大模型的表现进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 在联络中心的数据分析场景下，业务查询往往需要被自动分解成多个步骤、并交由结构化（如Text2SQL/Snowflake）和非结构化（如RAG/转录文本）工具处理。当前缺少用于评估和提升此类多工具协同计划生成能力的系统化方法和基准，因此需要新框架来覆盖复杂场景、并推动大模型智能体在“用工具”方面的进展。

Method: 1. 提出一个基于领域的计划生成和评测框架，包括参考式评价（指标评价与单步匹配）和数据集精炼循环（评价器优化器迭代采样获高质量计划流程）；2. 评估了14个不同类型和规模的大语言模型在多工具计划分解、步骤执行和工具分配能力，比较了有无“计划演化历史”对效果的影响。

Result: 实验证明，大模型在面对复杂、步骤较多（超过4步）的复合查询时能力有限，最优总分不到85%，顶级单步“极好/很好”命中率不足一半。计划演化历史对部分模型和可执行性有帮助，但整体提升有限。主要难点在于工具选择和用法覆盖。简短、结构简单的计划要容易许多。

Conclusion: 当前主流大模型在多工具协同的数据分析计划生成上仍有显著短板，尤其是在工具理解与工具间匹配上。本文框架和基准为后续研究和模型改进提供了系统的评测和优化路径。

Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the "A+" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.

</details>


### [241] [Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System](https://arxiv.org/abs/2602.14970)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 该论文评估了大型语言模型（LLM）在呼叫中心质检中可能存在的公平性问题，发现这些模型在多维度上存在系统性偏差，即便更大、更对齐的模型也无法完全消除不公。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在呼叫中心质检等高风险领域的广泛应用，对其潜在的群体和行为偏见日益引发关注。作者旨在系统性地量化和揭示LLM评估中可能存在的不公平现象。

Method: 作者提出了基于反事实公平性的评测框架，涵盖身份、情境和行为三大类的13个维度，利用CFR（反事实翻转率）和MASD（分数差异均值）对18个主流LLM在3,000条真实话务数据上的评判一致性与公正性进行定量分析。同时，还考察了基于提示工程提升公平性的效果。

Result: 实验揭示多种偏见普遍存在，不同维度的CFR在5.4%~13.0%，MASD也存在持续偏移，更大的模型相对偏见较低，但公平性并不与准确率完全正相关。历史绩效的情境提示会加剧偏见（CFR最高达16.4%）；语言身份暗示始终是主要的偏见来源。公平性提示（prompting）仅带来有限改进。

Conclusion: LLM在关键场景下部署前必须进行标准化的公平性评测，否则易造成带有偏见的绩效评估和员工反馈。当前解决方案（如公平性提示）作用有限，需持续研发和完善审计工具。

Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.

</details>


### [242] [Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation](https://arxiv.org/abs/2602.15005)
*Mengdan Zhu,Yufan Zhao,Tao Di,Yulan Yan,Liang Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的大语言模型方法，通过跨域用户信号生成高质量的兴趣驱动新闻搜索查询列表，并经过系统性实验在真实新闻推荐系统中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统往往只关注用户的表层行为，很难捕捉到可迁移、可复用的深层兴趣，且需要在大规模生产环境下保持可扩展性；因此需要一种能挖掘用户更深层兴趣的高效推荐框架。

Method: 将“生成兴趣驱动的查询列表”建模为策略优化问题，采用带多重奖励信号的GRPO强化学习训练大语言模型，从跨域信号推断用户兴趣。同时研究推理时采样和模型容量的扩展性，通过教师模型向学生模型的在策略蒸馏，实现高性能下的高效部署。

Result: 实验涵盖离线实验、消融研究和大规模线上A/B测试。结果显示：增加计算量带来持续性能提升，最终模型在兴趣建模和下游推荐表现上均有显著改进。

Conclusion: 提出的方法能有效捕捉和建模用户深层兴趣，在实际大规模推荐系统中提升了推荐效果，并具备良好的可扩展性和部署能力。

Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.

</details>


### [243] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: 本文提出了一种新的冷启动个性化方法Pep，能够在用户历史数据缺乏的情况下，更高效与用户交互并准确推断其偏好。


<details>
  <summary>Details</summary>
Motivation: 冷启动个性化难题在于，任务中有多个偏好维度，但用户只关心部分，且关心哪些因人而异。在问题预算有限的情况下，盲目提问难以识别出重要维度，现有强化学习方法未能有效利用数据结构。

Method: Pep方法将冷启动偏好获取分为两个阶段：离线学习偏好相关结构（通过完整用户画像学习偏好之间的关联），和在线贝叶斯推理（利用训练好的结构模型，在实际交互时结合用户响应无训练地选择问题与预测其完整偏好）。Pep模型模块化，仅需简单的概率信念模型，参数量小。

Result: 在医学、数学、社会和常识推理任务中，Pep生成的响应与用户表述的偏好一致度为80.8%，高于强化学习基线的68.5%，且交互次数减少3-5倍。Pep针对不同用户的不同回答调整后续提问的比例为39-62%，远高于强化学习基线（0-28%），且参数量仅万级，远小于RL基线的8B。

Conclusion: 冷启动偏好获取的瓶颈在于能否利用偏好数据的结构性。Pep通过结构化学习和贝叶斯推理，大幅提升了效率和准确性，且模型简洁高效，适用于多种下游任务。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


### [244] [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)
*Ruoxi Liu,Philipp Koehn*

Main category: cs.CL

TL;DR: 提出了一种基于参数高效微调大模型的文本风格迁移（TST）新方法，通过往返翻译自动合成平行数据，结合RAG提升风格一致性和健壮性，实验优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 文本风格迁移任务中缺乏高质量风格映射的平行语料，导致难以训练具备风格转换能力的大模型。本文旨在解决这一数据瓶颈，提升TST效果。

Method: 利用往返翻译（roundtrip translation）将单语语料合成去风格化的平行数据，实现训练时和推理时风格输入统一。对大模型进行参数高效微调，并借助检索增强生成（RAG）机制，辅助模型在术语和人名表达上保持鲁棒性和风格一致性。

Result: 在四个领域内实验，本文方法在BLEU分数和风格准确率上均优于零样本提示（zero-shot prompting）和少样本上下文学习（fewshot ICL）。结合RAG后，模型表现更加稳健、风格迁移一致性更高。

Conclusion: 提出的方法能在平行语料有限情况下，显著提升大模型的文本风格迁移能力，并通过结合知识检索机制进一步优化实际应用表现。

Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [245] [UAVGENT: A Language-Guided Distributed Control Framework](https://arxiv.org/abs/2602.13212)
*Ziyi Zhang,Xiyu Deng,Guannan Qu,Yorie Nakahira*

Main category: cs.RO

TL;DR: 本文提出一种多无人机系统的语言控制架构，实现了高层任务的语言指令控制，并在底层确保了系统的鲁棒性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统执行复杂任务时，如何融合自然语言指令的灵活性与物理层面稳定性的形式保证，是一大挑战。本研究旨在实现人机混合下既易用又可靠的多无人机协作。

Method: 设计了三层架构：1）人类操作员用自然语言下达任务；2）基于大模型（LLM）的监督器周期性解释、验证和修正任务指令以适应任务和环境变化；3）分布式闭环控制器利用本地信息跟踪指令。理论上分析了在扰动有限、参考轨迹分段平滑的条件下的跟踪表现。

Result: 实现了基于自然语言任务推理和分布式反馈控制的系统，在理论上证明了跟踪的鲁棒性和稳定性，并能适应大模型周期性更新带来的离散跳变。

Conclusion: 系统性地展示了如何将集中式语言任务推理与分布式物理控制结合，实现了安全可靠、行为复杂的多无人机协作控制。

Abstract: We study language-in-the-loop control for multi-drone systems that execute evolving, high-level missions while retaining formal robustness guarantees at the physical layer. We propose a three-layer architecture in which (i) a human operator issues natural-language instructions, (ii) an LLM-based supervisor periodically interprets, verifies, and corrects the commanded task in the context of the latest state and target estimates, and (iii) a distributed inner-loop controller tracks the resulting reference using only local relative information. We derive a theoretical guarantee that characterizes tracking performance under bounded disturbances and piecewise-smooth references with discrete jumps induced by LLM updates. Overall, our results illustrate how centralized language-based task reasoning can be combined with distributed feedback control to achieve complex behaviors with provable robustness and stability.

</details>


### [246] [DORA: Dataflow Oriented Robotic Architecture](https://arxiv.org/abs/2602.13252)
*Xiaodong Zhang,Baorui Lv,Xavier Tao,Xiong Wang,Jie Bao,Yong He,Yue Chen,Zijiang Yang*

Main category: cs.RO

TL;DR: DORA是一种为机器人中高效数据传输设计的新架构，通过消除不必要的数据序列化/反序列化，提高了系统的响应和效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件在处理大数据和异构数据场景下，通信效率低，尤其是在机器人内部通信和Python环境中；传统机制依赖（反）序列化，带来较大的性能开销。

Method: 提出了Dataflow-Oriented Robotic Architecture (DORA)，支持显式数据依赖和高效的零拷贝数据传输。系统开源，并通过仿真和实物机器人测试实验。

Result: 实验表明，与当前最先进的中间件系统相比，DORA有效减少了通信延迟和CPU开销。

Conclusion: DORA为数据密集型机器人应用提供了更高效的通信基础，显著提升了机器人系统的性能，特别适用于工业等高负载场景。

Abstract: Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.

</details>


### [247] [High-Fidelity, Customizable Force Sensing for the Wearable Human-Robot Interface](https://arxiv.org/abs/2602.13436)
*Noah Rubin,Ava Schraeder,Hrishikesh Sahu,Thomas C. Bulea,Lillian Chin*

Main category: cs.RO

TL;DR: 本论文提出并验证了一种基于流体感知的硅胶垫，用于高效、线性地测量人机接口处的机械交互力，可应用于可穿戴机器人。


<details>
  <summary>Details</summary>
Motivation: 精确测量人机接口的机械特性对于理解用户行为和优化可穿戴机器人性能非常重要，但受限于传感器制造复杂性和不线性响应，目前难以实现高性能传感。

Method: 设计了一种内嵌空气通道的3D打印硅胶垫，通过外部压力变化导致通道压力变化，利用现成压力传感器进行测量。在台架实验和临床等多种环境下验证其与机械力、膝关节力矩、肘关节角度等变量的相关性，并集成到下肢外骨骼装置实际测试。

Result: 实验结果显示，该系统测力与压力变化高度线性（R^2=0.998），与临床动力计测试结果高度相关（膝关节屈曲R^2=0.95，伸展R^2=0.75），在日常运动过程中能稳定追踪运动阶段和动态。

Conclusion: 流体感知垫是一种可定制、高信噪比、高时间分辨率的人机接口机械交互检测手段。长远来看，有望为可穿戴机器人实时控制及用户功能检测提供新的传感输入。

Abstract: Mechanically characterizing the human-machine interface is essential to understanding user behavior and optimizing wearable robot performance. This interface has been challenging to sensorize due to manufacturing complexity and non-linear sensor responses. Here, we measure human limb-device interaction via fluidic innervation, creating a 3D-printed silicone pad with embedded air channels to measure forces. As forces are applied to the pad, the air channels compress, resulting in a pressure change measurable by off-the-shelf pressure transducers. We demonstrate in benchtop testing that pad pressure is highly linearly related to applied force ($R^2 = 0.998$). This is confirmed with clinical dynamometer correlations with isometric knee torque, where above-knee pressure was highly correlated with flexion torque ($R^2 = 0.95$), while below-knee pressure was highly correlated with extension torque ($R^2 = 0.75$). We build on these idealized settings to test pad performance in more unconstrained settings. We place the pad over \textit{biceps brachii} during cyclic curls and stepwise isometric holds, observing a correlation between pressure and elbow angle. Finally, we integrated the sensor into the strap of a lower-extremity robotic exoskeleton and recorded pad pressure during repeated squats with the device unpowered. Pad pressure tracked squat phase and overall task dynamics consistently. Overall, our preliminary results suggest fluidic innervation is a readily customizable sensing modality with high signal-to-noise ratio and temporal resolution for capturing human-machine mechanical interaction. In the long-term, this modality may provide an alternative real-time sensing input to control / optimize wearable robotic systems and to capture user function during device use.

</details>


### [248] [FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation](https://arxiv.org/abs/2602.13444)
*Huajian Zeng,Lingyun Chen,Jiaqi Yang,Yuantai Zhang,Fan Shi,Peidong Liu,Xingxing Zuo*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的手-物交互（HOI）生成框架FlowHOI，能更好地完成长时序、接触丰富的操作任务，并成功验证了该方法在模拟与真实机器人环境中的有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型无法很好地处理长时序、手与物体接触丰富的操作场景，主要原因是缺乏对手-物交互结构的显式建模，导致动作难以验证与迁移。

Method: 作者提出FlowHOI，两阶段的流配准框架，生成具有语义与时序连贯性、包含手姿、物体姿态和接触状态的HOI序列。其利用三维高斯重建场景，并用运动-文本对齐损失，实现语义和物理场景的结合。此外，还开发了一个轨迹重建流程，从大规模头戴视角视频中自动提取高保真手-物轨迹数据，作为先验用于训练。

Result: 在GRAB与HOT3D基准上，FlowHOI取得了最高的动作识别准确率和1.7倍于主流扩散模型的物理仿真成功率，同时推理速度提升40倍，并在4个真实机器人任务中实现了方案迁移。

Conclusion: FlowHOI显著提升了复杂操作生成的可靠性、泛化能力和效率，为手-物交互任务的机器人验证与迁移提供了新途径。

Abstract: Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.

</details>


### [249] [Inferring Turn-Rate-Limited Engagement Zones with Sacrificial Agents for Safe Trajectory Planning](https://arxiv.org/abs/2602.13457)
*Grant Stagg,Cameron K. Peterson*

Main category: cs.RO

TL;DR: 本文提出了一种通过牺牲型代理估算追击-规避场景中追击者参数的学习框架，利用二元生存/拦截结果及可达区域模型，实现参数优化，并实现对高价值代理的安全路径规划。


<details>
  <summary>Details</summary>
Motivation: 在追击者运动能力受限（如转向速率受限）的对抗场景中，准确估计追击者参数对于最大化摧毁对手和保护己方关键目标至关重要。计算拦截区域并利用简单代理试探敌方参数，有助于更好地制定规避策略。

Method: 设计了两种拦截区模型（边界拦截和内部拦截），通过大量牺牲型代理直线冲向敌方并汇报存活或被捕状态，将观测二元结果与可达区域参数关联。采用多起点梯度优化，针对不同模型自定义损失函数；并提出两种代理轨迹选择策略：几何启发式和基于最大信息增益的贝叶斯实验设计。

Result: 通过蒙特卡洛模拟实验，使用5~12个牺牲代理能准确恢复追击者参数。基于学习到的模型，进一步可为高价值目标规划安全的、时间最优的规避路径，避开所有潜在追击威胁区域。

Conclusion: 该框架能有效利用牺牲代理的信息，通过学习快速准确地恢复追击者参数，并服务于高价值目标的安全移动，具备实际应用潜力。

Abstract: This paper presents a learning-based framework for estimating pursuer parameters in turn-rate-limited pursuit-evasion scenarios using sacrificial agents. Each sacrificial agent follows a straight-line trajectory toward an adversary and reports whether it was intercepted or survived. These binary outcomes are related to the pursuer's parameters through a geometric reachable-region (RR) model. Two formulations are introduced: a boundary-interception case, where capture occurs at the RR boundary, and an interior-interception case, which allows capture anywhere within it. The pursuer's parameters are inferred using a gradient-based multi-start optimization with custom loss functions tailored to each case.
  Two trajectory-selection strategies are proposed for the sacrificial agents: a geometric heuristic that maximizes the spread of expected interception points, and a Bayesian experimental-design method that maximizes the D-score of the expected Gauss-Newton information matrix, thereby selecting trajectories that yield maximal information gain. Monte Carlo experiments demonstrate accurate parameter recovery with five to twelve sacrificial agents. The learned engagement models are then used to generate safe, time-optimal paths for high-value agents that avoid all feasible pursuer engagement regions.

</details>


### [250] [AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge](https://arxiv.org/abs/2602.13476)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: AsyncVLA是一种异步控制框架，将大型视觉-语言基础模型与边缘轻量模型结合，在动态机器人环境中兼顾高语义理解与实时反应，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型依赖大规模视觉-语言模型，虽然具备很强的泛化能力，但推理延迟高，无法满足动态环境下对实时控制的需求，从而影响了其在现实场景中的安全和部署。

Method: 提出AsyncVLA异步控制架构，将语义推理（大模型）与反应执行（轻量模型）解耦。大模型在远端工作站执行高层决策，通过通信向机器人发送指导，机器人本地Edge Adapter以高频率调整动作。通过端到端微调和轨迹重加权策略，弥合异步流之间的域间差距，强化动态交互能力。

Result: 在实际视觉导航任务中，AsyncVLA在通讯延迟高达6秒的情况下，成功率比现有最先进基线高40%。

Conclusion: AsyncVLA有效桥接了大模型的语义智能与机器人边缘设备对反应速度的需求，提升了在动态、延迟环境下的实际部署能力。

Abstract: Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.

</details>


### [251] [ONRAP: Occupancy-driven Noise-Resilient Autonomous Path Planning](https://arxiv.org/abs/2602.13577)
*Faizan M. Tariq,Avinash Singh,Vipul Ramtekkar,Jovin D'sa,David Isele,Yosuke Sakamoto,Sangjae Bae*

Main category: cs.RO

TL;DR: 本文提出了一种基于占用栅格（occupancy grid）的动态路径规划方法，能够在感知和定位存在噪声的情况下，实时生成安全且可行的路径，适用于静态和动态障碍物环境。该方法无需复杂手动调参，泛化能力强，能兼容主流控制系统。


<details>
  <summary>Details</summary>
Motivation: 在实际无人驾驶和机器人导航中，路径规划需要在感知噪声、不确定定位及环境语义信息不完整等挑战下保持可靠，保证安全、高效的动态路径生成。传统方法往往依赖复杂的启发式规则，或需要精确环境建模，受限较多，因此亟需一种更为健壮且易实现的噪声鲁棒规划方法。

Method: 作者提出一种可实现性强的新型路径规划器，采用基于占用栅格的空间表示，核心为改进的自行车运动学模型下的非线性优化规划。该方法通过对速度、路线和避障行为进行直接优化，引入显式的碰撞惩罚项，在缺失语义信息时依然可处理未知障碍与异构目标运动。此外，该方法可结合占用流预测模块，对动态障碍做出响应，整体流程实时（>10Hz），易于与标准车辆控制栈集成。

Result: 作者在强感知和定位噪声的仿真环境，以及真实F1TENTH平台上进行了测试。结果表明，该方法可稳定通过狭窄通道、复杂路况，路径平滑且安全，具有较强的鲁棒性和预测规划能力。

Conclusion: 该工作构建了一个无需手工启发式规则的、对噪声敏感度低的动态路径规划基础架构，证明了实际应用下的高可靠性和良好扩展性，为未来更智能、自动化的交通系统奠定了基础。

Abstract: Dynamic path planning must remain reliable in the presence of sensing noise, uncertain localization, and incomplete semantic perception. We propose a practical, implementation-friendly planner that operates on occupancy grids and optionally incorporates occupancy-flow predictions to generate ego-centric, kinematically feasible paths that safely navigate through static and dynamic obstacles. The core is a nonlinear program in the spatial domain built on a modified bicycle model with explicit feasibility and collision-avoidance penalties. The formulation naturally handles unknown obstacle classes and heterogeneous agent motion by operating purely in occupancy space. The pipeline runs in real-time (faster than 10 Hz on average), requires minimal tuning, and interfaces cleanly with standard control stacks. We validate our approach in simulation with severe localization and perception noises, and on an F1TENTH platform, demonstrating smooth and safe maneuvering through narrow passages and rough routes. The approach provides a robust foundation for noise-resilient, prediction-aware planning, eliminating the need for handcrafted heuristics. The project website can be accessed at https://honda-research-institute.github.io/onrap/

</details>


### [252] [TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment](https://arxiv.org/abs/2602.13579)
*Youngsun Wi,Jessica Yin,Elvis Xiang,Akash Sharma,Jitendra Malik,Mustafa Mukadam,Nima Fazeli,Tess Hellebrekers*

Main category: cs.RO

TL;DR: 该论文提出一种新的跨实体触觉对齐方法TactAlign，可将人类通过可穿戴设备收集的触觉信号有效转移到具备不同形态的机器人上，在无需配对数据和手动标注的情况下，实现更广泛、低成本的人-机器人策略迁移。


<details>
  <summary>Details</summary>
Motivation: 现有人-机器人策略迁移方法受限于传感器相同、数据配对和实体差异小，难以支持不同形态的机器人以及实际推广应用。如何让机器人理解并利用人类演示中的丰富触觉信号，成为提升机器人灵巧操作能力的关键挑战。

Method: 提出TactAlign方法：利用rectified flow技术，将来自人类和机器人不同传感器的触觉观测转化为共享潜在表征，并通过手-物交互衍生伪配对进行无监督对齐，摆脱对配对数据和特权信息的依赖。

Result: 实验表明，TactAlign可在多种高接触任务（如旋转、插入、盖盖）中提升人-机器人策略迁移的表现，对于未见过的物体和任务也有良好泛化，仅需不到5分钟的人类数据，还实现了零样本微调的灯泡旋入等高灵巧任务。

Conclusion: TactAlign拓展了人类触觉示范用于机器人学习的边界，突破了以往方法的形态锁定和数据配对限制，为实际机器人复杂操作的快速泛化和低成本迁移奠定了基础。

Abstract: Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).

</details>


### [253] [AgentRob: From Virtual Forum Agents to Hijacked Physical Robots](https://arxiv.org/abs/2602.13591)
*Wenrui Liu,Yaxuan Wang,Xun Zhang,Yanshu Wang,Jiashen Wei,Yifan Xiang,Yuhang Wang,Mingshen Ye,Elsie Dai,Zhiqi Liu,Yingjie Xu,Xinyang Chen,Hengzhe Sun,Jiyu Shen,Jingjing He,Tong Yang*

Main category: cs.RO

TL;DR: 论文提出了AgentRob框架，使LLM自主体能够通过社区论坛与物理机器人协作，实现用自然语言在论坛操控真实机器人并反馈结果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM自主体主要应用于虚拟环境，与真实物理世界的交互局限于直接控制接口，缺少社区、多代理、多物理体协同的新范式。

Method: 提出AgentRob，包括三层架构：论坛层（异步持久论坛交互）、代理层（监听@提及的命令）、机器人层（VLM与机器人硬件，解析命令并行动）。系统实现了在线论坛与物理机器人实时互动。

Result: 框架支持多个身份各异、自主体化的物理机器人在同一论坛共存互动，展示了论坛介导的多智能体物理机器人编队的可行性。

Conclusion: AgentRob推动了LLM自主体与物理世界深度协作的新模式，为大规模、多主体、异步交互的机器人智能体系统提供技术路径。

Abstract: Large Language Model (LLM)-powered autonomous agents have demonstrated significant capabilities in virtual environments, yet their integration with the physical world remains narrowly confined to direct control interfaces. We present AgentRob, a framework that bridges online community forums, LLM-powered agents, and physical robots through the Model Context Protocol (MCP). AgentRob enables a novel paradigm where autonomous agents participate in online forums--reading posts, extracting natural language commands, dispatching physical robot actions, and reporting results back to the community. The system comprises three layers: a Forum Layer providing asynchronous, persistent, multi-agent interaction; an Agent Layer with forum agents that poll for @mention-targeted commands; and a Robot Layer with VLM-driven controllers and Unitree Go2/G1 hardware that translate commands into robot primitives via iterative tool calling. The framework supports multiple concurrent agents with distinct identities and physical embodiments coexisting in the same forum, establishing the feasibility of forum-mediated multi-agent robot orchestration.

</details>


### [254] [Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation](https://arxiv.org/abs/2602.13640)
*Siyuan Li,Jiani Lu,Yu Song,Xianren Li,Bo An,Peng Liu*

Main category: cs.RO

TL;DR: 本文提出了一种分层模态融合框架，将音频、视觉和本体感知有机结合，提升了机器人操作在包含音频线索的复杂环境下的表现, 并显著优于以往主流工作。


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作主要依靠视觉和本体感知，在真实部分可观环境下难以准确推断接触状态，而音频信号在接触中自然蕴含丰富交互动态信息，但在多模态融合任务中未被充分挖掘。现有多模态融合方法对模态角色假定是对称的，不适用于稀疏、接触驱动的音频信号，因此需要更精细的融合机制。

Method: 提出分层模态融合框架，首先以音频信息作为条件，增强视觉和本体感知表征，随后建模高级别的跨模态相互作用，综合提取各模态互补信息。最终将融合后的特征输入扩散式策略网络，端到端生成机器人的连续动作。

Result: 在真实环境下的液体倒入、柜门开启等任务上进行实测，所提方法在利用音频信息时显著优于SOTA多模态融合方法，尤其在音频提供了视觉难以感知的关键线索时优势明显。此外，通过互信息分析进一步解释了音频在多模态融合中的具体作用。

Conclusion: 分层音频主导的多模态融合结构能高效融合音频、视觉和本体感知，在需要音频辅助决策的复杂操作场景下显著提升机器人性能，为多模态感知系统的设计提供了新思路。

Abstract: Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.

</details>


### [255] [SPLIT: Sparse Incremental Learning of Error Dynamics for Control-Oriented Modeling in Autonomous Vehicles](https://arxiv.org/abs/2602.13641)
*Yaoyu Li,Chaosheng Huang,Jun Li*

Main category: cs.RO

TL;DR: SPLIT框架通过稀疏增量学习，提升了混合高斯过程车辆模型的实时控制能力，兼顾了模型精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶控制对车辆模型的准确性、计算效率及自适应能力有较高要求。混合高斯过程方法尽管有效，但面临维度灾难、在线学习与推理效率低等瓶颈，限制了其实时应用能力。因此，亟需一种既高效又适应性好的控制导向车辆建模方法。

Method: 提出SPLIT框架，包含三大创新：1）模型分解，将车辆动力学模型拆分为不变元与可变元，通过实验校准与残差补偿降低维度；2）局部增量学习，将特征空间划分并实施高效的流式在线学习；3）GP稀疏化，引入贝叶斯委员会机制提升高斯过程的可扩展性。整体集成于模型预测控制器，实现实时、高效学习。

Result: 在激进仿真与实车实验中，SPLIT提升了模型的准确性与控制性能，实现了对车辆动力学变化的快速适应，并在全新场景下展现出强鲁棒性和泛化能力。

Conclusion: SPLIT为自动驾驶控制提供了高效、适应性强的车辆动力学建模解决方案，有效解决了传统GP模型维度高、计算慢、适应性弱的难题，推动了其实时部署应用。

Abstract: Accurate, computationally efficient, and adaptive vehicle models are essential for autonomous vehicle control. Hybrid models that combine a nominal model with a Gaussian Process (GP)-based residual model have emerged as a promising approach. However, the GP-based residual model suffers from the curse of dimensionality, high evaluation complexity, and the inefficiency of online learning, which impede the deployment in real-time vehicle controllers. To address these challenges, we propose SPLIT, a sparse incremental learning framework for control-oriented vehicle dynamics modeling. SPLIT integrates three key innovations: (i) Model Decomposition. We decompose the vehicle model into invariant elements calibrated by experiments, and variant elements compensated by the residual model to reduce feature dimensionality. (ii) Local Incremental Learning. We define the valid region in the feature space and partition it into subregions, enabling efficient online learning from streaming data. (iii) GP Sparsification. We use bayesian committee machine to ensure scalable online evaluation. Integrated into model-based controllers, SPLIT is evaluated in aggressive simulations and real-vehicle experiments. Results demonstrate that SPLIT improves model accuracy and control performance online. Moreover, it enables rapid adaptation to vehicle dynamics deviations and exhibits robust generalization to previously unseen scenarios.

</details>


### [256] [A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking](https://arxiv.org/abs/2602.13656)
*Zhongxiang Lei,Lulu Cao,Xuyang Wang,Tianyi Qian,Jinyan Liu,Xuesong Li*

Main category: cs.RO

TL;DR: 本论文提出了KungFuAthlete数据集，专注于高度动态的武术动作，并提出了一个能同时学习动态动作追踪与摔倒恢复的统一训练范式，以提升人形机器人在现实高动态环境下的鲁棒表现。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人动作追踪系统在高动态、高强度动作（如武术）下表现不佳，且缺乏针对极限场景的数据集。同时，以往的研究很少关注机器人从非安全状态（如摔倒）中自主恢复的能力。

Method: 1）通过专业运动员训练视频采集并构建了KungFuAthlete武术高动态数据集，具体区分地面动作和跳跃动作，并与常用数据集做了运动强度和复杂性的对比；2）提出了一种新型训练范式，使单一策略能联合学习高动态动作追踪与摔倒恢复，打破只追踪且假定始终处于安全状态的传统假设。

Result: KungFuAthlete数据集在动作速度、线速度和角速度上显著高于常用数据集，验证了其复杂性和极度动态特征。所提联合学习策略能让人形机器人在面对高动态、高风险动作时，不仅执行敏捷，还能实现自主恢复。

Conclusion: 本研究通过数据集与算法双重创新，推进人形机器人在高动态、复杂现实环境中的稳定性、鲁棒性与自主能力，实现动作执行与恢复的一体化，具有重要应用价值。

Abstract: Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.

</details>


### [257] [Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation](https://arxiv.org/abs/2602.13689)
*Wonju Lee,Matteo Grimaldi,Tao Yu*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人插入任务的视觉-触觉跨模态融合方法，大幅提升了任务成功率，接近理想状态。


<details>
  <summary>Details</summary>
Motivation: 单靠视觉无法解决插入等需精细接触的任务，简单融合视觉和触觉信息无法稳定提升表现，需要更有效的多模态融合方法。

Method: 提出跨模态Transformer（CMT）模型，利用自注意力和交叉注意力机制融合腕部摄像头的视觉信息与触觉传感信号。同时设计物理驱动的正则项，通过鼓励双侧力平衡稳定触觉特征嵌入。

Result: 在TacSL基准测试上，CMT加上对称正则化后插入成功率达96.59%，明显优于传统融合基线，接近理想的'视觉+力传感'配置（96.09%）。

Conclusion: 高精度插入任务需充分利用触觉信息，基于物理先验的多模态融合能最大发挥视觉与触觉的互补优势，现实感知下可接近理想表现。

Abstract: Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged "wrist + contact force" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.

</details>


### [258] [HybridFlow: A Two-Step Generative Policy for Robotic Manipulation](https://arxiv.org/abs/2602.13718)
*Zhenchen Dong,Jinna Fu,Jiaming Wu,Shengyuan Yu,Fulin Chen,Yide Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新的高效机器人操作生成方法HybridFlow，兼具速度与精度，实现了远超现有扩散方法的推理速度和较高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略受限于推理延迟，实时交互能力不足。虽然现有更快的生成方法（如flow matching）已部分替代传统扩散法，但速度与生成精度难以兼得，特别是MeanFlow等一阶方法在动作精度上未能满足机器人操作的高需求，因此急需一种既能高速推理又保证动作精度的新方法。

Method: 提出了HybridFlow，一种三阶段、两步生成（2-NFE）的方法，具体包含：1）用MeanFlow实现全局跳跃，2）重噪声（ReNoise）对分布进行对齐，3）用ReFlow模式进行局部精细化。该方法将MeanFlow的一步快速生成与精度优化结合，实现推理速度与动作质量的平衡。

Result: 在真实环境实验中，HybridFlow在成功率上超过16步扩散策略15-25%，推理延时从152ms降至19ms（加速8倍，约52Hz），在未见颜色的抓取任务上成功率达70.0%，在可变形物体折叠任务上成功率为66.3%。

Conclusion: HybridFlow是兼具低延时和高精度的新型生成方法，有潜力显著提升机器人操作策略在真实环境下的交互能力。

Abstract: Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \textbf{HybridFlow}, a \textbf{3-stage method} with \textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \textbf{15--25\%} in success rate while reducing inference time from 152ms to 19ms (\textbf{8$\times$ speedup}, \textbf{$\sim$52Hz}); it also achieves 70.0\% success on unseen-color OOD grasping and 66.3\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.

</details>


### [259] [FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments](https://arxiv.org/abs/2602.13720)
*Chen Feng,Yang Xu,Shaojie Shen*

Main category: cs.RO

TL;DR: 本文提出FC-Vision系统，实现了无人机在扫描目标结构时对视线遮挡的动态感知和实时规避，显著提升了扫描质量。


<details>
  <summary>Details</summary>
Motivation: 现有无人机扫描方法主要关注避障与效率，但往往忽略目标结构遮挡对扫描视野的负面影响，导致重建质量下降。因此，解决飞行过程中因不明障碍物带来的视线遮挡问题，提升扫描效果，具有重要实际意义。

Method: 提出FC-Vision实时重规划框架，包含显式的密集表面可见性约束，通过两级分解实时调整扫描路径：首先修复受遮挡的视点以保持覆盖范围，并最大限度减少对原扫描意图的偏离；然后在5自由度空间内分段连接新的无遮挡视点。此外还设计了插件式集成策略，可无缝嵌入现有无人机扫描系统，无需更改原有架构。

Result: 仿真与实地实验显示，FC-Vision在突发遮挡情况下显著提升了扫描质量，实现最高55.32%的覆盖提升和73.17%的遮挡比例下降，且仅带来适度的飞行时间增加，满足实时性能要求。

Conclusion: FC-Vision有效解决了无人机扫描中目标被遮挡的问题，可广泛应用于现有系统中，提升扫描结果的完整性与鲁棒性。源码将公开，便于推广应用。

Abstract: Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.

</details>


### [260] [Improving Driver Satisfaction with a Driving Function Learning from Implicit Human Feedback -- a Test Group Study](https://arxiv.org/abs/2602.13733)
*Robin Schwager,Andrea Anastasio,Simon Hartmann,Andreas Ronellenfitsch,Michael Grimm,Tim Brühl,Tin Stribor Sohn,Tim Dieter Eberhardt,Sören Hohmann*

Main category: cs.RO

TL;DR: 本文提出了一种针对高级驾驶辅助系统中纵向行驶功能的个性化算法，通过采集驾驶员主动接管行为反馈，持续优化系统的速度曲线，实现个性化驾驶体验。实验显示该方法能有效提升驾驶员满意度并减少干预频次。


<details>
  <summary>Details</summary>
Motivation: 当前高级驾驶辅助系统经常不符合驾驶员个人偏好，导致驾驶员频繁干预。如何利用驾驶员主动接管时的反馈实现辅助系统的个性化，有助于提升驾驶体验并减少手动干预。

Method: 提出一种算法，结合驾驶辅助系统原有的预测速度曲线和驾驶员主动演示的驾驶行为，迭代调整速度曲线，实现个性化。通过驾驶模拟器实验，对43名参与者采集数据，评估算法有效性。

Result: 实验结果显示，使用个性化纵向驾驶功能后，驾驶员满意度显著提高，干预频率显著降低。参与者反馈还帮助发现了系统进一步优化的方向。

Conclusion: 基于驾驶员接管行为个性化调整纵向驾驶功能能改善驾驶体验，降低干预频率，未来可进一步根据用户反馈优化系统。

Abstract: During the use of advanced driver assistance systems, drivers frequently intervene into the active driving function and adjust the system's behavior to their personal wishes. These active driver-initiated takeovers contain feedback about deviations in the driving function's behavior from the drivers' personal preferences. This feedback should be utilized to optimize and personalize the driving function's behavior. In this work, the adjustment of the speed profile of a Predictive Longitudinal Driving Function (PLDF) on a pre-defined route is highlighted. An algorithm is introduced which iteratively adjusts the PLDF's speed profile by taking into account both the original speed profile of the PLDF and the driver demonstration. This approach allows for personalization in a traded control scenario during active use of the PLDF. The applicability of the proposed algorithm is tested in a driving simulator-based test group study with 43 participants. The study finds a significant increase in driver satisfaction and a significant reduction in the intervention frequency when using the proposed adaptive PLDF. Additionally, feedback by the participants was gathered to identify further optimization potentials of the proposed system.

</details>


### [261] [XIT: Exploration and Exploitation Informed Trees for Active Gas Distribution Mapping in Unknown Environments](https://arxiv.org/abs/2602.13739)
*Mal Fazliu,Matthew Coombes,Sen Wang,Cunjia Liu*

Main category: cs.RO

TL;DR: 作者提出了一种针对移动机器人有害气体分布建图（GDM）的自主路径规划方法XIT，显著提升了在未知环境中气体探测任务的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数气体分布建图移动机器人系统依赖人工遥控，限制了任务规模和响应速度。要实现自主GDM，需在环境未知、复杂阻挡的情况下，使机器人能高效探索、建图以及从稀疏气体检测数据推断气体分布。但是，传统方法在探索-利用之间难以平衡，难以兼顾全局效率与实时数据采集。

Method: 作者将主动GDM建模为下一最佳轨迹的信息路径规划问题，提出XIT（Exploration-Exploitation Informed Trees）采样方法。XIT通过从当前气体后验推导的UCB信息场批量采样，生成平衡探索与利用的候选轨迹，并在轨迹扩展时结合路径代价、气体浓度和不确定性。同时，提出Wavefront Gas Frontier Detection（WGFD）算法，通过检测未观测但邻近高气体浓度区域（gas frontier），增强对污染源附近的探索能力。

Result: 在高保真模拟和真实环境实验中，XIT方案在建图精度和效率上均优于对比方法，能更快、更准确地完成气体分布建图。

Conclusion: XIT有效解决了自主移动机器人在未知环境中气体分布建图时的探索-利用平衡问题，提高了任务质量和效率。该方法同样适用于其它需要在未知环境中信息采集的机器人任务。

Abstract: Mobile robotic gas distribution mapping (GDM) provides critical situational awareness during emergency responses to hazardous gas releases. However, most systems still rely on teleoperation, limiting scalability and response speed. Autonomous active GDM is challenging in unknown and cluttered environments, because the robot must simultaneously explore traversable space, map the environment, and infer the gas distribution belief from sparse chemical measurements. We address this by formulating active GDM as a next-best-trajectory informative path planning (IPP) problem and propose XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements en route. XIT draws batches of samples from an Upper Confidence Bound (UCB) information field derived from the current gas posterior and expands trees using a cost that trades off travel effort against gas concentration and uncertainty. To enable plume-aware exploration, we introduce the gas frontier concept, defined as unobserved regions adjacent to high gas concentrations, and propose the Wavefront Gas Frontier Detection (WGFD) algorithm for their identification. High-fidelity simulations and real-world experiments demonstrate the benefits of XIT in terms of GDM quality and efficiency. Although developed for active GDM, XIT is readily applicable to other robotic information-gathering tasks in unknown environments that face the exploration and exploitation trade-off.

</details>


### [262] [The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control](https://arxiv.org/abs/2602.13747)
*Evan Eames,Priyadarshini Kannan,Ronan Sangouard,Philipp Plank,Elvin Hajizada,Gintautas Palinauskas,Lana Amaya,Michael Neumeier,Sai Thejeshwar Sharma,Marcella Toth,Prottush Sarkar,Axel von Arnim*

Main category: cs.RO

TL;DR: 本论文提出了一种完全在神经形态硬件（如Intel Loihi 2）上运行的视觉机器人控制流程，利用脉冲神经态机实现多个复杂网络的硬件内编排，突破了以往需依赖芯片外逻辑管理的限制，在能耗和延迟上达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前神经形态硬件在机器人领域有低能耗、低延迟等优势，但在处理需要多网络协调的复杂任务时，普遍需依赖芯片外的管理逻辑，制约了其应用扩展。该论文旨在突破多网络在神经形态硬件上内部自主管理的瓶颈，提升神经形态机器人系统的独立性和实用性。

Method: 提出并实现了基于脉冲神经态机的进程编排方法，将多个复杂神经网络完全调度在Loihi 2芯片内部无须外部管理。设计了完整的视觉控制流程，并分别在仿真和真实机器人臂平台进行了验证。

Result: 验证表明：所提流程的所有模块能在milli Watt（毫瓦级）低功耗下并行运行，具有与最新技术可比的低延迟性能。在仿真中完成了机器臂插拔任务的演示，关键流程也在实际机器人臂上成功测试。

Conclusion: 本研究首次实现了仅依靠神经形态硬件内部调度并执行复杂多网络视觉控制流程，显著拓宽了神经形态硬件在高复杂度机器人任务中的应用前景。

Abstract: It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.

</details>


### [263] [Impact-Robust Posture Optimization for Aerial Manipulation](https://arxiv.org/abs/2602.13762)
*Amr Afifi,Ahmad Gazar,Javier Alonso-Mora,Paolo Robuffo Giordano,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种优化具有运动学冗余的力矩控制机器人在碰撞时姿态的方法，以提升其抗冲击鲁棒性。通过最小化碰撞前后速度变动的度量值，显著减小机器人状态和输入命令的突变，提升安全性。本方法在物理仿真和多种机器人平台上均取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有力矩控制机器人在执行涉及碰撞的任务时，状态与控制量在冲击后容易产生突变，影响机器人的安全性与鲁棒性。特别是在具有运动学冗余的机器人中，如何利用其冗余自由度优化姿态以增强抗冲击能力，是一项亟待解决的挑战。

Method: 作者提出基于刚性碰撞模型的姿态优化方法，定义了一个与配置相关的指标，用于量化碰撞前后速度的变化幅度。通过最小化该指标，寻找抗冲击更优的机器人姿态。为实时执行，将原先难以实时求解的min-max优化问题，转化为基于梯度的运动任务，并将其嵌入到任务空间逆动力学（TSID）全身控制器框架下，与其他控制目标无缝结合。

Result: 方法在仿真中应用于有运动学冗余的空中操作机器人，进行重复点接触任务。与标准TSID方法相比，提出方法最高能减少51%的碰撞后姿态突变，并成功避免了执行器饱和。还在四足机器人和人形机器人上进行了数值仿真，同样实现了最高45%的突变减少。

Conclusion: 本文方法能有效利用运动学冗余，通过实时姿态优化提升力矩控制机器人在冲击事件下的鲁棒性和安全性。验证了在多种机器人平台上的通用性与高效性。

Abstract: We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.

</details>


### [264] [MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.13764)
*Heng Zhi,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: 提出了MOTIF方法，实现了机器人视觉-语言-动作模型在多种机体间的高效少样本迁移，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在不同机器人体型结构间迁移能力差，且依赖于高成本的数据采集，急需一种更高效的跨机体迁移方法。

Method: 提出MOTIF方法，通过向量量化提取和对齐身体无关的时空动作模式（motif），结合对策约束保证跨身体一致性，并设计了轻量的预测器将这些motif与机器人的特定状态结合用于动作生成。

Result: 在仿真和真实环境中，MOTIF方法在少样本迁移实验中分别比强基线高6.5%与43.7%。

Conclusion: MOTIF显著提升了机器人模型在不同结构下的迁移能力，为通用机器人学习提供了更高效的解决方案。

Abstract: While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.

</details>


### [265] [Ontological grounding for sound and natural robot explanations via large language models](https://arxiv.org/abs/2602.13800)
*Alberto Olivares-Alarcos,Muhammad Ahsan,Satrio Sanjaya,Hsien-I Lin,Guillem Alenyà*

Main category: cs.RO

TL;DR: 本文提出了一种结合本体推理与大语言模型（LLM）的混合框架，用于生成语义扎实且自然流畅的机器人解释，并在实验中显著提升了机器人解释的清晰度和简洁性。


<details>
  <summary>Details</summary>
Motivation: 有效的人机交互依赖于机器人能够做出符合逻辑并满足人类沟通期望的解释，但传统本体方法表达不够自然，LLM表达虽流畅但缺少语义扎根。该研究动机是融合两者优势，提高机器人解释的可理解性与透明度。

Method: 提出一种混合框架，将本体推理保证逻辑一致和语义扎根，LLM实现自然流畅的语言生成。具体方法包括静态对比叙事的本体算法与LLM融合，以人机协作实验数据为基础，机器人能判断事件的常见性并生成自适应解释。

Result: 实验验证该方法在工业协作场景中的有效性，所生成的本体叙述在清晰度和简洁性上有显著提升，且语义表达依然精准。初步评估表现出系统能根据用户反馈自适应改进解释。

Conclusion: 本研究证明了本体与LLM集成在提升机器人解释透明度方面的潜力，为更可解释的人机协作奠定基础。

Abstract: Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.

</details>


### [266] [Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation](https://arxiv.org/abs/2602.13833)
*Kevin Yuchen Ma,Heng Zhang,Weisi Lin,Mike Zheng Shou,Yan Wu*

Main category: cs.RO

TL;DR: 本文提出了一种新的3D语义-接触融合表示方法（SCFields），并通过两阶段仿真到实际学习流程，实现了在多样工具上的广泛泛化能力，在复杂工具操作实验中显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在富接触工具操作中缺乏精细物理感知，而依赖触觉感知的方式又难以泛化至多种工具，因两者无法兼顾物理精度和广泛泛化。作者旨在解决高保真物理感知与广泛工具泛化的矛盾。

Method: 提出Semantic-Contact Fields（SCFields），结合视觉语义与稠密接触估计。采用两阶段仿真到实际学习流程：一是在大规模仿真数据上预训练通用接触物理，二是在少量实际数据上结合几何和力学启发式进行伪标签微调，实现与真实传感器特性的对齐。SCFields作为扩散策略的观测输入，提升操作表现。

Result: 在刮擦、蜡笔绘画、削皮等任务上，SCFields实现了类别层面的泛化能力，操作表现优于仅使用视觉或原始触觉基线的模型。

Conclusion: 本文提出的SCFields能有效融合语义与接触信息，通过两阶段仿真到实际迁移学习，实现跨工具物理泛化，在复杂工具操作任务中表现突出，有望推动机器人的泛化操作能力发展。

Abstract: Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors.
  To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.

</details>


### [267] [Push-Placement: A Hybrid Approach Integrating Prehensile and Non-Prehensile Manipulation for Object Rearrangement](https://arxiv.org/abs/2602.13849)
*Majid Sadeghinejad,Arman Barghi,Hamed Hosseini,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 本论文提出了一种新的混合操作方式——推放（push-placement），结合了夹持与推移动作，用于桌面物品重排任务，有效减少了操作臂运动成本。


<details>
  <summary>Details</summary>
Motivation: 桌面物品重排任务面临由于碰撞和目标位置被阻挡而需临时缓存的问题。传统的夹持搬运方式需要更多动作，非夹持推移虽然高效但动作不精确，因此亟需一种更高效、精细的操作方法。

Method: 提出一种新型混合操作——推放（push-placement），在放置物体时用被夹持的物体推开障碍物，减少显式缓冲的需要。该原语集成进物理反馈的蒙特卡洛树搜索（MCTS）规划器，在PyBullet仿真环境中进行评测。

Result: 实验结果显示，相较于传统MCTS规划器，推放方法可将操作臂运动成本降低11.12%，较动态堆叠方法降低8.56%。

Conclusion: 混合夹持/非夹持操作原语能显著提升长时序重排任务的效率，为高效桌面重排提供了新方向。

Abstract: Efficient tabletop rearrangement remains challenging due to collisions and the need for temporary buffering when target poses are obstructed. Prehensile pick-and-place provides precise control but often requires extra moves, whereas non-prehensile pushing can be more efficient but suffers from complex, imprecise dynamics. This paper proposes push-placement, a hybrid action primitive that uses the grasped object to displace obstructing items while being placed, thereby reducing explicit buffering. The method is integrated into a physics-in-the-loop Monte Carlo Tree Search (MCTS) planner and evaluated in the PyBullet simulator. Empirical results show push-placement reduces the manipulator travel cost by up to 11.12% versus a baseline MCTS planner and 8.56% versus dynamic stacking. These findings indicate that hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.

</details>


### [268] [Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement](https://arxiv.org/abs/2602.13850)
*Minku Kim,Kuan-Chia Chen,Aayam Shrestha,Li Fuxin,Stefan Lee,Alan Fern*

Main category: cs.RO

TL;DR: 本论文提出了一种基于技能的架构，通过在任务层级串联可复用技能，实现类人机器人对箱子的长时间、连续重排。核心特色是所有技能共用一个通用的全身控制器（WBC），并针对长时序的鲁棒性问题，利用域随机化下的闭环技能执行轨迹对控制器进行聚合训练。作者提出了Humanoid Hanoi基准，验证了方法在仿真和真实类人机器人上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人完成复杂长时任务（如多箱组合移动）时，常要求针对每个动作或技能设计独立的低层控制器，导致系统复用性差、技能组合困难，且长过程中易累积误差降低鲁棒性。

Method: 作者提出所有技能都调用同一个通用、任务无关的全身控制器（WBC），通过汇总不同技能和组合在域随机化下的执行过程数据，对WBC进行聚合训练，以改善长时技能序列的表现。

Result: 在新提出的Humanoid Hanoi（塔汉诺伊盒子重排）测试中，方法在仿真和Digit V3类人机器人上都实现了全自动长时箱子重排，系统在鲁棒性和性能上均优于每个技能单独控制的传统基线。

Conclusion: 共享控制器结合数据聚合训练可大幅提升类人机器人在长时复杂任务中的自主能力和泛化性，为进一步实现多技能融合的自动化机器人铺路。

Abstract: We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.

</details>


### [269] [Modeling and Optimizing the Provisioning of Exhaustible Capabilities for Simultaneous Task Allocation and Scheduling](https://arxiv.org/abs/2602.13866)
*Jinwoo Park,Harish Ravichandar,Seth Hutchinson*

Main category: cs.RO

TL;DR: 本文提出了多机器人任务分配框架TRAITS，能在电池和时间约束下优化机器人团队任务分配，并较现有方法在可行性与效率上具备优势。


<details>
  <summary>Details</summary>
Motivation: 在异构多机器人团队长时间执行多个任务时，如何在电池和时间等约束下进行任务分配是一个计算难题，现有方法难以处理可消耗特质（traits）的精细分配。

Method: 提出了基于非线性规划的traits分配模块，用以优化机器人团队的trait供给速率，实现任务的可行且高效分配。同时结合电池消耗进行整体优化评估。

Result: 与两种主流方法对比，TRAITS在满足复杂的trait和电池约束方面表现更优，同时保持了计算可行性。

Conclusion: TRAITS能更准确地评估可行性和任务时间，有效优化异构多机器人长时间任务分配，在trait约束和资源管理上优于现有框架。

Abstract: Deploying heterogeneous robot teams to accomplish multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning. In this paper, we present a comprehensive, time-extended, offline heterogeneous multi-robot task allocation framework, TRAITS, which we believe to be the first that can cope with the provisioning of exhaustible traits under battery and temporal constraints. Specifically, we introduce a nonlinear programming-based trait distribution module that can optimize the trait-provisioning rate of coalitions to yield feasible and time-efficient solutions. TRAITS provides a more accurate feasibility assessment and estimation of task execution times and makespan by leveraging trait-provisioning rates while optimizing battery consumption -- an advantage that state-of-the-art frameworks lack. We evaluate TRAITS against two state-of-the-art frameworks, with results demonstrating its advantage in satisfying complex trait and battery requirements while remaining computationally tractable.

</details>


### [270] [UAV-SEAD: State Estimation Anomaly Dataset for UAVs](https://arxiv.org/abs/2602.13900)
*Aykut Kabaoglu,Sanem Sariel*

Main category: cs.RO

TL;DR: 该论文建立了一个大规模、真实世界的无人机状态估计异常数据集，为异常检测研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 无人机在执行任务过程中，状态估计异常会导致飞行行为偏离预期，影响安全与任务完成。因此，急需高质量的真实异常数据集促进相关检测算法发展，而现有数据集多为模拟或注入异常，缺乏现实代表性。

Method: 论文收集了1396个真实飞行日志（总计超52小时）。数据来源于多种传感器及多样的PX4无人机，涵盖正常和异常飞行，无合成或人为注入。根据异常类型，提出了机械电气类、外部位置类、全球位置类与高度类四个异常分类方法，细分多传感器数据内的异常。

Result: 得到了大规模、多样性高、真实世界的无人机飞行数据集，系统标注了飞行中出现的多类状态估计异常；数据涵盖室内外多场景、多种配置。

Conclusion: 该数据集突破了仅靠模拟/注入异常的局限，将极大推动无人机异常检测方法的开发与测试，提升实际环境下的无人机可靠性研究。

Abstract: Accurate state estimation in Unmanned Aerial Vehicles (UAVs) is crucial for ensuring reliable and safe operation, as anomalies occurring during mission execution may induce discrepancies between expected and observed system behaviors, thereby compromising mission success or posing potential safety hazards. It is essential to continuously monitor and detect such conditions in order to ensure a timely response and maintain system reliability. In this work, we focus on UAV state estimation anomalies and provide a large-scale real-world UAV dataset to facilitate research aimed at improving the development of anomaly detection. Unlike existing datasets that primarily rely on injected faults into simulated data, this dataset comprises 1396 real flight logs totaling over 52 hours of flight time, collected across diverse indoor and outdoor environments using a collection of PX4-based UAVs equipped with a variety of sensor configurations. The dataset comprises both normal and anomalous flights without synthetic manipulation, making it uniquely suitable for realistic anomaly detection tasks. A structured classification is proposed that categorizes UAV state estimation anomalies into four classes: mechanical and electrical, external position, global position, and altitude anomalies. These classifications reflect collective, contextual, and outlier anomalies observed in multivariate sensor data streams, including IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, and optical flow, that can be found in the PX4 logging mechanism. It is anticipated that this dataset will play a key role in the development, training, and evaluation of anomaly detection and isolation systems to address the critical gap in UAV reliability research.

</details>


### [271] [High-fidelity 3D reconstruction for planetary exploration](https://arxiv.org/abs/2602.13909)
*Alfonso Martínez-Petersen,Levin Gerdes,David Rodríguez-Martínez,C. J. Pérez-del-Pulgar*

Main category: cs.RO

TL;DR: 本文提出了一种结合NeRF和Gaussian Splatting的辐射场方法，用于行星机器人环境三维重建，直接处理车载原始视觉数据，在无GPS和通信延迟下获得高质量重建。


<details>
  <summary>Details</summary>
Motivation: 行星探测中，机器人需在极端环境和有限感知条件下自主导航，传统SfM/SLAM在几何一致性方面表现不错，但难以捕捉辐射细节且在非结构化地形下扩展性差。因此，需要新的方法提高视觉重建精度和实用性。

Method: 将Neural Radiance Fields（NeRF）和Gaussian Splatting等辐射场方法，与Nerfstudio和COLMAP框架集成，通过兼容ROS2工作流直接处理rover的rosbag数据，实现三维环境自动重建。

Result: 该系统从最少的视觉输入中生成高密度、逼真且计量一致的三维重建结果，显著提升了类似行星环境中的感知与规划能力。

Conclusion: 该管道为基于辐射场的行星表面映射开创了基础，将几何和神经表征有效结合，为未来行星探索机器人环境感知提供了新途径。

Abstract: Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.

</details>


### [272] [Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)](https://arxiv.org/abs/2602.13932)
*Omer Daube,Oren Salzman*

Main category: cs.RO

TL;DR: 本文提出并研究了联合任务协助规划（Joint Task Assistance Planning, JTAP）问题，并提出了一种高效的分层搜索算法，实现在协作机器人系统中最大化协助时间。


<details>
  <summary>Details</summary>
Motivation: 现有机器人协作优化方法通常侧重于单一任务的执行，未充分考虑多机器人间基于空间关系的协助最大化需求。为了提升机器人协作效率，作者提出新的JTAP问题，综合考虑任务执行与协助保障。

Method: 论文将机器人路径规划建模为图结构，其中主任务机器人需按时完成使命，协助机器人根据空间位置提供支持。为解决路径组合与时间因素带来的计算复杂性，作者设计了嵌套分支界定（nested branch-and-bound）算法，分层高效搜索最优路径组合。

Result: 通过实验验证，所提算法在实际应用中较传统基线方法实现了至多百倍的速度提升，有效提升了机器人协作中的协助时长。

Conclusion: 该研究为多机器人协作与智能规划提供了新模型和高效算法，有望提升机器人系统在实际任务中的协同效率，对未来多智能体系统协作研究具有启发意义。

Abstract: We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.

</details>


### [273] [WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL](https://arxiv.org/abs/2602.13977)
*Zhennan Jiang,Shangqing Zhou,Yutong Jiang,Zefang Huang,Mingjie Wei,Yuhui Chen,Tianxing Zhou,Zhen Guo,Hao Lin,Quanlu Zhang,Yu Wang,Haoran Li,Chao Yu,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种增强世界模型稳定性的强化学习框架WoVR，在视觉-语言-动作（VLA）任务中显著提升了虚拟仿真和实际机器人操作的强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在VLA模型上的应用受限于真实环境交互需求大，难以在实体机器人上部署。基于世界模型的强化学习虽然能减少实体交互，但闭环仿真容易产生错误累积（幻觉），削弱策略优化效果。为解决这一关键问题，作者提出新的方法避免优化利用与世界模型幻觉相关的伪进展信号。

Method: WoVR框架包括：（1）可控动作条件化视频世界模型以提升仿真稳定性；（2）关键帧初始化回滚（Keyframe-Initialized Rollouts）减少长时序错误影响；（3）世界模型与策略并行共同演化（共同进化）；从结构上控制RL与不完美虚拟环境的交互。

Result: 在LIBERO基准和真实机器人操作实验中，WoVR策略优化表现显著优于现有方法。LIBERO平均成功率由39.95%提升到69.2%，实际机器人任务由61.7%提升到91.7%。

Conclusion: 只要合理控制幻觉，学习到的世界模型可成为强化学习的有效模拟器，为实际机器人领域的VLA任务大规模应用带来可行性。

Abstract: Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.

</details>


### [274] [It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses](https://arxiv.org/abs/2602.13999)
*Haozheng Xu,Wenhao Li,Zifan Wei,Bo Jin,Hongxing Bai,Ben Yang,Xiangfeng Wang*

Main category: cs.RO

TL;DR: 本文指出现有的仓库机器人系统将订单调度和多智能体路径规划视为独立子问题，忽视了两者间的重要联系。为解决这一瓶颈，作者提出了WareRover仿真平台，实现高层调度和低层路径规划的紧密耦合，并加入更加真实的约束和异常，成为了更具挑战性的评测测试平台。


<details>
  <summary>Details</summary>
Motivation: 当前主流方法将订单调度与路径规划分开考虑，导致高层任务分配和低层交通拥堵间的关键依赖被掩盖，进而影响系统协调优化。缺乏能反映实际复杂性的仿真环境，限制了更先进算法的发展评价。

Method: 提出WareRover平台，将订单调度和多智能体路径规划在一个闭环优化接口内紧密结合。平台支持动态订单流、具物理特性的运动约束和异常恢复机制，通过统一仿真循环评估算法在现实场景下的表现。

Result: 实验显示，当前最优（SOTA）算法在WareRover提出的真实耦合条件下表现欠佳，揭示了已有方法在面对现实复杂性时的不足。WareRover能够揭示并考验算法的鲁棒性。

Conclusion: WareRover平台填补了仓库机器人领域仿真与实际应用需求之间的空白，为开发和评测更强健的协调算法提供了重要工具，对推动仓库自动化系统的进步具有积极意义。

Abstract: The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.

</details>


### [275] [RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation](https://arxiv.org/abs/2602.14032)
*Xinhua Wang,Kun Wu,Zhen Zhao,Hu Cao,Yinuo Zhao,Zhiyuan Xu,Meng Li,Shichao Fan,Di Wu,Yixue Zhang,Ning Liu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: RoboAug提出了一种新的生成式数据增强框架，仅需单图像标注，显著提升机器人在多样未知场景下的泛化能力，性能大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实世界中常需面对未知和多变的环境，但现有方法对大规模预训练数据或完美目标识别依赖过强，难以实际应用。

Method: RoboAug利用单张图片的边界框标注，结合预训练生成模型进行准确的语义数据增强，并引入区域对比损失，引导模型关注与任务相关的区域，无需大规模数据即可提升泛化。

Result: 在三种不同的机器人（UR-5e、AgileX与天工2.0）和超35,000次真实测试中，RoboAug在未知场景下的任务成功率相比未增强和主流增强方法有大幅提升，成功率最高提升近0.5。

Conclusion: RoboAug极大缓解了对大数据和完美识别的需求，提高了机器人在真实操控任务中的泛化与有效性，优于现有数据增强技术。

Abstract: Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.

</details>


### [276] [ProAct: A Dual-System Framework for Proactive Embodied Social Agents](https://arxiv.org/abs/2602.14048)
*Zeyi Zhang,Zixi Kang,Ruijie Zhao,Yusen Feng,Biao Jiang,Libin Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为ProAct的双系统框架，实现了具身社交机器人能够在响应当前输入的同时，具备前瞻性社会行为。系统有效提升了机器人社交互动的主动性与自然性。


<details>
  <summary>Details</summary>
Motivation: 现有具身社交机器人虽然能同步语音和姿态，但大多数系统只具备被动响应能力，缺乏对累计上下文与意图推理的主动性社交行为，且高延迟的深度推理与实时互动的低延迟要求存在矛盾。

Method: 提出ProAct双系统框架：低延迟行为系统负责实时多模态交互，高延迟认知系统负责长时段社会推理与生成高层主动意图。使用基于ControlNet的流式行为生成模型实现非语言行为的连续性，并支持异步意图植入，兼顾反应和主动动作的融合。

Result: 在物理人形机器人上部署该系统，并通过用户研究对动作质量和交互效果进行评估。结果表明，在主动性、社交临场感和用户参与度评价上，ProAct均优于传统被动反应式系统。

Conclusion: ProAct双系统方法在兼顾低延迟响应和高层推理主动行为的同时，提升了具身社交机器人的自然互动体验，验证了主动控制架构的有效性。

Abstract: Embodied social agents have recently advanced in generating synchronized speech and gestures. However, most interactive systems remain fundamentally reactive, responding only to current sensory inputs within a short temporal window. Proactive social behavior, in contrast, requires deliberation over accumulated context and intent inference, which conflicts with the strict latency budget of real-time interaction. We present \emph{ProAct}, a dual-system framework that reconciles this time-scale conflict by decoupling a low-latency \emph{Behavioral System} for streaming multimodal interaction from a slower \emph{Cognitive System} which performs long-horizon social reasoning and produces high-level proactive intentions. To translate deliberative intentions into continuous non-verbal behaviors without disrupting fluency, we introduce a streaming flow-matching model conditioned on intentions via ControlNet. This mechanism supports asynchronous intention injection, enabling seamless transitions between reactive and proactive gestures within a single motion stream. We deploy ProAct on a physical humanoid robot and evaluate both motion quality and interactive effectiveness. In real-world interaction user studies, participants and observers consistently prefer ProAct over reactive variants in perceived proactivity, social presence, and overall engagement, demonstrating the benefits of dual-system proactive control for embodied social interaction.

</details>


### [277] [SemanticFeels: Semantic Labeling during In-Hand Manipulation](https://arxiv.org/abs/2602.14099)
*Anas Al Shikh Khalil,Haozhi Qi,Roberto Calandra*

Main category: cs.RO

TL;DR: 本文提出了SemanticFeels系统，使机器人在操作物体时能同时感知物体的几何形状与材料属性，并实验证明了其在多材料识别上的高准确率（平均79.87%）。


<details>
  <summary>Details</summary>
Motivation: 随着机器人应用的普及，机器人在手内操作过程中同时感知物体形状和材料属性变得非常关键，能够提升其自适应性与智能化水平。以往多侧重于形状或材料其中之一，缺乏统一高效的多维感知方法。

Method: 作者将语义标签集成到神经隐式形状表示（NeuralFeels）中，通过结合视觉与触觉信息，实现对物体几何形状与连续材料区域的联合预测。具体做法是，使用微调后的EfficientNet-B0 CNN处理Digit高分辨率触觉数据，预测局部材料分布，并将其嵌入到带有材料属性的签名距离场（SDF）网络中，获得更丰富的物体表征。

Result: 实验结果显示，该系统在单一或多材料物体上均能取得较高的材料预测与真实标签的一致性。在多次多材料物体操作实验中，平均匹配准确率达到79.87%。

Conclusion: SemanticFeels系统可让机器人更准确地结合视觉和触觉，识别和理解复杂物体的材料与形状，并在机器人抓取与操作任务中具有良好的应用前景。

Abstract: As robots become increasingly integrated into everyday tasks, their ability to perceive both the shape and properties of objects during in-hand manipulation becomes critical for adaptive and intelligent behavior. We present SemanticFeels, an extension of the NeuralFeels framework that integrates semantic labeling with neural implicit shape representation, from vision and touch. To illustrate its application, we focus on material classification: high-resolution Digit tactile readings are processed by a fine-tuned EfficientNet-B0 convolutional neural network (CNN) to generate local material predictions, which are then embedded into an augmented signed distance field (SDF) network that jointly predicts geometry and continuous material regions. Experimental results show that the system achieves a high correspondence between predicted and actual materials on both single- and multi-material objects, with an average matching accuracy of 79.87% across multiple manipulation trials on a multi-material object.

</details>


### [278] [Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects](https://arxiv.org/abs/2602.14104)
*Xinan Rong,Changhuang Wan,Aochen He,Xiaolong Li,Gangshan Jing*

Main category: cs.RO

TL;DR: 本文提出一种新的多指协调控制框架，实现了无需触觉反馈下对易损物体的高精度操作，突破了商业灵巧手缺乏力反馈传感器与依赖指尖接触的操作局限。


<details>
  <summary>Details</summary>
Motivation: 多指灵巧手因缺乏钳持力和准确的接触力反馈，难以高精度操作易损物体，且当前多数商业产品缺少校准的力/力矩传感器，限制了其实用性。

Method: 设计了一个双层控制框架：上层通过结合图刚性与力闭合约束进行协调力规划，下层采用力-位置映射将力规划轨迹转为各关节运动轨迹，从而在无触觉反馈情况下实现高精度控制。

Result: 在定制的灵巧手实体平台上，通过实际操作展示了对毛线、塑料杯、生鸡蛋等易损物体的稳健高精度抓取与操控，验证了方法的有效性与安全性。

Conclusion: 该方法为商业灵巧手无触觉/力反馈条件下实现复杂抓取与高精度操作提供了新思路，有效提升了此类手在现实复杂应用中的能力。

Abstract: Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.

</details>


### [279] [Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation](https://arxiv.org/abs/2602.14174)
*Yifei Yang,Anzhe Chen,Zhenjie Zhu,Kechun Xu,Yunxuan Mao,Yufei Wei,Lu Chen,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的仿真到现实（sim-to-real）迁移方法，用于处理涉及复杂接触的机器人操作任务。方法借助专家设计的有限状态机控制器，为策略学习提供高质量引导，并突出利用接触力方向而非幅值，显著提升了现实中的任务成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人在接触丰富的操作任务中，仿真与现实环境下的接触动力学存在较大差异，导致训练出的控制策略难以直接应用。现有方法依赖昂贵的实际数据，或采用不能适应环境变化的固定控制器。本研究旨在克服这些困难，提升sim-to-real迁移的效率和效果。

Method: 作者提出使用人类专家设计的有限状态机（FSM）位置/力控制器，在仿真中为深度策略提供“特权”监督。训练阶段，策略预测末端执行器位姿、接触状态和接触力方向（而不是易受仿真误差影响的力幅值）。部署时，策略结合人工设定的固定力幅值，驱动具有自适应性的力-感知顺应控制器，实现高效迁移。

Result: 在微波炉门开启、插孔、擦白板和门把手开启四个真实任务中，该方法在成功率和鲁棒性方面均显著优于现有强基线方法，并通过理论分析验证方法的稳定性和扰动鲁棒性。

Conclusion: 通过结合专家任务逻辑与接触力方向预测，本文方法实现了高效、低人工调参成本的策略迁移，突破了以往仿真到现实迁移在复杂接触任务中的主要痛点，具有良好的泛化和实际应用潜力。

Abstract: Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy's directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.

</details>


### [280] [Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation](https://arxiv.org/abs/2602.14193)
*Yue Chen,Muqing Jiang,Kaifeng Zheng,Jiaqi Liang,Chenrui Tie,Haoran Lu,Ruihai Wu,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的三维特征表示方法PA3FF，用于提升机器人对多样关节物体的操控泛化能力，并结合模仿学习算法PADP在多种任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 关节类物体的操控在实际机器人任务中十分关键，但由于物体类别和形状多样，传统方法泛化能力有限。通过关注物体功能部件（如把手、旋钮）可增强泛化，但现有特征多基于2D，未能有效捕捉3D几何信息且存在效率低、空间分辨率低和几何信息不足等问题。

Method: 作者提出PA3FF（一种基于部分感知的密集三维特征场），通过大规模带标签的3D数据及对比学习训练，可在单次前馈下根据点云预测连续三维特征场，特征距离能反映功能部分的接近性。基于此特征，提出了PADP模仿学习框架以提升样本效率和泛化。

Result: 实验表明，所提PA3FF特征在模拟和现实世界任务中对操控表现优于各种2D和3D特征（如CLIP、DINOv2、Grounded-SAM）。此外，PA3FF也适用于姿态对应、分割等下游任务，展示了其通用性。

Conclusion: PA3FF为机器人操控任务提供了泛化性强、空间感知能力出色的三维特征基础，且与PADP联合应用可推动多任务表现，具有广阔的实际应用前景。

Abstract: Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io

</details>


### [281] [Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors](https://arxiv.org/abs/2602.14222)
*Antonio Franchi*

Main category: cs.RO

TL;DR: 本论文提出了一种基于几何结构的新方法，系统分析了多旋翼飞行器在能量经济性与运动响应性之间的权衡，揭示了硬件约束如何影响最优性能分配，并为飞行器设计和控制带来理论指导。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器在实际任务中常常面临能量消耗与运动敏捷性（或“运动准备度”）的矛盾。本研究旨在系统揭示和解析这种权衡机制，为以往基于经验或黑盒算法的任务执行和控制分配过程提供理论依据。

Method: 作者引入动力学“及时性（promptness）”的新度量标准，将能耗与响应性能视为多目标优化，通过几何纤维丛结构（fiber-bundle）将冗余分配问题转化为仿射纤维上的多目标规划，并利用微分同胚变换线性化了动力模型。此外，结合实例研究，探索了不同硬件配置下目标函数空间的性质。

Result: 通过在六旋翼四自由度分配的案例分析，发现当推进器仅能单向工作时，最优解在内部分布且面临较短的帕累托前沿；而可逆推进器的引入，则会导致系统可以通过对抗性马达协同动作提升响应性，但牺牲能耗最优性，两者不可兼得。这些最优解的分布受到硬件约束、目标函数和几何关系的共同影响。

Conclusion: 论文提出的基于几何的多目标优化框架，不仅揭示了能耗与敏捷性的根本矛盾，还为实现面向机动性的飞行器结构设计、认证标准制定及威胁感知飞行控制提供了理论基础，促进了控制分配从经验性向理论化的转变。

Abstract: In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.

</details>


### [282] [Path Planning Optimisation for SParse, AwaRe and Cooperative Networked Aerial Robot Teams (SpArC-NARTs): Optimisation Tool and Ground Sensing Coverage Use Cases](https://arxiv.org/abs/2602.14247)
*Maria Conceição,António Grilo,Meysam Basiri*

Main category: cs.RO

TL;DR: 本文提出了一种新的路径规划工具，用于提升多无人机系统在环境探索任务中的协作与效率，充分考虑通信约束、能量限制和环境先验信息的不确定性。


<details>
  <summary>Details</summary>
Motivation: 多无人机及地面站（NART）因通信受限与先验信息不全，在执行协作任务时存在决策和效率瓶颈。优化路径规划与决策过程，促进协作和信息共享，有助于提升系统任务效率和弹性。

Method: 作者设计了SpArC-NART路径规划工具，能够在任务规划时综合考虑：环境信息先验的不确定性、无人机能量限制、感知与通信能力限制、组网形式异质性。其通信模型结合用户自定义无线电技术限制和物理自然现象。核心合作机制在于对无人机运动进行柔性约束，并结合动态奖励，在每个时刻基于移动价值和预期通信可用性引导无人机协作。

Result: 通过地面感知覆盖案例验证，所提工具促进了无人机间协作，减少了信息汇报时间，提高了环境感知范围，并在需求变化时能更好地支持任务重规划。

Conclusion: SpArC-NART路径规划工具有效提升了多无人机在通信受限和能量约束下的协作能力和任务效率，适用于信息不全、环境复杂的探索类任务。

Abstract: A networked aerial robot team (NART) comprises a group of agents (e.g., unmanned aerial vehicles (UAVs), ground control stations, etc.) interconnected by wireless links. Inter-agent connectivity, even if intermittent (i.e. sparse), enables data exchanges between agents and supports cooperative behaviours in several NART missions. It can benefit online decentralised decision-making and group resilience, particularly when prior knowledge is inaccurate or incomplete. These requirements can be accounted for in the offline mission planning stages to incentivise cooperative behaviours and improve mission efficiency during the NART deployment. This paper proposes a novel path planning tool for a Sparse, Aware, and Cooperative Networked Aerial Robot Team (SpArC-NART) in exploration missions. It simultaneously considers different levels of prior information regarding the environment, limited agent energy, sensing, and communication, as well as distinct NART constitutions. The communication model takes into account the limitations of user-defined radio technology and physical phenomena. The proposed tool aims to maximise the mission goals (e.g., finding one or multiple targets, covering the full area of the environment, etc.), while cooperating with other agents to reduce agent reporting times, increase their global situational awareness (e.g., their knowledge of the environment), and facilitate mission replanning, if required. The developed cooperation mechanism leverages soft-motion constraints and dynamic rewards based on the Value of Movement and the expected communication availability between the agents at each time step. A ground sensing coverage use case was chosen to illustrate the current capabilities of this tool.

</details>


### [283] [A Latency-Aware Framework for Visuomotor Policy Learning on Industrial Robots](https://arxiv.org/abs/2602.14255)
*Daniel Ruan,Salma Mozaffari,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: 本文提出了一种针对工业机器人视觉-动作策略的延迟感知框架，有效提升了机器人在高延迟环境下的任务执行稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 工业机器人在复杂组装等接触密集型任务中面临显著的感觉—执行延迟，现有的视觉-动作学习策略难以直接在高延迟工业环境下部署，亟需专门针对延迟挑战的系统方法。

Method: 作者设计了一个包括多模态感知、时序同步、统一通信与遥操作采集的延迟感知执行框架，并提出基于时序可行性的有限时域动作预测调度，无需更改原策略结构即可实现异步推理和执行。

Result: 在工业装配任务中，实验证明该框架下的延迟感知执行策略（对比阻塞式和朴素异步基线）能在不同延迟下保持运动流畅性、合理接触与持续任务推进，同时减少空闲等待并避免不稳定现象。

Conclusion: 显式处理延迟对于工业机器人视觉-动作策略的可靠闭环部署至关重要，本文方法在不更改学习策略的前提下有效提升了系统鲁棒性，适用于实际工业场景。

Abstract: Industrial robots are increasingly deployed in contact-rich construction and manufacturing tasks that involve uncertainty and long-horizon execution. While learning-based visuomotor policies offer a promising alternative to open-loop control, their deployment on industrial platforms is challenged by a large observation-execution gap caused by sensing, inference, and control latency. This gap is significantly greater than on low-latency research robots due to high-level interfaces and slower closed-loop dynamics, making execution timing a critical system-level issue. This paper presents a latency-aware framework for deploying and evaluating visuomotor policies on industrial robotic arms under realistic timing constraints. The framework integrates calibrated multimodal sensing, temporally consistent synchronization, a unified communication pipeline, and a teleoperation interface for demonstration collection. Within this framework, we introduce a latency-aware execution strategy that schedules finite-horizon, policy-predicted action sequences based on temporal feasibility, enabling asynchronous inference and execution without modifying policy architectures or training. We evaluate the framework on a contact-rich industrial assembly task while systematically varying inference latency. Using identical policies and sensing pipelines, we compare latency-aware execution with blocking and naive asynchronous baselines. Results show that latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across a wide range of latencies while reducing idle time and avoiding instability observed in baseline methods. These findings highlight the importance of explicitly handling latency for reliable closed-loop deployment of visuomotor policies on industrial robots.

</details>


### [284] [Autonomous Robotic Tissue Palpation and Abnormalities Characterisation via Ergodic Exploration](https://arxiv.org/abs/2602.14287)
*Luca Beber,Edoardo Lamon,Matteo Saveriano,Daniele Fontanelli,Luigi Palopoli*

Main category: cs.RO

TL;DR: 本文提出了一种新型自主机器人触诊框架，利用实时粘弹性组织建模，提高组织弹性分布的探查效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前医学机器人触诊面临准确性和实时性不足，对病理区域探查效果有限，亟需改进勘探路径和参数估算方法以提升诊断价值。

Method: 该方法融合基于力/力矩传感器的参数估算、以信息期望密度驱动的遍历性控制策略，以及扩展卡尔曼滤波在线估算粘弹性模型参数。空间弹性分布通过高斯过程回归建模，并通过热方程驱动的覆盖控制实现自适应路径规划。

Result: 在合成刚度分布上的仿真显示该方法在刚度重建、区域分割和对硬包块的鲁棒检测方面均优于传统贝叶斯优化方法。

Conclusion: 通过在模拟病理组织的硅胶模型上实验证实，该方法具备成为自主组织特征检测和筛查应用的潜力。

Abstract: We propose a novel autonomous robotic palpation framework for real-time elastic mapping during tissue exploration using a viscoelastic tissue model. The method combines force-based parameter estimation using a commercial force/torque sensor with an ergodic control strategy driven by a tailored Expected Information Density, which explicitly biases exploration toward diagnostically relevant regions by jointly considering model uncertainty, stiffness magnitude, and spatial gradients. An Extended Kalman Filter is employed to estimate viscoelastic model parameters online, while Gaussian Process Regression provides spatial modelling of the estimated elasticity, and a Heat Equation Driven Area Coverage controller enables adaptive, continuous trajectory planning. Simulations on synthetic stiffness maps demonstrate that the proposed approach achieves better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimisation-based techniques. Experimental validation on a silicone phantom with embedded inclusions emulating pathological tissue regions further corroborates the potential of the method for autonomous tissue characterisation in diagnostic and screening applications.

</details>


### [285] [Exploiting Structure-from-Motion for Robust Vision-Based Map Matching for Aircraft Surface Movement](https://arxiv.org/abs/2602.14311)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 本文提出了一种视觉辅助导航（VAN）流程，通过结合间接法的高效性与直接图像方法的稳健性，提升自主飞机地面导航的完整性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自主飞机地面导航系统在定位精度和异常识别方面存在不足，尤其在利用地图匹配与图像数据时，漂移及匹配歧义影响了结果的可靠性。因此，需要一种更稳健且具备异常检测能力的导航方法。

Method: 该方法首先处理飞机滑行时采集的地面图像，采用特征点结构光束法（SfM）进行图像配准。然后，通过单应性变换构建地面平面马赛克，将其与卫星图像进行SSD（强度平方差）匹配。同时，算法集成异常检测与匹配模糊识别机制。

Result: 实验表明，在SfM中存在与惯性导航系统类似的持续漂移，会影响宽基线地图匹配的定位精度。但所提算法具备可靠的异常识别和匹配歧义检测能力，能有效识别地图注册异常和模糊匹配情况。

Conclusion: 该视觉辅助导航流程有助于抑制异常和离群现象，为实现能够认证的自主飞机地面移动导航提供了一种稳健的解决方案。

Abstract: In this paper we introduce a vision-aided navigation (VAN) pipeline designed to support ground navigation of autonomous aircraft. The proposed algorithm combines the computational efficiency of indirect methods with the robustness of direct image-based techniques to enhance solution integrity. The pipeline starts by processing ground images (e.g., acquired by a taxiing aircraft) and relates them via a feature-based structure-from-motion (SfM) solution. A ground plane mosaic is then constructed via homography transforms and matched to satellite imagery using a sum of squares differences (SSD) of intensities. Experimental results reveal that drift within the SfM solution, similar to that observed in dead-reckoning systems, challenges the expected accuracy benefits of map-matching with a wide-baseline ground-plane mosaic. However, the proposed algorithm demonstrates key integrity features, such as the ability to identify registration anomalies and ambiguous matches. These characteristics of the pipeline can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft.

</details>


### [286] [AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation](https://arxiv.org/abs/2602.14363)
*Morgan Byrd,Donghoon Baek,Kartik Garg,Hyunyoung Jung,Daesol Cho,Maks Sorokin,Robert Wright,Sehoon Ha*

Main category: cs.RO

TL;DR: 提出了一种新的人形机器人自主导航、搬运和送货的方法，融合了多项关键技术，并显著优于依赖人类演示的传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人联合导航与操作（loco-manipulation）方法常依赖人类演示，鲁棒性不足，难以应对实际环境中的干扰，亟需更自主、通用和抗干扰性强的方法。

Method: 提出AdaptManip框架，包含三个强化学习训练的部分：（1）递归对象状态估计器，用于在视野受限和遮挡下实时追踪目标；（2）结合全身运动和残差操作的基座控制策略，实现稳定搬运；（3）基于激光雷达的全球位置估计，实现抗漂移定位。所有组建均在仿真中训练，无需人类演示或远程操作，并直接在真实机器人上零样本部署。

Result: 实验证明AdaptManip在适应性和任务成功率上大幅优于基线方法（包括模仿学习方法）；尤其在遮挡情况下，准确的对象估计提升了操作表现。

Conclusion: AdaptManip显著提升了人形机器人在现实场景下的自主导航、搬运和送货能力，并展现了较强的泛化性和鲁棒性。

Abstract: This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.

</details>


### [287] [A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation](https://arxiv.org/abs/2602.14434)
*Steven Oh,Tomoya Takahashi,Cristian C. Beltran-Hernandez,Yuki Kuroda,Masashi Hamaya*

Main category: cs.RO

TL;DR: 本文提出了一种新型软腕机构CLAW，能够大幅提升机器人在复杂接触任务中的鲁棒性，并在多项实验中优于传统和其他软体夹持器。


<details>
  <summary>Details</summary>
Motivation: 现有软体末端执行器存在变形范围有限、缺乏各向异性刚度控制、或需要复杂驱动结构等问题，无法有效应对非结构化环境中的复杂接触挑战，因此亟需一种实用且性能优异的创新设计。

Method: 作者设计了CLAW软腕机构，核心由两根正交安装的板簧与旋转关节（配锁定装置）构成，不仅能6自由度大幅变形（横向40mm、纵向20mm），还能在三种模式下调节各向异性刚度，同时结构轻便、成本低廉。

Result: 利用模仿学习方法、在标准插销任务上测试CLAW装置，取得76%的成功率，明显优于Fin Ray夹爪（43%）和刚性夹爪（36%）。对于高精度和易损物体操作等多种复杂接触任务同样表现优异。

Conclusion: CLAW软腕不仅克服了传统软体夹持器的核心局限，还显著提高了复杂接触环境下的操作鲁棒性，为机器人学习和实际应用提供了更优解决方案。

Abstract: Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/

</details>


### [288] [RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems](https://arxiv.org/abs/2602.14438)
*Hamid Khabazi,Ali F. Meghdari,Alireza Taheri*

Main category: cs.RO

TL;DR: 本文提出了一种以LLM（大语言模型）和VLM（视觉语言模型）为基础的智能多智能体框架，专为机器人操作器问题自动分析与求解设计。框架支持文本和视觉输入，可实现正/逆运动学求解、速度与加速度计算、3D仿真及运动控制，并在多个基准测试中表现出很高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人领域对自动化分析与求解复杂操作任务的需求增长，传统方法在可扩展性和通用性方面有限；而LLM与VLM具备强大的理解及推理能力，但尚未与机器人问题高度集成。作者欲探索二者结合，在提升自动化和智能分析能力方面的潜力。

Method: 作者开发了一个框架，可接受文本或视觉输入，利用LLM和VLM自动处理机器人操作器（Manipulator）的正运动学、逆运动学、速度、加速度、仿真与控制任务。通过三类基准测试，分别用文本描述提取运动学、视觉输入推理、以及全流程综合任务，检验与不同基础大模型集成后的系统性能。

Result: 在文本输入基准测试中，集成GPT-4o的框架正运动学准确率达0.97，远高于单独原始模型的0.30；DeepSeek-V3.2、Claude-Sonnet-4.5等模型中，框架同样显著提升准确率。视觉输入测试中，与Gemini 2.5 Pro VLM结合，框架准确率达0.93，较原始模型提升约20%。第三类机器人综合任务测试中，框架准确率同样高达0.97。

Conclusion: 该多智能体框架能够实现机器人复杂任务的高度自动化与智能求解，整合LLM与VLM显著提升了问题分析、推理与执行的准确性，对机器人智能系统设计具有重要意义。

Abstract: This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.

</details>


### [289] [Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing](https://arxiv.org/abs/2602.14473)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: 该论文提出了一种两阶段端到端深度强化学习方法，使四足机器人能够自主攀爬U型楼梯，并验证了策略在不同楼梯类型间的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人在建筑施工中的应用广泛，但在不同室内楼梯结构上的自主攀爬依然是实现自动化建筑任务中的一大挑战。

Method: 采用两阶段端到端深度强化学习方法：先在仿真环境中针对金字塔形楼梯训练Unitree Go2机器人，再将学到的策略用于U型楼梯，并探索策略在不同楼梯类型间的迁移。

Result: （1）机器人在有停滞惩罚的情况下成功完成U型楼梯攀爬任务；（2）训练所得策略可迁移到直线、L型、螺旋等不同楼梯结构，并且其他模型训练的策略也可以迁移到U型楼梯。

Conclusion: 端到端深度强化学习方法能够显著提升四足机器人在各类楼梯上的自主攀爬能力，实现了不同楼梯形态间的有效策略迁移，为建筑施工机器人的实际应用提供了新思路。

Abstract: Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.

</details>


### [290] [TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations](https://arxiv.org/abs/2602.14526)
*Guy Freund,Tom Jurgenson,Matan Sudry,Erez Karpas*

Main category: cs.RO

TL;DR: 本文提出了一种无需人工示范的机器人打结新方法（TWISTED-RL），显著提升了各类复杂结的自动打结成功率和速度，被验证为机器人打结领域的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 机器人打结因涉及柔性物体与拓扑限制，是机器人领域的难题。现有无示范方案虽可拆解问题，但在一般性和效率上仍有限。

Method: 采用多步强化学习（RL）替代传统的单步逆模型，通过抽象拓扑动作条件化决策，无需基于目标状态训练，避免冗余无效数据采集，可处理更复杂结形。

Result: TWISTED-RL 能打出原来做不到的高复杂度结（如8字结、单结），成功率更高、规划速度更快，数据收集更高效。

Conclusion: TWISTED-RL 成为无需人工演示下，机器人自动打结任务的最新SOTA方法，在泛化能力和效率等方面显著优于前作。

Abstract: Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED's single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.

</details>


### [291] [Multimodal Covariance Steering in Belief Space with Active Probing and Influence for Autonomous Driving](https://arxiv.org/abs/2602.14540)
*Devodita Chakravarty,John Dolan,Yiwei Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶在复杂交通场景下安全交互的新方法，将预测推理、主动探索和风险评估结合起来，实现了更高效且安全的决策。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法通常将基于预测的规划和风险感知控制分开处理，这限制了在交互环境下对行为和推断耦合关系的建模，尤其在不确定场景下，单纯依赖预测可能导致不安全或过于保守的行为。

Method: 引入层次化的信念模型，通过粗粒度的意图和细粒度的运动模式结构化人类行为，并以贝叶斯推断进行多层次解释。提出主动探索策略，通过规划可揭示对方意图和引导其安全行为的动作，并结合条件风险价值（CVaR）对主动探索行为进行风险评估，保证操作在可接受风险范围内。

Result: 在模拟测试的道路并线和无信号路口场景中，所提方法相较现有方案表现出更高的交互成功率和更短的完成时间。

Conclusion: 将信念推断、主动探索与风险动态监控耦合的自动驾驶决策框架，有效提升了不确定场景下的安全性和效率，同时具备良好的解释性。

Abstract: Autonomous driving in complex traffic requires reasoning under uncertainty. Common approaches rely on prediction-based planning or risk-aware control, but these are typically treated in isolation, limiting their ability to capture the coupled nature of action and inference in interactive settings. This gap becomes especially critical in uncertain scenarios, where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior. Our central insight is that safe interaction requires not only estimating human behavior but also shaping it when ambiguity poses risks. To this end, we introduce a hierarchical belief model that structures human behavior across coarse discrete intents and fine motion modes, updated via Bayesian inference for interpretable multi-resolution reasoning. On top of this, we develop an active probing strategy that identifies when multimodal ambiguity in human predictions may compromise safety and plans disambiguating actions that both reveal intent and gently steer human decisions toward safer outcomes. Finally, a runtime risk-evaluation layer based on Conditional Value-at-Risk (CVaR) ensures that all probing actions remain within human risk tolerance during influence. Our simulations in lane-merging and unsignaled intersection scenarios demonstrate that our approach achieves higher success rates and shorter completion times compared to existing methods. These results highlight the benefit of coupling belief inference, probing, and risk monitoring, yielding a principled and interpretable framework for planning under uncertainty.

</details>


### [292] [Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction](https://arxiv.org/abs/2602.14551)
*Taichi Kato,Takuya Kiyokawa,Namiko Saito,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种增强型人机协作机器人系统，结合视觉-语言模型（VLM）和双重纠错机制，有效提升机器人理解与执行人类指令的能力，实验中显著提升了协作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 人类对机器人发出的指令常常存在歧义和信息不完整，导致机器人难以生成可行且协作的行为。虽然已有研究采用VLM模型辅助理解，但仍存在推理失真与无法预判物理执行失败的问题。为了解决这些实际挑战，作者提出引入纠错机制以增强HRC系统的可靠性和实用性。

Method: 该方法在VLM推理基础上，引入了双重纠错机制：一为内部纠错模型，在动作执行前核查推理结果的逻辑一致性与任务可行性；二为外部纠错模型，在动作执行后通过反馈发现并修正物理失败。整个框架通过仿真消融实验和真实机器人协作装配任务进行评估。

Result: 仿真和现实实验均表明，该方法较未采用纠错模型的基线显著提高了机器人协作任务的成功率。机器人在固定物体、准备工具等装配协作中，能够根据人类实时指令进行互动性重规划。

Conclusion: 引入双重纠错机制能有效解决VLM辅助的人机协作任务中的理解和执行问题，提升机器人实际协作能力，具备良好的实用前景。

Abstract: Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.

</details>


### [293] [Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills](https://arxiv.org/abs/2602.14561)
*Arik Laemmle,Balázs András Bálint,Philipp Tenbrock,Frank Naegele,David Traunecker,József Váncza,Marco F. Huber*

Main category: cs.RO

TL;DR: 该论文提出了一种基于仿真的自动化装配方法，用于电气端子在DIN导轨上的力控装配，通过深度强化学习和可参数化机器人技能，实现高效、灵活的小批量制造自动化。


<details>
  <summary>Details</summary>
Motivation: 传统的电气端子装配自动化受到程序编写复杂和产品多样性的限制。作者希望通过更灵活、自适应的方法减少人工编程量，提升小批量制造的自动化程度。

Method: 方法上，作者结合了深度强化学习（DRL）与参数化机器人技能，并在物理仿真环境下（通过分析模型和MuJoCo刚体模型建模装配过程）进行训练，采用SAC和TD3算法进行策略优化，并运用域随机化提升训练的泛化能力。

Result: 训练好的策略无需额外调优即可移植到UR10e机器人，实验证明无论在仿真还是现实环境下，在存在显著位置与姿态偏差的情况下仍能获得高达100%的装配成功率，对新型端子也具备较好泛化能力。

Conclusion: 结合仿真学习与模块化机器人技能能显著降低自动化编程成本，提高系统灵活性及泛化能力，该方案对小批量制造自动化具重要意义。未来将研究混合学习及自动化参数优化等。

Abstract: This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.

</details>


### [294] [Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments](https://arxiv.org/abs/2602.14666)
*Ruofeng Wei,Kai Chen,Yui Lun Ng,Yiyao Ma,Justin Di-Lang Ho,Hon Sing Tong,Xiaomei Wang,Jing Dai,Ka-Wai Kwok,Qi Dou*

Main category: cs.RO

TL;DR: 本文提出了一种用于腔内机器人手术的视觉感知平台，可通过单目内镜图像实现灵活手术器械的位置与姿态识别，并精确测量其与组织的距离。通过2D与3D学习算法和基于物理的仿真平台，提升了机器人的操作效率与环境理解能力，手术路径跟踪任务操作时间减少70%以上。


<details>
  <summary>Details</summary>
Motivation: 腔内手术因其微创性在早期消化道与泌尿道肿瘤治疗中具有巨大潜力，但传统手术受限于器械操作灵活性和学习曲线。连续体机器人柔性高、可精准操作，但对于其感知及控制仍面临技术难题。开发高效感知与控制平台对提高手术质量尤为关键。

Method: 提出了基于单目内镜图像的2D和3D感知算法，用于识别和估计柔性机器人器械的位置、姿态和与组织的距离。开发了物理真实感仿真器，模拟器械动态，生成高真实性腔内场景，用于控制测试和数据采集。使用连续体机器人原型进行了功能和系统性能评估。

Result: 系统性评估表明，提出的感知算法大幅提升了对柔性器械的控制效率，手术路径跟踪任务操作时间减少超过70%；并显著增强了对复杂手术场景的理解能力。

Conclusion: 本文所提出的视觉感知系统和仿真平台，有效提升了连续体机器人在腔内手术中的控制与理解能力，有望增强手术的精准性与稳健性，为微创手术的进一步发展提供了技术基础。

Abstract: Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.

</details>


### [295] [ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions](https://arxiv.org/abs/2602.14726)
*Kohio Deflesselle,Mélodie Daniel,Aly Magassouba,Miguel Aranda,Olivier Ly*

Main category: cs.RO

TL;DR: 提出了一种名为ManeuverNet的深度强化学习（DRL）框架，专为双阿克曼转向机器人设计，显著提升了机器人在农业场景下的运动能力并优化了操控效率。


<details>
  <summary>Details</summary>
Motivation: 目前常用的路径规划方法（如TEB）对参数依赖性强，需要频繁重新调参，实际部署难度大。同时，传统的端到端DRL方法由于奖励函数不适应于非完整约束，常常得不到理想策略，泛化性差。

Method: ManeuverNet结合Soft Actor-Critic算法与CrossQ方法，并专为姿态约束设计了四种奖励函数，完全不依赖专家数据或人工规则。其性能与多种主流DRL基线及TEB路径规划方法进行了系统对比。

Result: 与主流DRL基准相比，ManeuverNet在操控成功率、可操作性上提升超40%，在真实试验中轨迹效率提升达90%，同时显著减少TEB方案中的参数敏感问题。

Conclusion: ManeuverNet能够有效提升双阿克曼机器人在复杂场景下的运动能力和轨迹效率，具备实际部署价值，突破了现有算法参数敏感及泛化能力弱等难题。

Abstract: Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.

</details>


### [296] [Analysis of a Cuspidal 6R Robot](https://arxiv.org/abs/2602.14794)
*Alexander Feeß,Martin Weiß*

Main category: cs.RO

TL;DR: 本文分析了一种名为“Transpressor”的6R奇异机械臂的运动学，提出了解析与数值求解方法，并给出了机器人奇异性的理论证明。


<details>
  <summary>Details</summary>
Motivation: 6R机械臂广泛应用于工业，但其逆运动学解复杂，尤其是具有多组解（cuspidal）的机械臂，分析和求解困难。本文旨在几何解释、解析与数值方法上突破该难题。

Method: 作者首先对“Transpressor”6R机械臂的逆运动学进行了理论和数值分析，最大可达16组解。针对特殊目标位姿给出了解析解，对一般情况则设计了简洁的数值求解器。同时，分析了雅可比行列式，在不同解之间的路径上进行估算，从而对该类机器人奇异性给出理论证明。

Result: 得到了“Transpressor”机械臂逆运动学全部16组解的几何描述，部分位姿给出了解析解，并实现了普适的数值求解器。进一步理论分析证明了此类6R机械臂的cuspidality特性。

Conclusion: 本文为复杂6R奇异机械臂的逆运动学求解和奇异性分析提供了有效的方法和工具，对相关机器人设计与应用有重要意义。

Abstract: We present a theoretical and numerical analysis of the kinematics for the "Transpressor", a cuspidal 6R robot. It admits up to 16 inverse kinematics solutions which are described geometrically. For special target poses, we provide the solutions analytically and present a simple numerical solver for the general case. Moreover, an analytical estimate of the Jacobian determinant on a path between two solutions proves cuspidality for a class of robots similar to the transpressor.

</details>


### [297] [Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization](https://arxiv.org/abs/2602.14799)
*Javier González Villasmil*

Main category: cs.RO

TL;DR: 本论文提出了一种面向多机器人路径规划问题的QUBO（Quadratic Unconstrained Binary Optimization）新方法，通过结构优化与算法改进，在算例中实现了较高求解效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于多智能体路径规划（MAPF）在机器人领域中是基础性难题，随着机器人数量增加，传统集中式方法的联合状态复杂度呈指数增长，导致效率低下，因此需要寻找新的高效可扩展方法。

Method: 作者提出面向机器人应用的QUBO公式，结合基于BFS的逻辑预处理（变量减少超过95%），自适应惩罚项用于碰撞和约束的建模，引入时间窗口分解策略以适应当前硬件能力。

Result: 实验证明在最多四台机器人、密集场景的格点环境中，该方法实现了近最优解，且扩展性优于经典的顺序路径规划方法。

Conclusion: 本文提出的QUBO多机器人路径规划方法为未来量子及量子启发式路径协调提供了高效、实用、可复现的基线。

Abstract: Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.

</details>


### [298] [Affordance Transfer Across Object Instances via Semantically Anchored Functional Map](https://arxiv.org/abs/2602.14874)
*Xiaoxiang Dong,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出了SemFM框架，实现了机器人从单次视觉演示中跨对象转移操作能力，提高了学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的演示学习方法依赖大量的物理演示，效率低且难以扩展；虽然可以从人类视频中提取交互线索，但在几何形态差异大的同类功能对象间泛化交互方式依然是难题。

Method: 方法以单张图像重建的粗略三维网格为起点，自动识别对象间语义对应的功能区域，选择具备互斥性的语义锚点，并基于功能映射将这些锚点约束传播至整个表面，获得致密且语义一致的对象对应关系。

Result: 在合成物体类别和真实机器人操作任务上，SemFM可以低计算成本实现高精度的操作区域迁移。

Conclusion: SemFM适合实际机器人感知到动作流程，具有高效、可解释和良好泛化能力，在实用化方面表现突出。

Abstract: Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.

</details>


### [299] [Kalman Filtering Based Flight Management System Modeling for AAM Aircraft](https://arxiv.org/abs/2602.14948)
*Balram Kandoria,Aryaman Singh Samyal*

Main category: cs.RO

TL;DR: 本文提出一种基于卡尔曼滤波的新颖不确定性传播方法，用于高级空中出行（AAM）飞行计划验证，并在真实ADS-B数据上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AAM飞行器的不确定性估计方法由于现实性能数据有限，普遍采用保守的线性方法，难以准确预测飞行过程中由天气、空域限制等引入的不确定性。因此，亟需更精准且可自适应的不确定性建模方法，以安全高效地进行飞行计划验证。

Method: 作者提出了一种基于卡尔曼滤波的不确定性传播方法，该方法通过Sigmoid函数对测量噪声协方差进行融合建模，可以持续自适应地调整对测量的信任程度，随着飞行进展自动调整FMS纠错行为，且可根据控制输入比例缩放、灵活调整以匹配不同飞行器或航线条件。在实际ADS-B数据（训练集、验证集）上进行参数调优与测试。

Result: 在对训练集参数调优后，方法对验证集的到达时间预测准确率达76%，展现出较好的效果。

Conclusion: 该方法为AAM运行中的战略飞行计划验证提供了更为有效的不确定性建模工具，能够自然反映FMS修正行为，适用于不同类型飞行器与复杂运行条件，验证了其实用价值。

Abstract: Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations.

</details>


### [300] [Morphing of and writing with a scissor linkage mechanism](https://arxiv.org/abs/2602.14958)
*Mohanraj A,S Ganga Prasath*

Main category: cs.RO

TL;DR: 本文研究了剪刀结构机构的运动学，通过优化方法实现了形变和写字等功能，并通过实验验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 许多机械装置希望以较少的驱动自由度实现可重复且复杂的运动，因此寻找结构简单但运动多样的机制具有重要意义。剪刀结构机构因其结构紧凑、单自由度可驱动整体变形，被认为是一种有潜力的运动单元。

Method: 作者分析了由两个线性刚性构件通过销轴连接而成的剪刀单元，研究其运动学规律，并推导出有效曲率和末端轨迹与几何参数的关系。通过将形状变换和书写任务转化为优化问题，利用可微仿真工具对机构进行任务规划，最后进行了桌面实验验证。

Result: 理论分析和仿真结果均表明，剪刀单元的几何参数可以有效编程以实现机构末端轨迹的自动化控制，包括实现复杂形状变换和指定轨迹的写字功能。实验验证了理论设计的可行性。

Conclusion: 本研究证明了剪刀单元结构在自动导航和复杂环境检测方面的应用潜力，并展示了基于优化方法的高效任务编程可能性。但作者也指出，缺乏反馈的实验仍存在快速编程和无误实施的挑战。

Abstract: Kinematics of mechanisms is intricately coupled to their geometry and their utility often arises out of the ability to perform reproducible motion with fewer actuating degrees of freedom. In this article, we explore the assembly of scissor-units, each made of two rigid linear members connected by a pin joint. The assembly has a single degree of freedom, where actuating any single unit results in a shape change of the entire assembly. We derive expressions for the effective curvature of the unit and the trajectory of the mechanism's tip as a function of the geometric variables which we then use as the basis to program two tasks in the mechanism: shape morphing and writing. By phrasing these tasks as optimization problems and utilizing the differentiable simulation framework, we arrive at solutions that are then tested in table-top experiments. Our results show that the geometry of scissor assemblies can be leveraged for automated navigation and inspection in complex domains, in light of the optimization framework. However, we highlight that the challenges associated with rapid programming and error-free implementation in experiments without feedback still remain.

</details>


### [301] [PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement](https://arxiv.org/abs/2602.14968)
*Yian Wang,Han Yang,Minghao Guo,Xiaowen Qiu,Tsun-Hsuan Wang,Wojciech Matusik,Joshua B. Tenenbaum,Chuang Gan*

Main category: cs.RO

TL;DR: 本文提出了一种能够自动生成高复杂度、物理合理的3D互动场景（PhyScensis），为机器人数据收集提供支持。该方法采用LLM代理配合物理引擎，带来比现有方法更高的场景复杂性、物理准确性及视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法主要关注于物体的空间摆放，忽略了对象之间的物理关系（如接触、支撑、平衡、包容），导致难以生成复杂且真实的操作场景。要支持如桌面排列、货架整理或箱子打包等任务，需要解决高密度物体、多样物理关系及对空间和物理特性的联合建模等挑战。

Method: 提出的PhyScensis框架包括三部分：（1）LLM代理负责根据空间和物理属性迭代提出物体及其属性；（2）基于物理引擎的求解器实现这些属性在3D场景中的真实还原；（3）求解器通过反馈机制帮助LLM代理优化配置。此外，方法利用概率编程与启发式规则共同调节场景的稳定性与空间关系，实现可控的细粒度场景生成。

Result: 实验结果表明，PhyScensis在场景复杂度、视觉质量和物理准确性上均优于之前的相关方法，可显著提升仿真环境的真实感和可操作性。

Conclusion: PhyScensis为复杂物理场景的自动生成提供了统一的技术管线，更好地支持了机器人操作研究与数据收集，推动了3D仿真场景生成领域的发展。

Abstract: Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.

</details>


### [302] [DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI](https://arxiv.org/abs/2602.14974)
*En Yu,Haoran Lv,Jianjian Sun,Kangheng Lin,Ruitao Zhang,Yukang Shi,Yuyang Chen,Ze Chen,Ziheng Zhang,Fan Jia,Kaixin Liu,Meng Zhang,Ruitao Hao,Saike Huang,Songhan Xie,Yu Liu,Zhao Wu,Bin Xie,Pengwei Zhang,Qi Yang,Xianchi Deng,Yunfei Wei,Enwen Zhang,Hongyang Peng,Jie Zhao,Kai Liu,Wei Sun,Yajun Wei,Yi Yang,Yunqiao Zhang,Ziwei Yan,Haitao Yang,Hao Liu,Haoqiang Fan,Haowei Zhang,Junwen Huang,Yang Chen,Yunchao Ma,Yunhuan Yang,Zhengyuan Du,Ziming Liu,Jiahui Niu,Yucheng Zhao,Daxin Jiang,Wenbin Tang,Xiangyu Zhang,Zheng Ge,Erjin Zhou,Tiancai Wang*

Main category: cs.RO

TL;DR: 本文提出了DM0框架，一种面向物理人工智能的原生体感视觉-语言-动作（VLA）系统，融合多样化数据源联合预训练与阶段性训练方式，在机器人操控和导航任务中实现了先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法多将物理任务适配为互联网预训练模型的微调问题，忽视了物理基础与语义知识的原生统一。该文旨在通过从头联合学习体感与认知，实现更通用和高效的物理智能体。

Method: 提出了三阶段流程：1）在VLM上使用网络文本、自动驾驶和体感日志等多源数据进行预训练，获得语义和物理先验；2）在VLM基础上训练流匹配动作专家；3）采用混合训练：体感数据仅优化动作专家，不反向传播至VLM，非体感数据VLM可训练。此外，引入“空间支架”策略，促进空间链式推理，约束动作解空间。

Result: 在RoboChallenge基准上，DM0在专才与通才两类任务（Table30）均达到SOTA性能，显示其优越的泛化与任务适应能力。

Conclusion: DM0有效统一并提升了机器人操控与导航任务的表现，强调了原生融合视觉、语言与物理知识在打造通用物理智能体中的重要价值。

Abstract: Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.

</details>


### [303] [RynnBrain: Open Embodied Foundation Models](https://arxiv.org/abs/2602.14979)
*Ronghao Dang,Jiayan Guo,Bohan Hou,Sicong Leng,Kehan Li,Xin Li,Jiangpin Liu,Yunxuan Mao,Zhikai Wang,Yuqian Yuan,Minghao Zhu,Xiao Lin,Yang Bai,Qian Jiang,Yaxi Zhao,Minghua Zeng,Junlong Gao,Yuming Jiang,Jun Cen,Siteng Huang,Liuyi Wang,Wenqiao Zhang,Chengju Liu,Jianfei Yang,Shijian Lu,Deli Zhao*

Main category: cs.RO

TL;DR: 该论文提出了RynnBrain，一个面向具身智能的开源时空基础模型，在20个具身基准和8个视觉理解基准上表现优异，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型发展迅速，但具身智能领域仍缺乏能够整合感知、推理和规划，并且具备物理世界理解能力的统一基础模型。

Method: RynnBrain实现了全面的第一视角理解、多样的时空定位、物理推理与物理感知规划能力；分为三个规模，并有四种针对不同下游任务的后训练版本。

Result: RynnBrain在广泛基准测试中明显优于现有具身基础模型，其后训练版本在物理推理与规划等方面表现突出，并易于适配多种具体具身任务。

Conclusion: RynnBrain展现了统一具身基础模型在物理推理、规划和多任务适应中的强大潜力，有望成为未来具身智能领域的重要基础设施。

Abstract: Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.

</details>


### [304] [BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames](https://arxiv.org/abs/2602.15010)
*Max Sobol Mark,Jacky Liang,Maria Attarian,Chuyuan Fu,Debidatta Dwibedi,Dhruv Shah,Aviral Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种名为Big Picture Policies（BPP）的方法，通过利用视觉-语言模型检测到的关键帧，提升机器人在需要依赖历史信息的任务中的泛化能力，实验表明BPP在真实世界操作任务中的表现大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前顶尖的机器人策略普遍只依赖当前观测，难以处理需要参考历史信息的任务。而直接利用历史观测容易导致策略学到偶然关联，导致泛化能力差。这一问题的根源是训练过程覆盖的历史状态空间有限，无法覆盖所有情况。现有的正则化方法对这一问题效果有限。

Method: 提出Big Picture Policies（BPP）方法：采用视觉语言模型自动检测并提取任务关键帧，将长历史压缩为由少量关键事件组成的序列，策略基于这一精简表示制定决策。这样既保留历史关键信息，又减少训练和部署时的分布差异。

Result: 在4个真实世界操作任务和3个仿真任务（均要求依赖历史）上进行了评估。结果显示，BPP在真实世界任务中的成功率比表现最佳的对比方法高70%。

Conclusion: BPP有效缓解了因历史信息分布差异带来的策略泛化问题，可显著提升需要历史依赖任务的机器人策略性能。

Abstract: Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.

</details>


### [305] [Neurosim: A Fast Simulator for Neuromorphic Robot Perception](https://arxiv.org/abs/2602.15018)
*Richeek Das,Pratik Chaudhari*

Main category: cs.RO

TL;DR: Neurosim是一个高性能传感器仿真库，支持多种传感器和多旋翼无人机的快速仿真，并与高效的数据通信库Cortex集成，适用于机器学习和机器人学领域。


<details>
  <summary>Details</summary>
Motivation: 当前对高性能、实时、多模态传感器仿真工具的需求强烈，特别是在神经形态感知、控制算法的开发和验证过程中。传统方案在速度、集成性和可用性上存在局限。

Method: 作者开发了Neurosim这套仿真库，支持动态视觉传感器、RGB相机、深度传感器、惯性传感器等的实时高帧率仿真，并能模拟多旋翼无人机在复杂环境下的动态表现。为实现高效数据流，集成了基于ZeroMQ的Cortex通信库，实现Python和C++下快速、低延迟的数据传输，支持NumPy和PyTorch数据类型。

Result: Neurosim在台式GPU上可达约2700FPS的高帧率。通过Cortex库可无缝集成至机器学习和机器人学工作流，实现自监督学习、多模态同步数据训练等。

Conclusion: Neurosim和Cortex为开发和验证神经形态算法提供了实用工具，支持实时闭环测试和大规模数据传输，对相关领域研究具有推进作用。

Abstract: Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .

</details>
