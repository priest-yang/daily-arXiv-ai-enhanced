<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 本文针对Deformable Attention Transformers（DAT）在硬件部署中的高效性问题，提出了一套硬件友好的优化框架，实现了几乎无精度损失的大幅内存访问优化。


<details>
  <summary>Details</summary>
Motivation: DAT虽然在计算机视觉任务中表现优异，但其依赖数据的采样机制导致不规则的内存访问，难以高效部署到硬件，现有方法要么硬件开销大，要么损失模型准确率。

Method: 提出基于神经网络结构搜索（NAS）的方法，并设计全新切片策略，将输入特征划分为均匀的patch，在不改变原有模型结构的情况下，规避内存冲突。通过联合优化硬件代价和推理精度，自动探索最佳切片配置。此外，设计了基于FPGA的验证系统，在实际边缘硬件上测试框架性能。

Result: 在ImageNet-1K数据集上，提出的方法在精度上仅较基线DAT下降0.2%。在Xilinx FPGA的硬件实验中，DRAM访问次数减少至现有DAT加速方法的18%。

Conclusion: 所提硬件友好型DAT优化框架能在几乎不损失精度的前提下，大幅优化内存访问，适用于高效边缘部署，兼顾了硬件成本与模型性能。

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [2] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的时空交通预测方法——可变形动态卷积网络（DDCN），在准确性与效率之间取得平衡，并在多个真实数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有交通流预测方法难以同时处理精度、效率以及时空异质性问题，GNN虽然主流但在节点规模大时扩展性差并且需预定义邻接矩阵。CNN对非欧空间建模有局限。研究动机是开发一种无需预定义邻接关系、高效、且能动态适应多变时空异质性的模型。

Method: 提出Deformable Dynamic Convolution Network（DDCN），结构上借鉴变压器式CNN，包含编码器-解码器架构，其中编码器引入了基于偏移的可变形卷积及时空注意力模块来提升特征捕获能力，解码器通过前馈模块补充编码器输出。

Result: 在四个真实交通数据集上，DDCN的预测准确率具有竞争力，验证了模型的有效性和潜力。

Conclusion: DDCN有效克服了目前GNN和CNN方法在时空交通预测中的主要缺陷，兼具精度和计算效率，表明基于CNN的新方法具有良好前景。

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [3] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了Inversion-DPO，一种用于扩散模型（DMs）高效对齐的新方法，无需奖励模型，在文本到图像及复杂成分图像生成任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的对齐需要额外的奖励模型和高昂的计算资源，且可能影响模型精度和训练效率。研究者希望能在精度和效率之间找到更优解，简化对齐流程。

Method: 提出Inversion-DPO方法，通过将Direct Preference Optimization（DPO）与DDIM反演结合，对扩散模型进行无奖励建模情况下的高效后训练。方法利用确定性反演将优劣样本反推至噪声空间，实现新的采样与对齐方式，无需传统奖励模型或近似。

Result: 实验在文本到图像和成分复杂的图像生成任务上，Inversion-DPO均显著优于现有后训练方法。同时作者构建了一个包含11,140张带结构注释与综合评分的新成分图像数据集，验证其提升生成模型的成分一致性和高保真表现。

Conclusion: Inversion-DPO为扩散模型的高精度高效率对齐开辟了新途径，大大简化了后训练流程，并提升了模型在复杂现实场景的生成能力，有望推动其更广泛应用。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [4] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: 本文提出ST-VFM框架，首次系统性地改编视觉基础模型（VFM）用于通用时空预测，通过双分支架构和两阶段重编程机制大幅提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型如LLM虽应用于时序预测，但主要捕捉一维序列依赖，难以有效建模更复杂且关键的时空相关性，而视觉基础模型虽对空间建模强，但缺乏时序能力和模态差异，限制其在时空预测任务中的应用。

Method: ST-VFM采用双分支结构，将原始时空数据与辅助时空流输入结合，并引入两阶段重编程：预-VFM阶段通过时序感知Token适配器，嵌入时序信息并对齐特征空间；后-VFM阶段通过双边交叉提示协调模块，实现各分支动态交互，增强联合表征，同时无需修改冻结的VFM主骨干。

Result: 在10个时空基准数据集上广泛实验证明，ST-VFM超越了多项最先进方法，并在不同VFM骨干（如DINO、CLIP、DEIT）及消融实验中均表现出色，展示强大的时空预测泛用性与鲁棒性。

Conclusion: ST-VFM为基础视觉模型扩展到时空预测任务提供了通用、高效的框架，显著提升了现有方法的准确性和适应性，为未来通用时空建模研究开辟新方向。

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [5] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 提出了一种新颖的多生成器GAN（xOp-GAN），通过为不同图像质量训练多个专家生成器，提升了水下图像恢复的效果，并且在LSUI数据集上达到了新的性能高度。


<details>
  <summary>Details</summary>
Motivation: 水下图像由于复杂的光传播、散射和深度依赖的衰减，导致多种变形伪影，使得图像恢复任务极具挑战。现有单一生成器的深度网络和传统GAN方法难以应对各种类型的视觉退化。

Method: 设计了一种包含多个专家生成器的GAN模型xOp-GAN。每个生成器只专注于某一类特定图像质量的子集，分别训练以最大化针对该子集的恢复性能。在推理阶段，所有生成器恢复图像后，由判别器根据感知置信度分数选择最佳恢复结果。

Result: 在大规模水下图像数据集（LSUI）实验证明，xOp-GAN达到了最高25.16dB的PSNR，超过了所有单一回归器模型，并且模型复杂度较低。

Conclusion: xOp-GAN首次实现了推理阶段判别器辅助的多生成器GAN模型，显著提升了水下图像恢复性能，为应对多样性降质问题提供了有效的新思路。

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [6] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: 本论文综述并实证分析了基于步态的年龄估计，覆盖多种传感器数据与超7.5万名受试者，提出深度网络可将误差降至3年以下。


<details>
  <summary>Details</summary>
Motivation: 步态年龄估计在医疗、安防和人机交互等领域应用潜力巨大，但相关研究存在准确率、泛化能力以及不同传感器下的表现不一致等问题，因此需要综合评估方法性能并提出优化建议。

Method: 1.回顾及Meta分析59篇步态年龄估计文献，涵盖摄像头、可穿戴设备和雷达数据。2.对OU-ISIR大规模步态数据集（63846步态周期）分析年龄与五个关键步态参数的相关性。3.微调ResNet34神经网络并借助Grad-CAM可视化，其关注的身体区域与年龄相关步态变化一致。4.在VersatileGait数据库10万样本上对SVM、决策树、随机森林、多层感知机与卷积神经网络等机器学习方法进行对比。

Result: 1.卷积神经网络平均误差约4.2年，惯性传感器模型约4.5年，多传感器融合最优可达3.4年。2.实验量化了步幅、速度、步频等与年龄的相关性（最高相关系数≥0.27）。3.深度网络关注膝部与骨盆区域，符合步态老化机理。4.CNN准确率最高可达96%，单样本处理时间＜0.1秒。

Conclusion: 通过对文献的系统回顾及大规模实验，本文建立了步态年龄估计的性能基准和可解释分析，为真实环境下将误差降至3年以下提出了切实指导。

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [7] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: 本文探讨了如何利用计算机视觉（CV）方法，实现对澳大利亚野外中的野猫个体进行图像再识别（re-ID），以便更好地开展相关监测与保护工作。作者提出并改进了一种基于部分姿态引导网络（PPGNet）的模型，命名为PPGNet-Cat，在野猫个体识别任务上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 野猫是对澳大利亚本地野生动物造成危害最严重的入侵物种之一。为了减缓其影响，必须对野猫个体进行精确、持续的监测。传统方法效率较低，因此亟需找到自动化的影像识别技术来提升监测效果。

Method: 本文主要将原本应用于东北虎再识别的PPGNet模型进行改进，使之适应野猫图像的特征，提出了PPGNet-Cat模型。同时，设计了一系列实验（包括对比学习如ArcFace损失）以优化性能。

Result: PPGNet-Cat在野猫个体识别任务上表现突出，平均精度（mAP）达到0.86，Rank-1准确率达到0.95。对比其它方法，模型具有很强的竞争力。

Conclusion: PPGNet-Cat能够有效提升野猫个体识别的精度和效率，对于野猫监测与管理具有重要应用价值，同时在野生动物保护领域的再识别任务中表现出较高的实用潜力。

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [8] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: 本文提出了一种名为SketchDNN的生成模型，能更好地生成CAD草图，模型通过创新的连续-离散扩散过程和Gaussian-Softmax diffusion显著提升了生成质量，在SketchGraphs数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有CAD草图生成模型难以同时处理连续参数（如长度、角度等）与离散类别（如几何图元类型），且容易受到图元参数异质性和顺序不变性影响，导致生成质量有限。

Method: 作者提出了Gaussian-Softmax diffusion方法，将带高斯噪声的logits通过softmax投影到概率单纯形，实现对离散变量的‘混合’建模，并统一连续和离散参数的联合生成，从而更好地表达CAD草图中的结构。

Result: 在SketchGraphs数据集上，所提出方法将FID由16.04降至7.80，NLL由84.8降至81.33，显著优于现有方法，提升生成草图的质量与合理性。

Conclusion: SketchDNN首次实现了CAD草图中连续与离散参数的联合扩散式建模，解决了参数异质性与顺序不变性问题，为CAD草图生成领域树立了新的性能标杆。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [9] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 本研究提出用变分自编码器（VAE）替代传统卷积神经网络（CNN）作为特征编码器，用于直肠癌淋巴结转移分期，结果显示该方法在MRI影像上取得了更高的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 现有直肠癌淋巴结转移分期主要基于影像学的大小、形状和纹理特征，诊断准确率有限。使用CNN提取特征虽然常见，但潜在空间有时不够可解释。研究者希望通过VAE获取更具判别力和可解释性的特征，提升分期精度。

Method: 替换传统大规模CNN模型为变分自编码器（VAE）作为影像特征编码器，将VAE特征作为MLP分类网络输入，建立VAE-MLP模型。模型在168例未接受新辅助治疗的直肠癌患者MRI数据集上进行训练和评估，以术后病理N分期为金标准验证模型性能。

Result: VAE-MLP模型在交叉验证下取得AUC达到0.86±0.05，灵敏度0.79±0.06，特异度0.85±0.05，整体达到目前最优水平。

Conclusion: VAE作为影像特征编码器可提升直肠癌淋巴结分期的诊断准确率，相较传统CNN更具解释性与实用价值，有助于临床精确分期和治疗决策。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [10] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 本文提出了一种基于人体姿态识别推断心理状态的方法，并将其验证应用于板球运动的视频中，实现了对运动意图（进攻或防守）的有效区分。方法表现优异，且有望推广到更广泛的领域。


<details>
  <summary>Details</summary>
Motivation: 在医疗和运动领域，推断人的心理和疲劳状态对于预防损伤、提升表现非常关键，但由于人为数据的敏感性，缺少大规模公开数据集，限制了相关工具的研究和应用。该工作探索通过体育运动场景，规避数据隐私问题，收集丰富的姿态与情感数据。

Method: 研究选取板球运动作为实验场景，通过视频分析人体动作和姿态，基于运动模式判别运动员的进攻或防守意图。进一步，借助现有数据统计量作为弱监督手段，增强对实验结果的验证能力。

Result: 所提方法在区分进攻和防守意图上取得了超过75%的F1分数和80%的AUC-ROC，表明姿态信息对意图推断有显著作用，即便数据本身存在噪声。

Conclusion: 该研究表明：在噪声条件下，姿态信号依然能够有效泄露个体意图信息。通过弱监督实现了无需大量人工标注的数据验证。研究不仅推动了运动分析技术，也为更广泛的行为分析应用提供了可能路径。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [11] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: 提出了一个叫VISTA的新方法，实现了在不同视角和季节下的高效、紧凑的全局定位，大幅优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在缺乏坐标先验的情况下，不同会话或不同主体生成的地图间进行全局定位对自动导航至关重要，但在视角变化、季节更替、空间混淆和遮挡等情况下，现有方法效果不佳。

Method: VISTA框架有两部分：首先基于对象的分割与跟踪前端，然后进行子地图间的对应匹配，通过几何一致性对齐参考帧。方法无需领域特定训练或微调，可适用于多样环境。

Result: 在有季节和视角变化的航拍数据集上，VISTA的召回率比基线高69%；且采用紧凑的对象地图，仅为最节省内存基线的0.6%，适用于资源受限平台的实时处理。

Conclusion: VISTA显著提升了非结构化环境下的全局定位表现，尤其适应视角和季节变化，且具备高效、轻量的特性，适合实际部署。

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [12] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: 本文系统性比较了多模态视觉-语言模型与轻量级CNN OCR在天气恶劣等现实场景下广告牌文字识别的表现，发现二者各有优势，并公开了全新的天气增强基准。


<details>
  <summary>Details</summary>
Motivation: 在现实户外环境下，广告牌文字由于天气、字体等变化显著，传统OCR难以准确识别。新兴的视觉-语言多模态模型（VLMs）表现突出，但其实用性及与轻量级CNN OCR的对比研究尚不充分。

Method: 作者选取主流VLMs（如Qwen 2.5 VL 3B、InternVL3、SmolVLM2）与紧凑型CNN OCR（PaddleOCRv4）为基准，在两大公开数据集（ICDAR 2015和SVT）上，通过引入合成天气失真，对模型在真实恶劣环境下的表现进行系统评测。

Result: 实验显示，VLMs虽然在整体场景理解和推理上有优势，但轻量级CNN OCR在文字裁剪识别方面仍保持了接近水平的识别准确率且运算成本极低，更适合部署于边缘设备。

Conclusion: 两类模型各有长处：VLMs适合全局理解复杂场景，轻量级OCR则在资源受限时表现更优。文中还贡献了天气增强评测基准与开源代码，有助于后续相关研究。

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [13] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 该论文提出了一种统一的条件生成解算器（UCGS），可在单一框架下解决多种抽象视觉推理（AVR）任务，实现了跨任务的零样本推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前的抽象视觉推理方法通常针对具体任务定制，需要针对每一个新任务重新训练甚至调整模型架构，增加了解决问题的成本。为此，作者希望设计一种统一的方法，能够跨多任务进行推理，提升泛化能力和效率。

Method: 作者将多种知名AVR任务归纳为问题面板中目标图像可预测性的估计问题，并在此基础上提出统一条件生成解算器（UCGS）。只需训练一个条件生成模型，通过多任务联合训练，UCGS能够适应和解决各种AVR任务。

Result: 通过实验证明，UCGS经过一次多任务训练后，在多个AVR任务上都表现出了抽象推理能力，尤其展现了零样本推理能力，即对测试阶段未见过的新任务也具有良好推理能力。

Conclusion: UCGS作为一个统一框架，突破了现有方法对任务定制的依赖，在解决AVR任务时具备强泛化能力和高效性，为通用视觉推理系统奠定了基础。

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [14] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE是一种新颖的图像配对剔除框架，能在跨领域和跨场景下提升鲁棒性及精度。


<details>
  <summary>Details</summary>
Motivation: 现有图像配对方法在处理密集配对点去除离群值时，通常假设影像来自相似的视觉域，难以应对场景与领域的多样性。本研究旨在解决配对结果在跨域、跨场景情况下的稳健性和泛化能力。

Method: 提出了CorrMoE框架，包括两个关键设计：1) 去风格化双分支（De-stylization Dual Branch）利用显式与隐式图特征混合，减少领域特有表征对配对的负面影响；2) 双融合专家混合模块（Bi-Fusion Mixture of Experts）采用线性复杂度注意力和动态专家路由，自适应多视角特征融合以应对多样场景。

Result: 在多个基准数据集上，CorrMoE在准确性和泛化性方面均优于现有最先进方法。

Conclusion: CorrMoE在跨域、跨场景图像配对中表现出更强的鲁棒性和泛化能力，适合多种计算机视觉下游任务。代码和预训练模型已开源。

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [15] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: 提出了一种用于开放集小样本图像分类的新方法ProtoConNet，通过整合背景上下文信息提升模型泛化能力，并有效识别未知类别。


<details>
  <summary>Details</summary>
Motivation: 现有小样本开放集分类方法主要依赖单个图像的视觉信息，忽视了上下文的作用，导致泛化能力和对未知类别的识别能力有限。

Method: 提出ProtoConNet，包含三大模块：CDS模块挖掘多样化数据模式并保留核心特征；CSR模块构建上下文字典并增强语义表示；PA模块对齐图像与类别原型，放大已知与未知类别特征距离。

Result: 在两个常用数据集上，ProtoConNet在表征学习和开放集样本识别上均超越了现有方法。

Conclusion: 整合上下文增强的ProtoConNet能有效改善小样本开放集分类任务中的泛化与未知类别识别能力，优于现有方法。

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [16] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法GRACE，结合动态运动建模、语义文本增强和跨模态对齐，有效提升动态表情识别的准确性，并在多个数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言表情识别方法对文本中的细腻情感线索利用不足，且缺乏有效机制过滤与情感无关的面部动态，限制了情感识别的精度。

Method: 提出了GRACE方法：构建了细粒度情感文本（通过CATE模块）、通过运动差异加权突出与表情相关的面部动态，并通过熵正则最优传输在token级实现语义与视觉信号的对齐。

Result: 在三个基准数据集上实验表明，该方法在面对含糊或类别不平衡表情时，情感识别的表现显著提升，两项关键指标UAR和WAR均取得了新的SOTA。

Conclusion: GRACE方法能更精准地捕捉与情感相关的时空特征，有效提升动态表情识别性能，特别在挑战性任务下优势明显。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [17] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的空间频率调制（SFM）方法，有效缓解了下采样过程中的混叠和细节丢失问题，从而提升了语义分割和多项视觉任务的精度。


<details>
  <summary>Details</summary>
Motivation: 高空间频率信息（例如纹理等精细细节）对语义分割准确性极为重要，但在经过步幅卷积等下采样层时易受到混叠和失真影响。为解决此问题，需要新的方法以保护和恢复高频信息。

Method: 作者提出SFM方法：在下采样前通过自适应重采样（ARS）将高频特征调制至低频以降低混叠风险，并在上采样时通过多尺度自适应上采样（MSAU）对特征进行解调，恢复高频信息。ARS密集采样高频区域，降低信号频率；MSAU通过多尺度非均匀上采样显式加强信息交互，并适配CNN和Transformers。

Result: 实验与可视化分析显示，该方法能有效缓解混叠，并在解调后保留细节。SFM方法也被拓展并验证于图像分类、对抗鲁棒性、实例分割和全景分割等任务，均取得良好效果。

Conclusion: SFM及其模块能无缝集成至多种主流架构，有效提升多项视觉任务性能，为高频信息保护与恢复提供了新范式。

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [18] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文针对高动态范围（HDR）视频质量评价提出了CompressedVQA-HDR框架，有效提升了HDR压缩视频的质量评估效果，并在相关比赛中获得冠军。


<details>
  <summary>Details</summary>
Motivation: 随着视频类型的多样化（尤其是HDR内容），现有的视频质量评价方法在泛化能力方面存在不足，难以胜任新时代视频质量评估的需求。因此，亟需设计一种能够适应HDR等新型视频类型的VQA方法。

Method: 提出了CompressedVQA-HDR框架。全参考（FR）模型采用Swin Transformer为主干，利用其中间层特征来计算参考与失真帧的结构和纹理相似性。无参考（NR）模型采用SigLip 2主干，使用最终层特征图的全局均值表示视频质量。针对HDR训练数据有限的问题，FR模型先在大规模SDR数据集上预训练，再在HDR数据集上微调，NR模型则采用多数据集迭代训练策略并最终微调。

Result: 实验结果表明该方法（无论是FR还是NR模型）均优于同类现有方法，并且CompressedVQA-HDR-FR在IEEE ICME 2025相关比赛中获FR组冠军。

Conclusion: CompressedVQA-HDR框架在HDR压缩视频质量评价方面展现出卓越效果，具备优良的泛化能力和实际应用前景。代码已开源，便于推广与复现。

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [19] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 本论文提出了一个合成的基于事件传感器的人体姿态估计数据集SEPose，用于提升行人与交通监控系统中特殊场景下的数据覆盖，并测试了主流模型在此数据集上的表现及其现实泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人和交通监控系统在处理因分心等引起的特殊动作时，受限于数据缺乏，无法充分利用事件传感器带来的高动态范围和低延时优势。该问题阻碍了模型在安全关键场景下的可靠性。因此，亟需一个大规模多样化的高质量数据集来促进模型开发和评估。

Method: 作者在CARLA仿真环境下，采用动态视觉传感器生成并注释了近35万名带关键点的人体姿态图像，涵盖多种人群密度、照明、天气和多样环境。并利用SEPose数据集训练RVT和YOLOv8等主流姿态估计模型，将其在真实的事件感知数据上进行测试，以分析数据集的仿真到现实泛化能力。

Result: 通过在SEPose数据集上训练，所选模型能较好地泛化到真实事件传感器数据，验证了SEPose在提升现有算法实际应用性能方面的价值。

Conclusion: SEPose数据集有效拓展了事件感知人体姿态估计领域的数据资源，能够显著提升模型在复杂交通监控环境下的泛化能力，为实际智能监控的安全性与可靠性提供有力支撑。

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [20] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出了一种事件辅助的3D高斯溅射（GS）框架Dark-EvGS，能够在低光环境下从任意视角重建明亮图像帧。通过设计新的监督机制与色调匹配模块，显著提升了渲染质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 在低光环境下，传统相机难以获得清晰多视角图像，原因在于动态范围有限和长曝光导致的运动模糊。事件相机具备高动态范围和高速特性，能够缓解上述问题，但现有基于3D GS的方法依然面临噪声高、图像质量低及色彩不一致等挑战。

Method: 作者提出Dark-EvGS框架，结合事件相机和3D高斯溅射，实现明亮图像的多视角重建。创新地引入了三元组级别监督，提升整体和局部细节学习能力，并加入色调匹配模块以保证渲染帧的色彩一致性。同时，首次采集并公开了用于事件引导的3D GS数据集。

Result: 实验表明，Dark-EvGS在低光环境下的辐射场重建质量优于现有方法，能有效生成明亮且细节丰富的一致性帧。相关代码与数据集已在补充材料中提供。

Conclusion: Dark-EvGS首次实现了基于事件相机引导的多视角低光影像明亮重建，有效提升了低光条件下三维场景的渲染效果和色彩一致性，为实际应用和未来研究奠定了基础。

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [21] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: 本论文提出Hyperphantasia基准，用于评估多模态大语言模型（MLLMs）的心理表象能力，通过四类程序生成益智题揭示了人类与模型在心理可视化能力上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的评测主要集中于被动视觉感知，忽视了模型主动构建和操作内部视觉表象（即心理表象）的能力。但心理表象对推理、预测和抽象等高级认知至关重要，因此作者希望通过新基准补足此类能力的评估。

Method: 作者设计了一个名为Hyperphantasia的合成基准，包含四种类型的程序生成益智任务，并设定三个难度级别，有针对性地考察MLLMs在越来越复杂情景下的心理表象表现。利用这些任务系统性评估了多种最先进的MLLMs，同时还实验性引入了强化学习方法来提升模型能力。

Result: 实验结果表明，各类先进MLLM在部分任务中能够识别视觉模式，但整体与人类在心理可视化能力上仍存在明显差距。强化学习带来一定改进，但模型仍不能像人类一样健全地进行心理可视化。

Conclusion: 尽管有部分模型在视觉模式识别上具备一定能力，当前的多模态大模型仍难以可靠实现人类级的心理视觉表象，相关领域仍存在巨大提升空间。

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [22] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: RaDL框架提升了文本生成图像模型在多实例、关系和属性控制方面的能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着T2I（文本到图像）模型的发展，如何在单条指令中有效生成包含多个对象以及处理它们之间关系和多属性的问题成为挑战。现有方法难以兼顾实例间的关系准确性和属性不混淆。

Method: 提出了关系感知解耦学习（RaDL）框架。通过可学习参数增强实例属性，并基于从全局描述中提取的动作动词，利用关系注意机制生成更关系感知的图像特征。

Result: 在COCO-Position、COCO-MIG和DrawBench等公开评测集上，RaDL在位置准确率、多属性及实例关系的生成能力上显著超越现有方法。

Conclusion: RaDL能生成同时兼顾多实例关系和属性的图像，是当前多实例多关系T2I生成任务的有效解决方案。

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [23] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的通用语义分割方法PPAR，通过引入CLIP生成的两类原型与渐进对齐和重加权机制，提升了跨域泛化能力，并在多个数据集上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于类别原型的泛化分割方法存在粗糙对齐、原型易过拟合和样本一视同仁三大问题，导致泛化性能不足。论文希望解决原型质量与适应性问题，提高分割模型在未知域上的稳定性和准确率。

Method: 提出PPAR框架，利用CLIP模型获取原始文本原型（OTP）和视觉文本原型（VTP）作为对齐基础，并通过渐进对齐策略按难度逐步匹配特征，降低域间差异。同时，引入原型重加权机制，动态调整源数据对学习的影响，抑制无关或有害特征的负面迁移。

Result: 在多个主流数据集上进行了广泛实验，对比多种方法，PPAR在全部基准上均取得了当前最优的分割性能。

Conclusion: PPAR有效改善了类别原型对齐的细致程度及原型的可靠性，通过CLIP增强泛化能力与理论支撑，在通用语义分割领域展示了显著优势。

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [24] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: 提出LG-CAV-MAE，通过引入预训练文本编码器，将语言信息融入音视频对比自编码器，提高多模态（音频、视觉、文本）表示学习表现，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的音视频表示学习通常只关注音频和视觉两种模态，未能充分利用语义丰富的语言信息，且多数需耗费大量有标注数据。

Method: 将预训练文本编码器集成进对比音视频掩码自编码器，并提出自动生成未标注视频的音频-视觉-文本三元组的方法（包括帧级图像描述生成和基于CLAP的筛选确保高一致性），无需人工标注。

Result: LG-CAV-MAE在音视频检索和分类任务中均大幅超越现有方法，检索任务的Recall@10提升最高达5.6%，分类任务提升3.2%。

Conclusion: 将语言信息作为桥梁有效促进了音视频多模态表示学习，LG-CAV-MAE在实际任务上展示了强大的性能和通用性，且可自动利用无标签数据生成训练样本。

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [25] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 本文提出了一个针对多模态大模型（MLLMs）在短视频中的安全性评估框架，介绍了新的对抗性数据集和攻击方式，并揭示了现有模型的显著安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型日益应用于内容审核，但其在短视频场景下的鲁棒性尚未被充分探索，现有评估大多仅基于单一模态攻击，忽视了组合攻击的威胁。

Method: 提出了包含多样短视频的人为引导合成对抗样本的数据集SVMA，并设计了一种新的三模态攻击策略ChimeraBreak，能够同时攻击视觉、听觉和语义推理通道。使用当前最先进的MLLMs进行了广泛实验，并采用LLM-as-a-judge评估攻击效果。

Result: 实验显示，现有MLLMs在三模态攻击下表现出高攻击成功率，暴露出对良性与违规内容的误分类偏差，并揭示了不同的失败模式。

Conclusion: 该研究展示了MLLMs在复杂短视频安全场景下的显著脆弱性，并提出的数据集与发现为未来构建更安全、更健壮的多模态模型提供了重要参考。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [26] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种高效且有效的视觉语言模型测试时自适应方法GS-Bias，在15个基准数据集上实现了最先进表现，并大幅降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在性能和效率之间难以平衡：有的需要大量调优文本提示，开销大；有的通过视觉特征增强，但效果不稳定。如何实现提升泛化性能同时提高效率，是亟需解决的问题。

Method: 提出Global-Spatial Bias Learner (GS-Bias)，在测试时引入两种可学习偏置：global bias用于利用多视角增强提取全局语义一致性，spatial bias用于捕捉图像各区域的空间语义一致性。两者直接加在VLM输出的logit上，无需对VLM进行完整反向传播。

Result: GS-Bias方法在15个基准数据集上取得了最佳性能。例如，在跨数据集泛化任务上较TPT提升2.23%，在领域泛化上提升2.72%，并且仅用ImageNet上TPT内存消耗的6.5%。

Conclusion: GS-Bias不仅实现了显著的性能提升，还极大地提高了推理效率，是VLM测试时自适应的优选方法。

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [27] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: 本文提出了EC-Diff，一种高效的边缘-云协作的扩散模型推理方法，实现高质量生成的同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型体量和延迟的提升，用户体验下降。因此需要结合云端的高质量生成和边缘端的低延迟推理，设计更高效的协作推理框架。

Method: EC-Diff通过梯度噪声估计加速云端推理，并提出K步噪声近似策略及双阶段贪心搜索，用于降低云端推理次数并优化云-边切换时机，保证生成质量和推理速度。

Result: 大量实验证明，EC-Diff相较仅在边缘推理能显著提升生成质量，相较纯云端推理实现平均2倍加速。

Conclusion: EC-Diff兼顾生成质量和推理速度，为扩散模型在实际应用中的落地提供了优秀的云-边协作方案。

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [28] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的无监督部件发现方法——Masked Part Autoencoder (MPAE)，能在没有标签的情况下鲁棒地提取图像中的有意义部件特征，且适用多类别和复杂场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于部件的图像理解因为缺乏细粒度标签而进展有限，无监督方法虽能缓解标签依赖但现有方案在跨类别和复杂场景下鲁棒性不足，限制了其应用范围。

Method: MPAE首先从输入图像学习部件描述子和特征图，对原图片的掩码版本生成patch特征。再依据局部特征与部件描述子的相似性，用学到的描述子补全掩码区域。通过这些描述子的引导，掩码patch重建时更符合真实部件的形状。同时，提出了更宽松有效的约束，提升部件发现的普适性。

Result: MPAE在各种类别和复杂场景下都能鲁棒地发现与真实目标部件形状高度匹配的有意义部分，显著优于现有方法。

Conclusion: MPAE实现了无监督条件下的鲁棒部件发现，为解决遮挡与跨类别部件相似性等问题提供了新基础，对多场景、多类别图像理解有大力推动作用。

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [29] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像扩散模型，可以在一张图像中，通过掩码精确地混合和控制多个艺术风格区域，实现零样本、多风格分区生成，并且结构连贯。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的文本到图像合成模型，尽管在生成多样化图像和捕捉特定风格上表现优秀，但其潜空间交织且缺乏平滑插值，导致难以在一张图像内区域性、受控地混合多种艺术风格，通常一种风格会主导其他风格。论文旨在解决多风格区域混合及控制的难题。

Method: 提出了一种零样本扩散流程，利用分别训练的多风格专用模型，在去噪过程中对各自的低噪声潜变量进行风格级成组合。通过空间掩码，将不同模型在潜空间的信息融合，实现分区的风格控制。同时，结合ControlNet中的深度图条件输入，确保结构一致性。

Result: 定性和定量实验表明，该方法能够根据掩码精准地对区域施加不同风格，实现高保真、结构连贯的多风格融合图像生成。

Conclusion: 所提方法有效解决了多风格区域混合和控制的难点，实现了任意区域、任意风格的文本到图像生成，对艺术创作和视觉内容生成具有重要意义。

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [30] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D Gaussian Splatting表示的定位系统SGLoc，能够在没有先验位姿的情况下通过语义信息实现全局定位，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的定位方法通常需要初始位姿估计或依赖于特定场景，导致其实用性受限。如何不依赖先验位姿、且能提升全局定位的准确性是该领域亟需解决的问题。

Method: 方法上，SGLoc利用2D图像和3D场景语义信息的关系，提出多级位姿回归策略、语义检索算法：1）通过2D-3D语义描述符对齐，进行粗位姿估计；2）再通过渲染误差优化细化位姿。系统完全不依赖初始位姿信息。

Result: 在12scenes和7scenes数据集上的实验表明，SGLoc在没有初始位姿先验的情况下，相较于基线方法取得了更优的全局定位性能。

Conclusion: SGLoc为无先验的室内场景定位提供了高效、准确的新方案，对相关应用如机器人导航具有实际意义。

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [31] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ID-EA的新框架，有效提升了基于文本到图像扩散模型的个性化人像生成中的身份保持，并大幅提升了生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Textual Inversion方法在个性化人像生成时，文本和视觉特征之间存在语义不一致的问题，导致身份信息不能很好地保存。本研究旨在解决两者语义对齐及身份一致性差的问题。

Method: 提出了ID-EA框架，包括ID-Enhancer（身份增强器）与ID-Adapter（身份适配器）两大模块。ID-Enhancer将文本身份锚点与视觉身份嵌入进行整合，从而优化视觉身份嵌入。ID-Adapter在此基础上调整交叉注意力模块，使文本特征能更好地对应到人像中的关键信息，提高身份保持能力。

Result: 在多项定量与定性评测中，ID-EA框架在身份保持方面显著优于现有主流方法，并且在人像生成速度上约为现有方法的15倍。

Conclusion: ID-EA极大提升了文本生成个性化人像的身份保持能力，并带来了优越的效率，展现了在实际应用中的广阔前景。

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [32] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文针对道路交通体预测，提出利用推理意图指导轨迹预测的新方法，通过逆强化学习(IRL)推理行为意图，并使用该意图提升运动预测效果，实验表明方法优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多直接预测交通体未来轨迹，缺乏对行为意图的解释与利用，导致预测不够精准且可解释性不足。

Method: 作者提出“先推理，再预测”的思路，设计基于查询中心的逆强化学习框架，首先将交通参与体和场景统一向量化表达，通过IRL推理得到奖励分布表示意图，并据此进行多种意图的策略回滚，最后基于分层DETR式解码器及双向状态空间模型输出带概率的轨迹预测。

Result: 在Argoverse和nuScenes大规模运动预测数据集上，提出的方法在提升预测置信度和准确性方面显著优于主流方法，达到了极具竞争力的性能。

Conclusion: 显式推理行为意图并作为辅助能有效提升自动驾驶中的运动预测性能，本文方法在大规模实验中展现了出色效果，为解释性和安全性提供了有力保障。

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [33] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAMST的半监督语义分割方法，通过结合大模型SAM的泛化能力与小模型的训练效率，提升遥感图像的伪标签质量和分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有公开遥感数据集存在分辨率变化大与地物类型定义不一等通用性问题，且标注数据稀缺，影响了遥感场景的精准语义分割。因此需开发能有效利用大量未标注遥感数据的新方法。

Method: SAMST方法利用Segment Anything Model（SAM）进行零样本泛化与边界检测，通过伪标签自循环迭代优化。其核心包括两部分：（1）用标注数据和伪标注数据进行半监督自训练；（2）构建SAM基础的伪标签精修器，包括预处理阈值筛选、连接域提示生成、标签拼接模块，提升伪标签准确性。

Result: 在Potsdam等遥感分割数据集上实验表明，SAMST有效提升了伪标签准确性和整体分割表现，验证了该方法在标注数据有限背景下的可行性和优势。

Conclusion: SAMST方法通过融合大模型和小模型各自优势，为标注不足的遥感语义分割提供了新思路，实验结果显示其具备实际应用潜力。

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [34] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: 本文提出了AutoVDC框架，利用视觉-语言模型（VLM）自动检测和清理自动驾驶视觉数据集中的错误标注，从而提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统训练需要大量高质量、精确标注的数据集，但人工标注容易出错，且人工复查成本高昂。因此，开发自动化工具以高效提升数据集质量成为必要。

Method: 提出AutoVDC框架，利用视觉-语言模型对数据集进行自动错误标注检测。实验采用KITTI和nuImages两个自动驾驶常用数据集，在其中人为注入错误标注，以测试AutoVDC的错误检测能力，并比较不同VLM及其微调对检测效果的影响。

Result: AutoVDC在错误检测和数据清理实验中表现优异，不同VLM对检测率有影响，微调后效果进一步提升。

Conclusion: 该方法能够显著提升大规模生产数据集的可靠性和准确性，为自动驾驶系统的数据准备工作提供了有价值的自动化工具。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [35] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: 本文提出了AUBlendSet数据集和AUBlendNet网络，实现了基于32种面部动作单元（AUs）的高精度3D人脸表情风格化操控，支持跨身份的精细化3D表情编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D人脸动画虽取得进展，但由于缺乏适合的数据集，实现精细化、风格化的3D表情操控仍有挑战。特别是在支持多身份与连续表情操作方面，现有资源有限。

Method: 1）构建了AUBlendSet数据集，包含32种标准面部动作单元及500个身份的3D blendshape数据，并配有详细AU标注的附加表情姿态；2）提出AUBlendNet网络，可针对任意身份并行预测风格化AU-Blendshape基向量，实现个性化、风格化的3D表情操控。

Result: 通过定性和定量实验，AUBlendSet和AUBlendNet在风格化表情操控、语音驱动情感表情动画和情感识别数据增强等多项任务上验证有效，表现出强大的泛化与灵活性。

Conclusion: AUBlendSet为首个支持任意身份通过AUs实现连续3D表情操控的数据集，AUBlendNet为首个对应网络。两者为3D人脸表情动画领域带来重要进展，为未来相关研究提供基础数据和方法支持。

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [36] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: 本论文提出了一种频率动态注意力调制（FDAM）方法，旨在解决ViTs因低通滤波和频率消失导致细节信息损失的问题。该方法通过两个新技术提升了模型表现，并在多项视觉任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer由于注意力机制，使每一层都表现为低通滤波器，导致累计后丢失重要的高频细节和纹理信息。这种频率消失问题影响了模型的表达能力和性能。

Method: 提出FDAM，包含注意力反转（AttInv）和频率动态缩放（FreqScale）两项技术。AttInv模拟电路理论，将注意力矩阵中的低通滤波反转得到高通滤波，通过动态融合两种滤波；FreqScale用于针对不同频率成分进行加权，实现细粒度的响应调控。

Result: 通过特征相似度分析和有效秩评估验证，方法可避免表征塌缩。FDAM在SegFormer、DeiT和MaskDINO等多种模型及语义分割、目标检测、实例分割等任务上均带来持续性能提升。在遥感检测单尺度设定下也达到SOTA。

Conclusion: FDAM是一种高效且易于集成于ViT结构的频率调制方法，能够弥补现有方法高频细节损失的不足，有效提升Transformer在多视觉任务中的表达与性能。

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [37] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督领域自适应（UDA）方法MaskTwins，通过将掩码重构纳入主训练流程，有效提升了图像分割任务中的领域不变特征提取能力。实验表明，其性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作将掩码图像建模（MIM）与UDA中的一致性正则化联系起来，但只把随机掩码视为输入扰动，缺乏理论分析，未能充分挖掘其提升特征提取与表达学习的潜力。因此，本文旨在从理论和方法上深化对MIM在UDA中作用的理解与利用。

Method: 本文将掩码重构理论化为稀疏信号重建问题，并证明互补掩码有更强的提取领域无关特征的能力。基于此，提出MaskTwins框架，在训练过程中对以互补掩码方式遮盖的图像输出结果进行一致性约束，从而实现端到端的领域泛化能力提升。

Result: MaskTwins在自然和生物图像分割的多项实验中，显著优于传统基线方法，能够提取更具通用性的领域不变特征，并且不依赖单独的预训练。

Conclusion: MaskTwins为领域自适应分割任务提供了新的思路，理论和实验证明了其在提升模型泛化能力方面的显著优势，显示了掩码重构与一致性正则化结合的巨大潜力。

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [38] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的深度神经编码-解码模型，利用fMRI数据对自然刺激下的大脑活动进行编码和解码，不仅能预测视觉皮层区域脑活动，还能从脑活动重建视觉输入。


<details>
  <summary>Details</summary>
Motivation: 随着自然场景下脑活动研究的需求增加，需要更好地结合神经成像数据和复杂视觉刺激，并突破fMRI与动态视觉刺激时间分辨率的差异。

Method: 采用片段级时间卷积神经网络模型处理连续电影画面，实现对fMRI信号的编码和对视觉刺激的解码，利用显著性图揭示关键脑区。

Result: 模型能有效预测视觉相关区域脑活动，并从神经数据成功重建对应视觉内容。显著性分析显示中枢枕叶、梭状回、距状回等区域对解码贡献最大。

Conclusion: 本研究所提模型不仅提升了对自然电影刺激下视觉编码-解码的理解，也为用深度模型探索大脑视觉处理机制提供了有力工具。

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [39] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: cs.CV

TL;DR: 本文提出利用无监督机器学习为肝脏磁共振影像建立可量化的组织模式词汇，从而定量分析肝脏疾病的治疗反应。此方法比传统的非影像学指标更好地区分治疗组和对照组，并能用影像数据预测活检特征。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病进展和治疗效果的评估需要准确、可量化的影像模式，目前对于弥漫性肝脏疾病，标准化的组织变化量化工具有限。作者旨在通过机器学习建立一种新的影像组织词汇，用于提升对治疗反应的量化分析、辅助个体化治疗与新疗法开发。

Method: 采用无监督的深度聚类网络，对肝脏磁共振影像的图像块进行编码和聚类，构建低维潜空间中的组织词汇。分析非酒精性脂肪性肝炎（NASH）随机对照试验数据，比较安慰剂组与治疗组在随访期间的肝脏组织变化并进行预测和验证。

Result: 新方法建立了能区分肝脏组织类型的词汇，准确捕捉肝组织的空间和时间变化，并且对比传统非影像学指标，能更好地分离治疗组与对照组。该方法还可用影像推断活检相关特征，并在独立队列中得到了验证。

Conclusion: 该无监督影像组织词汇为肝脏疾病治疗反应和进展提供了更精准的定量工具，有望促进个体化治疗的实现，并辅助新疗法评估。方法具有良好的可推广性。

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [40] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: 本文针对RGB到红外（RGB-IR）无人监督领域自适应目标检测（UDAOD）难题，提出解耦-耦合新框架，通过频谱域方法有效分离和重组领域不变及特定特征，显著提升了多场景领域适应检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有UDAOD方法普遍将RGB域视为单一整体，忽视其内部如白天、夜晚、雾天等多个子域，导致领域适应过程未能充分解决跨子域区别，影响红外目标检测。需要更细致区分与处理领域不变和特定特征，以提升RGB-IR领域转移性能。

Method: 作者提出SS-DC解耦-耦合框架。核心包括：(1) 频谱自适应幂等解耦（SAID）模块，通过频谱分解精确区分领域不变（DI）与领域特定（DS）特征，并借助基于滤波器组的频谱处理与自蒸馏驱动解耦损失提升解耦效果；(2) 空间-频谱联合耦合，基于特征金字塔综合利用空间与频谱两个域的DI特征，并把DS特征引入帮助减少领域偏差。

Result: 在多个RGB-IR数据集（包括文中基于FLIR-ADAS的新实验协议）上，提出方法显著优于现有UDAOD方法，并大幅提升基线检测性能，实验结果充分验证了其有效性。

Conclusion: 新提出的解耦-耦合策略结合频谱分解和空间-频谱特征融合，能更好地处理多子域下RGB-IR自适应目标检测问题，为跨领域场景提供了更有效的技术路径。

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [41] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: 本文提出了一种名为DOV4MM的新方法，用于验证一个黑盒模型是否曾在特定无标签数据集上进行过预训练，尤其适用于日益流行的掩码模型。该方法在多个图像和文本数据集上表现出优于以往所有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 高质量开源数据集推动深度学习快速发展，但也面临被滥用的风险，如何保护数据集所有权变得尤为重要。目前的数据集所有权验证方法大多只适用于监督学习或对比自监督模型，难以应用到主流的掩码模型上，存在明显的空白。

Method: 作者提出了DOV4MM方法，基于实证观察：若模型在目标数据集上预训练，其在嵌入空间重建被mask信息的难度明显不同。DOV4MM通过评估black-box模型在特定无标签数据集上的预训练证据，来检测和验证模型的数据集使用情况。

Result: DOV4MM在10个掩码图像模型（ImageNet-1K）和4个掩码语言模型（WikiText-103）上的测试显示，该方法能显著拒绝虚无假设（p值远小于0.05），验证效果优于此前所有方法。

Conclusion: DOV4MM为掩码模型的数据集所有权验证提供了首个有效方案，实验证明其准确可靠，对数据集所有者的数据权益保护具有重要意义。

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [42] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: 提出了一种多元自回归空气污染物预测模型MVAR，能够利用更短的历史数据，实现对多种污染物的长达120小时预测，并通过新架构提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有空气污染物预测方法大多只关注单一污染物，忽视了多种污染物之间的相互作用以及时空响应的多样性，且缺乏标准化数据集。

Method: 提出了多元自回归空气污染物预测模型（MVAR），该模型减少对长时间窗口输入的依赖，提高了数据利用率，结合多元自回归训练范式，实现长序列预测，并加入气象耦合空间Transformer结构以灵活融合AI气象预测和学习多污染物互动。还构建了涵盖6种污染物、75个城市、2018至2023年的大规模标准化数据集。

Result: 模型在新的大规模数据集上经过实验，结果显示比当前先进方法具有更好预测性能，验证了新架构的有效性。

Conclusion: MVAR模型在多污染物、长时序空气污染预测任务上准确度更高，数据利用更高效，新方法和新数据集能推动领域进步。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [43] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文提出了3D-MoRe方法，生成大规模3D-语言数据集用于室内场景理解，显著提升了问答和描述任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着3D场景任务（如问答和密集描述）对多样化和大规模数据需求的增长，现有3D-语言数据集有限、难以充分训练和评估多模态模型。亟需自动化、高效的生成大规模高质量3D-语言配对数据的方法。

Method: 提出3D-MoRe框架，整合多模态嵌入、跨模态交互、语言模型解码器等核心模块，自动处理自然语言指令与3D场景数据。以ScanNet场景为基础，结合ScanQA和ScanRefer的文本注释，并应用数据增强和语义过滤，生成大量高质量问答对和物体描述。

Result: 3D-MoRe框架共生成62,000组QA对和73,000条物体描述。实验表明在ScanQA和ScanRefer数据集上，3D-MoRe对比SOTA方法分别带来2.15%和1.84%的CIDEr提升。

Conclusion: 3D-MoRe能够显著增强3D场景的问答与描述能力，所生成数据集高效促进模型性能提升。代码和数据集将开源，有望推动社区发展。

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [44] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 该论文提出了一种针对多视角新类别发现（NCD）的问题的新方法，首次将NCD扩展到多视角数据，并通过特定矩阵分解和多视角协同机制提升聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有NCD方法大多只关注单一视角（如仅图像），而实际中常见多视角数据（如多组学数据在疾病诊断中的应用）；且依赖伪标签的方式对聚类结果稳定性不足。作者希望设计一种能处理多视角且更鲁棒于伪标签噪声的方法。

Method: 提出了IICMVNCD框架：1）在视角内，通过矩阵分解将特征分为共享基础矩阵（描述数据分布一致性）和因子矩阵（建模样本间关联）；2）在视角间，利用已知类别的多视角关系指导未知类别聚类，通过加权融合因子矩阵预测标签，并根据监督损失动态调整已知类别的视角权重以迁移给新类别。

Result: 实验结果验证了该方法对多视角新类别发现任务的有效性，在数据上表现优越于现有方法。

Conclusion: IICMVNCD首次将NCD扩展到多视角情景，通过高效的特征分解和视角协同极大提升了新类别聚类的效果，对实际多源异构数据分析具重要意义。

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [45] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 本文介绍了MoViAD，一个面向视觉异常检测（VAD）的高模块化库，涵盖前沿方法、训练器、数据集及工具，并支持多种实际部署场景如边缘端与IoT。


<details>
  <summary>Details</summary>
Motivation: 视觉异常检测领域面临异常样本稀缺、通常需无监督训练等挑战，研究和实际部署难度大，缺乏一站式高效工具是加速领域发展的瓶颈。

Method: MoViAD以高度模块化设计集成了主流VAD模型、训练器和数据集，并支持诸如持续学习、半监督、少样本、噪声环境等多种场景。此外，针对边缘设备和IoT实际部署需求，提供了模型优化、量化与压缩工具，集成强大的评估度量与分析工具，支持自定义扩展。

Result: MoViAD实现了对现有VAD模型和常用数据集的快速接入与测试，同时优化了在多种终端设备上的高效运行，为开发者和研究者提供了良好兼容性和易用性。

Conclusion: MoViAD为视觉异常检测领域的研究与实际部署提供了一站式、灵活高效的工具支持，降低了门槛，加快了创新方法的实验与落地进程。

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [46] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种面向人脸防伪的新方法InstructFLIP，通过引入视觉-语言模型和指令调优机制，提高了跨域泛化性能，并减少了训练冗余，实验结果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有面向人脸防伪的研究大多关注在跨域泛化，但仍存在两个核心挑战：一是对攻击类型语义理解有限，二是不同域间训练存在较多冗余。

Method: 提出InstructFLIP框架，结合视觉-语言模型（VLMs），采用通过文字指令提升语义理解，同时采用meta-domain策略训练统一模型。独特之处在于将指令显式分离为内容（攻击本质语义）和风格（环境及拍摄设备差异）两部分，只在单一域上训练。

Result: 大量实验表明，该方法在防伪准确率上超过了当前最优（SOTA）模型，并且显著减少了各域间的训练冗余。

Conclusion: InstructFLIP能够有效提升人脸防伪算法的泛化能力，并能用更少的训练资源获得更好的性能，具有实际推广价值。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [47] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: 本文提出MS-DETR新方法，通过联合建模运动与语义特征提升视频片段检索和精彩片段检测任务表现，在多个基准数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DETR框架已在视频片段检索和精彩片段检测任务取得进展，但对视频中时间运动与空间语义复杂关系的利用尚不足。该文旨在更好地挖掘运动和语义两种信息间的联系，从而提升视频理解相关任务的准确度。

Method: 提出Motion-Semantics DETR（MS-DETR）框架。其编码器分别对运动和语义维度的模态内相关进行解耦建模，并在文本查询引导下实现信息融合。解码器利用任务相关的跨维度相关性，实现精确的基于查询的定位和精彩片段边界生成。针对运动和语义信息稀疏的问题，作者通过生成策略扩充数据并采用对比去噪学习以提升鲁棒性。

Result: 在四个视频片段检索和精彩片段检测基准数据集上，MS-DETR均明显优于最新方法，表现出更强的准确率和定位能力。

Conclusion: 通过有效联合学习运动和语义信息，并针对数据稀疏性进行改进，MS-DETR极大提升了MR和HD任务性能，为视频理解任务提供了更优解决方案。

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [48] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: 本文提出了一种在无人机视角下追踪鸟类等小巧灵活多目标（SMOT）的新方法，并在MVA 2025“Finding Birds”挑战中获冠军。采用检测-跟踪框架，对检测和关联模块都进行了创新。提出SliceTrain提升检测，同时设计了完全无外观依赖、强化运动建模的追踪器。该方法在竞赛公开测试集上性能优异，证明了其实用性和先进性。


<details>
  <summary>Details</summary>
Motivation: 无人机空中视频中追踪小鸟等目标十分困难，主要因为：1）目标外观特征极为稀少，2）相机和目标自身的复杂动态导致运动混杂，3）高密度聚集带来频繁遮挡与ID混淆。现有方法很难高效且准确地完成此类任务。

Method: 作者采用tracking-by-detection方案，分别针对检测和跟踪设计创新：在检测上，提出SliceTrain训练框架，实现图像分片全覆盖采样和分片级增强，提升小目标学习；在跟踪上，在无需外观信息的基础上提出包含运动方向维护（EMA）与结合框扩展与距离惩罚的自适应相似度度量机制，集成进OC-SORT，使追踪器更稳定地处理不规则动态和ID保持。

Result: 在SMOT4SB公开测试集上，提出方法取得了SO-HOTA 55.205的成绩，优于现有同类方法，刷新了该领域的基准。

Conclusion: 作者的方法显著提升了无人机视角下小目标多目标追踪的性能，验证了SliceTrain与无外观追踪框架的有效性。展示了直接面向复杂现实任务的解决方案，具有广泛实际应用与推广前景。

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [49] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 本论文提出并评估了一种针对多发性硬化症患者大脑皮质病灶MRI自动检出与分割的多中心基准方法，改善了检测精度和泛化能力，并公开了代码与模型。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症皮质病灶（CLs）作为诊断和预后生物标志物极具价值，但在实际临床中因MRI表现隐匿、专家标注难度大、缺乏标准化自动方法而难以常规应用。

Method: 收集来自四个机构、采用不同MRI序列（3T与7T，MP2RAGE与MPRAGE）、共656份影像及专家共识标注，使用自适应的nnU-Net深度学习框架进行病灶自动分割，并针对CL检测进行模型调整，系统评估模型的泛化能力（含域内外测试），同时分析模型内部特征及常见错误。

Result: 模型在域内检测得分F1=0.64，域外F1=0.5，表现出较强的泛化能力。对数据差异、病灶模糊性及协议不同对模型表现的影响进行了分析，并提出改进意见。

Conclusion: 多中心大样本MRI数据下，基于适配的nnU-Net方法可实现较为可靠的芯病灶自动检测，是推动此类AI工具临床落地的重要一步；研究成果与工具代码已公开，以促进标准化和复现性。

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>


### [50] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的3D车辆稀疏视角重建方法，在输入视角稀疏的情况下也能得到高质量重建效果，实验效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法如NeRF和Gaussian Splatting对输入视角稠密性要求高，而实际应用中摄像视角常常有限，制约了这些方法在车辆检测、维护、城市规划等领域的应用。

Method: 方法上，作者通过结合深度图和强健的姿态估计架构（DUSt3R），合成新视角并扩充数据集。同时，对Gaussian Splatting算法进行改进：一是只对高置信度像素施加选择性光度损失；二是用DUSt3R替换传统SfM管线以提升姿态估计精度。并构建了覆盖合成与真实交通工具车辆的新数据集。

Result: 新方法在多个基准数据集上取得了当前最优（state-of-the-art）的重建性能，能够在稀疏输入情况下也实现高质量的3D重建。

Conclusion: 所提方法有效提升了在稀疏输入视角条件下的3D车辆重建能力，克服了现有方法实际应用受限的问题，对车辆相关领域有重要工程意义。

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [51] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: 本论文提出了DeepShade模型，通过结合3D仿真和深度学习，实现了高质量的城市阴影预测，支持更安全、更舒适的城市步行路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划系统无法有效纳入阴影信息，影响极端高温下居民的健康出行体验。主要原因是阴影难以从卫星图像直接估算，且缺乏训练生成模型所需的大规模阴影数据。

Method: 作者首先构建了一个涵盖不同地理区域、建筑密度和城市布局的大型数据集，通过Blender3D仿真结合建筑轮廓，获取全年各时间段的建筑阴影数据，并与卫星图像对齐。随后提出DeepShade扩散模型，联合RGB与Canny边缘特征，并引入对比学习以捕捉阴影随时间的变化规律，同时结合文本条件（如时间、太阳高度角）生成高质量阴影预测。

Result: 作者在亚利桑那州坦佩市实际场景中用预测结果计算路径阴影比，验证了方法的实用性。DeepShade生成的阴影图像在时间变化和细节还原上均优于现有方法，提升了高温条件下导航的参考价值。

Conclusion: 本研究为城市极端高温应对提供了技术基础，有潜力在城市规划和居民出行等实际领域产生积极影响。

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [52] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Med-OoD的数据驱动方法，利用Out-of-Distribution (OoD)数据监督来提升医学图像分割的准确性，无需外部数据、特征正则化或额外标注，可直接融合于现有分割网络。实验显示该方法能显著减少像素级别误分类，尤其在Lizard数据集上表现突出。


<details>
  <summary>Details</summary>
Motivation: 医学分割网络在有限且不完美的医学图像数据集上训练时，前景与背景之间容易出现误分类，影响了分割准确性。为解决这一问题，作者受到OoD数据在其他视觉任务中提升泛化能力的启发，尝试将OoD数据引入全监督医学分割中，以提升分割表现和鲁棒性。

Method: 提出Med-OoD方法，通过将OoD数据的监督信号融入全监督的分割网络训练过程中，无需外部新数据、特征正则化目标或额外人工标注。该方案可直接应用到多种分割网络架构中，无需结构改动。

Result: 在多个分割网络和Lizard医学数据集上，Med-OoD显著减少了像素误分类，分割性能有明显提升。此外，作者还展示了仅用OoD数据、完全不依赖前景类别标签训练分割网络的新范式，在测试集上取得了76.1%的mIoU成绩。

Conclusion: Med-OoD不仅能够减缓医学图像分割中的误分类问题，还提出了一种新的使用OoD数据训练分割网络的思路，有望推进大家对OoD数据作用的重新思考。

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [53] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新颖的方法生成对抗性人脸图像，可有效欺骗人脸识别系统。与传统的基于迭代优化的方法不同，该方法利用人脸特征空间的结构属性，无需多次自适应查询即可实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常需要大量自适应查询或者依赖可迁移性及开源代理模型，但实际应用中对商用人脸识别系统频繁查询往往不可行。因此，亟需一种查询次数极低、无需依赖迁移性的对抗样本生成方法。

Method: 作者发现在人脸识别系统的特征空间中，具有相同属性（如性别或种族）的人脸形成带属性的子球面。基于这一结构特征，他们提出一种利用子球面特征、仅需极少量（一次性100张）非自适应查询即可生成对抗人脸的新方法。

Result: 新方法在仅进行一次非自适应查询、上传100张人脸图片的前提下，在AWS CompareFaces API默认阈值条件下达到超过93%的攻击成功率。

Conclusion: 提出的方法极大降低了对抗性攻击对商用人脸识别系统的操作成本和难度，无需依赖可迁移性或代理模型，并可产生符合攻击者指定高层属性的对抗性人脸图像，对人脸识别系统的安全性构成了严峻挑战。

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [54] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 本文提出LidarPainter，一种基于扩散模型的实时动态驾驶场景重建方法，显著提升了新视角下的重建一致性和质量，同时速度快、资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有动态驾驶场景重建在输入轨迹偏移时容易产生背景与车辆模型的损坏，且普遍存在一致性差、变形与耗时问题，这严重影响了数字孪生和自动驾驶仿真的应用效果。

Method: 作者提出LidarPainter方法，使用一步扩散模型，以稀疏LiDAR数据和受损渲染为条件输入，生成一致且高质量的驾驶视角，支持实时运行。该方法还能通过文本提示实现风格多样化生成，如“雾天”“夜晚”等。

Result: LidarPainter在速度、质量和资源利用方面均优于现有方法。例如，其推理速度为StreetCrafter的7倍，显存需求仅为其1/5。

Conclusion: LidarPainter极大提升了动态驾驶场景的新视角重建表现，并具备良好的资源效率和风格可控性，对智能驾驶仿真等领域具有重要应用价值。

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [55] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D分层场景图的室内物体开放词汇定位方法OVIGo-3DHSG，结合多模态模型与层级关系，实现更高效和鲁棒的场景理解及物体定位。


<details>
  <summary>Details</summary>
Motivation: 当前室内环境物体定位和理解任务对开放词汇和复杂空间关系的支持有限，缺乏可处理复杂空间查询和推理的高效方法。作者希望通过结合分层场景图和大语言模型提升复杂室内环境下的物体定位与空间推理能力。

Method: 提出了一种基于3D分层场景图（含楼层、房间、位置、物体）的表示方法，利用RGB-D序列和基础模型提取信息，通过与大语言模型结合，实现多步空间推理，支持层间和层内的关系建模。

Result: 在Matterport 3D多楼层语义场景上评测，OVIGo-3DHSG在场景理解和物体定位准确率上优于现有方法，表现出高效和鲁棒性。

Conclusion: OVIGo-3DHSG可有效支持复杂空间推理和室内场景理解，具备应用于需要空间理解与推理等场景的巨大潜力。

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [56] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: 本文提出了一种基于块的对称剪枝与融合（BSPF-ViT）方法，可以有效降低ViT模型的计算复杂度，在减少计算量的同时提升表现。


<details>
  <summary>Details</summary>
Motivation: ViT在视觉任务中表现出色，但其高计算成本限制了实际应用。现有通过剪枝降低复杂度的方法常因独立剪枝Q/K令准确率降低。本文旨在解决这一问题，通过同时优化Q/K的剪枝，并充分考虑token之间的交互。

Method: 提出BSPF-ViT：利用块状对称剪枝和融合方法，联合剪枝Q/K token，不再只考虑一个方向，而是结合周围token的信息。保留的tokens通过相似性融合进一步压缩信息，同时利用Q/K共享权重形成对称注意力矩阵，通过仅对上三角区域剪枝来进一步加速。

Result: BSPF-ViT在各种pruning level下均优于当前最优方法，在ImageNet上DeiT-T模型提升1.3%，DeiT-S 提升2.0%，同时计算成本减少50%。整体实现40%加速，且提升了准确率。

Conclusion: BSPF-ViT能够在显著降低ViT计算开销的基础上，超越现有剪枝方法的效果，在速度和精度上实现双赢，有望大幅推动ViT的实际应用。

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [57] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 本文提出了一种结合双边网格与像素自适应MLP的新型图像增强框架（BPAM），突破了现有方法的线性变换限制，实现了复杂高效的非线性色彩映射。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的双边网格方法由于只支持线性仿射变换，难以处理复杂色彩关系；而传统MLP虽然能实现非线性映射，却只采用全局参数，难以处理空间上的局部变化。因此需要一种能兼顾空间适应性和非线性能力的图像增强方法。

Method: 作者提出BPAM框架，将MLP参数以双边网格方式存储，使每个像素都能根据空间与强度信息动态获取专属MLP参数，从而生成个性化的非线性色彩映射。同时设计新的网格分解策略和多通道引导图，将不同类别的MLP参数拆分存放于子网格，并由引导图辅助精细检索，实现高效且精准的参数生成。

Result: 在公开数据集上的大量实验证明，BPAM框架在性能上优于当前主流方法，同时保持实时推理速度。

Conclusion: BPAM框架有效解决了传统方法无法兼顾空间自适应性和非线性映射的问题，为高效图像增强提供了新思路，具有实际应用价值。

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [58] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AD-GS的自监督框架，无需人工注释即可高质量地渲染动态城市驾驶场景，且效果优于现有无标注方法，接近有标注方法。


<details>
  <summary>Details</summary>
Motivation: 当前高质量的城市场景动态渲染方法依赖于代价高昂的手动物体轨迹注释，而自监督方法无法准确建模动态物体及正确分解场景，导致渲染效果差，因此需要一种无需人工注释且能高质量还原动态场景的方法。

Method: 提出AD-GS框架，核心是融合局部B样条曲线与全局三角函数的可学习运动模型，实现灵活精准的动态建模。通过简化的二维伪分割自动分解场景为物体和背景，动态物体用高斯分布和双向时序可见性掩码表示。引入可见性推理和物理刚性正则化以增强鲁棒性。

Result: 在大量测试中，AD-GS无需任何注释就能显著优于当前最好的无标注方法，且性能与需要注释的方法接近。

Conclusion: AD-GS实现了高质量、免注释的动态驾驶场景建模与渲染，为自动驾驶仿真等应用提供了有效工具。

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [59] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 本文提出一种基于神经网络的正态化流(normalizing flows)用于人体姿态先验建模，显著提升了表达能力和兼容性，在动作捕捉和三维重建等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 已有的人体姿态先验多为启发式或表达能力有限，难以全面刻画人体姿态的高维复杂分布。本文旨在利用更具表现力的概率模型，提升姿态先验的建模效果，为相关应用如动作捕捉带来更好基础。

Method: 采用RealNVP正态化流对使用6D旋转格式表示的人体姿态进行概率建模。为处理6D旋转的流形结构，设计了反向Gram-Schmidt过程以实现稳定训练，并确保结果可用于主流旋转相关框架。该方法通用性强、易于复现。

Result: 通过定性可视化、定量实验和消融分析，验证了所提方法优于传统先验建模技巧，并在实际人体动作捕捉和三维人体重建任务中取得了优秀效果。

Conclusion: 该工作为人体姿态先验建模提供了扎实的概率基础，方法通用且有效，有助于推动人体动作捕捉及相关任务的发展。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [60] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: 本论文针对细粒度图像识别（FGIR）提出了一种不依赖大规模预训练模型的新训练框架TGDA，通过数据感知增强和弱监督蒸馏，实现在低算力、低分辨率场景下也能获得高精度识别，打破传统对ImageNet等预训练的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像识别方法严重依赖于在大规模数据集（如ImageNet）预训练的骨干网络，这限制了模型在算力受限环境中的适应性，也阻碍了针对FGIR任务场景定制架构的开发。论文的动机是打破这种依赖，实现从零训练高性能FGIR模型，扩展模型在实际场景下的适用性和效率。

Method: 提出TGDA训练框架，结合数据感知增强方法和基于知识蒸馏的细粒度弱监督教师网络，并设计了适用于低分辨率场景的LRNets以及高效推理的ViTFS（改进型Vision Transformer）系列，均能从头训练，无需依赖预训练权重。

Result: 在三个FGIR基准测试上，TGDA训练的模型在低/高分辨率设置下的精度均匹配或超过了依赖预训练的最新方法。LRNets在低分辨率下准确率提升最高达23%，模型参数、计算量和训练数据需求大幅降低。ViTFS-T能以15.3倍更少参数和数量级更少的数据匹配ImageNet-21k预训练的ViT B-16性能。

Conclusion: TGDA为细粒度图像识别系统提供了一种无需预训练的有效新途径，可更灵活地设计面向特定任务和硬件的高效模型，展现了替代传统预训练方法的巨大潜力，为实际应用下的高效细粒度视觉系统铺平了道路。

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [61] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 本文提出了一种双重集成框架，用于提升脑部肿瘤MRI影像的自动分类效果，通过集成深度学习特征提取和集成调优的机器学习分类器，显著提高诊断准确率，并验证了各个组件的有效性。


<details>
  <summary>Details</summary>
Motivation: 人工评估MRI影像容易因专家疲劳、经验有限和图像细节不足而出现误诊，尤其小肿瘤容易被漏检或与健康组织混淆，因此迫切需要更精准的自动化脑肿瘤检测方法。

Method: 该方法分为两部分：第一，利用多种预训练深度卷积神经网络和视觉Transformer网络，集成提取MRI脑部影像深度特征；第二，利用多种机器学习分类器，通过超参数精细调优并集成分类预测。全流程包括详尽预处理和数据增强，结合特征融合及分类器融合来提升分类性能，并开展消融实验分析。

Result: 在三个公开Kaggle脑部MRI肿瘤数据集上的实验结果表明，所提深度特征融合与分类器集成方法优于现有技术，尤其超参数精细调优带来了明显性能提升；消融研究也揭示出每个组件对准确分类的贡献。

Conclusion: 本文提出的双重集成框架能有效提升脑部肿瘤MRI自动化分类的准确率和可靠性，为实际医学诊断提供更具潜力的辅助工具。

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [62] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种基于小波变换和特征空间解耦的低光立体图像增强方法，通过分离低频和高频信息，分别提升图像照明和细节，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法通常将所有退化因素编码于同一特征空间，导致特征高度缠结，模型表现为黑盒，容易出现捷径学习，影响增强效果。作者希望通过更细致的特征分离，提升增强效果与可解释性。

Method: 方法核心包括：1）利用小波变换将低光图像分解为低频（用于照明调整）与多个高频分支（用于纹理细节增强）；2）提出高频引导的跨视图交互模块（HF-CIM），在高频分支间提取多视角细节信息；3）基于交叉注意力机制设计细节和纹理增强模块（DTEM）；4）在均匀与非均匀照明的图像数据集上训练和评估模型。

Result: 实验表明，该方法在真实和合成低光图像上的光照调整与高频信息恢复方面效果显著优于现有方法。

Conclusion: 小波变换特征空间解耦结合高频引导、跨视图交互与细节增强模块，能有效提升低光立体图像增强质量，对照明和细节恢复均有突出表现。

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [63] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: 本文提出三种针对印度古迹保护的先进数字方法，通过分割、区域填充和超分辨率提升技术，有效提升文化遗产数字化修复的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 在多学科合作下，应用机器学习、深度学习等新技术以解决传统文化遗产保护中修复精细度、自动化效率与创新性不足的问题，特别针对富有建筑艺术与美学价值的印度古迹。

Method: 1）提出基于图像处理的分割方法“分形卷积”，用于揭示文化建筑的细微结构；2）针对西孟加拉邦的Bankura陶土寺庙，开发自敏感瓷砖填充（SSTF）及创新数据增强方法“MosaicSlice”；3）利用超分辨率策略提升遗产图像质量。上述方法在提升还原真实性的同时也兼顾成本效益与自动化。

Result: 所提出方法成功实现了无缝区域填充、高度细节还原且保持真实性，有效提升了文化遗产保护中图像处理的质量与效率。

Conclusion: 该研究不仅以创新手段推动了文化遗产数字化保护的效率和美学质量，还实现了传统与创新的高度平衡，为文化遗产保护领域带来了全新高度。

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [64] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为RODS的新方法，通过优化视角重审扩散模型采样过程，可检测并修正采样中高风险步骤，从而减少幻觉现象并提升采样质量，无需重新训练且只需极低的额外推理开销。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成建模任务中表现卓越，但其采样过程易受幻觉问题影响，原因往往是得分函数近似不准确。因此，提升采样阶段的稳健性和可靠性成为亟需解决的问题。

Method: 作者将扩散采样过程用优化理论进行重新解释，提出RODS方法，利用损失景观的几何信息来检测高风险采样步骤并进行修正。RODS实现对采样轨迹的平滑约束，并自适应调节各步的扰动大小，不需重新训练模型，仅需极少的额外运算成本。

Result: 在AFHQv2、FFHQ和11k-hands数据集上的实验表明，RODS能提升采样的保真度和鲁棒性，对幻觉样本的检测率超过70%，修正率超过25%，且不会引入额外伪影。

Conclusion: RODS作为一种稳健优化启发的扩散模型采样器，能有效检测和修正采样幻觉，提升生成样本质量和鲁棒性，为扩散模型的实际应用提供实用手段。

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [65] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: 本文提出了一个更丰富的深度伪造画面问答数据集（DD-VQA+）和全新的伪造检测框架（MGFFD-VLM），通过引入多属性驱动训练和多粒度提示学习，提升了视觉大语言模型在伪造检测和解释方面的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉大语言模型（VLM）的伪造检测虽可判别及解释伪造面孔，但未能充分利用面部质量等属性，且训练策略有局限，影响检测模型的表现与解释能力。

Method: 作者扩展了伪造问答数据集，引入更多伪造相关属性和样本，创建了DD-VQA+。方法上，提出了MGFFD-VLM框架，融合属性驱动的混合LoRA策略、多粒度提示学习和伪造感知训练策略，并设计了辅助损失，通过将检测与分割结果转化为提示以提升VLM的能力。

Result: 实验显示，该方法在文本伪造判断和分析任务上，准确率和解释性均优于现有方法。

Conclusion: 新提出的数据集和训练策略显著提升了伪造检测VLM的性能和可解释性，为后续伪造检测技术研究提供了新的思路和基线。

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [66] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出利用生成式扩散模型进行医学图像短语定位，相比于现有判别式方法大幅提升了无监督定位精度，并通过创新后处理方法进一步优化结果。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像短语定位大多依赖判别式的自监督对比模型，其在泛化性与精度上存在局限。生成式模型（如扩散模型）虽在其他领域显示出优越性能，但在短语定位任务中的潜力尚未充分挖掘。作者希望探索生成式模型在无监督短语定位中的表现，特别是在医学影像领域，以实现更强的泛化和解释能力。

Method: 作者使用生成式文本到图像的扩散模型，通过交叉注意力图实现短语定位。同时，将领域特定的语言模型（如CXR-BERT）冻结，与扩散模型结合进行微调，并与领域无关的模型做对比。此外，提出了一种新颖的双模态偏置融合（BBM）后处理技术，通过对齐文本和图像偏置进一步提升定位准确率。

Result: 基于上述方法，模型在无监督短语定位任务上的mIoU得分比现有判别式方法高出一倍。使用BBM技术后，在交叉注意力图基础上定位精度进一步提升，展现了生成式方法的显著优势。

Conclusion: 生成式扩散模型结合领域专用语言模型在医学图像短语定位上表现优异，并通过创新的偏置融合方法进一步提升准确性。该方法为医学影像短语定位任务提供了新的有效范式，有望提高临床应用的稳健性和可解释性。

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [67] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文主要研究如何通过视频分析对体能训练中的静态技巧进行自动化识别与时序分割，提出并发布了相关数据集，并验证了基线方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 体能训练（Calisthenics）对技巧动作的评判高度依赖于动作难度和持续时间。目前缺乏能够自动识别并分割静态技巧持续时间的视频识别工具，这对训练和比赛评判都具有重要意义。

Method: 作者构建了一个包含静态体能技巧动作视频的数据集，每个视频都进行了动作持续性的时间标注。随后，作者提出并实现了一个基线模型来处理该时序分割问题，基于视频中的人体姿态信息分析不同技巧的起止时间。

Result: 在新建的数据集上，基线方法取得了一定的效果，能够初步证明该问题具有可行性，但结果显示还有较大的提升空间。

Conclusion: 论文为体能训练技能自动识别和分段问题提供了数据资源和基线研究，证明了该问题的研究价值和可行性，为后续改进算法和实际应用打下了基础。

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [68] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文对Keras、PyTorch和JAX三种深度学习框架在医学图像分类任务中的表现进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习框架多样，但其在医学图像分类场景下的实际对比还不充分，研究动机是填补这一空白，帮助研究者选择合适工具。

Method: 基于PathMNIST数据集，分别利用三种主流深度学习框架实现并训练CNN模型，综合评估其训练效率、分类准确率和推理速度。

Result: 实验揭示了不同框架之间在计算速度和模型准确率上的权衡。

Conclusion: 研究为医学图像分析领域提供了关于不同深度学习工具选择的实证参考，对实际应用具有指导意义。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [69] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法分析极低出生体重儿出生24小时内的胸部X线片，实现了支气管肺发育不良（BPD）预后的早期预测，有望减少低风险婴儿不必要的治疗风险。


<details>
  <summary>Details</summary>
Motivation: BPD预防干预有显著副作用，因此需及早准确预测BPD结局，以避免对低风险新生儿的不必要伤害。出生后常规X线片有望成为低侵袭性的预测工具。

Method: 收集163名极低出生体重早产儿的出生24小时内胸部X线。采用在成人胸片上预训练的ResNet-50，通过分步冻结、分层学习率控制防止过拟合，并评估CutMix增强法和线性探查。对比了领域内预训练和ImageNet初始化的模型表现。

Result: 最佳模型（领域内预训练+分步冻结+线性探查+CutMix）预测中重度BPD的AUROC为0.78±0.10，平衡准确率0.69±0.10，F1分数0.67±0.11。领域内预训练显著优于ImageNet初始化（p=0.031）。常规IRDS分级预测效能较低（AUROC 0.57±0.11）。

Conclusion: 领域特定的预训练和创新训练策略能提升极早期BPD结局预测准确性，且方法计算成本低，适合实际临床和未来多中心学习部署。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [70] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 本文提出了FADE方法，用于有效擦除扩散模型中的指定概念（如隐私或有害刻板印象），在不牺牲整体生成质量的前提下，提升了内容移除和保真度的均衡表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成上表现优异，但存在记忆敏感信息和延续偏见的风险。作者希望开发一种方法，通过移除指定概念，提升模型的隐私保护和公平性。

Method: FADE方法结合轨迹感知微调和对抗性目标，保证指定概念从生成能力中被可靠移除，同时理论上最小化了概念与输出间的互信息。方法评估中应用于主流扩散模型（如Stable Diffusion），并在现有的对象、名人、敏感内容和风格擦除基准上与ESD、UCE等方法对比。

Result: FADE在指定概念擦除和输出图像质量上均优于现有方法，移除-保真度调和平均提升5-10%。消融实验显示对抗与轨迹目标均对优异表现有贡献。

Conclusion: FADE为可控、安全、合规的生成式建模设立了新标准，可以在无需从头训练的情况下，安全高效地卸载敏感或有害概念。

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [71] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本论文提出了一种新的体操技能识别方法，通过结合深度估计和人体区域提取，避免了高计算成本的人体姿态估计步骤，实现更快且更准的技能识别。


<details>
  <summary>Details</summary>
Motivation: 传统体操技能识别方法依赖于姿态估计算法，不仅计算量大，推理时间长，且设置复杂，不适合实时应用和移动设备。因此，亟需一种更高效、更简便的方法。

Method: 作者提出直接结合深度估计（使用Depth Anything V2）和运动员检测（使用YOLOv10）的方法，将人体从背景中分割出来，跳过姿态估计。这样得到的图像块再用于技能分类。该方法具备模块化设计，支持灵活更换组件。

Result: 与基于骨骼的方法相比，该方案推理速度提升38.3倍，RGB图像补丁下更快且深度补丁下分类准确率提升（0.837对0.815）。

Conclusion: 新方法兼具高效与准确，并具备很强的适应性和扩展性，适用于实时或移动端体操技能识别场景，有助于后续研究和实际应用。

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [72] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像表征方式——离散潜码（DLC），并展示其在扩散模型中的应用，大幅提升了无条件图像生成的质量和生成多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在生成复杂分布上表现优异，但这种成功主要来源于输入条件（condition）。现有用于条件输入的图像表征存在泛化性有限和生成难度等瓶颈。因此，作者致力于寻找更高效、更具有组合性的表征方法，以提升生成质量并扩展模型能力。

Method: 作者提出了一种基于自监督学习的Simplicial Embeddings训练方法，生成离散潜码（DLC），即一系列离散符号序列，用于条件扩散模型。与传统的连续特征不同，DLC更易生成且具备组合性，支持生成训练数据之外的新颖图像。作者通过实验证明DLC在ImageNet等任务中显著提升了生成效果。

Result: 基于DLC训练的扩散模型在ImageNet无条件图像生成任务上取得当前最优的生成质量。同时，实验展示了通过组合不同DLC能够合成语义融合的、训练之外的全新图像。此外，还能基于大规模文本生成模型，通过微调实现高效的文生图（text-to-image）生成。

Conclusion: DLC是一种优异的图像表征，其离散性和组合性显著增强了扩散模型的泛化能力和表现力。方法简单易用，有望拓展至各类条件生成任务并激发更多创新应用。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [73] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出了KeyDiff3D，一个无需监督即可从单张图像准确预测3D关键点的框架，显著降低了3D关键点估计对高成本标注或多视角数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D关键点估计方法依赖手工标注或多视角校准数据，这些数据获取代价高昂。因此作者希望设计一种只需单视角图像即可进行高精度3D关键点估计的方法。

Method: 该方法利用了预训练多视角扩散模型中的几何先验能力：首先用扩散模型为单张输入图像生成多视角图像，作为监督信号提供3D几何信息；同时将扩散模型作为强大的2D多视角特征提取器，利用其中间特征构建3D特征体，从而将深度隐含的3D先验转变为显式的3D特征。此外，作者还构建了一个可操纵扩散模型生成3D对象的流程。

Result: 在Human3.6M、Stanford Dogs及多种野外和域外数据集上，KeyDiff3D在准确性、泛化能力及可操纵性方面均表现优异。

Conclusion: KeyDiff3D显著降低了对高成本数据的依赖，实现了单张图像驱动的3D关键点高精度估计，并首次支持基于单图像的扩散模型生成3D对象的操控，为单目3D理解与下游3D生成任务提供新思路。

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [74] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet Oğuz Saltık,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: 本文探讨了如何通过知识蒸馏（CWD与MGD）提升轻量级目标检测模型（YOLO11n）在杂草检测任务中的性能，使其适合在资源受限设备上的实时部署。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要准确检测杂草以实现精准喷洒，减少环境影响，但高精度检测模型通常过于庞大，难以在如智能喷雾系统等资源受限平台上高效运行，尤其面对外观相近的杂草品种检测更为困难。

Method: 作者采用基于YOLO11框架的知识蒸馏方案。以YOLO11x为教师模型，YOLO11n为学生模型和参考模型，采用了通道级知识蒸馏（CWD）与掩码生成蒸馏（MGD）方法，将教师模型的知识有效迁移到轻量的学生模型，对比蒸馏和基线模型。

Result: 在真实甜菜及四类杂草数据集上，CWD和MGD提升了所有类别的AP50性能。CWD模型mAP50提升2.5%，MGD提升1.9%，且无模型复杂度增加。学生模型在Jetson Orin Nano和Raspberry Pi 5嵌入式设备上通过多次试验验证了其实时部署和性能稳定性。

Conclusion: CWD与MGD是提升轻量级模型检测精度的高效且实用手段，适合在精准农业和植物表型分析场景中的深度学习杂草检测任务，实现了精度和部署效率的平衡。

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [75] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督视觉表征学习方法CueCo，通过融合对比学习与聚类方法，有效提升特征区分性和聚集性，在多个数据集上取得了领先的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督视觉表征学习大多依赖对比学习或聚类方法，但单独使用它们各自存在局限，例如对比学习注重区分但不足以提升类内一致性，聚类则难以保持类间区分性。作者希望设计一种能兼顾类间分散和类内紧凑的学习方法。

Method: CueCo方法由两个神经网络（query和key）组成，key网络通过query网络输出的慢速均值更新。方法包括对比损失（增强类间区分）和聚类目标（提升类内聚集），实现特征空间的分散与对齐。

Result: 在ResNet-18骨干网络基础上，CueCo方法在CIFAR-10、CIFAR-100和ImageNet-100上分别获得了91.40%、68.56%和78.65%的top-1线性评估分类准确率，优于许多现有方法。

Conclusion: CueCo通过有效结合对比学习和聚类目标，显著提升了无监督视觉表征学习的性能，为领域发展提供了新的研究方向。

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [76] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本驱动的多平面视觉交互框架（Text-SemiSeg），用于半监督医学图像分割，通过文本信息增强三维医学图像的视觉特征，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据的标注代价高，标注数据有限，因此需要半监督分割方法。与此同时，文本信息可为医学图像提供额外上下文，但在3D医学图像与视觉语义增强结合的研究较少。

Method: 提出Text-SemiSeg框架，包括三个主要模块：1）TMR模块实现文本与多平面视觉特征的交互，提高类别感知能力；2）CSA模块通过可学习变量实现文本特征与视觉特征中间层的跨模态对齐；3）DCA模块通过有标签与无标签数据的交互，减少数据分布差异，提升模型鲁棒性。

Result: 在三个公开医疗数据集上的实验结果表明，Text-SemiSeg能有效利用文本增强视觉特征，并且分割性能超越已有方法。

Conclusion: 通过引入文本增强机制，Text-SemiSeg在医学图像半监督分割中显示出更强的表现和泛化能力，对未来多模态医学图像分析具有参考和推动价值。

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [77] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: 本文提出了两个专门用于现实监控场景下视觉目标检测的新数据集：OD-VIRAT Large和OD-VIRAT Tiny，并基于此对多种主流目标检测算法进行了基准评测。


<details>
  <summary>Details</summary>
Motivation: 现实中的监控数据集对于提升计算机视觉模型在复杂环境下的效果至关重要。现有监控数据普遍缺乏多样性和挑战性，难以全面检测模型在实际应用环境中的表现，因此亟需大规模、真实并带有丰富标注的监控数据集。

Method: 作者构建了覆盖10种不同监控场景的两个大规模视频数据集，提供细致的目标类别与边界框标注。利用OD-VIRAT Large和OD-VIRAT Tiny，系统评测了RETMDET、YOLOX、RetinaNet、DETR和Deformable-DETR等新一代目标检测方法在复杂环境下的表现。

Result: OD-VIRAT Large包含约87万个标注目标，OD-VIRAT Tiny包含约29万个标注目标。评测显示，目前流行的目标检测方法在高难度监控场景下，如复杂背景、遮挡和小目标检测等，精度与鲁棒性仍有改进空间。

Conclusion: 本工作首次系统地评估了先进检测算法在高真实度监控数据上的表现，为模型开发者提供了参考基线，推动了更强大、鲁棒的人体监控自动检测技术的发展。

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [78] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为Query-Relevant Retrieval through Hard Negative Sampling (QuRe)的新方法，用于提升由图片和文本组成检索任务的性能，特别是减少错误的负样本，提高人类满意度。


<details>
  <summary>Details</summary>
Motivation: 现有的组合图片检索方法忽视了目标图片之外的图片的相关性，且由于对比学习方法把非目标图片均视为负样本，容易出现‘假负样本’，进而导致无关图片被检索出来，降低用户体验。

Method: 提出一种‘困难负样本抽样’（hard negative sampling）策略，通过优化奖励模型目标，选出相关性分数在目标图像后出现两次急剧下降区间之间的图片作为困难负样本，从而有效减少假负样本。

Result: 在FashionIQ和CIRR数据集上，QuRe方法取得了最优性能，并在新构建的HP-FashionIQ（抓取用户偏好）数据集上表现出最符合人类偏好的检索结果。

Conclusion: QuRe方法在提升组合图片检索准确性及用户满意度方面显著优于现有方法，并且为该领域提供了新的数据集和评测标准。

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [79] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 本文提出了一种新的目标检测回归损失函数InterpIoU，通过使用插值框与目标之间的IoU替代传统手工设计的几何惩罚项，提升了小目标检测效果，并解决了现有IoU损失的若干优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有IoU类损失函数在处理非重叠目标时，需引入手工设定的几何惩罚项以改善回归表现，但这些惩罚项对框形状、尺寸及分布非常敏感，导致小目标优化欠佳，甚至出现框增大等问题。

Method: 提出InterpIoU损失函数，用预测框与真实框之间的插值框的IoU项代替原有手工惩罚项，实现非重叠情况下梯度更有意义，并本质上避免了框无意义扩大的问题。同时，设计了Dynamic InterpIoU，根据IoU动态调整插值系数，使其更适应多样目标分布场景。

Result: 在COCO、VisDrone、PASCAL VOC等多个数据集及检测框架上的实验表明，InterpIoU及其动态版本相较于现有最优IoU损失函数表现更优，特别在小目标检测上提升显著。

Conclusion: InterpIoU和Dynamic InterpIoU有效解决了现有IoU回归损失的缺陷，无需手工几何惩罚项，在各类目标检测场景下具有更强的适应性和优越性能，尤其提升了小目标检测表现。

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [80] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: 本文提出了一种高效轻量的视频识别网络DVFL-Net，融合了知识蒸馏和时空特征调制，从大型教师模型中学到时空信息，提高了在人类动作识别任务中的表现且计算资源消耗低，适用于设备端实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有视频识别领域从3D CNN转向Transformer，虽然准确率提高，但计算资源消耗巨大，难以设备端部署。因此需要一种同时兼具高效与高性能的视频识别模型。

Method: 提出DVFL-Net，通过知识蒸馏，将大规模预训练的Video-FocalNet Base（教师模型）的时空知识迁移到紧凑的学生模型VFL-Net，并采用前向KL散度及时空特征调制，实现本地与全局信息的有效融合，降低模型复杂度并保持精准度。

Result: 在UCF50、UCF101、HMDB51、SSV2和Kinetics-400等主流数据集上进行实验，对比最新方法。DVFL-Net表现出更低的内存消耗、较少的GFLOPs以及强大的准确率。消融实验进一步验证了前向KL散度的有效性。

Conclusion: DVFL-Net在准确率和高效性之间实现了最优平衡，适用于实时人类动作识别等实际场景，具有良好的应用推广价值。

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [81] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的交通感知时空图卷积网络（TA-STGCN），通过引入交通信号灯状态和行人包围框大小等特征，显著提升了对行人意图的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有行人意图预测模型未充分考虑动态交通信号和场景上下文信息，而这些因素对于自动驾驶安全至关重要。本文旨在解决该问题。

Method: 提出TA-STGCN模型，将红绿灯等交通信号灯的动态状态和行人包围框大小作为输入关键特征，结合时空图卷积捕捉复杂城市环境下的时空依赖关系。

Result: 与现有基线模型相比，TA-STGCN在PIE数据集上的行人意图预测准确率提升了4.75%。

Conclusion: 通过融合交通信号等多元信息，TA-STGCN极大提升了自动驾驶场景下的行人意图预测能力，为自动驾驶安全提供了更强有力的技术支持。

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [82] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 本文提出并探讨了基于Describe Anything Model（DAM）的区域感知能力在处理文本密集型视觉问答（VQA）任务中的优势，并提出了更有效的DAM-QA方法。


<details>
  <summary>Details</summary>
Motivation: 现有DAM模型能够对图像的任何区域生成详细描述，但其在文本密集型视觉问答任务中理论上更有用，因为该场景对精细化文本信息抽取能力要求极高。作者希望利用DAM的区域描述能力来提升对包含大量文本的VQA场景的理解和推理能力。

Method: 提出DAM-QA框架，并设计特定的评测协议以评估区域感知能力。具体方法包括融合来自图像不同区域视角的答案机制，从而更好识别与文本元素相关的证据，实现对文字丰富图像的理解与推理。

Result: 在六个VQA基准测试中，DAM-QA方法较基础DAM取得了持续领先，尤其在DocVQA数据集上提升超过7分，并以更少参数达到目前区域感知模型的最佳表现，显著缩小了与强大通用VLM模型的差距。

Conclusion: DAM及类似模型在配合合理高效的使用和集成策略下，对于文本丰富型及更广泛的VQA任务展现巨大潜力，能够提升相关领域的模型表现。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [83] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: 本文提出了一种仅依靠摄像头感知的高效障碍物规避系统，在大学校园复杂环境下测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在复杂环境中避障需求迫切，但对于仅利用摄像头（而非多传感器融合）的感知与规划体系，仍需提升其精度和效率。该研究旨在探索高效、成本较低的避障方案。

Method: 系统集成了YOLOv11目标检测和最新的单目深度估计算法（如Depth Anything V2），通过Frenet-Pure Pursuit路径规划方法实现轨迹避障，并对不同深度估计算法进行了对比实验。

Result: 实验证明，该系统在大学校园多场景下能够有效识别和规避各种障碍物，较好提升了自动导航能力；对深度估计算法进行了性能比较，评估了其精度与鲁棒性。

Conclusion: 结果显示，基于摄像头的感知与Frenet-Pure Pursuit规划方法可实现高效避障，为低成本自动驾驶方案提供了有价值的参考。

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [84] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: 本文提出SENTINEL框架，通过无人工标注的方法大幅降低多模态大模型（MLLMs）生成幻觉（内容虚构）的现象，效果显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态理解取得突破，但仍频繁产生与视觉输入相矛盾的幻觉内容。现有幻觉缓解方法计算代价高或训练分布不符，急需高效且实用的新方法。

Method: 提出SENTINEL框架，通过模型输出自举生成偏好对，结合两个开集检测器交叉验证物体存在性，对句子分类（幻觉/非幻觉），然后构建上下文感知偏好数据，最后用句子级的上下文感知偏好损失（C-DPO）训练模型，重点抑制幻觉传播环节。

Result: SENTINEL能使幻觉率降低90%以上，且在多个幻觉检测基准及通用能力基准上优于现有最先进方法，展现了卓越性能和泛化能力。

Conclusion: SENTINEL框架无需人工标注，能有效抑制多模态大模型幻觉，提升模型可靠性，并为相关领域提供了实用工具与资源。

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [85] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新方法RadGazeIntent，用于建模放射科医生在医学影像解读中的注视意图。该方法结合时空注视数据以理解并预测医生在图像审查中的诊断目标和意图，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的模型难以捕捉放射科医生注视点背后的具体意图，而医生在解读医学影像时存在有目的、结构化的扫视行为。本研究希望弥补这一空白，通过更好地理解医生的解读意图提升模型性能。

Method: 作者提出了基于Transformer的深度学习架构RadGazeIntent，能同时处理注视数据的时序与空间特征，将细粒度注视信息转化为诊断意图的高层次表达。通过对医学视线跟踪数据进行再处理，构建了三类具有意图标签的数据子集，并在这些数据集上进行了实验。

Result: 实验证明，RadGazeIntent能够有效预测放射科医生在特定时间注视的医学发现点，且在所有带有意图标签的数据集上均优于对比方法。

Conclusion: RadGazeIntent为理解和建模医学影像解读中的意图驱动行为提供了新思路，有助于提升相关辅助诊断技术的智能化和准确性。

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [86] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: SpatialTrackerV2是一种用于单目视频的前馈式3D点追踪方法，将点追踪、单目深度与相机位姿估计三者统一为一个高效的端到端跟踪系统，显著提升了准确率与速度。


<details>
  <summary>Details</summary>
Motivation: 现有的3D点追踪方法往往基于模块化方案，依赖现成组件，难以充分利用点追踪、深度估计和相机位姿之间的内在联系，导致准确率和效率受到限制。因此，作者希望提出一种能统一三者、性能更佳的新方法。

Method: SpatialTrackerV2将世界空间的3D运动分解为场景几何、相机运动和逐像素的物体运动，采用全可微、端到端的神经网络架构，能够在大规模异构数据（合成、RGB-D、无标注视频）上进行高效训练，实现整体联合学习。

Result: SpatialTrackerV2在各种数据集上训练后，相比已有3D追踪方法提升了30%的准确率，同时追上主流动态3D重建方法的精度，并且推理速度快50倍。

Conclusion: SpatialTrackerV2通过把3D点追踪相关任务端到端统一，充分利用多源信息，实现了比当前方法更高的准确率与显著的速度提升，在单目3D点追踪领域树立了新的标杆。

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [87] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本论文提出了一个大规模的人类行为分析基准MMHU，收集和标注了人类在驾驶场景下的多维度行为数据。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶领域虽然涉及了人类行为的一些方面，但缺乏专门评估人类行为理解的综合基准，这限制了安全驾驶系统的研究和发展。

Method: 作者构建了MMHU数据集，包含57000个人体运动片段和173万帧图像，数据来源广泛，包括Waymo等公开驾驶数据、YouTube自然场景视频及自采集数据。通过人机协作标注流程，丰富了行为描述和标签。

Result: 作者对数据集进行了详细分析，并针对多项任务（如运动预测、运动生成、行为问答等）设立了基准测试，为人类行为理解研究提供了广泛的评估平台。

Conclusion: MMHU填补了自动驾驶领域人类行为理解评测的空白，为推进自动驾驶安全和人机交互等相关研究提供了有力工具。

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [88] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: 本文提出了一种名为CytoSAE的稀疏自编码器，用于医学影像中的血液细胞图像分析，不仅提升了解释性，还在多任务表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着医学影像中基础模型的增多，对其推理过程的解释性需求提升，但缺乏有效解释工具。稀疏自编码器在视觉领域的应用为解释Transformer模型带来新契机，作者希望填补医学影像领域解释性工具的空白。

Method: 作者提出并训练了CytoSAE稀疏自编码器，利用4万多张外周血单细胞图像，探索其在不同医学图像领域（包括域外如骨髓细胞学）的泛化能力，并结合医学专家对生成的形态相关概念进行验证。此外，还探索了生成针对特定患者和疾病的细胞特征概念。最后，将这些概念用于AML亚型分类任务，并与最新方法进行性能对比。

Result: CytoSAE能泛化到不同和域外数据集，能找出有医学意义的形态学概念，并通过医学专家验证。在AML患者分型任务中，利用CytoSAE提取的概念，取得了与当前主流方法相当的分类性能。

Conclusion: CytoSAE不仅提升了医学影像基础模型的可解释性，还兼具高性能和临床相关性，为医学影像分析提供了新的工具。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [89] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 现有3D生成主要关注几何和纹理，忽视了物理属性，导致合成3D资产不适合现实应用。本文提出PhysX，一个端到端的物理感知3D资产生成方法，并构建了物理标注数据集PhysXNet，以及基于物理知识的生成框架PhysXGen，实现了物理合理的高质量3D生成。


<details>
  <summary>Details</summary>
Motivation: 3D资产用于实际物理场景（如仿真、具身AI）时，除了要具备形状和纹理，物理属性（如尺度、材料、功能）同样重要。现有生成方法忽略物理属性，限制了3D资产的现实应用。

Method: 1. 构建PhysXNet数据集，首个全面标注物理属性（尺度、材料、可供性、运动学、功能描述）的3D数据集，利用人机协作和视觉-语言模型标注。2. 提出PhysXGen框架，通过双分支结构，显式建模3D结构与物理属性之间关系，从图像生成内在物理可控的3D资产。

Result: PhysXGen在物理属性预测和3D资产几何保真度上表现优异，实验验证了其性能和泛化能力。

Conclusion: PhysX实现了物理感知的3D资产生成，为未来物理智能生成研究奠定基础，相关代码、数据全部开源，推动该领域发展。

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 论文将大型语言模型（LLMs）作为“主观文学批评家”来评估科幻文学作品，发现不同模型在评判文学美学时表现出类似于人类批评学派的独特评价特征。


<details>
  <summary>Details</summary>
Motivation: 近年来，LLMs在文本分析和生成中表现出色，但它们在文学艺术领域，尤其是评价主观性极强的文学作品方面能否表现出人类般的鉴赏和批评差异性，有待深入探究。

Method: 作者选取了10篇日文科幻短篇小说，翻译成英文后让6个先进LLM在七次独立会话中进行文学评价。采用主成分分析和聚类方法分析了LLMs的评价模式和一致性，并用TF-IDF方法分析不同LLM使用的评价词汇。

Result: 主成分分析和聚类揭示了LLMs在文学评价一致性上的显著差异（α在1.00至0.35之间），并识别出5种不同的评价模式。不同故事的评价方差最多相差4.5倍，TF-IDF分析发现不同LLM有各自独特的评价词汇。七次会话的设计有效减少了外部偏差，便于观察LLMs在RLHF训练下形成的隐性价值体系。

Conclusion: LLMs在文学评价上展现出主观性和“批评风格”，类似于人类文学批评学派，而非作为绝对中立的评价工具。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [91] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本文提出了一个多样化的地图可视化问答（Map-VQA）数据集，并对多种多模态大模型在六类地图分析任务上的表现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有Map-VQA研究仅关注于色斑图（choropleth map），覆盖的分析任务和主题有限，缺乏针对其他常见地图类型的系统性基准和分析。

Method: 作者构建了MapIQ数据集，包含14,706组问答，涵盖三种地图类型（色斑图、卡托图和比例符号图）和六大主题。对多个MLLM进行性能测试，并设计实验考察地图设计变化对模型表现的影响。

Result: 多模态大模型在不同地图类型和分析任务上的表现各异，部分情况下与人类基线有差距。模型对地图设计的改动表现出一定敏感性和局限性。

Conclusion: 研究表明现有MLLM在地图可视化分析任务中尚有提升空间，尤其是在处理多样化地图类型、适应设计变动及利用地理知识方面。MapIQ数据集为未来改进Map-VQA模型和研究提供了新的基准和方向。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [92] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 本研究探讨了结合小样本学习与增量学习，实现低资源语言波斯语的跨语言情感分析，并取得了高达96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为低资源语言，情感分析训练数据稀缺，如何利用高资源语言的知识迁移到波斯语，提升分析准确性，是该研究的主要动机。

Method: 选用XLM-RoBERTa、mDeBERTa和DistilBERT三种多语言预训练模型，结合小样本学习（few-shot learning）与增量学习方法，在Twitter、Instagram、Digikala、Snappfood和Taaghche等多来源波斯语数据上进行微调。

Result: 实验显示，mDeBERTa和XLM-RoBERTa模型在波斯语情感分析任务上表现优秀，准确率高达96%。

Conclusion: 小样本学习与增量学习结合多语言预训练模型能够有效提升低资源语言的情感分析表现。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [93] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: 本文提出了一种名为PgM的多模态学习框架，通过分割单模特征和配对模特征，提升了多模态学习的表现，并在多项任务中验证了其有效性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习广泛应用，但现有方法未能充分区分和深度利用单模态（只依赖单一模态）与配对模态（依赖模态间交互）两种特征。针对这一问题，作者希望更彻底地学习和调控这两类特征，以提升模型性能和适应性。

Method: 作者设计了PgM框架，包含模态分割器、单模学习器、配对模学习器和单-配对模态解码器。框架首先使用分割器将模态特征划分为单模和配对模特征，然后分别通过两个专用学习器进行针对性学习，最后利用解码器重建整体特征。此外，PgM允许对两类特征的分布和学习率进行灵活调整，以适应不同任务。

Result: 在四项多模态任务上的实验显示，PgM在性能上优于现有方法，并表现出良好的迁移能力。论文还通过可视化展示不同特征在各任务中的分布和作用。

Conclusion: PgM能更有效地分离和利用多模态特征，适用范围广，能为多模态学习任务带来实用提升，并对特征分工提供了新见解。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [94] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: ExpliCIT-QA 系统是一种多模块的多模态问答系统，能够处理复杂表格图像，并针对每一步推理过程给出可解释的答案。其透明可审计，帮助提高金融和医疗等领域的结果可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端表格视觉问答（TableVQA）系统存在解释性不足的问题，难以应用于对可审计性要求高的领域如金融和医疗。作者旨在通过增强系统的可解释性与透明度来弥补这一缺陷。

Method: 系统采用模块化结构，包括：1）多模态表格理解，利用链式思维提取并转换表格图像内容；2）基于语言的推理，以自然语言逐步生成解释；3）自动代码生成，将推理步骤转为Python/Pandas脚本并实现异常处理；4）代码执行，得到最终答案；5）自然语言说明如何得出答案。所有中间结果和代码均可追溯。

Result: 在TableVQA-Bench基准测试上，与现有基线方法对比，ExpliCIT-QA在可解释性和透明度方面表现更优。

Conclusion: ExpliCIT-QA 改善了表格问答系统的解释性和可审计性，为高敏感领域的实际应用（如金融和医疗）提供了更安全可靠的解决方案。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [95] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: 该论文提出了一种结合浅层语法分析和大语言模型（LLM）的策略，提升了对Python数据科学/机器学习notebook中信息流与执行依赖的理解。


<details>
  <summary>Details</summary>
Motivation: 在数据科学与机器学习领域，Jupyter Notebook等notebook广泛应用于实验和开发，但理解和复用他人notebook时，数据与软件依赖常难以解决，重新执行成本高。单靠LLM分析源码也受限于幻觉与长上下文处理问题，因此需要更有效的notebook理解方法。

Method: 作者提出一种“钳形策略”（CRABS）：首先通过抽象语法树（AST）对notebook做浅层语法解析，初步界定各单元格间的I/O依赖；然后针对解析结果中尚存的歧义，由LLM逐格进行zero-shot推理，最终确定详细的单元输入输出关系，并画出信息流和执行依赖图。

Result: 在50份高赞Kaggle Notebook（含3454个真实单元输入输出）的数据集上测试，CRABS方法能辅助LLM正确解决1425个歧义中的1397个（98%）；整体F1分数为信息流识别98%，执行依赖识别99%。

Conclusion: 结合浅层语法分析与LLM的CRABS策略，极大提升了notebook的信息流和依赖分析准确率，为notebook的可评估、复用和适配提供有力工具。该方法解决了仅用LLM时的理解障碍，适合未来notebook自动化分析与复用场景。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [96] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: 本文介绍AI Wizards团队在CLEF 2025 CheckThat! Lab主观性检测任务上的工作，通过将情感分数融入Transformer模型提升主观/客观句子的分类效果，在多语言和零样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 主观性检测对信息可信性分析至关重要，尤其是在不同语言环境和开放场景下，现有模型在泛化和类别不平衡问题上存在限制。

Method: 提出基于Transformer（如mDeBERTaV3-base、ModernBERT-base、Llama3.2-1B）的方法，将情感分数作为辅助特征与句子向量融合，并通过决策阈值校准解决类别不平衡。训练数据覆盖多语言，最终评估含未见语言。

Result: 情感特征融入显著提升了各语言主观句子的F1分数，团队在希腊语上取得宏F1第一（0.51），整体排名靠前。

Conclusion: 将情感分数与Transformer模型结合能有效增强多语言、零样本下的主观性检测能力，尤其有助于处理类别不平衡问题，具有较强的实际与推广价值。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [97] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本文复现并对比了三项关于大语言模型（LLMs）中事实与反事实信息竞争机制的研究，重点研究注意力头在其中的作用与模式，并发现注意力头在促进事实输出时通常通过通用抑制机制而非选择性机制，其行为表现出领域相关的特异性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛应用，其在处理真实与矛盾信息时的内部机制和可解释性问题引起关注。此前的多项研究在注意力机制如何调解事实与反事实输出方面有不同结论，因此本研究试图复现和整合这些结论，澄清注意力头的实际作用及其领域通用性或特异性。

Method: 复现实验结合三项相关文献的任务与分析方法，采用机械解释性工具（Mechanistic Interpretability tools），主要研究注意力头强度与事实输出比例的关系、不同抑制机制下注意力头的表现，并对模型在不同领域的适应性进行了检测。

Result: 实验发现，注意力头在促进事实输出时，往往通过“通用抑制”机制（如整体抑制copy），而非有选择地抑制反事实；增强注意力头有时还会错误地抑制正确事实。此外，注意力头的表现具有领域依赖性，模型越大，其注意力头越呈现出专门化和对类别敏感的行为模式。

Conclusion: 1）注意力头在事实输出中，主要依赖通用抑制机制，非仅靶向反事实信息；2）此行为具有明显领域依赖性并随模型规模增强专化特征。这些发现进一步丰富了我们对大语言模型内部运作与信息竞争机制的理解，对下一步提升模型解释性与可控性具有意义。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [98] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: 这篇论文针对印度多语言环境下的语言识别任务，发布了包含英语及22种官方印度语言的大型数据集，并提出了强健的基线模型。


<details>
  <summary>Details</summary>
Motivation: 多语言自然语言处理应用激增，准确的语言识别是极为关键的前置步骤。印度众多语言存在词汇和语音相似性，甚至共用文字脚本，给自动化语言识别带来了研究和应用上的巨大挑战。

Method: 作者收集和创建了包括英语与所有22种官方印度语言的带标签数据集，共计23万句子。采用主流的机器学习和深度学习技术，训练了适应噪音环境、短文本、混合语言的基线识别模型。

Result: 发布的数据集丰富，涵盖23万句标注样本。其基线模型达到甚至可与最先进模型媲美的语言识别性能，特别适用于复杂多变和语种混杂的场景。

Conclusion: 该工作显著推动了印度语言识别研究，数据集和模型为多语言处理相关研究提供了有力的资源基础，并且展现了实际应用中的很强适应性。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [99] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 该论文提出一种改进自回归语言模型推理速度和并行能力的新框架，实现多token的同时预测和生成，相比传统方法显著加快生成速度且保证生成质量不下降。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型生成过程严格顺序，一次生成一个token，导致推理速度慢且并行性差，特别是在生成后期，文本语义已经趋于确定时，这一限制尤为明显。

Method: 1) 引入masked-input方法，从同一前缀同时预测多个未来token；2) 采用gated LoRA结构，在不破坏原有模型功能前提下实现多token预测；3) 增加轻量化、可学习的采样模块，保证生成序列连贯；4) 设计辅助损失（如一致性损失），提升多token联合生成准确性和一致性；5) 创新使用speculative generation，未来token呈二次扩展，提高生成效率。通过对预训练模型进行有监督微调，实现以上方法。

Result: 通过上述方法，模型在代码和数学任务的生成速度提升近5倍，在通用对话及知识问答任务上提升约2.5倍，同时生成质量无损失。

Conclusion: 该框架大幅提升了自回归大模型的推理速度和并行能力，实现高效率、低损失的多token预测，对LLM的实际部署和应用具有现实价值。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [100] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文主要围绕PII（个人可识别信息）识别模型的跨领域迁移、多领域数据融合和小样本学习能力进行了分析。


<details>
  <summary>Details</summary>
Motivation: PII识别对于文本自动匿名化至关重要，但现有方法在应对不同领域文本时表现参差不齐，因此需要探索模型在跨领域、融合及小样本情景下的表现。

Method: 文章采用来自医疗（I2B2）、法律（TAB）和传记（Wikipedia）领域的标注语料，评估了PII识别模型在域内表现、跨领域迁移、数据融合和小样本学习四个维度的表现。

Result: 实验发现，法律领域的数据对传记文本有良好的迁移性，而医疗领域的文本对外来迁移表现较差。数据融合的效果依赖领域，高质量识别在低专业领域仅用10%的训练数据即可实现。

Conclusion: PII识别模型的迁移和融合效果明显依赖领域特性，低专业化领域下模型可以用极少数据实现高质量结果，但高专业化领域仍具有挑战。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [101] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文提出COLA-GEC双向框架，实现语法纠错（GEC）与语法可接受性判断（COLA）的知识互补，显著提升两项任务的表现，并在多个多语言基准上达到新水平。


<details>
  <summary>Details</summary>
Motivation: GEC和COLA作为NLP核心任务，虽都依赖于基本语法知识，但各自独立发展。作者希望通过知识共享进一步提升两者的性能。

Method: 提出COLA-GEC框架：一方面用GEC数据增强COLA模型性能，另一方面通过动态损失函数将语法可接受性信号整合进GEC模型，指导纠错输出。

Result: 在多语言基准测试中，所提方法取得了最先进的成绩。

Conclusion: COLA-GEC框架能有效促进两个任务的协同提升。误差分析发现标点纠错仍有难点，为后续研究指明方向。

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [102] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的强化学习框架DualReward用于自动生成完形填空干扰项，并在两类数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的干扰项生成通常依赖监督式学习或静态生成模型，难以区分高质量的人类干扰项和模型生成干扰项，也难以自适应调整学习信号，限制了模型性能。

Method: DualReward采用具备自适应尺度的双重奖励结构，区分金标准（人工）和模型生成干扰项，并根据模型表现和信心动态调整奖励信号。该方法在段落级（CLOTH-F）和句子级（MCQ）数据集上进行了实验评估。

Result: 在同质数据集CLOTH-F上获得了小幅但稳定提升，在多样性更强的MCQ数据集上获得了3.48-3.86%的P@1显著优化，优于最先进基线。

Conclusion: 自适应奖惩机制能平衡借鉴高质量人类示例和探索新颖优质干扰项，从而为自动化测试生成提供了更具灵活性和泛化力的解决方案。

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [103] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: 本文探讨了LLM内部对有害性的理解与其拒绝有害指令行为的差异，提出LLM有独立的“有害性”表征，可以作为检测有害输入的新安全机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能拒绝有害指令，但目前尚不清楚其内部是否真正理解什么是有害。作者希望更深入理解LLM内部处理有害性与拒绝行为的机制，提出新的AI安全分析维度。

Method: 作者通过实验发现LLM内部存在与拒绝不同的“有害性”方向，即有害性被独立编码。通过因果干预，研究有害性和拒绝性的区别，并分析了越狱（jailbreak）方法和对抗微调对内部有害性表征的影响。

Result: 作者发现：1）模型内部有害性表征与拒绝响应方向不同，2）改变有害性方向会影响模型对指令有害与否的判断，而拒绝方向只影响是否拒绝，但不改变有害性判断，3）许多越狱方法仅降低拒绝信号，难以改变有害性内在表征，4）基于内部有害性表征，可实现Latent Guard机制，对抗越狱和微调更为有效。

Conclusion: LLM内部的有害性理解比简单拒绝机制更稳健，Latent Guard能作为本体安全机制应用于检测危险输入，并降低过度拒绝，对AI安全研究和实际部署有重要意义。

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [104] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了Marco-Bench-MIF，一个涵盖30种语言的多语言指令跟随能力评测基准，对20多个大语言模型进行了系统评估，发现不同语种间存在显著性能差异，且机器翻译数据低估了真实准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的指令跟随评测数据集多以英语为主，或仅采用简单的机器翻译方式，难以适应多语言和本地语境下的评测需求。为更全面地评价LLM的多语言能力，需要开发更专业且本地化的数据集。

Method: 作者对原有IFEval数据集进行了本地化扩展，采用混合流程（人工与验证结合）制作了涵盖30种语言、兼顾语言习惯与文化差异的新数据集Marco-Bench-MIF，并对20多个主流LLM进行了全面评测，分析其在不同语种下的表现。

Result: （1）高/低资源语言之间存在25-35%的准确率差距；（2）模型规模显著影响表现，但脚本类型依然带来挑战；（3）机器翻译数据相比本地化数据低估准确率7-22%。还揭示了多语种下的关键字一致性和组合约束等难题。

Conclusion: Marco-Bench-MIF更真实反映多语言场景下LLM指令跟随能力，评价准确性优于单纯机器翻译方案，为未来多语言模型评测和提升提供了可靠基准。

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [105] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题求解中的应用，包括任务综述、方法回顾、评测方法分析及未来挑战讨论，并提供了相关资料列表。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解在教育、AI数学能力评估与多模态能力测试等领域有重要意义。随着深度学习和多模态大模型的兴起，相关研究迅速发展，因此有必要对该领域的进展进行系统梳理。

Method: 本文主要采用文献调研的方法，系统总结了几何问题求解的相关任务，全面回顾了主流深度学习技术，并对各类评估指标与方法进行详细分析，还针对当前主要挑战和未来研究方向展开讨论。

Result: 本文汇总了当前几何问题求解的主要任务类型和最新深度学习方法，归纳了常用的评测指标，并对现有的研究挑战进行了批判性剖析。文末还提供了一个持续更新的相关论文列表（GitHub）。

Conclusion: 深度学习在几何问题求解领域取得了显著进展，但仍面临诸如多模态融合、复杂推理等挑战。本文为该领域研究者提供了全面且实用的参考，有助于推动领域发展。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [106] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 该论文提出了首个涵盖10种语言的大规模多语言图表问答基准PolyChartQA，用于评估多语言下的图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前的图表理解基准数据集主要以英语为主，缺乏对全球多语种用户的适用性和包容性，限制了多语言视觉-语言模型的发展。

Method: 作者设计了一种去耦合的数据生成流程，将图表数据与渲染代码分离，然后用先进的大语言模型翻译数据，保证生成高质量的多语种图表。最终覆盖22,606个图表和26,151个问答对，涉及10种语言。

Result: 实验显示，当前的主流视觉-语言模型在英语上的表现远优于其他语言，尤其是在资源较少、使用非拉丁字母的语言上性能显著下降。

Conclusion: PolyChartQA为推动全球化、具备多语言能力的视觉-语言模型奠定了基础，并突显了当前模型在多语种环境下的局限性。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [107] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE是一种针对GPU优化的BPE分词算法，极大提升了大批量推理任务的分词速度，最高可比现有工具快2~2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有主流分词器（如HuggingFace Tokenizers和tiktoken）主要依赖CPU，而且在GPU批量推理任务上效率低下，Regex预分词导致运算瓶颈，难以满足大规模、高吞吐量应用的需求。

Method: 提出BlockBPE，一种并行GPU实现的BPE分词方法。它去除了Regex预分词这一步（轻微牺牲生成质量），并通过在线程块内高度并行的token合并操作，把总体复杂度从O(nlogn)降低到O(nd)，其中d远小于n。

Result: 在大batch推理任务上，BlockBPE分词的吞吐量比tiktoken高2倍，比HuggingFace Tokenizers快2.5倍，展现出显著的提速效果。

Conclusion: 在大批量GPU推理场景下，BlockBPE可以为主流大模型带来显著的分词速度提升，有助于大规模部署与商业应用。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [108] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种动态注意力感知的任务无关型提示压缩方法（DAC），通过结合信息熵及注意力机制，有效提升大语言模型在长文本场景下的提示压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有任务无关的提示压缩方法过于依赖信息熵，把词语压缩至信息损失最小，但忽略了压缩过程中注意力关键词的作用及信息熵的动态变化，导致表现受限。

Method: 提出DAC方法，结合信息熵和模型注意力信号，在压缩过程中动态感知信息熵变化，对提示内容进行细粒度、动态优化的压缩，从而在保留关键信息的同时缩减冗余。

Result: 在LongBench、GSM8K、BBH等多领域数据集和多种大语言模型上，DAC方法较现有方法实现了更稳健、显著的性能提升。

Conclusion: DAC方法在压缩提示信息的同时更好地保留任务所需的关键内容，为长上下文场景下的提示优化提供了高效有效的新路径。

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [109] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖方法，通过小型和大型LLM间注意力矩阵映射，显著加速推理前置阶段并减少缓存消耗，提升大模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长上下文时资源消耗巨大，尽管已有方法优化推理效率，但主要聚焦于模型内部稀疏性，未充分利用外部信息。作者发现不同规模LLM之间的注意力矩阵高度相似，这为优化提供了新视角。

Method: 作者首先系统性分析了注意力矩阵相似性的度量方式、映射层选择及映射一致性问题。基于分析结果，提出IAM框架，通过在小型和大型LLM之间进行注意力映射，实现了注意力计算加速和KV缓存占用减少。

Result: 实验结果显示，IAM框架能在几乎不损失模型性能的情况下，使预填加速15%，KV缓存占用减少22.1%。进一步在多种型号LLM上验证了该方法的通用性，同时与多种已有KV缓存优化技术兼容。

Conclusion: 本文方法通过跨模型注意力映射，有效提升了大模型的推理效率。IAM不仅提高计算速度、降低内存消耗，还能与其他优化技术协同，为LLM高效推理提供了新工具。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [110] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文提出了一种针对WikiData的多阶段查询型知识图谱问答框架，特别提升了多跳推理与时序问题的能力，对实体链接与谓词匹配采用了链式思维（CoT）技术，小模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在问答任务中能力强大，但仍然难以胜任多跳推理和处理时序类复杂问题，现有直接生成答案的方式存在透明性和可控性不足的问题。基于知识图谱生成可执行查询的方式具备更强的可解释性与可控性，有望突破上述瓶颈。

Method: 作者设计了一个多阶段的查询式知识图谱问答（KGQA）框架，第一步分阶段拆解和生成中间查询，第二步提出了结合链式思维推理（CoT）的实体链接和谓词匹配方法，并在多跳与时序问答基准集上展开稳健性和泛化性实验对比。

Result: 实验表明，多阶段KGQA方法在多跳和时序WikiData问答数据集上效果显著优于基线方法，尤其是对于小型语言模型，准确率和鲁棒性均有较大提升。

Conclusion: 多阶段查询式KGQA框架及其创新实体链接、谓词匹配方案能够显著提升小参数模型在多跳和时序推理问答中的表现，对复杂问答处理具有较大应用前景。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [111] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 本文提出了一种新的适用于大语言模型（LLM）权重的POT（power-of-two）量化框架，在极低精度下提升准确率，并加速了GPU上的去量化推理速度。通过两步式后训练算法优化量化尺度，实现了更高的推理性能和准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然POT量化是一种降低大模型推理资源消耗的通用方法，但现有POT量化在GPU上的效率和低精度下的准确率并不理想，限制了其实际部署价值。

Method: 本文提出了新型POT量化框架，并设计了两步式后训练算法：首先用稳健方式初始化量化尺度，然后利用极小规模校准集进行细化。该方法针对极低精度（2/3比特）优化了模型权重的量化与去量化流程。

Result: 提出的POT后训练量化在2/3比特等低精度下准确率优于现有SOTA整数量化方法。在NVIDIA V100与RTX 4090上去量化步骤分别实现了3.67倍和1.63倍的加速。

Conclusion: 新方法在保证极低精度高准确率的同时，大幅提升了GPU去量化速度，为大语言模型高效部署提供了可能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [112] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出了一个两阶段的翻译框架，使翻译系统能够保留低资源语言（如新加坡式英语Singlish）中有害言论的细微差别，包括俚语和文化因素，提升了跨语言安全与内容审核的能力。


<details>
  <summary>Details</summary>
Motivation: 随着网络交流中低资源语言和方言的增多，标准翻译系统很难保留本地俚语、夹杂用语和有害言论的文化特性，特别是在恶意言论的检测和内容审核中存在较大挑战。因此，开发能更好地处理和翻译这些内容的系统非常必要。

Method: 作者提出了两阶段的翻译框架。首先，采用人工验证的few-shot提示工程，精心选择包含丰富俚语和有害言论特征的示例，并对其进行排序。其次，通过语义相似度和直译—回译的对比，优化不同大模型的提示组合，并用定量人工评估验证翻译结果的有效性。以新加坡式英语的数据集为例进行了实证。

Result: 经过人工量化评估，提出的框架在保留有害言论的同时，提高了低资源语言的翻译效率和质量。能有效检测和翻译多文化、夹杂语言中的有害内容。

Conclusion: 该框架不仅提升了翻译有害内容的质量，还有助于多文化大型语言模型的安全管理和内容审核，强调了保持社会语言细微差别对于实际平台治理的重要性。Singlish被用作测试场景，突出了包容性NLP在实际内容审核应用中的意义。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [113] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: 研究比较了人类与大型语言模型（LLM）在阅读理解和信息处理过程中的异同，特别关注在图结构语义表示下，两者对关键信息的关注分布。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，需要深入探究其与人类在不同语境下理解和应用语言的方式差异，尤其是在实际功能任务如推理、情感理解和信息检索方面的表现。本研究希望进一步解决仅基于单词层面分析带来的理解局限。

Method: 本研究借助LLM驱动的AI智能体，将文本按语义和问题导向分组，构建图结构（节点、边）进行表示。随后，通过眼动追踪，比较人类注视分布与LLM判定重要节点和边的匹配情况。

Result: 结果发现，LLM在图拓扑结构层面上与人类在语言理解表现出高度一致性，尤其体现在对关键信息节点和边的关注分布上。

Conclusion: 研究深化了人类与AI共同学习的理解机制，提出图结构有助于探索和提升人机在阅读理解中的协同潜力。

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [114] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: 该论文探讨了在由大语言模型（LLMs）驱动的AI代理社会中，价值观相似性是否会影响信任与人际亲密关系，结果显示价值观相似的代理之间的信任与亲密度均更高。


<details>
  <summary>Details</summary>
Motivation: 在现实人类社会中，价值观的相似性能够促进信任与亲密关系，但该现象在由LLM代理组成的人工社会中尚未被验证。作者旨在研究AI代理间价值观相似性是否也会影响其关系构建。

Method: 首先进行初步实验，测试不同LLM模型和提示设计对控制代理价值观的有效性。然后在主要实验中，让被赋予特定价值观的代理两两对话，并分析其在对话后对彼此信任和亲密感的评价。实验分别在英语和日语环境下进行，以考察语言影响。

Result: 实验结果显示，价值观相似的LLM代理对彼此的信任和人际亲密感均高于价值观不相似的代理。该现象在不同语言环境下都得到了验证。

Conclusion: LLM代理模拟为社会科学理论提供了有价值的测试平台；该研究揭示了价值观影响AI代理关系构建的机制，也为社会科学理论创新提供了基础和启示。

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [115] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在针对不同群体（如普通用户、儿童、语言学习者）简化同形异义词（多义词）定义时的表现，发现简化会导致定义完整性下降、误导用户，并通过微调有效提升了定义质量。


<details>
  <summary>Details</summary>
Motivation: 同形异义词有多个含义，过度简化定义可能遗漏关键信息，从而误导依赖LLM输出的用户，尤其是对学习者等特殊群体。作者关注如何在简化表达和覆盖完整意义之间取得平衡。

Method: 作者构建了跨多语言的两个新评测数据集，针对三类目标群体（普通、简易、ELI5）测试了多个主流大型语言模型（如DeepSeek v3、Llama 4 Maverick、GPT-4o等），采用LLM模型和人工标注两种方式评价定义质量，并通过DPO微调Llama 3.1 8B模型提升表现。

Result: 实验结果表明，简化定义会显著降低多义词定义的完整性，增加误解风险；但通过DPO微调后，Llama 3.1 8B在所有提示类型下的表现均显著提升。

Conclusion: 研究强调在教育NLP领域，简化与完整性需要平衡，以确保所有学习者都能获得可靠且有上下文的词义解释。建议未来模型开发要重视这一问题，兼顾易懂性和信息准确传达。

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [116] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: 本论文围绕神经语言模型在数据和参数效率上的挑战，提出了基于表征分析和新型优化方法的大幅改进，显著提升了模型性能和泛化能力，尤其在低资源场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决神经语言模型对大量标注数据和高参数量的依赖，提升其在低标注、动态环境中的鲁棒性与普适性。

Method: 方法包括：（1）基于表征平滑性的正则化策略，利用Jacobian和Hessian矩阵减少模型对输入扰动的敏感性；（2）结合主动学习和高效参数微调，通过表征平滑性分析指导，无需标注验证集的Early Stopping技术；（3）利用in-context learning的弱监督方式，挖掘无标注数据提升泛化。

Result: 实验显示，所提方法在多项NLP任务中相较传统方法显著提升了性能、稳定性和效率，尤其在低资源和数据动态变化场景下表现尤为突出。

Conclusion: 结论是，通过表征分析和优化，结合主动学习及弱监督机制，可显著提升神经语言模型的数据和参数效率，减小对标注数据和计算资源的依赖，增强了模型的泛化与鲁棒性。

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [117] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: 本文提出了一种通用的语言创造力测试方法，比较了人类与大语言模型（LLMs）在新词生成和隐喻语言运用等方面的创造力表现，并量化分析了各自的优势。


<details>
  <summary>Details</summary>
Motivation: 当前对AI语言模型创造力的评估缺乏通用、定量化的标准，尤其是在新词构造和隐喻创新等领域。因此，作者希望开发一套既适用于人类又适用于LLM的创造力测试体系，从而更科学地对比人工与智能在复杂语言任务上的能力差异。

Method: 设计一份包含新词生成（派生与复合）及隐喻使用的多任务语言测试。测试对象包括24位人类和同等数量的LLM实例，使用OCSAI工具从原创性、阐述性与灵活性三个维度自动评估答案，并统计答案唯一性，同时辅以简要人工分析。

Result: LLMs在全部评估指标上均明显优于人类，被测的8项任务中有6项都表现更佳。唯一性分析则显示人类与LLMs在答案多样性上只有细微差异，且人工分析发现人类更擅长扩展型创造力，而LLMs更倾向于固定型创造力。

Conclusion: 大语言模型在多项语言创新能力测试中已超越人类，尤其在标准化任务中表现突出，但在人类擅长的扩展性创新方面仍有差距。这为AI创造力研究及相关评测方法提供了重要参考。

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [118] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: 本文测试了28个大型语言模型在理解和推理“方位词”（如东南西北方向）能力，通过模版化数据全面评估模型在不同场景下的表现。结果表明，即便是最新的大型推理模型，在这类任务上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 近年来，尽管大型语言模型在多种NLP任务中表现优异，但其在空间推理、尤其是方位词推理领域的能力尚未被系统评估。该领域对现实应用和认知建模有重要意义。

Method: 研究团队基于一套模板系统，生成包含多样化要素（如移动方式、主语视角等）的方位推理题，对28种大型语言模型进行统一评测。

Result: 实验表明，无论是传统LLMs还是最新的推理类大型模型，在全部方位推理问题上都无法做到完全正确，表现存在明显局限。

Conclusion: 当前的大型语言模型在方位推理与空间认知类任务上依然不足，相关领域仍需进一步研究创新，以提升模型的空间理解和推理能力。

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [119] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 该论文提出了一种基于模块化风格分析的AI检测方法，利用spaCy进行文本预处理和特征提取，并结合LightGBM进行分类，依靠超50万机器生成文本进行训练。


<details>
  <summary>Details</summary>
Motivation: 在当前AI生成文本迅速发展的背景下，有效区分人类与AI生成内容至关重要。现有方法多依赖深度学习，但往往成本较高且缺乏可解释性，因此作者试图用非神经、低计算量且可解释的方法解决这一问题。

Method: 1. 使用spaCy对文本执行分词、实体识别、依存句法、词性标注和形态分析；2. 提取数千个语言学特征（如上述注释的n-gram频率）；3. 利用LightGBM提升树做二分类；4. 构建超50万条机器生成文本数据集用于模型训练，并尝试多种参数及模型容量扩展。

Result: 经过大量机器生成数据训练后，模型在AI生成文本检测任务中取得了有效结果，并验证了非神经、可解释方法在该任务上的可行性。

Conclusion: 该非神经、模块化风格分析流水线，既高效又易解释，在大规模训练语料支持下，对AI生成文本有较强识别能力，为相关任务提供了一种成本较低但效果良好的解决方案。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [120] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文提出了首个面向“整本书”级别的指代消解数据集BOOKCOREF，并基于自动化流程对完整叙事文本进行高质量标注，显著提升了超长文档上指代消解系统的评测与研究能力。


<details>
  <summary>Details</summary>
Motivation: 现有指代消解评测集文档长度有限，无法有效反映模型在超长文本（如整本书）上的性能，缺乏相关评价资源和基准。

Method: 作者设计了一个用于整本书级别文本的自动化高质量指代标注流程，并据此构建了平均长度超20万token的新数据集BOOKCOREF。

Result: 实验结果显示，该自动流程具有较强的稳健性，利用BOOKCOREF可以帮助现有长文档指代系统在整本书评测时获得高达+20个CoNLL-F1分的提升。同时，结果也揭示了在超长文档场景下现有模型远不及中短文本的性能。

Conclusion: BOOKCOREF为指代消解领域研究超长文本的挑战和模型改进提供了新资源，望推动相关技术的发展，数据和代码已开源。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [121] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: 本文提出了利用Socratic method、链式思维（CoT）、简化游戏化以及形成性反馈结合的大语言模型（LLM）辅助数学学习方法（称为MEGA），并与传统CoT方法进行对比，发现MEGA在提高大学生数学理解与学习体验上更具优势，尤其在处理更难的数学问题时表现显著更好。


<details>
  <summary>Details</summary>
Motivation: 许多学生因为数学基础薄弱或教学方法不佳，导致对数学学科产生回避，严重影响跨学科发展。本研究针对教学方法进行改进，以提升学生的数学学习成效。

Method: 将学生随机分组，对比MEGA方法与传统CoT方法，运用GSM8K和MATH两个数据集，从每个数据集随机抽取样本，共涉及两种主流LLM（GPT4o与Claude 3.5 Sonnet），分析学生对于不同方法下的数学学习体验。

Result: 研究结果显示，在两个数据集上，学生普遍认为MEGA方法在学习体验上优于传统CoT方法，尤其是在难度更高的MATH数据集上，MEGA的优势更为明显（47.5%对26.67%）。

Conclusion: 结合苏格拉底法、链式思维、游戏化与形成性反馈，基于AI大语言模型的MEGA方法能有效改善大学生数学学习体验，尤其在理解复杂问题时优势突出，具备在数学教学中推广的应用前景。

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [122] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估大语言模型（LLM）文本数据增强的新方法，解决了现有技术在大规模和迭代生成下语义保持差的问题，并在实际任务中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 传统的数据增强方法在文本生成过程中难以保证语义的一致性，尤其是在低资源场景和大规模/多次生成时易产生冗余和语义漂移，影响NLP模型的任务表现。

Method: 提出了包含两部分的评价框架：（1）可扩展性分析——当增强数据量增加时监测语义一致性；（2）带摘要优化的迭代增强（IASR）——通过递归释义和摘要来评估语义漂移。实验评估了多种主流LLM模型性能。

Result: 实验表明，GPT-3.5 Turbo 在语义保真度、多样性与效率之间取得了最好的平衡。在实际主题建模任务中，实现了主题粒度提升400%，并彻底消除了主题重叠。

Conclusion: 提出的方法有效提升了LLM数据增强的结构化评估能力，并在NLP实际 pipelines 中具有明显实用价值。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [123] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT 是一项用于评估生成式语言模型的高层次标准的共享任务集，其中 Sensemaking 任务包括生成问题、回答问题以及对回答进行评分。本文报告了 2025 年 Sensemaking 比赛的流程与发现，涵盖多源多语种材料，并探讨了 LLMs 在不同环节的表现及挑战。


<details>
  <summary>Details</summary>
Motivation: 目前，缺乏统一、可测试和高层次的标准来系统性地评测生成式语言模型的理解与推理能力。ELOQUENT 旨在通过模拟课堂问答的方式，为模型评测提供更实用、可量化的场景。

Method: Sensemaking 任务分为三步：1）教师系统基于指定材料生成问题；2）学生系统回答问题；3）评估系统对答案打分。2025 年竞赛中，涵盖了多种类型的测试材料（事实核查、教材、讲座转录和教育视频），并支持多语言（英语、德语、乌克兰语和捷克语）。本次有 4 支队伍参赛，额外引入了商用大模型作为基线，并设计了自动与极简人工评估的对比实验。

Result: 在“出题”任务上，现有方法难以区分问题质量，评估标准有待改进。在“答题”任务上，受控文本条件下 LLMs 表现尚可，但对输入材料的约束性不够。在“评分”任务中，发现基于大模型评判的系统会错误地将无关或混乱的问题-答案配对判为合理，存在判别漏洞。

Conclusion: ELOQUENT 为语言模型评测提供了新框架，但现阶段三项任务（出题、答题、评分）仍面临评估标准欠成熟、文档依赖薄弱和自动评判易被攻击等挑战。后续需完善题目质量评估和更鲁棒的评分系统。

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [124] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: 本文提出了行为翻译风格空间（BTSS），用于描述和模拟翻译过程中的行为模式，并通过多层嵌套结构组织这些模式。


<details>
  <summary>Details</summary>
Motivation: 当前翻译过程的行为特征（如按键和注视等数据）虽可观察，但其隐藏的认知与情感机制尚未系统建模与揭示。该文希望通过行为数据分析，更深入理解翻译的认知与情感动态。

Method: 通过分析翻译过程中的按键轨迹与眼动数据，挖掘出行为背后的隐性认知结构，并将其构建为多层嵌套的行为风格空间（BTSS）。

Result: 成功将行为模式分层组织，构建了可反映情感、自动化行为和认知变化的行为翻译风格空间，为理解翻译动态提供了模型基础。

Conclusion: BTSS能够作为计算翻译代理的基础，模拟人类翻译过程中情感、行为和认知的时间动态，为翻译过程研究与自动化翻译系统的发展提供理论与方法支持。

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [125] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: 本文提出了一种无需自动语音识别（ASR）的儿童孤立单词朗读评估方法，通过对比儿童语音与少量成人模板进行评估，使用大规模自监督学习模型的中间特征，但在儿童语音任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏环境下，缺乏用于训练ASR系统的数据，且评估儿童朗读能力需求很大，因此亟需简单、实用、少样本的方法。

Method: 采用少量成人录音作为模板，将输入儿童语音和成人模板都编码为大规模自监督语音模型的中间层特征，并对比其相似性，探索特征离散化和模板重心平均等设计。

Result: 在理想化实验中，对成人语音测评表现尚可，但对儿童输入（即使采用儿童模板）表现大幅下降。

Conclusion: 自监督模型特征虽对低资源语音任务有帮助，但在儿童语音的少样本分类中效果有限，暴露出现有方法的不足。

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [126] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出了一种结合 Token 级与短语级融合的大模型辅助端到端语音识别方法，实现了对专有名词等关键词更精确的识别，同时保持总体转写准确率。


<details>
  <summary>Details</summary>
Motivation: 端到端ASR模型在识别一般语音时表现良好，但对关键词（如人名、地名等）识别效果较差。以往融合关键词词典的方法存在操作粒度不一致的劣势，影响性能。

Method: 提出多粒度融合方法，将Token级（逐字引导）与短语级（直接复制短语）两种机制与大语言模型（LLM）结合，并采用后融合策略同时兼顾声学特征与上下文信息，在保持细粒度精准的同时提升整体理解。

Result: 在中英文数据集上，方法在与关键词相关的指标上取得了SOTA性能，同时在其他非关键词文本的转录准确率也保持较高水平。消融实验证明Token级和短语级组件均对性能提升有重要贡献。

Conclusion: 多粒度融合框架能够协同提升ASR模型对关键词的识别能力与整体准确率，为场景化ASR应用带来更优表现，代码和模型将开源。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [127] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的定量测量翻译腔（translationese）的方法——T-index，通过对比微调的语言模型的似然比来计算。T-index 能有效、泛化地衡量文本的翻译腔水平，并能较好地反映人工标注结果。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译质量评价方法（如BLEU、COMET）很难刻画和量化“翻译腔”现象，即译文偏离自然或本地化表达的程度，影响机器翻译的质量理解，因此需要提出一种新的可量化翻译腔的方法。

Method: 作者提出并定义了Translationese-index（T-index），利用两个对比微调的语言模型，对输入文本的可能性进行打分，取两种模型的似然比作为T-index指标。该方法在合成数据和真实世界翻译数据上进行了实验以验证其跨领域泛化能力和与人工评判的一致性。

Result: T-index在两个0.5B参数的语言模型，仅用1-5k对合成数据微调后，就能在真实世界翻译场景中有效捕捉翻译腔现象。T-index与人工判别结果高度一致（相关性r=0.568），能够预测人工获得的成对翻译腔标注。同时，T-index与现有MT质量评价指标（如BLEU、COMET）相关性低，表明其测量纬度互补。

Conclusion: T-index是一种鲁棒、高效且可泛化的翻译腔测量指标，能够补充现有机器翻译质量评价工具，帮助更全面地理解和评判译文的自然性及本地化水平。

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [128] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: 本文提出了一个新的端到端框架Infherno，用于自动将临床自由文本转化为结构化FHIR资源，实现更高效的医疗数据集成和互操作。该方法结合LLM智能体、代码执行和医学术语数据库工具，在结构符合性和通用性上优于以往方法，表现接近人工基线。


<details>
  <summary>Details</summary>
Motivation: 以往自动化将临床自由文本转换为FHIR资源的方法，如基于规则系统或LLM结合指令调优，存在结构不一致和泛化能力差的问题。当前医疗数据集成和互操作性需求强烈，急需更鲁棒和标准化的自动转换方案。

Method: 提出了Infherno框架，集成大型语言模型(LLM)智能体、代码执行功能与医疗术语数据库工具，确保生成结果符合FHIR文档结构要求。系统具有前端界面，支持定制与合成数据，兼容本地与商用模型。

Result: Infherno能够有效地从非结构化文本预测出标准FHIR资源，其性能媲美人工基线，并能够适应多样化的数据输入，满足临床数据集成与多机构互操作的需求。

Conclusion: Infherno框架在FHIR标准结构化转换任务上表现优异，有望推动临床数据自动化处理和跨机构互操作性的进步，对智慧医疗领域具有广泛应用前景。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [129] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 本文提出了一个文本异常检测的评测基准，系统对比了多种预训练语言模型（包括主流大模型）在多领域文本上的异常检测效果，并开源了相关工具包。实验发现，深度方法与传统浅层方法在使用大模型嵌入时效果无明显差异，嵌入质量是关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准且全面的文本异常检测评测基准，导致方法难以严谨对比与创新发展，尤其是在大语言模型蓬勃发展的背景下，亟需权威的基准和系统性分析。

Method: 作者构建了覆盖新闻、社交媒体、学术出版等多领域的数据集，采用GloVe、BERT、LLaMA等多种预训练语言模型生成文本嵌入，并通过KNN、Isolation Forest等浅层与深度方法比较异常检测性能。采用多种评价指标（如AUROC、AUPRC）系统分析方法表现，并挖掘性能矩阵的低秩特性用于模型快速评估选型。

Result: 实验显示基于大模型的嵌入驱动异常检测时，传统和深度方法效果无统计学差异，主要决定因素是嵌入质量。同时，发现模型表现矩阵具有显著低秩结构，可加速模型与嵌入的实用评估。

Conclusion: 本工作首次为文本异常检测提供全面评测基准，揭示了嵌入质量关键性，并证明传统算法依然具实用价值。开源工具包将为后续研究奠定基础，推动该领域发展。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [130] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 本研究评估了主流代码大模型在VHDL代码生成与摘要任务上的表现，并提出了“描述链（CoDes）”方法，显著提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 代码大模型在硬件描述语言（HDL），尤其是VHDL领域的应用和优化研究较少，现有模型在相关任务中的表现不佳，存在改进需求。

Method: 使用两个VHDL相关数据集（VHDL-Eval和VHDL-Xform）系统性评测主流代码大模型在VHDL代码生成与摘要中的表现，并提出Chain-of-Descriptions（CoDes）方法，通过让大模型先生成中间描述步骤，强化模型对任务的理解，再综合输入生成最终结果。

Result: 实验显示，现有大模型在各项指标上表现远不理想；引入CoDes方法后，模型在两个数据集上的指标均有大幅提升，明显优于标准提示。

Conclusion: 当前大模型并不适用于VHDL等HDL任务，CoDes方法为提升LLM在VHDL代码生成与摘要任务中的表现提供了有效途径，并为后续HDL代码任务AI研究奠定了基础。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [131] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: 本研究发现阿尔茨海默病（AD）语音感知中存在性别偏见，尤其在中文语境下更为突出，且与语音声学特征相关。


<details>
  <summary>Details</summary>
Motivation: 语音感知任务中常见性别偏见，这可能影响AD的检测与判别。本研究旨在揭示AD语音感知中性别偏见的存在及其机制。

Method: 通过16名中文受试者对中文与希腊语的AD语音进行感知实验，结合声学特征分析，探讨性别、语言及声学指标对AD判别的影响。

Result: 结果显示，男性语音更常被误判为AD，这一偏见在中文中尤为显著。声学分析发现，男性语音中的shimmer值与AD感知显著相关，语音部分与AD判别呈负相关。语言类型对判别无显著影响。

Conclusion: 研究强调在AD语音自动检测模型开发中必须关注性别偏见，并建议在不同语言背景下进一步检验模型性能。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [132] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体辩论框架，通过让多种大语言模型（LLMs）协同辩论，有效提升对用户请求歧义的检测与解决能力，显著优于单模型效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在理解和生成自然语言上表现突出，但在处理含糊、歧义性用户请求时仍有不足。为提升模型对复杂交互中歧义的应对能力，有必要探寻更高效的解决方案。

Method: 作者设计了一个由三种主流LLM（Llama3-8B、Gemma2-9B、Mistral-7B）组成的多智能体辩论框架，并配合多样歧义的数据集进行验证，通过让模型协同讨论、辩论推动共识以解决歧义。

Result: 辩论框架显著提升了Llama3-8B和Mistral-7B的性能，尤以Mistral-7B为主导时效果最佳，达到了76.7%的成功率，尤其针对复杂歧义和高效达成共识具有突出优势。

Conclusion: 多模型结构化辩论能够显著增强LLM理解与澄清歧义的能力，有望推动更健壮、适应性强的交互系统研发。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [133] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: 本研究评估了具备网页浏览能力的大语言模型（LLM）能否通过社交媒体用户名推断用户人口属性，结果发现LLM在一定程度上可以准确访问内容并预测用户特征，但也存在偏见及潜在风险。


<details>
  <summary>Details</summary>
Motivation: 以往LLM只能利用静态训练数据，缺乏对实时信息的获取。随着具备网页浏览功能的LLM出现，研究者希望了解其解析和分析社交媒体数据的能力，尤其是能否仅凭用户名推断人口属性。这对于API限制后的社交科学研究极具研究意义。

Method: 作者采用了两个数据集：一个是包含48个X（Twitter）账号的合成数据集，另一个是由1384名国际参与者组成的问卷数据集。通过让具备网页浏览能力的LLM分析这些账号的信息，测试其识别和推断用户人口属性（如性别、政治倾向等）的准确性。

Result: 实验表明，这些LLM能够访问社交媒体内容并在一定程度上准确预测账户持有者的人口特征。在分析过程中，模型对活跃度低的账号更易引入性别和政治偏见。

Conclusion: 具备网页浏览能力的LLM在社交数据解析方面为社会科学研究提供了新工具，但同时伴随信息操纵、定向广告等风险。作者建议：限制LLM公开应用中的这类能力，仅对经认证的研究用途提供受控访问。

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [134] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本研究探讨了是否能够利用语言模型内部激活状态来检测算术错误。通过对加法问题的实验，发现简单探针不仅能准确解码模型输出及正确答案，还能以高准确率预测模型输出是否正确。进一步扩展到复杂推理链问题，探针具备良好的泛化性。最终，通过探针辅助选择性重启推理步骤，整体任务表现提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在算术任务上容易出错，尤其是在连锁推理和复杂问题中。如果能通过内部激活状态实时检测错误，可以为自我修正和增强模型能力提供技术路径。

Method: 1. 在受控的三位数加法情境下，训练简单探针（probes）从隐藏层状态解码模型输出及正确答案；2. 以此基础训练轻量级误差检测器，判断模型输出准确性；3. 将上述方法扩展至GSM8K中的加法推理链问题，测试探针的泛化能力；4. 使用探针指导对错误推理步骤选择性重新提示（re-prompt），分析对任务准确率的影响。

Result: 1. 简单探针能准确地从隐藏层提取模型输出和正确答案信息，无论模型实际输出对错；2. 误差检测器对模型是否正确判断的准确率超过90%；3. 在更复杂推理链任务中，先前探针能很好泛化，表明内部表征一致性；4. 通过选择性re-prompt纠错，提高了整体任务表现，同时对正确输出影响很小。

Conclusion: 内部激活状态已蕴含丰富算术正误信息，通过设计简单探针可高效检测出错误，并实现低成本的模型自我修正，对后续提升语言模型数学推理能力有重要意义。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [135] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: 提出一种先进的RAG框架，通过混合检索策略和结构化优化，实现企业异构数据的高效问答与信息检索，大幅提升准确性与相关性。


<details>
  <summary>Details</summary>
Motivation: 企业越来越依赖专有数据（如HR、结构性或表格文档）做决策，现有LLM和RAG很难高效处理结构化/半结构化数据，因此需要改进框架以提升企业场景下的检索和生成效果。

Method: 结合dense embedding（all-mpnet-base-v2）与BM25实现混合检索，通过SpaCy NER进行基于元数据过滤，并用cross-encoder重新排序。采用语义分块保证文本连贯，保留表格结构维护数据完整性。量化索引提升检索效率，引入人工反馈和会话记忆增强适应性。

Result: 在企业数据集上的实验表明，Precision@5提升15%，Recall@5提升13%，MRR提升16%；主观评分（Faithfulness、Completeness、Relevance）也大幅提高，均在5分制上超过原系统。

Conclusion: 该框架能为企业任务提供更准确、全面与上下文相关的响应，未来将拓展到多模态数据和智能代理检索。代码开源。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [136] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文研究了如何利用开源语言模型生成的推理链（CoT）来预测模型最终输出的安全性，发现基于中间激活的探针相比文本方法有更高的准确性，并能够提前预警不安全输出。


<details>
  <summary>Details</summary>
Motivation: 目前主流的开源推理语言模型通过生成较长的推理链（CoT）来提升输出质量，但这也引入了更多输出有害内容的风险。合理监控和预测模型输出的对齐（安全）性成为现实需求。

Method: 作者对多种监测方法进行了系统评估，包括人工评判、高性能大模型以及使用CoT文本或中间激活信息的文本分类器。并专门训练了简单的线性探针，利用CoT生成过程中的激活，预测最终输出是否安全。

Result: 结果显示，基于CoT激活的线性探针明显优于所有基于文本的方法，无论是人工判断还是文本分类器。并且这些激活信号在推理尚未结束时就能给出准确预测，性能在不同模型规模、架构和安全基准测试上具有普适性。

Conclusion: 本文发现通过轻量激活探针可实现实时、高效的安全性监控，并有望在生成早期介入，有助于后续构建更安全的开源大模型应用。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [137] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了S2WTM，一种采用球面切片Wasserstein距离的变分自编码器主题模型，实现了比现有主流主题模型更有效的主题建模和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有球面结构的变分自编码器神经主题模型（VAE-NTM）存在后验坍塌现象，使潜变量建模效果大减，影响主题建模质量。需要寻找新的对齐方式以提升潜表示和主题多样性。

Method: 作者提出使用支持在单位超球面上的先验分布，并用球面切片Wasserstein距离来对齐先验与后验聚合分布，称为S2WTM（Spherical Sliced Wasserstein Autoencoder for Topic Modeling）。

Result: 实验表明，S2WTM在主题一致性、多样性和下游任务性能方面均优于最新的主题模型。

Conclusion: S2WTM能有效避免后验坍塌问题，提升主题表达能力，为高维文本数据主题建模提供了更强工具。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [138] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 本文提出了一种名为BETR（benchmark-targeted ranking）的数据选择方法，通过将预训练数据与目标基准任务高度匹配，有效提升了大模型在基准任务上的表现，并指出最佳的数据选择策略需随模型规模变化而自适应。


<details>
  <summary>Details</summary>
Motivation: 当前的数据选择方法往往隐式地以提升基准测试成绩为目标，但并未直接对齐预训练数据与具体评测任务。作者关注这样一个问题：若将这种目标对齐做显式优化，能否显著提升模型性能？

Method: 该方法BE​​TR通过将基准任务的训练样本和部分预训练语料嵌入至同一向量空间，根据相似度为语料打分，再用轻量级分类器扩展到全量语料数据，完成有针对性的数据筛选。同时，作者系统地比较了500+模型（FLOPs覆盖10^19至10^22）在不同数据选择策略下的表现，并拟合了扩展法则。

Result: 实现BETR后，对比DCLM-Baseline获得2.1倍算力增益（对比未筛选数据为4.7倍），在跨规模的10项任务中9项性能提升。并且BETR对未在评测集中的多样化基准任务也表现良好。同时，扩展分析表明：随着模型变大，所需数据筛选强度降低。

Conclusion: 直接对齐预训练数据至目标任务能精准塑造模型能力，最佳的数据选择策略需根据模型规模进行调整。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways](https://arxiv.org/abs/2507.11621)
*Tianyi Wang,Yangyang Wang,Jie Pan,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 本文针对两车道高速公路匝道并入区域的异质交通流，提出了一种分层协同匝道并入控制（HCOMC）框架，通过仿真验证了其在不同交通密度及CAV渗透率下的综合优势。


<details>
  <summary>Details</summary>
Motivation: 高速公路匝道并入区域是交通拥堵和事故的常见瓶颈，目前基于网联自动驾驶车辆（CAVs）的协同控制策略可有效缓解此问题，但由于CAVs尚未完全普及，因此需要一种兼容人驾车（HDVs）与CAVs的分层协同控制框架。

Method: 扩展纵向跟驰模型和横向换道模型以同时适用HDVs和CAVs，并结合人因与协同自适应巡航。提出的HCOMC框架包括基于修改虚拟车的分层协同规划模型、基于博弈论的换道模型及基于多目标优化的精英非支配排序遗传算法。

Result: 通过仿真，对比基线方法，在不同交通密度和CAV渗透率下，HCOMC在提升群体车辆安全、稳定和加快并道过程、优化通行效率及降低油耗等方面表现出显著优势。

Conclusion: HCOMC适用于尚未完全CAV化的异质交通条件，能提升匝道并道区域的整体交通安全性、效率与节能效果，有望为未来智能交通系统提供有效的过渡解决方案。

Abstract: Highway on-ramp merging areas are common bottlenecks to traffic congestion
and accidents. Currently, a cooperative control strategy based on connected and
automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs
are not fully widespread, it is necessary to propose a hierarchical cooperative
on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on
two-lane highways to address this gap. This paper extends longitudinal
car-following models based on the intelligent driver model and lateral
lane-changing models using the quintic polynomial curve to account for
human-driven vehicles (HDVs) and CAVs, comprehensively considering human
factors and cooperative adaptive cruise control. Besides, this paper proposes a
HCOMC framework, consisting of a hierarchical cooperative planning model based
on the modified virtual vehicle model, a discretionary lane-changing model
based on game theory, and a multi-objective optimization model using the
elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and
efficient merging process. Then, the performance of our HCOMC is analyzed under
different traffic densities and CAV penetration rates through simulation. The
findings underscore our HCOMC's pronounced comprehensive advantages in
enhancing the safety of group vehicles, stabilizing and expediting merging
process, optimizing traffic efficiency, and economizing fuel consumption
compared with benchmarks.

</details>


### [140] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Main category: cs.RO

TL;DR: 本文提出了一条面向气候变化相关机器人的研究路线图，呼吁机器人和气候领域专家合作，推动机器人技术应对气候挑战。


<details>
  <summary>Details</summary>
Motivation: 气候变化是21世纪重大挑战之一，机器人领域也希望为应对气候变化做出贡献，但缺乏系统性的研究方向和跨学科合作的路径指引。

Method: 作者通过梳理气候相关领域（如能源、环境、交通、工业、土地利用和地球科学）的应用需求，提出机器人技术（包括实体机器人及规划、感知、控制与估计算法）在这些领域的具体应用机会。论文汇聚了机器人学家和气候领域专家的合作建议，协助规划高影响力的研究主题。

Result: 明确了机器人技术在能源系统优化、建筑改造、精准农业、自动驾驶物流和环境监测等多个领域的研究机会，提出了可操作的合作方向。

Conclusion: 本研究为机器人学界提供了气候相关研究的路线图，鼓励开展具体协作与创新研究，将机器人专家的能力用于紧迫的气候优先事项。

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [141] [CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment](https://arxiv.org/abs/2507.11716)
*Yifan Xu,Qianwei Wang,Jordan Lillie,Vineet Kamat,Carol Menassa,Clive D'Souza*

Main category: cs.RO

TL;DR: 本论文提出了一种基于共享控制的新型智能轮椅系统CoNav Chair，并通过实验验证了其在安全性和效率方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有动力轮椅多为手动或全自动模式，灵活性有限，在狭小空间中导航困难，且完全自主或单纯手动的方案可能降低效率和用户信任。因此需要新方法提升轮椅的导航效率、安全性和易用性。

Method: 设计了基于ROS的CoNav Chair，具备共享控制导航与障碍避让功能。通过对比手动、共享和全自动三种导航模式，招募21名健康被试在室内环境中进行使用性测试，对比碰撞次数、完成任务时间、轨迹长度和平滑度等指标，并收集主观使用评价。

Result: 共享控制模式发生的碰撞显著更少，完成任务时间、轨迹长度和平滑度与手动及全自动模式相当甚至更优。用户主观认为共享控制更安全、更高效。

Conclusion: CoNav Chair系统在安全性和性能上表现良好，为后续针对实际依赖轮椅的残障群体的进一步测试奠定了基础。

Abstract: As the global population of people with disabilities (PWD) continues to grow,
so will the need for mobility solutions that promote independent living and
social integration. Wheelchairs are vital for the mobility of PWD in both
indoor and outdoor environments. The current SOTA in powered wheelchairs is
based on either manually controlled or fully autonomous modes of operation,
offering limited flexibility and often proving difficult to navigate in
spatially constrained environments. Moreover, research on robotic wheelchairs
has focused predominantly on complete autonomy or improved manual control;
approaches that can compromise efficiency and user trust. To overcome these
challenges, this paper introduces the CoNav Chair, a smart wheelchair based on
the Robot Operating System (ROS) and featuring shared control navigation and
obstacle avoidance capabilities that are intended to enhance navigational
efficiency, safety, and ease of use for the user. The paper outlines the CoNav
Chair's design and presents a preliminary usability evaluation comparing three
distinct navigation modes, namely, manual, shared, and fully autonomous,
conducted with 21 healthy, unimpaired participants traversing an indoor
building environment. Study findings indicated that the shared control
navigation framework had significantly fewer collisions and performed
comparably, if not superior to the autonomous and manual modes, on task
completion time, trajectory length, and smoothness; and was perceived as being
safer and more efficient based on user reported subjective assessments of
usability. Overall, the CoNav system demonstrated acceptable safety and
performance, laying the foundation for subsequent usability testing with end
users, namely, PWDs who rely on a powered wheelchair for mobility.

</details>


### [142] [Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies](https://arxiv.org/abs/2507.11770)
*Giang Nguyen,Mihai Pomarlan,Sascha Jongebloed,Nils Leusmann,Minh Nhat Vu,Michael Beetz*

Main category: cs.RO

TL;DR: 本文提出了一种将多种异构场景描述格式（如MJCF、URDF、SDF）统一转换为Universal Scene Description（USD）格式的方法，并结合机器人本体实现环境知识的标准化与可用化。


<details>
  <summary>Details</summary>
Motivation: 当前机器人环境数据采用多种不同且不兼容的描述格式，影响了场景信息的有效融合与智能决策能力。因此，亟需一种能够统一格式、促进知识推理和应用的方法。

Method: 作者开发了一个统一的场景图谱模型，将MJCF、URDF、SDF等格式标准化为USD。同时，将USD场景与机器人本体结合，通过语义注释将环境数据转为知识图谱。实验采用程序化3D环境转换、语义标注与知识问答验证方法，并开发了Web可视化工具辅助语义映射。

Result: 方法实现了不同格式场景数据的统一管理和高效转换，能够将复杂环境数据转化为能够支持实时决策的机器知识。知识图谱可有效回答相关能力问题，验证了其实用性。

Conclusion: 统一的场景描述方法显著提升了机器人环境知识的获取及认知能力，为基于知识的机器人控制提供了坚实支持，并且所开发的可视化工具提升了使用与管理效率。

Abstract: In robotics, the effective integration of environmental data into actionable
knowledge remains a significant challenge due to the variety and
incompatibility of data formats commonly used in scene descriptions, such as
MJCF, URDF, and SDF. This paper presents a novel approach that addresses these
challenges by developing a unified scene graph model that standardizes these
varied formats into the Universal Scene Description (USD) format. This
standardization facilitates the integration of these scene graphs with robot
ontologies through semantic reporting, enabling the translation of complex
environmental data into actionable knowledge essential for cognitive robotic
control. We evaluated our approach by converting procedural 3D environments
into USD format, which is then annotated semantically and translated into a
knowledge graph to effectively answer competency questions, demonstrating its
utility for real-time robotic decision-making. Additionally, we developed a
web-based visualization tool to support the semantic mapping process, providing
users with an intuitive interface to manage the 3D environment.

</details>


### [143] [The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey](https://arxiv.org/abs/2507.11840)
*Gaofeng Li,Ruize Wang,Peisen Xu,Qi Ye,Jiming Chen*

Main category: cs.RO

TL;DR: 本文综述了机器人灵巧操作的发展历程，从机械编程到具身智能的演化，以及从简单夹爪到多指灵巧手的进步，重点关注当前的数据采集方法和技能学习框架，并总结了三大关键挑战。


<details>
  <summary>Details</summary>
Motivation: 人类级别的灵巧机器人操作是机器人领域的核心目标与难题。随着人工智能的发展，推动机器人操作取得了快速进展，亟需系统梳理现有发展，指出尚待解决的重要问题。

Method: 通过回顾机械编程到具身智能的发展历程，归纳多指灵巧手的主要特征与挑战，并详细介绍当前主流的数据采集方法（如仿真、人类演示和远程操作）及技能学习框架（模仿学习和强化学习）。

Result: 总结了数据采集与技能学习领域的最新进展，并基于已有工作梳理了影响灵巧机器人操作进一步发展的三大关键挑战。

Conclusion: 技术上虽已取得初步进展，但灵巧操作机器人仍面临数据收集、学习框架和泛化能力等三项制约发展的问题，未来需聚焦突破相关核心挑战。

Abstract: Achieving human-like dexterous robotic manipulation remains a central goal
and a pivotal challenge in robotics. The development of Artificial Intelligence
(AI) has allowed rapid progress in robotic manipulation. This survey summarizes
the evolution of robotic manipulation from mechanical programming to embodied
intelligence, alongside the transition from simple grippers to multi-fingered
dexterous hands, outlining key characteristics and main challenges. Focusing on
the current stage of embodied dexterous manipulation, we highlight recent
advances in two critical areas: dexterous manipulation data collection (via
simulation, human demonstrations, and teleoperation) and skill-learning
frameworks (imitation and reinforcement learning). Then, based on the overview
of the existing data collection paradigm and learning framework, three key
challenges restricting the development of dexterous robotic manipulation are
summarized and discussed.

</details>


### [144] [Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers](https://arxiv.org/abs/2507.11852)
*Mohammed Hassanin,Mohammad Abu Alsheikh,Carlos C. N. Kuhn,Damith Herath,Dinh Thai Hoang,Ibrahim Radwan*

Main category: cs.RO

TL;DR: 本文综述了自动驾驶（AD）技术在两轮微出行工具（如电动滑板车与电动自行车）自动骑行（AR）系统上的应用与挑战，分析了相关感知、规划、控制等核心组件，并指出现有研究的不足与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 两轮微出行工具迅速流行，但其自动化技术（AR）还不成熟，存在平台不稳定、体积与功率受限及环境不可预测等问题，为公路安全带来巨大隐患。该文旨在总结AD经验，为AR发展提供借鉴和方向。

Method: 作者通过系统性文献调研和技术对比，从AD的感知、规划、控制等核心技术入手，评估AR领域现有成果，梳理优势与不足，提炼对未来研究具有潜力的方向与方法。

Result: 发现AR领域在多任务综合感知系统、产业和政策推动、研究投入等方面存在显著短板，并提出如多模态轻量传感与边缘深度学习等具有前景的研究主题。

Conclusion: 论文呼吁结合AD成熟经验与AR实际需求，推动安全、高效、规模化的两轮自动骑行技术发展，助力未来城市交通变革。

Abstract: The rapid adoption of micromobility solutions, particularly two-wheeled
vehicles like e-scooters and e-bikes, has created an urgent need for reliable
autonomous riding (AR) technologies. While autonomous driving (AD) systems have
matured significantly, AR presents unique challenges due to the inherent
instability of two-wheeled platforms, limited size, limited power, and
unpredictable environments, which pose very serious concerns about road users'
safety. This review provides a comprehensive analysis of AR systems by
systematically examining their core components, perception, planning, and
control, through the lens of AD technologies. We identify critical gaps in
current AR research, including a lack of comprehensive perception systems for
various AR tasks, limited industry and government support for such
developments, and insufficient attention from the research community. The
review analyses the gaps of AR from the perspective of AD to highlight
promising research directions, such as multimodal sensor techniques for
lightweight platforms and edge deep learning architectures. By synthesising
insights from AD research with the specific requirements of AR, this review
aims to accelerate the development of safe, efficient, and scalable autonomous
riding systems for future urban mobility.

</details>


### [145] [A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications](https://arxiv.org/abs/2507.11880)
*Jinyuan Liu,Minglei Fu,Ling Shi,Chenguang Yang,Wenan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新的系绳机器人路径规划算法（CDT-TCS），通过结合代数拓扑与几何优化，有效解决了系绳机器人在专用环境下路径受限和缠绕风险等难题。


<details>
  <summary>Details</summary>
Motivation: 系绳机器人在灾害救援、地下勘探等特殊环境具有稳定供电和可靠通信的优势，但常因绳索长度及缠绕受限，导致运动规划困难。因此，亟需突破现有方法在路径寻优和可行性判定上的局限。

Method: 作者提出CDT-TCS算法，利用CDT编码作为同伦不变量来描述路径的拓扑状态，将代数拓扑与几何优化融合，并发展出三种应用算法：CDT-TPP（最优系绳路径规划）、CDT-TMV（受系绳约束的多目标访问）、CDT-UTPP（无系绳距离最优路径规划），所有理论结果经过严格证明。

Result: 大量仿真实验显示，该算法在多个任务上均超越了现有先进方法。此外，真实机器人实验验证了其在工程上的可行性和实用价值。

Conclusion: 本文工作为系绳机器人的运动规划提供了理论创新和工程方案，可显著提升其在实际应用中的效率与可靠性。

Abstract: Tethered robots play a pivotal role in specialized environments such as
disaster response and underground exploration, where their stable power supply
and reliable communication offer unparalleled advantages. However, their motion
planning is severely constrained by tether length limitations and entanglement
risks, posing significant challenges to achieving optimal path planning. To
address these challenges, this study introduces CDT-TCS (Convex Dissection
Topology-based Tethered Configuration Search), a novel algorithm that leverages
CDT Encoding as a homotopy invariant to represent topological states of paths.
By integrating algebraic topology with geometric optimization, CDT-TCS
efficiently computes the complete set of optimal feasible configurations for
tethered robots at all positions in 2D environments through a single
computation. Building on this foundation, we further propose three
application-specific algorithms: i) CDT-TPP for optimal tethered path planning,
ii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for
distance-optimal path planning of untethered robots. All theoretical results
and propositions underlying these algorithms are rigorously proven and
thoroughly discussed in this paper. Extensive simulations demonstrate that the
proposed algorithms significantly outperform state-of-the-art methods in their
respective problem domains. Furthermore, real-world experiments on robotic
platforms validate the practicality and engineering value of the proposed
framework.

</details>


### [146] [NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy](https://arxiv.org/abs/2507.11889)
*Adnan Abdullah,Alankrit Gupta,Vaishnav Ramesh,Shivali Patel,Md Jahidul Islam*

Main category: cs.RO

TL;DR: 该论文提出了NemeSys系统，实现了水下机器人（AUV）在无GPS、通信受限环境下的实时任务适应与参数动态重构。系统通过浮标进行光学与磁电信号传输，实现低带宽下的任务交互、探索及语义任务编码，并经过实地及实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数AUV只能执行预设任务或依赖低效的声学/有线通信，不能适应动态任务变化，限制了其在复杂海洋环境的灵活性和自主性。本文为解决这些限制提出了新方法。

Method: 设计了基于浮标的光学及磁电信号传输系统（NemeSys），开发了完整的控制架构和语义任务编码框架，使AUV能够以低带宽方式接受实时任务调整。系统通过理论分析、受控实验和实际海域测试进行了验证。

Result: 实验证明NemeSys能够实现AUV任务的在线自适应和语义级任务更新，无需高带宽通信，提高了AUV在动态复杂环境中的自主性和适应性。

Conclusion: NemeSys系统为AUV实时动态任务重构提供了可行、有效的技术途径，提升了水下自主平台在不确定环境中任务执行与调整的能力。

Abstract: Adaptive mission control and dynamic parameter reconfiguration are essential
for autonomous underwater vehicles (AUVs) operating in GPS-denied,
communication-limited marine environments. However, most current AUV platforms
execute static, pre-programmed missions or rely on tethered connections and
high-latency acoustic channels for mid-mission updates, significantly limiting
their adaptability and responsiveness. In this paper, we introduce NemeSys, a
novel AUV system designed to support real-time mission reconfiguration through
compact optical and magnetoelectric (OME) signaling facilitated by floating
buoys. We present the full system design, control architecture, and a semantic
mission encoding framework that enables interactive exploration and task
adaptation via low-bandwidth communication. The proposed system is validated
through analytical modeling, controlled experimental evaluations, and
open-water trials. Results confirm the feasibility of online mission adaptation
and semantic task updates, highlighting NemeSys as an online AUV platform for
goal-driven adaptive autonomy in dynamic and uncertain underwater environments.

</details>


### [147] [Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments](https://arxiv.org/abs/2507.11920)
*Jeongyong Yang,KwangBin Lee,SooJean Han*

Main category: cs.RO

TL;DR: HyPRAP是一种针对密集不确定环境下的实时路径规划框架，通过混合多种障碍物运动预测模型和动态风险评估，有效提升安全性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在密集、不确定的动态环境中为智能体实时路径规划时，大规模障碍物的未来运动预测极具挑战，单一预测模型计算负担重且效果有限。因而亟需方法在保持高安全性的同时，降低预测和规划的计算开销。

Method: 提出了HyPRAP框架，混合多种障碍物运动预测模型，并基于创新的预测碰撞风险指数（P-CRI）动态区分高风险和低风险障碍物，智能体对高风险障碍物进行高精度预测，对低风险障碍物采用简化预测，从而优化计算资源分配。同时通过混合保形预测，实现多模型下的不确定性量化，给出预测置信区间。

Result: 理论分析表明HyPRAP可在保证安全性的前提下，显著提升计算效率。仿真实验结果显示，HyPRAP在通用场景下优于单一预测模型方法，P-CRI风险评估也优于传统距离最近的风险评估方式。

Conclusion: HyPRAP充分利用多种预测模型的多样性，在密集动态环境下实现了风险感知路径规划的安全与效率平衡，是对现有方法的重要改进。

Abstract: Real-time path planning in dense, uncertain environments remains a
challenging problem, as predicting the future motions of numerous dynamic
obstacles is computationally burdensome and unrealistic. To address this, we
introduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a
prediction-based risk-aware path-planning framework which uses a hybrid
combination of models to predict local obstacle movement. HyPRAP uses a novel
Prediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by
each obstacle, enabling the selective use of predictors based on whether the
agent prioritizes high predictive accuracy or low computational prediction
overhead. This selective routing enables the agent to focus on high-risk
obstacles while ignoring or simplifying low-risk ones, making it suitable for
environments with a large number of obstacles. Moreover, HyPRAP incorporates
uncertainty quantification through hybrid conformal prediction by deriving
confidence bounds simultaneously achieved by multiple predictions across
different models. Theoretical analysis demonstrates that HyPRAP effectively
balances safety and computational efficiency by leveraging the diversity of
prediction models. Extensive simulations validate these insights for more
general settings, confirming that HyPRAP performs better compared to single
predictor methods, and P-CRI performs better over naive proximity-based risk
assessment.

</details>


### [148] [A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning](https://arxiv.org/abs/2507.11938)
*Hao Chen,Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种摆脱传统学习框架的新颖方法，通过相似性匹配已知物体模型，指导机器人单视角下对未知物体的抓取，并在多层次特征匹配和新型几何描述子的加持下显著提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大规模数据集的抓取方法虽然取得进展，但对噪声和环境变化敏感，导致通用性和鲁棒性不足。因此，作者希望摆脱依赖学习模型，探索更加稳健的抓取方式。

Method: 该方法共分三步：1）利用视觉特征从现有数据库检索与观测物体相似的已知模型；2）利用候选模型的抓取知识对未知物体进行模仿抓取规划；3）对抓取结果进行局部优化。此外，提出多层次相似性匹配框架，融合语义、几何和尺寸特征，并设计了新型C-FPFH点云描述子、半定向包围盒和基于平面检测的新点云配准方法以提升匹配精度。

Result: 实验表明，该方法在单视角、存在观测不完备和噪声情况下均能提升未知物体的抓取成功率，在鲁棒性和匹配准确性上优于传统学习方法。

Conclusion: 相似性匹配范式在未知物体抓取领域展现出强大潜力，通过整合多层次特征和创新描述子，实现了更通用且鲁棒的抓取策略。

Abstract: Grasping unknown objects from a single view has remained a challenging topic
in robotics due to the uncertainty of partial observation. Recent advances in
large-scale models have led to benchmark solutions such as GraspNet-1Billion.
However, such learning-based approaches still face a critical limitation in
performance robustness for their sensitivity to sensing noise and environmental
changes. To address this bottleneck in achieving highly generalized grasping,
we abandon the traditional learning framework and introduce a new perspective:
similarity matching, where similar known objects are utilized to guide the
grasping of unknown target objects. We newly propose a method that robustly
achieves unknown-object grasping from a single viewpoint through three key
steps: 1) Leverage the visual features of the observed object to perform
similarity matching with an existing database containing various object models,
identifying potential candidates with high similarity; 2) Use the candidate
models with pre-existing grasping knowledge to plan imitative grasps for the
unknown target object; 3) Optimize the grasp quality through a local
fine-tuning process. To address the uncertainty caused by partial and noisy
observation, we propose a multi-level similarity matching framework that
integrates semantic, geometric, and dimensional features for comprehensive
evaluation. Especially, we introduce a novel point cloud geometric descriptor,
the C-FPFH descriptor, which facilitates accurate similarity assessment between
partial point clouds of observed objects and complete point clouds of database
models. In addition, we incorporate the use of large language models, introduce
the semi-oriented bounding box, and develop a novel point cloud registration
approach based on plane detection to enhance matching accuracy under
single-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.

</details>


### [149] [IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving](https://arxiv.org/abs/2507.11940)
*Kanghyun Ryu,Minjun Sung,Piyush Gupta,Jovin D'sa,Faizan M. Tariq,David Isele,Sangjae Bae*

Main category: cs.RO

TL;DR: 提出了一种能够考虑与周围交通参与者交互行为的自动驾驶汽车运动规划方法（IANN-MPPI），提升了在密集交通中的合流效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆在密集交通下，因无法有效预测与响应周围车辆的交互，常表现得过于保守，无法顺利实现规划目标。

Method: 提出Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral (IANN-MPPI)控制方法：通过神经网络预测周围车辆对自动驾驶车辆每个控制序列的反应，将其融入采样式模型预测路径积分（MPPI）控制框架，并结合可提升道路线变换效率的样条（spline）先验分布。

Result: 在密集交通环境下的合流场景，通过实验验证了IANN-MPPI具有高效的合流能力和良好的交互行为预测，优于传统非交互规划方法。

Conclusion: IANN-MPPI能有效提升自动驾驶车辆在复杂交通环境下的规划表现，实现更协同、更高效的行驶行为。

Abstract: Motion planning for autonomous vehicles (AVs) in dense traffic is
challenging, often leading to overly conservative behavior and unmet planning
objectives. This challenge stems from the AVs' limited ability to anticipate
and respond to the interactive behavior of surrounding agents. Traditional
decoupled prediction and planning pipelines rely on non-interactive predictions
that overlook the fact that agents often adapt their behavior in response to
the AV's actions. To address this, we propose Interaction-Aware Neural
Network-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which
enables interactive trajectory planning by predicting how surrounding agents
may react to each control sequence sampled by MPPI. To improve performance in
structured lane environments, we introduce a spline-based prior for the MPPI
sampling distribution, enabling efficient lane-changing behavior. We evaluate
IANN-MPPI in a dense traffic merging scenario, demonstrating its ability to
perform efficient merging maneuvers. Our project website is available at
https://sites.google.com/berkeley.edu/iann-mppi

</details>


### [150] [A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming](https://arxiv.org/abs/2507.11974)
*Waseem Akram,Muhayy Ud Din,Lyes Saad Soud,Irfan Hussain*

Main category: cs.RO

TL;DR: 本论文综述了生成式人工智能（GAI）在水产养殖领域的最新应用与前景，涵盖基础架构、实验系统、实际部署及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着水产养殖业迈向数据驱动和自动化，行业对智能化决策和数字化运营的需求日益增加。GAI技术为提升环境监测、疾病诊断、市场分析等环节提供了创新机会，因此有必要对其应用与挑战进行系统总结。

Method: 本文通过文献回顾和案例分析，系统梳理了GAI在水产养殖中的各类应用，包括底层模型架构、实验平台、实地部署和具体案例。同时，提出了涵盖感知、控制、优化、通讯及法规遵从的新型应用分类法，并系统评述了现阶段的主要技术和非技术挑战。

Result: 论文归纳了GAI在水下感知、数字孪生建模、远程操作规划等方面的实际进展，列举了多种应用场景，同时详述了数据受限、实时性不足、可解释性差、环境代价高和监管不确定等关键限制。

Conclusion: GAI不仅是水产养殖自动化的工具，更是推动智能、弹性与生态友好型养殖体系的核心支撑技术。今后需关注数据基础完善、模型优化、法规制定与持续创新，以实现其可持续发展潜力。

Abstract: Generative Artificial Intelligence (GAI) has rapidly emerged as a
transformative force in aquaculture, enabling intelligent synthesis of
multimodal data, including text, images, audio, and simulation outputs for
smarter, more adaptive decision-making. As the aquaculture industry shifts
toward data-driven, automation and digital integration operations under the
Aquaculture 4.0 paradigm, GAI models offer novel opportunities across
environmental monitoring, robotics, disease diagnostics, infrastructure
planning, reporting, and market analysis. This review presents the first
comprehensive synthesis of GAI applications in aquaculture, encompassing
foundational architectures (e.g., diffusion models, transformers, and retrieval
augmented generation), experimental systems, pilot deployments, and real-world
use cases. We highlight GAI's growing role in enabling underwater perception,
digital twin modeling, and autonomous planning for remotely operated vehicle
(ROV) missions. We also provide an updated application taxonomy that spans
sensing, control, optimization, communication, and regulatory compliance.
Beyond technical capabilities, we analyze key limitations, including limited
data availability, real-time performance constraints, trust and explainability,
environmental costs, and regulatory uncertainty. This review positions GAI not
merely as a tool but as a critical enabler of smart, resilient, and
environmentally aligned aquaculture systems.

</details>


### [151] [Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers](https://arxiv.org/abs/2507.11991)
*Juanran Wang,Marc R. Schlichting,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 本文通过深度生成模型提升自动驾驶车辆在交叉路口的安全性，提出将多步扩散模型蒸馏为单步模型以加快推理速度，并用作鲁棒规划器，实验显示故障率和延迟率均优于基准控制器。


<details>
  <summary>Details</summary>
Motivation: 交叉路口作为高风险交通区域，是交通事故高发地带。传统自动驾驶决策方法难以全面应对因感知噪声导致的极端碰撞情境，急需智能技术提升在此类复杂场景下的安全性和鲁棒性。

Method: 1. 训练1000步去噪扩散概率模型（denoising diffusion probabilistic model），模拟自动驾驶车辆基于“入侵者”相对位置与速度产生的、导致碰撞的传感噪声序列；2. 采用生成对抗式架构，将多步扩散模型蒸馏为单步去噪扩散模型，实现快速推理；3. 将该单步模型集成到鲁棒规划器内，在线采样潜在失败情境，指导车辆决策。

Result: 仿真实验表明，采用基于单步扩散模型的鲁棒规划器，车辆的失败率和延迟率均显著低于基线的Intelligent Driver Model（IDM）控制器。

Conclusion: 基于深度生成模型的鲁棒规划方法能够有效提升自动驾驶车辆在复杂交叉路口环境下的安全性与决策鲁棒性。单步扩散模型在保证采样质量的同时大大提高了推理速度，具备实际集成应用的潜力。

Abstract: High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.

</details>


### [152] [Robust Route Planning for Sidewalk Delivery Robots](https://arxiv.org/abs/2507.12067)
*Xing Tong,Michele D. Simoni*

Main category: cs.RO

TL;DR: 本文以人行道配送机器人为对象，研究不确定的行驶时间对机器人路径规划的影响，并提出多种鲁棒路由方法显著提升运行可靠性，特别在复杂或拥挤环境下效果突出。


<details>
  <summary>Details</summary>
Motivation: 人行道送货机器人可缓解城市交通压力，但其行驶时间因人流密度和路况变化而高度不确定，影响配送效率。为了让机器人在现实环境中更高效可靠运行，需要在路径规划中考虑这些不确定因素。

Method: 本研究通过将优化算法与模拟结合，重现人行道障碍和人流影响，得到更接近实际的行驶时间。探讨了预算型、椭球型、基于支持向量聚类（SVC）的3种不确定集构造方法及分布鲁棒最短路（DRSP），并在斯德哥尔摩真实人流情形下对各种方法与机器人设计进行了评估。

Result: 结果表明，鲁棒路径规划在多变条件下比传统最短路径（SP）大幅提升配送可靠性，尤其椭球型和DRSP效果最佳，平均及最坏延误较小。在机器人本体较宽、速度慢、导航保守或恶劣天气、高人流情况下，鲁棒方法优势更为明显。

Conclusion: 考虑不确定性的鲁棒路径规划方法能极大改善人行道运输机器人在现实环境下的实用性和稳定性，为城市配送智能化与可靠性提升提供了有力工具。

Abstract: Sidewalk delivery robots are a promising solution for urban freight
distribution, reducing congestion compared to trucks and providing a safer,
higher-capacity alternative to drones. However, unreliable travel times on
sidewalks due to pedestrian density, obstacles, and varying infrastructure
conditions can significantly affect their efficiency. This study addresses the
robust route planning problem for sidewalk robots, explicitly accounting for
travel time uncertainty due to varying sidewalk conditions. Optimization is
integrated with simulation to reproduce the effect of obstacles and pedestrian
flows and generate realistic travel times. The study investigates three
different approaches to derive uncertainty sets, including budgeted,
ellipsoidal, and support vector clustering (SVC)-based methods, along with a
distributionally robust method to solve the shortest path (SP) problem. A
realistic case study reproducing pedestrian patterns in Stockholm's city center
is used to evaluate the efficiency of robust routing across various robot
designs and environmental conditions. The results show that, when compared to a
conventional SP, robust routing significantly enhances operational reliability
under variable sidewalk conditions. The Ellipsoidal and DRSP approaches
outperform the other methods, yielding the most efficient paths in terms of
average and worst-case delay. Sensitivity analyses reveal that robust
approaches consistently outperform the conventional SP, particularly for
sidewalk delivery robots that are wider, slower, and have more conservative
navigation behaviors. These benefits are even more pronounced in adverse
weather conditions and high pedestrian congestion scenarios.

</details>


### [153] [Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards](https://arxiv.org/abs/2507.12093)
*David Rapado-Rincon,Gert Kootstra*

Main category: cs.RO

TL;DR: 本文提出了一种专为果园环境设计的Tree-SLAM语义建图方法，能在GPS信号不稳定和同质性树木导致SLAM误差的情况下，实现高精度的单棵树定位与地图构建。


<details>
  <summary>Details</summary>
Motivation: 果园高精度树木地图对于实现精准农业和机器人自主作业至关重要，但传统的方法在GPS信号弱和树木外观高度重复时表现不佳，急需新方法提升定位和建图能力。

Method: 方法基于RGB-D图像，对树干进行实例分割检测，然后通过级联图-数据关联算法完成树干重识别。之后将重识别的树干作为因子图中的地标，将其与GPS、里程计和观测数据融合，提升定位精度，最终完成单株树地图的构建。

Result: 本方法在苹果和梨果园的多时节数据集上验证，能在GPS不稳定时实现18厘米的地理定位误差，误差低于种植距离的20%，表现出良好的地图准确性和鲁棒性。

Conclusion: Tree-SLAM实现了在果园场景下单棵树高精度建图，有效解决了现有技术在GPS失效和树形同质导致的SLAM混淆问题，为果园机器人与精准农业应用提供了坚实支撑。

Abstract: Accurate mapping of individual trees is an important component for precision
agriculture in orchards, as it allows autonomous robots to perform tasks like
targeted operations or individual tree monitoring. However, creating these maps
is challenging because GPS signals are often unreliable under dense tree
canopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)
approaches struggle in orchards because the repetitive appearance of trees can
confuse the system, leading to mapping errors. To address this, we introduce
Tree-SLAM, a semantic SLAM approach tailored for creating maps of individual
trees in orchards. Utilizing RGB-D images, our method detects tree trunks with
an instance segmentation model, estimates their location and re-identifies them
using a cascade-graph-based data association algorithm. These re-identified
trunks serve as landmarks in a factor graph framework that integrates noisy GPS
signals, odometry, and trunk observations. The system produces maps of
individual trees with a geo-localization error as low as 18 cm, which is less
than 20\% of the planting distance. The proposed method was validated on
diverse datasets from apple and pear orchards across different seasons,
demonstrating high mapping accuracy and robustness in scenarios with unreliable
GPS signals.

</details>


### [154] [Leveraging Sidewalk Robots for Walkability-Related Analyses](https://arxiv.org/abs/2507.12148)
*Xing Tong,Michele D. Simoni,Kaj Munhoz Arfvidsson,Jonas Mårtensson*

Main category: cs.RO

TL;DR: 本研究利用配备传感器的人行道配送机器人，自动、实时地收集影响步行性的街道人行道特征数据。通过在瑞典KTH校园部署机器人，分析收集到的人行道数据揭示了人行道特征与行人行为之间的关系。


<details>
  <summary>Details</summary>
Motivation: 步行性是可持续城市发展的重要组成部分，但用传统方法收集相关详细数据成本高且难以大规模应用。随着人行道配送机器人在城市环境中的普及，作者希望探索这些机器人是否能作为新的城市数据采集平台。

Method: 在瑞典KTH校园内部署一台配有传感器的机器人，完成101次行程，覆盖900个人行道路段，收集包括机器人行程特征、人行道状况和人行道利用度等多种数据，并对其与步行性相关性进行分析。

Result: 分析发现，行人移动模式受人行道密度、宽度和表面状况等因素显著影响；密度高、宽度窄和路面不平整都会导致移动速度下降及轨迹的不稳定。此外，机器人速度能很好地反应行人行为，有望作为评估行人动态的代理指标。

Conclusion: 本文提出的机器人数据采集框架可实现人行道状况和行人行为的持续监测，有助于更具步行性、包容性和响应性的城市环境建设。

Abstract: Walkability is a key component of sustainable urban development, while
collecting detailed data on its related features remains challenging due to the
high costs and limited scalability of traditional methods. Sidewalk delivery
robots, increasingly deployed in urban environments, offer a promising solution
to these limitations. This paper explores how these robots can serve as mobile
data collection platforms, capturing sidewalk-level features related to
walkability in a scalable, automated, and real-time manner. A sensor-equipped
robot was deployed on a sidewalk network at KTH in Stockholm, completing 101
trips covering 900 segments. From the collected data, different typologies of
features are derived, including robot trip characteristics (e.g., speed,
duration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk
utilization (e.g., pedestrian density). Their walkability-related implications
were investigated with a series of analyses. The results demonstrate that
pedestrian movement patterns are strongly influenced by sidewalk
characteristics, with higher density, reduced width, and surface irregularity
associated with slower and more variable trajectories. Notably, robot speed
closely mirrors pedestrian behavior, highlighting its potential as a proxy for
assessing pedestrian dynamics. The proposed framework enables continuous
monitoring of sidewalk conditions and pedestrian behavior, contributing to the
development of more walkable, inclusive, and responsive urban environments.

</details>


### [155] [Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach](https://arxiv.org/abs/2507.12158)
*Nawshin Mannan Proma,Gricel Vázquez,Sepeedeh Shahbeigi,Arjun Badyal,Victoria Hodge*

Main category: cs.RO

TL;DR: 本文提出了一种基于系统性情境提取、概率建模与验证的新颖方法，用于工业自动驾驶地面车辆的安全性验证，并通过概率模型检查，有效识别高风险情境，提供定量安全保障。


<details>
  <summary>Details</summary>
Motivation: 随着工业自动驾驶地面车辆在安全关键环境中应用日益广泛，保障其在多变环境下的安全运行成为核心挑战。现有方法对复杂环境以及不确定性因素下的安全验证仍有不足，因此需要更系统和定量的验证手段。

Method: 作者提出了情境覆盖网格（situation coverage grid）的方法，系统地枚举车辆运行环境下的所有配置。在实际基于情境的测试中收集概率数据，描述情境间的概率转移，进而构建出包含正常和不安全行为的概率模型。最后将从危害分析中提取的安全属性形式化为时序逻辑，并利用概率模型检查对其进行验证。

Result: 结果显示，该方法能够有效识别高风险情境，为安全性提供定量保障，并支持合规性要求，验证了方法的有效性和实用性。

Conclusion: 该方法有助于实现自动驾驶系统的稳健部署，为其在各种环境下的安全性验证提供了系统化和量化的支持，对行业规范和标准的遵从也具重要意义。

Abstract: As industrial autonomous ground vehicles are increasingly deployed in
safety-critical environments, ensuring their safe operation under diverse
conditions is paramount. This paper presents a novel approach for their safety
verification based on systematic situation extraction, probabilistic modelling
and verification. We build upon the concept of a situation coverage grid, which
exhaustively enumerates environmental configurations relevant to the vehicle's
operation. This grid is augmented with quantitative probabilistic data
collected from situation-based system testing, capturing probabilistic
transitions between situations. We then generate a probabilistic model that
encodes the dynamics of both normal and unsafe system behaviour. Safety
properties extracted from hazard analysis and formalised in temporal logic are
verified through probabilistic model checking against this model. The results
demonstrate that our approach effectively identifies high-risk situations,
provides quantitative safety guarantees, and supports compliance with
regulatory standards, thereby contributing to the robust deployment of
autonomous systems.

</details>


### [156] [Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties](https://arxiv.org/abs/2507.12174)
*Zhenmin Huang,Yusen Xie,Benshan Ma,Shaojie Shen,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种高效并具良好扩展性的多智能体交互轨迹规划方法，能够有效应对智能体意图不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体轨迹规划方法面临的核心难题在于高维复杂的交互性，特别是在处理智能体意图不确定性时，现有博弈论方法计算负担重，效率和扩展性差，因此亟需新方法应对此类情形。

Method: 作者将含意图不确定性的智能体交互建模为贝叶斯博弈，并在一定假设下将其等价表示为势博弈。通过统一的优化问题求解贝叶斯纳什均衡，进而获得最优轨迹。还提出了基于分布式双重共识交替方向乘子法（ADMM）的算法，实现了问题的并行高效求解。

Result: 实验和仿真结果显示，该方法可以在多种存在一般意图不确定性的情形下高效稳定地规划多智能体轨迹，并且扩展性明显优于现有的集中式和分布式基线方法，实现了实时交互轨迹规划。

Conclusion: 本文所提方法能有效处理多智能体间复杂交互中的意图不确定性，具备高效、可扩展、可实时运行的优势，在实际多智能体场景下应用前景广阔。

Abstract: Trajectory planning involving multi-agent interactions has been a
long-standing challenge in the field of robotics, primarily burdened by the
inherent yet intricate interactions among agents. While game-theoretic methods
are widely acknowledged for their effectiveness in managing multi-agent
interactions, significant impediments persist when it comes to accommodating
the intentional uncertainties of agents. In the context of intentional
uncertainties, the heavy computational burdens associated with existing
game-theoretic methods are induced, leading to inefficiencies and poor
scalability. In this paper, we propose a novel game-theoretic interactive
trajectory planning method to effectively address the intentional uncertainties
of agents, and it demonstrates both high efficiency and enhanced scalability.
As the underpinning basis, we model the interactions between agents under
intentional uncertainties as a general Bayesian game, and we show that its
agent-form equivalence can be represented as a potential game under certain
minor assumptions. The existence and attainability of the optimal interactive
trajectories are illustrated, as the corresponding Bayesian Nash equilibrium
can be attained by optimizing a unified optimization problem. Additionally, we
present a distributed algorithm based on the dual consensus alternating
direction method of multipliers (ADMM) tailored to the parallel solving of the
problem, thereby significantly improving the scalability. The attendant
outcomes from simulations and experiments demonstrate that the proposed method
is effective across a range of scenarios characterized by general forms of
intentional uncertainties. Its scalability surpasses that of existing
centralized and decentralized baselines, allowing for real-time interactive
trajectory planning in uncertain game settings.

</details>


### [157] [UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization](https://arxiv.org/abs/2507.12194)
*Hongming Shen,Xun Chen,Yulin Hui,Zhenyu Wu,Wei Wang,Qiyang Lyu,Tianchen Deng,Danwei Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种新的激光雷达全局定位（LGL）方法UniLGL，实现了空间、材质和传感器类型的统一，提升了异构激光雷达传感器下的定位与应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LGL方法只利用部分激光雷达观测信息，或只针对同质传感器，忽视了整体一致性，限制了其在多平台、多场景下的应用能力。

Method: 作者设计了端到端的多BEV特征融合网络，将点云的几何和材质信息编码为空间和强度BEV图像，并引入视角不变性假设，提高了异构传感器下的特征一致性。同时提出了全局位姿估算器，实现了无需额外配准的高精度定位。

Result: 大量真实场景实验表明，UniLGL在定位精度和鲁棒性上优于其他主流LGL方法。

Conclusion: UniLGL不仅具备理论优势和工程通用性，还已成功部署在多种平台（如重卡、MAV），支持复杂场景下的高精度定位和多平台协作，具有广阔的应用前景。

Abstract: Existing LGL methods typically consider only partial information (e.g.,
geometric features) from LiDAR observations or are designed for homogeneous
LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL
method is proposed, termed UniLGL, which simultaneously achieves spatial and
material uniformity, as well as sensor-type uniformity. The key idea of the
proposed method is to encode the complete point cloud, which contains both
geometric and material information, into a pair of BEV images (i.e., a spatial
BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network
is designed to extract uniform features, equipping UniLGL with spatial and
material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a
viewpoint invariance hypothesis is introduced, which replaces the conventional
translation equivariance assumption commonly used in existing LPR networks and
supervises UniLGL to achieve sensor-type uniformity in both global descriptors
and local feature representations. Finally, based on the mapping between local
features on the 2D BEV image and the point cloud, a robust global pose
estimator is derived that determines the global minimum of the global pose on
SE(3) without requiring additional registration. To validate the effectiveness
of the proposed uniform LGL, extensive benchmarks are conducted in real-world
environments, and the results show that the proposed UniLGL is demonstratively
competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL
has been deployed on diverse platforms, including full-size trucks and agile
Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping
as well as multi-MAV collaborative exploration in port and forest environments,
demonstrating the applicability of UniLGL in industrial and field scenarios.

</details>


### [158] [Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot](https://arxiv.org/abs/2507.12273)
*Luca Garello,Francesca Cocchella,Alessandra Sciutti,Manuel Catalano,Francesco Rea*

Main category: cs.RO

TL;DR: 本论文介绍了一款名为Alter-Ego的自主博物馆导览机器人，其集成了先进的导航和大模型对话能力，在真实博物馆环境中进行测试，提升了参观者的互动体验。


<details>
  <summary>Details</summary>
Motivation: 现有公共空间中，提升参观者体验的需求不断增长，尤其是在文化和教育场所。人工智能驱动的机器人为改善导览、增强可及性与知识获取提供了新机遇，因此亟需探索其实际表现及局限性。

Method: 设计并实现Alter-Ego机器人，结合大语言模型实现展品相关问答对话，融合SLAM实现空间自主导航和路径自适应。在真实博物馆环境下，邀请34名参与者，通过对话内容的质性分析和问卷的量化分析进行评估。

Result: 机器人整体上受到参与者欢迎，提升了博物馆参观体验；但在理解和响应方面存在一定不足。

Conclusion: AI驱动的机器人在文化场馆具有提升可及性和知识传播的潜力，但在复杂环境下仍需克服理解和响应等技术局限，未来可进一步优化其人机交互表现。

Abstract: Autonomous robots are increasingly being tested into public spaces to enhance
user experiences, particularly in cultural and educational settings. This paper
presents the design, implementation, and evaluation of the autonomous museum
guide robot Alter-Ego equipped with advanced navigation and interactive
capabilities. The robot leverages state-of-the-art Large Language Models (LLMs)
to provide real-time, context aware question-and-answer (Q&A) interactions,
allowing visitors to engage in conversations about exhibits. It also employs
robust simultaneous localization and mapping (SLAM) techniques, enabling
seamless navigation through museum spaces and route adaptation based on user
requests. The system was tested in a real museum environment with 34
participants, combining qualitative analysis of visitor-robot conversations and
quantitative analysis of pre and post interaction surveys. Results showed that
the robot was generally well-received and contributed to an engaging museum
experience, despite some limitations in comprehension and responsiveness. This
study sheds light on HRI in cultural spaces, highlighting not only the
potential of AI-driven robotics to support accessibility and knowledge
acquisition, but also the current limitations and challenges of deploying such
technologies in complex, real-world environments.

</details>


### [159] [Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning](https://arxiv.org/abs/2507.12391)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文评估了多模态大型语言模型（LLM）在机器人路径规划中的表现，发现视觉输入对提升模型性能有限，尤其在更大、更复杂环境下模型表现下降。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多模态任务中的应用逐渐增多，作者关注其在机器人路径规划这一关键任务中的潜力，尤其想验证视觉输入是否能提升路径规划能力。

Method: 作者搭建了一个2D网格环境，模拟简化的机器人路径规划任务，对15个多模态LLM进行了基准测试。测试内容包括在不同网格复杂度和模型规模下，以文本输入与文本+视觉输入两种方式生成有效且最优的路径。

Result: 在简单小网格环境中，多模态模型有中等成功率，视觉输入或few-shot文本提示有一定帮助。随着网格变大，所有模型表现显著下降，揭示了扩展性上的困难。总体来看，大模型表现更好；视觉模态在这些系统中未必优于精良文本输入。

Conclusion: 当前多模态LLM在空间推理、约束遵守和大规模集成方面存在明显不足。视觉输入对路径规划的提升有限，尤其是复杂环境下。未来需加强这些方面以提升机器人路径规划任务的能力。

Abstract: Large Language Models (LLMs) show potential for enhancing robotic path
planning. This paper assesses visual input's utility for multimodal LLMs in
such tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on
generating valid and optimal paths in 2D grid environments, simulating
simplified robotic planning, comparing text-only versus text-plus-visual inputs
across varying model sizes and grid complexities. Our results indicate moderate
success rates on simpler small grids, where visual input or few-shot text
prompting offered some benefits. However, performance significantly degraded on
larger grids, highlighting a scalability challenge. While larger models
generally achieved higher average success, the visual modality was not
universally dominant over well-structured text for these multimodal systems,
and successful paths on simpler grids were generally of high quality. These
results indicate current limitations in robust spatial reasoning, constraint
adherence, and scalable multimodal integration, identifying areas for future
LLM development in robotic path planning.

</details>


### [160] [Regrasp Maps for Sequential Manipulation Planning](https://arxiv.org/abs/2507.12407)
*Svetlana Levit,Marc Toussaint*

Main category: cs.RO

TL;DR: 本文提出了一种在限制多、环境复杂的操作任务中，通过构建regrasp map（重新抓取地图），增强任务与运动规划（TAMP）求解器以加速搜索、多次抓取路径的优化方案。该方法通过猜测切换模式和额外约束，实现对复杂抓取任务的高效求解。


<details>
  <summary>Details</summary>
Motivation: 在受限、杂乱的环境中，操作任务往往需多次调整抓取位置（regrasp），而这些抓取点通常事先未知。这类任务对任务与运动规划（TAMP）方法带来极大挑战，现有方法效率较低，因此亟需新的高效搜索策略。

Method: 作者提出了构建和迭代regrasp map的方法：将不同空间中可能抓取组合抽象进状态空间，为TAMP求解器提供模式切换的参考和放置约束。通过交替生成、根据失败结果适应性更新regrasp map，并子任务分解求解，提高搜索的鲁棒性与效率。

Result: 该方法在解决需要多次regrasp的复杂操作任务上取得了显著的鲁棒性提升和搜索效率提升。

Conclusion: 通过regrasp map与任务—运动规划的结合，能有效解决多次regrasp需求下的复杂操作任务，具备很强的实用性和适应性。

Abstract: We consider manipulation problems in constrained and cluttered settings,
which require several regrasps at unknown locations. We propose to inform an
optimization-based task and motion planning (TAMP) solver with possible regrasp
areas and grasp sequences to speed up the search. Our main idea is to use a
state space abstraction, a regrasp map, capturing the combinations of available
grasps in different parts of the configuration space, and allowing us to
provide the solver with guesses for the mode switches and additional
constraints for the object placements. By interleaving the creation of regrasp
maps, their adaptation based on failed refinements, and solving TAMP
(sub)problems, we are able to provide a robust search method for challenging
regrasp manipulation problems.

</details>


### [161] [Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement](https://arxiv.org/abs/2507.12431)
*Connor Burgess,Kyle Douin,Amir Kordijazi*

Main category: cs.RO

TL;DR: 本文介绍了一种全自动化接触角测试仪（ACAT），能够自动测量3D打印材料表面的润湿性，通过机械、电子和软件的集成实现高精度、高重复性和高安全性的自动测试。


<details>
  <summary>Details</summary>
Motivation: 手动接触角测试存在精度不高、可重复性差和效率低等不足，限制了3D打印材料表面性能的快速表征和高通量测试需求。

Method: 设计并实现了一个集成的机器人工作单元，包括:1）符合工业标准的电气控制和安全系统；2）以Raspberry Pi与Python为基础，集成故障检测与操作界面的软件控制系统；3）由三轴机器人、气动执行和高精度液体分配组成的机械系统，整体包覆在安全框架内。

Result: ACAT设备实现了高通量、高自动化的表面润湿性表征，提升了测试的精确性、一致性和操作安全性。

Conclusion: ACAT为3D打印材料表面测试提供了高效自动化平台，并为未来在智能制造、材料发现等领域的集成应用奠定了基础。

Abstract: The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work
cell developed to automate the measurement of surface wettability on 3D-printed
materials. Designed for precision, repeatability, and safety, ACAT addresses
the limitations of manual contact angle testing by combining programmable
robotics, precise liquid dispensing, and a modular software-hardware
architecture. The system is composed of three core subsystems: (1) an
electrical system including power, control, and safety circuits compliant with
industrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software
control system based on a Raspberry Pi and Python, featuring fault detection,
GPIO logic, and operator interfaces; and (3) a mechanical system that includes
a 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser
enclosed within a safety-certified frame. The ACAT enables high-throughput,
automated surface characterization and provides a robust platform for future
integration into smart manufacturing and materials discovery workflows. This
paper details the design methodology, implementation strategies, and system
integration required to develop the ACAT platform.

</details>


### [162] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本论文提出通过利用以第一人称视角的人类视频数据训练视觉-语言-动作模型，并通过逆运动学实现机器人操作，从而减少机器人硬件数据收集的限制。作者还提出了新的仿真基准，并证明了该方法在多任务操作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习高度依赖于真实机器人硬件收集数据，限制了数据规模和场景多样性。采用人类视频数据可以突破这一瓶颈，扩展任务和环境复杂性。

Method: 作者首先收集第一人称视角的人类操作视频，并训练视觉-语言-动作（VLA）模型以预测人类手腕和手部动作。之后，利用逆运动学和动作重定向将人类动作转化为机器人动作。最后结合少量真实机器人演示进行微调，形成最终的机器人策略（EgoVLA）。为评估性能，作者构建了Isaac Humanoid Manipulation Benchmark基准，包含多种双手操作任务。

Result: 实验证明，利用人类数据预训练和少量机器人数据微调的EgoVLA显著优于仅用机器人数据的基线方法，并通过消融实验说明人类数据对提升性能的重要性。

Conclusion: 利用大规模人类视角视频数据预训练，并通过少量真实演示微调的方法能有效扩展机器人的操作能力，提升泛化和效率。提出的仿真基准也有助于后续工作评估方法效果。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>
