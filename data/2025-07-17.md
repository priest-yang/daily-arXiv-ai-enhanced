<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 本文针对Deformable Attention Transformers（DAT）在硬件部署中由于不规则内存访问带来的效率瓶颈，提出一种硬件友好型优化框架，实现高效推理且几乎不损失准确率。


<details>
  <summary>Details</summary>
Motivation: DAT因其自适应地关注图像重要区域，在计算机视觉任务中表现优异，但其数据依赖的采样机制导致内存访问不规则，阻碍了高效的硬件部署。此外，现有的加速方法要么导致较高的硬件开销，要么牺牲模型精度，亟需在硬件效率和模型表现间取得更好平衡。

Method: 提出一种基于神经架构搜索（NAS）和新切片策略的方法，将输入特征自动划分为均匀patch，避免修改模型架构的前提下防止内存冲突，通过联合优化硬件代价与推理精度寻找最优切片配置。并设计了基于FPGA的验证系统，在边缘侧硬件上评估该方法。

Result: 在ImageNet-1K数据集上的算法实验显示，该方法相较原始DAT精度损失仅0.2%。在Xilinx FPGA上，经本方法处理后，DRAM访问次数降低至现有DAT加速方法的18%。

Conclusion: 该硬件友好型优化框架能在几乎不影响准确率的前提下，显著提升DAT在硬件上的运行效率，适用于边缘计算场景。

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [2] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为Deformable Dynamic Convolution Network (DDCN)的新型神经网络结构，用于提升空间-时间交通预测的准确性与效率，在实际数据集上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有的交通预测方法不能很好地捕捉地区和时间上交通模式的异质性，且主流的GNN由于需要预设邻接矩阵及高复杂度，难以扩展到大规模节点数据。传统CNN在建模非欧几里得空间和空间-时间异质性方面也有限。为了解决这些问题，提出DDCN。

Method: DDCN通过动态可变形卷积核（deformable filters）来灵活建模空间-时间异质性，结合transformer风格的CNN，以编码器-解码器结构实现。编码器部分引入空间和时空注意力机制突出重要特征，解码器利用前馈模块补充编码器输出。

Result: DDCN在四个真实交通数据集上进行了全面实验，表现出具有竞争力的预测性能，不仅准确，而且高效。

Conclusion: 实验结果表明，基于CNN的方法，通过结构创新和动态建模，可以达到高效且准确的空间-时间交通预测，有效突破了现有GNN和传统CNN的局限。

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [3] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了Inversion-DPO，这是一种避免了高计算消耗奖励建模的新扩散模型对齐方法，通过DDIM逆推和直接偏好优化（DPO），显著提升了生成模型的精度与训练效率。


<details>
  <summary>Details</summary>
Motivation: 目前主流的扩散模型对齐方法依赖于训练基础模型和奖励模型，这不仅计算开销大，而且可能影响模型准确性和训练效率。作者希望提出一种高效且无损的对齐方法。

Method: 作者提出Inversion-DPO方法：通过将DPO和DDIM inversion结合，绕过构建奖励模型的步骤，而是通过无损确定性逆推采样来直接对扩散模型后验分布建模，进而实现后训练。

Result: 采用Inversion-DPO在文本到图像生成以及复合图像生成任务上进行了实验，相较现有后训练方法表现出明显的性能提升。并在复杂结构标注和综合得分的图像对数据集上，显著增强了模型的复合生成能力。

Conclusion: Inversion-DPO为扩散模型提供了一种高效且高精度的对齐新路径，避免了奖励建模带来的问题，提升了生成模型在复杂现实生成任务中的适用性。代码也已开源。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [4] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新框架ST-VFM，将视觉基础模型（VFM）系统化地改编用于时空预测任务，并在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）虽被用于时间序列预测，但难以建模时空相关性。视觉基础模型（VFM）天然具备空间建模能力，但缺乏对时序和时空数据的内在建模，尚未能用于通用时空预测。

Method: 提出ST-VFM，采用双分支结构处理原始时空输入和辅助时空流（flow）输入，通过两个重编程阶段：1）预VFM阶段用时间感知Token适配器对输入进行时序上下文嵌入与对齐，2）后VFM阶段用双向跨提示协调模块实现两分支特征间的动态交互。同时VFM主干保持冻结。

Result: 在十个常用时空数据集上，ST-VFM在准确率和鲁棒性方面均优于当前主流方法，且在不同VFM主干（如DINO、CLIP、DEIT）下表现突出。消融实验验证了各模块有效性。

Conclusion: ST-VFM能够充分激发视觉基础模型空间先验，成功适应一般性时空预测任务，为领域推广提供了有力工具。

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [5] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 论文提出了xOp-GAN，一个包含多个专家生成器的生成对抗网络（GAN）模型，用于应对复杂水下图像退化的修复问题，大幅度提升了水下图像恢复任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的方法通常采用单一生成器，难以应对水下图像中复杂且多样的失真情况。因此需要一种能够针对不同图像质量有针对性修复的更灵活结构。

Method: xOp-GAN采用多个专家生成器网络，每个生成器专门针对特定质量分布的子集进行训练。推理时，所有生成器分别对输入图像进行恢复，然后由判别器基于感知置信度选择效果最佳的修复结果。

Result: 在大型水下图像数据集LSUI上，xOp-GAN获得了最高25.16 dB的PSNR，大幅超过了所有单一回归器模型，同时保持了较低的模型复杂度。

Conclusion: xOp-GAN首次实现了判别器在GAN回归任务推理阶段的作用，并通过多个生成器提升了复杂场景下的水下图像修复效果，具有显著的实际应用前景。

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [6] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: 本论文综述并实证分析了基于步态估算年龄的方法，提出多模态融合与深度学习模型在步态年龄估算领域的准确率和应用前景。


<details>
  <summary>Details</summary>
Motivation: 基于步态的年龄估算对于医疗、安防与人机交互等领域具有重要应用价值，但该任务受限于多个因素，准确度有限，缺乏统一的基准和实践指导。作者致力于综述现有进展并提出提升精度的方法。

Method: 作者回顾了59项涉及7万5千名受试者的数据，分析了不同传感器（视频、可穿戴、雷达）和模型（卷积神经网络、惯性传感器模型、多传感器融合）的表现。同时，基于OU-ISIR大规模数据集，研究了年龄与五项步态指标的相关性，并用ResNet34+Grad-CAM解释网络关注区域。最后，在VersatileGait数据库上比较多种主流机学习和深度学习方法效果。

Result: 卷积神经网络平均误差为4.2年，惯性传感器模型为4.5年，多模态融合最低可至3.4年。步长、速度、步频等指标与年龄的相关系数至少为0.27。ResNet34主要关注膝部与骨盆，深度模型准确率最高达96%并且速度极快。

Conclusion: 通过综合文献分析和大规模实证，本文给出了步态年龄估算领域的性能基准，并提出有望将实际场景估算误差降至3年以内的实用建议，为后续应用提供了可靠的参考。

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [7] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的野猫个体重识别方法（PPGNet-Cat），能够高效、准确地通过相机陷阱照片监测野猫个体，对野生动物保护具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 野猫对澳大利亚野生动物造成了极大威胁，已经成为全球最危险的入侵物种之一。有效监测野猫个体对于减少其危害至关重要，因此亟需能够自动、准确识别野猫个体的新型技术手段。计算机视觉（CV）和re-ID技术为野猫监测提供了潜力。

Method: 本研究在Amur虎re-ID模型PPGNet基础上，针对野猫的图像特点改进模型，提出PPGNet-Cat。同时进行了对比学习（如ArcFace loss）等多种实验，探讨并优化模型性能。

Result: PPGNet-Cat模型在野猫个体识别任务中取得了优异成绩，平均精确度（mAP）达0.86，Rank-1准确率达0.95，表现出强竞争力。

Conclusion: PPGNet-Cat能够有效、高性能地完成野猫个体识别任务，为利用re-ID技术监控野猫、保护澳大利亚野生动物提供了有力工具。

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [8] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: 本文提出了一种名为SketchDNN的生成模型，通过联合连续-离散扩散过程，用于高质量CAD草图的生成。核心创新是引入了Gaussian-Softmax扩散方法，有效提升了生成效果，并在SketchGraphs数据集上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前CAD草图生成面临连续参数与离散类型联合建模的困难，尤其是原始结构的异质性和元素排列的不变性尚未得到很好的解决。现有方法在生成质量和灵活性方面仍有较大提升空间。

Method: 采用Gaussian-Softmax扩散方法，将带有高斯噪声的logits投影至概率单纯形，从而支持离散变量的混合类型建模。同时联合建模连续参数和离散类型，并针对CAD草图原始参数结构的异质性与元素排列的不变性进行建模优化。

Result: 在SketchGraphs数据集上测试，FID从16.04降至7.80，NLL从84.8降至81.33，显著优于已有方法，取得了最佳的生成效果。

Conclusion: SketchDNN有效兼顾了CAD草图中连续与离散参数的联合建模，采用Gaussian-Softmax扩散方法后，生成质量大幅提升。其方法在实际CAD生成任务中具有较强的适用性和推广价值。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [9] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 本论文提出用变分自编码器（VAE）取代传统大型CNN模型，对直肠癌MRI数据中的淋巴结转移进行分期判断，实现了更高的准确率和解释性。


<details>
  <summary>Details</summary>
Motivation: 目前临床常用的淋巴结影像学分期标准（基于大小、形状、纹理）准确率有限，现有深度学习方法依赖大型预训练CNN，但其特征解释性较弱。本文尝试用VAE模型获得结构化、可解释性更强的图像特征表征，以提升分期诊断效果。

Method: 本文将VAE作为特征编码器，替换掉传统的大型CNN，对168例未经新辅助治疗的直肠癌患者MRI数据进行建模。利用病理N分期作为真值，对模型预测进行评估，并与传统方法对比。

Result: 所提VAE-MLP模型在数据集上表现优异，交叉验证AUC达到0.86±0.05，灵敏度0.79±0.06，特异性0.85±0.05，均优于传统方法。

Conclusion: VAE编码器不仅提升了分期诊断的准确性，还增强了模型特征的可解释性，为直肠癌淋巴结转移的影像学分期提供了新的技术路径。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [10] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 该论文提出了一种通过姿态分析从运动视频中推断运动员心理状态的方法，并在板球运动中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 通过识别运动员的意图和心理状态，可以在疲劳诊断、防止受伤和提升表现等多个领域带来重要价值。传统的视觉诊断受到数据敏感性的限制，亟需找到可行的数据收集和分析新方式。

Method: 以板球运动为例，采集运动员在多种情绪状态下的动作视频，利用运动分析技术，通过姿态识别来判别进攻或防守意图。同时，利用现有数据统计作为弱监督手段，提高数据标注效率。

Result: 方法在判别进攻与防守意图时，F1分数超过75%，AUC-ROC超过80%。说明姿态中蕴含强烈的意图信号，即使在数据存在噪声的情况下依然效果显著。

Conclusion: 运动场景为采集高质量心理状态数据提供了可能。该项研究为运动分析及更广泛的人类行为分析提供了通用技术基础，并为数据标注受限难题提出了可行解决方案。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [11] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: 提出了一种名为VISTA的新型全局定位框架，利用基于物体分割和跟踪的方法进行参考帧的对齐，可在不同视角和季节下实现高效定位，并且不仅精度提升显著，还极度节省内存。


<details>
  <summary>Details</summary>
Motivation: 在无人自主导航中，特别是在地图由不同时间、设备生成时，如何实现全球定位（全局定位）是难题。现有方法通常受视角、季节变化、空间混淆和遮挡影响，可靠性不足。

Method: VISTA框架包括：1）物体级别的分割与跟踪前端；2）利用环境地图的几何一致性进行子地图对应搜索，从而实现车辆参考帧的对齐。该方法无需领域特定训练或微调，即可适应多视角、季节变化。

Result: 在季节变化和俯视角航拍数据集上，VISTA的召回率比基线提升了最多69%；同时所需地图内存仅为最节省内存基线的0.6%，支持实时运行。

Conclusion: VISTA能够在资源有限平台上，实现对多种环境和视角变化的高适应性和高精度的全局定位，显著优于传统方法。

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [12] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: 本文对比评估了最新的多模态视觉-语言模型（VLMs）与轻量化CNN-OCR方案在户外广告文本可见性检测中的表现，并引入天气扰动模拟真实退化，发现VLMs在整体场景理解方面表现优异，但CNN方案在裁剪文本识别下效率极高，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 传统OCR在复杂户外场景（如广告牌）下，由于字体变化、天气噪声等因素表现不佳，而新兴的多模态视觉-语言模型可能提供更强的端到端场景理解能力。因此，迫切需要系统性对比这两类方法在真实、复杂环境下的性能。

Method: 选取了主流VLM模型（Qwen 2.5 VL 3B、InternVL3、SmolVLM2）与CNN-OCR基线（PaddleOCRv4），在公开数据集（ICDAR 2015及SVT）上，结合合成的天气干扰进行系统化测试和基准评测。

Result: 结果显示，多模态VLM模型在整体场景推理能力上确实优于传统OCR，但在仅针对裁剪文本的纯文本识别任务上，轻量级CNN方案依然能以极低计算成本取得有竞争力的准确率。

Conclusion: 多模态VLM为户外广告等复杂场景下的文本检测带来新希望，能实现更强的场景理解。但在资源受限场景，CNN-OCR方案仍具实际应用价值。该文还公开了天气增强的基准数据和评测工具，为后续研究提供了新平台。

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [13] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 本文提出了一种统一的条件生成式求解器（UCGS），可以用一个模型同时应对各种抽象视觉推理任务，并表现出零样本推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的深度AVR（抽象视觉推理）方法通常针对不同任务设计特定结构或参数，需要重复训练和调整模型，效率低下、泛化性差，难以像人类一样在新任务上灵活迁移。作者希望突破这一瓶颈，实现多任务统一和更好的迁移泛化能力。

Method: 作者提出了统一条件生成式求解器（UCGS）。他们首先将典型AVR任务统一描述为目标图像可预测性估计问题，在这一框架下，仅需训练一个条件生成模型即可跨任务进行推理。模型采用多任务联合训练方式，避免为每个任务专门设计结构。

Result: 实验结果显示，UCGS经过一次多任务训练后，在多种AVR任务上展现了良好的抽象推理能力，并且具备零样本推理能力，在未见过的任务上也能进行抽象视觉推理。

Conclusion: UCGS解决了多任务AVR模型难以统一及泛化差的问题，用单一模型达成跨任务抽象推理和零样本迁移，为人工智能系统人类式推理能力的提升提供了新思路。

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [14] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: 本文提出了CorrMoE框架，通过引入去风格化双分支和双融合专家混合模块，有效提升了图像对应点剔除的鲁棒性，特别是在跨域与跨场景情况下精度明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像对应点剔除方法多假定输入图像具有一致的视觉域，难以应对不同场景结构带来的域间与场景多样性问题。

Method: 1. 提出De-stylization Dual Branch（去风格化双分支），分别在隐式和显式图特征进行风格混合，减缓域表示偏差对鲁棒性的影响。
2. 设计Bi-Fusion Mixture of Experts（双融合专家混合）模块，利用线性复杂度注意力和动态专家路由，实现多视角特征的自适应融合。

Result: 在多个基准数据集上实验表明，CorrMoE的准确率和泛化能力明显优于当前先进方法。

Conclusion: CorrMoE能够有效解决跨域和跨场景条件下的图像对应点剔除问题，具备较强的泛化性和实用价值。

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [15] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: 本文提出ProtoConNet，通过增强和对齐原型，实现更高效的开放集小样本图像分类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本开放集图像分类方法主要依赖单张图片视觉信息，忽略了丰富的上下文信息，导致类别区分能力受限，泛化能力不足。为此，作者希望通过引入多样上下文，提升模型的判别和泛化能力。

Method: 提出ProtoConNet方法，由三部分组成：1）基于聚类的数据选择(CDS)，挖掘多样数据模式并保留核心特征；2）语义上下文增强模块(CSR)，构建上下文字典提升特征表现；3）原型对齐模块(PA)，缩小图像特征与类别原型的差距，放大已知与未知类别间距离。

Result: 在两个公开数据集上的实验结果表明，该方法能有效提升小样本下的特征表征能力，对开放集样本的识别效果也优于现有方法。

Conclusion: ProtoConNet在开放集小样本图像分类任务中表现优异，能够提升模型的特征多样性与鲁棒性，在不同场景下具有更好的泛化和识别新类别能力。

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [16] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的动态面部表情识别方法GRACE，通过细粒度语义文本与动态视觉特征的对齐，显著提升了识别效果，尤其是在情感类别不平衡或难以区分的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言方法虽然引入了文本描述辅助情感识别，但对文本中细微情感线索的利用不足，同时缺乏有效机制筛除与表情无关的动态，人脸表情识别的精度受到限制。

Method: 提出GRACE方法，融合动态运动建模、语义文本精炼与Token级跨模态对齐。采用CATE模块优化情感文本描述，并通过运动差异加权机制突出与表情相关的人脸动态特征，最终用熵正则化最优传输实现细粒度文本-视觉对齐。

Result: 在三个主流数据集上，GRACE方法在识别准确率（UAR和WAR）方面均取得了最新的SOTA成绩，尤其在情感类别模糊或不均衡情况下表现突出。

Conclusion: GRACE通过细粒度动态与文本对齐，显著提升了动态面部表情识别的效果，为跨模态情感理解提供了新思路。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [17] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: 该论文提出了空间频率调制（SFM）机制，通过在下采样前将高频特征调制为低频并在上采样时恢复，有效缓解高频信息混叠和细节损失问题，提高了语义分割等任务的表现。


<details>
  <summary>Details</summary>
Motivation: 高空间频率成分承载了图像的细节信息，但在如步幅卷积等下采样过程中易受失真和混叠影响，导致语义分割效果下降。因此，如何在深度网络中保留和恢复高频信息是一个重要挑战。

Method: 作者提出空间频率调制（SFM）方法。在下采样前，通过自适应重采样（ARS）将高频特征信号‘缩放’至低频域降低混叠风险，随后在上采样过程用多尺度自适应上采样（MSAU）模块对其进行非均匀恢复，增强高低频区域的信息交互。该方法为轻量级插件，可兼容多种网络架构。

Result: 可视化和实验均表明，SFM显著缓解了混叠问题并有效保留了图像细节。同时，SFM广泛适用于语义分割、分类、对抗鲁棒性、实例分割和全景分割等多种任务，提升了整体性能。

Conclusion: SFM是一种通用且高效的高频信息处理方案，能够在不增加太多计算量的前提下集成到现有模型中，显著改善下采样带来的高频损失，对于多种视觉任务具有良好的推进作用。

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [18] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了CompressedVQA-HDR框架，专为高动态范围（HDR）视频压缩质量评估设计。采用Swin Transformer和SigLip 2作为全参考（FR）与无参考（NR）模型的核心骨干，并通过多种预训练与微调策略处理HDR数据有限的问题，最终在多个数据集上实现了最优性能并在相关竞赛中获奖。


<details>
  <summary>Details</summary>
Motivation: 当前视频压缩质量评估方法在面对视频内容日益多样化，特别是高动态范围（HDR）视频时存在泛化能力不足的问题。作者旨在设计一种更适应HDR视频质量评估需求的新型方法。

Method: 提出一个名为CompressedVQA-HDR的评估框架。FR模型采用Swin Transformer提取结构和纹理特征以衡量参考与失真帧的相似性，NR模型则利用SigLip 2的最后层特征均值进行质量感知。为缓解HDR训练数据稀缺的问题，FR模型先在SDR大数据集上预训练再微调，NR模型则通过混合多数据集迭代训练并微调。

Result: 提出的FR和NR模型在多个VQA数据集上取得领先表现，并在IEEE ICME 2025的相关竞赛中荣获FR赛道第一名。

Conclusion: CompressedVQA-HDR框架有效提升了HDR视频压缩质量评估的准确性和泛化性，展示出优于现有主流方法的能力，有望大幅推动HDR视频质量实际评估和压缩技术发展。

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [19] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 本文提出了SEPose，一个针对行人感知的合成事件基人体姿态估计数据集，用于提升事件相机在行人和交通监测系统中的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机在低延迟和高动态范围下，能有效应对行人和交通监测中的极端或危险场景（如分心行走），但相关极端场景的数据资源稀缺，限制了研究和应用的发展。因此需要新的、大规模、覆盖多场景的数据集。

Method: 作者基于CARLA模拟器，利用动态视觉传感器生成高达35万例带有人体关键点注释的行人数据，涵盖城市、郊区和农村多种场景与天气下的4路交叉路口，模拟了固定交通摄像机视角下的真实应用环境，并支持多人姿态估计。

Result: 作者在SEPose数据集上训练了如RVT和YOLOv8等主流姿态估计算法，并在真实事件基础数据上做测试，验证了SEPose在合成到实际应用（sim-to-real）中的良好泛化能力。

Conclusion: SEPose数据集极大丰富了事件相机下行人姿态估计的训练资源，有助于提升事件视觉在真实复杂交通场景中的应用表现，推动仿真到现实的泛化研究，并对安全关键场景的监测系统具有重要意义。

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [20] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出Dark-EvGS框架，实现了在低光环境下利用事件相机及3D高斯喷溅（GS）重建多视角的高质量明亮图像。


<details>
  <summary>Details</summary>
Motivation: 传统相机在低光环境下因动态范围受限和长曝光导致的运动模糊，难以获取清晰的多视角图像。事件相机具备高动态范围和高速性能，结合3D高斯喷溅方法有潜力提升低光多视角成像质量，但面临事件数据噪声大、帧质量差及色调不一致等困难。

Method: 提出了Dark-EvGS，这是首个事件辅助的3D高斯喷溅（GS）框架，可重建沿相机轨迹任意视角的明亮帧。方法引入三元组级监督以增强特征学习和细节渲染，并设计色调匹配模块保证渲染帧色彩一致性。此外，作者公开了首个用于事件引导的低光明亮帧合成任务的真实数据集。

Result: 实验结果显示，Dark-EvGS在低光条件下的辐射场重建效果优于现有方法，有效生成质量更高的明亮多视角图像。

Conclusion: Dark-EvGS突破了低光下多角度高质量成像难题，为事件相机辅助3D辐射场重建与明亮图像合成提供了新方法，并且具有更好的色调一致性和细节表现，推动了相关领域发展。

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [21] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: 本文提出了Hyperphantasia基准，用于评估多模态大语言模型（MLLMs）的心理可视化能力，发现人类与模型在该任务上表现差距显著，模型在内部视觉模拟方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型主要评估被动视觉感知，缺乏对主动、内部构建视觉表征能力（如心理可视化）的评估。但人类利用心理可视化在空间导航、物理预测与问题解决中具有关键作用。为此，作者希望弥补MLLM评估中的这一空白。

Method: 作者提出Hyperphantasia，一个完全合成的基准，包括四类程序生成的谜题，并设置三个难度等级，专门用于测量MLLM内部构建和操控视觉表征的能力。同时，对现有最先进模型进行综合测试，还探索了强化学习手段以提升模型心理可视化能力。

Result: 评估表明，当前最先进的MLLM在心理可视化任务中的表现与人类相差甚远，仅展现出有限的视觉模式识别能力；强化学习虽有一定提升，但仍难以应对复杂的视觉模拟任务。

Conclusion: 尽管部分MLLM具备简单视觉模式识别能力，强大、鲁棒的心理可视化能力仍是当前MLLM面临的重要挑战，对未来模型设计与评测提出了新方向。

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [22] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: RaDL框架通过关系注意机制和可学习参数，有效提升了多实例图像生成中的实例属性表达及实例间关系建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像(T2I)方法在生成多实例场景时，难以准确还原实例间的关系和保持多属性信息，导致属性泄漏和关系失真问题。

Method: 提出了关系感知的解耦学习（RaDL）框架。该方法通过可学习参数增强实例特有属性表达，并基于从全局提示词中提取的动词实现关系感知的图像特征生成，具体采用Relation Attention机制。方法在COCO-Position、COCO-MIG和DrawBench等数据集上进行了系统评测。

Result: 实验证明RaDL在位置精度、多属性表达及实例关系建模方面均优于现有方法，实现了显著改进。

Conclusion: RaDL为多实例场景下的文本到图像生成任务提供了有效方案，可同时兼顾实例间的关系和实例的多属性信息。

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [23] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc是一种新型定位系统，结合3D Gaussian Splatting与语义信息，实现无需先验位姿信息的摄像头6自由度定位，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有定位方法通常依赖初始位姿信息或在2D-3D匹配中效率和准确率受限。3D Gaussian Splatting作为新兴的3D场景表达方式，其与2D图像的语义关联尚未被充分利用，用于提升无初始先验下的全球定位能力。

Method: SGLoc结合3DGS语义信息，提出多级位姿回归策略：首先通过语义特征，将2D图像和3DGS场景进行全局检索匹配，获得粗略位姿；随后基于3DGS渲染结果与输入图像的差异，采用迭代优化，实现位姿细化。整个系统不需要初始位姿先验。

Result: SGLoc在12scenes与7scenes等公开数据集上的全球定位任务中表现优异，超越了多个基线方法，特别是在无初始位姿先验的条件下表现出色。

Conclusion: SGLoc作为一套无需初始先验位姿的端到端定位系统，充分利用了3D Gaussian Splatting的语义信息，在多个室内场景定位任务中验证了其实用性和优越性，具有较强推广潜力。

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [24] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的语义分割通用化框架PPAR，结合CLIP模型强化跨域泛化能力，通过原始文本与视觉文本原型渐进对齐和重加权机制，有效提升了在多领域基准的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于类原型的语义分割跨域泛化方法易受粗糙对齐、原型过拟合及样本粗等权影响，导致性能受限。因此，作者旨在提升语义分割模型在未见领域的泛化能力。

Method: 提出基于CLIP的Prototypical Progressive Alignment and Reweighting（PPAR）框架。定义两类原型：原始文本原型（OTP）和视觉文本原型（VTP），以CLIP生成。采用渐进式对齐策略，从易到难拉近特征空间分布，并通过原型重加权机制估计源域数据可靠性，降低无关或有害样本影响。同时，给出理论分析佐证方法合理性。

Result: 在多个基准数据集上，PPAR方法均取得了性能新高，显著优于现有对比方法。

Conclusion: PPAR框架有效解决了原型对齐和样本差异带来的泛化瓶颈，证明了渐进对齐与重加权机制的有效性，为跨域语义分割提供了可推广的解决思想。

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [25] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动驾驶场景下交通参与者运动预测方法，通过显式推理交通参与者的行为意图提升轨迹预测的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前大多数运动预测方法基于数据直接预测未来轨迹，缺乏对行为意图的推理，导致解释性不足且难以提升预测自信度。因此，研究者希望引入行为意图推理，增强预测结果的合理性和可解释性。

Method: 该方法提出“先推理，后预测”，采用基于反向强化学习（IRL）的奖励驱动意图推理框架，首先对交通参与者及场景元素进行向量化编码和上下文聚合，得到奖励分布，再通过策略滚动推理多种可能的意图，辅助后续轨迹生成。轨迹生成采用类似DETR的分层解码器，并结合双向选择性状态空间模型，输出精准的未来轨迹及其概率。

Result: 在Argoverse和nuScenes等大规模运动预测数据集上实验，结果显示该方法提升了轨迹预测的可信度，性能达到甚至超过现有主流方法。

Conclusion: 显式引入基于奖励的行为意图推理，可显著提升自动驾驶中运动预测的准确性与可解释性，对提升自动驾驶安全性有重要意义。

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [26] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: 提出了一种结合文本引导的音频-视觉对比自编码器（LG-CAV-MAE），通过引入文本信息提升多模态表示学习表现，且无需人工标注即可生成训练数据。实验显示该方法在相关任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前音视频多模态表示学习的效果受限，主要依赖大量标注数据，且未充分利用文本信息。作者希望通过结合文本数据，提高跨模态学习能力，减少对人工标注的依赖。

Method: 提出LG-CAV-MAE模型，把预训练的文本编码器引入对比式音频-视觉自编码器，实现音频、视觉和文本的跨模态联合学习。训练数据通过对无标注视频自动生成音频-视觉-文本三元组（由图像描述和CLAP过滤确保质量），无需人工标注。

Result: 在音视频检索和音视频分类任务上，LG-CAV-MAE取得了显著提升。如在检索任务中recall@10提升5.6%，分类任务提升3.2%。

Conclusion: 利用自动生成且高质量的音频-视觉-文本三元组进行训练，可以显著提升多模态模型的检索与分类表现，为减少人工标注和提升跨模态学习能力提供了新途径。

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [27] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型（VLM）的自动数据清洗新方法AutoVDC，可自动识别自动驾驶数据集中错误标注，提升数据质量，并在公开数据集实验中获得了优异的错误检测和数据清洗效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统的训练依赖大量高质量带标注的数据集，但人工标注存在错误且审核成本高，因此急需高效可靠的自动数据清洗方案以保证数据集质量。

Method: 提出AutoVDC框架，利用视觉-语言模型自动识别数据集中的错误标注。方法在KITTI和nuImages两大自动驾驶对象检测数据集上进行验证，通过人为注入标注错误，用不同的VLM对检测能力进行评测，并研究VLM微调对性能的影响。

Result: AutoVDC在错误检测和数据清洗实验中表现优异，不同VLM的检测率对比，微调也带来一定提升，验证了其较强的通用性和高效性能。

Conclusion: AutoVDC能够高效提升大规模自动驾驶数据集的准确性和可靠性，降低人工审核成本，具有重要的实际应用价值。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [28] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 该论文针对短视频领域多模态大模型（MLLMs）在内容审核中的安全性展开研究，提出了三模态攻防评测框架，从数据集、攻击方法和实验验证三方面揭示现有模型的脆弱性，并为未来改进提供方向。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在短视频内容审核中应用广泛，但其面临的安全风险和鲁棒性问题尚未充分研究，尤其是通常只关注单一模态攻击，忽略了多模态联合攻击下的安全隐患。为此，作者试图构建能更全面评测MLLMs安全性的框架和数据集，弥补现有研究空白。

Method: 作者首先构建Short-Video Multimodal Adversarial (SVMA)数据集，包含多样短视频及人工引导的合成三模态对抗攻击。其次，提出三模态攻击策略ChimeraBreak，针对视觉、听觉和语义推理通路同时发起挑战。最后，通过丰富实验量化MLLMs在新任务下的攻击成功率，并用大模型评审手段分析攻击推理过程。

Result: 实验证明，现有多模态大模型在三模态攻击下表现出严重漏洞，攻击成功率高，容易将合规与违规内容误判为对方类别，暴露出模型显著的安全风险和偏见。

Conclusion: 本文揭示了当前MLLMs在多模态短视频内容审核中未被充分认知的脆弱性，提出的数据集与评测框架将为后续提升模型鲁棒性与安全性提供重要参考。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [29] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视觉-语言模型测试时自适应方法GS-Bias，在提升泛化能力的同时，极大降低了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型的测试时自适应方法，普遍存在效率低下或增强效果不稳定的问题，难以兼顾性能和资源消耗。作者希望设计一种方法，既能显著提升模型无监督泛化能力，又具有极高效率。

Method: 提出Global-Spatial Bias Learner (GS-Bias)，在测试时为模型引入两类可学习偏置：全局偏置用于学习增强视图间全局语义一致性，空间偏置关注图像局部区域间的语义连贯。这些偏置直接加到预训练模型的输出上，无需端到端回传更新主干网络参数。

Result: GS-Bias在15个基准数据集上获得了新的SOTA表现。在ImageNet等任务上，对比主流方法TPT，跨数据集泛化提升2.23%，域泛化提升2.72%，内存占用仅为TPT的6.5%。

Conclusion: GS-Bias方法兼顾了高效性与优异效果，极大推动了VLM的实用性和推广能力，为测试时自适应提供了新思路。

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [30] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的边缘-云协同扩散模型推理框架EC-Diff，能够提升推理速度并兼顾生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面表现优秀，但模型变大和推理延迟增加影响了实际应用体验。现有的边缘-云协同框架虽能加速推理，但存在云端去噪过多或过少时导致推理时间过长或语义不清的问题。

Method: 作者提出了EC-Diff，利用梯度估计实现云端噪声加速推理，并通过K步噪声近似策略减少云端推理频率。同时，设计了两阶段贪心搜索算法，自动寻找最佳噪声近似和边缘切换参数，以最大化质量和速度的平衡。

Result: 实验表明，该方法在生成质量上明显优于纯边缘推理模式，且相比纯云端推理可平均加速2倍。

Conclusion: EC-Diff能够有效提升扩散模型在边缘-云场景下的推理效率与生成质量，具备较强的实际应用价值。

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [31] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Masked Part Autoencoder (MPAE)的无监督部件发现新方法，能够在无需精细标注的情况下，从图片中鲁棒地发现有意义的部件。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度部件特征对于图像理解非常重要，但传统方法由于缺乏标注和现有无监督方法在多类别及复杂场景下不够鲁棒，限制了其应用范围。

Method: MPAE方法首先从图像中学习部件描述符和特征图，通过对图像局部随机遮挡生成patch特征，并用已学习的部件描述符填充被遮挡区域，利用未遮挡区域的外观引导对部件位置和形状的对齐。其引入了更宽松但有效的约束条件，提升了部件发现的普适性和鲁棒性。

Result: 大量实验表明，MPAE能在不同类别和复杂场景下鲁棒地发现与真实物体部件相匹配的有意义的部件区域。

Conclusion: MPAE为无监督部件发现提供了一种更有效的新范式，为处理遮挡和探索多类别部件相似性等挑战奠定了基础，具有较强的实用性和拓展性。

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [32] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型方法，实现了区域可控的多风格图像生成，用户可以通过空间掩码在图像的不同区域融合不同风格，同时保持结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的文本到图像生成模型虽然能够反映风格个性化，但在实现多个绘画风格的区域化控制时存在困难，常常出现某一种风格主导全图的现象，缺乏平滑过渡和细致的空间风格融合能力。

Method: 作者提出了一种零样本扩散管线，通过空间掩码和风格专用的已训练扩散模型，对预测的去噪潜变量进行风格合成，并在扩散过程中利用低噪声潜变量进行风格信息的融合。同时，为了保持结构一致性，引入了ControlNet的深度图引导。

Result: 实验表明，该方法能够根据所给掩码，在图像中实现精准的区域风格混合，保持每种风格的真实感和结构一致性，并通过质性与量化对比验证优于现有方法。

Conclusion: 该方法为多风格区域可控图像合成提供了有效手段，提升了风格融合的灵活性和图像结构的协调性，有望推动扩散模型在美术创作等细分领域的应用。

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [33] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ID-EA的新框架，有效提升了基于文本到图像扩散模型的个性化人像生成中的身份保持能力，并大幅加快了生成速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于Textual Inversion的文本到图像扩散模型在个性化人像生成中，仍然难以保证人脸身份的一致性，主要是因为文本和视觉嵌入空间中关于身份的语义无法有效匹配。

Method: 提出了ID-EA框架，包含两部分：ID-Enhancer和ID-Adapter。ID-Enhancer融合人脸识别模型提取的身份嵌入与文本ID锚点，优化视觉身份嵌入；ID-Adapter再利用增强过的身份嵌入调整文本条件，通过修改预训练UNet中的跨注意力模块，实现身份保持并提升生成效率。

Result: 大量定量和定性实验表明，ID-EA在身份保持指标上显著优于现有方法，并且在生成速度上实现了大约15倍的提升。

Conclusion: ID-EA有效解决了文本-视觉嵌入空间身份语义失配的问题，实现了更高质量、更高效率的个性化人像生成。

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [34] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: 本文提出了一种用于遥感图像语义分割的半监督方法SAMST，通过结合大模型（SAM）的泛化能力和小模型的训练效率，有效提升了伪标签质量和分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前公开的遥感数据集普遍存在分辨率差异大、地物类别定义不一致等问题，且标注数据有限，制约了遥感语义分割任务的普适性和性能提升。如何充分利用大量未标记的遥感数据，成为解决此类难题的关键。

Method: SAMST方法采用半监督学习框架，分为两个主要部分。其一是利用带标签和伪标签数据进行监督模型自训练；其二是基于SAM的大模型设计“伪标签精炼器”，包括阈值过滤模块、用于SAM激活的提示生成模块，以及最终标签拼接模块。该方法通过不断迭代优化伪标签，提高伪标签准确性。

Result: 在Potsdam数据集上的实验证明，SAMST能够有效提高伪标签质量和分割模型的整体性能，验证了该方法的有效性和可行性。

Conclusion: SAMST通过融合大、小模型优势，有效缓解了遥感语义分割中标注数据不足，以及类别不一致带来的挑战，对提升遥感图像理解具有重要意义。

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [35] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: 该论文提出了AUBlendSet，一个基于AU-Blendshape表示的3D人脸数据集，用于跨身份的精细化表情操控，并提出了AUBlendNet网络，能够实现风格化3D表情操控。实验证明该方法在多项任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前3D人脸动画虽取得进展，但缺乏细粒度、可风格化表情操控的合适数据集，难以实现精确控制和跨身份表情迁移，因此需要新数据集和方法支持更精细的人脸表情研究与应用。

Method: 作者构建了包含500个身份、32个标准面部表情单元（AUs）的AUBlendSet数据集，并基于此提出AUBlendNet网络，可针对不同角色风格学习AU-Blendshape基向量，从而实现精细、风格化的人脸表情操控。

Result: 通过在风格化表情操控、语音驱动表情动画、情感识别数据增强等任务上进行定量和定性实验，表明AUBlendSet数据集和AUBlendNet网络在3D表情动画任务中的有效性和潜力。

Conclusion: AUBlendSet是首个基于AU的连续型3D人脸表情操控数据集，AUBlendNet则是首个能够针对任意身份实现连续3D表情操控的网络。这两者为3D人脸动画领域提供了有价值的资源和工具。

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [36] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: 本文提出一种新的方法，名为频率动态注意力调制（FDAM），用于解决Vision Transformer（ViT）在低通滤波下导致细节和纹理信息损失的问题。FDAM通过两项技术（注意力反转和频率动态缩放）增强模型对不同频率特征的表达能力，并在多种视觉任务中取得了一致性提升。


<details>
  <summary>Details</summary>
Motivation: ViT尽管在视觉任务中表现强劲，但其层叠结构的低通滤波效应导致高频信息（如图像细节和纹理）逐层丢失。现有结构容易发生频率衰减和特征塌陷，影响模型性能，因此亟需改进模型的频率响应。

Method: 提出FDAM机制，包括两项核心技术：（1）注意力反转（AttInv）——在注意力矩阵中增加入高通滤波效果，实现与原低通滤波的互补，并动态组合；（2）频率动态缩放（FreqScale）——对不同频率成分加权，实现输出频率响应的细致调节。方法可无缝集成到现有ViT模型中。

Result: 通过特征相似性分析和有效秩评估证明，FDAM能有效避免表征塌陷。实验证明FDAM在SegFormer、DeiT、MaskDINO等模型，以及语义分割、目标检测和实例分割等多项任务中均带来稳定且一致的性能提升。在遥感检测任务中，在单尺度情境下取得SOTA结果。

Conclusion: FDAM是一种通用而高效的增强方法，能显著提升ViT在多种视觉任务中的表现，特别是在细节和纹理表达方面效果显著，具有广泛的应用潜力。

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [37] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无监督领域自适应方法MaskTwins，通过掩码重建与补充掩码一致性，实现了无需预训练即可跨领域图像分割性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有将掩码图像建模（MIM）和一致性正则在无监督领域自适应（UDA）中联系起来的方法只把掩码当做输入扰动，缺乏理论分析，未能充分发掘其在提取领域无关特征上的潜力。

Method: 作者将掩码重建重新表述为稀疏信号重建问题，理论证明了互补掩码的双重形式有助于提取领域无关的特征。在此基础上，提出了MaskTwins框架，将掩码重建直接整合进主训练流程，通过对互补掩码下的图像输出结果进行一致性约束，实现端到端的领域泛化。

Result: 大量实验验证了MaskTwins在自然与生物图像分割领域均优于其他基线方法，能有效提取领域不变特征。

Conclusion: MaskTwins为领域自适应分割提供了一种新的方向，无需单独预训练，在增强领域泛化、提高特征提取能力方面表现突出。

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [38] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的神经网络模型，通过fMRI数据对观看电影时的大脑活动进行编码与解码，实现了对视觉输入的预测与重建。


<details>
  <summary>Details</summary>
Motivation: 用深度学习方法揭示大脑如何处理自然视觉刺激，并提升用fMRI分析复杂视觉刺激（如电影）时的时间分辨率和解释能力。

Method: 利用编码-解码神经网络，结合时序卷积层对fMRI数据进行建模，可以整合连续电影帧的信息以桥接电影刺激与fMRI获得之间的时间分辨率差异。该模型对视觉皮层及其周边的体素活动进行预测，并可从神经活动中重建相应视觉输入。同时用显著性图分析参与视觉解码的重要脑区。

Result: 模型能够准确预测视觉皮层相关的体素活动，并重建包含边缘、面部和对比度等信息的视觉输入。通过显著性图发现，中枢枕区、梭状区和距状区在视觉解码中贡献最大，这些区域分别负责形状感知、复杂（如人脸）识别及基本视觉特征的处理。

Conclusion: 该方法通过深度学习模型实现了对观看电影时大脑视觉加工机制的探查，表明神经网络不仅能还原视觉输入，同时有助于理解大脑各区域在视觉处理中的分工。

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [39] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: cs.CV

TL;DR: 该论文提出了一种通过无监督机器学习建立肝脏MRI图像的组织词汇库，从而定量化评估弥漫性肝病治疗反应的方法，并在临床试验队列中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏有效工具能定量刻画和追踪肝脏疾病进展及对治疗的反应，尤其基于医疗成像的数据分析方法有待提升，而个性化治疗及新药开发依赖于高效的影像定量策略。

Method: 作者利用深度聚类网络，将肝脏MRI图像切片编码和聚类到低维潜在空间中，建立肝组织影像词汇库，并将其用于分析随机对照试验中NASH（非酒精性脂肪性肝炎）患者的肝脏组织变化。

Result: 该方法成功区分了治疗组与安慰剂组的肝脏变化路径，且比传统非影像生物标志物更优。此外，基于MRI词汇可预测肝组织活检特征，并在独立队列中获得了验证。

Conclusion: 通过无监督深度学习建立的肝脏组织影像词汇，有望成为非侵入性、量化治疗反应和疾病进展的新工具，适用性获得初步验证，对相关临床与研究有重要意义。

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [40] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: 本文关注于从可见光域到红外域的无监督领域自适应目标检测问题，提出通过频谱分解精细解耦领域不变与特定特征，并利用空间-频谱联合策略增强检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前RGB-IR领域适应方法通常将RGB看作单一域，忽略了其下属的昼夜、雾天等子域间的特征多样性，导致领域适应效果受限。针对这一问题，作者认为应解耦不同子域间的领域不变和特定特征，以提升RGB到IR域的泛化能力。

Method: 提出SS-DC框架，包含频谱自适应幂等解耦（SAID）模块，利用频谱分解更精确地解耦领域不变与特定特征；通过自蒸馏驱动的解耦损失提升解耦效果，并提出基于滤波器组的频谱处理方法。又创新性设计空间-频谱联合特征金字塔，实现特征的有效耦合，并引入在解耦中获得的DS信息减少领域偏差。

Result: 在多个RGB-IR域适应目标检测数据集上进行大量实验，所提方法可大幅提升基线表现，并优于现有主流UDAOD方法；同时在FLIR-ADAS新实验协议上取得领先效果。

Conclusion: 通过频谱精细解耦和空间-频谱耦合策略，显著提升了RGB到IR的领域适应检测性能，为多子域场景下的检测自适应提供了新思路。

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [41] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: 本文提出了首个面向掩码建模（masked modeling）场景的数据集所有权验证方法DOV4MM，有效解决了此领域长期未解决的问题。通过实验证明，该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 深度学习离不开高质量开放数据集，但这些数据集存在被滥用的风险，保障数据集所有权变得非常重要。现有验证方法仅适用于监督式或对比预训练模型，不能直接用于掩码模型。随着掩码模型流行，该领域亟需新的所有权验证技术。

Method: 提出了一种新的掩码建模数据集所有权验证方法DOV4MM。该方法基于实证发现：若模型在目标数据集上预训练，则在嵌入空间中重构掩码信息的难度有显著不同。进而通过对比怀疑模型在重构时的表现，实现所有权验证。

Result: 在ImageNet-1K上的十个掩码图像模型、WikiText-103上的四个掩码语言模型上验证了方法的有效性。实验结果显示DOV4MM能够以远低于0.05的p值拒绝原假设，各项指标均超过已有方法。

Conclusion: DOV4MM首次解决了掩码建模模型的数据集所有权验证问题，为数据集所有者维权提供了新工具，并在多个数据集及模型上验证了其实用性和优越性。

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [42] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: 提出了一种新的多变量自回归空气污染物预测模型（MVAR），能够更准确并长时间预测多种污染物浓度，同时建立了大规模标准数据集，并在实验中优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有空气污染物预测多集中于单一污染物，忽略了污染物间的相互作用和空间差异，难以满足实际污染预警和决策需要。

Method: 提出MVAR模型，减少对长时序输入的依赖，提高数据利用率；设计了多变量自回归训练范式，实现120小时连续预测；开发了气象耦合空间Transformer模块，将气象预测灵活融合并学习污染物间及空间响应的关系；构建了包含6种主要污染物、75个城市、2018-2023年数据的大型标准数据集。

Result: 在实验中，MVAR模型在大规模数据集上实现了对多种污染物的120小时预测，在准确性等方面均超越了现有先进方法，验证了模型结构与训练方式的有效性。

Conclusion: MVAR为多变量空气污染物长时序预测提供了一种高效且实用的解决方案，将促进空气质量预警系统和相关政策制定。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [43] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文提出了3D-MoRe，一个利用基础模型生成大规模3D-语言数据集的新范式，有助于提升室内场景的问答与描述任务效果。


<details>
  <summary>Details</summary>
Motivation: 当前室内场景相关任务（如问答与细致描述）亟需多样化、可扩展的高质量3D-语言结合数据集，而现有数据集有限，难以支持更高级的模型训练和应用。

Method: 提出3D-MoRe框架，结合多模态嵌入、跨模态交互和语言模型解码器，能高效处理自然语言指令和3D场景数据。基于ScanNet、ScanQA、ScanRefer等数据集，通过数据增强与语义过滤，自动生成高质量问答对和物体描述数据。

Result: 在ScanQA和ScanRefer两个主流任务中，3D-MoRe分别带来了2.15%和1.84%的CIDEr分数提升，显著优于当前最优的基线方法。

Conclusion: 3D-MoRe有效提升3D场景理解和生成任务的性能，生成的新数据集和代码将开源，推动社区发展。

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [44] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 本文解决了基于多视角数据进行新类别发现（NCD）的问题，提出了一个新的多视角NCD框架IICMVNCD，在基于已知类别的知识指导下对新类别进行聚类。实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有NCD方法局限于单一视角数据（如仅限图片），忽视了现实中常见的多视角数据（如疾病诊断中的多组学数据），且依赖伪标签导致聚类不稳定。针对现有方法的不足，本文提出针对多视角NCD的新方法。

Method: 提出IICMVNCD框架，在单一视角内部通过矩阵分解分离特征，提取已知与新类别的分布一致性和样本间关系；在不同视角之间通过已知类别的视图关系融合预测标签，并自适应地调整视图权重，将这些知识迁移用于新类别聚类。

Result: 实验结果证明，在多视角NCD任务中，IICMVNCD方法在聚类新类别时表现优异，优于以往单视角和简单伪标签方法。

Conclusion: IICMVNCD是首个针对多视角新类别发现的框架，解决了现有方法无法有效处理复杂多模态数据且聚类结果不稳定的问题，为实际多视角分类和聚类任务提供了创新思路和方法。

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [45] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 本文介绍了 MoViAD，这是一个面向视觉异常检测（VAD）领域的高模块化、易用库。MoViAD 支持多种训练和应用场景，集成主流模型、数据集及评估工具，便于高效研究和实际部署。


<details>
  <summary>Details</summary>
Motivation: 视觉异常检测领域因异常数据稀缺和无监督训练需求而面临挑战，缺乏统一高效的工具库限制了该领域研究和应用的推进。

Method: 作者提出了 MoViAD 库，内置多种VAD模型、训练器、数据集及工具，并针对持续学习、半监督、少样本、嘈杂等多样化场景进行了支持。还针对边缘和物联网部署环境，优化了模型结构、量化和压缩工具，兼容自定义扩展。

Result: MoViAD 实现了多样的主流骨干网络、完善的像素级与图像级评估标准、性能分析工具。它能够快速部署和充分适配不同需求，已为工程师和学者提供了灵活高效的开发环境。

Conclusion: MoViAD 提供了高效、可扩展的视觉异常检测端到端开发平台，既能加速科研创新，也促进真实世界部署应用，填补了该领域的工具生态空白。

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [46] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉-语言模型（VLMs）和指令微调的新型人脸防伪系统InstructFLIP，通过内容与风格分离型指令增强了跨域泛化能力，同时减少了训练冗余，在多个领域实验中超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸防伪系统在应对不同域的攻击时，往往缺乏对攻击类型的语义理解且跨域泛化时存在训练资源浪费，因此需要一种能提升泛化能力且理解多样攻击的解决方案。

Method: 作者引入了视觉-语言模型（VLMs）来提升对输入图像语义的理解。为提升模型在多个域上的泛化能力，采用了meta-domain的策略，训练一个可跨域泛化的统一模型。核心方法InstructFLIP将训练指令解耦为内容型和风格型：内容型关注于攻击语义本质，风格型则适应环境与相机差异，并通过文字引导提升泛化，且仅需单域训练。

Result: 大量实验表明，InstructFLIP不仅准确率超过了当前最优模型（SOTA），且极大减少了跨域训练冗余。

Conclusion: InstructFLIP通过指令分离和VLMs引入，实现了有效的人脸防伪跨域泛化和训练高效性，展现了在实际多域人脸防伪任务中的应用潜力。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [47] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: 本文提出MS-DETR框架，通过统一学习捕捉视频中的运动和语义特征，提升视频片段检索与高光检测性能，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的视频时刻检索（MR）和高光检测（HD）任务虽然在DETR（Detection Transformer）基础的联合框架取得进展，但尚未充分利用视频内容中时序运动与空间语义的复杂关系。

Method: 提出MS-DETR框架。其编码器首先在运动和语义维度下显式建模与文本查询相关的单模态关系，解码器进一步结合任务需求，融合时序运动与空间语义以实现准确定位和高光界定。此外，通过生成策略丰富训练数据，并引入对比去噪学习，增强模型鲁棒性。

Result: 在四个公认的MR/HD基准数据集上的大量实验证明，MS-DETR性能明显优于当前最优模型。

Conclusion: MS-DETR框架能够有效联合运动和语义信息，提升文本引导的视频时刻检索与高光检测精度，在该领域树立了新的性能标杆。

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [48] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的无人机视角下小型敏捷多目标（如鸟群）跟踪方案，集成了改进的检测与关联方法，有效提升了小目标追踪性能，并在国际竞赛中取得冠军。


<details>
  <summary>Details</summary>
Motivation: 小型敏捷目标（如鸟群）由于外观特征极度稀缺、动态复杂且易遮挡，传统目标跟踪方法难以应对。提升无人机平台下的小型多目标检测与跟踪性能，对动物行为研究等领域具有重要意义。

Method: 方法采用“检测-跟踪”范式，在检测层引入名为SliceTrain的训练增强框架，通过全覆盖切片和切片级增强提升小目标学习效率；在关联层设计了无外观信息的跟踪模块，并结合运动方向维护（EMA机制）与自适应相似度指标（融合框扩展和距离惩罚）对接OC-SORT框架，提升对复杂运动及身份保持的鲁棒性。

Result: 该方法在SMOT4SB公开测试集上获得SO-HOTA分数为55.205，达到当前最佳性能，证明了框架在复杂实际问题中的有效性和先进性。

Conclusion: 提出的方法可以有效解决无人机下小型敏捷多目标跟踪中的小目标检测困难、复杂运动关联等核心难题，在相关应用领域具有较高的实用与推广价值。

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [49] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 该论文提出了一个多中心的多发性硬化皮层病灶（CL）MRI检测与分割基准，并验证了自适应nnU-Net模型的有效性。数据和模型将公开。


<details>
  <summary>Details</summary>
Motivation: 皮层病灶CL作为MS的有力生物标志物，对诊断和预后很重要，但其影像特征隐蔽、人工标注困难且自动方法缺乏，导致临床应用受限。为推动标准化和临床落地，需要公开基准和高效方法。

Method: 收集了来自四家机构，共656份3T/7T MRI数据，用MP2RAGE与MPRAGE序列及专家共识注释。基于nnU-Net深度学习分割框架，并针对CL检测做了方法调整。同时做了域外泛化测试和模型内部特征分析。

Result: 在域内/域外测试中，CL检测F1分数分别为0.64和0.5。分析了数据变异、病灶模糊和协议差异对模型性能的影响，并探讨了AI模型决策机理和误差来源。

Conclusion: 适配后的nnU-Net在多中心数据上展现出较好泛化能力。研究为解决CL检测标准化和临床障碍提出了建议，并通过数据和代码开源促进了复现性和未来研究。

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>


### [50] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出了一种能够在稀疏视角下进行车辆高质量三维重建的新方法，并通过改进现有技术实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，车辆三维重建常常面临输入视角稀疏的问题，现有方法对密集视角依赖性大，限制了其在现实世界中的应用。本文旨在提升在稀疏视角条件下车辆三维重建的准确性与实用性。

Method: 方法上，本文基于Gaussian Splatting进行改进：一方面结合了仅对高置信度像素施加的选择性光度损失，另一方面采用DUSt3R结构替代传统的Structure-from-Motion流程以提升相机位姿估计的鲁棒性。同时，提出了包含合成及真实公共交通工具的新数据集用于系统评估。

Result: 实验表明，该方法在多个基准数据集上均取得了先进性能，在视角受限条件下依然能实现高质量的三维重建。

Conclusion: 该方法有效提升了稀疏视角三维重建的实用性和准确性，为车辆相关多个应用场景提供了有力技术支撑。

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [51] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: 本论文提出利用3D模拟与深度学习生成和预测城区中的阴影分布，从而提升城市高温环境下的路线规划。


<details>
  <summary>Details</summary>
Motivation: 全球变暖加剧，高温热浪对公共健康构成威胁。现有路线规划系统未充分考虑阴影信息，主要因为从卫星图像估算阴影困难、训练数据不足。

Method: 研究者首先构建了涵盖不同城市布局、建筑密度和经纬度的大型数据集，通过Blender和建筑轮廓3D模拟，生成不同季节和时段的阴影，并与卫星图像对齐。然后，提出基于扩散机制的DeepShade模型，结合RGB与Canny边缘特征，并引入对比学习以捕获阴影的时序变化，还可根据描述性条件生成不同时间的阴影图像。

Result: 实验验证在亚利桑那州坦佩市的城市路线规划中，利用生成的阴影分布预测，能够有效计算实际路径的阴影覆盖率，提升了路线规避热浪的能力。

Conclusion: 论文工作为极端高温天气下的城市健康和路线规划提供了数据与模型支持，具有实际应用及城市规划指导价值。

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [52] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: 作者提出了Med-OoD方法，通过引入“分布外（OoD）数据监督”来提升医学影像分割的准确性，无需额外数据或模型修改，显著减少了前景与背景的误分类，并取得了优异实验结果。


<details>
  <summary>Details</summary>
Motivation: 医学分割网络在医学影像小数据集上容易发生前景与背景的错误分类。已有方法难以解决该问题，且通常需要引入额外数据、复杂正则项或额外标注。作者受到OoD数据在视觉任务中的积极作用启发，试图让OoD数据帮助提升医学分割性能。

Method: 提出Med-OoD框架，在完全监督的医学分割任务中，引入OoD数据作为监督信息，但不需要外部数据源、特征正则化或额外标注，且网络架构无需修改。该框架可无缝集成到现有医学分割网络中。

Result: 在Lizard数据集和多种分割网络上，Med-OoD显著降低了医学图像误分类，分割性能有明显提升。此外，作者尝试只用OoD数据（无前景类别标签）训练分割网络，仍能取得76.1%的mIoU指标。

Conclusion: Med-OoD验证了OoD数据监督在医学影像分割中的有效性，推动了结合OoD数据的学习范式。该方法为重新思考OoD数据在医学任务中的角色提供了新视角。

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [53] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的对抗性人脸生成方法，无需反复查询或依赖迁移性，在极少次数（一次100张图片）的非自适应查询下，即可有效地欺骗商业人脸识别系统（如AWS CompareFaces API），成功率高达93%。该方法还能控制生成的人脸高层次属性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统广泛用于身份验证，然而其对抗性攻击防护能力薄弱，导致严重的安全与隐私风险。传统攻击方法依赖迁移性或需多次查询，难以攻破商业闭源系统。因此，亟需一种低查询成本、高效且能控制生成属性的对抗攻击方法。

Method: 作者提出利用人脸识别系统特征空间中的结构特性，发现具有相同属性（如性别、种族）的人可形成属性子球面。基于此，生成在特定子球面上的对抗性人脸，只需一次性查询100张图片即可，彻底杜绝多次自适应查询和对开源替代模型的依赖。

Result: 在无需多次自适应查询的前提下，仅用一次非自适应查询（100张脸），对AWS CompareFaces API命中率超过93%。此外，生成的人脸还可主动带有攻击者选择的高层属性。

Conclusion: 该方法极大提升了对商业人脸识别系统攻击的隐蔽性与有效性，在低查询条件下实现高成功率和属性可控，将促进对FRS安全性的再评估，呼吁系统提供者加强防御措施。

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [54] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为LidarPainter的新型动态驾驶场景重建方法，利用单步扩散模型在稀疏LiDAR数据和受损渲染图下实时恢复高质量一致性视图，大幅提升了重建速度、质量与资源利用效率，并支持基于文本提示的风格化生成。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生和自动驾驶仿真中，动态驾驶场景重建在视角偏离输入轨迹后质量急剧下降，导致背景和车辆模型损坏。现有方法在一致性、变形和耗时等方面存在局限，亟需一种兼顾高质量、实用性和效率的解决方案。

Method: 提出LidarPainter模型——一种单步扩散模型。它结合稀疏LiDAR条件和受损渲染图，实时恢复一致的驾驶视图。还实现了基于文本提示（如“雾天”“夜晚”）的多样风格化生成。

Result: 实验显示，LidarPainter在速度、画质和资源利用上均优于最新方法，速度提升7倍，显存需求仅为StreetCrafter的1/5。

Conclusion: LidarPainter显著提升了动态驾驶场景在新轨迹下的重建质量与效率，并通过文本提示灵活扩展资产库，对数字孪生和自动驾驶仿真等场景具有重要应用意义。

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [55] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: 本文提出了OVIGo-3DHSG方法，通过3D分层场景图和开词汇视觉模型，实现对室内环境中物体的开放词汇定位与空间理解。


<details>
  <summary>Details</summary>
Motivation: 当前针对室内复杂环境中的目标定位与空间推理任务，难以高效处理跨层次、跨区域的空间关系与开放词汇查询。

Method: 构建基于RGB-D序列的分层场景图（涵盖楼层、房间、位置、物体层次），结合开词汇基础模型和传感器数据处理；将场景图与大语言模型集成，实现复杂空间查询和多步推理。

Result: 在Habitat Matterport 3D多楼层场景下，验证了分层表示在语义与几何准确性上的优越性；与现有方法相比，场景理解与目标定位更高效和鲁棒。

Conclusion: OVIGo-3DHSG为需要空间推理和室内环境理解的应用提供了有效手段，在开放词汇室内目标定位任务中展现出强大的潜力。

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [56] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: 提出了一种新颖的基于块的对称剪枝与融合方法（BSPF-ViT），能够在保证精度的同时大幅降低Vision Transformer计算量。


<details>
  <summary>Details</summary>
Motivation: 现有ViT剪枝方法独立剪除Q/K tokens，未考虑token间相互作用，导致性能下降。作者旨在在不牺牲准确率的前提下，提高ViT推断效率。

Method: BSPF-ViT通过联合优化Q/K tokens的剪枝，利用token交互信息决定保留哪些token，并通过相似性融合压缩保留token。此外，利用Q/K共享权重，实现对称注意力矩阵，仅对其上三角部分剪枝以进一步提速。

Result: BSPF-ViT在所有剪枝率下均优于现有ViT方法。在ImageNet分类任务上，DeiT-T提升1.3%，DeiT-S提升2.0%，同时计算量减少50%。在各类ViT上均实现40%推理加速并提高精度。

Conclusion: BSPF-ViT能显著降低ViT的计算成本，同时提升或保持准确率，对实际应用更为友好。

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [57] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 本文提出了一种基于双边网格的像素自适应多层感知机（BPAM）框架，结合双边网格的空间建模和多层感知机（MLP）的非线性映射能力，实现了高效且更强表现力的图像增强。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的双边网格处理方法受限于线性仿射变换，难以建模复杂的颜色关系；MLP虽然具备非线性表达能力，但传统参数全局共享，难以处理局部变化。本文旨在解决这两个难题。

Method: 提出BPAM框架：生成包含MLP参数的双边网格，每个像素根据空间坐标和强度值动态获取独特的MLP，实现针对每个像素的非线性颜色映射。同时，设计了新的网格分解策略，将不同类别的MLP参数存储于独立子网格，并采用多通道引导图按类别提取参数，提升切片和参数生成的精度。

Result: 在公开数据集上的大量实验表明，该方法在性能上优于当前最先进的方法，且可保持实时处理速度。

Conclusion: BPAM框架能够有效结合空间和非线性特性，实现更强表现力的图像增强任务，兼顾高性能和实时性，具有较强的实用价值。

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [58] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: 提出了AD-GS，一种无需人工标注即可高质量建模和渲染城市动态驾驶场景的新自监督框架，通过结合新颖的运动模型、自动分割和动态高斯建模，实现了与需标注的方法媲美甚至超越的效果。


<details>
  <summary>Details</summary>
Motivation: 当前高质量城市场景建模依赖昂贵的人工标注，而自监督方法又无法准确建模动态对象，导致渲染结果存在明显缺陷。因此需要一种无需手工标注、又能高质量建模动态驾驶场景的新方法。

Method: AD-GS核心为一种新颖的可学习运动模型，将局部敏感的B样条曲线和全局三角函数结合，实现对动态目标的精准建模。同时，自动将场景划分为目标和背景，引入动态高斯表征和双向可见性掩码，并通过可见性推理和刚性正则增强模型健壮性。

Result: 无需人工标注的AD-GS在大量实验中，显著超越现有的同类无标注方法，并可媲美甚至接近依赖标注的方法。

Conclusion: AD-GS实现了高质量、无标注的动态驾驶场景自由视点渲染，为自动驾驶仿真带来更低成本和更实用的场景建模新方案。

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [59] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 本文提出一种基于正则流（normalizing flows）的神经先验学习方法，用于建模人体姿态分布，实现了灵活、表达力强的概率建模，在人体运动捕捉和重建中表现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有对人体姿态建模的先验方式通常表达能力有限或依赖启发式方法，难以充分建模高维复杂分布，因此需要一种更灵活且泛化能力强的方法。

Method: 采用RealNVP正则流，对以6D旋转格式表示的姿态建立概率密度，训练过程中通过反向Gram-Schmidt过程处理旋转流形分布，确保学习稳定且兼容现有旋转框架。

Result: 通过定性和定量实验评估了所提方法的有效性，并进行了消融分析，验证建模和训练框架的优势。

Conclusion: 本文方法为人体姿态先验的集成提供了概率基础，可为人体动作捕捉和三维重建等任务提供更高质量的先验模型。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [60] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的训练框架TGDA，能够无需预训练模型从零训练高性能的细粒度图像识别(FGIR)系统，并设计了高效的新架构，实验显示其性能优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统FGIR方法过度依赖大规模数据集上预训练的骨干网络，这在资源受限环境下不适用，同时限制了为FGIR任务专设高效架构的可能性。作者希望打破对预训练的依赖，提升灵活性和效率。

Method: 作者提出TGDA训练框架，结合了数据相关的增强和微弱监督，通过知识蒸馏引入细粒度感知的教师模型。此外，设计了适应低分辨率的LRNets和为高效推理优化的ViTFS视觉Transformer架构。

Result: 在多个细粒度识别基准上，TGDA框架下的LRNets和ViTFS与同类预训练方法相比，能在精度、参数量、计算量和数据需求上全面超越。例如LRNets在低分辨率任务上准确率提升最大23%，参数量减少20.6倍，训练数据需求也更低。ViTFS-T参数量为ViT B-16的1/15，并用极少数据也能达到同等性能。

Conclusion: TGDA为无需预训练也能高效实现细粒度识别提供了新路径，支持为特定场景和硬件定制网络架构，有望推动更高效的视觉系统发展。

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [61] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 本文提出了一个双重集成框架，通过结合预训练深度学习模型和集成优化机器学习模型，显著提升了脑部MRI肿瘤的识别准确率。


<details>
  <summary>Details</summary>
Motivation: MRI虽可清晰显示肿瘤，但因人工诊断容易因疲劳、经验不足等导致漏诊或误诊，特别是对小肿瘤或与健康脑组织重叠的情况。为提升诊断准确性，急需自动化、精准的图像分析方法。

Method: 方法包括：1）预处理与扩增；2）利用多种预训练卷积神经网络与视觉Transformer进行迁移学习，抽取脑MRI深度特征；3）对多种机器学习分类器进行超参数调优与集成，最终将特征集成与分类器集成结合。

Result: 在三个人工可公开获取的Kaggle脑肿瘤MRI数据集上测试，深度特征集成加分类器集成方法超越当前主流方法，超参数优化为最终性能带来了显著提升。

Conclusion: 所提出的双重集成学习框架在脑肿瘤分类任务上取得了优异效果，验证了各环节对提升诊断精度的重要性，可为实际临床提供更准确的辅助诊断工具。

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [62] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波变换和特征空间解耦的低照度立体图像增强方法，通过分别调整低频与高频特征，有效提升了增强质量，克服了信息混叠和黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 现有低照度图像增强方法通常将所有退化因素编码进一个潜在空间，导致特征高度纠缠和模型黑盒化，容易陷入捷径学习，影响最终增强效果。作者基于对小波变换信息解耦能力的观察，提出针对性改进。

Method: 方法上，利用小波变换对特征空间做解耦：将低频分支用于光照调整，高频分支用于纹理增强与细节恢复。进一步提出了高频引导的跨视图交互模块（HF-CIM）和基于交叉注意力机制的细节纹理增强模块（DTEM），分别提升立体信息利用和高频特征恢复。训练使用了均匀和非均匀光照的图像数据。

Result: 实验证明，在真实与合成数据集上，该方法在光照调整和高频信息恢复方面效果显著优于其他算法。

Conclusion: 本文基于小波特征空间解耦及创新模块的低照度图像增强方法，有效分离与利用不同频域信息，提升了图像质量，具有明显优势，相关代码和数据已开源。

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [63] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: 本文提出了三种针对印度文物保护与修复的创新计算机视觉方法，包括分形卷积分割、SSTF自敏感填充和MosaicSlice数据增强，并探讨了高分辨率重建技术，以提升相关图片质量。这些方法在保证文化真实性的同时提升了自动化与审美效果，对文物数字化保护有积极促进作用。


<details>
  <summary>Details</summary>
Motivation: 随着计算机技术的发展，文化遗产保护领域引入了机器学习、深度学习和计算机视觉等现代技术。印度具有独特的建筑艺术和丰富的文化遗产，因此亟需结合创新技术对其进行高效、低成本且真实的数字化保护和修复。

Method: 1. 分形卷积方法用于对印度文物的细致建筑图案进行分割和特征提取；2. 针对Bankura陶砖寺庙提出自敏感瓷砖填充（SSTF）方法，同时提出MosaicSlice新型数据增强策略，用以提升模型泛化能力和区域填充质量；3. 采用超分辨率技术提升文物图像的清晰度，无损提升细节表现。

Result: 所提出的三项方法有效提升了文物图像的分割、区域填充和增强质量，实现了高精度的细节还原和区域无缝填充；数据增强策略与自动化工具结合，使整体成本降低，效率提升。

Conclusion: 新颖的计算机视觉和数据增强方法在保护和修复印度文物过程中取得良好成效，并为保持传统与创新的平衡提供了高效、优质的解决方案。该研究推动了文化遗产数字化保护的发展。

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [64] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的扩散采样方法RODS，通过优化视角检测和纠正采样过程中的高风险步骤，从而大幅减少扩散模型中的幻觉现象，提高采样质量与鲁棒性，并无需重新训练，推理开销小。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模领域表现突出，但其采样过程易出现幻觉（即生成不真实或错误内容），主要源于分数近似不准确。为了解决该问题，作者希望提升采样可靠性，减少幻觉的发生。

Method: 作者从优化的视角重新解读扩散采样过程，提出RODS（Robust Optimization-inspired Diffusion Sampler），利用损失景观的几何信息检测采样中的高风险步骤，并自适应地调整扰动，使采样轨迹更加平滑，实现对高风险步骤的纠正。

Result: 在AFHQv2、FFHQ和11k-hands数据集上的实验表明，RODS能够检测超过70%的幻觉样本，纠正超过25%的幻觉样本，同时提升采样保真度与鲁棒性，且不会引入新的伪影。

Conclusion: RODS作为一种高效、无需重新训练的方法，有效改善了扩散模型的采样质量和稳健性，对解决扩散生成模型中的幻觉问题具有现实意义。

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [65] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: 本文通过改进视觉大语言模型（VLM），实现对人脸篡改图像的更准确检测及解释。提出新的数据集和训练策略，并在实验中取得优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型在深度伪造检测方面虽取得较高准确率，但在利用面部质量相关属性和训练策略方面存在不足。为提升检测效果与模型解释能力，需要更丰富的数据集和新型检测框架。

Method: 首先扩展了VQA数据集，构建DD-VQA+，包含更丰富属性和多样样本；提出MGFFD-VLM框架，引入属性驱动的混合LoRA策略、多粒度提示学习和伪造感知训练策略。将分类和分割结果转化为提示词，提升模型解释性，并辅以多种伪造辅助损失提升检测性能。

Result: 实验表明，该方法在文本判别伪造及分析任务上均优于现有方法，准确率更高，解释性更强。

Conclusion: 通过丰富的数据集、创新的训练框架和多重提示策略，显著提升了视觉大语言模型在深度伪造检测与解释任务中的表现，为相关应用提供了新思路。

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [66] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 该论文提出利用生成式文本到图像扩散模型，通过交叉注意力图，实现医学图像中自然语言短语的高效定位，结果优于现有判别式方法，并引入了一种提升定位精度的新后处理技术。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像短语定位多依赖自监督判别式方法，但尚未充分挖掘生成式扩散模型在该任务中的潜力，且判别式方法在零样本情境下表现有限。作者希望探讨生成式模型与领域特定语言模型结合后的优势，并解决定位精度不足的问题。

Method: 作者使用生成式text-to-image扩散模型，将交叉注意力图用于短语定位。与以往方法不同，模型与冻结的、医学领域特定的语言模型（如CXR-BERT）结合进行微调。同时，提出Bimodal Bias Merging（BBM）后处理技术，用于融合文本和图像偏置信息，提升准确性。

Result: 实验结果显示，基于冻结领域语言模型微调的扩散模型在零样本短语定位上，mIoU分数提升显著，达到现有判别式方法的两倍。BBM进一步提升了定位精度。整体上，生成式方法表现出更强的短语定位能力。

Conclusion: 生成式扩散模型与领域特定语言模型结合，在医学图像短语定位任务中优于主流判别式方法，并且BBM技术能进一步提高定位准确度。这表明生成式方法在该领域具有很大潜力，可提升临床应用的鲁棒性和可解释性。

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [67] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本文为体操运动中的静态技能动作提出了一个新的视频时序分割任务，并发布了相关数据集及基线方法，为后续智能化评测工具的开发提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着徒手体操（Calisthenics）运动的流行，尤其是对高难度静态动作技能的评判与训练的需求增长，然而当前尚无专门针对体操技能时序分割的视频理解技术与相关工具。

Method: 作者提出并发布了一个包含运动员表演静态技能的新视频数据集，每段视频均按动作技能进行了详细的时序标注，并以基线方法对技能时序分割进行了初步实验和验证。

Result: 基线实验结果表明该任务具有可行性，同时也显示算法在准确性和精度上仍有较大提升空间。

Conclusion: 该研究填补了体操技能时序分割领域的空白，为开发视频自动识别与测量系统奠定了基础，对运动判罚与训练辅助具有重要意义。

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [68] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文系统比较了Keras、PyTorch和JAX三大深度学习框架在医学图片分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然CNNs和深度学习框架推动了医学图像分类进步，但不同框架在实际医学影像任务下的优劣性并不清晰，缺乏系统性对比。

Method: 作者在PathMNIST数据集上，分别用Keras、PyTorch和JAX实现CNN模型，从训练效率、分类准确率、推理速度多个指标进行横向评测。

Result: 三大框架在速度和准确率之间各有权衡：部分框架推理更快，部分则训练或分类精度更高。

Conclusion: 研究结果为医学图像分析领域提供了框架选择的参考，帮助科研与实际应用中权衡计算性能和模型效果。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [69] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的胸部X光影像分析方法，用于预测极低出生体重早产儿是否会发展为中重度支气管肺发育不良（BPD），并证实了领域特定（即成人胸片）预训练模型的优越性。


<details>
  <summary>Details</summary>
Motivation: BPD是极低出生体重儿常见的慢性肺病，相关的早期干预手段副作用大，因此亟需早期无创、准确的预测工具以指导临床决策、减少低风险婴儿毒性暴露。

Method: 收集163例极低出生体重早产儿出生24小时内的胸片，采用在成人胸片上预训练的ResNet-50模型，结合逐步冻结层训练、分层学习率调整、CutMix数据增强及线性探查，比较了不同预训练及评分方法的预测表现。

Result: 模型在预测中/重度BPD上取得AUROC 0.78、平衡准确率0.69和F1分数0.67，显著优于依赖传统IRDS分级（AUROC仅0.57）。领域特定预训练优于ImageNet初始化（p=0.031）。

Conclusion: 基于领域预训练与优化策略的深度学习方法能够从常规第1天胸片中准确预测BPD，且具备较高可扩展性，适合实际医院部署及未来多中心联邦学习。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [70] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FADE的创新方法，可在不影响生成图像质量的前提下，从扩散模型（如Stable Diffusion）中可靠地移除特定概念（如隐私信息或有害刻板印象），提升模型的安全性与公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成任务中表现优异，但存在记忆敏感信息或加剧偏见的风险。因此有必要开发有效方法以安全地从生成模型中删除这些不希望出现的特定概念。

Method: FADE方法将“轨迹感知微调策略”与“对抗性目标”相结合，实现对指定概念的清除，并保持模型整体生成能力。理论上，FADE通过最小化模型输出与待移除概念间的互信息来保证隐私与公平性。

Result: 在Stable Diffusion和FLUX等主流扩散模型及公开基准测试集上，FADE方法在概念移除效果和生成图像质量上均超越现有方法（如ESD、UCE、MACE和ANT），概念移除与保真度的调和平均提升5-10%。消融实验验证对抗性和轨迹保留目标都是FADE优异性能的重要组成部分。

Conclusion: FADE方法为安全与公平的生成建模设定了新标准，可在无需从零重新训练模型的情况下高效“遗忘”指定概念，有助于推进生成模型的实际应用。

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [71] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本文提出了一种用于健身技能分类的新方法，通过结合深度估计和运动员图像分割，绕过了传统的人体姿态估计算法，实现了更快且精度更高的技能分类。


<details>
  <summary>Details</summary>
Motivation: 传统的健身技能识别依赖于姿态估计，其计算代价高、推理速度慢、难以在实时或移动设备上应用。本文旨在解决这些实际应用中的瓶颈。

Method: 作者提出了基于深度估计（Depth Anything V2）和运动员定位（YOLOv10）的新管线，通过分割图像中的运动员而非姿态点获得输入特征，然后直接进行技能分类，避免了高昂的姿态估计过程。

Result: 新方法比基于骨架的方法推理速度快38.3倍，并且利用深度图得到的分类精度（0.837）也优于基于姿态的方法（0.815）。

Conclusion: 该工作展示了直接基于分割和深度估计的方法在效率和准确性上的优势，并由于管线模块化，具有良好的扩展性和现实应用前景。

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [72] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像表征方法Discrete Latent Code（DLC），通过改进扩散模型的条件输入，提升生成样本的质量和多样性，并在ImageNet上取得了无条件图像生成的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 作者指出目前扩散模型在复杂分布建模上的成功主要归功于其输入条件机制，但现有的表征不易组合和泛化，影响新颖样本的生成能力，因此需要设计更理想的条件表征。

Method: 提出一种基于自监督目标训练的Simplicial Embeddings，得到离散的DLC序列作为图像的语义表征，这些DLC易于生成并可组合。将DLC作为条件用于图像扩散模型，并探索将其与大规模预训练语言模型结合，用于文本到图像生成。

Result: 使用DLC条件训练的扩散模型在ImageNet无条件生成任务上提升了生成样本的保真度，达到了新的SOTA。同时，通过DLC组合，模型能够生成语义合理、超出训练分布的新颖图像；通过文本扩散语言模型生成DLC，实现了文本到图像的新颖生成。

Conclusion: DLC是一种高效、可组合的图像语义表征，极大提升了扩散模型在图像生成任务中的表现和泛化能力，并为多模态生成提供了新思路。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [73] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为KeyDiff3D的无监督单目3D关键点估计算法，仅需单张图片即可准确预测3D关键点，无需人工标注或多视角校准图片。通过利用预训练多视角扩散模型的几何先验，实现了高效准确的3D信息提取和关键点估计。实验显示该方法在多个数据集和场景下具有出色的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前单目3D关键点估计依赖人工标注或多视角校准数据，获取成本高，限制了大规模实际应用。因此，本文希望提出仅需单视角图片便能高效准确完成无监督3D关键点估计的新方法。

Method: 作者利用已经预训练好的多视角扩散模型，从单张图片生成多视角图片作为监督信号，为模型提供丰富的3D几何信息。同时，借助扩散模型作为2D多视角特征提取器，将其中间特征构建成3D特征体，将隐式3D先验转化为显式的3D特征。还提出了一条利用扩散模型实现3D对象操作的流程。

Result: 在Human3.6M、Stanford Dogs以及多个野外和跨域数据集上的实验表明，该方法在关键点估计准确性、泛化能力等方面有明显优势，并支持对单张图片生成的3D对象进行操控。

Conclusion: 本文提出的KeyDiff3D方法可在无需人工标注或多视角数据的前提下，实现高效准确的单目3D关键点估计，具备良好的泛化能力和实际应用潜力。

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [74] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet Oğuz Saltık,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: 本文提出通过信道知识蒸馏（CWD）和遮罩生成蒸馏（MGD）提升轻量级深度学习模型在农业杂草检测中的表现，实现精确且高效的实时智能喷药应用。


<details>
  <summary>Details</summary>
Motivation: 精确农业需要准确识别杂草，便于定向施药，减少环境影响。但受限于资源有限的平台，将复杂模型部署在实际农田等场景并不现实，尤其是在需区分视觉相似杂草种类时，模型压缩与知识迁移尤为关键。

Method: 采用YOLO11x为教师模型，YOLO11n为参考与学生模型，应用CWD与MGD两种知识蒸馏方法，将大型模型的表征能力迁移至轻量级学生模型。模型效果在包含甜菜及四类杂草的真实数据集下（Cirsium、Convolvulus、Fallopia、Echinochloa）测试，并通过在Jetson Orin Nano与Raspberry Pi 5端部署，评估其实时性和稳定性。

Result: 经实验，CWD和MGD均显著提升YOLO11n模型AP50指标，CWD平均提升2.5%，MGD提升1.9%，且模型复杂度未增加。端侧部署结果表明方法在随机初始化下表现稳定，具备实时应用潜力。

Conclusion: CWD与MGD能高效将复杂模型知识转移，提升轻量化目标检测网络在农业杂草识别中的准确度与实用性，为精确农业与植物表型领域提供了有效的低算力解决方案。

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [75] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督视觉表征学习方法——Cluster Contrast（CueCo），结合了对比学习和聚类的优点，在多个数据集上取得了较好表现。


<details>
  <summary>Details</summary>
Motivation: 无监督视觉表征学习需要提升特征的判别能力，但传统方法往往只注重特征分散或聚集，难以兼顾类间分离与类内紧致。作者希望开发一种能同时实现这两个目标的方法。

Method: CueCo方法采用两套神经网络（query和key），key网络由query的输出缓慢更新。通过对比损失扩展类间距离，通过聚类目标使同簇特征靠近，实现特征空间的有效组织。

Result: 在CIFAR-10、CIFAR-100和ImageNet-100数据集上，CueCo通过线性分类评估，分别取得了91.4%、68.56%和78.65%的Top-1分类准确率，均采用ResNet-18骨干网络。

Conclusion: CueCo有效融合了对比学习和聚类策略，提升了无监督视觉特征学习的效果，为该领域提供了新的发展方向。

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [76] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种利用文本信息增强3D医学图像分割的半监督方法Text-SemiSeg，显著提升了在有限标注数据下的分割效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像的标注成本极高，而在标注数据有限时，提升分割性能尤为关键。文本信息可为视觉理解提供额外的语义上下文。然而，目前鲜有研究探索如何将文本信息应用于3D医学图像分割领域。

Method: 作者提出了Text-SemiSeg框架，包含三大模块：（1）文本增强多平面表示（TMR），通过视图与文本交互提升视觉特征的类别感知能力；（2）类别感知语义对齐（CSA），借助可学习变量实现文本与视觉特征的跨模态语义对齐；（3）动态认知增强（DCA），通过有标与无标数据的互动，缓解分布差异，提高鲁棒性。

Result: 在三个公开医学图像分割数据集上的实验表明，Text-SemiSeg在利用文本增强视觉特征后，性能优于现有方法。

Conclusion: Text-SemiSeg通过引入文本信息提升了3D医学图像半监督分割的性能，为有限数据情景下的医学影像分析提供了有效方案。

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [77] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: 本文提出了两个新的、专注于人物监控场景的目标检测数据集OD-VIRAT Large和OD-VIRAT Tiny，并对多种主流目标检测算法在这些实际复杂监控场景下的性能进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的人类监控数据集对提升计算机视觉目标检测算法在真实环境下的表现至关重要，当前公开数据集在真实感、多样性和挑战性方面存在不足，亟需更丰富、更复杂的监控数据来推动该领域发展。

Method: 作者采集了包含10种不同监控场景的大量视频帧，精细标注了目标框和类别，分别构建了OD-VIRAT Large（87万个实例，近60万张图片）和OD-VIRAT Tiny（28万实例，约2万张图片）两个数据集。并在这些数据集上基准评测了RETMDET、YOLOX、RetinaNet、DETR和Deformable-DETR等主流检测模型表现。

Result: 基准实验显示，在具有复杂背景、遮挡和小目标等挑战条件的真实监控数据上，不同主流目标检测模型表现存在差异，总体上揭示了各个模型在实际监控任务上的优劣及局限性，为后续算法优化提供依据。

Conclusion: OD-VIRAT数据集填补了现实复杂监控领域视觉检测数据的空白，公开的基准和实验结果有助于社区理解现有算法的适用性，并为开发更强大、高效的监控目标检测系统提供了重要基础。

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [78] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: 本文提出了一种用于组合式图像检索（CIR）的新方法QuRe，重点解决了现有方法中错误负样本干扰检索准确性的问题，并通过引入新的采样策略和数据集提升了模型的人类偏好匹配能力。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法只关注目标图像的检索，忽视了其他相关图片的匹配度，并容易在对比学习中产生错误负样本，最终导致检索结果与用户需求不符。解决这一问题有助于提升用户满意度。

Method: 提出Query-Relevant Retrieval through Hard Negative Sampling (QuRe)方法，通过奖励模型目标来降低错误负样本影响；同时提出新的hard negative采样策略，从相关度分数显著变化的区间挑选难负样本。另构建了考虑用户真实偏好的HP-FashionIQ数据集，用于更准确评估模型人机一致性。

Result: QuRe在FashionIQ和CIRR数据集上取得了新SOTA表现，在新提出的HP-FashionIQ数据集上，也表现出最佳的人类偏好对齐能力。

Conclusion: QuRe有效缓解了CIR中错误负样本的问题，检索结果更贴合用户偏好，为提升CIR实际应用中的用户体验提供了新的解决思路和方法。

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [79] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 本文提出了一种新型的边界框回归损失InterpIoU及其动态变体Dynamic InterpIoU，解决了现有IoU损失的多种弊端，在主流检测任务中取得了更优表现，尤其是在小目标检测方面。


<details>
  <summary>Details</summary>
Motivation: 现有的IoU-based损失函数为了解决非重叠框在优化时无梯度等问题，会加入手工设计的几何惩罚项，但这些惩罚项对目标形状、大小和分布高度敏感，造成在小目标及特殊情形下优化效果不佳，甚至引发边界框过度扩大等不良现象。

Method: 提出InterpIoU损失函数，用插值框与目标框之间的IoU来取代传统手工几何惩罚项，从而在非重叠情况下也能提供有效梯度并避免不合理的框扩大问题。此外，提出Dynamic InterpIoU，根据IoU动态调整插值系数，提高对不同目标分布场景的适应性。

Result: 在COCO、VisDrone和PASCAL VOC等数据集的实验表明，InterpIoU及其动态版本在多个检测框架下均优于当前主流IoU-based损失，提升效果明显，特别是在小目标检测任务中。

Conclusion: 改进后的IoU损失不仅摆脱了人工几何惩罚项的局限，更自洽地对齐了优化目标，优化性能得到显著提升，为未来目标检测任务提供了更高效、泛化能力更强的损失设计。

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [80] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: 本文提出了DVFL-Net，一种轻量级的视频识别网络，通过知识蒸馏将大模型的时空知识传递到小模型，实现高效且准确的视频识别，适用于边缘和实时应用。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的视频识别方法精度高，但计算成本大，难以实现高效部署，尤其是在资源受限设备上。因此，需探索既高效又精准的视频识别解决方案。

Method: 提出DVFL-Net结构，通过知识蒸馏框架，将大型预训练Transformer教师模型中的时空知识迁移到小型学生模型；结合时空特征调制和前向KL散度，有效融合教师模型的全局与局部上下文信息，同时大大降低运算量。

Result: 在UCF50、UCF101、HMDB51、SSV2 和 Kinetics-400等数据集上，DVFL-Net与当前主流方法相比，在内存消耗、GFLOPs及识别准确率上均表现出色。消融实验验证了前向KL散度对提升性能的效果。

Conclusion: DVFL-Net在保持高识别精度的同时，实现了低计算复杂度和低内存占用，是适合实时人类动作识别（HAR）且易于实际部署的高性价比视频识别方案。

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [81] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: 提出了一种结合交通信号灯状态的新型时空图卷积网络（TA-STGCN），用于提升自动驾驶中行人意图预测的准确性，实验证明其在PIE数据集上较基线模型提升4.75%。


<details>
  <summary>Details</summary>
Motivation: 现有行人意图预测模型往往忽视动态交通信号和场景上下文信息，导致在实际道路环境中的表现有限。因此，亟需更有效地结合这些关键环境信息，提高预测精度，保障自动驾驶安全。

Method: 提出了TA-STGCN（Traffic-Aware Spatio-Temporal Graph Convolutional Network）模型，将交通信号灯的变化状态（红、黄、绿）及其与行人关系作为输入特征，结合行人包围框大小信息，通过时空图卷积网络同时建模空间与时间上的依赖关系。

Result: 实验证明，TA-STGCN在PIE数据集上较主流基线模型准确率提升了4.75%，在复杂城市环境中更优地预测了行人意图。

Conclusion: 交通信号灯和场景上下文的引入显著提升了行人意图预测模型的准确性，所提TA-STGCN在实用性和安全性方面具有明显优势，为自动驾驶系统的安全导航提供了技术支撑。

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [82] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了DAM-QA框架，通过聚合图像多个区域的信息，有效提升了在文本密集图片上的视觉问答(VQA)性能。实验结果显示，其显著优于基础DAM模型，在DocVQA等数据集上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 区域感知的视觉-语言模型（如DAM）能够描述图像中具体区域，具备细粒度信息提取能力。作者认为这样的能力对于处理包含大量文本的VQA任务极为重要，因此提出了进一步发掘与评估此类模型在文本丰富VQA问题中应用价值的需求。

Method: 作者提出了DAM-QA，一个结合区域感知能力的VQA框架。该方法设计了新的评测协议，并引入了聚合多区视角答案的机制，从而能更好识别和利用与文本有关的证据，为文本密集型VQA问题提供支持。

Result: 在六个VQA基准数据集上进行的实验表明，DAM-QA在所有数据集上均优于基础DAM模型，特别是在DocVQA上取得了超过7分的提升。同时，其在参数数量较少的情况下，整体表现超过了同类区域感知模型。

Conclusion: DAM-QA框架有效利用了区域感知模型的细粒度描述能力，极大提升了在文本丰富VQA任务中的表现。该结果说明DAM类模型在经过合理集成策略后，具有应用于更广泛VQA任务的巨大潜力。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [83] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: 本文提出了一种高效的基于摄像头的障碍物规避系统，集成了YOLOv11目标检测和最新的单目深度估计方法，并通过Frenet-Pure Pursuit路径规划，有效提升了自动驾驶车辆在复杂场景下的避障能力。


<details>
  <summary>Details</summary>
Motivation: 自动车辆在复杂环境中安全避障是实现自主驾驶的关键问题。为此，提升感知精度和运动规划能力成为当前的研究重点，尤其是在不依赖昂贵传感器的前提下。

Method: 系统采用摄像头作为唯一感知传感器，通过YOLOv11对障碍物进行检测，利用Depth Anything V2等最先进的单目深度估计算法获取物体距离。随后利用Frenet-Pure Pursuit算法进行运动规划，并在真实大学校园环境中进行对比与实验评估。

Result: 系统在多种真实校园场景下进行了实地测试，结果显示该方法可以准确检测障碍物、估算距离，并通过高效的路径规划实现稳定的避障。对多种深度估计模型进行了比较分析，验证了其准确性、效率和鲁棒性。

Conclusion: 文中提出的基于摄像头的障碍物规避系统能够有效提升无人车在实际复杂环境中的避障能力，并证明相关感知与规划方法具有较好的实用性和推广前景。

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [84] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出SENTINEL框架，通过自动化、句子级偏好学习显著减少多模态大模型中的幻觉问题，无需人工标注，实验效果优越。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态理解中表现优异，但仍面临严重的幻觉（即模型编造与视觉输入不符内容）问题。现有方法要么计算开销过大，要么引入分布失配。因此，亟需一种高效、实用的幻觉抑制方法。

Method: 作者提出SENTINEL框架，核心思想是幻觉主要在文本生成早期阶段产生并传递。方法包括：（1）模型输出自举采样，通过两个开放词汇检测器交叉验证，自动判定句子是否存在幻觉，构建高质量偏好对；（2）利用上下文一致的正样本和带幻觉的负样本，逐步建立上下文感知偏好数据集；（3）通过句子级、上下文感知的偏好损失（C-DPO）训练模型，着重在幻觉初现阶段学习判别能力。

Result: SENTINEL方法在幻觉抑制上，相比原始模型幻觉率降低超过90%，并且在所有主流幻觉及通用能力基准上优于现有最优方法。

Conclusion: SENTINEL无需人工标注，极大提升多模态模型的鲁棒性和泛化能力，对学术与实际应用具有广泛意义，相关资源已开源。

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [85] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出了一种新方法RadGazeIntent，通过深度学习模型分析并预测放射科医生在医学影像阅读过程中的具体意图与注视点行为。


<details>
  <summary>Details</summary>
Motivation: 当前模型无法反映放射科医生每次注视背后的真实诊断意图，仅捕捉到注视的表面行为。作者希望揭示医生有意识搜索潜在疾病的真实目标，并提升对诊断流程的理解和支持。

Method: 提出了基于Transformer的深度学习架构，处理注视数据的时间和空间信息，将细粒度的眼动特征转化为对诊断意图的粗粒度理解。同时，整理并意图标注了三类医学眼动数据子集：系统顺序搜索（RadSeq）、不确定探索（RadExplore）和混合模式（RadHybrid）。

Result: 实验表明，RadGazeIntent能够有效预测放射科医生在特定时刻关注的诊断对象，并在所有意图标注的数据集上均优于其他基线方法。

Conclusion: RadGazeIntent能更深入地理解和预测医生的真实诊断意图，有助于未来医学图像分析和智能辅助诊断系统的发展。

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [86] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: 论文提出了SpatialTrackerV2，一种用于单目视频的前馈式3D点跟踪方法，在精度和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D点追踪方法通常依赖于分步管线，例如利用现成的深度估计和相机位姿模块。这种方式难以联合优化，也难以扩展到不同类型的数据。

Method: SpatialTrackerV2将点跟踪、单目深度估计和相机位姿估计融合为一个端到端、完全可微分的网络模型。它将世界中的3D运动分解为场景几何、相机自运动和像素级物体运动，能够在不同类型的数据集上规模化训练，包括合成、带标签和无标签的视频。

Result: SpatialTrackerV2相比现有3D跟踪方法提升了30%的准确率，并且达到了领先的动态3D重建方法的精度，同时速度提升达50倍。

Conclusion: 通过联合学习几何和运动信息，SpatialTrackerV2实现了高效且高精度的单目3D点跟踪，具有较强的通用性和扩展空间。

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [87] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出了MMHU大规模人类行为分析基准，涵盖丰富标注并支持多项任务，填补了自动驾驶领域人类行为认知基准的空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对自动驾驶中人类行为理解的综合性评测标准，限制了相关技术的研究进展与安全系统的开发。

Method: 作者构建了一个包含57,000个人体运动片段、173万帧、跨多源（包括Waymo、YouTube和自采数据）的数据集。采用“人类参与标注”流程，生成丰富的运动描述、意图及关键行为标签，并基于该数据集设计了运动预测、生成与行为问答等多任务基准测试。

Result: 实验展示了数据集与任务的多样性和挑战性，支持了从运动预测到行为问答的多维评测，提升了人类行为理解研究的广度和深度。

Conclusion: MMHU为自动驾驶中的人类行为分析提供了完整的数据和评测平台，将促进相关算法和系统的进步，为自动驾驶安全性与智能化发展奠定基础。

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [88] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: 本文提出了一种新的稀疏自编码器（CytoSAE），用于医学图像领域中的血液细胞图像解析，在保证可解释性的同时，也具备良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在医学影像领域的涌现，如何解释其推断成为亟需解决的问题，特别是在关键疾病诊断如血液学领域。当前缺乏针对医学影像基础模型的有效可解释工具。

Method: 作者设计了CytoSAE，一种稀疏自编码器，在超过4万张外周血单细胞图像上训练。该方法能适应多样的及域外数据集，并以医学专家验证的方式，自动发现与病理相关的视觉概念，并能在患者或疾病层面生成特异性概念，实现细胞级别的异常检测。

Result: CytoSAE在包括骨髓细胞学的多样数据集上表现出色，能识别形态学相关的细胞概念，包括可追溯到具体片段的异常，支持病理学专家的验证。在AML亚型分类任务中，CytoSAE的表现可与最先进方法媲美。

Conclusion: CytoSAE不仅提升了基础医学影像模型的可解释性，也保持了高分类性能，可用于亚型疾病细胞检测，在临床辅助诊断等场景具有应用前景。源码和模型权重已开源。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [89] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出PhysX，一个将物理属性和先验纳入3D资产生成流程的端到端方法，并引入首个系统注释的物理属性3D数据集PhysXNet与新型3D生成架构PhysXGen，提升了所生成3D资产在物理预测和几何质量上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前3D资产生成侧重于几何和纹理，而忽略物理属性，导致生成结果难以应用于需要真实物理交互的场景（如模拟、虚拟现实、AI驱动的机器人），亟需将物理知识融入3D生成过程。

Method: 1) 构建PhysXNet物理属性3D数据集，系统标注五大基础物理属性（如尺度、材料、功能等），采用以视觉-语言模型为基础的可扩展人工参与流程；2) 提出PhysXGen架构，通过双分支结构建模3D结构和物理属性间的关系，将物理知识注入预训练的3D空间，实现物理感知的高质量3D资产生成。

Result: 通过大量实验，PhysXGen展示出在3D物理预测和结构质量方面优于现有方法的性能，并具有良好的泛化能力。

Conclusion: PhysX框架为物理感知3D资产生成提供了新范式，有望推动仿真、实体AI等领域的物理生成研究，相关代码、数据和模型将对外开放，促进社区发展。

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 本研究将大语言模型（LLMs）定位为“主观文学评论家”，分析它们对日语科幻小说的美学评价偏好和模式。结果表明LLMs在文学评价上具有较大差异性和独特的倾向，并不像中立打分工具，更像有主观风格的评论家。


<details>
  <summary>Details</summary>
Motivation: 目前对LLMs的应用多为中立任务，但它们在涉及主观判断（如文学评论）中的表现和内在机制尚不清楚。作者希望研究LLMs能否像人类评论家一样展现出独特的批评风格及价值取向。

Method: 选取十篇日译英科幻短篇小说，六种先进LLMs各自独立开展七轮评价，用主成分分析、聚类及TF-IDF手段分析评分一致性、内部差异和用词特点，同时设计“同日七次评价”流程尽量去除外部偏差。

Result: LLMs的评分一致性差异显著（Cronbach’s α 1.00至0.35），共分出五种独特的评价模式。不同小说的评价方差最大相差4.5倍，各模型表现出明显不同的评价词汇风格。

Conclusion: LLMs并非中立的自动评测工具，而是像人类评论家一样有独特的批评风格和隐含价值体系。这对理解和使用LLMs进行文学等主观性任务具有重要启示意义。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [91] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本文提出了一个包含多种地图类型和主题的新Map-VQA数据集MapIQ，并系统性分析了多模态大模型在地图问答任务中的表现、鲁棒性与改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在图表理解与地图视觉问答（Map-VQA）中已获关注，但现有Map-VQA研究过于集中于等值区地图（choropleth maps），任务和主题广度有限，难以全面测试大模型对不同地图类型和分析任务的理解能力。因此，作者希望填补这些空白，推动更广泛和系统的评估。

Method: 作者构建了一个新的Map-VQA基准数据集MapIQ，涵盖等值区地图、变体地图和比例符号地图三种类型，话题横跨六大主题，共14,706个问答对。采用六类视觉分析任务，评测多种MLLMs，并与人类表现做对比。另设地图设计变更实验，分析颜色、图例等设计对模型表现的影响，并探讨模型对地理知识的内在依赖。

Result: 评测显示，大模型在不同地图类型和任务上的表现有明显差异，整体仍不如人类。设计变量（如颜色方案、图例样式调整等）会影响模型准确率，暴露了模型对某些视觉或地理线索的敏感性和脆弱性。

Conclusion: 现有MLLMs在复杂的地图理解和问答任务下还有明显不足，设计变化影响更大，这提示未来可通过丰富数据集、改进模型结构和增强地理知识嵌入等手段提升Map-VQA的性能。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [92] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 本研究探讨了在波斯语情感分析中结合小样本学习和增量学习方法，通过多语种预训练模型，在数据有限情况下获得高性能。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为资源稀缺语言，在情感分析领域的数据和工具非常有限。研究的动机是希望能利用高资源语言的知识，通过跨语言迁移和少量标注数据，实现对波斯语的高效情感分析。

Method: 采用三种多语种预训练模型（XLM-RoBERTa、mDeBERTa、DistilBERT），结合小样本学习和增量学习方法，在来自X、Instagram、Digikala、Snappfood和Taaghche的少量波斯语数据上微调模型，并使之能适应不同上下文类型。

Result: 实验显示，mDeBERTa和XLM-RoBERTa在波斯语情感分析任务中表现优异，准确率高达96%。

Conclusion: 将小样本学习、增量学习与多语种预训练模型相结合，可以在数据稀缺的语言中实现高效的情感分析。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [93] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: 提出了一种新的多模态学习框架PgM，将模态特征分为单模态和配对模态两部分，并分别学习和解码，实现了更灵活和高效的多模态表征。


<details>
  <summary>Details</summary>
Motivation: 多模态学习通常未能充分区分和利用来自单一模态与跨模态交互产生的特征。本文旨在分别优化这两类特征的学习与融合，以提升多模态任务的表现与泛化能力。

Method: PgM框架包括四个核心组件：模态分割器（区分单模态与配对模态特征）、单模态学习器、配对模态学习器，以及联合解码器。通过分离后分别学习和重构两类特征，支持灵活的分布调节和不同学习率分配。

Result: PgM在四类多模态任务上进行了大量实验，结果显示该方法有效提升了性能，并且具有很好的可迁移性；可视化分析也展示了两类特征在不同任务中的贡献。

Conclusion: PgM可以实现对单模态与配对模态特征的深度和灵活学习，为多模态学习领域带来更优的效果和广泛适用性。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [94] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: ExpliCIT-QA是一套多模态、可解释的表格视觉问答（TableVQA）系统，通过模块化流程实现对复杂表格图片的内容分析与问答，并显著提升了系统的可解释性与透明度。


<details>
  <summary>Details</summary>
Motivation: 当前的表格问答系统，特别是端到端的TableVQA方法，在可解释性和结果可审计性方面存在不足，限制了其在金融、医疗等高敏感领域的应用。作者希望通过开发可追溯中间过程、结果可审计的系统，弥补现有TableVQA系统在解释性方面的空白。

Method: ExpliCIT-QA采用模块化架构，包括：（1）用链式思维方式识别并整理表格图片信息；（2）自然语言生成逐步推理解释；（3）将推理步骤自动转为Python/Pandas代码，并能自动修正错误；（4）执行代码求出最终答案；（5）用自然语言说明答案的计算过程。所有中间结果和操作步骤均可溯源、检查。

Result: 在TableVQA-Bench数据集上的实验显示，ExpliCIT-QA在解释性和透明度方面较现有基线系统有明显提升，系统的中间输出、推理代码、最终答案均可供后续审计分析。

Conclusion: ExpliCIT-QA有效提高了表格视觉问答任务的可解释性和透明度，为金融、医疗等需要结果可审计的领域应用TableVQA技术提供了有力支撑。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [95] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: 本文提出了一种结合轻量语法分析与大语言模型（LLM）的“钳形策略（CRABS）”，用于更准确理解Python数据科学、机器学习笔记本中的信息流与执行依赖，显著提升了无需执行代码情况下的笔记本分析效果。


<details>
  <summary>Details</summary>
Motivation: 当前数据科学/机器学习笔记本常用于实验与分享，但由于数据与依赖环境复杂，复现或理解其数据流常常十分困难。虽然大语言模型可以“静态”理解代码，但在处理真实复杂笔记本时会因幻觉与长上下文问题出现理解偏差，亟需更稳健、准确的分析方法。

Method: 作者提出两步钳形策略（CRABS）：首先用浅层抽象语法树（AST）分析，对每个单元格的输入输出做上下界估计缩小理解范围；然后用LLM对剩余有歧义的单元逐步逐格进行零样本推理，输出最终的单元格真实输入/输出信息流与执行依赖图。

Result: 在50个Kaggle高赞真实笔记本、共3454组实际单元格输入输出的标注数据集上验证方法。AST分析后LLM能正确分辨98%歧义（1425个歧义单元分辨1397个），整体CRABS方法在识别单元格间信息流和传递执行依赖任务上F1分数分别为98%和99%。

Conclusion: 将浅层语法分析与LLM相结合，可有效克服LLM在静态理解真实复杂Notebook时的幻觉和上下文限制，显著提升信息流与执行依赖的推理能力，为笔记本复用、迁移和评估提供了更强有力的支撑。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [96] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: 本文介绍了AI Wizards团队在CLEF 2025 CheckThat! Lab的主观性检测任务的方案，该任务旨在对新闻句子进行主观/客观分类，包括单语、多语和零样本场景。核心方法是融合情感分数与transformer模型表示，并通过门限校准处理类别不平衡。实验表明该方法显著提升了性能，并在一些新语言上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 新闻文本中主观性检测对于虚假信息、舆论分析等任务意义重大，特别是在多语言和低资源环境下。现有transformer模型虽然有较强的语义理解能力，但直接微调在处理主客观分类时仍有提升空间。因此，作者希望通过引入额外情感信息来丰富模型表征，提高在主观性判别上的泛化和精度。

Method: 主要方法是在transformer模型（包括mDeBERTaV3-base、多语言和单语ModernBERT-base，以及Llama3.2-1B）基础上，提取每个句子的情感分数（来自辅助情感模型）并与句子表征拼接，然后用于主观/客观分类。此外，为应对各语言类别不平衡，采用了基于开发集最优的门限校准技术调整决策边界，以最大化F1得分。

Result: 在多语言和零样本场景下，融合情感特征的方法显著提升了模型对主观句的识别性能，特别是在主观F1分数上提升明显。在最终评测中，该方案在希腊语上取得了第一名，Macro F1达到0.51，在其他新语言（如罗马尼亚语、波兰语、乌克兰语）上也表现良好。

Conclusion: 结合情感特征的方法有效提升了多语言新闻主观性检测性能，尤其适用于主观句识别和零样本迁移场景，对低资源语言具有较强的泛化能力。未来可进一步拓展其它辅助特征和自适应方法以提升多语种文本分析的实用性和鲁棒性。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [97] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本论文复现实验分析了大语言模型在处理事实与反事实信息竞争时注意力头的作用，发现注意力头倾向于通过一般性复制抑制而非选择性反事实抑制来推动事实输出，且注意力模式具有领域依赖性。


<details>
  <summary>Details</summary>
Motivation: 近年来多项研究关注大语言模型内部处理事实与反事实竞争信息的机制，但结果存在一定分歧，本研究旨在复现并整合这些研究的发现，厘清注意力机制在信息竞争中的具体作用。

Method: 本研究回顾并复现Ortu等人、Yu等人以及McDougall等人三项相关工作，通过机制解释性工具分析注意力头强度与事实输出比例之间的关系，检验注意力头的抑制机制假设，并探讨这些注意力模式的领域特性。

Result: 研究发现，事实输出相关的注意力头发挥的是一般性复制抑制作用而非对反事实的选择性抑制，且增强这些注意力头可能同时抑制正确事实。另外，注意力头的行为表现出领域依赖性，大模型表现出更专业、更具类别敏感性的注意力模式。

Conclusion: 注意力头在事实与反事实信息竞争中的作用机制偏向于整体复制抑制，且其表达受领域及模型规模影响，这为理解大语言模型内部机制提供了新视角，对未来模型解释和调优具有参考价值。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [98] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: 本文介绍了一个包含英语及全部22种官方印度语言的语言识别数据集，同时提供了基于最新机器学习和深度学习方法的基线模型，旨在推动多语言环境下的语言识别研究。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下，尤其是印度各官方语言与英语混杂的场景，准确的语言识别对于后续NLP应用非常关键。印度多种语言存在词汇与语音相似、共用文字脚本等特点，导致在噪声、短文本、代码混合环境中语言识别极具挑战性。

Method: 作者构建并公开了包含23种语言（英语及22种印度官方语言）、共23万条标注句子的大型数据集。针对该任务，作者基于机器学习和深度学习发展了多种具有代表性的基线模型，并对它们在数据集上的表现进行了评估。

Result: 新发布的数据集为该任务提供了可靠的数据基础。作者开发的基线模型表现优良，与现有最优方法（state-of-the-art）效果相当。

Conclusion: 本文的贡献为多语言环境下语言识别研究提供了重要的数据和工具，有望推动包括机器翻译、信息检索等多项NLP任务的发展，尤其对印度及多语言场景具有显著现实意义。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [99] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，使自回归语言模型能同时预测多个未来token，大幅提升推理速度和并行效率，同时保证生成质量不下降。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型受限于逐步生成，每次只能预测一个token，导致推理速度慢，尤其在文本后期生成阶段这种限制更加突出。为提升并行性和生成速度，迫切需要新的方法突破这种单步生成瓶颈。

Method: 1）用“掩码输入”形式让模型能基于同一前缀，一次联合预测多个未来token；2）提出带门控机制的LoRA优化，使原有大模型支持多token预测且功能不受损；3）针对预测未来token设计轻量级可学习采样模块，用于生成连贯序列；4）引入一致性等辅助损失以加强联合预测的准确性和连贯性；5）提出推测式生成策略，在保证高保真的情况下，实现未来token的二次展开。整体通过在预训练模型上的有监督微调得到。

Result: 新方法微调后，代码与数学任务的生成速度提升近5倍，通用对话和知识任务提升近2.5倍，且生成质量无损失。

Conclusion: 通过联合预测和创新的采样及损失机制，自回归语言模型可大幅提升推理速度和并行性，为大模型高效推理提供了新方向。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [100] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文评估了跨领域模型迁移、多领域数据融合和小样本学习在PII识别中的有效性，发现领域间迁移具有方向性，数据融合对于某些领域有效，部分领域只需少量数据也能高质量识别。


<details>
  <summary>Details</summary>
Motivation: 在自动文本匿名化任务中，准确识别PII极为关键。目前跨不同领域的PII识别存在挑战，因此作者希望探究不同领域间模型迁移、数据融合及小样本学习对PII识别的影响，为实际应用提供指导。

Method: 作者选用三大领域（医疗I2B2、法律TAB和百科Wikipedia）的标注语料，围绕四个方面评估PII识别模型的表现：领域内、跨领域转移、多领域数据融合、小样本学习。

Result: 实验表明，法律域的数据在跨领域迁移到百科文本时效果较好，但医疗域模型很难被其他领域迁移。多领域数据融合带来的提升具有领域特异性。在低专化领域，仅用原始训练数据的10%就能实现高质量PII识别。

Conclusion: PII识别任务的领域差异显著，跨领域和多领域融合的效果不能一概而论。对于低专化领域，小数据集也能取得优秀表现，这为实际应用中的数据节省和泛化能力提供了参考。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [101] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文提出COLA-GEC框架，实现了语法纠错（GEC）与语法可接受性判断（COLA）任务的双向知识迁移，方法在多语言基准测试中取得了最优性能，并分析了具体改进空间。


<details>
  <summary>Details</summary>
Motivation: GEC和COLA任务都依赖于对语法知识的理解，但两者通常独立发展。本文旨在通过知识融合，提升两类任务的性能，并探索二者之间的协同潜力。

Method: 提出COLA-GEC双向框架：一方面借助GEC数据增强COLA模型，实现多语种性能提升；另一方面在GEC训练中引入可接受性信号及动态损失函数，引导输出语句更趋语法正确。

Result: 新框架在多个多语言基准上取得了最新最优结果。系统性误差分析揭示了如标点错误纠正等尚存难点。

Conclusion: COLA-GEC框架有效促进了GEC和COLA任务的互补发展，但在标点等细节仍有提升空间，为今后语法建模研究提供了启示。

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [102] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: DualReward提出了一种新颖的强化学习方法，提升了完形填空测试中自动干扰项生成的质量，尤其在多样化数据上的表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动生成干扰项的方法多依赖监督学习或静态模型，难以有效区分高质量（人工创建）和低质量（模型生成）的干扰项，且对于跨领域、多类型问题适应性不足。作者希望通过引入更灵活且有区分度的奖惩机制，提升自动生成干扰项的质量和多样性。

Method: 作者提出DualReward框架，采用双重奖励结构，并根据模型表现和置信度自适应调整奖励信号强度，区别对待人工干扰项和模型生成干扰项。此方法在完形填空（CLOTH-F）和句子级（MCQ）数据集上进行了实验验证。

Result: DualReward在同质化数据集（CLOTH-F）上有一定提升，在跨域多样化数据集（MCQ）上取得了3.48-3.86%的P@1提升，效果明显优于现有方法，展现出较强的适应不同题型和领域的能力。

Conclusion: DualReward框架能更好地利用可靠的人工样本进行学习，同时高效探索高质量的新型干扰项，为自动测试题生成提供了灵活有效的解决方案，特别适合处理多样化的题型和领域。

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [103] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: 本文发现大模型（LLM）内部对“有害性”的理解与其“拒绝”回应是两个不同维度。基于此，作者提出可利用大模型对有害性的隐式表征来提升安全检测效率。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型会拒绝执行有害指令，但尚不清楚它们是否真正理解了“有害性”，还是只是简单地依照规则拒绝而已。此前研究发现拒绝行为可由一维方向编码，本文进一步探究有害性在模型内部的独立编码方式。

Method: 作者分析了大模型内部的特征空间，找到“有害性方向”，并通过干预（steering）的方式考察模型在该维度上的反应。还分析了‘越狱’方法和对有害任务的微调，以及这些干预对模型内部有害性判断的影响。最后基于内在有害性表征设计Latent Guard安全方法，并与现有安全机制进行对比。

Result: 干预有害性方向会改变模型对指令有害性的判断，而仅调控拒绝方向只会让模型拒绝而不改变判断；一些‘越狱’方法是通过削弱拒绝信号绕过限制，但不会改变模型内部对有害性的信念；有害性微调同样难以改变其内在判断。Latent Guard利用模型对有害性的本征表征，在多种越狱攻击下检测表现优良。

Conclusion: 大模型对有害性的内部理解比单纯拒绝策略更为稳健，可用于开发本征安全检测机制。这为AI安全研究提供了新视角，也有助于提升模型对恶意指令的防御能力。

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [104] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个多语言、大规模的指令跟随能力评测基准Marco-Bench-MIF，解决了现有数据集单语化或机器翻译局限，覆盖30种语言，并对不同大模型进行了评测和分析。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的指令跟随能力评价数据集要么偏重英语，要么仅靠机器翻译，无法真实反映多语言环境下模型表现。因此，需要构建具备本地化、多语言特征的严格基准。

Method: 论文基于已有的IFEval数据集，设计了本地化多语言扩展Marco-Bench-MIF，覆盖30种语言。方法包括结合人工核查的混合翻译流程，对各语言做文化、语法及写作习惯的本地调整（如中文改动大小写要求、替换公司名等），并用该基准对20余种主流LLM进行了系统测试。

Result: 1. 资源丰富与匮乏语言之间准确率存在25-35%的差距；2. 参数规模提升使表现提升45-60%，但字符集相关挑战仍旧明显；3. 单纯用机器翻译数据低估模型真实准确率7-22%。

Conclusion: 多语言场景下，大模型指令跟随能力在不同语言间存在显著差异，仅用机器翻译无法得到可靠评估。Marco-Bench-MIF为真实、多语的模型能力检验提供了有效工具，对于后续改进多语言模型具有指导意义。

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [105] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题求解中的应用，涵盖任务分类、方法、评测方式、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决能力不仅是数学推理的重要组成部分，还在教育、AI能力评估和多模态评测等领域有广泛应用。由于深度学习和多模态大模型的飞速发展，该领域近年来受到极大关注，因此有必要对其进行系统性梳理和总结。

Method: 本文主要采用文献综述的方法，对相关任务、深度学习方法、评测指标和方式进行全面回顾与归纳，同时对现存挑战进行了梳理和批判性讨论。

Result: 文章整理和总结了目前几何问题求解领域相关的深度学习研究，分类描述了目前的方法和评测标准，并指出了存在的挑战。此外，作者还构建了持续更新的文献列表以供学术参考。

Conclusion: 本文为深度学习解决几何问题提供了详细的综述和参考，对相关任务、方法和指标进行了系统总结，有助于推动该领域未来的研究与发展。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [106] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文提出了首个大规模多语言图表问答数据集PolyChartQA，涵盖10种语言，旨在促进多语言图表理解的研究。实验证明当前模型在英语与其他语言（尤其是低资源、非拉丁语种）之间存在较大性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准主要集中于英语，限制了其全球适用性和公平性。因此需要一个多语言的基准，以支持更广泛的用户与研究。

Method: 作者提出了一套解耦的多语言图表数据生成流水线，将图表的数据部分与渲染代码分离，仅需翻译数据即可生成多语种图表。通过大模型辅助翻译并实施严格质量控制，保证数据的语言与语义一致性。最后构建了涵盖22,606个图表、26,151个问答对、10种语言的数据集PolyChartQA。

Result: 作者利用PolyChartQA对多种开源与闭源的视觉-语言大模型进行了评测，发现这些模型在英语之外的语言表现明显下降，尤其是低资源、非拉丁语种的差距更大。

Conclusion: PolyChartQA为系统性评估和推动多语言视觉-语言理解模型的发展奠定了基础，有助于实现全球包容的AI系统。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [107] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE 是一种针对 GPU 优化的 BPE 分词算法，在批量推理场景下显著提升了分词吞吐量，效率最高可达现有主流实现（如 tiktoken、HuggingFace Tokenizers）的 2~2.5 倍。


<details>
  <summary>Details</summary>
Motivation: 目前主流的大语言模型分词算法主要受限于 CPU 计算，无法充分利用 GPU 并行能力，尤其在推理大批量文本时效率极低。现有开源实现如 tiktoken 等还会用 Regex 进行预分词，导致复杂度较高，成为推理速度瓶颈。作者旨在解决高性能分词这一痛点，提升 LLM 的推理能力。

Method: 作者提出了 BlockBPE，将 BPE 分词的核心计算迁移到 GPU，并且放弃 Regex 预分词，允许一定的生成质量损失，强化并行合并操作，将算法复杂度从 O(n log n) 降低为 O(nd)（d 远小于 n），极大适配高吞吐推理场景。

Result: BlockBPE 在高批量推理任务下，吞吐量比 tiktoken 提升最高达 2 倍，比 HuggingFace Tokenizers 提升最高达 2.5 倍，性能优势明显。

Conclusion: BlockBPE 能够有效利用 GPU 并行加速分词，极大提升了大批量推理速度，针对高负载语言模型推理场景具有显著价值。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [108] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种动态注意力感知的无任务特定提示压缩方法（DAC），通过结合信息熵和注意力信息，实现对提示的高效压缩，从而在各种领域和任务中显著提升大模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前提示压缩方法主要基于信息熵压缩词汇单元，未充分考虑注意力关键token的重要性和信息熵在压缩过程中的动态变化，导致信息损失或压缩效果受限。

Method: 提出一种动态注意力感知方法，将信息熵与注意力信息结合，动态监测熵的变化，实现细粒度的提示压缩，既保留关键信息又减少冗余。

Result: 在LongBench、GSM8K和BBH等多领域任务上进行实验，结果显示DAC方法在多种任务和大模型中都带来稳健且显著的性能提升。

Conclusion: DAC方法有效解决了之前忽视注意力关键token和信息熵动态变化的问题，充分验证了其通用性和优越性，有望在长文本场景等实际应用中推广。

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [109] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出IAM框架，通过在小型和大型LLMs间进行Attention映射，加速计算并减少KV cache使用量，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLMs）在长上下文处理时资源消耗巨大。尽管已有多种提升推理效率的方法，但多数仅针对模型内部稀疏性进行优化，并未利用外部信息。作者发现不同规模LLMs的注意力矩阵具有高度相似性，这为资源优化带来了新思路。

Method: 作者首先系统性分析了注意力矩阵间的相似度度量方式、映射层的选择以及映射的一致性。在这些分析基础上，作者设计了IAM（Attention Mapping）框架，将小模型的注意力信息映射到大模型，实现双重优化：加速注意力计算、减少KV缓存开销。

Result: IAM框架在加速预填充（prefill）环节上达到了15%的提升，并减少了22.1%的KV缓存使用，且在性能上几乎无损。多种模型系列实验验证了其普适性。此外，IAM可与现有多种KV cache优化方法结合使用，进一步扩展其应用范围。

Conclusion: IAM方法为现有大模型推理资源优化提供了新的方向，能够有效提升推理效率，减少显存需求，并具有良好的通用性和兼容性，是提升LLMs效率的有力补充。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [110] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本论文提出一种多阶段基于查询的知识图谱问答（KGQA）框架，该框架在多跳推理和涉及时间信息的问题上显著提升了性能，尤其适用于小型语言模型。论文还提出了结合CoT推理的实体链接和谓词匹配新方法，并通过实验验证了通用性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答任务中表现优异，但对于需要多步推理和涉及时间推断的问题表现不佳。作者希望通过模块化手段提升模型在这些复杂问答上的能力，同时减少对超大模型参数的依赖。

Method: 论文提出了多阶段的查询生成框架，将复杂自然语言问题转化为可执行的知识图谱查询，逐步解决多跳和时间推理难题。同时引入基于CoT（Chain-of-Thought）推理的实体链接和谓词匹配方法以改善检索质量。

Result: 提出的方法在WikiData上的多跳推理和时间推理数据集上均取得了优于现有方法的表现。论文还通过泛化和拒绝机制实验，表现出该方法对多数据集的良好鲁棒性。

Conclusion: 多阶段的基于查询的KGQA框架，结合CoT推理的新型实体链接与谓词匹配方法，能够有效提升小型语言模型在复杂问答任务上的表现，为高效、可解释的问答系统设计提供了新思路。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [111] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 本文提出了一种新的适用于大型语言模型（LLM）权重的幂次量化（POT）方法，不仅在极低精度下保持了较高的精度，还大幅提升了GPU上的推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现优异，但其部署受限于巨大的计算资源消耗。现有幂次量化方法虽可提升CPU端效率，但GPU端存在位操作效率瓶颈。因此，需要新的量化方法，既能保证模型效果，又能在GPU上高效运行。

Method: 作者提出了一套新的POT量化方法，包括两个关键步骤：一是采用鲁棒初始化获得良好的量化比例因子起点；二是通过少量校准数据进一步微调这些比例因子。这种方法优化了权重量化与反量化的过程，并特别针对GPU平台做了效率提升。

Result: 在2位和3位等极低精度下，该方法在准确率上超越了现有最先进的整数量化算法。推理速度方面，POT量化使得NVIDIA V100提升3.67倍，RTX 4090提升1.63倍，相较于传统的整数量化反量化过程。

Conclusion: 该方法有效地缓解了大模型部署的计算资源压力，实现了低精度高效推理，特别适用于GPU平台，为LLM的实际应用和落地提供了重要技术手段。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [112] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 该论文提出了一种保留有毒性内容的翻译框架，重点关注在低资源、带有俚语和方言的语言对之间如何更准确地翻译有害言论。以新加坡式英语（Singlish）为例，展示了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的翻译系统在处理地区俚语、方言混合及携带文化含义的有害言论时经常失真，尤其是在低资源语言之间。与此同时，安全机制过度过滤导致有害内容丧失，使内容审核和技术测试难以真实反映社交现实。

Method: 提出一个可复现的两阶段框架：首先通过人工甄选和排序 few-shot 样本，针对 Singlish 编写富含俚语和毒性示例的提示；其次，基于语义相似度与正反向翻译结果，对多种大型语言模型进行模型与提示搭配优化。整个流程辅以人工验证。

Result: 实验表明，该框架在翻译质量和效率上均优于传统管道。通过定量的人类评测，证实了方法的有效性。

Conclusion: 该框架不仅提升了本科领域特有语言背景下的翻译质量，还促进了多元文化大模型的安全测试和治理，强调了在真实应用场景下保留社会语言细节的重要性。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [113] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: 本文利用大语言模型（LLM）与眼动追踪实验，探讨人类与LLM在阅读理解尤其是功能任务（如推理、情感解读、信息检索）中的异同。通过将文本转为图结构并结合眼动数据，发现LLM在语义理解层面表现出与人类高度一致的拓扑结构分布。


<details>
  <summary>Details</summary>
Motivation: 此前研究仅关注单词层面的相关性，忽略了语句与整体结构对理解的影响，使得结论较为片面。本文旨在通过引入图结构分析，更全面地比较人类和LLM的复杂阅读理解过程。

Method: 采用LLM驱动的AI agent，将阅读材料按语义和问题为导向分组成图结构（节点和边），并用眼动追踪比较人类对重要节点和边的注视分布。

Result: 实验发现，LLM在图结构级别的语言理解与人类表现出高度一致性，重要节点和边的注视分布也有明显规律。

Conclusion: LLM在复杂语义理解方面已与人类有较高一致性，为未来的人机协同学习策略提供了理论和方法支持。

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [114] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: 该研究通过实验探讨了大型语言模型（LLM）代理在价值观相似性对信任与亲密关系建立中的作用。结果发现：价值观越相似的LLM代理会表现出更高的互信与亲密感。


<details>
  <summary>Details</summary>
Motivation: 在人类社会中，价值观相似性是建立信任和亲密关系的重要基础，但该原则在由LLM代理组成的人工社会中尚未被检验。作者希望验证价值观相似性是否同样适用于人工智能代理。

Method: 作者首先通过初步实验评估了如何有效控制LLM的价值观设定，找到了最佳模型和提示设计方案。接着在主实验中，生成具有特定价值观的LLM代理对，让它们对话后相互评估信任与亲密度。实验覆盖英语和日语两种语言。

Result: 实验显示，价值观越相似的LLM代理对，其互信与亲密关系的评分越高。这一现象在英语和日语环境下均得到验证，表明具有一定的普适性。

Conclusion: LLM代理模拟可以作为社会科学理论检验的有效平台。研究表明，价值观对关系建立的影响机制可通过LLM模拟揭示，有助于社会科学理论的推进并为新理论的产生提供基础。

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [115] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在为不同群体（如普通用户、儿童、语言学习者）简化同形异义词（homonym）定义时，可能导致定义信息丢失的问题。通过多模型和多语料评测，发现简化过程往往忽视词语多义性，增加误解风险，但经过特定优化，模型表现可提升。


<details>
  <summary>Details</summary>
Motivation: 同形异义词存在多个含义，对于儿童或语言学习者等群体，定义简化是刚需，但过度简化会忽略多义性，可能误导用户。本研究旨在系统评估简化对同形异义词定义质量的影响。

Method: 作者构建了包含多语言的新型评测数据集，设置了Normal、Simple和ELI5三种目标群体提示，评估DeepSeek v3、Llama 4 Maverick、Qwen3-30B A3B、GPT-4o mini和Llama 3.1 8B等多个LLM，采用LLM自动评判和人工标注方式。还尝试对Llama 3.1 8B进行直接偏好优化的微调。

Result: 研究发现，定义简化往往导致多义词涵盖不全，信息不完整，误解风险增加。但对Llama 3.1 8B进行直接偏好优化微调后，模型在所有提示下定义质量显著提升。

Conclusion: 简化定义虽便于特定群体理解，但必须平衡信息完整性和简明性。教育类NLP系统需关注词义多样性，以保证给所有学习者提供可靠且具上下文相关性的信息。

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [116] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: 本论文主要通过引入新颖的表示分析方法与优化技术，提高神经语言模型在数据和参数利用上的效率，显著提升了模型的性能、稳定性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经语言模型在实际应用中常常面临对大规模标注数据与众多参数的依赖，导致训练成本高、泛化能力有限。为解决数据与参数的低效利用问题，推动模型向高效、鲁棒、广泛适应的方向发展，本论文提出对语言模型表示的深入研究与优化。

Method: 论文首先分析了神经语言模型内部的语言表示性质和动态，提出基于表示平滑性的正则化方法，并利用Jacobian和Hessian矩阵稳定训练过程，提高模型对输入扰动的鲁棒性。其次，结合主动学习与参数高效微调，研发基于平滑性分析的early-stopping技术，不依赖人工标注的验证集，同时通过创新性结合主动学习与参数高效微调减少标签和计算资源消耗。最后，探讨了通过in-context learning提升弱监督效果的方案，让模型更优利用未标注数据。

Result: 在多项NLP任务中的广泛实验结果表明，文中提出的平滑性引导和数据高效策略，大幅度超越传统方法，在性能、训练稳定性和利用效率上均有显著提升。弱监督结合in-context learning在低资源和动态数据环境下也带来了显著的准确率和泛化能力提升。

Conclusion: 论文提出的一系列方法有效提升了神经语言模型对有限标签和参数的利用效率，提高了模型在实际场景中的表现和适应性，为数据高效、参数高效的智能语言模型发展提供了新思路和方法。

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [117] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: 本文提出了一种通用的语言创造力测试，比较了人类和大型语言模型（LLM）在新词创造和隐喻使用等任务上的表现。结果显示LLM在大多数指标上超过了人类，但创造力类型有差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于系统性评估和比较人类与LLM在语言创造力方面的异同，探索LLM能否超越人类在创造新词汇和语言表达方面的能力。

Method: 设计包含派生、复合和隐喻等多类语言任务的测试，邀请24名人类和等量LLM参与，并用OCSAI工具从原创性、细致度和灵活性三个方面自动评分，辅以部分人工分析。

Result: LLM在所有评估标准上总体优于人类，在八项任务中有六项得分更高。个别答案的独特性上，人类和LLM差异较小。人工分析发现人类更偏E-创造力，LLM更偏F-创造力。

Conclusion: LLM在语言创造力测试中已具备超越一般人类的能力，但两者在创造力类型和表现上仍有区别。

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [118] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: 本论文评估了28个大语言模型在理解和推理方位（如南、北、东、西）方面的能力，发现即使是最新的大型推理模型依然难以稳定正确判断。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多领域表现优异，其空间推理能力尤其在实际应用中的表现受到关注。方位推理是空间理解和导航的重要组成部分，因此有必要系统性地测评现有模型在该领域的能力极限。

Method: 作者构建了一个基于模板的大型基准题库，模板涵盖了多种变量，包括不同移动方式以及叙述视角（第一、二、三人称）。利用这些题库对28个市面上流行的大语言模型进行了系统评测，考查其在特定情境下计算并判断正确方位的能力。

Result: 实验结果显示，无论是传统的大语言模型还是结合推理能力的新一代大型模型，在所有题目场景下均无法稳定准确地给出正确方位答案。模型在变换移动方式和视角时表现尤为不佳。

Conclusion: 目前的大型语言模型在方位判断和空间推理任务上仍有明显短板。这提示未来模型训练应更重视空间推理能力的提升，以满足实际应用场景需求。

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [119] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 本论文提出一个基于模块化文体分析的AI文本检测系统，结合spaCy语言特征提取和轻量提升机分类器，并利用大规模机器生成文本数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本生成的普及带来了检测与甄别其真伪的需求，作者希望用易于解释且资源消耗低的方法提升检测系统的能力。

Method: 使用spaCy进行文本预处理，提取包含词法、句法、实体等信息的几千种特征；采用light-gradient boosting machine作为分类工具，借助50万以上机器生成文本的大数据集进行训练，探索了多种参数优化方法。

Result: 该方法无需神经网络技术，依旧能有效区分AI生成与人工生成文本，并具备较高可解释性和低计算资源消耗。

Conclusion: 基于模块化文体特征和轻量级提升机的非神经方案，可高效、可解释地检测大规模AI生成文本，是既实用又有效的解决路径。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [120] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: 该论文提出了一个新的自动化流程，为整本叙事文本生成高质量指代消解标注，并基于此创建了首个图书级别的指代消解基准数据集BOOKCOREF。该资源大大扩展了以往评测的文本长度，并揭示了现有模型在大规模文本中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有指代消解系统主要在较短或中等长度文档上评测，而现实中如书籍这样的大文本却缺乏合适的基准数据集，无法反映系统在大规模场景下的性能。该论文旨在填补评测长文本、特别是“整本书”规模下指代消解能力的数据空白。

Method: 作者首先提出了自动化指代消解标注流程，然后利用该流程为整本叙事文本生成高质量标注，最终构建出平均长度逾20万token的BOOKCOREF数据集，并通过实验评估了流程的鲁棒性和新数据集的应用价值。

Result: 实验显示该自动化生成的标注在质量上是可靠的，并且利用该书籍级基准能帮助现有长文档指代消解模型的评测得分提升最高达+20 CoNLL-F1分。同时，研究发现这些指代模型在处理超长文本时表现远不如在中小型文档。

Conclusion: BOOKCOREF作为首个书籍级别的指代消解数据集，对推动长文档指代消解研究具有重要意义，同时揭示了当前技术在大规模文本处理上尚存重大挑战。作者公开了数据和代码以促进该领域进一步发展。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [121] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: 论文提出了一种融合苏格拉底式提问、链式推理、简化游戏化和形成性反馈四种教学手段的数学学习方法（MEGA），结果显示该方法比传统链式推理(CoT)法更有助于大学生理解数学，尤其是在难度较高的问题上表现显著更好。


<details>
  <summary>Details</summary>
Motivation: 许多大学生因数学学习困难而回避相关学科，主要原因之一是教学方法不佳；因此，研究旨在探索能更有效帮助学生理解数学的方法。

Method: 设计对照实验，将大学生参与者随机分配不同试题，比较MEGA方法（结合苏格拉底、CoT、游戏化、反馈）与传统CoT方法在数学学习上的效果。选取GSM8K和MATH两套题集各60题作为样本，使用学生评价对GPT4o和Claude 3.5 Sonnet两款LLM生成的解释进行效果评估。

Result: 实验结果表明，MEGA方法在大多数情况下被认为更有助于学习，尤其是在更难的MATH数据集上，MEGA方法的认可度（47.5%）远高于CoT方法（26.67%）。

Conclusion: 综合来看，融合多重教学策略的MEGA方法比单一的链式推理法更能帮助学生理解复杂的数学问题，为优化数学教学提供了新思路。

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [122] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型（LLM）文本数据增强的新评估框架，旨在平衡语义一致性、多样性和生成效率，提升下游NLP任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前文本增强技术难以在大规模或多轮生成时保证语义不变，容易产生冗余和语义漂移，特别是在低资源场景下效能有限。

Method: 提出了两个主要评估模块：1) 可扩展性分析——在增强规模扩增时评测语义一致性；2) 迭代增强与摘要优化（IASR）——评估递归改写中发生的语义漂移。并对主流LLM进行实证评测。

Result: 实验证明GPT-3.5 Turbo在语义保真、多样性以及生成效率间达成最优平衡。应用于真实话题建模任务，结合GPT增强的少样本标注，主题精细度提升400%，且完全消除主题重叠。

Conclusion: 所提评估框架能有效支持LLM文本增强的结构化评估，验证了其实用性，为实际NLP任务中的数据增强提供了新的理论和应用指导。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [123] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT是一个针对生成式语言模型的评测任务集，本文介绍了2025年Sensemaking子任务，通过教师出题、学生答题和评委评分三步，对模型进行全面测评，涵盖多语言和多种材料类型，实验揭示了当前问题与挑战。


<details>
  <summary>Details</summary>
Motivation: 评估生成式语言模型时缺乏高层、通用又可操作的评价标准。Sensemaking任务借鉴了课堂考试的设置，目的是系统性地检测模型在理解、推理和表达等多个层面的能力，特别关注模型是否能严格基于给定材料进行推理和作答。

Method: Sensemaking分为三步：（1）教师系统针对材料生成问题；（2）学生系统基于材料回答问题；（3）评价系统对答案打分。2025年任务使用来自事实核查、教材、讲座录音和教育视频的7类多语种材料，参与团队各自提交教师、学生与评价系统，也采用商业大型语言模型作为基线，采用全自动与人工最简评价进行对比分析。

Result: 1. 问题生成环节中，现有评价策略难以区分不同问题集质量；2. 问题回答环节，主流LLM整体表现尚可，但严格限定于材料的作答有明显不足；3. 答案评价环节，基于LLM打分的评委系统对胡乱搭配的问题和答案也会错误地给出可接受评分，暴露出评委系统的弱点。

Conclusion: ELOQUENT Sensemaking在多语言、多源材料和多环节下系统分析了当前生成式模型的实际表现，为后续改进出题、作答和自动评价流程提供了数据和问题点，尤其暴露了自动化评估手段现存的不足，需要开发更鲁棒的评价方案。

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [124] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: 本文提出了一种行为翻译风格空间（BTSS），通过分析翻译过程中的眼动和敲击数据，构建了多层次的行为模式结构，用于模拟人与机器在翻译中的行为和认知动态。


<details>
  <summary>Details</summary>
Motivation: 传统翻译过程的研究多关注语言结果，却较少关注翻译过程中的行为表现（如眼动、敲击等）及其背后的认知和情感机制，作者希望借助这些过程性数据深入揭示翻译者的心理特征和翻译风格。

Method: 作者采集了翻译者在翻译时的按键记录和眼动追踪数据，将这些可观测的行为数据作为认知处理过程的外在表现，通过多层嵌套的结构对行为翻译风格进行建模，形成了BTSS体系。

Result: 成功构建了BTSS模型，并基于实验数据将行为表现映射到认知与情感过程，从而揭示了自动化行为、情感变化和高级认知之间的关系。

Conclusion: BTSS为理解翻译过程中的行为、情感和认知互动提供了有效架构，也为开发能够模拟真实翻译过程的计算翻译智能体提供了理论基础。

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [125] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: 本论文提出了一种无需自动语音识别（ASR）的少样本孤立词阅读能力评估方法。通过对比儿童语音与成人参照模板，利用大型自监督学习（SSL）模型的中间层特征进行编码。实验结果显示，方法对成人表现良好，但对儿童语音效果显著下降，揭示了SSL特征在儿童语音处理中的局限。


<details>
  <summary>Details</summary>
Motivation: 当前低资源环境下，儿童语音评估面临缺乏高质量ASR系统的问题。儿童语音与成人有显著差异，现有基于成人数据的模型难以泛化，因此亟需开发适用于儿童及低资源场景的新方法。

Method: 无需ASR地，通过对比输入的儿童语音与一小组成人语音模板，采用大型自监督学习（SSL）模型提取中间层特征。对特征进行离散化与模板重心平均，并通过孤立词少样本分类进行评估。

Result: 理想实验条件下对成人语音评估效果较好，但儿童语音输入的评估性能显著下降，即使采用儿童模板也无法弥补。此外，SSL特征在少样本条件下对儿童语音的区分能力有限。

Conclusion: 尽管SSL特征在低资源语音任务中表现优秀，但在少样本儿童语音评估系统中存在局限，未来需针对儿童数据开发更合适的特征及方法。

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [126] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出了一种结合了token级和短语级融合的新型多粒度融合方法，有效提升了端到端语音识别系统对关键词（尤其是专有名词和用户实体）的识别性能。该方法在中英双语数据集上取得了关键词相关指标的最新最优结果，同时保持了对非关键词文本的高准确率。


<details>
  <summary>Details</summary>
Motivation: 端到端语音识别（ASR）模型在一般语音转录表现优异，但在识别特定关键词（如专有名词、用户相关实体）时表现不佳。以往尝试利用关键词字典进行改进，但token级和短语级融合各有优缺点，且难以兼顾。本文旨在弥补两者的不足，提出一种融合策略提升关键词识别。

Method: 本方法采用多粒度融合机制，将token级（逐字生成指导）和短语级（整词、整短语直接复制）融合分别引入解码过程中，并结合大语言模型（LLM）丰富的上下文知识。在解码后期采用late-fusion策略，将ASR的声学信息与LLM的上下文信息有效结合，实现粒度间的平衡。

Result: 在中英文数据集上的实验显示，本方法在与关键词相关的重要指标上取得了最优状态，同时并未损害对普通文本的识别准确率。消融实验表明，token级和短语级融合组件均为性能提升做出了重要贡献，互为补充。

Conclusion: 所提出的多粒度融合机制在提升关键词识别能力的同时，兼顾整体识别准确度，显示出较强的通用性和有效性。相关代码和模型将开源，有利于社区进一步研究与应用。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [127] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: 本文提出了一种量化翻译腔（translationese）的新指标——T-index，并通过对比微调的语言模型（LM）进行计算，验证了其在不同领域数据与人工评价中的有效性和泛化能力。T-index结果稳健且高效，可作为机器翻译质量评估的新补充指标。


<details>
  <summary>Details</summary>
Motivation: 以往缺乏精准可量化且通用的“翻译腔”（translationese）测量方法，现有的机器翻译质量评估指标如BLEU和COMET也难以直接反映翻译腔的程度，因此亟需一个新的指标来量化、检测翻译腔，并与现有评估指标互补。

Method: 提出T-index，通过两个对比微调的语言模型的似然比，来量化翻译腔指标。分别利用合成数据集和真实翻译数据验证了T-index的有效性，并用人工标注作为对照。

Result: T-index能够稳定高效地捕捉到翻译腔特性，即便仅用1-5k合成数据对0.5B语言模型微调亦可实现。T-index的相对差异能够很好地预测人工对翻译腔的标注，绝对值也与人工评价的相关性较高（Pearson相关系数0.568）。与BLEU、COMET等已有质量评估指标相关性低。

Conclusion: T-index是一个有效且高效的翻译腔量化指标，能够补充现有的机器翻译质量评估体系，具备实际应用潜力。

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [128] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM代理、代码执行和医疗术语工具的端到端框架Infherno，用于将临床自由文本自动转化为结构化的FHIR标准资源。


<details>
  <summary>Details</summary>
Motivation: 现有临床自由文本到FHIR标准资源的自动化转换方法（如规则系统或LLMs）通用性差、结构不一致，不能很好满足临床数据整合和互操作的需求。

Method: 提出一种通过大语言模型代理、可执行代码组件和医疗术语库联合实现的端到端框架Infherno，自动将无结构临床文本转化为FHIR格式。Infherno能处理自定义与合成数据，兼容本地及专有模型。

Result: Infherno可生成符合FHIR文档结构的资源，其效果接近人工转化基线，且提升了结构规范性和通用性。

Conclusion: Infherno有助于改善医疗数据互操作，实现高质量、通用型的临床数据结构化转换，促进机构间的数据集成。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [129] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 本文针对文本异常检测领域，提出并公开了一个基于多预训练语言模型嵌入、多领域数据集的标准化基准，并系统性评测现有方法。实验发现：深度模型在基于LLM嵌入情况下并不优于传统浅层算法。


<details>
  <summary>Details</summary>
Motivation: 虽然文本异常检测关系到反欺诈、谣言识别等多个实际应用，但此前缺乏标准化、全面的评测基准，导致各种检测方法缺乏严格对比和持续创新。

Method: 该研究利用GloVe、BERT、LLaMa-2/3、Mistral、OpenAI等多种大型语言模型的嵌入，结合KNN、Isolation Forest等浅层算法，与各类文本数据集（新闻、社交媒体、学术论文）进行系统性实验，对比AUROC、AUPRC等多项评价指标。

Result: 实验揭示：嵌入质量强烈影响异常检测效果，基于LLM的深度学习模型对提升检测性能并无显著优势，且跨模型表现具有低秩特征，可高效评估和选型。

Conclusion: 工作公开了基准工具包和相关资源，为后续鲁棒可扩展的文本异常检测研究搭建基础平台，有助于领域标准化和方法创新。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [130] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLMs）在硬件描述语言（HDL，主要是VHDL）代码生成与总结任务上的表现进行评估，并提出了显著提升效果的新方法。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs已广泛应用于代码相关任务，但专注于VHDL等HDL领域的研究和模型优化极为稀少，而这对于电子设计自动化（EDA）非常关键。

Method: 本文使用两个数据集（VHDL-Eval和自建VHDL-Xform）系统性评估了现有代码LLM在VHDL代码生成与总结上的表现，并提出了Chain-of-Descriptions（CoDes）方法，通过生成一系列中间描述步骤并结合输入，为LLM提供信息，从而改善输出质量。

Result: 现有LLM在VHDL任务上表现不佳，而CoDes方法在两个数据集上，通过多项指标表现出明显的效果提升，优于标准提示方法。

Conclusion: CoDes可以大幅提升LLM在VHDL代码生成和总结任务上的准确性和实用性，为未来优化代码LLM在HDL领域应用提供有效框架和研究基础。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [131] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: 本研究发现，在阿尔茨海默病（AD）语音感知中存在显著的性别偏见，尤其在中文语音中更为突出。男性语音更容易被误判为AD。


<details>
  <summary>Details</summary>
Motivation: 性别偏见在普通语音感知任务中已被广泛观察，但在阿尔茨海默病语音感知中的体现尚不明确。研究旨在揭示性别因素是否影响AD语音感知，并为后续AD检测模型的公平性提供依据。

Method: 安排16名中国听众对中文和希腊文语音样本进行AD感知实验，并结合声学特征分析（如shimmer值和语音比例）探讨影响因素。

Result: 实验发现男性语音更容易被识别为AD，尤其是在中文语音中。声学分析显示男性语音的shimmer值与AD感知密切相关，而语音比例与AD识别呈负相关。语言本身对AD的感知影响不显著。

Conclusion: 研究强调应关注语音感知中的性别偏见，建议在开发AD检测模型时纳入性别考量，并呼吁在多语言环境中进一步验证检测模型的表现。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [132] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: 提出并评估了一种多智能体辩论框架，有效提升了多种大语言模型在处理歧义问题时的表现。


<details>
  <summary>Details</summary>
Motivation: LLM虽能理解和生成自然语言，但在处理用户请求歧义时表现有限，因此需要有效的机制增强其检测和解决歧义的能力。

Method: 构建了一个多智能体辩论系统，集合Llama3-8B、Gemma2-9B和Mistral-7B三种模型，在多样化含歧义的数据集上进行实验和评价，通过辩论促进模型间共识和准确判断。

Result: 辩论框架显著提升了Llama3-8B与Mistral-7B的性能，Mistral-7B主导的辩论组成功率高达76.7%，表现出处理复杂歧义和高效达成共识的优势；不同模型对协作策略的响应也存在差异。

Conclusion: 结构化的多智能体辩论是提升LLM歧义解析能力的有效方法，为构建更强健、适应性更强的自然语言理解系统提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [133] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: 本论文探讨了具备网页浏览能力的大语言模型（LLM）能否通过分析社交媒体用户名来推断用户人口属性，并评估其准确性和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型加入网页浏览功能，具备即时获取和分析网络新信息的能力，而现有研究尚未系统探索其直接获取和解析社交媒体数据，尤其是通过用户名推断人口信息的能力与风险。

Method: 作者构建了一个包含48个合成X（Twitter）账号和1,384个国际参与者的调查数据集，并让具备网页浏览功能的LLM访问真实社交媒体内容，推断用户的人口属性。随后分析了模型解析社交媒体资料的方式及其潜在偏见。

Result: 结果显示，LLMs能够访问社交媒体内容，并在一定准确率上预测用户人口属性；但发现对于活跃度低的账号，模型可能引入性别和政治倾向等偏见。

Conclusion: 这种能力对后API时代的计算社会科学研究有积极意义，但也存在被用于信息操控和广告定向等风险。作者建议对公共应用中该能力进行限制，仅允许受控研究访问。

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [134] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 作者发现，通过对语言模型内部激活进行探测，可以有效检测加法任务中的算术错误，并利用这一能力提高模型自动纠错的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在执行算术任务如加法运算时仍会犯错，能够在模型输出前就提前检测到错误将有助于模型的自我纠正，提高实际应用的可靠性。

Method: 研究首先在3位数加法的受控环境下，利用简单探针器(probes)从模型的隐藏状态中解码模型预测与正确答案。随后训练轻量化的错误检测器来识别模型输出正确与否，并将这些探针推广到更复杂的GSM8K链式思维（Chain-of-Thought）推理轨迹中。同时，尝试用这些探针引导有选择性的重提示（re-prompting），纠正推理过程中的单步失误。

Result: 探针准确解码模型预测和正确答案，错误检测准确率超过90%。在复杂任务中，这些探针具有较好泛化能力，能反映出一致的内部表示结构。基于探针的重提示能提升整体任务准确率且对正确输出影响很小。

Conclusion: 仅依据内部激活状态即可预测语言模型算术输出的正确性，且简单的探针工具不仅提供轻量高效的错误检测方案，还有助于轻量化的模型自我纠错。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [135] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: 本文提出了一种面向企业数据的先进RAG（检索增强生成）框架，结合了多种检索策略和结构化数据处理，显著提升了生成结果的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 当前企业日益依赖各种结构化与半结构化专有数据用于决策，但大型语言模型（LLM）难以有效处理多样数据类型，并存在知识滞后、上下文窗口受限等问题。传统RAG方法对于结构化数据的处理依然不足，因此亟需优化框架以适应企业真实需求。

Method: 提出的框架结合了稠密向量（all-mpnet-base-v2）和BM25的混合检索方法，并利用SpaCy NER进行元数据过滤及交互式编码器重排序。采用语义分块保持文本连贯性，并对表格数据保留结构以保证信息完整。通过量化索引提升检索效率，引入人机反馈和对话记忆增强框架适应性。

Result: 在企业数据集实验中，与传统RAG相比，Precision@5提升15%，Recall@5提升13%，MRR提升16%。定性评价中Faithfulness（4.6 vs 3.0）、Completeness（4.2 vs 2.5）、Relevance（4.5 vs 3.2）均显著提升。

Conclusion: 该框架能够为企业任务提供更准确、全面和语境相关的生成结果。未来计划扩展至多模态数据和引入代理检索。源码即将开源。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [136] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 这篇论文研究了如何通过分析大语言模型（LLM）在生成思维链（CoT）过程中的中间激活，来提前预测最终回答是否存在不安全内容。


<details>
  <summary>Details</summary>
Motivation: 开放权重的大语言模型在回答问题前会生成思维链（CoT），提高了推理能力，但也可能导致有害内容出现在思维链和最终输出中。如何有效预测并监控这种潜在的输出失衡，提升安全性，是当前对LLM应用的关键挑战。

Method: 作者比较了多种监控方法，包括人工审核、高级大语言模型、文本分类器，以及基于思维链文本或模型激活的检测。核心方法是训练一个简单的线性探针，利用模型在CoT阶段的中间激活预测最终输出的安全性。

Result: 结果表明，基于CoT激活的线性探针在预测输出安全性上明显优于所有基于文本的方法。不仅如此，该方法还能在推理尚未完成时就做出准确预测，并且对不同模型规模、家族和安全基准具有普适性。

Conclusion: 通过在CoT中间激活上训练轻量级探针，可实现对LLM生成内容的实时安全监控和早期干预。该方法比传统基于文本的监控更加可靠，提升了安全性，有望在实际大型模型部署中发挥重要作用。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [137] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM)，通过使用单位球面上的先验分布和Spherical Sliced-Wasserstein距离，有效缓解了VAE型主题模型中的posterior collapse问题，并提升了主题建模质量和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前变分自编码器（VAE）结构的神经主题模型（NTMs）采用von Mises-Fisher先验以捕捉高维文本数据的方向性相似性，但常常由于KL散度项趋近于零而陷入posterior collapse，使得潜在表征效果不佳。如何在保持球面对称结构的同时缓解posterior collapse，是提升模型表现的关键。

Method: 本文提出了S2WTM模型，在潜在空间采用支撑在单位超球面上的先验分布，并用Spherical Sliced-Wasserstein距离来对齐聚合的后验分布与先验分布。与传统VAE的KL散度项不同，这种方法能够更有效地防止posterior collapse，同时保持对球面对称结构的建模能力。

Result: 实验结果显示，S2WTM模型相比现有的主流主题模型表现更优，能够生成更具一致性和多样性的主题，并在多个下游任务中取得了更好的成绩。

Conclusion: S2WTM通过结合Sliced Wasserstein距离和球面潜在结构，成功缓解了VAE-NTM中的posterior collapse，显著提升了主题建模表现及其实用性。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [138] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 本文提出了一种直接针对评测基准优化的预训练数据选择方法（BETR），通过与基准任务样本的相似性筛选训练数据，在多个任务和不同模型规模上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择方法往往隐含地以评测基准为目标，但缺乏明确的、系统化的基准对齐策略。作者旨在揭示显式对齐对模型性能影响，并优化数据选择过程。

Method: 作者提出了BETR方法：首先，将基准训练样本和预训练文档样本嵌入到同一语义空间；然后根据与基准的相似性为样本打分；最后训练轻量级分类器对整个语料库进行预测性筛选。通过大规模实验比较不同数据选择方法。

Result: BETR方法在500多个模型、10个任务上的实验中，相比强基线DCLM有2.1倍算力效率提升（与未筛选数据相比为4.7倍），9/10任务性能提升，并在外部任务上也优于基线。还发现：更大模型所需的数据筛选强度更低。

Conclusion: 直接将预训练数据与评测目标进行对齐能够精确塑造模型能力，并且随着模型规模增大，数据筛选策略需相应调整以获得最佳效果。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways](https://arxiv.org/abs/2507.11621)
*Tianyi Wang,Yangyang Wang,Jie Pan,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 该论文提出了一种分层协同匝道汇入控制（HCOMC）框架，专门用于异构交通流（包含人工驾驶车辆和联网自动驾驶车辆）的两车道高速公路合流问题。通过数值模拟，验证了该方法在安全性、效率和能耗等方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有高速公路匝道汇入处易发生拥堵和事故，是交通瓶颈。由于联网自动驾驶车辆（CAVs）尚未普及，亟需设计能兼容人工驾驶车辆（HDVs）与CAVs的汇入控制方法，提升汇入区安全与通畅性。

Method: 论文扩展了纵向智能驾驶人模型和横向五次多项式变道模型，以适应HDVs和CAVs，充分考虑了人因和自适应巡航控制。提出的HCOMC包括：1）基于修正虚拟车辆模型的分层协同规划模型；2）基于博弈论的自主变道模型；3）利用精英非支配排序遗传算法的多目标优化模型。

Result: 通过在不同交通密度和CAV渗透率条件下的仿真实验，表明HCOMC显著提升了汇入安全性、队列车流的稳定性与效率、优化了交通效率，并降低了能耗。

Conclusion: HCOMC框架在提升汇入区安全性、通行效率和燃油经济性等方面具有全面且明显的优势，是解决现阶段混合交通流道路汇入问题的有效方法。

Abstract: Highway on-ramp merging areas are common bottlenecks to traffic congestion
and accidents. Currently, a cooperative control strategy based on connected and
automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs
are not fully widespread, it is necessary to propose a hierarchical cooperative
on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on
two-lane highways to address this gap. This paper extends longitudinal
car-following models based on the intelligent driver model and lateral
lane-changing models using the quintic polynomial curve to account for
human-driven vehicles (HDVs) and CAVs, comprehensively considering human
factors and cooperative adaptive cruise control. Besides, this paper proposes a
HCOMC framework, consisting of a hierarchical cooperative planning model based
on the modified virtual vehicle model, a discretionary lane-changing model
based on game theory, and a multi-objective optimization model using the
elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and
efficient merging process. Then, the performance of our HCOMC is analyzed under
different traffic densities and CAV penetration rates through simulation. The
findings underscore our HCOMC's pronounced comprehensive advantages in
enhancing the safety of group vehicles, stabilizing and expediting merging
process, optimizing traffic efficiency, and economizing fuel consumption
compared with benchmarks.

</details>


### [140] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Main category: cs.RO

TL;DR: 本文为机器人技术如何应对气候变化制定了研究路线图，指出了机器人学与气候领域专家合作的高影响力机会，并具体列举了能源、建筑、交通、农业等多个应用场景。强调机器人学除硬件外，还可通过算法工具包赋能气候问题研究，旨在激发机器人学界参与气候变化应对。


<details>
  <summary>Details</summary>
Motivation: 气候变化是21世纪人类面临的重大挑战，机器人领域亟需找到能够做出贡献的切入点。

Method: 通过梳理气候相关领域（如能源、环境、运输、建筑、农业等）的关键问题，与各领域专家合作，提炼出机器人学可深度参与的研究方向，包括但不限于物理机器人应用及算法工具。

Result: 明确了机器人在能源优化、建筑改造、自动驾驶、环境监测、精准农业等方面的具体应用机会；还提出了算法层面的应用前景，制定了学科交叉、可执行的研究路线。

Conclusion: 机器人学不仅能通过物理硬件，还能通过算法、感知与规划等技术手段，为气候领域带来变革。呼吁机器人学者积极投身气候议题，与相关领域合作，共同应对全球气候变化。

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [141] [CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment](https://arxiv.org/abs/2507.11716)
*Yifan Xu,Qianwei Wang,Jordan Lillie,Vineet Kamat,Carol Menassa,Clive D'Souza*

Main category: cs.RO

TL;DR: 本文提出了一种基于ROS的智能轮椅CoNav Chair，具有共享控制的导航与避障功能，在实验中相比手动和全自主模式表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前智能轮椅多为手动控制或完全自主，灵活性有限，在狭小空间内难以导航，且对用户信任和效率有影响。需要一种兼具效率与用户体验的新模式。

Method: 设计了一种新型轮椅CoNav Chair，结合共享控制导航和避障算法。实验对比了手动、共享和自主三种导航模式，招募21名健康参与者在室内环境中完成任务，评估各项性能指标。

Result: 共享控制模式下，碰撞次数显著减少，在任务完成时间、轨迹长度与平滑性等方面与其他两种模式相当甚至更优。用户主观评价也认为其更安全、效率高。

Conclusion: CoNav Chair系统在安全性和性能方面表现良好，适合后续在残障人士中进一步测试和应用。

Abstract: As the global population of people with disabilities (PWD) continues to grow,
so will the need for mobility solutions that promote independent living and
social integration. Wheelchairs are vital for the mobility of PWD in both
indoor and outdoor environments. The current SOTA in powered wheelchairs is
based on either manually controlled or fully autonomous modes of operation,
offering limited flexibility and often proving difficult to navigate in
spatially constrained environments. Moreover, research on robotic wheelchairs
has focused predominantly on complete autonomy or improved manual control;
approaches that can compromise efficiency and user trust. To overcome these
challenges, this paper introduces the CoNav Chair, a smart wheelchair based on
the Robot Operating System (ROS) and featuring shared control navigation and
obstacle avoidance capabilities that are intended to enhance navigational
efficiency, safety, and ease of use for the user. The paper outlines the CoNav
Chair's design and presents a preliminary usability evaluation comparing three
distinct navigation modes, namely, manual, shared, and fully autonomous,
conducted with 21 healthy, unimpaired participants traversing an indoor
building environment. Study findings indicated that the shared control
navigation framework had significantly fewer collisions and performed
comparably, if not superior to the autonomous and manual modes, on task
completion time, trajectory length, and smoothness; and was perceived as being
safer and more efficient based on user reported subjective assessments of
usability. Overall, the CoNav system demonstrated acceptable safety and
performance, laying the foundation for subsequent usability testing with end
users, namely, PWDs who rely on a powered wheelchair for mobility.

</details>


### [142] [Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies](https://arxiv.org/abs/2507.11770)
*Giang Nguyen,Mihai Pomarlan,Sascha Jongebloed,Nils Leusmann,Minh Nhat Vu,Michael Beetz*

Main category: cs.RO

TL;DR: 本文提出了一种将多种场景描述格式（如MJCF、URDF、SDF）统一到USD格式的场景图模型，并结合机器人本体和知识图谱，实现了环境数据到可操作知识的转换，提升了机器人认知控制能力。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中，环境数据格式多样且不兼容，导致数据整合和利用效率低，影响复杂环境下机器人的决策与控制能力。

Method: 提出统一场景图模型，将不同的3D场景格式转换为USD格式；通过语义标注并转化为知识图谱，结合机器人本体进行语义推理；开发了基于网页的可视化工具，辅助语义映射和环境管理。

Result: 成功将程序生成的3D环境转换为具备语义标注的USD格式，并能高效地转化为知识图谱，以回答能力相关问题，验证了该方法对实时机器人决策的有效性。

Conclusion: 通过统一场景描述和知识图谱管理，增强了机器人的环境感知和智能决策能力，为复杂环境下认知型机器人控制提供了新方案。

Abstract: In robotics, the effective integration of environmental data into actionable
knowledge remains a significant challenge due to the variety and
incompatibility of data formats commonly used in scene descriptions, such as
MJCF, URDF, and SDF. This paper presents a novel approach that addresses these
challenges by developing a unified scene graph model that standardizes these
varied formats into the Universal Scene Description (USD) format. This
standardization facilitates the integration of these scene graphs with robot
ontologies through semantic reporting, enabling the translation of complex
environmental data into actionable knowledge essential for cognitive robotic
control. We evaluated our approach by converting procedural 3D environments
into USD format, which is then annotated semantically and translated into a
knowledge graph to effectively answer competency questions, demonstrating its
utility for real-time robotic decision-making. Additionally, we developed a
web-based visualization tool to support the semantic mapping process, providing
users with an intuitive interface to manage the 3D environment.

</details>


### [143] [The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey](https://arxiv.org/abs/2507.11840)
*Gaofeng Li,Ruize Wang,Peisen Xu,Qi Ye,Jiming Chen*

Main category: cs.RO

TL;DR: 本文综述了机器人灵巧操作的发展历程，特别关注于当前具身智能下的多指灵巧手操作，并分析了数据采集与技能学习的最新进展及三大挑战。


<details>
  <summary>Details</summary>
Motivation: 实现类人灵巧的机器人操作是机器人学中的核心目标与难点。随着人工智能的发展，推动了机器人操作研究的飞跃，但仍面临诸多技术和方法上的挑战。

Method: 文章通过文献调研的方式，梳理了从机械编程到具身智能的发展脉络，聚焦于灵巧操作的数据采集方法（仿真、人类演示与远程操作）和技能学习框架（模仿学习与强化学习），并系统总结当前存在的关键难题。

Result: 总结了当前具身灵巧操作所涉及的数据采集与学习方法的主要进展，梳理了三项限制其进一步发展的核心挑战。

Conclusion: 实现高水平的类人机器人灵巧操作仍面临重大障碍，未来需重点突破数据采集规模化、多模态技能学习与现实应用间的迁移难题。

Abstract: Achieving human-like dexterous robotic manipulation remains a central goal
and a pivotal challenge in robotics. The development of Artificial Intelligence
(AI) has allowed rapid progress in robotic manipulation. This survey summarizes
the evolution of robotic manipulation from mechanical programming to embodied
intelligence, alongside the transition from simple grippers to multi-fingered
dexterous hands, outlining key characteristics and main challenges. Focusing on
the current stage of embodied dexterous manipulation, we highlight recent
advances in two critical areas: dexterous manipulation data collection (via
simulation, human demonstrations, and teleoperation) and skill-learning
frameworks (imitation and reinforcement learning). Then, based on the overview
of the existing data collection paradigm and learning framework, three key
challenges restricting the development of dexterous robotic manipulation are
summarized and discussed.

</details>


### [144] [Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers](https://arxiv.org/abs/2507.11852)
*Mohammed Hassanin,Mohammad Abu Alsheikh,Carlos C. N. Kuhn,Damith Herath,Dinh Thai Hoang,Ibrahim Radwan*

Main category: cs.RO

TL;DR: 本文综述了微型出行工具（如电动滑板车、电动自行车）自动驾驶技术的最新进展，剖析其感知、规划与控制三大核心模块，并指出当前自动骑行系统（AR）与传统自动驾驶（AD）的差距及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着小型两轮车（例如电动滑板车、电动自行车）在城市中的快速普及，对可靠的自动骑行技术的需求日益迫切。然而，两轮车辆的物理不稳定、体积与动力受限，以及复杂的环境特性，使自动骑行比传统汽车自动驾驶面临更多挑战，尤其是安全问题日益突出。

Method: 本文从自动驾驶（AD）技术的观点系统梳理和分析了自动骑行系统（AR）的感知、规划、控制等核心模块，归纳当前AR领域的研究现状与不足，并深入评估了感知系统、产业与政府支持和学术研究关注的短板。

Result: 作者识别出AR领域存在感知系统覆盖不全、行业与政府支持不足、研究关注度低等关键问题，并指出多模态传感、轻量化感知设备与边缘深度学习架构等为未来主要技术发展方向。

Conclusion: 本文呼吁借鉴自动驾驶领域的成熟经验，结合自动骑行的特殊需求，加强学界与业界合作，推动更安全、高效且可扩展的城市微型出行自动化系统的发展。

Abstract: The rapid adoption of micromobility solutions, particularly two-wheeled
vehicles like e-scooters and e-bikes, has created an urgent need for reliable
autonomous riding (AR) technologies. While autonomous driving (AD) systems have
matured significantly, AR presents unique challenges due to the inherent
instability of two-wheeled platforms, limited size, limited power, and
unpredictable environments, which pose very serious concerns about road users'
safety. This review provides a comprehensive analysis of AR systems by
systematically examining their core components, perception, planning, and
control, through the lens of AD technologies. We identify critical gaps in
current AR research, including a lack of comprehensive perception systems for
various AR tasks, limited industry and government support for such
developments, and insufficient attention from the research community. The
review analyses the gaps of AR from the perspective of AD to highlight
promising research directions, such as multimodal sensor techniques for
lightweight platforms and edge deep learning architectures. By synthesising
insights from AD research with the specific requirements of AR, this review
aims to accelerate the development of safe, efficient, and scalable autonomous
riding systems for future urban mobility.

</details>


### [145] [A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications](https://arxiv.org/abs/2507.11880)
*Jinyuan Liu,Minglei Fu,Ling Shi,Chenguang Yang,Wenan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种针对牵引机器人动力学约束的新型路径规划算法CDT-TCS，并在多种实际及仿真环境中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 牵引机器人在灾害救援、地下探索等特殊领域由于电源和通信稳定而有独特优势，但其路径规划受到牵引绳长度和纠缠风险的约束，急需高效且可行的规划算法。

Method: 提出基于凸剖分拓扑（CDT）编码作为同伦不变量来表示路径的拓扑状态。将代数拓扑与几何优化结合，设计了CDT-TCS算法，实现通过一次计算获得任意二维环境、任意位置的牵引机器人全局最优可行状态。同时提出针对不同应用的三种派生算法：CDT-TPP用于最优牵引路径规划，CDT-TMV用于多目标访问，CDT-UTPP用于无牵引机器人最优距离路径规划。所有理论和算法都进行了严格证明和详细讨论。

Result: 算法在大量仿真中较现有方法获得了显著性能提升，并通过机器人实机实验验证了其实用性和工程价值。

Conclusion: CDT-TCS及其衍生算法突破了牵引机器人运动规划的约束瓶颈，理论成熟、性能突出，有望在实际机器人系统中广泛应用。

Abstract: Tethered robots play a pivotal role in specialized environments such as
disaster response and underground exploration, where their stable power supply
and reliable communication offer unparalleled advantages. However, their motion
planning is severely constrained by tether length limitations and entanglement
risks, posing significant challenges to achieving optimal path planning. To
address these challenges, this study introduces CDT-TCS (Convex Dissection
Topology-based Tethered Configuration Search), a novel algorithm that leverages
CDT Encoding as a homotopy invariant to represent topological states of paths.
By integrating algebraic topology with geometric optimization, CDT-TCS
efficiently computes the complete set of optimal feasible configurations for
tethered robots at all positions in 2D environments through a single
computation. Building on this foundation, we further propose three
application-specific algorithms: i) CDT-TPP for optimal tethered path planning,
ii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for
distance-optimal path planning of untethered robots. All theoretical results
and propositions underlying these algorithms are rigorously proven and
thoroughly discussed in this paper. Extensive simulations demonstrate that the
proposed algorithms significantly outperform state-of-the-art methods in their
respective problem domains. Furthermore, real-world experiments on robotic
platforms validate the practicality and engineering value of the proposed
framework.

</details>


### [146] [NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy](https://arxiv.org/abs/2507.11889)
*Adnan Abdullah,Alankrit Gupta,Vaishnav Ramesh,Shivali Patel,Md Jahidul Islam*

Main category: cs.RO

TL;DR: 本文提出了一种名为NemeSys的新型自主水下航行器（AUV）系统，能够通过浮标实现低带宽下的实时任务重构和在线适应。论文提供了系统架构、任务语义编码框架，并通过建模、实验和海试验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的AUV系统在GPS不可用、通信受限的海洋环境下适应性较差，任务一般难以在中途根据新情况更改，严重限制了其灵活性和自动化水平。因此迫切需要一种支持任务动态重构和参数自适应的AUV解决方案。

Method: 作者提出了NemeSys系统，通过浮标结合紧凑的光学和磁电（OME）信号为AUV提供低带宽、实时的任务重构能力。包括具体的系统设计、控制架构和语义化任务编码框架，并用理论建模、实验室测试和实地海试进行了全面验证。

Result: 该系统成功实现了AUV在水下有限通信条件下的在线任务适配和语义化任务更新。实验表明NemeSys具备在线适应能力，能够在动态和不确定环境中实现目标驱动的高度自主运行。

Conclusion: NemeSys推动了AUV系统向实时自适应和任务灵活变更方向的发展，为复杂和变化的水下应用场景下的智能自治提供了新的解决方案。

Abstract: Adaptive mission control and dynamic parameter reconfiguration are essential
for autonomous underwater vehicles (AUVs) operating in GPS-denied,
communication-limited marine environments. However, most current AUV platforms
execute static, pre-programmed missions or rely on tethered connections and
high-latency acoustic channels for mid-mission updates, significantly limiting
their adaptability and responsiveness. In this paper, we introduce NemeSys, a
novel AUV system designed to support real-time mission reconfiguration through
compact optical and magnetoelectric (OME) signaling facilitated by floating
buoys. We present the full system design, control architecture, and a semantic
mission encoding framework that enables interactive exploration and task
adaptation via low-bandwidth communication. The proposed system is validated
through analytical modeling, controlled experimental evaluations, and
open-water trials. Results confirm the feasibility of online mission adaptation
and semantic task updates, highlighting NemeSys as an online AUV platform for
goal-driven adaptive autonomy in dynamic and uncertain underwater environments.

</details>


### [147] [Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments](https://arxiv.org/abs/2507.11920)
*Jeongyong Yang,KwangBin Lee,SooJean Han*

Main category: cs.RO

TL;DR: 提出了一种高效且灵活的新路径规划框架（HyPRAP），能在动态密集且不确定的环境下实现更安全和高效的实时路径规划。


<details>
  <summary>Details</summary>
Motivation: 在密集及高度不确定的动态环境中进行实时路径规划是一项困难任务，主要因为需要对大量动态障碍物未来运动进行预测，既费时又计算复杂。作者希望提升路径规划在复杂环境下的安全性与计算效率。

Method: 提出Hybrid Prediction-based Risk-Aware Planning (HyPRAP)框架，通过混合多种障碍物预测模型（高精度与低开销选择性结合），利用新的预测性碰撞风险指数（P-CRI）动态评估障碍物风险。同时, 结合混合保形预测实现不确定度量，动态确定可信区间。针对高风险障碍用高精度模型，低风险障碍则简化预测或忽略，从而降低总体预测算力压力。

Result: 理论分析证明HyPRAP可以有效在安全性与计算效率间取得平衡。大量仿真测试显示，HyPRAP优于单一预测方法，而P-CRI优于简单基于距离的风险评估。

Conclusion: HyPRAP 能在动态、拥挤与不确定性环境下实现更精确、更高效、更安全的路径规划，在处理大规模障碍物的环境中尤其具有优势。

Abstract: Real-time path planning in dense, uncertain environments remains a
challenging problem, as predicting the future motions of numerous dynamic
obstacles is computationally burdensome and unrealistic. To address this, we
introduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a
prediction-based risk-aware path-planning framework which uses a hybrid
combination of models to predict local obstacle movement. HyPRAP uses a novel
Prediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by
each obstacle, enabling the selective use of predictors based on whether the
agent prioritizes high predictive accuracy or low computational prediction
overhead. This selective routing enables the agent to focus on high-risk
obstacles while ignoring or simplifying low-risk ones, making it suitable for
environments with a large number of obstacles. Moreover, HyPRAP incorporates
uncertainty quantification through hybrid conformal prediction by deriving
confidence bounds simultaneously achieved by multiple predictions across
different models. Theoretical analysis demonstrates that HyPRAP effectively
balances safety and computational efficiency by leveraging the diversity of
prediction models. Extensive simulations validate these insights for more
general settings, confirming that HyPRAP performs better compared to single
predictor methods, and P-CRI performs better over naive proximity-based risk
assessment.

</details>


### [148] [A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning](https://arxiv.org/abs/2507.11938)
*Hao Chen,Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本论文提出了一种基于相似性匹配的新方法，用于单视角下对未知物体的抓取，通过多层次特征匹配和优化，实现了更健壮的机器人抓取。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的方法在面对传感噪声和环境变化时表现出较差的鲁棒性，限制了其泛化能力。为此，作者希望探索无需大量学习的更具通用性的抓取方法。

Method: 核心方法包括三步：1）利用目标物体的视觉特征，在数据库中进行相似性匹配，筛选出与目标最相似的已知物体模型；2）借助这类候选模型的已有抓取知识，规划对未知目标的模仿式抓取策略；3）通过局部优化提升抓取质量。为提高匹配准确率，提出多层次（语义、几何、尺寸）特征融合的匹配框架，并设计了新的C-FPFH点云几何描述符。此外，还结合了大语言模型，半定向包围盒，以及基于平面检测的点云配准方法。

Result: 实验验证所提方法在单视角未知物体抓取任务中表现出较强的鲁棒性和优异的抓取成功率，相较于传统学习方法对于噪声和变化具有更好的适应能力。

Conclusion: 基于相似性匹配的新方法可有效提升机器人对未知物体的抓取性能，尤其在实际场景中的鲁棒性和泛化性方面有显著优势，突破了传统学习方法的局限。

Abstract: Grasping unknown objects from a single view has remained a challenging topic
in robotics due to the uncertainty of partial observation. Recent advances in
large-scale models have led to benchmark solutions such as GraspNet-1Billion.
However, such learning-based approaches still face a critical limitation in
performance robustness for their sensitivity to sensing noise and environmental
changes. To address this bottleneck in achieving highly generalized grasping,
we abandon the traditional learning framework and introduce a new perspective:
similarity matching, where similar known objects are utilized to guide the
grasping of unknown target objects. We newly propose a method that robustly
achieves unknown-object grasping from a single viewpoint through three key
steps: 1) Leverage the visual features of the observed object to perform
similarity matching with an existing database containing various object models,
identifying potential candidates with high similarity; 2) Use the candidate
models with pre-existing grasping knowledge to plan imitative grasps for the
unknown target object; 3) Optimize the grasp quality through a local
fine-tuning process. To address the uncertainty caused by partial and noisy
observation, we propose a multi-level similarity matching framework that
integrates semantic, geometric, and dimensional features for comprehensive
evaluation. Especially, we introduce a novel point cloud geometric descriptor,
the C-FPFH descriptor, which facilitates accurate similarity assessment between
partial point clouds of observed objects and complete point clouds of database
models. In addition, we incorporate the use of large language models, introduce
the semi-oriented bounding box, and develop a novel point cloud registration
approach based on plane detection to enhance matching accuracy under
single-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.

</details>


### [149] [IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving](https://arxiv.org/abs/2507.11940)
*Kanghyun Ryu,Minjun Sung,Piyush Gupta,Jovin D'sa,Faizan M. Tariq,David Isele,Sangjae Bae*

Main category: cs.RO

TL;DR: 本文提出了一种名为IANN-MPPI的交互感知神经网络增强型模型预测路径积分控制方法，用于提升自动驾驶车辆在密集交通中的运动规划效率，通过预测周围交通参与者对AV动作的反应，实现更高效的合流并避免过度保守。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶在密集交通中的规划行为往往保守且无法充分达成目标，主要因为难以预测和应对周围多车的交互行为，传统方法忽略了其他车辆对自动驾驶车辆行为的反应。为提升运动规划效果，需要方法能主动感知和预判这类交互。

Method: 提出了一种交互感知神经网络增强的MPPI控制（IANN-MPPI），即通过神经网络预测交通参与者对不同控制序列的反应，进而指导AV的轨迹规划。同时引入了样条约束以提升车道转换能力，实现更高效的采样和计划。

Result: 在密集交通合流场景中，IANN-MPPI表现出更高效的合流能力，能更好地预测和利用周围车辆的行为反应，提高运动规划目的的实现率。

Conclusion: IANN-MPPI实现了运动规划与交互性行为预测的有机结合，在密集交通场景下能够提升自动驾驶车辆的决策能力及合流效率。

Abstract: Motion planning for autonomous vehicles (AVs) in dense traffic is
challenging, often leading to overly conservative behavior and unmet planning
objectives. This challenge stems from the AVs' limited ability to anticipate
and respond to the interactive behavior of surrounding agents. Traditional
decoupled prediction and planning pipelines rely on non-interactive predictions
that overlook the fact that agents often adapt their behavior in response to
the AV's actions. To address this, we propose Interaction-Aware Neural
Network-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which
enables interactive trajectory planning by predicting how surrounding agents
may react to each control sequence sampled by MPPI. To improve performance in
structured lane environments, we introduce a spline-based prior for the MPPI
sampling distribution, enabling efficient lane-changing behavior. We evaluate
IANN-MPPI in a dense traffic merging scenario, demonstrating its ability to
perform efficient merging maneuvers. Our project website is available at
https://sites.google.com/berkeley.edu/iann-mppi

</details>


### [150] [A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming](https://arxiv.org/abs/2507.11974)
*Waseem Akram,Muhayy Ud Din,Lyes Saad Soud,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文综述生成式人工智能（GAI）在水产养殖领域的应用及挑战，包括其在感知、优化、决策、合规等方面的作用，并探讨了未来发展趋势与局限性。


<details>
  <summary>Details</summary>
Motivation: 随着水产养殖行业向数字化、自动化方向变革，即“水产养殖4.0”时代的到来，对智能、自动、高效决策工具的需求愈发迫切。GAI具备处理多模态数据和生成智能解决方案的能力，被认为是推进行业转型的关键技术。

Method: 本文梳理了GAI在水产养殖的基础模型（如扩散模型、变换器、增强检索生成等），并系统回顾了相关实验系统、试点项目及实际案例。同时，提出了一套涵盖感知、控制、优化、通信及合规的GAI应用分类法，并分析行业内当前的主要技术、实施与合规挑战。

Result: GAI已在水下感知、数字孪生建模、遥控运营任务的自主规划等方面初步展现出重要作用。相关的试点与应用案例在环境监测、疾病诊断、基础设施规划、市场分析等领域取得积极成果。同时，GAI实际部署中也遇到数据、实时性、可解释性、信任、环境成本及监管等瓶颈。

Conclusion: GAI正逐步确立作为推动智慧和可持续水产养殖系统的核心驱动力地位，但必须关注其局限性并持续完善相关技术、数据与监管体系，以促进其广泛和安全的实际应用。

Abstract: Generative Artificial Intelligence (GAI) has rapidly emerged as a
transformative force in aquaculture, enabling intelligent synthesis of
multimodal data, including text, images, audio, and simulation outputs for
smarter, more adaptive decision-making. As the aquaculture industry shifts
toward data-driven, automation and digital integration operations under the
Aquaculture 4.0 paradigm, GAI models offer novel opportunities across
environmental monitoring, robotics, disease diagnostics, infrastructure
planning, reporting, and market analysis. This review presents the first
comprehensive synthesis of GAI applications in aquaculture, encompassing
foundational architectures (e.g., diffusion models, transformers, and retrieval
augmented generation), experimental systems, pilot deployments, and real-world
use cases. We highlight GAI's growing role in enabling underwater perception,
digital twin modeling, and autonomous planning for remotely operated vehicle
(ROV) missions. We also provide an updated application taxonomy that spans
sensing, control, optimization, communication, and regulatory compliance.
Beyond technical capabilities, we analyze key limitations, including limited
data availability, real-time performance constraints, trust and explainability,
environmental costs, and regulatory uncertainty. This review positions GAI not
merely as a tool but as a critical enabler of smart, resilient, and
environmentally aligned aquaculture systems.

</details>


### [151] [Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers](https://arxiv.org/abs/2507.11991)
*Juanran Wang,Marc R. Schlichting,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 本文通过深度生成模型提升自动驾驶车辆在路口等高风险交通区域的安全性，提出了一种快速且有效的潜在事故场景生成方法，并集成到车辆规划中显著提升了安全与效率。


<details>
  <summary>Details</summary>
Motivation: 路口等高风险区域是交通事故的主要发生地，传统驾驶模型难以预测并应对复杂异常场景。亟需一种高效的自动生成潜在事故场景的方法，辅助自动驾驶系统提升安全性。

Method: 作者基于去噪扩散概率模型（DDPM）训练了一个1000步的深度生成网络，用于针对当前路口中异常车辆的相对位置和速度，生成可能导致碰撞的传感器噪声序列。然后通过生成对抗架构，将该1000步模型蒸馏为单步去噪扩散模型，加快推理速度，并保持采样质量。最终将单步模型用于自动驾驶车辆的高效安全决策规划中。

Result: 在仿真实验中，集成单步去噪扩散模型的鲁棒规划器相比传统智能驾驶模型（IDM）控制器，显著降低了失效率和延迟率，验证了方法的有效性与效率。

Conclusion: 本文提出的基于深度生成模型的安全规划方法能够高效生成风险场景，并指导自动驾驶车辆实现更加安全、可靠的决策，在高风险交通场景下具有良好的应用前景。

Abstract: High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.

</details>


### [152] [Robust Route Planning for Sidewalk Delivery Robots](https://arxiv.org/abs/2507.12067)
*Xing Tong,Michele D. Simoni*

Main category: cs.RO

TL;DR: 本研究针对人行道配送机器人在城市运输中的不确定性旅行时间问题，提出并评估了多种稳健路径规划方法，实现对障碍物和人流等实际因素的建模。结果表明，稳健路径规划显著提升了机器人的运行可靠性，尤其是在复杂环境条件下。


<details>
  <summary>Details</summary>
Motivation: 人行道配送机器人有望缓解城市货运拥堵并提升安全性，但其在实际应用中因人流密度、障碍物及基础设施状况变化导致旅行时间不确定，影响效率，因此需要研究能够应对这些不确定性的路径规划方法。

Method: 将优化方法与仿真结合，模拟障碍物和人流对机器人旅行时间的影响，提出了三种不确定性集构建方法（预算式、椭球式、基于支持向量聚类）以及一种分布式鲁棒最短路径（DRSP）方法，并在斯德哥尔摩市中心的真实步行模式下进行案例评估。

Result: 稳健路径规划方法在可变人行道条件下的运行可靠性显著优于传统最短路径算法，其中椭球式和DRSP方法在平均和最坏延误表现上最好。稳健方法对于体积较大、速度较慢或保守导航策略的机器人效果尤为明显，在恶劣天气和高人流环境中优势更大。

Conclusion: 稳健路径规划能有效提升人行道配送机器人在多变、复杂环境下的运营效率与可靠性，特别适用于高风险和极端工况，椭球式与DRSP方法表现尤为突出，建议在未来城市配送场景中推广应用。

Abstract: Sidewalk delivery robots are a promising solution for urban freight
distribution, reducing congestion compared to trucks and providing a safer,
higher-capacity alternative to drones. However, unreliable travel times on
sidewalks due to pedestrian density, obstacles, and varying infrastructure
conditions can significantly affect their efficiency. This study addresses the
robust route planning problem for sidewalk robots, explicitly accounting for
travel time uncertainty due to varying sidewalk conditions. Optimization is
integrated with simulation to reproduce the effect of obstacles and pedestrian
flows and generate realistic travel times. The study investigates three
different approaches to derive uncertainty sets, including budgeted,
ellipsoidal, and support vector clustering (SVC)-based methods, along with a
distributionally robust method to solve the shortest path (SP) problem. A
realistic case study reproducing pedestrian patterns in Stockholm's city center
is used to evaluate the efficiency of robust routing across various robot
designs and environmental conditions. The results show that, when compared to a
conventional SP, robust routing significantly enhances operational reliability
under variable sidewalk conditions. The Ellipsoidal and DRSP approaches
outperform the other methods, yielding the most efficient paths in terms of
average and worst-case delay. Sensitivity analyses reveal that robust
approaches consistently outperform the conventional SP, particularly for
sidewalk delivery robots that are wider, slower, and have more conservative
navigation behaviors. These benefits are even more pronounced in adverse
weather conditions and high pedestrian congestion scenarios.

</details>


### [153] [Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards](https://arxiv.org/abs/2507.12093)
*David Rapado-Rincon,Gert Kootstra*

Main category: cs.RO

TL;DR: 本文提出了Tree-SLAM，一种针对果园环境下单树精准地图构建的语义SLAM方法，在GPS信号不可靠和树木外观重复环境下取得了高精度的单树地图。


<details>
  <summary>Details</summary>
Motivation: 在果园中实现精准农业和机器人自主作业需要准确的单树地图，然而密集树冠下GPS不稳定，且树木外观高度重复，造成基于传统SLAM和GPS的方法误差较大。因此迫切需要能应对此环境的新方法。

Method: 作者提出Tree-SLAM方法，利用RGB-D图像和实例分割模型检测树干，通过基于级联图的数据关联算法进行树干重识别，将树干作为地标融合到因子图中，并整合噪声GPS、里程计及观测数据，生成高精度单树地图。

Result: 在苹果和梨树果园多季节多样化数据集上验证，单树地理定位误差最低达18厘米（植树距离的20%以内），映射精度高，对低质量GPS信号环境下仍表现出较好鲁棒性。

Conclusion: Tree-SLAM结合语义分割与数据关联，有效解决了果园树干重复与GPS信号不稳问题，能生成高精度单树地图，为果园自动化和精准农业提供有力工具。

Abstract: Accurate mapping of individual trees is an important component for precision
agriculture in orchards, as it allows autonomous robots to perform tasks like
targeted operations or individual tree monitoring. However, creating these maps
is challenging because GPS signals are often unreliable under dense tree
canopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)
approaches struggle in orchards because the repetitive appearance of trees can
confuse the system, leading to mapping errors. To address this, we introduce
Tree-SLAM, a semantic SLAM approach tailored for creating maps of individual
trees in orchards. Utilizing RGB-D images, our method detects tree trunks with
an instance segmentation model, estimates their location and re-identifies them
using a cascade-graph-based data association algorithm. These re-identified
trunks serve as landmarks in a factor graph framework that integrates noisy GPS
signals, odometry, and trunk observations. The system produces maps of
individual trees with a geo-localization error as low as 18 cm, which is less
than 20\% of the planting distance. The proposed method was validated on
diverse datasets from apple and pear orchards across different seasons,
demonstrating high mapping accuracy and robustness in scenarios with unreliable
GPS signals.

</details>


### [154] [Leveraging Sidewalk Robots for Walkability-Related Analyses](https://arxiv.org/abs/2507.12148)
*Xing Tong,Michele D. Simoni,Kaj Munhoz Arfvidsson,Jonas Mårtensson*

Main category: cs.RO

TL;DR: 本论文探讨利用步道机器人自动收集人行道步行适宜性数据，克服传统方法成本高、难以大规模应用的问题。实验证明，机器人可持续监测人行道状态并反映行人行为，助力打造更宜步行的城市环境。


<details>
  <summary>Details</summary>
Motivation: 步行适宜性对可持续城市发展至关重要，但以往依赖人工收集步行环境数据，成本高且效率低，难以满足大规模城市监测需求。随着步道配送机器人普及，作者提出利用其作为移动数据平台，低成本高频率地收集人行道相关数据。

Method: 作者在斯德哥尔摩KTH校区部署传感器机器人，对900个人行道段进行101次巡视，自动收集速度、用时、人流密度、人行道宽度和表面平整度等信息，并对数据进行特征分类、轨迹分析和相关性统计。

Result: 研究表明：人行道属性显著影响步行与机器人移动轨迹，如高密度、窄道和地面不平会导致速度降低和轨迹更不稳定。机器人运行状态与行人行为高度相关，可作为间接评价行人动态的工具。

Conclusion: 基于机器人自动收集的平台可连续监测人行道与行人动态，为城市步行适宜性提升、宜居性政策制定和智能城市响应能力提供数据支撑和科学依据。

Abstract: Walkability is a key component of sustainable urban development, while
collecting detailed data on its related features remains challenging due to the
high costs and limited scalability of traditional methods. Sidewalk delivery
robots, increasingly deployed in urban environments, offer a promising solution
to these limitations. This paper explores how these robots can serve as mobile
data collection platforms, capturing sidewalk-level features related to
walkability in a scalable, automated, and real-time manner. A sensor-equipped
robot was deployed on a sidewalk network at KTH in Stockholm, completing 101
trips covering 900 segments. From the collected data, different typologies of
features are derived, including robot trip characteristics (e.g., speed,
duration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk
utilization (e.g., pedestrian density). Their walkability-related implications
were investigated with a series of analyses. The results demonstrate that
pedestrian movement patterns are strongly influenced by sidewalk
characteristics, with higher density, reduced width, and surface irregularity
associated with slower and more variable trajectories. Notably, robot speed
closely mirrors pedestrian behavior, highlighting its potential as a proxy for
assessing pedestrian dynamics. The proposed framework enables continuous
monitoring of sidewalk conditions and pedestrian behavior, contributing to the
development of more walkable, inclusive, and responsive urban environments.

</details>


### [155] [Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach](https://arxiv.org/abs/2507.12158)
*Nawshin Mannan Proma,Gricel Vázquez,Sepeedeh Shahbeigi,Arjun Badyal,Victoria Hodge*

Main category: cs.RO

TL;DR: 该论文提出了一种基于情境覆盖网格、概率建模与验证的方法来验证工业级自主车辆的安全性。通过系统测试采集概率数据，结合概率模型检测，能定量保证系统在各环境下的安全性。


<details>
  <summary>Details</summary>
Motivation: 工业自主车辆在关键安全环境下应用越来越广泛，但传统的安全验证方法难以应对复杂多变的工况，需要新的方法来识别高风险情境并量化安全保证，以满足合规和实际部署需求。

Method: 本方法首先利用情境覆盖网格，穷举自主车辆操作相关的环境配置，并结合系统测试的数据注入概率信息，建成描述正常与不安全行为转移的概率模型。随后，通过将由危害分析提取的安全属性形式化为时序逻辑，并用概率模型检测技术进行验证。

Result: 方法能够有效识别高风险情境，并给出定量的安全保证。实践结果表明，该方法有助于实现合规，并支持自主系统的稳健部署。

Conclusion: 情境驱动的概率建模与验证方法为工业自主车辆的安全性验证提供了一种系统且有效的手段，能够量化风险并满足监管标准，推动其安全可靠地应用于复杂环境。

Abstract: As industrial autonomous ground vehicles are increasingly deployed in
safety-critical environments, ensuring their safe operation under diverse
conditions is paramount. This paper presents a novel approach for their safety
verification based on systematic situation extraction, probabilistic modelling
and verification. We build upon the concept of a situation coverage grid, which
exhaustively enumerates environmental configurations relevant to the vehicle's
operation. This grid is augmented with quantitative probabilistic data
collected from situation-based system testing, capturing probabilistic
transitions between situations. We then generate a probabilistic model that
encodes the dynamics of both normal and unsafe system behaviour. Safety
properties extracted from hazard analysis and formalised in temporal logic are
verified through probabilistic model checking against this model. The results
demonstrate that our approach effectively identifies high-risk situations,
provides quantitative safety guarantees, and supports compliance with
regulatory standards, thereby contributing to the robust deployment of
autonomous systems.

</details>


### [156] [Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties](https://arxiv.org/abs/2507.12174)
*Zhenmin Huang,Yusen Xie,Benshan Ma,Shaojie Shen,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种高效可扩展的多智能体交互轨迹规划方法，通过新的博弈论模型和分布式算法，有效应对了智能体的意图不确定性，实验结果显示方法在各种场景下均优于现有方案，支持实时规划。


<details>
  <summary>Details</summary>
Motivation: 多智能体的轨迹规划因复杂交互长期存在挑战，尤其是在处理智能体意图不确定性时，传统博弈方法计算开销大、扩展性差，无法很好地实现高效实时规划。

Method: 作者将智能体间含意图不确定性的交互建模为一般贝叶斯博弈，并证明在一定假设下可等价转化为势博弈。最优交互轨迹通过统一优化求解贝叶斯纳什均衡。进一步，提出了基于分布式ADMM的算法，支持问题的并行求解，提高了方法的扩展性。

Result: 仿真和实验证明，该方法可在各种存在广泛意图不确定性的环境下实现高效轨迹规划，且扩展性显著优于现有集中式和分布式基线方法，可支持实时应用。

Conclusion: 所提出的博弈论轨迹规划框架，兼顾效率和扩展性，为多智能体在高度不确定环境下的实时轨迹规划提供了有力工具，推动了相关领域发展。

Abstract: Trajectory planning involving multi-agent interactions has been a
long-standing challenge in the field of robotics, primarily burdened by the
inherent yet intricate interactions among agents. While game-theoretic methods
are widely acknowledged for their effectiveness in managing multi-agent
interactions, significant impediments persist when it comes to accommodating
the intentional uncertainties of agents. In the context of intentional
uncertainties, the heavy computational burdens associated with existing
game-theoretic methods are induced, leading to inefficiencies and poor
scalability. In this paper, we propose a novel game-theoretic interactive
trajectory planning method to effectively address the intentional uncertainties
of agents, and it demonstrates both high efficiency and enhanced scalability.
As the underpinning basis, we model the interactions between agents under
intentional uncertainties as a general Bayesian game, and we show that its
agent-form equivalence can be represented as a potential game under certain
minor assumptions. The existence and attainability of the optimal interactive
trajectories are illustrated, as the corresponding Bayesian Nash equilibrium
can be attained by optimizing a unified optimization problem. Additionally, we
present a distributed algorithm based on the dual consensus alternating
direction method of multipliers (ADMM) tailored to the parallel solving of the
problem, thereby significantly improving the scalability. The attendant
outcomes from simulations and experiments demonstrate that the proposed method
is effective across a range of scenarios characterized by general forms of
intentional uncertainties. Its scalability surpasses that of existing
centralized and decentralized baselines, allowing for real-time interactive
trajectory planning in uncertain game settings.

</details>


### [157] [UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization](https://arxiv.org/abs/2507.12194)
*Hongming Shen,Xun Chen,Yulin Hui,Zhenyu Wu,Wei Wang,Qiyang Lyu,Tianchen Deng,Danwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种统一的激光全局定位（LGL）方法UniLGL，通过对点云的空间与材质信息编码，用多BEV融合网络实现空间、材质和传感器类型的统一，具备更强的泛化能力，并在多类平台上验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LGL方法只考虑点云的部分信息，如几何特征，或仅适用于同类激光雷达传感器，忽视了LGL应用中的统一性需求，限制了其跨场景、跨平台的应用能力。

Method: UniLGL将完整点云（含几何及材质信息）分别编码为空间和强度BEV图像，通过端到端多BEV融合网络提取统一特征。同时引入视角不变性假设，替代传统平移等变性，从而提升跨异构传感器的鲁棒性。结合2D BEV与点云的映射，不需额外配准即可实现全局位姿精确估计。

Result: UniLGL在多项真实环境基准测试中展现出优于现有方法的性能。此外已在卡车、微型无人机等不同平台上完成部署，支持高精地图定位、多无人机协同探索，适用性强。

Conclusion: UniLGL实现了LGL方法在空间、材质及传感器类型上的统一，显著提高了跨平台和实际工业场景中的定位鲁棒性与应用广度。

Abstract: Existing LGL methods typically consider only partial information (e.g.,
geometric features) from LiDAR observations or are designed for homogeneous
LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL
method is proposed, termed UniLGL, which simultaneously achieves spatial and
material uniformity, as well as sensor-type uniformity. The key idea of the
proposed method is to encode the complete point cloud, which contains both
geometric and material information, into a pair of BEV images (i.e., a spatial
BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network
is designed to extract uniform features, equipping UniLGL with spatial and
material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a
viewpoint invariance hypothesis is introduced, which replaces the conventional
translation equivariance assumption commonly used in existing LPR networks and
supervises UniLGL to achieve sensor-type uniformity in both global descriptors
and local feature representations. Finally, based on the mapping between local
features on the 2D BEV image and the point cloud, a robust global pose
estimator is derived that determines the global minimum of the global pose on
SE(3) without requiring additional registration. To validate the effectiveness
of the proposed uniform LGL, extensive benchmarks are conducted in real-world
environments, and the results show that the proposed UniLGL is demonstratively
competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL
has been deployed on diverse platforms, including full-size trucks and agile
Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping
as well as multi-MAV collaborative exploration in port and forest environments,
demonstrating the applicability of UniLGL in industrial and field scenarios.

</details>


### [158] [Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot](https://arxiv.org/abs/2507.12273)
*Luca Garello,Francesca Cocchella,Alessandra Sciutti,Manuel Catalano,Francesco Rea*

Main category: cs.RO

TL;DR: 本文介绍了一种用于博物馆的自主导览机器人Alter-Ego，结合大语言模型（LLM）互动问答和SLAM导航技术，在实际环境中进行了测试并取得良好用户反馈。


<details>
  <summary>Details</summary>
Motivation: 现有的博物馆互动方式有限，无法充分利用AI与机器人技术提升参观体验。作者旨在通过集成LLM和SLAM技术，研发可自主导航并实时与参观者互动的机器人，以提升文化空间的知识获取与可达性。

Method: 作者设计并实现了一款自主博物馆导览机器人Alter-Ego。该机器人集成了先进SLAM用于环境感知和导航，采用大语言模型对观众提问进行实时、上下文相关的对话回答，并能根据用户请求自适应路线。系统在真实博物馆环境下与34名参与者进行了测试，采用访谈和前后问卷调查相结合的方式对人机交互体验进行了定性和定量分析。

Result: 实验结果显示Alter-Ego机器人总体受访者评价良好，能有效提升博物馆参观的互动性和体验感。尽管存在理解和响应偶有不足的局限，但其在实际应用环境下表现出较好的可用性和受欢迎度。

Conclusion: 研究表明，AI驱动的机器人在提升文化空间互动体验、知识获取及无障碍方面具有显著潜力。但当前技术在真实复杂环境应用中仍面临感知和响应等挑战。该成果为今后文化场所的HRI（人机交互）与智能导览系统发展提供了有价值的经验。

Abstract: Autonomous robots are increasingly being tested into public spaces to enhance
user experiences, particularly in cultural and educational settings. This paper
presents the design, implementation, and evaluation of the autonomous museum
guide robot Alter-Ego equipped with advanced navigation and interactive
capabilities. The robot leverages state-of-the-art Large Language Models (LLMs)
to provide real-time, context aware question-and-answer (Q&A) interactions,
allowing visitors to engage in conversations about exhibits. It also employs
robust simultaneous localization and mapping (SLAM) techniques, enabling
seamless navigation through museum spaces and route adaptation based on user
requests. The system was tested in a real museum environment with 34
participants, combining qualitative analysis of visitor-robot conversations and
quantitative analysis of pre and post interaction surveys. Results showed that
the robot was generally well-received and contributed to an engaging museum
experience, despite some limitations in comprehension and responsiveness. This
study sheds light on HRI in cultural spaces, highlighting not only the
potential of AI-driven robotics to support accessibility and knowledge
acquisition, but also the current limitations and challenges of deploying such
technologies in complex, real-world environments.

</details>


### [159] [Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning](https://arxiv.org/abs/2507.12391)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文评估了当前多模态大语言模型（LLM）在机器人路径规划任务中的表现，发现它们在简单任务上有一定能力，但在复杂环境中表现有限，尤其在空间推理与扩展性方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在文本推理任务上表现优秀，但其结合视觉输入进行空间任务（如机器人路径规划）的能力尚未明晰。既有研究缺乏对多模态输入（文本+视觉）在路径规划复杂性上的系统比较，因此本研究提出基准，系统评测多模态LLM在路径规划中的实际能力及其局限。

Method: 论文设计了一个包括不同规模和复杂度的2D网格环境的基准，用以模拟简化的机器人路径规划任务。评估了15种不同的多模态LLM，通过单纯文本输入、文本+视觉输入（如图片等）等多种方式，考察它们在生成合法、最优路径时的有效性，并分析了模型规模、网格复杂度对结果的影响。

Result: 在简单、小规模网格中，多模态LLM取得了中等的成功率，视觉辅助与文本few-shot提示在特定情况下有所帮助。然而，在更大更复杂网格下，模型表现显著下降，未能很好扩展。总体上，较大模型平均成功率更高，但视觉模态并未在所有场景下超越结构化文本输入。路径质量在简单网格中表现优秀。

Conclusion: 现有多模态LLM在空间推理、约束遵循和大规模场景的一致性能方面存在明显局限，视觉输入的优势条件受限，需进一步研究提升这类模型在机器人路径规划中的鲁棒性和扩展性。この领域未来应加强模型的空间理解能力和多模态融合策略。

Abstract: Large Language Models (LLMs) show potential for enhancing robotic path
planning. This paper assesses visual input's utility for multimodal LLMs in
such tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on
generating valid and optimal paths in 2D grid environments, simulating
simplified robotic planning, comparing text-only versus text-plus-visual inputs
across varying model sizes and grid complexities. Our results indicate moderate
success rates on simpler small grids, where visual input or few-shot text
prompting offered some benefits. However, performance significantly degraded on
larger grids, highlighting a scalability challenge. While larger models
generally achieved higher average success, the visual modality was not
universally dominant over well-structured text for these multimodal systems,
and successful paths on simpler grids were generally of high quality. These
results indicate current limitations in robust spatial reasoning, constraint
adherence, and scalable multimodal integration, identifying areas for future
LLM development in robotic path planning.

</details>


### [160] [Regrasp Maps for Sequential Manipulation Planning](https://arxiv.org/abs/2507.12407)
*Svetlana Levit,Marc Toussaint*

Main category: cs.RO

TL;DR: 该论文提出了一种通过重抓取(regrasp)区域和抓取序列信息加快优化型任务与运动规划(TAMP)求解的方法。核心在于利用重抓取映射对搜索过程做引导，从而提升在复杂受限环境下操作的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 在受限和杂乱环境中，机器人操作任务往往需要多次调整抓取姿态，且抓取点位置未知。传统TAMP方法在这类任务上搜索效率低且易失败，因此急需提升TAMP在复杂重抓取操作中的性能。

Method: 作者提出通过建立一种状态空间抽象——重抓取映射(regrasp map)，描述在不同配置空间下可用抓取的组合，为TAMP求解器提供抓取切换的猜测及额外的物体放置约束。具体做法是交错进行重抓取映射的建立、针对失败情形调整该映射、并分步求解TAMP子问题，实现稳健高效的搜索。

Result: 算法能够在复杂重抓取操作问题中，利用重抓取映射信息显著加速搜索过程，并提升求解的成功率和鲁棒性。

Conclusion: 通过对重抓取操作问题引入映射抽象与自适应调整，结合分阶段TAMP求解，该方法在解决复杂、高难度机器人操作任务时展现了优越性能。

Abstract: We consider manipulation problems in constrained and cluttered settings,
which require several regrasps at unknown locations. We propose to inform an
optimization-based task and motion planning (TAMP) solver with possible regrasp
areas and grasp sequences to speed up the search. Our main idea is to use a
state space abstraction, a regrasp map, capturing the combinations of available
grasps in different parts of the configuration space, and allowing us to
provide the solver with guesses for the mode switches and additional
constraints for the object placements. By interleaving the creation of regrasp
maps, their adaptation based on failed refinements, and solving TAMP
(sub)problems, we are able to provide a robust search method for challenging
regrasp manipulation problems.

</details>


### [161] [Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement](https://arxiv.org/abs/2507.12431)
*Connor Burgess,Kyle Douin,Amir Kordijazi*

Main category: cs.RO

TL;DR: 本文介绍了一种专为3D打印材料表面润湿性测试设计的自动化接触角测试仪（ACAT），其集成了编程机器人、精确分液和模块化软硬件架构，实现了高效、可重复和安全的自动测量。


<details>
  <summary>Details</summary>
Motivation: 传统的手动接触角测试存在精度低、重复性差和操作不安全等问题，尤其在面对大批量3D打印材料时效率低下。为解决这些痛点，推动智能制造与材料发现自动化成为亟需。

Method: ACAT系统分为三大子系统：（1）符合工业安全标准的电气系统；（2）基于树莓派和Python开发的软件控制系统，内含故障检测和人机交互接口；（3）含三轴直角坐标机器人、气动执行机构和精密分液器的机械系统，全部封装于安全认证框架中。

Result: ACAT实现了高通量、自动化的表面特性表征，为智能制造和材料发现的集成应用提供了强大平台。

Conclusion: ACAT不仅克服了传统接触角测试的局限性，还为未来在智能制造和新材料筛选流程中的自动化应用奠定了坚实基础。

Abstract: The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work
cell developed to automate the measurement of surface wettability on 3D-printed
materials. Designed for precision, repeatability, and safety, ACAT addresses
the limitations of manual contact angle testing by combining programmable
robotics, precise liquid dispensing, and a modular software-hardware
architecture. The system is composed of three core subsystems: (1) an
electrical system including power, control, and safety circuits compliant with
industrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software
control system based on a Raspberry Pi and Python, featuring fault detection,
GPIO logic, and operator interfaces; and (3) a mechanical system that includes
a 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser
enclosed within a safety-certified frame. The ACAT enables high-throughput,
automated surface characterization and provides a robust platform for future
integration into smart manufacturing and materials discovery workflows. This
paper details the design methodology, implementation strategies, and system
integration required to develop the ACAT platform.

</details>


### [162] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种利用人类第一视角（egocentric）视频训练视觉-语言-动作（Vision-Language-Action, VLA）模型的新方法，通过将人类视频中的手部动作转化为机器人动作，实现机器人学习复杂操控任务，相比仅用机器人数据更高效扩展数据规模和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿效学习的机器人数据收集受限于真实机器人硬件，导致数据难以大规模扩展，且场景及任务不够丰富。人类视频作为数据源，不仅数量庞大，还能覆盖更多样化的操作任务。作者旨在解决数据规模和复杂性受限的问题，使机器人能够更好地学习复杂操控行为。

Method: 方法包括三步：1）使用人类第一视角视频训练VLA模型，预测人类手腕和手部动作；2）通过逆运动学与动作重定向，将人类动作转换为机器人动作；3）利用少量机器人操控演示微调模型。作者还提出了Isaac Humanoid Manipulation Benchmark仿真基准，用于多种双手操作任务的评测。

Result: 实验在所设计的仿真基准上验证了EgoVLA策略。结果显示，EgoVLA在多样性的仿真操控任务中显著优于基线方法，并通过消融实验证明，人类视频数据对于提升模型性能至关重要。

Conclusion: 论文证明了利用大规模人类视频训练并迁移到机器人操控任务的可行性和有效性，同时提出的数据集和仿真基准为今后机器人学习提供了新的研究方向。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>
