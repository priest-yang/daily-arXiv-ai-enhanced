<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: 作者提出了VL4Gaze，这是首个专门用于研究视觉-语言模型（VLM）对“凝视”理解能力的大规模基准数据集，并验证了当前主流VLM在相关任务中的表现及其改进方式。


<details>
  <summary>Details</summary>
Motivation: 当前的VLM虽然能完成多种视觉任务，但对“凝视”这一关键社交线索的理解能力鲜有系统性研究。缺乏用于评估和训练VLM凝视理解能力的数据集与基准，非常制约相关方向的发展。作者希望填补这一空白。

Method: 作者构建了VL4Gaze数据集，包含49万对自动生成的问题和答案，覆盖12.4万张图片，设计了四类凝视相关VQA任务，然后评测了多种主流的VLM（包括商用与开源型号），包括零样本、上下文学习和微调情境。

Result: 实验发现，现有大模型在未经过专门凝视任务训练时难以可靠完成相关推理，而使用VL4Gaze有针对性地监督训练后，各项凝视理解任务精度明显提升。

Conclusion: 作者认为，发展VLM的凝视理解能力需要有针对性的多任务监督，VL4Gaze数据集对推动此方向发展具有重要意义，数据和代码将公开以促进后续研究。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种面向资源受限边缘与物联网设备的垃圾检测神经网络架构搜索方法，大幅提升了模型性能和部署效率。


<details>
  <summary>Details</summary>
Motivation: 随着边缘和物联网设备对环境智能感知需求增长，如何在严格计算与存储资源受限的条件下高效准确地检测垃圾，成为TinyML领域的难题。现有方法性能受限或资源消耗过高，难以满足实际部署需求。

Method: 本文提出了硬件感知的神经架构搜索框架，利用Once-for-All风格的ResDets超网络，并迭代进行主干骨干和检测头部的演化搜索。引入了群体传递机制及准确率预测器，显著降低搜索成本和提升搜索稳定性，从而生成多规格适应不同部署预算的TrashDets模型。

Result: 在TACO五类别子集上，TrashDet-l参数量30.5M，mAP50达到19.5，比现有方法提升3.6，同时参数更少。TrashDets系列覆盖1.2M至30.5M参数，准确率mAP50为11.4到19.5不等，适配不同硬件约束。在MAX78002微控制器和TrashNet数据集上，TrashDet-ResNet和TrashDet-MBNet均优于ai87-fpndetector基线，能耗降低88%、延迟降低78%、功耗降低53%。

Conclusion: 所提框架能在不同TinyML硬件约束下高效自动搜索并生成多种高效垃圾检测器，实现了更优的性能/资源权衡，具备实际部署价值。

Abstract: This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 论文提出了OccuFly：首个真实世界的、基于摄像头的空中场景语义补全（SSC）数据集与评测基准，以及一种基于视觉、无激光雷达的SSC数据生成方法。主要聚焦于空中无人机场景，补全当前仅聚焦地面SSC的局限。


<details>
  <summary>Details</summary>
Motivation: 尽管SSC在地面应用如自动驾驶上广泛研究，但空中应用（如无人机自主飞行）研究不足，主要由于激光雷达受限如法规、重量和能耗，以及高空激光点云稀疏。此外，现有SSC数据集均基于激光雷达，难以适用无人机领域，因此亟需新的数据生成方式和基准。

Method: 提出了基于摄像头的空中SSC数据集OccuFly，涵盖不同高度、季节及场景，同时设计了一套基于主流无人机摄像头的、不依赖激光雷达的数据生成和注释方案。采用经典3D重建，并利用已标注的2D掩码自动投影到3D点云，极大降低人工3D标注成本。

Result: 建立了包含22类别、覆盖多种场景和季节的空中SSC数据集，对当前主流方法进行了基准测试，指出了高空视角下SSC的特有难点。

Conclusion: OccuFly为基于视觉的空中3D场景理解提供了首个大规模真实数据集与方法，有效推动了SSC在无人机等新领域的应用发展，也为后续研究提供了具有挑战性的开放基准和数据资源。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 本文提出了一种能同时应对有无提示（prompt）情形的乳腺超声图像分割模型NullBUS。该方法在缺乏可靠元数据时依然表现优异，达到了当前最优的分割效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像分割对智能诊断和治疗规划至关重要。现有的可提示（promptable）分割方法效果虽好，但受限于多数公开数据集缺乏高质量的文本或空间提示，导致可用数据减少，模型泛化性和健壮性受到限制。

Method: NullBUS是一种多模态混合监督框架，能够学习同时处理带有或不带提示的图像。在缺失文本提示时，作者提出了“可空提示”机制，即通过可学习的空嵌入（null embeddings）及掩码，在缺元数据时退回到仅靠图像证据，在有元数据时则充分利用文本。

Result: 在三个公开乳腺超声数据集统一实验池上，NullBUS在有无提示相混的测试场景下取得了mean IoU 0.8568和mean Dice 0.9103的业界最优分割表现。

Conclusion: NullBUS能够有效利用带和不带提示的数据，显著提升乳腺超声分割模型在缺乏元数据条件下的实用性和鲁棒性，为真实临床多样场景提供了更强的技术支持。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [5] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

TL;DR: 本文提出了一种面向任务的联合优化框架，实现光学、传感器建模与轻量级语义分割网络的一体化端到端RAW到任务管线，相比传统流水线，在KITTI-360数据集上提升了mIoU，并保持了高效率和边缘可部署性。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶感知系统将摄像头设计与感知算法分离，采用固定光学和手工ISP，这在成像过程中会丢失对机器感知有用的信息，并且迫使模型适应各种传感器伪影，影响下游任务效果。作者意图通过端到端联合优化，提升自动驾驶感知的精度与鲁棒性。

Method: 借鉴DeepLens，将真实手机镜头模型、可学习的彩色滤光片阵列（CFA）、泊松-高斯噪声和量化过程集成为可训练模块，通过端到端优化直接对语义分割性能进行优化，实现感知任务驱动的传感器和光学协同设计。

Result: 在KITTI-360数据集上，该方案mIoU优于传统固定流水线，光学建模与CFA学习对提升表现尤其关键，特别是对细长或对低光敏感的类别。模型参数量约为1M，可28FPS实时运行，满足边缘部署需求。分析发现，协同设计的传感器更好地适应语义结构，在模糊、噪声及低比特率下仍可保持分割边界清晰与高准确度。

Conclusion: 全链路协同优化光学、传感器和网络，为高效、可靠且易部署的自动驾驶感知系统提供了理论和实践依据，证明联合设计相比传统分离方式有显著优势。

Abstract: Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: 该论文提出了CHAMMI-75，一个包含75项多样生物学研究的多通道显微图像公开数据集，用于提升细胞形态学模型的通用性和适应性。


<details>
  <summary>Details</summary>
Motivation: 目前细胞形态学机器学习模型通常仅针对单一类型显微图像训练，导致其难以在不同研究和设备间复用。技术标准不一和实验条件分布外问题亟需解决。

Method: 作者从公开资源筛选整理了75项研究的多通道异构显微图像，构建了CHAMMI-75数据集，并用于训练、评估可适应不同通道和显微图像类型的形态学分析模型。

Result: 实验表明，利用CHAMMI-75训练后的模型可在多通道生物成像任务中表现更优，主要归功于数据集涵盖了多种显微成像方式带来的高度多样性。

Conclusion: CHAMMI-75为开发新一代适用于各类生物学研究的泛化型细胞形态分析模型奠定了基础。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 论文提出了一种自适应视觉预处理方法，通过根据图片内容特征动态调整输入分辨率和空间覆盖范围，有效减少冗余计算，从而提升视觉-语言模型（VLM）在推理过程中的效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多模态推理任务中表现出色，但高分辨率图片处理时推理延迟大、计算成本高。即便部分架构（如FastVLM）提升了视觉编码效率，但静态预处理流程仍导致对简单图片的重复计算，限制实际应用与部署。

Method: 作者提出：在不修改VLM模型结构和无需重新训练的条件下，将内容感知图像分析、自适应分辨率选择与内容相关裁剪结合，对输入图片进行自适应视觉预处理，动态降低视觉冗余。该方法与FastVLM无缝集成，且仅需在推理端进行即可。

Result: 在DocVQA数据集的子集上评测，推理端启用自适应预处理后，每张图片的推理时间减少超50%，整体生成时间下降，视觉token数量平均下降超过55%，均优于基线流水线。

Conclusion: 输入内容感知的自适应预处理是提升VLM推理部署效率的有效、轻量策略。相关代码已开源，便于同行复现与进一步研究。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: 本论文提出了ALIVE系统，通过本地运行的虚拟讲师和内容检索，实现点播视频课程中的实时互动问答，提高了学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统的讲座视频虽灵活，但无法为学生的疑惑提供即时澄清，学生需自行查找信息，影响学习效率。现有的智能互动系统又大多云端部署，难以保证隐私，且缺乏内容感知和统一的检索解释整合。

Method: ALIVE系统本地运行，结合自动语音识别（ASR）、大语言模型（LLM）精炼和虚拟人脸合成，实现内容讲解。系统通过语义相似性和时间戳对齐的机制进行内容检索，用户可实时暂停视频，通过文本或语音提问，系统返回文本或虚拟人形象的回答。底层技术包括轻量化嵌入模型、FAISS检索和渐进式头像生成预加载。

Result: 系统在医学影像课程中进行测试，获得较高的检索准确率、响应延迟低，用户体验良好。ALIVE能为学生提供内容相关、实时且生动的讲解。

Conclusion: ALIVE展示了局部部署、多模态AI和内容感知检索的结合能极大提升网络讲座的教学价值，为下一代互动学习环境提供了可拓展的解决路径。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [9] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 本文提出了一个结合三维GPR物理特征和深度学习方法的地下管线智能检测框架，通过多视图联合分析和改进型YOLO算法，显著提升了复杂场景下小目标管线的检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有地下管线检测在多视图特征相关性弱、小尺度目标识别精度低、复杂场景鲁棒性不足。尤其是三维GPR在实际应用中，容易因噪声、目标小等问题导致检测效果差，需要一种融合物理建模与深度学习的新方法。

Method: 1. 提出B/C/D三视图联合分析策略，利用FDTD数值模拟与实测数据进行三维特征评估。2. 基于YOLOv11，提出DCO-YOLO，集成DySample、CGLU和OutlookAttention机制，提升小目标管线的边缘特征提取能力。3. 设计3D-DIoU空间特征匹配算法，结合三维几何约束和中心距离惩罚，实现多视图标注自动关联，解决单视图检测歧义。

Result: 在真实城市地下管线数据集上，所提方法在复杂多管场景的检测准确率、召回率、mAP分别达到96.2%、93.3%、96.7%，分别比基线模型提升2.0%、2.1%、0.9%。消融实验和Grad-CAM++可视化证明，动态特征增强模块和多视图融合显著提升了模型关注管线几何特征的能力。

Conclusion: 该研究将三维GPR的物理建模与深度学习优化策略有机结合，显著提升了地下管线智能识别与定位的精度与鲁棒性，为城市地下空间探测提供了高效可靠的新技术框架。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [10] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360是一种针对360度高清视频的高效视频解码与压缩方法，仅解码用户视角区域，提升了解码速度并大幅降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的基于NeRV的视频压缩方法在处理高分辨率360度全景视频时，存在内存占用高和解码速度慢的问题，难以应用于实时场景。

Method: 提出NeRV360框架，将用户视口提取与解码过程整合，通过空间-时间仿射变换模块实现按需、按视角和时间的条件解码，无需重建整幅全景帧。

Result: 在6K分辨率视频实验中，相较于HNeRV，NeRV360内存消耗降低7倍，解码速度提升2.5倍，图像质量在各项客观指标上更优。

Conclusion: NeRV360有效解决了高分辨率360度视频实时解码的内存和速度瓶颈，能为实时全景视频应用提供更优解。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [11] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Vision Foundation Models（VFMs）的新型跨模态舰船再识别方法，通过在特征空间进行轻量化调优，大幅减少了对大规模配对数据的依赖，并在保持模型冻结的前提下实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前跨模态舰船再识别严重依赖配对样本进行模态对齐，且现有权重空间微调方法在小模型上表现不佳。因此，亟需在小样本、少参数情况下提升跨模态能力。

Method: 受柏拉图表征假说启发，提出Domain Representation Injection（DRI）方法，在VFM冻结下，利用偏移编码器（Offset Encoder）提取融合模态与身份特征的域表示，经调制器（Modulator）引导，并通过加法注入中间层特征来适应下游任务，完成高效PEFT。

Result: 在HOSS-ReID数据集上，采用仅1.54M和7.05M参数分别取得了57.9%和60.5%的mAP，超越现有主流方法，实现最优表现。

Conclusion: 该方法在参数量极小的前提下，显著提升了跨模态舰船再识别性能，验证了通过特征注入实现高效PEFT的有效性与普适性。

Abstract: Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [12] [DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction](https://arxiv.org/abs/2512.20898)
*Xiao Yu,Zhaojie Fang,Guanyu Zhou,Yin Shen,Huoling Luo,Ye Li,Ahmed Elazab,Xiang Wan,Ruiquan Ge,Changmiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为DGSAN的双图时空注意力网络，能够高效融合多模态和多时序数据，提升肺结节分类的准确率，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节检测研究虽然融合了多模态与多时序信息，但主要采用简单的向量拼接或基础注意力机制，信息融合效率不足，制约了检测效果。因此需要一种更有效的信息融合方式。

Method: 提出Dual-Graph Spatiotemporal Attention Network（DGSAN），包括：1）全局-局部特征编码器用于提取和融合肺结节局部、全局特征；2）双图结构（模态间与模态内图）用于组织多模态特征；3）分层跨模态图融合模块进一步优化特征整合。此外，作者还构建了新的多模态数据集NLST-cmst。

Result: 在NLST-cmst和CSTL衍生数据集上进行大量实验，所提出的DGSAN在肺结节分类任务上在准确率和计算效率方面均显著优于当前最先进的方法。

Conclusion: DGSAN能有效融合多模态与多时序信息，极大提升了肺结节的分类性能，为相关研究和实际应用提供了新工具及数据支持。

Abstract: Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.

</details>


### [13] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了第一个专门评估视觉语言模型（VLM）对压缩图像理解能力的基准，发现压缩图像会导致VLM性能下降，并提出了一种通用适配器来显著提升VLM对各种压缩图像的处理能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型应用需求的增长，提高输入图像压缩效率变得重要。但目前VLM主要针对高码率图像开发，其对低码率压缩图像的理解尚未被系统研究。

Method: 作者构建了一个包含100万+压缩图像、适配多种主流图像编码器和多样视觉语言任务的基准集合，并通过实验分析性能下降原因，归结于压缩信息损失和VLM泛化能力不足。基于分析，提出了一个通用VLM适配器来提升模型性能。

Result: 实验发现，压缩图像内容信息损失不可逆，但适配器能缓解VLM的泛化能力不足问题。通用适配器能对不同编码器和码率下的图像提升VLM表现10%-30%。

Conclusion: 本文填补了VLM对压缩图像理解能力评测的空白，并提出了一种简单有效的通用适配器，能够大幅提升VLM对压缩图像的应用价值，为后续研究奠定了基础。

Abstract: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [14] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

TL;DR: 提出了PanoGrounder框架，通过将多模态全景表示与强大的预训练2D视觉-语言模型(VLMs)结合，实现了更强的3D视觉指代（visual grounding）能力，并在主流数据集取得了领先表现和更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 以往3D视觉-语言指代技术依赖于显式的3D几何建模，但由于3D视觉-语言数据稀缺且推理能力有限，导致模型泛化性差，无法胜任更广泛复杂的场景。作者希望借助2D VLMs的强大感知和推理能力来提升3D视觉指代的泛化能力。

Method: 核心方法是将3D场景渲染为多视角的全景图，并在全景渲染中融合丰富的3D语义与几何特征。构建三阶段流程：1）选择场景布局和几何特征优化的全景观察点；2）用VLM对每个全景图进行文本指代定位；3）融合各视角预测，通过lift操作还原为3D bounding box，实现最终三维指代。

Result: PanoGrounder在ScanRefer和Nr3D等3D视觉指代主流数据集上取得了最新最优（state-of-the-art）的实验结果，且相比传统方法，在新场景和不同文本表达泛化能力更强。

Conclusion: 通过创新性地引入多模态全景中间表示和预训练2D VLM，PanoGrounder显著提升了3D视觉指代的性能与泛化性，为视觉-语言理解向机器人等实际应用迈进了一步。

Abstract: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [15] [Self-supervised Multiplex Consensus Mamba for General Image Fusion](https://arxiv.org/abs/2512.20921)
*Yingying Wang,Rongjin Zhuang,Hui Zheng,Xuanhua He,Ke Cao,Xiaotong Tu,Xinghao Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种用于通用图像融合的自监督多路共识Mamba（SMC-Mamba）框架，在多种图像融合任务及下游视觉任务上超越了现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有任务特定的图像融合方法大多只关注跨模态信息整合，难以兼顾多种任务；而通用图像融合方法需在提升性能的同时避免复杂性增加。

Method: 提出SMC-Mamba框架，包括：（1）模态无关特征增强（MAFE）模块，通过自适应门控保留细节并利用空间-通道和频率-旋转扫描增强全局表征；（2）多路共识跨模态Mamba（MCCM）模块，实现跨专家动态协作达成共识，并通过跨模态扫描提升特征交互和信息融合；（3）双层自监督对比损失（BSCL），在不增加算力开销下保留高频信息并提升下游任务性能。

Result: 大量实验表明，该方法在红外-可见光、医学、多焦点、多曝光等多种图像融合任务及相关下游视觉任务上，性能优于SOTA算法。

Conclusion: SMC-Mamba框架能够高效整合多模态互补信息，同时提升多任务性能且不增加复杂性，为通用图像融合提供了新颖有效的解决思路。

Abstract: Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.

</details>


### [16] [Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting](https://arxiv.org/abs/2512.20927)
*Yoonwoo Jeong,Cheng Sun,Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D高维特征渲染策略——Quantile Rendering (Q-Render)，以及基于此的Gaussian Splatting Network (GS-Net)，显著提升了3D开放词汇分割任务的速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇分割方法在渲染高维特征时面临效率低下与特征信息损失的问题，尤其在代码本和特征压缩阶段影响分割精度。需要一种高效且高保真度的渲染方法。

Method: 文中提出Q-Render渲染策略，仅对射线上占主导影响的3D高斯体进行稀疏采样，有效兼顾高维特征处理效率与保真度。并结合Q-Render构建可泛化的GS-Net以预测高斯特征。

Result: 在ScanNet和LeRF数据集上，所提出方法在分割表现上超越了现有主流方法，并实现在512维特征图上约43.7倍的实时渲染速度提升。

Conclusion: Q-Render和GS-Net在保证3D高维特征完整性的同时，显著提升了分割质量及渲染效率，为3D开放词汇分割领域带来新突破。

Abstract: Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

</details>


### [17] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 该论文提出了一种能够通过自身经验自我进化工具的视觉编程框架（TVP），实现了3D场景空间推理任务上的性能突破。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在3D空间推理上存在几何计算难题。视觉编程通过调用专用工具分解问题，提升了解决效率，但现有方法依赖固定工具集或推测式工具诱导，导致工具使用效率不高和结果次优。因此，亟需一种能根据实际解题经验持续进化工具的方法。

Method: 作者提出了Transductive Visual Programming（TVP）框架。其核心流程是：1）先用基础工具解决问题并积累解题案例；2）从案例库抽象出高阶可复用工具，逐步扩充进化工具库，形成自我经验驱动的工具演化机制；3）用进化出的更强工具解决新问题。

Result: TVP在Omni3D-Bench数据集上实现了SOTA性能，比GPT-4o高22%，超越以往最佳视觉编程系统11%。进化出的工具作为核心依赖的调用频率显著高于以往诱导式工具（5倍），并对新空间任务表现出很强泛化性，在SpatialScore-Hard测试集同样优于其他方法。

Conclusion: 经验驱动的transductive工具创建为构建自进化的视觉编程智能体提供了强大范式，可大幅提升复杂空间推理任务的效率和泛化能力。

Abstract: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [18] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种多智能体协作推理框架，通过将语义规划与视觉合成解耦，有效提升了不可见区域推断（Amodal Completion）的结构完整性与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有渐进式方法在推断不可见部分时易受推理不稳定和误差累积影响，难以保持语义和结构完整性，有必要提出新框架解决这些瓶颈。

Method: 方法上，作者采用多智能体协作结构，将推理过程分为两个阶段：语义规划和视觉合成。引入了自纠正验证智能体，利用链式思考方法优化可见区域分割和遮挡物识别，以及多样假设生成器，为不可见区域给出多种合理语义解释。同时，提出了MAC-Score这一新评测指标，更好地反映结构完整性和语义一致性。

Result: 实验表明，该方法在多个数据集上显著优于现有SOTA方法，在语义一致性和结构完整性评测上表现突出，MAC-Score与人工评价高度一致。

Conclusion: 文中提出的协作推理和多机制框架不仅提升了Amodal Completion的性能，也推动了该领域评测标准的进步，为后续相关研究提供了更扎实的技术与评测基础。

Abstract: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [19] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Real-centric Envelope Modeling（REM）的新方法，通过模拟真实图像分布而非依赖生成模型的伪迹，实现了更鲁棒、更具泛化能力的生成图像检测。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法普遍依赖于捕捉特定生成器的伪迹，但这些伪迹在图像经过多轮平台分享、后处理和降质等真实场景下非常容易消失，导致检测效果大幅下降。因此，亟需一种能在真实环境下保持鲁棒性的检测方法。

Method: REM方法旨在建模真实图像的分布，通过自重建过程中引入特征级扰动，生成近真实样本，并利用包含跨域一致性的envelope estimator，学习包围真实图像流形的边界。此外，作者还建立了包含开源及商业生成器和多重真实场景降质的新基准数据集RealChain。

Result: 在八个基准评测中，REM方法比现有最先进方法平均提升了7.5%。尤其是在严重降质的RealChain基准上，REM依然展现出极佳的泛化性和鲁棒性。

Conclusion: REM建立了一种新范式，为真实场景下的合成图像检测奠定了坚实基础。相关代码和RealChain基准集将在论文接收后公开。

Abstract: The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>


### [20] [SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking](https://arxiv.org/abs/2512.20975)
*Yujin Noh,Inho Jake Park,Chigon Hwang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于地图和LLM（大语言模型）代理的系统SPOT，实现了车辆在多CCTV环境下盲区内的追踪和轨迹预测，无需事先训练，比现有方法更有效。


<details>
  <summary>Details</summary>
Motivation: 多CCTV监控系统普遍存在因摄像头视野限制导致的盲区，车辆在摄像头间穿行时常出现ID切换和轨迹丢失，影响实时路径预测的可靠性。为提高车辆追踪连续性，有必要开发能跨摄像头环境不中断追踪的技术。

Method: 提出SPOT系统，将道路结构（路标点）和CCTV布置信息根据2D空间坐标文档化，经分块处理后用于实时查询与推理。同时，利用CCTV图像中的相对位置及视野信息，将车辆位置映射至真实世界坐标，结合车辆运动方向、速度和驾驶模式，通过交叉口级的beam search推测车辆经过盲区后最有可能出现的下一个CCTV位置。

Result: 在虚拟城市环境（基于CARLA模拟器）的实验中，SPOT能够在CCTV盲区段准确预测车辆下一次出现的摄像头，实现比现有方法更连续有效的车辆轨迹追踪。

Conclusion: SPOT方法无需预训练，通过结合地图空间信息与车辆动态，实现了多CCTV环境下的盲区车辆追踪和实时预测，提高了轨迹连续性，对提升城市监控系统的可靠性和实用性具有实际意义。

Abstract: CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.

</details>


### [21] [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976)
*Zeqing Song,Zhongmiao Yan,Junyuan Deng,Songpengcheng Xia,Xiang Mu,Jingyi Xu,Qi Wu,Ling Pei*

Main category: cs.CV

TL;DR: 提出了一种高效的神经LiDAR增量建图框架XGrid-Mapping，结合显式与隐式网格，显著提升了大规模实时建图的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经LiDAR建图方法多数依赖稠密隐式表达，不充分利用场景几何结构，且基于体素引导的方法难以实时应用。因此需要能兼顾效率与高质量建图的新方法。

Method: 提出一种混合网格结构XGrid-Mapping，将稀疏显式网格（用于几何先验和结构引导）与隐式稠密网格（丰富场景表达）结合；采用VDB结构和子图组织方式，降低计算量，实现高效的增量建图。引入基于蒸馏的重叠区域对齐策略，保持子图间一致性，并配合动态移除模块增强鲁棒性与采样效率。

Result: 通过大量实验表明该方法取得了更高的建图质量，并克服了体素方法的效率瓶颈，在精度和速度上均优于现有先进方法。

Conclusion: XGrid-Mapping有效提升了大规模神经LiDAR增量建图的性能，为自主系统的环境感知与导航/决策提供了更优的基础。

Abstract: Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

</details>


### [22] [X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data](https://arxiv.org/abs/2512.20980)
*Xinquan Yang,Jinheng Xie,Yawen Huang,Yuexiang Li,Huimin Huang,Hao Zheng,Xian Wu,Yefeng Zheng,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新型数据合成流程，通过丰富的正常胸部X线片增强稀有长尾病变数据，并结合大语言模型知识引导和渐进式增量学习，有效提升了对罕见肺部异常的诊断能力。


<details>
  <summary>Details</summary>
Motivation: 长尾分布的肺部异常在胸部X光片中很难诊断，仅靠当前扩散模型因样本稀缺而难以对罕见病变进行有效增强，导致自动化诊断准确率受限。

Method: 收集大量正常胸部X光片以训练扩散模型，再用该模型对异常X光片中的常见病变部分进行修复，从而保留并增强稀有病变（长尾类）样本。同时，结合大语言模型知识引导模块和渐进式增量学习来稳定修复过程。

Result: 在MIMIC和CheXpert等公开肺部数据集上进行的综合实验表明，该方法在罕见病变的识别上达到了新的性能基准。

Conclusion: 所提出的扩散模型数据增强流程及其配套的知识引导和增量学习策略，显著提升了长尾肺部异常的诊断性能。

Abstract: Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.

</details>


### [23] [PUFM++: Point Cloud Upsampling via Enhanced Flow Matching](https://arxiv.org/abs/2512.20988)
*Zhi-Song Liu,Chenhang He,Roland Maier,Andreas Rupp*

Main category: cs.CV

TL;DR: 本文提出了一种增强的点云流匹配框架PUFM++，能够从稀疏、带噪和不完整的观测中重建高质量致密点云，并在多项基准任务中刷新了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 点云上采样在三维视觉任务（如三维重建、表面建模等）中非常重要，但现有方法在提升几何保真度、鲁棒性及与实际后续任务的一致性方面仍存挑战。为此，作者希望通过改进生成式建模框架，提升点云的还原质量和实用性。

Method: PUFM++主要包含：1）两阶段流匹配策略，先从稀疏点到致密目标学习直线路径，再用噪声扰动样本进一步优化分布；2）自适应时间调度机制，通过数据驱动的方法提高采样效率与稳定性；3）在采样过程中加入流形约束，确保生成点与真实表面对齐；4）引入递归接口网络提升特征表达与重建质量。

Result: 在合成数据集与真实扫描集上的大量实验表明，PUFM++在视觉保真度、量化精度等多个指标上均超越现有方法，达成了点云上采样的新SOTA水平。

Conclusion: PUFM++为稀疏及低质量点云的高质量重建提供了强有力的解决方案，通过多项创新显著提升性能，对三维视觉应用具有重要意义。

Abstract: Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.

</details>


### [24] [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003)
*Xiangzuo Wu,Chengwei Ren,Jun Zhou,Xiu Li,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的多视角逆渲染方法，能够一致地从多张图片中恢复场景的几何、材质和光照信息，并通过一致性微调提升在真实场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的单视图逆渲染方法在多视图情况下无法保证结果一致，多视图优化方法则计算量大且可拓展性差。因此，急需一种既能实现多视角一致性、又高效的逆渲染框架，特别是在缺乏真实训练数据的情况下依然能泛化到现实世界。

Method: 作者提出了一种前馈式多视角逆渲染框架，可直接从多张RGB序列图像中预测空间变化的反照率、金属度、粗糙度、漫反射阴影和表面法线。方法核心是交替关注单视图内和多视图间的特征，实现一次前向推理内的全局一致。为增强在真实场景下的表现，设计了一种基于一致性的微调策略，通过未标注的真实视频提高模型的一致性和鲁棒性。

Result: 在多个基准数据集上，所提方法在多视角一致性、材质与法线估计质量及对真实数据的泛化能力上均达到了最新水平。

Conclusion: 该方法有效解决了多视角逆渲染中的一致性与效率难题，同时通过一致性微调实现了在真实场景下的强泛化能力，推动了逆渲染在实际应用中的落地。

Abstract: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

</details>


### [25] [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004)
*Jinghan Li,Yang Jin,Hao Jiang,Yadong Mu,Yang Song,Kun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的自回归视觉生成预训练框架NExT-Vid，通过掩码未来帧预测来联合建模图像和视频，在多个下游分类任务上效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成预训练方法大多采用BERT式掩码建模，忽略了视频分析关键的时序信息。现有极少数自回归视觉预训练方法也存在语义定位不准和生成质量较差等问题，因此需要开发更有效的视觉生成预训练方式。

Method: 作者提出NExT-Vid框架：采用掩码的下一帧预测方式进行自回归建模，并引入上下文隔离的自回归预测器，将语义表示与目标解码解耦；同时提出条件流匹配解码器以提升生成的质量和多样性。整个预训练过程采用上下文隔离的流匹配策略。

Result: 在大规模模型预训练和下游分类任务（采用attentive probing）上做了大量实验，结果显示NExT-Vid在视觉表征学习方面始终优于现有生成式预训练方法。

Conclusion: NExT-Vid通过结合自回归预测与新的解码和训练机制，显著提升了视觉表征的质量和下游任务的性能，为视觉生成预训练提供了更优方案。

Abstract: Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

</details>


### [26] [Granular-ball Guided Masking: Structure-aware Data Augmentation](https://arxiv.org/abs/2512.21011)
*Shuyin Xia,Fan Chen,Dawei Dai,Meng Yang,Junwei Han,Xinbo Gao,Guoyin Wang*

Main category: cs.CV

TL;DR: 本文提出了GBGM，一种通过Granular-ball Computing(粗粒球计算)指导的结构感知掩码增强方法，可自适应保留重要区域、抑制冗余，有效提升视觉模型鲁棒性和泛化能力。大量实验证明其在多任务、多模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在计算机视觉中取得巨大成功，但严重依赖大规模标注数据且在样本有限或分布变化时易过拟合。现有掩码增强方法常忽略结构信息，易丢失关键语义，因此需要结构感知的数据增强方案。

Method: 作者提出Granular-ball Guided Masking(GBGM)方法，利用Granular-ball Computing(GBC)指导掩码过程，通过粗到细的分层操作自适应保留语义丰富、结构关键的图像区域，抑制冗余区域，得到代表性和判别性兼具的数据增强样本。

Result: 在多个基准数据集上进行大量实验，GBGM显著提升了分类准确率和掩码图像重建效果，对CNN和Vision Transformers等主流模型均有提升，且适用范围广。

Conclusion: GBGM是一种简单且与模型无关的结构感知数据增强策略，实现了结构保护与信息去冗余的平衡，为数据增强提供了新范式。

Abstract: Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.

</details>


### [27] [FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing](https://arxiv.org/abs/2512.21015)
*Mingshu Cai,Yixuan Li,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本论文提出了FluencyVE，一种针对视频编辑的高效方法，通过整合Mamba线性时序模块和优化的注意力机制，实现了一次性、低计算开销的视频编辑，并在多种现实视频属性编辑任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模文本到图像的扩散模型在图像生成与编辑上取得了巨大成功，但将其扩展到视频编辑时仍面临时序一致性差和计算开销高等难题。本文旨在提升视频编辑的效果与效率。

Method: 作者提出FluencyVE，将Mamba线性时序模块集成到基于预训练Stable Diffusion的视频编辑模型中，替代传统的时序注意力层，以实现全局帧级别关注并降低计算成本。此外，采用低秩近似矩阵替换因果注意力中的查询和键权重矩阵，并在训练中引入加权均值更新注意力分数，进一步优化生成能力与效率。

Result: 实验和分析显示，该方法能有效编辑现实世界视频中的多种属性、主体和场景，且在编辑质量和计算资源消耗间取得良好平衡。

Conclusion: FluencyVE有效减轻了计算负担，同时保持了模型的生成能力和视频编辑的一致性，为文本引导下的视频编辑提供了更实用的解决方案。

Abstract: Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.

</details>


### [28] [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019)
*Rui-qing Sun,Xingshan Yao,Tian Lan,Hui-Yang Zhao,Jia-Ling Shi,Chen-Hao Cui,Zhijing Wu,Chen Yang,Xian-Ling Mao*

Main category: cs.CV

TL;DR: 该论文提出了一种高效保护3D场景口型同步换脸视频的防御框架，在不损失视频质量的前提下，有效阻止换脸模型获取三维信息。


<details>
  <summary>Details</summary>
Motivation: 随着3D场景下基于视频驱动的说话人换脸（TFG）技术的发展，个人肖像视频更容易被高保真地仿造，带来严重的隐私与安全风险，然而目前缺乏高效且有效的针对3D场TFG方法的视频防护手段。传统2D扰动防御方法会导致严重的视频降质和计算量激增，而且无法扰乱三维语义。

Method: 作者提出了一套面向3D场TFG的防御方法，通过扰乱视频中的三维信息捕获过程，在保证高视频质量的同时，阻断换脸模型建模3D几何与外观。方法包括：(1) 相似性引导参数共享机制，提高计算效率；(2) 多尺度双域注意力模块，联合优化空间频率域扰动，有效生成扰动信号。

Result: 实验表明，所提方法在不降低视频质量的前提下，实现了较强的防御效果，并且对现有防御方法加速47倍。同时，抗缩放和抗最新净化攻击性能突出。消融实验进一步佐证了各核心设计的有效性。

Conclusion: 相比于现有2D扰动防御，本工作显著缓解了计算与质量瓶颈，能高效防护3D场TFG对个人视频的滥用，为视频隐私保护提供了新思路。

Abstract: State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

</details>


### [29] [Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model](https://arxiv.org/abs/2512.21032)
*Mingshu Cai,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的新方法，将红外热成像下的人脸图像高质量地转化为可见光图像，同时保留关键身份特征，显著提升了夜间人脸识别的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别模型主要基于可见光图像训练，难以适应夜间使用红外图像的数据，导致识别性能严重下降，传统基于特征的方法和现有的生成式方法均存在特征损失及失真问题，亟需新的方法提升红外人脸识别效果。

Method: 1）提出一种基于潜在扩散的图像生成模型，将红外热成像人脸转换为可见光人脸图像；2）引入多属性分类器辅助提取关键面部属性，缓解特征损失问题；3）提出Self-attn Mamba模块，实现跨模态特征的全局建模并提升推理速度。

Result: 在两个基准数据集上进行了实验，所提出方法在生成图像质量和身份特征保持上均取得了当前最优表现，优于以往方法。

Conclusion: 该方法有效解决了红外到可见光人脸识别中跨模态域迁移和特征丢失问题，为夜间人脸识别应用提供了高效且准确的技术支持。

Abstract: Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.

</details>


### [30] [Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising](https://arxiv.org/abs/2512.21038)
*Yiwen Shan,Haiyu Zhao,Peng Hu,Xi Peng,Yuanbiao Gou*

Main category: cs.CV

TL;DR: 本文提出了一种名为Next-Scale Prediction（NSP）的自监督真实图像去噪新方法，通过跨尺度训练对有效分离噪声去相关与细节保留任务，显著提升了去噪效果，同时支持噪声图像超分辨。


<details>
  <summary>Details</summary>
Motivation: 现有盲点网络（BSN）利用下采样去相关噪声时，强下采样会损失图像细节，弱下采样则无法有效去除相关噪声，因此存在噪声去相关和细节保留之间的矛盾。

Method: 作者提出NSP，通过构建跨尺度训练对，让BSN利用低分辨率且已完全去相关的子图像预测高分辨率含细节的目标，从而实现噪声去相关与细节还原的解耦。

Result: 大量实验证明，NSP在真实世界自监督去噪基准上取得了最优性能，能够显著缓解去相关与细节保留之间的长期矛盾。此外，NSP还可直接支持带噪声图像的超分辨，无需再训练或额外修改。

Conclusion: NSP是一种创新、高效的自监督去噪框架，在提升去噪效果和图像细节还原上均优于现有技术，具有实际应用潜力。

Abstract: Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.

</details>


### [31] [A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography](https://arxiv.org/abs/2512.21040)
*Jaehong Lee,You Chan No,YoungWoo Kim,Duksu Kim*

Main category: cs.CV

TL;DR: 本论文提出了KOREATECH-CGH数据集，包含6000对高质量RGB-D图像与全息图，并提出了提升全息重建质量的新后处理方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习生成全息图（ML-CGH）技术发展迅速，但受限于高质量、大规模全息图数据集的缺乏。研究动机是解决数据集稀缺和提升全息重建质量的问题。

Method: 作者公开了KOREATECH-CGH数据集，涵盖多分辨率及大深度范围RGB-D与复全息图对。提出了幅度投影（amplitude projection）后处理方法，在不同深度层替换全息波场的幅度，保留相位以增强重建质量。实验对比了该方法与最新优化方法在重构指标上的差异。

Result: 采用幅度投影后处理后，在最大深度范围内全息重建达到了27.01 dB的PSNR和0.87的SSIM，分别较对标方法提升了2.03 dB和0.04。数据集在多项ML全息生成和超分辨实验中展现良好适用性。

Conclusion: KOREATECH-CGH数据集为ML-CGH系统的训练和评测提供了新的高质量资源，所提幅度投影方法有效提升了大深度范围下的全息重建质量，对下一代基于机器学习的全息生成系统具有推动作用。

Abstract: Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.

</details>


### [32] [Matrix Completion Via Reweighted Logarithmic Norm Minimization](https://arxiv.org/abs/2512.21050)
*Zhijie Wang,Liangtian He,Qinghua Zhang,Jifei Miao,Liang-Jian Deng,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的重加权对数范作为低秩矩阵补全问题中的秩函数替代项，并采用ADMM进行优化，实验在图像修复上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的核范数替代秩的凸松弛方法存在对奇异值收缩过度、导致解次优的问题。为获得更接近原始秩函数的效果，亟需新的非凸替代方法。

Method: 提出了一种新的重加权对数范作为非凸替代，设计利用ADMM算法高效求解优化问题。

Result: 在图像修复实验中，该方法无论是视觉效果还是定量指标均优于当前主流的低秩矩阵补全方法。

Conclusion: 重加权对数范作为新的秩近似能有效改善补全效果，是低秩矩阵补全更优的非凸替代选择。

Abstract: Low-rank matrix completion (LRMC) has demonstrated remarkable success in a wide range of applications. To address the NP-hard nature of the rank minimization problem, the nuclear norm is commonly used as a convex and computationally tractable surrogate for the rank function. However, this approach often yields suboptimal solutions due to the excessive shrinkage of singular values. In this letter, we propose a novel reweighted logarithmic norm as a more effective nonconvex surrogate, which provides a closer approximation than many existing alternatives. We efficiently solve the resulting optimization problem by employing the alternating direction method of multipliers (ADMM). Experimental results on image inpainting demonstrate that the proposed method achieves superior performance compared to state-of-the-art LRMC approaches, both in terms of visual quality and quantitative metrics.

</details>


### [33] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种利用事件相机和光流引导的6自由度物体位姿跟踪方法，以提升在极端光照、运动模糊等场景下的跟踪准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统相机在物体跟踪任务中易受运动模糊、噪声、遮挡及光照变化影响，限制了其实用性。新兴的类生物事件相机具有高动态范围和低延迟，有望克服这些问题，因此亟需相应的高效物体位姿跟踪方法。

Method: 该方法采用二维-三维混合特征提取策略，结合事件数据与三维物体模型检测角点和边缘。通过在时空窗口内最大化角点概率，实现角点的光流估计，再借助光流建立角点与边缘的相关性，最后通过优化角点与边缘距离来迭代优化6自由度物体位姿，实现连续跟踪。

Result: 无论在仿真还是真实事件数据上，方法在准确性和鲁棒性方面均优于现有的基于事件的物体跟踪方法。

Conclusion: 提出的方法利用事件相机的特性，有效提升了物体位姿跟踪的表现，为复杂环境下的多媒体感知应用提供了先进的技术方案。

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [34] [DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors](https://arxiv.org/abs/2512.21054)
*Kaustubh Kundu,Hrishav Bakul Barua,Lucy Robertson-Bell,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 本文提出DexAvatar，一种能够从单目手语视频中精准重建生物力学精细手部和身体动作的框架，相较于现有方法提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成技术依赖大量高精度2D/3D肢体姿态数据，但现有数据集多为视频、仅有粗略2D关节点数据且缺少高质量3D信息。自动3D姿态估计算法在手语视频中存在遮挡、噪声和模糊问题，导致重建效果差。

Method: 提出DexAvatar框架，通过学习获得的3D手和身体先验，从野外（monocular）单视角视频重建生物力学准确的细粒度手势与动作。

Result: 在唯一可用的SGNify动捕数据集上，该方法在手与身体姿态估计方面相比现有最佳方法提升了35.11%的性能。

Conclusion: DexAvatar能显著提升手语视频中3D肢体与手势的重建精度，为手语生成和理解提供了全新工具。

Abstract: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.

</details>


### [35] [Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control](https://arxiv.org/abs/2512.21058)
*Minghao Han,YiChen Liu,Yizhou Liu,Zizhi Chen,Jingqun Tang,Xuecheng Wu,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 该论文提出了UniPath，一个利用成熟诊断理解能力实现可控生成的语义驱动病理图像生成框架，并收集和公开了大规模高质量数据集，实验结果表明其在生成质量和语义控制上均达到业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 目前病理图像理解模型已达到医疗级水平，但图像生成模型主要关注像素层面模拟，缺乏精细语义控制。其主要问题包括高质量图文数据稀缺、缺少细粒度语义控制、以及诊断术语多样导致文本难以标准化。

Method: UniPath结合三路控制：1) 原始文本流，2) 诊断大模型冻结后提取的鲁棒语义token和属性捆绑的高层语义流，3) 组件级原型流进行形态学控制。同时，作者构建了265万配对图文大数据集和6.8万高质量细标注子集。设计了分层病理场景评测体系。

Result: UniPath在病理图生成行业多个基准上取得SOTA，Patho-FID高达80.9（比第二名高51%），语义控制精度高达98.7%。

Conclusion: UniPath不仅技术上提高了控制性和真实度，还公开了大规模高质量数据、源码与模型，加速了病理图像生成领域的发展。

Abstract: In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.

</details>


### [36] [Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition](https://arxiv.org/abs/2512.21064)
*Hongsong Wang,Heng Fei,Bingxuan Dai,Jie Gui*

Main category: cs.CV

TL;DR: 提出了一种自监督多模态骨骼动作表示学习框架，兼顾效率和性能，解决了多模态融合中的主要难题。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态动作识别方法在性能和计算效率之间难以兼顾。晚期融合虽然性能好但计算开销大，早期融合则效率高但性能不佳，需要新的方法平衡两者。

Method: 本文提出了一种“分解与组合”自监督学习框架。分解阶段将融合后的多模态特征拆分为单模态特征，并与真实单模态特征对齐；组合阶段将多种单模态特征融合，并引入自监督策略，提升多模态表示学习效果。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II三个数据集上实验，验证了该方法在计算成本和模型性能之间可以取得很好的平衡。

Conclusion: 新方法有效提升了多模态行为理解的效率和表现，为多模态动作识别开辟了一条兼顾精度和速度的新路径。

Abstract: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.

</details>


### [37] [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078)
*Tianchen Deng,Xun Chen,Ziming Li,Hongming Shen,Danwei Wang,Javier Civera,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉定位方法UniPR-3D，通过结合多视角信息，实现了对环境的更稳健识别，并创造了新的性能记录。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉定位任务中多视图具有明显优势，但现有研究对其探究较少且泛化能力有限。作者希望通过有效整合多视角信息，提升视觉定位的鲁棒性和泛化能力。

Method: 提出UniPR-3D架构，基于VGGT骨干网络，能够编码多视角的3D特征。作者设计了2D与3D特征的专用聚合模块，并采用单帧与多帧聚合策略以及可变长度序列检索，联合利用VGGT生成的2D和3D token，提高描述子的表现力。

Result: UniPR-3D在多个基准上超越了单视图和多视图对比方法，性能达到最新水平，验证了几何约束token在视觉定位中的有效性。

Conclusion: 该方法首次实现了多视角整合在视觉定位任务中的高效应用，同时提升了跨环境泛化能力，为后续研究提供了新的思路。

Abstract: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

</details>


### [38] [Hierarchical Modeling Approach to Fast and Accurate Table Recognition](https://arxiv.org/abs/2512.21083)
*Takaya Kawakatsu*

Main category: cs.CV

TL;DR: 本文提出了一种新型多任务表格识别模型，通过非因果注意力机制全面建模表格结构，并结合并行推理算法以加速单元内容的识别，在两个公开大数据集上验证了其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前从多文档中提取和利用丰富知识面临挑战，尤其表格元素识别需要不同方法。尽管多任务学习等方法提升了精度，但推理慢且原理说明不充分，因此需要更高效、可解释的模型。

Method: 提出基于非因果注意力机制的多任务模型，可以全局捕捉表格结构。同时设计并行推理算法显著加快单元格内容识别。

Result: 在两个大型公开表格数据集上进行可视化和统计评估，实验结果显示模型识别性能优越且推理速度快于现有方法。

Conclusion: 所提模型在表格识别任务中兼具高效推理和优异精度，具有更好的实际应用前景。

Abstract: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.

</details>


### [39] [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094)
*Zhe Cao,Tao Wang,Jiaming Wang,Yanghai Wang,Yuanxing Zhang,Jialu Chen,Miao Deng,Jiahao Wang,Yubin Guo,Chenxi Liao,Yize Zhang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了T2AV-Compass，一个用于全面评估文本到音频视频生成（T2AV）的基准，解决现有评估碎片化的问题，促进行业发展。


<details>
  <summary>Details</summary>
Motivation: 现有T2AV系统评估主要依赖单模态或有限的基准，难以衡量跨模态对齐、指令遵循和真实感，不能满足复杂场景下的全面评估需求。

Method: 作者构建了一个包含500条语义丰富、物理可行复杂提示的大型基准集，采用多层次评价体系：兼顾客观的信号级指标（画面、音频、跨模态同步）和主观的大模型评审（指令遵循、真实感）双重评价。

Result: 对11个代表性T2AV系统的评测显示，即使是目前最强模型，在音频真实感、细粒度同步和指令遵循上与人类表现差距较大，存在明显的改进空间。

Conclusion: T2AV-Compass为复杂T2AV任务提供了更具挑战性和诊断意义的统一评测基准，有助于推动该领域模型能力的持续进步。

Abstract: Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

</details>


### [40] [UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters](https://arxiv.org/abs/2512.21095)
*Yongkun Du,Zhineng Chen,Yazhen Xie,Weikang Baiand Hao Feng,Wei Shi,Yuchen Su,Can Huang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一个只有1亿参数的统一识别模型UniRec-0.1B，能在不同层次上高效识别文本和公式，并且模型小巧、速度快，识别效果优于大模型和同行模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本和公式作为文档核心信息内容，对它们的准确、高效识别是文档解析的重要基础。虽然大型视觉-语言模型（VLMs）在这方面表现优异，但其计算与存储消耗大，不适用于资源有限的应用场景。因此需要一个轻量级、通用且高效的识别模型。

Method: 1. 构建UniRec40M大规模数据集（包含文本、公式和混合样本共4000万）。2. 针对轻量统一模型存在的结构多样性和语义缠结难题，提出分层监督训练（显式指导结构理解）和语义解耦分词器（区分文本与公式表达）。3. 开发多语言、多领域、多层次的评测基准。

Result: 在自建和公开基准上，UniRec-0.1B 相比通用VLM和主流文档解析模型识别效果更佳，同时推理速度提升2-9倍。

Conclusion: UniRec-0.1B兼具高效能和高准确率，为文本与公式统一识别提供了计算友好、实用性强的新方案，有望推进文档解析系统的实际落地。

Abstract: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.

</details>


### [41] [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104)
*Chao Gong,Dong Li,Yingwei Pan,Jingjing Chen,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出FreeInpaint方法，通过在推理过程中直接优化扩散模型的潜变量，实现更加精准的文本引导图像修复，提高文本和修复区域的对齐度及视觉合理性，无需微调原有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本引导图像修复方法虽然生成效果逼真，但在用户提示的对齐及生成内容的视觉合理性上仍存在兼顾困难。研究者希望提升在修复区域与文本提示对齐度的同时，也保持视觉上的一致和自然。

Method: 提出FreeInpaint，一种即插即用且无需微调的方法。方法上，通过在推理阶段直接优化扩散模型的初始噪声，并结合专门为修复任务设计的复合引导目标，实现对中间潜变量的动态优化，引导模型更关注有效修复区域与文本语义。

Result: 通过大量实验，涵盖多种扩散修复模型与评测指标，验证了FreeInpaint在文本对齐度和视觉合理性上的有效性和鲁棒性。

Conclusion: FreeInpaint能显著提升文本引导图像修复任务中的对齐度与视觉结果，且适配性强，无需模型微调，为高质量文本引导修复提供了新的解决方案。

Abstract: Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

</details>


### [42] [MarineEval: Assessing the Marine Intelligence of Vision-Language Models](https://arxiv.org/abs/2512.21126)
*YuK-Kwan Wong,Tuan-An To,Jipeng Zhang,Ziqiang Zheng,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文提出了首个大规模海洋领域视觉语言模型（VLM）评测数据集MarineEval，并对现有VLM在该领域的能力进行了全面评测。结果显示，目前主流VLM在专业海洋领域问题解答上表现不佳，未来仍需大量改进，以提升其专业应用价值。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM在诸多领域取得进展，但其是否具备处理复杂和高专业性海洋领域问题的能力尚不明确。为了评估和推动VLM在专业领域的实用性，作者希望检验VLM是否能够胜任海洋领域专家的角色。

Method: 作者构建了第一个大规模的海洋视觉语言数据集MarineEval，包含2000组基于图像的问题与答案，涵盖7个任务类型和20种能力维度，同时邀请海洋领域专家参与数据审核。研究中对17个主流VLM模型进行了系统性能评测。

Result: 实验显示，现有17个VLM在MarineEval上的表现均无法有效解答专业海洋领域问题，存在显著的性能短板。

Conclusion: 现有VLM尚未准备好作为海洋领域的专家助手。MarineEval为未来相关研究提供了测试基准，鼓励学者关注专业领域任务的多样性和挑战性，以推动模型能力提升。

Abstract: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/

</details>


### [43] [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135)
*Gaoren Lin,Huangxuan Zhao,Yuan Xiong,Lefei Zhang,Bo Du,Wentao Zhu*

Main category: cs.CV

TL;DR: 本文提出一种名为TGC-Net的医学图文分割方法，通过结合CLIP基础模型与高效适配组件，实现更精确的医学图像分割，并显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本引导的医学图像分割方法面临三个挑战：1）细粒度解剖结构保留不足；2）对复杂临床文本建模能力弱；3）医学领域图文特征语义错配。作者希望在利用CLIP模型先进特征对齐能力的同时，解决这些医学专属问题。

Method: TGC-Net 基于 CLIP 框架，提出了三个核心模块：1）Semantic-Structural Synergy Encoder（SSE），通过加入CNN分支对ViT进行多尺度结构细化；2）Domain-Augmented Text Encoder（DATE），将大模型的医学知识注入文本编码器；3）Vision-Language Calibration Module（VLCM），在共空间内优化跨模态对齐。这样既利用了CLIP预训练优势，又进行针对性高效调整。

Result: 在五个胸部X射线和胸部CT公开数据集上的实验表明，TGC-Net在多个具有挑战性的分割基准测试中取得了最优性能（Dice分数提升明显），且参数量大幅减少。

Conclusion: TGC-Net 有效解决了CLIP在医学图像分割中的细粒度结构、专业语义和文本差异等问题，能够以更高效的参数适应任务，获得了最先进分割效果，显示出其在医学领域的应用前景。

Abstract: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

</details>


### [44] [ORCA: Object Recognition and Comprehension for Archiving Marine Species](https://arxiv.org/abs/2512.21150)
*Yuk-Kwan Wong,Haixin Liang,Zeyu Ma,Yiwei Chen,Ziqiang Zheng,Rinaldi Gotama,Pascal Sebastian,Lauren D. Sparks,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 该论文提出了ORCA多模态基准数据集，包含14,647张图像和综合注释，用于推动海洋视觉理解领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前海洋视觉理解进展缓慢，主要因为缺少高质量、系统化的数据集，且缺乏将海洋领域挑战和标准计算机视觉任务进行有效结合的任务设定。

Method: 作者构建了ORCA数据集，涵盖478个物种的14,647张图像、42,217个边界框和22,321条由专家验证的实例描述。针对三个任务（目标检测、实例描述、视觉定位）对18个主流模型进行了全面评估。

Result: 评测结果显示，物种多样性、形态相似和特定领域的需求使海洋视觉理解面临很大挑战，现有模型表现有限。

Conclusion: ORCA数据集为海洋视觉理解研究提供了全面的基准，有助于推动该领域方法和模型的发展。

Abstract: Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.

</details>


### [45] [A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation](https://arxiv.org/abs/2512.21174)
*Chenghao Xu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法（EFR）来解决小样本图像生成中源域与目标域的分布对齐问题，使用自旋转特征空间进行对齐以提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在很少的目标域样本下直接对齐源域与目标域分布，过于严格或过于宽松都可能导致生成效果下降。主要原因是源域和目标域分布结构差异，以及目标域样本稀缺难以精确估计分布。

Method: 提出等变特征旋转（EFR）策略：在可参数化的Lie Group下，对源域和目标域特征进行自适应旋转，映射到一个等变代理空间，在该空间进行对齐。旋转矩阵可学习，保证域内结构信息不丢失并促使跨域知识迁移。

Result: 在多种常用数据集上的综合实验表明，该方法在目标域的生成性能有显著提升。

Conclusion: EFR能有效缓解小样本图像生成中的域对齐难题，通过可学习特征旋转提升了模型的适应性与生成效果。

Abstract: Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.

</details>


### [46] [Towards Arbitrary Motion Completing via Hierarchical Continuous Representation](https://arxiv.org/abs/2512.21183)
*Chenghao Xu,Guangtao Lyu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新型的连续人类动作序列表示方法，可以对任意输入动作序列实现高质量插值、补帧甚至外推，支持任意帧率。方法基于分层隐式神经表征，并引入傅里叶变换驱动的参数化激活函数，提升了复杂动作表现能力，在多个数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有动作序列表征通常依赖离散帧，受限于固定帧率，难以平滑生成或复用动作，也难以捕捉动作中的复杂时序模式。因此，开发连续且能自如调整帧率的动作表征方式至关重要，可用于更自然的动画生成和动作分析。

Method: 作者提出一个基于隐式神经表征（INR）的分层时序编码框架NAME。该方法设计了一个多层次的时序编码器，能从不同时间尺度抽取动作特征，并在MLP解码器中引入了傅里叶变换驱动的参数化激活函数，提升了动作序列的连续表达与复杂时序关系建模能力。

Result: 在多个动作序列基准数据集上进行大量实验，结果证明本方法在动作插值、补帧、外推等任务表现优异，兼具高精度与强鲁棒性。

Conclusion: 本文首次实现了动作序列的连续、可参数化隐式表征，支持灵活生成和处理任意帧率的人体动作数据。该方法为动作生成、动画插值等相关领域提供了更高质量和更灵活的解决方案。

Abstract: Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.

</details>


### [47] [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185)
*Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan*

Main category: cs.CV

TL;DR: 本报告提出了UltraShape 1.0，一种可扩展的高保真三维几何生成扩散框架，通过两阶段生成管线，实现从粗略结构到细节精致的3D重建。同时，开发了全新的数据预处理流程，以提升公开3D数据质量，实验表明UltraShape 1.0在几何生成和数据处理方面均具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有三维几何生成方法在结构精度、细节重建和数据集质量方面存在局限，需要开发更高保真的3D生成框架，并提升公开3D数据集的可用性。

Method: 采用两阶段生成管线，先合成整体结构，再细化细节。数据端提出了闭壳处理和高质量过滤等新流程，提升数据集质量。建模方面，引入空间定位与细节生成解耦，利用RoPE编码的体素定位进行局部细节合成，模型全部在公开数据上训练。

Result: UltraShape 1.0在三维数据预处理和几何生成任务上，性能与现有开源方法持平或超越，且可在受限训练资源下获得高质量三维结构。

Conclusion: UltraShape 1.0为高保真3D几何生成提供了有力工具，并将公开相关代码和模型，有助于推动三维生成领域研究发展。

Abstract: In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

</details>


### [48] [VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs](https://arxiv.org/abs/2512.21194)
*Brigitta Malagurski Törtei,Yasser Dahou,Ngoc Dung Huynh,Wamiq Reyaz Para,Phúc H. Lê Khac,Ankit Singh,Sofian Chaybouti,Sanath Narayan*

Main category: cs.CV

TL;DR: 本文提出了VisRes Bench基准，用于系统评估Vision-Language Models在不含语言上下文监督的自然视觉场景中的推理能力，发现这些模型在面对感知扰动时表现接近随机，显示出视觉推理能力的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在包括视觉问答和图像描述等任务上取得了显著进展，但这些模型究竟是在进行真实的视觉推理，还是主要依赖于语言先验，尚不清楚。因此，作者希望通过专门针对视觉推理的基准，揭示并分析这些模型的真实能力。

Method: 作者提出了VisRes Bench基准，并设计了三个复杂性层级：1）对感知补全和全局图像匹配进行扰动测试（如模糊、纹理变化、遮挡和旋转）；2）基于单一属性（如颜色、计数、朝向）的规则推理测试；3）考察需要整合多个视觉属性的组合推理。总共涉及1.9万多张严格控制的任务图片。

Result: 最新的VLMs在面对较微妙的感知扰动时，表现几乎与随机猜测无异，显示出它们在感知和关系视觉推理方面存在明显不足，更多地依赖模式识别而不是抽象推理。

Conclusion: VisRes Bench为多模态研究中视觉抽象推理能力的提升提供了统一的测试框架，为未来更高级的视觉推理模型的发展指明了方向。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.

</details>


### [49] [Human Motion Estimation with Everyday Wearables](https://arxiv.org/abs/2512.21209)
*Siqi Zhu,Yixuan Li,Junfu Li,Qi Wu,Zan Wang,Haozhe Ma,Wei Liang*

Main category: cs.CV

TL;DR: 提出了一种基于日常穿戴设备的轻量化人体动作捕捉方法（EveryWear），无需复杂校准，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于穿戴设备的人体运动估计方法通常佩戴不便、成本高或校准繁琐，影响实际应用。作者希望开发一种基于日常智能设备（如智能手表、耳机、眼镜等）且实用的动作捕捉技术，推动相关应用普及。

Method: 方法基于日常穿戴设备（智能手机、手表、耳机、配有多摄像头的智能眼镜），无需显式校准。构建了Ego-Elec数据集，包括9小时、56类活动、17种场景的真实3D标注数据。模型采用多模态的teacher-student框架，融合穿戴摄像头的视觉信息和惯性传感器信号，并直接在真实数据上训练，避免了虚拟数据导致的领域差距。

Result: 实验证明，该方法在全身动作估计上优于现有基线模型，显示出更好的实用性和准确性。

Conclusion: EveryWear充分利用日常可穿戴设备，简化了动作捕捉流程，实现了高效、实用、无明显校准需求的运动估计，有助于进一步推动相关技术在实际生活中的落地应用。

Abstract: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.

</details>


### [50] [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218)
*Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种无需显式监督的视觉推理token机制，使大模型在视觉任务中有效提取信息，提升了多种视觉推理任务的能力，并取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多多模态大模型仍以语言为核心，难以处理以视觉推理为主的任务。现有方法依赖手工制作的中间视觉标注，通用性差且成本高，需要新的方式让模型自我发现有用的视觉推理信息。

Method: 设计了一种任务无关的机制，让模型在没有额外监督的情况下，通过引入和训练“视觉推理token”，实现主动聚焦和重编码图像，使模型学会适应不同任务自动提取有效视觉信息。

Result: 提出的方法优于直接微调，且在多种以视觉为核心的任务（包括难以人工指定中间表示的任务）上取得了最优效果。同时在多任务指令微调方面有较好泛化能力。

Conclusion: 通过无监督的视觉推理token机制，大幅提升了多模态模型在视觉推理任务的能力，减少了人工标注依赖，有效扩展了模型的应用范围。

Abstract: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>


### [51] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种高效且轻量的两阶段图像检索方法，结合事件驱动的实体提取与先进的多模态模型，在实际复杂场景下显著提升图像-文本检索性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像-文本检索因查询不明确、语言多样性和扩展性需求等问题具备很大挑战。现有方法难以兼顾准确率和效率，需要更智能且实用的方案。

Method: 方法分为两阶段：第一阶段利用基于BM25的候选过滤，聚焦于文本中的关键实体，提高初筛效率；第二阶段用BEiT-3多模态模型精细捕捉深层语义，对候选结果进行重排序，从而显著增强检索质量。

Result: 在OpenEvents v1基准测试中，该方法的平均精度为0.559，远超先前所有基线方法。

Conclusion: 结合事件感知的实体筛选与长文本视觉-语言建模，可以有效提升真实场景下图像检索的准确性与效率。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [52] [SegMo: Segment-aligned Text to 3D Human Motion Generation](https://arxiv.org/abs/2512.21237)
*Bowen Dang,Lin Wu,Xiaohang Yang,Zheng Yuan,Zhixiang Chen*

Main category: cs.CV

TL;DR: SegMo提出了一种基于分段对齐的文本生成3D人体动作的方法，在对齐文本和动作的细粒度上取得进展，并在重要指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法只在序列层面对齐文本与动作，忽略了两者内部可分解的语义结构，难以实现更细粒度的匹配。该研究希望利用动作和文本内部的原子语义单元，实现更精准的对齐生成。

Method: 提出了SegMo框架，包括：1）文本片段提取，将复杂文本分解为顺序、原子动作短语；2）动作片段提取，将完整动作切分为对应的动作片段；3）细粒度文本-动作对齐，利用对比学习将文本片段与动作片段对齐。

Result: 在两个主流数据集上，SegMo显著提升了效果，在HumanML3D数据集上TOP1分数达到了0.553，并支持动作检索等多种任务。

Conclusion: SegMo通过分段对齐在文本驱动动作生成和检索领域取得优异表现，为相关应用提供了新的方法基础。

Abstract: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.

</details>


### [53] [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252)
*Jiawei Liu,Junqiao Li,Jiangfan Deng,Gen Li,Siyu Zhou,Zetao Fang,Shanshan Lao,Zengde Deng,Jianing Zhu,Tingting Ma,Jiayi Li,Yunqiu Wang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 论文提出DreaMontage框架，实现由用户输入自由引导的高质量一镜到底（one-shot）视频生成，显著提升连贯性与表达力。


<details>
  <summary>Details</summary>
Motivation: 现实中拍摄一镜到底影片成本高且难度大，现有虚拟生成方法无法保证视觉连贯和时序一致，限制了该艺术形式的普及和创作自由。

Method: 方法包括：1. 在DiT架构中整合轻量中间调控机制，通过自适应调整策略提升任意帧控制能力；2. 构建高质量数据集并采用视觉表达微调，提出专用DPO策略，优化运动合理性与转场平滑性；3. 设计分段自回归推理策略，实现高效内存管理，支持长序列生成。

Result: 实验证明该方法生成的视频视觉效果突出、连贯性强且计算高效，实现高水平一镜到底视频的自动化创作。

Conclusion: DreaMontage方案赋能用户用碎片化输入自主创作完整、高质量一镜到底电影，推动该影视美学风格的虚拟实践与普及。

Abstract: The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

</details>


### [54] [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264)
*Changwei Wu,Yifei Chen,Yuxin Du,Mingxuan Liu,Jinying Zong,Beining Wu,Jie Dong,Feiwei Qin,Yunkang Cao,Qiyuan Tian*

Main category: cs.CV

TL;DR: 本论文提出一种可适应任意MRI模态缺失情况下的统一异常检测框架，实现了鲁棒的异常检测与定位，并在多项脑部MRI数据集和多种模态组合下超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床实际环境下由于标注异常病例稀缺和关键成像模态常缺失，现有异常检测方法依赖固定模态、重复训练，且泛化能力差，限制了实际应用价值。

Method: 构建了一个融合双通道DINOv2编码器和特征分布对齐机制的框架，通过统计学特征对齐补全缺失模态特征，并引入“内在正常原型（INP）”提取器及INP引导解码器，仅重构正常解剖结构以放大异常。训练时采用随机模态掩蔽和间接特征补全，实现无需重训即可适配各种模态组合。

Result: 在BraTS2018、MU-Glioma-Post和Pretreat-MetsToBrain-Masks等三个公开脑部MRI数据集，以及7种模态组合测试下，该方法均优于现有工业和医学异常检测基线模型，表现出更强自适应和泛化能力。

Conclusion: 提出的方法为多模态医学影像异常检测提供了高度可扩展性的新范式，具有解决真实场景中模态不完备问题的应用前景。

Abstract: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

</details>


### [55] [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](https://arxiv.org/abs/2512.21268)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为注意力条件扩散（ACD）的新型视频扩散模型，能够通过监督模型注意力实现对条件的直接控制，提升了视频合成的可控性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成对条件的精确控制需求越来越高，但现有的无分类器引导或基于分类器的方法在精准对齐条件方面表现有限。本研究旨在解决这些不足，提升视频扩散模型的控条件效果。

Method: 本文提出ACD方法，通过将模型的注意力图与外部控制信号对齐，实现直接的条件控制。具体做法包括引入稀疏3D物体布局作为高效的条件输入、设计专用的布局ControlNet，以及自动化注释流程以便大规模集成。

Result: 在视频生成基准数据集上的大量实验表明，ACD方法能显著提升模型对输入条件的对齐度，同时保持视频时序连续性和视觉质量。

Conclusion: ACD提供了一套高效且可扩展的视频条件生成新范式，为未来高可控视频合成奠定了基础。

Abstract: Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.

</details>


### [56] [GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation](https://arxiv.org/abs/2512.21276)
*Snehal Singh Tomar,Alexandros Graikos,Arjun Krishna,Dimitris Samaras,Klaus Mueller*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成图像序列的方法，通过先生成低分辨率序列后再提升单帧分辨率，显著提升生成质量与速度。该方法无需复杂的结构调整，可推广至多种数据集，并在现有方法上实现了更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常将图像序列简单堆叠成大张量，但这种方式在生成任务中存在效率低、延展性差等瓶颈。作者希望找到一种更有效的图像序列生成方式，以克服当前技术的局限。

Method: 作者的方法分为两步：首先，用Diffusion Transformer(扩散变换器)直接对低分辨率的帧网格进行建模，捕获帧间相关性，相当于把2D生成器扩展为低分辨率的3D序列生成器；然后，对每帧独立进行超分辨处理，补充高分辨率细节。无需对网络结构做额外修改。

Result: 与现有生成模型相比，本文方法在合成质量、序列一致性、任意长度生成和推理/训练效率方面都取得了更优的表现。实验证明，该方法不仅在质量上超越SoTA，在速度上也至少快一倍，并可有效泛化到不同数据域。

Conclusion: 本文提出的分解式图像序列生成方法有效克服了当前主流方法的瓶颈，不仅提升了生成效果和效率，还具备良好的拓展性和通用性，为相关生成任务提供了一种优异的解决方案。

Abstract: Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

</details>


### [57] [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284)
*Shihao Zou,Jingjing Li,Wei Ji,Jincai Huang,Kai Wang,Guo Dan,Weixin Si,Yi Pan*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SpikeSurgSeg的尖峰神经网络（SNN）视频Transformer框架，实现了高效、实时的外科手术场景分割，可在非GPU设备上运行，并大幅降低了延迟与能耗。该方法在多个外科场景数据集上展现了与主流人工神经网络（ANN）模型媲美的分割精度，同时推理速度提升至少8倍，对基准基础模型更有超过20倍的加速优势。


<details>
  <summary>Details</summary>
Motivation: 现有的手术场景分割方法多依赖高算力和高能耗的深度学习模型，不适用于资源受限的手术环境。要实现手术过程中的实时智能理解，需要开发兼顾精度和计算效率的分割框架。此外，外科手术视频标注数据稀缺，也对建模带来挑战。

Method: 作者提出了SpikeSurgSeg，是首个面向手术视频的尖峰驱动Transformer分割框架。主要方法包括：1）引入手术场景掩码自编码预训练策略，通过管状(mask tube)遮罩进行SNN的稳健时空表征学习；2）基于预训练主干构建轻量级尖峰分割头，实现时序一致且低延迟的分割输出。系统可在非GPU平台具备实时潜力。

Result: 在EndoVis18和自建SurgBleed数据集上的实验显示，SpikeSurgSeg的mIoU分割精度可与现有SOTA的ANN模型媲美，但推理延迟减少至少8倍；相比主流基础模型推理加速超过20倍。

Conclusion: SpikeSurgSeg为手术场景分割带来了卓越的效率与实时性，在不降低精度的前提下显著降低了推理延迟和对计算资源的依赖，具有在实际手术环境部署的巨大潜力。

Abstract: Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

</details>


### [58] [Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction](https://arxiv.org/abs/2512.21287)
*Suren Bandara*

Main category: cs.CV

TL;DR: 本文提出了一种新的多尺度信号处理方法，从噪声或分辨率不佳的表格图像中精准检测表格分割界线，显著提升了表格结构化数据提取的准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法对低质量图像（如低分辨率、噪声、数据不完整）适应性差，传统的掩码边缘检测容易受噪声影响或计算资源消耗高，因此亟需鲁棒且高效的表格结构检测方法。

Method: 将表格的行、列边界视为一维信号，通过逐步递增方差的高斯卷积去噪，再用统计阈值检测边界，最后将检测得到的信号峰值映射回图像以获得准确的分割边界，同时利用零填充和缩放提升对分辨率变化的适应性。

Result: 在PubLayNet-1M基准测试上，结合TableNet和PyTesseract OCR，该方法使列边界检测的Cell-Aware Segmentation Accuracy（CASA）指标从67%提高到76%。

Conclusion: 该方法对分辨率和噪声具备良好鲁棒性，能生成高质量的结构化表格数据，优化下游数据分析流程。

Abstract: Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.

</details>


### [59] [AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents](https://arxiv.org/abs/2512.21302)
*Yue Cao,Yingyao Wang,Pi Bu,Jingxuan Xing,Wei Jiang,Zekun Zhu,Junpeng Ma,Sashuai Zhou,Tong Lu,Jun Song,Yu Cheng,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了AndroidLens评测框架，用于挑战和评测移动端GUI智能体在复杂、长步骤任务上的表现，涵盖中文和英文环境，任务难度高，现有模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 目前GUI智能体的评测受限于应用数量少、任务简单、指标粗糙，缺乏能反映真实复杂场景的基准。为了推动该领域的发展，亟需一个多样性高、细粒度的评测框架。

Method: 作者开发了AndroidLens，包含571个跨38个领域、平均26步以上复杂度的移动GUI任务（中英文）。任务来源于真实用户场景，包含多目标、多约束、专业领域等类型。框架支持静态评测（保留异常、允许多路径减少偏差）和动态评测（基于里程碑的细粒度测量ATP）。

Result: 在AndroidLens上现有最优模型的任务成功率仅12.7%，ATP也仅为50.47%。表明当前模型在真实复杂环境下效果有限。

Conclusion: AndroidLens为移动GUI智能体提供了具挑战性的真实世界评测平台，揭示了当前模型在环境异常、自适应探索和长期记忆方面的巨大挑战，对推动该领域技术进步具有重要意义。

Abstract: Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.

</details>


### [60] [TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning](https://arxiv.org/abs/2512.21331)
*Varun Belagali,Saarthak Kapse,Pierre Marza,Srijan Das,Zilinghan Li,Sofiène Boutaj,Pushpak Pati,Srikar Yellapragada,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Prateek Prasanna,Stergios Christodoulidis Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: TICON是一种基于Transformer的图像块表示上下文化方法，可对病理大图像的任意tile模型输出进行统一、增强，有效提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 病理切片大图（WSI）常被分割成多个小tile分析，但传统方案通常忽略tile间的上下文信息，不同tile-encoder模型适用的下游任务也不同，亟需统一方法提升tile及全图级别表达能力。

Method: 提出TICON，一个基于Transformer的上下文编码器，通过mask预测学习目标，对任何tile-level基础模型的嵌入进行统一和上下文化处理；可作为通用模块融入各类计算病理任务。

Result: TICON产生的嵌入在多个tile级、slide级基准任务（HEST-Bench、THUNDER、CATCH、Patho-Bench）大幅刷新了最新性能，同时该方案预训练的slide级模型以远少于他法的数据量超过了主流SoTA。

Conclusion: TICON方法极大提升了计算病理中tile表达的上下文感知能力，推进了tile到slide多层次任务的统一和性能极限。

Abstract: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.

</details>


### [61] [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333)
*Avilasha Mandal,Chaoning Zhang,Fachrina Dewi Puspitasari,Xudong Wang,Jiaquan Zhang,Caiyan Qin,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种文本引导的token剪枝框架，提升了SAM2模型在视频对象分割任务中的推理效率，显著减少计算与显存消耗，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM2等基础视觉模型在基于提示的视频对象分割任务上表现突出，但由于需处理大量视觉token并进行下游时序推理，导致计算与显存开销很大，限制了其实用性和扩展性。

Method: 作者在视觉编码之后、时序推理之前，引入基于文本引导的轻量级token剪枝机制。该机制综合利用局部视觉上下文、根据对象描述文本获得的语义相关性以及不确定性信息，对token进行打分，只保留信息量最大的token用于后续处理，无需改动原有分割架构。

Result: 在多个视频分割基准上，所提方法实现了推理速度提升高达42.5%、GPU显存消耗降低37.41%，并能维持与未剪枝SAM2相当的分割精度（J和F评分）。

Conclusion: 早期token选择和剪枝可有效提升变换器类视频分割模型的可扩展性，为实时和资源受限应用提供了可行且高效的技术路径。

Abstract: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

</details>


### [62] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了Streamo，这是一种实时流视频大模型助手，可支持多种视频相关的交互任务，如实时解说、动作识别、事件描述、事件定位及时间敏感问答，突破了以往仅限于答题或字幕生成的局限，并可统一多任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有的在线视频大模型功能较狭窄，通常局限于视频问答或字幕生成，难以满足多样化、实时性的交互式视频理解需求，因此需要探索更通用、丰富的视频助手。

Method: 作者构建了一个大规模、覆盖多任务和多时间语境的流视频指令集数据集Streamo-Instruct-465K，并基于其进行端到端统一训练，从而使模型具备良好的时序推理及多任务泛化能力。

Result: Streamo在多个主流流视频基准任务上展现出强劲的时序推理、响应交互和多任务泛化能力，超越了现有离线视频理解模型，并能高效支持实时多模态视频助手应用。

Conclusion: Streamo有效弥合了离线视频感知与实时多模态助手之间的差距，向实现统一智能的连续视频流理解迈出了一步。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


### [63] [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/abs/2512.21337)
*Li-Zhong Szu-Tu,Ting-Lin Wu,Chia-Jui Chang,He Syu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该论文发现主流视觉-语言模型（VLMs）在处理著名建筑图片时准确率远高于普通建筑，显示出严重的“流行度偏置”。作者引入了大规模开放数据集YearGuessr并制定新指标，系统性量化并分析了模型偏向记忆而非泛化理解的问题。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs在视觉推理和理解任务中表现突出，但对模型泛化能力的质疑逐渐增加，特别是在处理未见或不知名样本时。作者关注到这些模型可能更多依赖于对流行、见过事物的记忆而非真正理解，因此希望揭示并量化这种流行度偏置。

Method: 作者构建了YearGuessr数据集，涵盖55,546张来自157国的建筑图片，带有建筑年份（作为序数标签）、GPS和流行度代理指标（浏览量）。以序数回归的方式预测建筑年代，并提出了“流行度敏感区间准确率”新指标。以该数据集对30多个VLMs模型（包括自家模型YearCLIP）做系统性benchmark分析。

Result: 实验证实：现有VLMs模型在著名、高流行度建筑预测任务上表现大幅优于普通不知名建筑，准确率相差最高达34%。说明这些模型较多依赖对流行事物的记忆。未被记忆的新样本上表现明显较弱。

Conclusion: 主流视觉-语言模型存在严重的流行度偏置问题，泛化能力有限。未来模型需在提升泛化推理能力方面做改进，不能仅靠记忆实现高性能。

Abstract: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/

</details>


### [64] [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/abs/2512.21338)
*Haonan Qiu,Shikun Liu,Zijian Zhou,Zhaochong An,Weiming Ren,Zhiheng Liu,Jonas Schult,Sen He,Shoufa Chen,Yuren Cong,Tao Xiang,Ziwei Liu,Juan-Manuel Perez-Rua*

Main category: cs.CV

TL;DR: HiStream通过空间、时间和时间步三重压缩机制，大幅提升高分辨率视频生成速度，实现超过百倍加速且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于高分辨率视频生成时计算复杂度高，推理速度慢，难以实用化。作者希望提升效率，使高分辨率视频生成更实用和可扩展。

Method: 提出HiStream高效自回归框架，从空间、时间和时间步三个维度减少冗余：空间压缩（低分辨率去噪+高分辨率精炼）、时间压缩（分块处理，锚点缓存）、时间步压缩（后续分块减少去噪步数并利用缓存）。

Result: 主模型(空间+时间压缩)在1080p基准上达到SOTA视觉质量，比Wan2.1基线快76.2倍且质量损失极小。更快的HiStream+（加上时间步压缩），加速达107.5倍，实现速度和质量的良好平衡。

Conclusion: HiStream和HiStream+大幅提升了高分辨率视频生成的速度，几乎无质量损失，推动该技术在实际数字媒体和影视领域的落地应用。

Abstract: High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: 本论文提出了一种基于稀疏自编码器(SAEs)的新方法，可以自动发现大语言模型(LLMs)在基准测试上的细粒度能力缺陷（模型缺口）和基准自身的覆盖不足（基准缺口），实现了更细致、概念级别的评估。


<details>
  <summary>Details</summary>
Motivation: 传统的聚合指标可能掩盖了模型在特定子领域的弱点和基准测试自身的不均衡覆盖，研究者希望通过更深入的分析揭示模型和基准测试中的具体短板，推动模型和评估手段的进步。

Method: 使用稀疏自编码器自动提取模型内部的概念激活，对基准数据施加显著性加权后，获得概念层面的性能分数，以揭示模型和基准测试的具体缺口。该方法无需人工监督，通过模型内部表示进行能力和评估分解，并可跨不同基准进行比较。

Result: 通过在两个开源模型和十个基准上的实例验证，发现模型在与奉承行为相反的能力（如礼貌地拒绝请求、设定界限）和安全相关的概念上表现不佳；同时发现许多基准过度代表与服从、权威等相关的概念，忽略了其范围内的核心概念。

Conclusion: 提出的基于模型表示的能力缺口评估方法能够自动细粒度揭示模型和基准的不足，是传统聚合分数的有力补充。该方法有助于理解模型表现原因，并指导基准的完善和模型能力的提升。

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [66] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本论文提出了SA-DiffuSeq，一种结合稀疏注意力机制的扩散模型，用于高效地生成长文本。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的长文本生成方法在面对长序列时，面临高昂的计算和内存开销。作者希望通过方法改进，提升长文生成的计算效率和扩展性。

Method: 作者提出了一种名为SA-DiffuSeq的扩散框架，该方法将稀疏注意力机制整合进扩散过程，并引入了适用于稀疏注意力的软吸收态，以稳定扩散轨迹、加速序列重建，提高对长距离依赖的建模能力。

Result: 在多个实验中，SA-DiffuSeq无论在训练效率还是采样速度上都明显优于现有的扩散模型，尤其是在超长序列任务中提升更为显著。

Conclusion: 结构化稀疏注意力机制的引入显著提升了扩散生成模型在长文本生成任务中的效率与表现，适用于科学写作、大规模代码生成及长对话等场景。

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [67] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: 本文提出了TokSuite，一个用于研究分词器对语言模型（LM）影响的工具和基准，系统性地评估了分词方案对模型性能的实际影响。


<details>
  <summary>Details</summary>
Motivation: 现有对分词器的研究有限，分词方案作为语言模型文本处理的基础，如何影响模型性能仍不清楚。缺乏能够单独评估分词器影响的系统性方法和基准。

Method: 作者设计并训练了十四个仅分词器不同、其他（如架构、数据、训练预算、初始化等）完全一致的模型，并开发了新的基准，用以模拟现实条件下可能影响分词并衡量模型性能的扰动。

Result: TokSuite能有效剥离分词器对模型的影响，比较了不同主流分词器在各种扰动场景下的优劣，首次提供了系统对比分析结果。

Conclusion: TokSuite为深入理解分词器如何塑造语言模型行为提供了坚实工具和基准，也揭示了分词器设计的重要性及其实际影响。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [68] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 本文提出了一种对抗训练框架，用于提升面向任务的对话系统（TOD）中用户模拟器的真实性，尤其适用于心理健康支持聊天机器人。实验表明，该方法能够更有效地暴露系统缺陷，比未微调模型表现更优。最终，获得的用户模拟器能高度还原真实失败率分布，并为系统上线前的评估提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 目前，面向任务的对话系统需要高真实度的用户模拟器进行训练和评估，然而现有模拟器常无法准确反映人的行为，限制了对系统失败点的发现与改进能力。提升模拟器的真实性是提升对话系统整体性能与可靠性的关键。

Method: 作者设计了一种对抗训练框架，将用户模拟器作为生成器，与判别器进行迭代训练。其中生成器生成用户行为，判别器区分模拟行为与真实行为。不断的对抗训练促使生成器生成更逼真的仿真数据。研究以心理健康支持对话为应用场景，并进行了微调、零样本测试和多轮对抗比较。

Result: 微调后的模拟器相比零样本基础模型，更能揭示聊天机器人的系统缺陷。对抗训练后，用户行为的多样性、分布一致性和预测效度均提升，模拟器生成的失败模式与真实分布高度相关。判别器准确率随对抗轮数显著下降，反映出行为逼真度的提升。

Conclusion: 对抗训练是提升面向任务对话系统用户模拟器真实性的有效方法，尤其适用于心理健康领域。该框架可帮助开发者在系统上线前快速、低成本地评估和优化对话机器人。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [69] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本文比较了专家型导师、新手导师和多种大语言模型在数学辅导中的表现，发现大语言模型与专家水平相当，但策略和语言表现不同。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注大语言模型在数学辅导中的应用，但模型的教学行为与专家人类导师的一致性仍不确定。本文希望深入比较大语言模型与人类导师在具体教学反应上的异同。

Method: 采用受控实验，对专家型导师、新手导师、以及多个大语言模型面对相同数学对话的回复，在教学策略（如复述、强调准确性等）和语言特性（词汇多样性、可读性、礼貌度、主动性等）方面进行了详细分析与比较。

Result: 发现大语言模型的感知教学质量平均接近专家水平，但在复述策略使用率较低，回复更长、词汇更丰富、更有礼貌。统计分析还表明：复述、词汇多样性、追求准确性与高教学质量正相关，而主动性和礼貌度过高反而负相关。

Conclusion: 最新的大语言模型在感知教学质量上与专家人类导师相当，但依赖的教学和语言策略不同。评估智能辅导系统时需重视其具体教学方法和语言特性分析。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [70] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: 本文探讨如何高效且彻底地从大参数量的LLMs中删除特定信息，同时尽量减少对其他知识的干扰。作者通过模型编辑算法实现部分优于基础unlearning方法的遗忘效果，但仍存在性能权衡难题。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型（LLM）中，如何高效、彻底地删除不需要的信息，同时保持模型对其他知识的完整性，是机器unlearning领域的重要挑战。现有方法往往效率低下或导致模型遗忘不完全、性能大幅下降，因此需要新的思路。

Method: 作者考察了现有的模型编辑算法（ROME、IKE、WISE），并为unlearning情境设计了新的编辑目标，实验对比了这些方法与传统unlearning技术在删除信息时的表现和对保留知识的影响。

Result: 在某些设置下，模型编辑方法在删除目标信息时优于传统unlearning方法，但像传统方法一样，这些编辑算法在不损害模型整体性能的前提下，难以严格限定需被遗忘信息的范围。

Conclusion: 模型编辑方法为LLM中的unlearning问题提供了有效补充，但在遗忘性能与模型整体能力之间依然存在难以避免的平衡问题，需要进一步研究。

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [71] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 本文探究了大语言模型中的人口统计偏见机制与识别能力是否独立，并通过多任务实验检验了在去偏的同时是否能保留模型对人口属性的识别。作者比较了两种定位偏见特征的方法，并发现针对性的信号消融可以在不损害识别能力的前提下减少偏见，并提出不同偏见应采用不同的技术干预。


<details>
  <summary>Details</summary>
Motivation: 当前大模型经常在包含人名、职业、教育等任务中表现出针对性的人口统计偏见，但我们期望去除偏见的同时保留模型识别人口属性的能力。因此，研究去偏方法对两者的影响机制具有现实意义。

Method: 作者设计了多任务评估框架，将人口属性与人名、职业、教育等关联，采用特征归因法（attribution-based）和相关性法（correlation-based）定位模型偏见特征，并用稀疏自编码器进行特征消融，比较不同方法对模型偏见消减和识别性能的影响。

Result: 实验发现，基于归因的特征消融能降低职业中的种族和性别刻板印象，并不影响人名识别；而相关性方法则更适合教育偏见。定性分析还发现错误地在教育任务中消融归因特征反而会加剧整体偏见。

Conclusion: 论文表明，人口统计偏见主要来自具体任务机制，而非对人口属性的整体识别。通过机制层面的特定干预，能实现去偏同时保留核心能力，对后续模型公平性提高和技术落地具有指导意义。

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [72] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 本文通过实验探讨当前大语言模型（LLMs）在面对新颖符号系统时的推理与抽象能力，发现模型往往受表层语义误导，推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型常被用于涉及人类价值判断的决策任务，但其是否具备抽象与符号推理能力存在疑问。尤其在遇到语义误导时，模型是否还能做出准确判断与推理，是决定其安全性与可靠性的关键。

Method: 作者设计了一套实验，将常规数字和运算符用新符号替代，通过这种“语义欺骗”考察模型是否能够抵制误导性语义，依然正确地进行符号运算。实验覆盖了四种不同的LLMs，对比其表现。

Result: 结果显示，语义误导极大影响了LLMs的推理表现。即便在非常简单的任务中，大模型也表现出对学习到的表层语义的过度依赖，抽象和符号操作能力薄弱；连“思维链（chain-of-thought）”推理方式也会加重这种依赖。

Conclusion: 研究揭示了现有LLMs在处理符号信息时的重大局限，对其推理能力应保持警惕。特别是在决策任务中，这种易受语义影响的弱点，可能带来伦理和社会风险。未来大模型的设计应关注真正的符号推理能力以保障其可靠性。

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [73] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: 论文提出EssayCBM系统，通过对8个写作关键概念分开评分，提高作文自动评分的可解释性，并实现和黑盒模型相当的评分表现。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分系统往往像黑盒，缺乏透明度，老师和学生难以理解评分依据，导致反馈无法直接指导改进。

Method: 通过构建EssayCBM，在编码器基础上为每个关键写作概念（如论点清晰度、证据使用等）设置独立评分头，生成各概念得分后再用简洁网络计算总分。提供可视化界面，允许老师调整概念分并实时观察总分变化。

Result: EssayCBM在透明度大幅提升的同时，评分性能不逊色于传统黑盒模型，并能提供具体概念层面的反馈。

Conclusion: EssayCBM兼顾评分准确性和可解释性，促进师生间更高效、可控的作文评价和反馈过程。

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [74] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出MediEval评测基准和CoRFu微调方法，提升了大语言模型在医学领域的准确性与安全性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在医学中应用广泛，但由于可靠性和安全性问题，其普及受限。现有评测方法要么只单独考察医学知识，要么只针对病患推理而未验证正确性，存在重要空白。

Method: 作者构建了MediEval基准，将MIMIC-IV电子健康档案(EHRs)与UMLS等生物医学知识库结合，生成真实患者语境下的多样医学陈述，结合4象限评测框架衡量知识真实性与情景一致性。此外，提出基于DPO的CoRFu微调方法，通过不对称惩罚机制重点减少模型的误判与幻觉输出。

Result: 使用MediEval发现主流LLM存在“幻觉支持”和“真伪颠倒”等关键失败模式。CoRFu方法能使模型的macro-F1值提升16.4%，并彻底消除真伪颠倒错误。

Conclusion: MediEval基准和CoRFu方法能系统提升医学LLM的可靠性与安全性；为医学应用落地提供了更坚实的保障。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [75] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3 Nano 30B-A3B 是一种高效的混合型 Mamba-Transformer 混合专家语言模型，在准确性、速度和推理能力上相较前代和同类模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型追求更高效、更强推理和对话能力，但通常成本高、推理速度慢。该工作旨在通过技术创新提升效率和准确性，从而降低部署和应用门槛。

Method: 作者提出 Nemotron 3 Nano 30B-A3B，结合混合专家（MoE）与 Mamba-Transformer 架构，并在包含 25 万亿词的超大文本数据集上进行预训练（新数据量远超 Nemotron 2），随后融合了有监督微调和大规模强化学习（RL）。

Result: Nemotron 3 Nano 在推理时仅激活不到一半参数，准确率超过上代 Nemotron 2 Nano，并在推理速度上达到同体量开放模型的 3.3 倍，诸如 GPT-OSS-20B 和 Qwen3-30B 等。同时在常用基准上表现更佳，支持最长 1M 上下文。

Conclusion: Nemotron 3 Nano 30B-A3B 兼具高效能与高准确性，显著提升了推理、对话和长上下文处理能力，对相关领域有重要推动，并已在 Hugging Face 发布模型权重。

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [76] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 本文针对在大型、动态知识库中检索相关文档数量未知的现实场景，提出了一种无需已知相关文档总数的检索质量评估方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实检索场景下，查询相关文档的总数往往未知，导致召回率指标无法计算，影响检索系统的效果评估。因此需要寻找无需相关文档总数的新评估方法。

Method: 本文衡量了多种已有检索评估策略与基于大语言模型（LLM）对检索结果生成应答质量的相关性，遍历多个相关文档数量较少（2-15）的数据集。同时，提出了一种简单的新检索质量指标，无需相关文档总数知识。

Result: 实验展示了该指标与LLM判定的应答质量有良好相关性，在具体检索任务中的评估表现不逊于传统指标。

Conclusion: 新提出的检索质量指标可以在不知相关文档总数的情况下，有效评估检索效果，适用于实际大型知识库检索应用。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [77] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3 系列模型（Nano、Super、Ultra）具备强大的智能体、推理和对话能力，采用混合专家架构支持更长上下文和更高效率。Nano具有较高准确性且推理成本低，Super适合高负载自动化，Ultra拥有最先进的准确率与推理表现。Nano已开放，Super和Ultra即将发布，权重、软件和可分发数据都会开放。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在推理能力、对话性、多步工具调用和效率上存在局限，尤其是在需要长上下文处理和高吞吐场景下。Nemotron 3 的提出旨在提升这些方面的性能，并促进模型的开放共享。

Method: Nemotron 3 采用混合专家（Mixture-of-Experts, MoE）的 Mamba-Transformer 架构，Super 和 Ultra 有 NVFP4 与 LatentMoE 技术提升质量，并集成 MTP 层加速文本生成。所有模型都通过多环境强化学习后训练，实现推理和多步工具调用能力，并支持推理预算控制。

Result: Nano 模型在同级别模型中准确性表现突出且推理成本低；Super 针对高并发与协作场景优化，适合如 IT 工单自动化等高负载任务；Ultra 在准确率和推理能力上达到最新水平。

Conclusion: Nemotron 3 系列不仅在推理能力和成本效率上取得突破，也承诺开放模型权重及相关开发资源，将推动开放大模型社区的发展和应用。

Abstract: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [78] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 本文系统性研究了在计算资源受限下，小型语言模型的架构选择和训练预算如何交互影响其性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型取得了显著进展，但在实际应用中，许多场景需要高效的小型模型。小模型如何在有限计算下提升表现，及大模型的成功方法是否同样适用，是未解问题。

Method: 作者从最基础的线性预测器出发，逐步引入非线性、自注意力机制和多层transformer结构，并在Tiny Shakespeare字符级、PTB与WikiText-2词级数据集上系统比较这些架构。通过实验，评估测试集负对数似然（NLL）、参数量和训练FLOPs，分析模型精度与计算效率的权衡。还特别研究了RoPE位置编码在小模型下的效果。

Result: 实验发现，即使在小规模下，基于注意力的模型在每FLOP计算效率上优于MLP模型。但如果简单增加模型深度或输入上下文而没有强化优化，反而可能导致性能下降。此外，RoPE等大模型常用方法未必适用于小模型。

Conclusion: 小型语言模型的架构必须在效率和精度之间权衡，且不能简单照搬大模型成功经验。设计小模型时需针对其特定限制进行调整优化。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [79] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理蒸馏能力溯源分析框架，并基于该框架探索和提升学生模型对教师推理能力的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏技术通过让学生模型学习教师模型的推理路径来提升学生的推理能力，但过去对此类模型在新环境中的泛化能力及其能力来源缺乏系统分析。主要问题包括：学生模型在测试时究竟能否保持与教师一致的推理行为，是否会退化回原有模式，以及蒸馏模型能力提升的真正来源何在。

Method: 作者提出了cross-model Reasoning Distillation Provenance Tracing（推理蒸馏溯源追踪）框架。具体做法是，对于每个由蒸馏模型生成的行为（如一句话），分别获取教师、原始学生和蒸馏学生在相同上下文下的预测概率，进行比较并对行为归类，从而拆解每步推理的能力来源。此外，提出了一种基于教师-学生分歧度的数据选择方法，直接利用概率分布差异作为训练数据筛选依据。

Result: 实验证明，在测试情境下，蒸馏模型能够生成来源于教师模型的推理行为，这与蒸馏模型的性能提升密切相关。教师引导的数据选择策略进一步验证了在多种教师与学生模型组合下的有效性。提出的溯源框架对理解与提升推理蒸馏效果具有显著意义。

Conclusion: 推理蒸馏模型在新场景下具有一定泛化能力，其推理能力的提升主要来源于对教师推理行为的迁移。溯源分析方法和优化数据选择策略有助于更全面和系统地提升和评估推理蒸馏效果。作者希望通过公开此分析工具和洞察，推动推理蒸馏领域的进一步发展。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [80] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: 本论文提出FEND框架，用多模态（语音+文本）方法对多种神经精神类疾病进行早期检测，并在多语言、多数据集上进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有神经精神疾病的自动检测方法在多语言泛化和统一评估基准方面存在不足，缺乏能兼容多病种、多语种、跨寿命阶段的全面检测框架。

Method: 提出FEND框架，将语音与文本两种模态融合，测评对象包括阿尔茨海默症、抑郁症和自闭症。依托13个英语、中文、希腊语、法语和荷兰语的多语种数据集，系统对比多模态与单模态效果，并进行跨语种、跨任务实验。

Result: 多模态融合在阿尔茨海默症和抑郁症检测上效果优于单模态，但在自闭症检测上表现较差。普遍存在模态不均衡问题：多模态结果并未优于最佳单模态模型。跨数据集及多语种、跨任务测试时，模型性能显著下降。

Conclusion: FEND实现了多语种、全寿命周期、可复现的神经精神疾病自动评估，为领域内提供了新基准和性能影响因素分析。建议研究者采用FEND框架促进公平对比和研究复现。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [81] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang,Hongzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经网络的轻量级MLP探针方法，实现对大型语言模型生成内容的逐Token幻觉检测，并在多个基准数据集上超越了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成和问答任务上表现出色，但常常产生事实性错误（幻觉），尤其在高风险场景严重影响其应用价值。现有幻觉检测方法依赖不确定性估计和外部知识库，但在高置信下仍可能错误，且过度依赖检索效率和知识覆盖。因此，亟需更高效、可靠的检测机制。

Method: 作者提出冻结大语言模型参数，基于神经网络（MLP）探针对高层隐藏状态进行非线性建模，并采用多目标联合损失函数，以增强检测的稳定性和消歧能力。此外，通过建立探针插入层的位置-性能响应模型，并利用贝叶斯优化自动搜索最优插入层，以获得最佳效果。

Result: 在LongFact、HealthBench和TriviaQA等数据集上的实验证明，该MLP探针方法在保持低误报的前提下，在准确率、召回率和整体检测能力上均显著优于SOTA方法。

Conclusion: 神经网络型探针尤其是MLP探针对大模型幻觉内容检测具有高效、实时、低依赖的优点，为高风险领域大模型的安全应用提供了新的方向。

Abstract: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [82] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 本文提出了一种用于多语言和跨语言事实核查声明检索的新系统TriAligner，结合对比学习与多模态数据提升检索性能，并在相关基准上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 在虚假信息传播日益严重的今天，事实核查变得尤为重要。然而，现有的多语言和跨语言声明检索方法在准确性和鲁棒性方面仍有不足。因此，作者希望通过新方法提升多语言环境下的事实核查自动化和准确性。

Method: 作者提出TriAligner方法，采用双编码器架构并结合对比学习，能处理多种模态的原生及英文翻译数据。利用大型语言模型进行高效的数据预处理和增强，同时引入困难负样本采样机制，进一步提升表征能力。

Result: 实验结果显示，TriAligner在多语言和跨语言检索任务上的准确率和事实核查性能均明显优于现有基线方法。

Conclusion: TriAligner系统通过结构创新和数据增强，显著提升了事实核查声明在多语言和跨语言环境下的自动化检索效果，为对抗全球虚假信息传播提供了有力技术支持。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [83] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出在蛋白质和RNA等生物序列语言模型中实现类Chain-of-Thought（CoT）推理能力的新方法，通过扩展token集合实现中间推理，从而提升模型表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: CoT在自然语言中能显著提升模型推理能力，但由于蛋白质等生物语言token表达能力有限，CoT无法直接应用。论文旨在突破生物语言模型表达受限这一瓶颈，使其也能利用CoT推理带来的优势。

Method: 作者定义了“语言表达性”的概念，并分析蛋白质语言在token层面的表达限制。为增强表达性，首次在生物序列模型中引入reflection pretraining，通过辅助“思考token”让模型进行中间推理。理论上证明该方法显著提升表达能力，并实际预训练蛋白模型进行验证。

Result: 实验表明，采用reflection pretraining的蛋白质语言模型能够进行自我纠错，较标准预训练模型有显著性能提升。

Conclusion: 通过扩展token集合与反思预训练，生物序列语言模型获得更强的推理能力，为在非自然语言领域中应用CoT提供了可行方案。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [84] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedMistake的自动化流程，将大型语言模型（LLMs）在医患对话中犯的错误提取出来，转化成单轮问答对的基准测试数据集。该数据集覆盖了当前主流LLM在医疗场景下易犯的知识点和推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗领域应用越来越广泛，评测其推理和安全性的需求增加。但目前难以系统复现和对比LLM在医疗问答中的具体错误，大多依赖人工收集和转写。作者希望解决这一复现难题，并推动医疗场景下LLM错误的客观评测。

Method: 作者设计了自动化流程：(1) 用LLM生成复杂的医患对话数据；(2) 由2个LLM评审委员会多维度评估回答质量，自动标注错误；(3) 将错误转化为单轮问答对，形成标准化测试集。随后邀请医学专家验证一部分数据，最终评测了12个主流LLM模型。

Result: 作者生成了3390个LLM在医患对话中出现错误的QA对（MedMistake-All），其中211个问题由医生专家验证（MedMistake-Bench）。评测显示，GPT、Claude和Grok系列模型在经过医生验证的数据集上表现最佳。所有数据集已公开。

Conclusion: MedMistake提供了可自动提取和复用LLM在医疗场景中易犯错误的基准测试，有助于推动LLM医学能力的客观对比分析以及诊断模型瓶颈。

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [85] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本论文研究如何在知识蒸馏过程中，通过对不同推理序列段的选择性监督，提升学生模型的推理能力，同时降低计算开销。结果表明，仅对序列前半部分或关键推理片段进行训练，能大幅减少资源消耗而性能几乎不损失。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型的推理能力迁移到较小模型时，通常需要大量推理数据和高昂计算成本，尤其是在处理包含提示、推理链和答案等长序列时，提升效率成为关键需求。

Method: 论文分析了在知识蒸馏中，选择性地对不同序列段（提示、推理链、答案）的监督分配对学生模型性能的影响，提出只对推理链片段开展蒸馏，并引入截断协议，以定量分析序列长度与计算-性能的权衡。

Result: 实验证明，只对训练序列的前50%标记进行训练，在数学基准任务上依然可以保留约94%的完整性能，同时训练时间、内存和FLOPs均减少约50%。

Conclusion: 推理蒸馏时优先关注早期推理片段，可大幅缩减计算资源消耗，且对推理性能影响非常小，为推理蒸馏提供了简单高效的计算与性能权衡方法。

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [86] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: 本文提出了SFTKey，一种针对大语言模型的两阶段训练方法，通过在第二阶段专门微调答案片段，显著提升了模型在复杂推理任务中的准确率，较传统SFT方法提高了5%以上。


<details>
  <summary>Details</summary>
Motivation: 传统SFT过程中，模型往往对冗长的推理链(CoT)部分关注过多，忽视了关键答案片段，影响评测和实际使用中对答案准确性的需求。作者希望通过训练机制优化，让模型更加关注决定任务成败的答案部分。

Method: 提出“两阶段训练”方案——第一阶段依然采用常规的监督微调SFT以保证输出格式的正确性，第二阶段则只对最终的答案片段进行微调，加强模型对正确答案的学习。

Result: 在多个基准任务和模型家族上的实验表明，SFTKey方法在保证输出格式合理的前提下，平均准确率提升超过5%。

Conclusion: SFTKey有效地平衡了CoT推理链的学习和对答案片段的准确性优化，对大语言模型的微调提供了新思路，提升了复杂推理任务的表现。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [87] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya,Zehong Wang,Jiazheng Li,Ziming Li,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种数据自适应语义改进框架DAS，将GNN与LLM结合，有效提升了不同类型图结构数据的表征学习性能。


<details>
  <summary>Details</summary>
Motivation: 由于不同图结构数据中预测信号来源差异巨大，结构信号和语义信号主导地位各异，单一固定归纳偏置的图学习模型难以适应所有情境。现有方法多从模型角度加编码，但面对真实图数据的复杂多样性仍有限。因此，作者希望用一种灵活适应性更强的方式来提升图表征学习。

Method: 作者提出DAS框架，将固定的GNN与大型语言模型（LLM）闭环结合。GNN向LLM提供隐式监督信号以引导语义精炼，精炼后的语义反馈反过来用于更新图学习器，实现任务自适应的节点语义建模。

Result: 在包含丰富文本和无文本的各种图数据上进行评估，DAS在以结构为主导的图上表现出持续提升，在语义丰富的图上则表现与现有方法持平。

Conclusion: DAS通过数据自适应地整合结构与语义，能够有效适应结构-语义异质性的各种图数据，展现了数据中心语义自适应策略的有效性。

Abstract: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


### [88] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文探索了使用半监督学习方法提升大语言模型（LLMs）安全性任务的表现，强调了任务特定数据增强的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有安全分类器训练依赖大量带标签数据，但高质量标注难以获得，且易出错，常需合成数据。本研究为减少对大量标注数据的依赖，提升安全性分类器的效果。

Method: 采用半监督学习技术，结合有标签和无标签数据用于安全性任务训练；分析在LLM输入提示和输出回复两方面的性能提升，并对比任务特定与通用数据增强方法效果。

Result: 任务特定的数据增强显著提升半监督算法在安全分级任务中的性能，优于通用增强方法。

Conclusion: 半监督学习结合任务专属数据增强是提升LLM安全分类器性能的有效方法，有助于缓解标签数据匮乏和质量问题。

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [89] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo,Yi Huang,Mukai Li,Shichang Meng,Fengyuan Liu,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 提出了ClarifyMT-Bench，这是一个针对多轮澄清需求的大模型评测基准，并提出ClarifyAgent方法以改善大模型在多轮交流中的澄清能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型澄清能力测试偏重单轮、合作性用户场景，缺乏对多轮、现实互动（如用户表达含糊不清等）的全面评估，无法有效反映实际应用中大模型面对歧义时的表现。

Method: 构建了ClarifyMT-Bench基准，结合五维歧义分类和六种不同用户画像，通过LLM-人工混合流程，生成6120条多轮对话，涵盖多种歧义和互动形式；对十个典型大模型进行测试，并提出将澄清行为分解为认知、预测、跟踪和规划四步的ClarifyAgent方法。

Result: 发现当前大模型在多轮对话中普遍存在“过早回答”而非积极澄清的问题，且随着对话深度增加，表现明显下降。ClarifyAgent方法显著提升了模型在各类歧义情况下的稳健性和澄清能力。

Conclusion: ClarifyMT-Bench为衡量和改进大模型澄清行为提供了可复现的标准，有助于推动大模型在真实复杂人机对话中有效处理歧义问题。

Abstract: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [90] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文提出SpidR-Adapt方法，使语音自监督模型能以极少无标注数据，高效适应新语言，数据效率大幅提升，且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 婴儿通过极少的语音输入即可掌握新语言的基本单位，而目前主流的自监督语音模型需要大量数据才能获得良好表现，两者在学习效率上存在巨大差距。作者希望缩小这种效率差距，推动构建更接近人类学习机制的高效语音表征方法。

Method: 作者将低资源语音表征学习建模为元学习问题，提出多任务自适应预训练（MAdaPT）协议，将适应过程表述为双层优化问题。为降低元训练算力需求，创新性地提出了首阶双层优化（FOBLO）启发式算法，并通过交替无监督与有监督目标（交错监督）实现更稳定的初始化和训练。

Result: SpidR-Adapt在表征音位判别（ABX）、口语建模（sWUGGY, sBLIMP, tSC）等任务上，能在不足1小时目标语言音频训练后，快速取得明显提升，优于同域语言模型，比常规训练方法的数据利用率高出100倍以上。

Conclusion: SpidR-Adapt为数据高效、与架构无关的语音表征学习提供了切实可行的路径，有望实现类生物高效的语音模型。代码与模型已开源，为后续研究提供便利。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [91] [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)
*Divij Dudeja,Mayukha Pal*

Main category: cs.CL

TL;DR: SMART（结构化记忆与推理Transformer）是一种针对工程手册（EM）高效理解与回答的 transformer 新模型，通过结构化处理与紧凑记忆机制，提升准确率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 工程手册内容冗长且结构复杂，现有 transformer 模型以平面化 token 处理，导致准确性不足并需要大量参数与记忆。

Method: SMART 采用三层结构：（1）Tree LSTM 提取带语法关系的事实，形成主语-关系-宾语三元组；（2）MANN 模块以紧凑、有索引的方式存储这些事实向量；（3）6 层 transformer 融合召回事实用于生成答案。同时有已知文档的快速索引推断和新文档的动态推断两种模式。

Result: SMART 总参数仅 4551 万，比 GPT-2、BERT 大幅减少，但准确率比 GPT-2 高 21.3%。在应用中推理快，支持新老文档，且相较于同类小模型，答案更有依据，幻觉更少。

Conclusion: SMART 显著提高工程手册理解任务的效率和可靠性，为类似复杂结构长文档场景提供了新的实用 AI 模型方案。

Abstract: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.

</details>


### [92] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: 本文提出了一种新的并行生成方法PTP，可在一次Transformer调用中同时预测多个相关Token，显著降低生成延迟，并在不损失模型能力的前提下实现长序列的并行生成。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型序列生成方法由于自回归解码存在延迟瓶颈，已有的多Token预测方案常依赖强独立性假设，降低了生成质量。作者希望破解效率与建模能力之间的矛盾，实现高效高质量的并行生成。

Method: 提出PTP框架，在模型内部集成采样过程，一步联合预测多个相关Token。PTP可通过对已有模型蒸馏或无教师倒序训练获得，并理论证明其表达任意自回归分布能力。

Result: 在Vicuna-7B和Spec-Bench测试中，PTP每步平均可接受四个以上Token，推理延迟显著降低，达到目前最先进的投机式解码性能。

Conclusion: PTP证明了长序列并行生成在能力不下降情况下可行，为通用、高效的并行文本生成提供了理论和方法支持。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [93] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 该论文通过引入分离感知与推理的两阶段评测流程，发现主流模型在ARC类基准测试中的表现主要受感知能力限制，而非推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统ARC类推理基准常被用来评估AI推理能力，但前沿模型远未达到人类水平，普遍认为原因在于推理能力不足。论文挑战这一观点，提出主因其实是视觉感知的局限。

Method: 作者设计了两阶段实验流程，将感知与推理明确分开：首先将图片单独转为自然语言描述（感知阶段），再利用这些描述进行归纳与推理（推理阶段），防止感知与推理之间的信号混杂，并与端到端的一阶段方法进行对比。还对模型输出推理过程进行了人工检查。

Result: 在Mini-ARC、ACRE和Bongard-LOGO三个数据集上，两阶段流程表现与一阶段差异明显，感知阶段失误占模型失误大约80%。证明模型主要败在感知，而非推理能力。

Conclusion: ARC类基准测试实际混淆了感知与推理的评估，夸大了AI推理上的不足。因此，今后的评测协议需要将感知和推理分离，才能准确衡量AI的推理进步。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [94] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: 论文提出了一种新的代码嵌入模型家族C2LLM，在相同规模下取得了MTEB-Code基准的最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码嵌入时，主要采用EOS-based序列池化方法，信息利用存在瓶颈，且嵌入维度调整不灵活。该研究旨在突破这些限制，提升模型在代码理解、嵌入等任务上的表现。

Method: C2LLM基于Qwen-2.5-Coder作为骨干网络，引入了Pooling by Multihead Attention（PMA）模块来对token级嵌入进行聚合。PMA模块能够充分利用预训练期间获得的因果表示，同时打破了传统EOS池化的单一信息源问题，实现灵活的嵌入维度适配。模型在三百万条公开可用数据上进行训练。

Result: C2LLM在相同模型规模下，在MTEB-Code等代码嵌入基准测试中取得了新纪录。其中C2LLM-7B在总榜单上排名第一。

Conclusion: C2LLM通过创新的序列池化方式和模型架构，提升了代码嵌入能力，为代码相关任务提供了新的高性能解决方案。

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [95] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 本文首次提出了用可计算的不确定性度量—去噪熵（Denoising Entropy）指导Masked Diffusion Models (MDMs)生成路径，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: MDMs生成过程的非自回归特性带来了生成顺序的自由，但也导致生成质量对解码顺序极为敏感，需要方法理性优化路径以提升输出质量。

Method: 作者形式化分析了路径中累计预测不确定性带来的影响，提出了用去噪熵量化不确定性，并基于此提出了两种优化算法：一种是后置选择方法，一种是实时引导策略。

Result: 基于去噪熵指导的解码顺序优化能显著提升在推理、规划和代码等复杂任务上的生成准确率。

Conclusion: 去噪熵作为一种规律性的内在信号，使得MDM的不确定性可以被理解和利用，成为发现高质量解的优势工具。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search](https://arxiv.org/abs/2512.20711)
*Jan Mikula,Miroslav Kulich*

Main category: cs.RO

TL;DR: 本文提出了Milaps框架，通过将最小延迟问题方法应用于移动机器人在连续空间环境中搜索隐藏目标，显著提升了解决ETS（期望时间搜索）任务的效率和解质量。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在连续2D环境中高效搜索目标非常重要，但由于空间连续性和运动与感知约束的复杂交互，传统全局优化方法难以直接应用，现有方法要么只做部分离散化，要么解决相关的离散图上的近似问题，效果有限。

Method: 作者提出Milaps框架，将ETS问题建模为最小延迟问题，并融合了新的辅助目标。采用了近期提出的快速启发式算法（用于traveling deliveryman问题），实现了在严格时间限制内的高效全局优化。

Result: 实验证明，Milaps在作者构建的大规模测试集上，在解质量和计算时间之间取得了比现有最先进方法更优的权衡。最佳策略能够快速生成初步解、为感知配置赋予静态权重，并进行全局元启发式优化。同时，定性分析也展示了该框架良好的适应性。

Conclusion: Milaps框架不仅提升了ETS任务的解效率和质量，还在多种场景下展现了高度的灵活性和泛用性，为移动机器人高效搜索任务提供了新的解决思路。

Abstract: Expected-time mobile search (ETS) is a fundamental robotics task where a mobile sensor navigates an environment to minimize the expected time required to locate a hidden object. Global route optimization for ETS in static 2D continuous environments remains largely underexplored due to the intractability of objective evaluation, stemming from the continuous nature of the environment and the interplay of motion and visibility constraints. Prior work has addressed this through partial discretization, leading to discrete-sensing formulations tackled via utility-greedy heuristics. Others have taken an indirect approach by heuristically approximating the objective using minimum latency problems on fixed graphs, enabling global route optimization via efficient metaheuristics. This paper builds on and significantly extends the latter by introducing Milaps (Minimum latency problems), a model-based solution framework for ETS. Milaps integrates novel auxiliary objectives and adapts a recent anytime metaheuristic for the traveling deliveryman problem, chosen for its strong performance under tight runtime constraints. Evaluations on a novel large-scale dataset demonstrate superior trade-offs between solution quality and runtime compared to state-of-the-art baselines. The best-performing strategy rapidly generates a preliminary solution, assigns static weights to sensing configurations, and optimizes global costs metaheuristically. Additionally, a qualitative study highlights the framework's flexibility across diverse scenarios.

</details>


### [97] [A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets](https://arxiv.org/abs/2512.20769)
*Tanmay P. Patel,Erica L. Tevere,Erik H. Kramer,Rudranarayan M. Mukherjee*

Main category: cs.RO

TL;DR: 该论文提出了一个通用的视觉自主拦截动态非合作目标的机器人框架，并在无人机、四轮地面车、航天器等多种平台上验证了其实用性。该方法仅使用单目相机与标记进行目标跟踪，无需全局定位信息，在受限观测和传感条件下表现出色。实验结果显示该框架鲁棒、高效、泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，机器人需要在缺乏全局位置信息、观测受限（如视野有限、传感器间断、目标遮挡）下拦截动态非合作目标。现有解决方案往往依赖昂贵的传感器或严格的假设，缺乏通用与泛化能力。作者欲解决这一实际问题，并支撑不同运动平台的普适性。

Method: 该方法以单目相机+标记（fiducials）感知为基础，无需全局坐标，仅在本地观测系下工作。主要包括：1）用扩展卡尔曼滤波进行间断测量下的相对位姿估计；2）历史条件下的目标运动预测器用于动态目标轨迹推断；3）递归实时地解决带约束的凸优化问题产生时间高效且满足运动约束的拦截路径。平台涵盖无人机、地面车和航天器。

Result: 在感知受限的各种实验条件下进行仿真与实物测试。机器人能实现自主无人机动态目标降落、车辆会合与跟随、航天操作等多场景拦截任务，表现出低拦截误差、高成功率，并能实时部署在嵌入式芯片（如Jetson Orin、VOXL2、Raspberry Pi 5）上。

Conclusion: 该框架在各种运动平台和任务下均能高效、鲁棒、泛化地完成动态目标自主拦截任务，是一种面向多机器人系统、感知受限环境下的通用且高效的视觉拦截解决方案。

Abstract: This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.

</details>


### [98] [YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion](https://arxiv.org/abs/2512.20847)
*Parag Khanna,Karen Jane Dsouza,Chunyu Wang,Mårten Björkman,Christian Smith*

Main category: cs.RO

TL;DR: YCB-Handovers 数据集记录了2771个人与人之间交接物体（不同重量）时的动作数据，可用于研究重量变化如何影响交接行为，并为机器人直观运动规划提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 当前在机器人协作领域，尤其是人机交接任务中，缺乏能够反映物体重量变化对人类交接行为影响的真实世界运动数据。为提升机器人在人类交接中的直观性与适应性，亟需此类具有人类动作规律和重量变化的多样化数据集。

Method: 基于YCB标准物体数据库，采集2771次涵盖多种重量与交接情境的人对人交接运动数据，并分析了不同物体重量对动作过程中的影响，特别是对伸手动作的影响。涉及部分精细物体，以展现常规与特殊交接动作的对比。

Result: 构建了一个涵盖广泛重量区间、细致标注交接细节的数据集。通过数据分析，揭示了物体重量对人类交接动作、尤其是伸手阶段动作方式的具体影响。

Conclusion: YCB-Handovers 数据集为基于数据驱动、人类启发的重量敏感型运动规划和自适应机器人行为建模提供了关键支撑，有望促进人机交互中更直观和灵活的交接动作实现。

Abstract: This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.

</details>


### [99] [Early warning signals for loss of control](https://arxiv.org/abs/2512.20868)
*Jasper J. van Beers,Marten Scheffer,Prashant Solanki,Ingrid A. van de Leemput,Egbert H. van Nes,Coen C. de Visser*

Main category: cs.RO

TL;DR: 本文提出了一种基于动态弹性指标的系统安全监测方法，不依赖于传统的系统模型，能够早期预警反馈系统失稳，提升工程系统在损伤或异常情况下的韧性。


<details>
  <summary>Details</summary>
Motivation: 传统反馈系统依赖于精确模型来保持稳定，但在系统损伤或性能偏离模型时，这些方法失效，难以及时发现并预警潜在的失稳问题。

Method: 作者提出利用“临界减速”现象，即复杂系统接近临界状态时会表现出动力学响应变缓作为韧性指标，无需系统模型，通过实时监测来预警系统失稳。该方法已在无人机等工程设备上进行了验证。

Result: 结果表明，该方法能够有效预测反馈系统接近失控的临界点，并可实时预警，验证了该类动态指标在工程系统中的可行性和有效性。

Conclusion: 基于临界减速的无模型系统安全监控方法不仅适用于无人机，还可拓展至更广泛的复杂控制系统（如反应堆、飞机和自动驾驶汽车），为工程系统实时早期预警和韧性设计提供了新思路。

Abstract: Maintaining stability in feedback systems, from aircraft and autonomous robots to biological and physiological systems, relies on monitoring their behavior and continuously adjusting their inputs. Incremental damage can make such control fragile. This tends to go unnoticed until a small perturbation induces instability (i.e. loss of control). Traditional methods in the field of engineering rely on accurate system models to compute a safe set of operating instructions, which become invalid when the, possibly damaged, system diverges from its model. Here we demonstrate that the approach of such a feedback system towards instability can nonetheless be monitored through dynamical indicators of resilience. This holistic system safety monitor does not rely on a system model and is based on the generic phenomenon of critical slowing down, shown to occur in the climate, biology and other complex nonlinear systems approaching criticality. Our findings for engineered devices opens up a wide range of applications involving real-time early warning systems as well as an empirical guidance of resilient system design exploration, or "tinkering". While we demonstrate the validity using drones, the generic nature of the underlying principles suggest that these indicators could apply across a wider class of controlled systems including reactors, aircraft, and self-driving cars.

</details>


### [100] [Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task](https://arxiv.org/abs/2512.20876)
*Kanata Suzuki,Shota Shimizu,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本研究探讨将机器人运动数据融入视觉语言模型（VLM）进行视频字幕生成与任务分割，旨在提升机器人模仿学习效率并检验VLM在机器人任务中理解运动的能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流VLM主要基于离线图片和语言数据训练，缺乏对机器人低层运动信息的理解。为了推动机器人自主发展，需要验证和提升基础模型对机器人运动的理解能力。

Method: 提出了一种结合图片字幕与机器人轨迹数据（关节和末端执行器状态）的方法，用于生成机器人物理任务的视频字幕，并通过计算字幕嵌入间的相似度实现子任务分割。通过总结分段字幕生成完整任务描述。

Result: 仿真实验验证了该方法在字幕生成和任务分割方面的有效性，表明将运动数据作为输入能提升VLM对机器人任务的理解与描述能力。

Conclusion: 为基础模型在机器人领域的应用提供了新路径，展示了将低层次运动数据融入VLM有助于提升其对机器人复杂任务的表征和模仿学习支持能力。

Abstract: From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple "scene" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.

</details>


### [101] [Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms](https://arxiv.org/abs/2512.20888)
*Yiding Nie,Dongliang Fan,Jiatai Huang,Chunyu Liu,Jian S. Dai*

Main category: cs.RO

TL;DR: 本文提出了一种基于连续光谱滤波原理的可拉伸触觉传感器，实现了超高空间与力学分辨率，并展示了其在精确轨迹跟踪中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有可拉伸触觉传感器在实现高空间分辨率、自解耦能力和对非轴向干扰的不敏感性等方面存在挑战。提升这些性能对于软体机器人、医疗设备及人机交互等领域具有重要意义。

Method: 作者提出了一种新的连续光谱滤波原理，设计并制备了基于此原理的可拉伸触觉传感器。该传感器通过独特的结构和材料配置，实现对刺激的高灵敏度、高线性响应，同时能够抗拉伸、弯曲及外部损伤。

Result: 该传感器在拉伸和弯曲时，依然保持极高的空间线性度（0.996），空间分辨率高达7微米，力分辨率可达5毫牛，并表现出良好的结构可扩展性和抗刺穿、切割的鲁棒性。同时，集成到平面并联机构中，能够实现实时、高精度的轨迹跟踪（旋转分辨率0.02°）。

Conclusion: 基于连续光谱滤波原理的可拉伸触觉传感器在空间及力学分辨率、集成应用能力和结构鲁棒性等方面表现优异，有望推动可拉伸传感器在更多实际场景中的应用。

Abstract: Stretchable sensors indicate promising prospects for soft robotics, medical devices, and human-machine interactions due to the high compliance of soft materials. Discrete sensing strategies, including sensor arrays and distributed sensors, are broadly involved in tactile sensors across versatile applications. However, it remains a challenge to achieve high spatial resolution with self-decoupled capacity and insensitivity to other off-axis stimuli for stretchable tactile sensors. Herein, we develop a stretchable tactile sensor based on the proposed continuous spectral-filtering principle, allowing superhigh resolution for applied stimuli. This proposed sensor enables a high-linear spatial response (0.996) even during stretching and bending, and high continuous spatial (7 μm) and force (5 mN) resolutions with design scalability and interaction robustness to survive piercing and cutting. We further demonstrate the sensors' performance by integrating them into a planar parallel mechanism for precise trajectory tracking (rotational resolution: 0.02°) in real time.

</details>


### [102] [Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality](https://arxiv.org/abs/2512.20931)
*Baoshan Song,Matthew Giamou,Penggao Yan,Chunxi Xia,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本研究提出了一种可证实全局最优的姿态对准方法，将伪距或多普勒测量转化为凸松弛优化问题，并通过理论分析和实验验证了其性能，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS对准方法依赖大量卫星或易陷入局部最优，尤其在卫星数量有限的恶劣环境下表现不佳，急需一种既能保证解的全局最优、又能适用于卫星稀缺场景的对准算法。

Method: 作者将原始的姿态对准问题建模为非凸二次约束二次规划(QCQP)问题，并放松为可求解的拉格朗日对偶问题。通过松弛紧致性分析和可观测性分析，提出了解的可证实最优性判据。方法可将原始GNSS伪距或多普勒原始数据直接高效求解。

Result: 仿真和实际实验表明，即便仅依赖于2颗GNSS卫星的多普勒数据和二维车辆运动轨迹，所提方法依然可以获得可证实的全局最优解，而主流基于速度或先进融合方法可能收敛于局部最优且无法判别。

Conclusion: 本文方法在卫星数量极少的情况下依然能够获得全局可证实的对准解，极大提高GNSS导航在恶劣环境下的可靠性和鲁棒性，具有重要的理论和实际意义。所有代码和数据均已开源。

Abstract: Estimating the absolute orientation of a local system relative to a global navigation satellite system (GNSS) reference often suffers from local minima and high dependency on satellite availability. Existing methods for this alignment task rely on abundant satellites unavailable in GNSS-degraded environments, or use local optimization methods which cannot guarantee the optimality of a solution. This work introduces a globally optimal solver that transforms raw pseudo-range or Doppler measurements into a convexly relaxed problem. The proposed method is certifiable, meaning it can numerically verify the correctness of the result, filling a gap where existing local optimizers fail. We first formulate the original frame alignment problem as a nonconvex quadratically constrained quadratic program (QCQP) problem and relax the QCQP problem to a concave Lagrangian dual problem that provides a lower cost bound for the original problem. Then we perform relaxation tightness and observability analysis to derive criteria for certifiable optimality of the solution. Finally, simulation and real world experiments are conducted to evaluate the proposed method. The experiments show that our method provides certifiably optimal solutions even with only 2 satellites with Doppler measurements and 2D vehicle motion, while the traditional velocity-based VOBA method and the advanced GVINS alignment technique may fail or converge to local optima without notice. To support the development of GNSS-based navigation techniques in robotics, all code and data are open-sourced at https://github.com/Baoshan-Song/Certifiable-Doppler-alignment.

</details>


### [103] [ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2512.20940)
*Shuhao Ye,Sitong Mao,Yuxiang Cui,Xuan Yu,Shichao Zhai,Wen Chen,Shunbo Zhou,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合大规模数据扩展和强化学习微调的图方法框架ETP-R1，在视觉-语言导航（VLN-CE）任务上取得了新的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大模型的方法在利用大规模数据和先进训练范式方面优于图方法，二者性能存在差距。因此作者希望探索如何使图方法同样受益于大数据和强化学习以缩小性能差距。

Method: 提出了ETP-R1框架：1）利用Gemini API构建大规模、高质量的导航指令数据集，提供更丰富的监督信号；2）融合R2R与RxR任务实现联合预训练，增加泛化能力；3）设计了三阶段训练流程，引入了基于GRPO算法的闭环在线强化学习微调，将RFT首次应用到图方法VLN-CE模型。

Result: 在R2R-CE和RxR-CE数据集上的大规模实验显示，所提方法在全部主流指标上显著超越以往方法，取得了最新的最优性能。

Conclusion: 通过将大模型的训练范式引入图方法，ETP-R1极大提升了图方法在VLN-CE上的表现，推动了该方向的发展。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.

</details>


### [104] [From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection](https://arxiv.org/abs/2512.20951)
*Jiangen He,Wanqi Zhang,Jessica Barfield*

Main category: cs.RO

TL;DR: 本研究探讨了社会偏见如何影响人们在不同职业情境下对机器人的选择，发现人们对机器人也存在与人类类似的肤色及刻板印象偏好。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人越来越多地融入各类专业环境，人们开始关注社会偏见（如肤色和职业刻板印象）是否会转移到人机交互决策中，对机器人部署可能带来的社会影响进行预警。

Method: 作者通过两个实验（共计1038名参与者），让参与者在建筑、医疗、教育和体育四个领域中，从不同肤色和类人特征的人工智能代理中进行选择。第二个实验进一步考察了事先暴露于不同种族专业人士对后续机器人选择的影响。

Result: 结果显示：在医疗和教育领域参与者明显偏好浅肤色机器人，而建筑和体育领域则对深肤色机器人更为接受。参与者本身的种族也显著影响其选择。第二个实验表明，暴露于某种族人类专业人士后，会强化与刻板印象一致的机器人选择偏好。

Conclusion: 职业偏见和肤色歧视会从人与人交互直接迁移到人机决策中，这可能导致机器人部署无意中加剧现有社会不平等。

Abstract: As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.

</details>


### [105] [Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation](https://arxiv.org/abs/2512.20992)
*Tian-Ao Ren,Jorge Garcia,Seongheon Hong,Jared Grinberg,Hojung Choi,Julia Di,Hao Li,Dmitry Grinberg,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: 该论文提出了一种集成高分辨率视觉触觉成像与六轴力-力矩传感器的多模态传感器，有效提升了机器人在软组织环境中检测亚表面特征的能力。


<details>
  <summary>Details</summary>
Motivation: 在软组织环境下，现有依赖力传感的机器人触诊方法对微妙的亚表面结构（如肌腱走向等）难以可靠检测，力信号由于环境变化往往模糊且不稳定，因此亟需能够提供更丰富结构信息的新型传感技术。

Method: 设计并实现了一种紧凑型多模态传感器，将高分辨率的视觉触觉成像和六轴力-力矩传感器结合，完成在不同亚表面肌腱几何形状的硅胶模型上的实验，以比对力信号和触觉图像对于隐藏结构的检测效果。

Result: 实验发现，单靠力信号往往得不到清晰结构差异，而基于视觉的触觉成像可以明显区分亚表面结构的存在、直径、深度、交叉和数量。同时，精确力跟踪对于安全和稳定接触也是必不可少的。融合两种模态后，能实现对亚表面特征的稳健检测及受控的机器人触诊。

Conclusion: 将高分辨率触觉成像与力传感相结合，可大大提升机器人触诊对软组织亚表面结构的识别与交互控制能力，为后续医学机器人和康复领域应用奠定了基础。

Abstract: Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.

</details>


### [106] [Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction](https://arxiv.org/abs/2512.21043)
*Cheng-Yu Kuo,Hirofumi Shin,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 该论文提出了一种基于物理能量抽象的新方法，实现机器人多指动态抓取时的触觉驱动抓力控制，无需外部传感和物体先验知识，实验验证了其高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人抓取在多指滚动接触、物体属性未知和外部感知不可靠时，难以有效调节抓取力防止滑动。相比之下，人类能仅凭触觉快速调整抓力，本文旨在将此能力迁移到机器人系统中。

Method: 提出了物体虚拟能量容器的物理抽象，利用手指施加功率与物体保留能量的不一致性作为滑动感知信号，然后结合基于模型的学习与规划，通过触觉数据高效建模能量动力学，实现实时抓取力优化。

Result: 无论在仿真还是真实硬件平台上，该方法均能在数分钟内从零学习有效的抓力控制，显著减少滑动并延长各种运动-物体组合下的抓持时间，且不依赖外部传感或物体先验知识。

Conclusion: 基于物理能量抽象和触觉感知的抓取力控制方法，能够赋予机器人高效、泛化和自适应的动态抓取能力，为复杂操作任务扩展了应用前景。

Abstract: Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.

</details>


### [107] [Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation](https://arxiv.org/abs/2512.21065)
*Zebin Jiang,Tianle Jin,Xiangtong Yao,Alois Knoll,Hu Cao*

Main category: cs.RO

TL;DR: 本文提出了一种基于语言引导的新型抓取检测方法LGGD，通过多层次跨模态融合和动态卷积提升机器人在复杂场景中的抓取与指令对齐能力。在多个数据集和真实机器人平台上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的抓取方法在视觉与语言的对齐和语义理解方面效果有限，难以在多样且复杂的环境下有效完成任务。为提升语义落地和增强机器人对指令的理解与执行能力，作者设计了新方法。

Method: 提出Language-Guided Grasp Detection (LGGD)方法，采用粗到细的学习范式，结合CLIP视觉与文本特征，通过分层的跨模态融合逐步引入语言信息。同时引入语言调制动态卷积头和最终精细化模块，以提升抓取的任务相关性和鲁棒性。

Result: 在OCID-VLG和Grasp-Anything++数据集上，LGGD在泛化能力和对复杂语言/物体抓取任务的准确率上，超过了现有主流方法。在真实机器人实验中，展现出良好的实际可用性。

Conclusion: LGGD方法有效提升了机器人在语言引导下的抓取能力，实现了更强的视觉-语义对齐和任务执行性能，具有较强推广价值。代码将在论文录用后公开。

Abstract: Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.

</details>


### [108] [Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning](https://arxiv.org/abs/2512.21085)
*Shlok Deshmukh,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 本文提出并验证了一种通过差动机制将2自由度机械臂安装在四旋翼上的轻量级空中操作平台，并利用强化学习方法，实现了完整的末端执行器六自由度控制，兼顾了结构简化与高性能任务的完成。


<details>
  <summary>Details</summary>
Motivation: 现有空中操作器受限于机械臂重量和结构复杂度，影响续航和操控能力，急需兼具简单结构和高灵活性的解决方案。

Method: 设计了一个2自由度机械臂与四旋翼结合的系统，采用差动机构提升末端执行器可操作空间。利用PPO强化学习算法，在仿真中训练智能体输出四旋翼加速度、机体角速率和关节目标位指令，由非线性动态逆(INID)姿态控制器和PID关节控制器追踪，实现整体协同控制。

Result: 飞行实验证明，该方法可达到厘米级位置精度和度级姿态精度，同时在应对外部扰动力（如搬运重物、推拉任务）时具有良好的鲁棒性。

Conclusion: 基于强化学习的控制策略能够赋能结构简单轻量的空中操作平台，实现富接触操作任务，具备实际应用潜力。

Abstract: Aerial manipulators, which combine robotic arms with multi-rotor drones, face strict constraints on arm weight and mechanical complexity. In this work, we study a lightweight 2-degree-of-freedom (DoF) arm mounted on a quadrotor via a differential mechanism, capable of full six-DoF end-effector pose control. While the minimal design enables simplicity and reduced payload, it also introduces challenges such as underactuation and sensitivity to external disturbances, including manipulation of heavy loads and pushing tasks. To address these, we employ reinforcement learning, training a Proximal Policy Optimization (PPO) agent in simulation to generate feedforward commands for quadrotor acceleration and body rates, along with joint angle targets. These commands are tracked by an incremental nonlinear dynamic inversion (INDI) attitude controller and a PID joint controller, respectively. Flight experiments demonstrate centimeter-level position accuracy and degree-level orientation precision, with robust performance under external force disturbances. The results highlight the potential of learning-based control strategies for enabling contact-rich aerial manipulation using simple, lightweight platforms.

</details>


### [109] [Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives](https://arxiv.org/abs/2512.21109)
*Chen Liang,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文提出在MuJoCo MPC库中用WASP推导方法替代传统有限差分（FD）计算，从而显著提升微分的效率与稳定性，实现最大2倍加速，并提升MPC算法效果。


<details>
  <summary>Details</summary>
Motivation: Model Predictive Control（MPC）在机器人控制中依赖MuJoCo平台，但是当前MuJoCo MPC（MJPC）采用有限差分计算微分，计算量大、速度慢，限制了其实时应用，尤其在高自由度或复杂场景下问题更为突出，因此亟需更高效的推导方案。

Method: 将Web of Affine Spaces（WASP）推导方法集成到MJPC中，替代原有基于有限差分（FD）的微分计算。WASP能通过重复利用先前推导信息，高效且稳定地生成准确推导，适用于MPC迭代性强的特性。并在多种MJPC典型机器人任务上实验验证了WASP方法。

Result: WASP推导能无缝集成至MJPC，支持多类任务，且与FD方法相比，为推导式规划器（如iLQG）提供最高2倍的速度提升，并优于MJPC的随机采样规划器，展现更优的效率和稳定性表现。

Conclusion: WASP方法为MJPC微分运算提供了高效、准确且可拓展的新途径，极大增加其实时性和适用性，有望推动相关领域研究和应用（已开源以促进社区采用与后续工作）。

Abstract: MuJoCo is a powerful and efficient physics simulator widely used in robotics. One common way it is applied in practice is through Model Predictive Control (MPC), which uses repeated rollouts of the simulator to optimize future actions and generate responsive control policies in real time. To make this process more accessible, the open source library MuJoCo MPC (MJPC) provides ready-to-use MPC algorithms and implementations built directly on top of the MuJoCo simulator. However, MJPC relies on finite differencing (FD) to compute derivatives through the underlying MuJoCo simulator, which is often a key bottleneck that can make it prohibitively costly for time-sensitive tasks, especially in high-DOF systems or complex scenes. In this paper, we introduce the use of Web of Affine Spaces (WASP) derivatives within MJPC as a drop-in replacement for FD. WASP is a recently developed approach for efficiently computing sequences of accurate derivative approximations. By reusing information from prior, related derivative calculations, WASP accelerates and stabilizes the computation of new derivatives, making it especially well suited for MPC's iterative, fine-grained updates over time. We evaluate WASP across a diverse suite of MJPC tasks spanning multiple robot embodiments. Our results suggest that WASP derivatives are particularly effective in MJPC: it integrates seamlessly across tasks, delivers consistently robust performance, and achieves up to a 2$\mathsf{x}$ speedup compared to an FD backend when used with derivative-based planners, such as iLQG. In addition, WASP-based MPC outperforms MJPC's stochastic sampling-based planners on our evaluation tasks, offering both greater efficiency and reliability. To support adoption and future research, we release an open-source implementation of MJPC with WASP derivatives fully integrated.

</details>


### [110] [SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation](https://arxiv.org/abs/2512.21133)
*Xiaoyu Mo,Jintian Ge,Zifan Wang,Chen Lv,Karl Henrik Johansson*

Main category: cs.RO

TL;DR: 本文提出了一种高效、可扩展的稀疏图学习框架SparScene，用于多智能体交通场景下的轨迹生成。该方法依赖路网结构，将场景建模为结构感知的稀疏图，达到在大规模复杂交通环境下的高效性和强扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体轨迹生成方法，多数采用基于距离或完全连接的密集图结构，造成大量无用边和编码开销，效率低且难以扩展到规模更大的交通场景。因此，亟需一种能高效表示复杂场景交互且可扩展的方法。

Method: 提出SparScene框架：依据车道图拓扑而非距离构建体量稀疏的结构化图，采用轻量级图编码器高效汇聚体与车道、体与体之间的交互信息，从而获得紧凑的场景表示。

Result: 在Waymo开放数据集的预测基准上，SparScene取得具有竞争力的性能。在GPU 2.9GB内，仅需5ms即可为场景中200多个主体生成轨迹，并能在54ms时间内扩展到5000+主体及17000+车道，表现出卓越的效率与可扩展性。

Conclusion: SparScene在兼顾效率与准确性的基础上，实现了对大规模复杂交通场景的优异扩展，为多智能体交通建模提供了新思路。

Abstract: Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.

</details>


### [111] [Flocking phase transition and threat responses in bio-inspired autonomous drone swarms](https://arxiv.org/abs/2512.21196)
*Matthieu Verdoucq,Dari Trendafilov,Clément Sire,Ramón Escobedo,Guy Theraulaz,Gautier Hattenberger*

Main category: cs.RO

TL;DR: 本文提出了一种受动物群体启发的3D无人机集群算法，仅需最小化邻居间的局部对齐与吸引，实现了不同集群动态模式，并验证了其在应对外界扰动时的高响应性及恢复能力。


<details>
  <summary>Details</summary>
Motivation: 理解和借鉴动物群体运动原理，可提升自主无人机集群的协作、高效性与自适应能力。此前关于无人机集群的规则多较复杂，实际运行受限，切实可行的最简局部交互机制尚待探索。

Method: 设计了一种仿生的三维集群算法，每台无人机只与少数关键邻居互动，并仅依赖局部对齐与吸引信号，通过系统调节两种交互增益，构建了相变图。结合约十架无人机的户外实验和匹配动力学模型的仿真，分析了各种集群行为相和响应性。

Result: 揭示了从群集到群游的动力学跃迁区，该临界区表现出最大的灵敏度、极化波动和重组能力。实验证明在该区域，无人机群在遇到入侵者时能快速整体转向、暂时扩展并数秒内高效恢复一致行动。

Conclusion: 最简局部交互法则即可实现多种集群行为，通过调整参数增益，可高效调控无人机集群的稳定性、灵活性与抗扰性。这为实际无人机集群设计提供了简单且有效的调控机制。

Abstract: Collective motion inspired by animal groups offers powerful design principles for autonomous aerial swarms. We present a bio-inspired 3D flocking algorithm in which each drone interacts only with a minimal set of influential neighbors, relying solely on local alignment and attraction cues. By systematically tuning these two interaction gains, we map a phase diagram revealing sharp transitions between swarming and schooling, as well as a critical region where susceptibility, polarization fluctuations, and reorganization capacity peak. Outdoor experiments with a swarm of ten drones, combined with simulations using a calibrated flight-dynamics model, show that operating near this transition enhances responsiveness to external disturbances. When confronted with an intruder, the swarm performs rapid collective turns, transient expansions, and reliably recovers high alignment within seconds. These results demonstrate that minimal local-interaction rules are sufficient to generate multiple collective phases and that simple gain modulation offers an efficient mechanism to adjust stability, flexibility, and resilience in drone swarms.

</details>


### [112] [Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation](https://arxiv.org/abs/2512.21201)
*Yu He,Da Huang,Zhenyang Liu,Zixiao Gu,Qiang Sun,Guangnan Ye,Yanwei Fu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的零样本目标导航方法（Schrödinger's Navigator），有效提升了机器人在全新复杂环境下定位目标的能力，尤其在有严重遮挡和动态变化目标的场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标导航方法在现实复杂、遮挡严重、场景动态变化的情形下表现不佳，难以应对未知风险和动态目标。本文旨在解决现有方法对于遮挡、未知区空间推理、动态目标追踪等方面的瓶颈，提升机器人在未知环境中的导航能力。

Method: 提出Schrödinger's Navigator导航框架，基于薛定谔思想实验的不确定性，模型将未观察区域视为潜在的未来场景，并在决策前进行推理。方法输入自我中心视觉和三条候选路径，通过路径条件化3D世界模型对每个路径进行未来观测的想象，并将想象结果融合到地图及价值函数中，从而指导策略选择更安全、目标更明确的路径。该方法无需额外绕行或构建全局稠密地图。

Result: 在Go2四足机器人和三个高难度场景（严重静态遮挡、未知风险、动态目标）实验中，Schrödinger's Navigator在自我定位、目标定位、任务成功率等指标上均优于主流ZSON基线，特别是在遮挡较重的环境下表现突出。

Conclusion: 轨迹条件化3D世界想象能极大增强机器人零样本目标导航的鲁棒性，为在复杂真实场景中实现高效导航提供了新思路。

Abstract: Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.

</details>


### [113] [Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3](https://arxiv.org/abs/2512.21219)
*Muhtadin,Faris Rafi Pramana,Dion Hayu Fandiantoro,Moh Ismarintan Zazuli,Atar Fuady Babgei*

Main category: cs.RO

TL;DR: 该论文提出了一种无线足部嵌入式平衡系统，通过实时估算压力中心（CoP）并无线反馈，提升了人形机器人在单腿站立阶段的稳定性，且简化了机器人连线，提升了机械自由度。实验结果验证了系统在3度坡面下的高准确性和100%单腿平衡成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人平衡系统通常依赖有线传感器，限制了关节自由度，并引入机械噪声。特别是在舞蹈机器人等需要高自由度和复杂动作的场景，更加突出。因此，亟需一种保持高机械自由度并能实时反馈的稳定性控制方案。

Method: 作者设计了集成4个压力传感器（load cell）和ESP32-C3微控制器的自定义无线足部单元，实现实时压力中心（CoP）估算，并将数据无线发送到主控系统。主控端通过PID控制策略，利用CoP反馈调节躯干、髋、踝三个关节的翻转角度，以维持平衡。

Result: 实验测试表明，该系统传感器平均测量误差仅为14.8g。在优化PID参数（Kp=0.10, Kd=0.005）后，机器人在3度倾斜平台上，单腿抬起任务均实现了100%的平衡成功率，表现出高精度和高可靠性。

Conclusion: 无线足部压力中心反馈和闭环控制策略能够有效提升人形机器人的姿态稳定性，且不会影响机器人本身的机械灵活性，验证了该方案在实际舞蹈（高自由度）机器人中的应用价值。

Abstract: Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.

</details>


### [114] [Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot](https://arxiv.org/abs/2512.21226)
*Shuhan Zhang,Tin Lun Lam*

Main category: cs.RO

TL;DR: 本文提出了一种相对定位系统，集成了多种传感技术，实现了模块化机器人的高效协作。


<details>
  <summary>Details</summary>
Motivation: 模块化、可自重构机器人在协作和定位任务中对高精度、鲁棒的相对定位需求迫切。当前定位方案在复杂动态环境下存在局限，亟需更优的系统设计。

Method: 系统融合了ArUco标记识别、光流分析和IMU数据处理，采用统一融合框架，通过基于规则的策略提升在动态场景下的可靠性和准确性。

Result: 实验验证表明，该系统可在实时操作下实现鲁棒且准确的相对定位，在多种动态场景下均表现出较高的可靠性。

Conclusion: 该方案为模块化机器人系统的大规模部署奠定了基础，显示出良好的可扩展性和实用性。

Abstract: This paper presents the design and implementation of a relative localization system for SnailBot, a modular self reconfigurable robot. The system integrates ArUco marker recognition, optical flow analysis, and IMU data processing into a unified fusion framework, enabling robust and accurate relative positioning for collaborative robotic tasks. Experimental validation demonstrates the effectiveness of the system in realtime operation, with a rule based fusion strategy ensuring reliability across dynamic scenarios. The results highlight the potential for scalable deployment in modular robotic systems.

</details>


### [115] [UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer](https://arxiv.org/abs/2512.21233)
*Chi Zhang,Penglin Cai,Haoqi Yuan,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: 该论文提出了一种新的统一表征（UniTacHand），通过将人手和机器人手的触觉数据映射到一致的2D手部表面，结合对比学习方法，实现了人类触觉操作策略零样本迁移到机器人手，以及更高效、泛化的数据利用。


<details>
  <summary>Details</summary>
Motivation: 机器人手实现高水平灵巧操作受限于真实世界大规模触觉数据采集的困难。直接用人手触觉数据训练机器人手策略存在数据异构、结构不一致等问题，导致策略迁移困难。因此，迫切需要一种能对齐人手与机器人手触觉数据的新机制。

Method: 1）将人手与机器人手的触觉信号统一投影到MANO手部模型的形态一致2D表面空间，标准化异构数据结构；2）提出对比学习方法，用10分钟配对数据对齐两种数据到统一潜空间，实现触觉表征的一致性；3）该表征用于策略学习和迁移。

Result: 实现了基于触觉的操作策略从人到机器人手的零样本迁移，对预训练数据未见过的新物体具备泛化能力；混合使用人和机器人数据进行训练时，提升了性能和数据效率，相比单独用机器人数据更优。

Conclusion: UniTacHand为触觉型灵巧操作的泛化、可扩展和高效学习奠定了基础，显著促进了人手策略向机器人手的快捷迁移和实际应用。

Abstract: Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.

</details>


### [116] [RoboCade: Gamifying Robot Data Collection](https://arxiv.org/abs/2512.21235)
*Suvir Mirchandani,Mia Tang,Jiafei Duan,Jubayer Ibn Hamid,Michael Cho,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本论文提出采用游戏化的远程操作平台RoboCade，吸引广大用户参与机器人示范数据采集，以降低数据收集成本并提升效率。实验表明，这种方式能够提升机器人策略的训练效果和用户的参与积极性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人模仿学习高度依赖昂贵且耗时的人类演示数据采集，限制了训练数据规模，所以迫切需要一种高效、低成本、可扩展的数据采集方法。

Method: 作者设计了RoboCade平台，将游戏化元素（如视觉反馈、音效、进度条、排行榜等）嵌入远程机器人操作界面和任务中，吸引普通用户参与数据收集，并提出了游戏化任务设计原则。平台测试于三类操作任务，并与非游戏化平台进行对比。

Result: 通过RoboCade平台收集的数据与传统数据共训练机器人策略后，在非游戏化任务上成功率提升16%-56%。用户研究还证实，初学者认为游戏化平台的趣味性比标准平台高24%。

Conclusion: 游戏化的数据采集平台不仅能有效提升机器人学习的示范数据规模和多样性，还显著增强了普通用户的参与热情和体验，是一种可扩展且富有前景的数据收集方法。

Abstract: Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.

</details>


### [117] [LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation](https://arxiv.org/abs/2512.21243)
*Anatoly O. Onishchenko,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.RO

TL;DR: 本论文提出了LookPlanGraph方法，通过动态更新场景图实现大语言模型驱动的机器人指令执行任务，优于基于静态图的现有方法，并公开了新的GraSIF数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务执行前场景图的所有信息都准确无误，无法应对环境变化带来的信息失真或遗漏。如何让机器人系统在执行计划过程中动态适应和发现环境变化，是实现更智能、可靠的规划执行的关键挑战。

Method: 提出LookPlanGraph方法，场景图初始由静态资产和对象先验构成，在执行计划时，机器人通过视觉语言模型处理自我视角图像，不断验证和发现相关物体，对场景图实时更新。实验覆盖VirtualHome、OmniGibson虚拟环境和实际真实环境，验证了其效果。还发布了包含514个指令任务、自动验证流程的数据集GraSIF。

Result: 在虚拟和现实环境中，LookPlanGraph方法在任务完成率和适应动态场景变化能力上，均显著优于只用预定义静态场景图的传统基线方法。

Conclusion: LookPlanGraph能更好地适应和处理环境变化，提高依赖大模型的机器人在实际场景中执行指令任务的鲁棒性和可靠性。公开的GraSIF数据集和方法为相关领域发展提供了新资源和新范式。

Abstract: Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .

</details>


### [118] [Quadrupped-Legged Robot Movement Plan Generation using Large Language Model](https://arxiv.org/abs/2512.21293)
*Muhtadin,Vincentius Gusti Putu A. B. M.,Ahmad Zaini,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 本论文提出了一种结合大语言模型（LLM）的四足机器人自然语言控制框架，显著降低操作难度，实验结果表明，该系统可实现高效、鲁棒的自然语言导航。


<details>
  <summary>Details</summary>
Motivation: 传统的四足机器人控制需要专业技术知识，使用门槛较高，影响了其普及和应用。因此，作者希望通过整合大语言模型，让普通用户也能通过自然语言直观控制机器人。

Method: 作者设计了一个分布式控制架构，将高层次指令处理（由LLM实现）卸载到外部服务器，解决了机器人本体计算能力有限的问题。系统结合激光雷达、IMU和里程计进行实时传感器数据融合，将LLM生成的计划转化为ROS可执行导航指令。实验在结构化室内环境中进行，涵盖四种任务场景。

Result: 在所有四个场景中，系统的综合成功率超过90％，展现出优秀的鲁棒性和实用性。

Conclusion: 通过将LLM规划能力外包到服务器，本文验证了基于自然语言的机器人导航框架在真实四足机器人上的可行性和高效性，有望扩大其实际应用。

Abstract: Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.

</details>
