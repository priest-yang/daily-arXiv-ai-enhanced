<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 110]
- [cs.CL](#cs.CL) [Total: 25]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment](https://arxiv.org/abs/2511.15831)
*Wei Zhang,Yeying Jin,Xin Li,Yan Zhang,Xiaofeng Cong,Cong Wang,Fengcai Qiao,zhichao Lian*

Main category: cs.CV

TL;DR: UniFit提出了一种由多模态大语言模型（MLLM）驱动的通用虚拟试衣（VTON）框架，能够通过语义对齐和进阶训练策略，有效应对复杂多样的任务，取得了当前最优的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前VTON技术在应对多样、复杂场景上存在困难，主要包括文本和图像语义鸿沟以及复杂任务的数据稀缺问题，因此需要设计更通用且灵活的VTON方法。

Method: 提出UniFit框架，核心包括两个创新点：（1）开发了MLLM引导的语义对齐模块（MGSA），用多模态大模型和可学习查询对多模态输入实现深度语义融合，通过语义对齐损失收敛跨模态差异；（2）采用二阶段渐进训练策略和自我合成机制，使模型能从有限数据中学习复杂任务。

Result: 实验表明，UniFit不仅支持多衣物、多模特试穿等多种复杂VTON任务，而且在多个评价指标上均取得了领先SOTA的性能。

Conclusion: UniFit证明了MLLM和创新训练机制能大幅提升VTON框架的通用性和性能，为未来虚拟试衣领域提供了一个有效的普适解决方案。

Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.

</details>


### [2] [EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3](https://arxiv.org/abs/2511.15833)
*Chengxi Zeng,Yuxuan Jiang,Aaron Zhang*

Main category: cs.CV

TL;DR: 该论文提出了EfficientSAM3模型，通过级联蒸馏方法在保持高精度的前提下，大幅提升分割模型的设备端部署效率，适用于图像及视频的概念分割和跟踪任务。


<details>
  <summary>Details</summary>
Motivation: SAM3虽然具备强大的视觉理解和分割能力，但其统一架构（共享视觉主干、DETR检测器、密集内存跟踪器）模型过大，难以直接用于边缘设备。因此亟需提升分割模型的效率，便于实际落地应用。

Method: 提出了逐层级联蒸馏(PHD)策略，将SAM3的能力迁移到轻量学生模型，包括三步：(1) 编码器蒸馏，在大规模数据集通过带有提示的训练对齐特征空间；(2) 时序内存蒸馏，用精简的Perceiver模块替代原有的密集内存进行时空特征压缩和检索；(3) 端到端微调，在官方数据集上进一步提升整个流程的表现。采用RepViT、TinyViT、EfficientViT等轻量骨干作为学生模型。

Result: EfficientSAM3在主流视频对象分割（VOS）数据集上进行了基准测试，对比多种相关工作，在性能和效率之间实现了优越的平衡，即能在设备端运行又具备接近原模型精度。

Conclusion: EfficientSAM3在保证概念级分割和跟踪效果的同时，极大压缩了模型参数，实现端侧高效部署，推动了分割模型的实际应用前景。

Abstract: The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.

</details>


### [3] [WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion](https://arxiv.org/abs/2511.15874)
*Sajjad Pakdamansavoji,Yintao Ma,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: 本论文提出了一种针对遮挡场景下的6D姿态估计算法，有效提升了未见过物体的泛化能力，在速度和精度上均优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的6D姿态估计算法在遇到遮挡和处理未见过物体时精度下降较大，主因是传统多阶段流程容易在前期受遮挡干扰导致后续一系列环节出错。作者希望克服这些困难，提高在遮挡和泛化场景下的姿态估计表现。

Method: 论文提出四项创新：（1）动态非均匀密集采样，只关注可见区域以减小遮挡影响；（2）多假设推理机制，保留多个高置信姿态候选，降低单一路径失败风险；（3）迭代细化方法，逐步提升姿态精度；（4）专注遮挡的训练增强，强化模型鲁棒性和泛化。同时，设计了新的评估指标，以消除现有协议在遮挡下的偏差。

Result: 在ICBIN和BOP数据集上，方法在姿态估计精度上分别提升了5%和2%，推理速度提升约3倍。

Conclusion: 新提出的方法在遮挡和未见过物体场景下可显著提升6D姿态估计的准确性和效率，为相关应用提供更稳健和高效的解决方案。

Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

</details>


### [4] [Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation](https://arxiv.org/abs/2511.15875)
*Lukas Arzoumanidis,Julius Knechtel,Jan-Henrik Haunert,Youness Dehbi*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度生成技术和手工噪声退化方法，自动合成风格和噪声与真实历史地图相似的训练数据，从而缓解标注样本稀缺问题，并通过语义分割任务验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 历史地图的自动分析依赖深度学习，但缺乏高质量标注数据，尤其是在特定同质性地图语料中，标注成本高且耗时。如何低成本获得高质量、具有风格和噪声多样性的训练集是亟需解决的问题。

Method: 作者提出通过风格迁移将历史地图的制图风格迁移到矢量数据上，生成大量合成训练样本。同时，利用深度生成方法和手动随机退化（模拟扫描噪声和不确定性）进一步提升数据的真实感和多样性。最后利用自构图卷积网络（Self-Constructing GCN）对生成数据进行了同域语义分割实验评估。

Result: 所生成的数据集在域自适应语义分割任务中有效提升了模型的表现，证明了生成方法在实际任务中的可迁移性、有效性以及对数据稀缺问题的缓解作用。

Conclusion: 基于风格迁移和噪声建模的方法能够自动生成高质量、具有多样性的历史地图训练数据，有力支撑了深度学习在历史地图解析中的应用，为其他缺数据领域提供了借鉴。

Abstract: The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.

</details>


### [5] [Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes](https://arxiv.org/abs/2511.15884)
*Yintao Ma,Sajjad Pakdamansavoji,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: 提出了Box6D方法，在仓储环境中更加高效、准确地估算储物箱的6D位姿，相比现有方法推理速度提升76%。


<details>
  <summary>Details</summary>
Motivation: 现有的6D位姿估计方法在面对杂乱和遮挡时，不是灵活性不足（如基于CAD模型的方法），就是准确性不高（如示例或视频为基础的方法），或者泛化性太强忽略了环境和物体先验信息，难以满足工业实际需求。

Method: Box6D针对仓储箱体设计，仅需单帧RGB-D输入，通过快速二分搜索推断箱体尺寸，并以类别级的CAD模板（非实例特有）估算姿态，采用基于深度的可行性过滤和提前终止策略，剔除不合理假设并压缩计算量。

Result: 在现实仓储场景和公开基准数据上测试，Box6D在6D位姿估计准确率达到业界领先或更优，且推理时间缩短了约76%。

Conclusion: Box6D兼顾灵活性和准确性，提升了仓储自动化实际应用的可行性和效率，优于传统模型基和无模型方法，适用于工业级现实场景。

Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

</details>


### [6] [RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification](https://arxiv.org/abs/2511.15923)
*Meilong Xu,Di Fu,Jiaxing Zhang,Gong Yu,Jiayu Zheng,Xiaoling Hu,Dongdi Zhao,Feiyang Li,Chao Chen,Yong Cao*

Main category: cs.CV

TL;DR: 本文提出一种无需新标注，提高视觉语言模型（VLM）在特定领域视频分类任务表现的方法，通过自生理由辅助模型理解语义差距，显著提升少样本场景下的效果。


<details>
  <summary>Details</summary>
Motivation: VLM在多媒体理解中很重要，但在数据有限的特定领域视频分类任务中表现不足，原因在于现有数据难以弥合视频内容与抽象标签之间的语义差距。

Method: 采用两阶段自我提升范式：第一阶段，利用Prompt让VLM为每个视频生成详细理由（rationale），以此引导模型捕捉领域知识，并基于这些自我生成的理由进行微调；第二阶段再对任务标签做常规微调。

Result: 在多个数据集上实验，方法显著优于直接的有监督微调，表明自生理由显著促进了领域适应和模型效果提升。

Conclusion: 自生理由作为一种无需额外标注的高效范式，有效提升了VLM在特定领域视频分析上的适用性，为低资源场景下VLM适配提供了新思路。

Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.

</details>


### [7] [Boosting Medical Visual Understanding From Multi-Granular Language Learning](https://arxiv.org/abs/2511.15943)
*Zihan Li,Yiqing Wang,Sina Farsiu,Paul Kinahan*

Main category: cs.CV

TL;DR: 本文提出了多粒度语言学习（MGLL）框架，解决了视觉-语言对齐在多标签、多粒度场景下的局限性，特别是在医学影像等复杂领域。


<details>
  <summary>Details</summary>
Motivation: 现有如CLIP的视觉-语言预训练方法主要关注单标签、单粒度对齐，难以处理如医学影像这样涉及多标签、多粒度文本描述的复杂应用场景，因此亟需更适应实际需求的新方法。

Method: MGLL框架通过结构化多标签监督、整合多粒度文本描述，并引入点对点的软标签约束增强对齐；采用平滑Kullback-Leibler散度维护跨粒度一致性，该方法能作为即插即用模块集成进现有视觉-语言模型中。

Result: 在基于自建的大规模多粒度数据集及多个公开数据集的预训练和评测中，MGLL在下游任务上优于现有主流方法。

Conclusion: MGLL显著增强了多标签、多粒度的视觉-语言对齐能力，具有良好的泛化性和应用前景，可为相关领域任务带来性能提升。

Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.

</details>


### [8] [Automated Interpretable 2D Video Extraction from 3D Echocardiography](https://arxiv.org/abs/2511.15946)
*Milos Vukadinovic,Hirotaka Ieki,Yuki Sahasi,David Ouyang,Bryan He*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化方法，可以从3D心脏超声数据中生成标准2D视图，验证结果显示准确率高达96%，提升了心脏结构诊断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然3D心脏超声能够获取全面的心脏结构信息，但临床医生更习惯于使用标准2D视图进行解读。目前缺乏自动从3D超声生成2D标准视图的高效工具，这限制了3D超声在临床中的应用价值。

Method: 作者提出了结合深度学习视图分类器、基于解剖标志的启发式算法以及心脏病专家经验的自动2D视图提取方法。还通过三位心脏病专家盲态评估、AI异常检测模型和临床测量模型进行多维度验证。

Result: 该方法在1600个来自两家医院的视频上的准确率为96%。下游模型能够正确检测心脏异常并实现临床级的心脏结构测量。

Conclusion: 该方法实现了从3D心脏超声中自动高精度生成标准2D视图，便于医生沿用常规工作流，同时享受3D数据带来的空间信息优势，对实际临床具有很高应用价值。代码与部分数据已公开，为后续研究提供了资源。

Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .

</details>


### [9] [Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948)
*Raphael Ruschel,Hardikkumar Prajapati,Awsafur Rahman,B. S. Manjunath*

Main category: cs.CV

TL;DR: 该论文提出了Click2Graph，一个能结合用户交互和视频时空语义理解的全新交互式视频场景图生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有VSGG系统虽然能进行结构化视觉理解，但是封闭的流程，无法结合人工引导；而可交互分割模型如SAM2虽交互性强，但缺乏语义和关系推理能力。因此，亟需一个既能融入用户提示、又具备时空与语义推理的方案。

Method: Click2Graph通过一个用户提示（如点击或框选）进行主体分割与时序跟踪，自动发现与之交互的对象，并预测<主体,客体,谓词>三元组，生成时序一致的场景图。其创新组件包括动态交互发现模块（生成以主体为条件的对象提示）和语义分类头（联合实体与谓词推理）。

Result: 在OpenPVSG基准测试中，Click2Graph展现了在用户引导下高效的视频泛视角场景图生成能力，验证了方法有效性。

Conclusion: Click2Graph为用户可控的PVSG提供了坚实基础，表明人类提示与泛视角定位及关系推理的结合可实现更受控、更具解释性的视频场景理解。

Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.

</details>


### [10] [InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer](https://arxiv.org/abs/2511.15967)
*Muyao Yuan,Yuanhong Zhang,Weizhan Zhang,Lan Ma,Yuan Gao,Jiangyong Ying,Yudeng Xin*

Main category: cs.CV

TL;DR: 本文提出InfoCLIP方法，通过互信息理论视角将CLIP的对齐知识有效迁移到开放词汇语义分割任务中，稳健提升了细粒度语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 在开放词汇语义分割任务中，直接微调CLIP常导致过拟合和视觉-语言对齐丧失，现有方法难以兼顾泛化能力和对齐稳定性。如何在细粒度下高效稳健地迁移CLIP预训练的跨模态知识，是该领域亟需解决的问题。

Method: 提出InfoCLIP：一是通过压缩预训练CLIP的像素-文本对齐关系，减少由粗粒度监督导致的噪声；二是最大化预训练CLIP与微调模型对齐知识的互信息，优先传递紧凑、适用于分割任务的局部语义关系。

Result: 在多个主流基准上实证显示InfoCLIP有效提升了CLIP在开放词汇语义分割中的迁移、适应与细粒度性能表现，并在不对称任务迁移场景下表现优越。

Conclusion: InfoCLIP方法显著提升了CLIP微调在开放词汇语义分割任务中的效果，兼顾了预训练对齐知识的稳定传递与任务适应性，展现出良好泛化与迁移能力。

Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

</details>


### [11] [Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2511.15968)
*Jingru Zhang,Saed Moradi,Ashirbani Saha*

Main category: cs.CV

TL;DR: 提出了一种新的多任务学习方法，缓解了分割与分类任务间的干扰，显著提升了乳腺超声肿瘤分割泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在联合训练时常因任务间负面干扰导致性能不如单任务，限制了模型泛化能力，尤其是在医疗影像分割任务中。

Method: 设计了一种新的基于一致性正则化的多任务学习方法，利用可微分的BI-RADS启发形态特征，将分割与分类任务进行协调，以减少干扰并提升泛化能力。

Result: 在BrEaST数据集训练，并在三个外部乳腺超声数据集上验证，分割Dice系数均大幅优于基线方法（如0.81比0.59等），统计显著（p<0.001）。

Conclusion: 新方法能够有效抑制多任务学习中的破坏性干扰，显著提升乳腺超声肿瘤分割的泛化和鲁棒性，在外部验证数据上达到SOTA水平。

Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.

</details>


### [12] [UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition](https://arxiv.org/abs/2511.15984)
*Xinyu Nan,Lingtao Mao,Huangyu Dai,Zexin Zheng,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于检测引导的生成框架，可同时实现目标检测、类别预测和属性识别，解决了现有方法难以区分类别和捕捉属性多样性的难题，并在大规模电商数据集上取得了显著优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在实际大规模电商场景中，物体类别和属性丰富且细粒度，但现有方法多依赖全局相似度，难以识别类别间细微差别与部分类别的特殊属性分布，导致性能受限。作者希望提出一种能细致刻画类别和属性、实现统一语义理解的新框架。

Method: 作者设计了一个检测引导的生成式模型。每个检测到的目标，首先提取ROI特征，然后用BART生成器按粗到细序列化生成类别层次和属性-属性值对Token，并支持基于属性的条件识别。该方法支持更细粒度和层次化的信息抽取。

Result: 在大规模专有电商数据集和公开数据集上的实验表明，提出的方法在细粒度识别和统一推断方面显著优于现有的基于相似度管线和多阶段分类方法。

Conclusion: 检测引导的生成性框架为统一视觉语义理解任务提供了更优的能力，能够更好地区分细类别并捕捉类别特有属性，在实际大规模电商应用中表现出强大竞争力。

Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.

</details>


### [13] [Fairness in Multi-modal Medical Diagnosis with Demonstration Selection](https://arxiv.org/abs/2511.15986)
*Dawei Li,Zijian Gu,Peng Wang,Chuhan Song,Zhen Tan,Mohan Zhang,Tianlong Chen,Yu Tian,Song Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的去偏方法，以提升多模态大语言模型在医学影像推理任务中的公平性，无需依赖大规模标注数据或模型微调。作者提出用公平感知的示例选择方法，并在多个医学影像基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在医学影像推理中展现出强大潜力，但在不同人口群体间的公平性问题依然突出。现有去偏方法往往依赖大规模标注集或微调，不适用于基础模型。作者希望寻找一种更高效且无需微调的公平性提升途径。

Method: 作者将“in-context learning”（ICL，即上下文学习）作为去偏策略，具体提出了针对演示样例选择的公平感知方法（FADS）。该方法结合聚类采样，保证选取的示例在人口统计学属性上均衡，并兼顾语义相关性。

Result: 在多个医学影像基准数据集上，FADS方法能持续降低性别、种族等相关的不公平性，同时保持较高准确率，优于常规模型和示例选择策略。

Conclusion: 公平感知的上下文学习方法（FADS）可为基础大模型医疗推理任务带来高效、可扩展且数据需求低的公平性解决方案。这为实现公正的医学AI应用提供了新思路。

Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.

</details>


### [14] [Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection](https://arxiv.org/abs/2511.16015)
*Nimeshika Udayangani,Hadi M. Dolatabadi,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: 本文提出了一种基于图结构的长尾数据分布OOD检测方法，大幅提升了尾部类的检测能力和整体表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在部署时对异常分布（OOD）样本的检测能力有限，尤其是在实际存在长尾分布的数据集上，经常出现高误报率和对尾部类别不敏感的问题，因此迫切需要提升在这类场景下的OOD检测性能。

Method: 利用预训练模型的特征空间构建图结构，融合样本间关系。针对预训练和训练数据激活分布不一致问题，引入高斯化处理以缓解偏离标准正态分布的现象。随后通过图卷积网络（GCN）细化初始图表征，获得适合于长尾OOD检测的特征空间。

Result: 在CIFAR10-LT、CIFAR100-LT、ImageNet-LT三个长尾视觉基准上，所提方法在降低假阳性率（FPR）和提升尾部类识别准确率两方面均显著优于现有主流方法。

Conclusion: 基于图结构的特征表征与高斯化手段能够有效提升长尾分布下的OOD检测能力，尤其对尾部类别的识别效果有明显提升。

Abstract: Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.

</details>


### [15] [Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion](https://arxiv.org/abs/2511.16020)
*Dingkun Zhou,Patrick P. K. Chan,Hengxu Wu,Shikang Zheng,Ruiqi Huang,Yuanjie Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的人体穿戴式对抗攻击方法，通过优化衣服等物品上的图案，使其在整个运动视频序列中持续有效地欺骗深度神经网络的人体检测模型。该方法在真实和虚拟环境下均获得了良好的隐蔽效果。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击方法多在单帧图像下优化纹理，无法适应长视频中运动、姿态和服装变形带来的变化，导致在连续场景中隐蔽性不足。实际监控和隐私防护需求，需要在动态场景下持久且自然的攻击方法。

Method: 提出序列级优化框架，将服饰产品图片映射到UV空间，用调色板和控制点参数进行颜色压缩与可打印性锁定（ICC locking）。采用物理仿真技术还原运动、相机视角变化、衣服动力学和光照变化，并设计带时序权重的期望变换损失函数，对控制点进行优化，实现序列全局的检测置信度最小化。

Result: 实验显示，该方法生成的对抗服饰在虚拟和实体环境下均能实现稳定且持久的隐蔽效果，对不同视角和检测模型有很强的鲁棒性和可迁移性。实物服饰用升华印刷技术制作后，在室内外录制场景中也能有效抑制检测。

Conclusion: 提出的方法在安全性、隐私防护中具有现实价值，可在实际监控场景中穿戴应用，有效提升人体检测模型对抗攻击的实用性和可行性。

Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.

</details>


### [16] [Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution](https://arxiv.org/abs/2511.16024)
*Xiao He,Zhijun Tu,Kun Cheng,Mingrui Zhu,Jie Hu,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种结合稀疏门控专家混合（MoE）与LoRA模块的单步图像超分辨率新架构MoR，在保持计算预算不变的前提下，实现了更适应复杂多样退化样本的知识重组与共享，并通过引入退化感知动态专家分配机制取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA微调的扩散模型用于真实图像超分辨率时存在两大问题：一是模型“致密”，难以灵活应对复杂、多样的真实退化样本；二是知识共享和适应性有限，计算资源分配不够高效。针对上述问题，激发了将稀疏门控MoE引入该任务的动机。

Method: 作者提出Mixture-of-Ranks（MoR）架构，将LoRA中每个秩（rank）作为独立专家，通过细粒度专家划分实现灵活的知识重组，并设置固定位置rank为共享专家以保留共性特征。引入利用CLIP嵌入和正负文本对计算相对退化评分的退化估计模块，动态指导专家激活。此外设计了零专家槽和退化感知负载均衡损失，能据退化程度动态调整专家数量，实现计算资源最优分配。

Result: 实验结果表明，所提出方法在真实世界图像超分辨率任务上表现优异，相较于现有方法取得了更优的性能，验证了该框架的有效性和最优性。

Conclusion: 本文将稀疏门控MoE思想与LoRA相结合，设计了用于单步超分辨率的MoR架构，并提出退化感知的专家分配机制，有效提升了复杂场景超分辨率的表现和计算效率。

Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.

</details>


### [17] [Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning](https://arxiv.org/abs/2511.16026)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashur,Ahmed Elshinnawy*

Main category: cs.CV

TL;DR: 本论文提出了基于散斑成像和深度学习的激光切割材料识别方法，实现了高精度、鲁棒的材料分类，可有效提升激光切割过程的安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 激光切割过程中会产生大量粉尘和气溶胶，存在环境和健康风险。实现对切割材料的自动识别有助于实时调控切割过程，提高安全性和自动化水平。

Method: 利用材料表面的散斑图样，采集数据集，通过卷积神经网络（CNN）进行训练，识别不同材料类型。该方法还针对激光颜色变化对散斑分类影响进行了测试。

Result: 模型在训练集上达到了98.30%的准确率，验证集上为96.88%；在30种新材料的3000张新图像上测试，F1分数达0.9643，高效且具有良好泛化能力。

Conclusion: 提出的方法为基于散斑的材料识别提供了高精度且鲁棒的解决方案，即使在激光颜色变化条件下也能准确识别。该技术有助于实现材料感知型激光切割的智能监控与控制。

Abstract: Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.

</details>


### [18] [CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis](https://arxiv.org/abs/2511.16030)
*Zijian Wu,Mingfeng Jiang,Zidian Lin,Ying Song,Hanjie Ma,Qun Wu,Dongping Zhang,Guiyang Pu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CuriGS的课程引导框架，用于基于3D Gaussian Splatting的稀疏视角3D重建，通过引入伪视图增强监督，并采用多信号指标动态筛选优质伪视图以辅助训练，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在稀疏视角（可用视角有限）下难以获得高质量重建，原因在于监督不足与易过拟合，限制了3DGS在实际应用中的推广。

Method: CuriGS通过为每个真实视角（teacher）生成一组不同扰动程度的伪学生视角（student），按照课程学习机制逐步开放更高扰动等级；学生视图通过深度相关性和协同正则化进行约束，并用多重图像质量度量（SSIM、LPIPS等）筛选、保留高质量视图以增强训练集，从而实现稳定的数据增广。

Result: 在多个合成与真实的稀疏视角数据集上，CuriGS在渲染保真度和几何一致性方面均优于现有主流方法。

Conclusion: 通过结合课程学习和多信号筛选机制，CuriGS大幅提升了3DGS在稀疏视角条件下的3D重建表现，展现了方法的有效性和实用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/

</details>


### [19] [Crossmodal learning for Crop Canopy Trait Estimation](https://arxiv.org/abs/2511.16031)
*Timilehin T. Ayanlade,Anirudha Powadi,Talukder Z. Jubery,Baskar Ganapathysubramanian,Soumik Sarkar*

Main category: cs.CV

TL;DR: 本论文提出了一种跨模态学习策略，通过将无人机(UAV)级别的视觉细节融入高分辨率卫星影像，以提升作物冠层性状估算的精度。所训练的模型能够利用卫星和UAV配对影像，建立它们之间的细粒度光谱空间对应关系。结果表明，该方法生成的“类UAV”影像在多项农业监测任务上表现优于真实卫星影像。


<details>
  <summary>Details</summary>
Motivation: 虽然无人机在作物监测中表现优异，但难以大规模应用，而卫星影像则受限于空间分辨率，难以满足精细农业管理的需求。因此，亟需一种方法在两者之间建立联系，提升卫星影像的细节表现力。

Method: 收集了来自美国玉米带5个地点、84个玉米杂交品种重复试验地的卫星与配准UAV影像数据，利用跨模态学习模型训练，捕捉两种传感模态之间的光谱和空间细节对应关系，用于生成“类UAV”的卫星影像表示。

Result: 实验结果显示，通过该方法生成的增强型卫星影像，在产量预测和氮素预测等下游任务上，均优于仅用真实卫星影像的表现。

Conclusion: 跨模态对应关系学习能够有效弥合卫星与UAV传感在农业监测领域的差距，提升卫星影像在精细化农业中的应用价值。

Abstract: Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.

</details>


### [20] [LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets](https://arxiv.org/abs/2511.16037)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim,Qianru Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种结合大语言模型（LLM）的全新食品识别方法，有效缓解了数据分布、域迁移和细粒度分类难题，并在两个食品数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统食品识别面临互联网与真实场景图片间的域迁移、数据分布长尾以及细类别区分难度大等问题，现有方法难以兼顾这三者。作者希望借助LLM力量，提升食品识别的泛化能力和精细辨别能力。

Method: 方法包括：1）利用LLM解析食品图片，生成菜名和配料文本描述；2）将生成的文本与不同域的食品图片共同投影到同一嵌入空间；3）基于对齐后的多模态特征进行识别。该框架能够同时应对领域自适应、长尾分布与细粒度分类难点。

Result: 该方法在两个食品数据集上进行测试，在长尾数据分布、域自适应和细粒度分类方面均优于为这些任务定制的现有方法。

Conclusion: 结合LLM的多模态食品识别框架，在实际数据和各种挑战下表现出色，为食品识别任务提供了更强的泛化和区分类别的能力。

Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.

</details>


### [21] [AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers](https://arxiv.org/abs/2511.16047)
*Boxun Xu,Yu Wang,Zihu Wang,Peng Li*

Main category: cs.CV

TL;DR: 本文针对视觉自回归建模（VAR）在多尺度预测中的KV缓存瓶颈，提出了AMS-KV动态缓存策略，实现了大幅度缓存和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的KV缓存已广泛研究，但在以多尺度预测为基础的VAR模型中，KV缓存设计鲜有探讨，尤其是在尺度数量增加时的内存爆炸问题严重限制了扩展性。

Method: 作者系统分析了不同尺度KV的重要性与相似性，提出AMS-KV方法：优先缓存对生成质量贡献大的局部尺度与凝练尺度的KV，并通过跨尺度相似性判别高缓存需求层，灵活分配缓存资源，以在保证质量的同时减少内存占用。

Result: AMS-KV可减少最高84.83%的KV缓存使用，降低60.48%的自注意力延迟，并提升批量处理规模与吞吐量（如能将崩溃的批量128提升为256）。

Conclusion: 本文提出的AMS-KV极大提升了多尺度VAR模型的内存与运算效率，为大规模高效视觉生成奠定基础，具备很强实用价值。

Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

</details>


### [22] [LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving](https://arxiv.org/abs/2511.16049)
*Pei Liu,Songtao Wang,Lang Zhang,Xingyue Peng,Yuandong Lyu,Jiaxin Deng,Songxin Lu,Weiliang Ma,Xueyang Zhang,Yifei Zhan,XianPeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: 本文提出LiSTAR，一种直接作用于激光雷达原生几何结构的4D点云生成模型，通过创新表示和Transformer建模，实现高保真、可控的4D激光雷达数据合成。实验表明LiSTAR在生成、预测和条件生成等任务上均大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量、可控的4D激光雷达数据对自动驾驶仿真至关重要，但由于传感器的球面几何、点云时空稀疏及场景动态复杂，生成任务很有挑战。现有方法普遍受限于笛卡尔网格失真和时序建模能力不足。

Method: 提出Hybrid-Cylindrical-Spherical（HCS）表征以减少笛卡尔网格量化伪影，并设计Ray-Centric Transformer（START）以实现沿射线的时空建模；为可控生成，提出基于点云对齐体素布局的条件生成方法，并引入MaskSTART实现高效、可组合、高分辨率数据生成。

Result: 在4D激光雷达重建、预测和条件生成任务上，LiSTAR分别将生成MMD降低76%，重建IoU提升32%，预测L1 Med下降50%，均显著超越现有技术水平。

Conclusion: LiSTAR为实现高保真、可控自动驾驶系统仿真奠定了坚实基础，有望推动相关领域模拟与生成技术的进步。

Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.

</details>


### [23] [Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions](https://arxiv.org/abs/2511.16221)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Ruicong Liu,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文揭示了当前多模态大语言模型（MLLMs）在理解复杂社交情境和识别欺骗方面的重大不足，并提出了新的任务和数据集，推进模型更接近人类社交智能。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在推理能力上取得了巨大进展，但它们在判断复杂社交互动中的真伪、'察言观色'等人类关键社交智能方面表现不佳。作者为此设定新的评测任务，推动AI更具社交感知能力。

Method: 提出多模态互动欺骗评估（MIDA）任务，构建含有同步视频和文本及可验证标签的大规模多模态数据集。基于该任务，评测了12个领先的开放与闭源MLLMs，并设计了Social Chain-of-Thought（SoCoT）推理流程和Dynamic Social Epistemic Memory（DSEM）模块以提升模型表现。

Result: 大量实验显示，无论是如GPT-4o等强大的模型在区分真话与谎言时表现依然不理想，其主要原因是无法将语言有效与社交多模态线索结合，也无法很好地模拟他人知识、信念或意图。作者提出的SoCoT与DSEM方法取得了明显性能提升。

Conclusion: 当前MLLMs在社交推理和欺骗识别上存在显著短板，亟需新的策略来提升其可信与感知能力。本文提出的框架为构建更具人类社交智能的AI开辟了新路径。

Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.

</details>


### [24] [LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM](https://arxiv.org/abs/2511.16144)
*Sibaek Lee,Seongbo Ha,Kyeongsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.CV

TL;DR: 提出了一种名为LEGO-SLAM的系统，实现了3D高斯Splatting下的实时、开放词汇语义地图构建，同时大幅压缩内存需求，加快渲染速度，并在实验中证明了竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splatting的SLAM地图虽真实，但缺乏细致语义，难以支持通过自然语言的机器人交互，且存储高维语义信息成本高昂，不利于适应新环境。

Method: 设计了在线自适应的编码-解码器，将高维语言嵌入压缩为16维特征，精简每个高斯的存储，加速渲染。利用这些紧凑特征，提出了语义冗余剪枝和语言引导的回环检测，不需额外模型。

Result: 大幅减少地图高斯数量（减少60%以上）同时保持可视化质量，实现了15 FPS下实时、有竞争力质量和追踪精度的开放词汇语义地图。

Conclusion: LEGO-SLAM在提升SLAM系统语义表达能力的同时，有效降低了资源开销，为机器人等场景提供了可适用、开放语义的高效三维地图方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.

</details>


### [25] [VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)
*Zishan Xu,Yifu Guo,Yuquan Lu,Fengyu Yang,Junxin Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频推理分割方法VideoSeg-R1，通过强化学习提升了分割的推理与泛化能力，在多个基准数据集上表现优异，实现了复杂场景下的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频推理分割方法依赖有监督微调，通用性差且缺乏显式推理，难以应对分布外场景，因此亟需提升泛化能力与推理能力。

Method: VideoSeg-R1首次将强化学习引入视频推理分割，并采用解耦架构，将任务分为图像引用分割与视频蒙版传播两部分。具体流程包括：1）层次化文本引导帧采样，模拟人类注意力；2）推理模型输出空间线索与推理链；3）结合SAM2和XMem进行分割传播。同时引入难度自适应机制动态调整推理长度，提高效率和准确率。

Result: 在多个视频推理与分割基准上进行了全面评估，VideoSeg-R1在复杂视频推理与分割任务上取得了最先进（state-of-the-art）的性能。

Conclusion: VideoSeg-R1有效解决了传统方法泛化能力差和缺乏显式推理的问题，显著提升复杂视频场景推理分割的准确性和效率，为后续相关研究提供了新范式。

Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.

</details>


### [26] [SpectralTrain: A Universal Framework for Hyperspectral Image Classification](https://arxiv.org/abs/2511.16084)
*Meihua Zhou,Liping Yu,Jiawei Cai,Wai Kin Fung,Ruiguo Hu,Jiarui Zhao,Wenzhuo Liu,Nan Wan*

Main category: cs.CV

TL;DR: 该论文提出了SpectralTrain训练框架，将课程学习与PCA光谱降采样相结合，显著加快高光谱图像分类模型的训练速度，同时保持了准确率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类需要处理大量数据及高计算成本，导致深度学习模型难以实际部署。为了解决效率瓶颈，作者希望通过优化训练流程，实现更快、更高效的模型训练。

Method: 作者设计了SpectralTrain框架，将课程学习方法与PCA光谱降采样结合。通过逐步增加光谱复杂度，保留关键信息，使模型能在低计算资源下，学习到有效的光谱-空间特征。该框架不依赖具体的模型结构、优化器或损失函数，支持新旧多种模型。

Result: 在三个基准数据集（Indian Pines、Salinas-A、CloudPatch-7）上广泛实验，SpectralTrain展现出良好的泛化性和领域适应性。与原始训练方式相比，训练时间普遍减少2到7倍，且准确率损失较小（视主干网络而定）。在云分类等气候遥感任务上表现突出。

Conclusion: SpectralTrain极大提升了高光谱图像分类的训练效率，而且适用于各类模型和遥感场景。训练策略的优化能够有效补充模型结构创新，为相关应用带来更高计算效率。

Abstract: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.

</details>


### [27] [TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding](https://arxiv.org/abs/2511.16595)
*Boshen Xu,Zihan Xiao,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Qin Jin*

Main category: cs.CV

TL;DR: TimeViper是一种创新的多模态混合架构，能够高效理解超长视频，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 长视频理解任务对模型在高效性和长时序上下文处理能力方面提出了更高的要求，现有架构在处理超长视频（例如超过10,000帧的小时级视频）时存在效率和信息冗余问题。

Method: 该方法提出了TimeViper模型，采用结合了Mamba状态空间模型效率和Transformer注意力机制表达能力的混合主干网络。提出TransV模块，将视觉token信息高效转移并压缩为指令token，降低冗余，同时保持多模态理解能力。

Result: TimeViper能够高效处理超过10,000帧的小时级长视频，并在多个基准数据集上与当前最先进模型竞争。实验还揭示了视觉到文本token聚合现象，并分析了Mamba与Transformer层的注意力行为。

Conclusion: TimeViper展示了混合Mamba-Transformer架构在视频理解任务上的巨大潜力，并为其可解释性和压缩带来了新的见解，是长视频多模态理解的重要前进步伐。

Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.

</details>


### [28] [Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments](https://arxiv.org/abs/2511.16091)
*Renxiang Xiao,Wei Liu,Yuanfan Zhang,Yushuai Chen,Jinming Chen,Zilu Wang,Liang Hu*

Main category: cs.CV

TL;DR: 本文提出了Rad-GS系统，一种基于4D毫米波雷达与相机的SLAM方法，能在公里级室外环境中实现高效、高精度的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模三维重建主要依赖相机或激光雷达，受光照、天气等影响较大，且在大场景下存在噪声和存储瓶颈。毫米波雷达数据利用率低，信息融合难度大。本文旨在提升毫米波雷达在大规模重建中的表现，解决噪声、渲染伪影和存储效率问题。

Method: Rad-GS利用3D高斯作为可微空间表示，融合原始雷达点云（含多普勒信息）和几何增强点云，引导图像中动态目标掩码，抑制伪影并提升定位精度。系统还利用不同步图像全局优化高斯表达以提升纹理一致性，并结合全局八叉树与高斯体元管理策略，有效去噪及降低内存占用。

Result: 实验和消融研究表明，Rad-GS在公里级户外环境下的定位与重建精度与主流基于相机或激光雷达的3D高斯方法相当，高效抑制噪声，显著降低了内存消耗。

Conclusion: Rad-GS验证了4D毫米波雷达融合方法在大规模三维重建中的可行性和优越性，为复杂户外场景的高效、鲁棒重建提供了新方案。

Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.

</details>


### [29] [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](https://arxiv.org/abs/2511.16635)
*Guolin Huang,Wenting Chen,Jiaqi Yang,Xinheng Lyu,Xiaoling Luo,Sen Yang,Xiaohan Xing,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了SurvAgent体系，实现了多模态、生存分析中链式思维可解释化预测，在五个TCGA队列中效果优于传统及新兴方法。


<details>
  <summary>Details</summary>
Motivation: 现有癌症生存分析方法透明度不足，不符合临床需求。近期病理智能体虽在诊断任务上可解释性提升，但在生存预测上仍存在三大问题：无法整合多模态数据、ROI探索不足、缺乏历史经验学习。

Method: 提出SurvAgent多智能体体系，包括两阶段：第一，WSI-基因CoT增强案例库，结合分层影像分析（低倍筛查、跨模态/置信补丁挖掘）与基因功能分群，形成结构化推理记录；第二，基于二分推理的多专家智能体，以RAG检索相似病例并联合多模态报告与专家预测，进行区间递进式推理。

Result: 在5个TCGA癌症队列上实验证明，SurvAgent较传统方法、现有MLLM、医疗智能体均有明显性能提升。

Conclusion: SurvAgent建立了可解释、有效的多模态癌症生存预测新范式，推动精准肿瘤学AI临床应用。

Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.

</details>


### [30] [T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs](https://arxiv.org/abs/2511.16107)
*Shao-Jun Xia,Huixin Zhang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出T2T-VICL方法，在视觉语言模型(VLM)中实现跨视觉任务的视觉上下文学习（VICL），利用生成和选择差异性文本提示，结合感知分数推理和经典度量，取得卓越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉上下文学习在同一任务下表现优异，面对跨视觉任务时VLM能否仍有效尚不明确，因此需要探索和扩展VICL的能力边界。

Method: 作者设计了T2T-VICL管线，提出可自动生成和筛选最能区分两种低层视觉任务的文本提示机制，并建立了首个跨任务VICL数据集。在推断阶段，结合了基于感知分数的推理方法和传统评估指标。

Result: 该方法在九个跨任务场景中达到顶级表现，并在另外十个场景中取得次顶级表现，有效提升了VLM在跨任务VICL中的能力。

Conclusion: T2T-VICL显著拓展了视觉语言模型在不同视觉任务之间进行上下文学习的能力，并为跨任务VICL提供了新的研究范式。

Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.

</details>


### [31] [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](https://arxiv.org/abs/2511.16671)
*Ziyu Guo,Renrui Zhang,Hongyu Li,Manyuan Zhang,Xinyan Chen,Sifan Wang,Yan Feng,Peng Pei,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了TwiG（Thinking-while-Generating）框架，实现了视觉生成过程中与文本推理的实时交互，并探索了三种训练策略以优化生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成方法虽然结合了文本推理，但通常只在生成前或生成后进行，并未实现在生成过程中的多模态动态交互。为提升输出内容的语义和上下文相关性，亟需一种能在生成过程中持续嵌入推理的机制。

Method: 作者提出了TwiG框架，使文本推理与视觉生成过程交替进行，通过即时推理引导后续生成并反思已生成内容。方法涉及三种训练策略：零样本提示（zero-shot prompting）、TwiG-50K数据集上的有监督微调（SFT）、以及基于定制策略的强化学习（TwG-GRPO）。

Result: 初步实验表明，TwiG框架可以提升生成内容的上下文感知能力和语义丰富性，不同训练策略展现出互补优势。

Conclusion: TwiG为多模态协同推理和生成提供了全新思路，有望推动视觉生成领域的发展。作者呼吁进一步探索交错推理在视觉生成中的应用。

Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.

</details>


### [32] [Clustered Error Correction with Grouped 4D Gaussian Splatting](https://arxiv.org/abs/2511.16112)
*Taeho Kang,Jaeyeon Park,Kyungjin Lee,Youngki Lee*

Main category: cs.CV

TL;DR: 本文提出了一种改进的4D Gaussian Splatting方法，有效提升了动态场景的渲染质量和时序一致性。通过引入椭圆误差聚类与纠正增溅和分组4D高斯溅射两大机制，显著提升了动态区域的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有4DGS方法在动态场景重建时，普遍存在像素对应模糊、动态区域加密不足等问题，导致渲染质量和一致性下降。为此，本文提出针对性方法，旨在改善动态区域的重建精度。

Method: 1）提出椭圆误差聚类与纠正增溅，通过聚类渲染误差与有针对性的新增高斯溅点，提升动态区域拟合。2）采用分组4D高斯溅射，提升溅点与动态对象之间的映射一致性。对于不同类型的渲染错误（如颜色缺失和遮挡），采用反投影或前景分割等定向修正方法。

Result: 在Neural 3D Video和Technicolor数据集上的实验表明，所提方法明显提升了渲染的时序一致性，在Technicolor Light Field数据集上PSNR提升了0.39dB，并在可视化中展现出更优的溅点与动态对象对齐及误差修正能力。

Conclusion: 本文方法有效弥补了4DGS在动态场景渲染中的不足，显著提升了动态区域重建和渲染质量，实现了当前最优的感知渲染表现。

Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.

</details>


### [33] [Decoupling Complexity from Scale in Latent Diffusion Model](https://arxiv.org/abs/2511.16117)
*Tianxiong Zhong,Xingye Tian,Xuebo Wang,Boyuan Jiang,Xin Tao,Pengfei Wan*

Main category: cs.CV

TL;DR: DCS-LDM提出了一种新的视觉生成范式，将信息复杂度与尺度解耦，实现了高效且灵活的多尺度图像和视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在扩散模型在生成视觉内容时，将分辨率或帧率等尺度与内容的复杂度紧密耦合，导致高分辨率/高帧率下需要更多的潜在表示。但实际上，所需的潜在容量更多取决于内容本身的复杂度，尺度仅作为上限。作者希望通过解耦复杂度与尺度，提升生成的效率与灵活性。

Method: 作者提出DCS-LDM，构建了分层、与尺度无关的潜在空间，通过多层次token建模内容复杂度，同时允许在固定潜在表示下对任意分辨率和帧率进行解码。该方法还支持层次化地表达结构和细节，实现粗到细的逐步生成。

Result: 实验表明，DCS-LDM在视觉生成性能上可以与最新方法比肩，并且能够灵活适应不同的尺度和视觉质量需求，同时实现计算资源和生成质量之间的自由权衡。

Conclusion: DCS-LDM打破了传统扩散模型中复杂度与尺度绑定的限制，实现了灵活的多尺度视觉生成，为视觉建模带来了新的高效范式。

Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.

</details>


### [34] [VTinker: Guided Flow Upsampling and Texture Mapping for High-Resolution Video Frame Interpolation](https://arxiv.org/abs/2511.16124)
*Chenyang Wu,Jiayi Fu,Chun-Le Guo,Shuhao Han,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于流的视频帧插值方法VTinker，通过引入引导流上采样（GFU）和纹理映射，有效缓解了高分辨率下的运动估计模糊和伪影问题，并取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视频由于像素运动量大、计算开销高，现有方法一般采用低分辨率预测双向流后再上采样方式，但易导致边缘模糊、马赛克和细节丢失，最终导致插值帧出现伪影和断裂。

Method: 作者设计了两个核心模块：一是引导流上采样（GFU），利用输入帧内容作为引导，提升流边缘上采样的清晰度。二是纹理映射模块，生成中间代理帧，作为从输入帧中选取清晰纹理块并映射到代理帧的依据，最后经过重建模块产生最终插值帧。

Result: 大量实验表明，VTinker方法在主流VFI基准上实现了当前最优的插值效果，经主观和客观评价均优于以往方法。

Conclusion: VTinker通过改进运动估计上采样及引入纹理映射，显著改善了高分辨率视频帧插值的细节表现和视觉质量，为流基础VFI提供了有效新思路。

Abstract: Due to large pixel movement and high computational cost, estimating the motion of high-resolution frames is challenging. Thus, most flow-based Video Frame Interpolation (VFI) methods first predict bidirectional flows at low resolution and then use high-magnification upsampling (e.g., bilinear) to obtain the high-resolution ones. However, this kind of upsampling strategy may cause blur or mosaic at the flows' edges. Additionally, the motion of fine pixels at high resolution cannot be adequately captured in motion estimation at low resolution, which leads to the misalignment of task-oriented flows. With such inaccurate flows, input frames are warped and combined pixel-by-pixel, resulting in ghosting and discontinuities in the interpolated frame. In this study, we propose a novel VFI pipeline, VTinker, which consists of two core components: guided flow upsampling (GFU) and Texture Mapping. After motion estimation at low resolution, GFU introduces input frames as guidance to alleviate the blurring details in bilinear upsampling flows, which makes flows' edges clearer. Subsequently, to avoid pixel-level ghosting and discontinuities, Texture Mapping generates an initial interpolated frame, referred to as the intermediate proxy. The proxy serves as a cue for selecting clear texture blocks from the input frames, which are then mapped onto the proxy to facilitate producing the final interpolated frame via a reconstruction module. Extensive experiments demonstrate that VTinker achieves state-of-the-art performance in VFI. Codes are available at: https://github.com/Wucy0519/VTinker.

</details>


### [35] [How Noise Benefits AI-generated Image Detection](https://arxiv.org/abs/2511.16136)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Kai Zeng,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出了一种新的方法（PiN-CLIP）用于检测AI生成图像，并显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 大量生成模型的发展让真实与合成图像难以区分，尤其是在分布外泛化任务中现有检测方法表现较差，主要原因在于训练时模型依赖于虚假的捷径特征。

Method: 提出Positive-Incentive Noise for CLIP（PiN-CLIP）框架，通过联合训练噪声生成器和检测网络，并在特征空间引入由视觉与语义特征融合生成的激励噪声。优化过程中噪声被注入特征空间，强化鲁棒法医线索，抑制捷径特征影响，从而增强检测器的泛化能力。

Result: 在包含42种生成模型的开放世界数据集上实验，取得了比现有方法高5.4个百分点的平均准确率，达到最新最佳结果。

Conclusion: PiN-CLIP方法在检测AI生成图像任务中特别是在分布外场景下表现优异，提升了检测的鲁棒性和泛化能力。

Abstract: The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.

</details>


### [36] [Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video](https://arxiv.org/abs/2511.16137)
*Li Yu,Yingbo Zhao,Shiyu Wu,Siyue Yu,Moncef Gabbouj,Qingshan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种面向压缩视频质量增强的新方法，既能应对未知量化参数（QP）的盲增强场景，又能根据压缩程度动态调整网络计算量，在提升画质的同时显著缩短推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩视频增强方法（QECV）大多依赖已知的QP参数，并针对不同QP分别训练增强模型，在实际中QP可能未知或部分缺失，导致此类方法应用受限，催生了盲质量增强技术的需求。此外，目前盲增强方法只关注全局退化信息，忽略了空间细节，且方法无法针对不同压缩等级优化计算资源利用。

Method: 作者提出了预训练的退化表征学习（DRL）模块，可以从视频内容中解耦、提取高维多尺度的退化特征，用于指导去伪影处理。同时，提出了分层终止机制，根据压缩程度动态调整去伪影阶段数，以提高计算效率。

Result: 该方法在实验中显著优于最新盲增强方法，在QP=22时PSNR提升110%（从0.31 dB提升到0.65 dB）。此外，分层终止机制可使QP=22下平均推理时间相比QP=42下降一半。

Conclusion: 提出的方法能更好适应实际应用中的未知QP状况，并且结合内容自适应的特征提取及动态计算架构，显著提升了增强效果和推理效率。

Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.

</details>


### [37] [Real-Time 3D Object Detection with Inference-Aligned Learning](https://arxiv.org/abs/2511.16140)
*Chenyu Zhao,Xianwei Zheng,Zimin Xia,Linwei Yue,Nan Xue*

Main category: cs.CV

TL;DR: 本文提出了一种名为SR3D的3D目标检测框架，通过空间优先和排序感知机制，对点云进行高效、准确的目标检测，实验表明该方法在准确率和实时性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在点云3D目标检测中，训练与推理阶段之间由于空间可靠性和排序感知不足，导致性能下降。该论文旨在缩小这种训练推理差距，从而提升模型检测效果。

Method: SR3D包含两个新颖的训练组件：一是空间优先的最优传输分配方法，动态强调空间上定位准确、可靠的样本；二是排序感知的自适应自蒸馏机制，在训练中自适应地注入排序信息，使训练过程更贴合推理时的行为。

Result: 在ScanNet V2和SUN RGB-D两个数据集上，SR3D检测框架显著提升了检测精度，并且保持了实时推理速度，优于现有主流方法。

Conclusion: SR3D通过空间优先和排序感知机制，有效缩小了训练与推理之间的行为差距，推动了点云3D目标检测模型在实用场景中的准确性与高效性。

Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.

</details>


### [38] [A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection](https://arxiv.org/abs/2511.16143)
*Quanqing Ma,Jiaen Chen,Peng Wang,Yao Zheng,Qingzhan Zhao,Yuchen Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于遥感水体变化检测（WBCD）的高分辨率新数据集HSRW-CD，并设计了空间语义与连续性感知（SSCP）注意力模块，有效提升了检测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有WBCD研究多受限于高分辨率数据匮乏，难以满足城镇及农村对定位精度的高要求。同时，深度学习方法未能充分利用特征的空间语义与结构信息，降低了判别能力。

Method: 1）提出HSRW-CD高分辨率水体变化检测数据集，丰富样本类型与数量；2）设计SSCP注意力模块，包括多语义空间注意力（MSA）、结构关系感知全局注意力（SRGA）和通道自注意力（CSA），实现空间语义、结构与通道特征的深度融合。该模块能够以即插即用方式嵌入现有WBCD网络。

Result: 在HSRW-CD和Water-CD数据集上大量实验表明，SSCP模块能显著提升水体变化检测的精度与泛化能力，优于现有方法。

Conclusion: 本工作解决了高分辨率水体数据集不足及空间信息利用不足的问题，提出的HSRW-CD数据集与SSCP模块为WBCD研究和实际应用提供了有力支持，并可灵活集成到各类相关模型中。

Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.

</details>


### [39] [Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval](https://arxiv.org/abs/2511.16150)
*Chunxu Liu,Jiyuan Yang,Ruopeng Gao,Yuhan Zhu,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 提出了一种新的多模态嵌入方法（RGE），通过引入显式推理步骤提升多模态检索表现。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLM具备强大的生成与推理能力，但现有多模态嵌入提取方法仅直接编码，未充分利用其推理能力，可能限制了嵌入的表达质量。

Method: 提出Reasoning Guided Embeddings (RGE) 方法，通过指令引导MLLM生成结构化推理过程，并在推理展开后提取表示，结合对比训练以增强嵌入表现力。

Result: 在MMEB基准上的实验显示，推理引导下的嵌入比非推理基线提升了4.9%的多模态检索性能。

Conclusion: 显式引入推理过程可以有效提升多模态嵌入质量，RGE方法的应用带来了更优的下游任务表现。

Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.

</details>


### [40] [Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers](https://arxiv.org/abs/2511.16156)
*Jian Ma,Qirong Peng,Xujie Zhu,Peixing Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种针对Diffusion Transformer（DiT）架构的高效结构化剪枝方法（PPCL），在极大减少模型参数量的同时保持了图像生成质量，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: DiT在图像生成任务中表现卓越，但其庞大的参数量导致计算开销高，难以在设备资源有限的环境中部署。因此亟需开发能在保证性能的同时有效压缩模型规模的方案。

Method: 作者提出了PPCL框架，核心包括两步：首先利用线性探测与一阶导数趋势分析识别冗余层区间；随后通过以深度和宽度为主的“即插即用”师生交替蒸馏，实现单次训练下跨剪枝比例的灵活知识迁移，无需为每一种结构单独再训练。

Result: 在多个多模态DiT模型上的大量实验表明，PPCL可将参数量减少50%，而关键性能指标仅下降小于3%。同时保持了高质量的图像生成能力，并实现了更高的压缩率。

Conclusion: PPCL为DiT架构模型提供了一种高效可扩展的剪枝策略，提升其在资源受限场景下的实用性且不需为每一剪枝配置专门再训练，具有很高的实际推广价值，相关代码和模型权重已经开源。

Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.

</details>


### [41] [Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning](https://arxiv.org/abs/2511.16160)
*Yibin Huang,Wang Xu,Wanyue Zhang,Helu Zhi,Jingjing Huang,Yangbin Xu,Yangang Sun,Conghui Zhu,Tiejun Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架Video2Layout，用于从视频中重建基于度量的空间布局，克服现有栅格化认知地图在细粒度空间推理能力的不足。实验表明，该方法在主流空间推理任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型通过栅格化地图实现空间感知，但这种离散化方式限制了模型的细粒度空间推理能力。鉴于人类与真实物体的交互多数基于连续且可量化的空间信息，因此作者希望提升模型的空间理解和推理能力。

Method: 作者提出Video2Layout框架，利用对象的连续边界坐标量化目标间物理距离与尺寸，实现空间布局的精确重建。方法分为两阶段：第一，利用AI2THOR模拟器构建高质量数据集，进行有监督精细调优，学习视觉输入到边界坐标的映射；第二，强化学习阶段进一步提升模型在实际环境中的泛化能力。此外，提出了QVS-Bench基准，用于系统评估输入图像数量与空间推理准确性的关系。

Result: 在QVS-Bench及主流空间推理基准上，提出的V2LO-7B模型在各项指标上均优于基于栅格地图训练的模型，平均提升约4.92%。

Conclusion: Video2Layout通过度量化空间布局显著提升了细粒度空间推理能力，为多模态大模型理解物理世界带来了新的突破。

Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.

</details>


### [42] [Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion](https://arxiv.org/abs/2511.16161)
*Lirui Zhang,Zhengkai Zhao,Zhi Zuo,Pan Gao,Jie Qin*

Main category: cs.CV

TL;DR: 本文提出了Simba框架，用于高效且鲁棒的点云补全，结合对称先验与扩散模型，在多个基准上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法虽然能保留几何结构细节，但容易过拟合、对噪声敏感，缺乏泛化能力，需要更通用与鲁棒的处理框架。

Method: 将传统的点对点变换回归问题重构为概率分布学习，引入对称先验并结合扩散生成模型，设计分层Mamba网络实现高精度上采样。

Result: 在PCN、ShapeNet和KITTI等主流数据集上，实验显示新方法相比现有方法具有更好的细节还原能力和鲁棒性，性能达到目前最优。

Conclusion: Simba框架有效提升了点云补全的精度与泛化能力，展现出强壮的几何建模和抗噪声能力，为三维视觉任务提供了新思路。

Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.

</details>


### [43] [Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation](https://arxiv.org/abs/2511.16162)
*Yuting Lu,Ziliang Wang,Weixin Xu,Wei Zhang,Yongqiang Zhao,Yang Yu,Xiaohong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LNG-SWR（分层噪声引导选择性小波重建）的新方法，用以提升医学图像分割模型在分布变化和攻击下的鲁棒性，同时避免主流方法（如对抗训练）常见的精度损失与高开销问题，在多组数据集和不同攻击下均显著提升分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前医学图像分割模型在实际临床环境中面对分布漂移和干扰表现不稳，主流的对抗训练虽提升鲁棒性，但常带来准确率下降以及高训练和调优成本，限制实际应用，急需简单有效的新方法。

Method: 在训练阶段，于模型多层输出注入微小零均值噪声，引导模型学习频率偏置先验，避免对噪声敏感方向的依赖。随后，在输入或中间特征上进行先验引导的小波选择性重建，从频率域抑制噪声敏感成分、增强结构与边缘信息、保持频谱一致性。整个框架可灵活适配不同骨干网络，推理阶段开销低，可单独或与对抗训练结合应用。

Result: 在CT和超声数据集上，与PGD攻击、SSAH协议下评测，LNG-SWR在干净数据Dice/IoU稳定提升，在强攻击下性能下降显著收敛，同时与对抗训练结合可叠加提升鲁棒性且无准确率损失。

Conclusion: LNG-SWR方法为医学图像分割的鲁棒性提升提供了一条简单、有效、易工程化的解决路径，适用于标准与对抗训练，兼顾干净精度与鲁棒性，促进了方法的可扩展性与临床落地。

Abstract: Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.

</details>


### [44] [An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs](https://arxiv.org/abs/2511.16163)
*Zhi Luo,Zenghui Yuan,Wenqi Wei,Daizong Liu,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的攻击方法VTIA，通过对输入图像加入不可察觉的对抗扰动，诱使视觉-语言模型（VLMs）生成冗长、低信息密度文本，从而显著提高其能耗和token消耗。实验显示该方法在多个主流VLM上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽然在多模态任务中表现优异，但生成过程中token消耗大，且易被特定输入诱导生成冗长文本，导致能耗与计算成本升高。现有延长输出的对抗方法缺乏稳定性与可控性，未能最大化输出token数量。作者希望开发更高效、可控的攻击方法，提高模型部署时的鲁棒性认识。

Method: 作者提出了一种两阶段的对抗攻击框架。第一阶段，通过强化学习自动搜索能够诱导模型生成冗长回答的对抗prompt。第二阶段，对输入图像施加扰动，优化其视觉特征与对抗prompt特征的相似度，从而构造出能触发冗长文本生成的对抗样本。

Result: 在四种主流VLM（视觉-语言模型）上的实验证明，该方法在诱导模型生成冗长输出、效率和通用性等方面，均优于现有方法，显著增加了token消耗。

Conclusion: 本文提出的VTIA方法能有效地生成对VLMs极具攻击性的图像样本，引发其生成低密度、冗长的文本输出，对实际部署中的能耗和成本有重要影响，并为多模态模型的安全研究提供了新视角。

Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.

</details>


### [45] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA是一种自监督视觉-语言-动作模型，专为解决长时序机器人操作中的阶段幻觉问题，显著提高了任务成功率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在长时序多阶段任务中容易产生阶段幻觉（stage hallucination），即模型通过利用粗糙的评估信号“走捷径”，未真正完成任务步骤便报告高进度，限制了实际应用中的可靠性与泛化能力。

Method: 作者提出EvoVLA框架，包含三大核心组建：（1）Stage-Aligned Reward（SAR）- 使用与Gemini生成的困难负样本结合的三元组对比学习，有效阻止模型利用视觉捷径；（2）Pose-Based Object Exploration（POE）- 通过物体与抓取器的相对位姿而非像素数据驱动好奇心探索，更贴合真实操作需求；（3）Long-Horizon Memory - 利用选择性上下文保留和门控融合，提升长时序操作过程中的内在奖励稳定性。

Result: 在Discoverse-L长时序基准上的多阶段任务中，EvoVLA相比最强基线OpenVLA-OFT平均任务成功率提升10.2个百分点至69.2%，样本效率提升1.5倍，阶段幻觉发生率由38.5%降至14.8%。现实机器人实测，EvoVLA在4个操作任务中平均成功率达54.6%，较基线高出11个点。

Conclusion: EvoVLA在防止阶段幻觉、提升长时序操作的泛化能力和现实部署表现上具有显著优势，为VLA模型的可靠落地提供了新的解决思路和有效技术。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [46] [Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective](https://arxiv.org/abs/2511.16170)
*Jiahao Li,Yang Lu,Yachao Zhang,Yong Xie,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本论文提出了一种提升开集语义分割模型（OVSS）在像素级多模态对齐方面的新方法RF-CLIP，通过分析和模仿人类注意力再聚焦机制，提升了CLIP模型的细粒度对齐和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的开集语义分割方法在像素级多模态对齐方面存在性能瓶颈，但对此的可解释性机制研究较少。论文希望通过分析CLIP的内部注意力机制，挖掘其在密集预测任务的性能极限，并提升其对无关区域的注意力分配效率。

Method: 作者系统性分析了CLIP模型在密集预测过程中的注意力分布，发现其像人类注意力分散一样，将部分关注分配给了无关token，并且这些无关token源自于特定维度的过度激活。为此，提出了无需训练的ReFocusing CLIP（RF-CLIP）方法，可以自动过滤这些分散注意力的token，并重定向注意力至目标区域，优化多模态像素级对齐。

Result: RF-CLIP方法在八个开集语义分割基准测试上取得了当前最优的性能表现（SOTA），同时保持高度的推理效率。

Conclusion: 通过理解和仿真人类注意力再聚焦行为，RF-CLIP有效提高了CLIP模型在像素级多模态对齐和密集预测任务中的细粒度性能，为开集语义分割提供了更高效且可解释的新思路。

Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.

</details>


### [47] [Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight](https://arxiv.org/abs/2511.16175)
*Yi Yang,Xueqi Li,Yiyang Chen,Jin Song,Yihan Wang,Zipeng Xiao,Jiadi Su,You Qiaoben,Pengfei Liu,Zhijie Deng*

Main category: cs.CV

TL;DR: Mantis提出了一种解耦视觉预测与主体架构的VLA模型，通过引入Diffusion Transformer和meta queries实现高效、精准的动作和状态预测，显著提升理解、推理和泛化能力，并在多个基准和现实世界任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在高维视觉状态预测与有效利用语言监督之间存在瓶颈：直接预测视觉状态开销大、训练困难，压缩后信号又损失信息，且常忽视语言监督带来的理解和推理能力。

Method: Mantis框架采用可解耦的Disentangled Visual Foresight（DVF）设计，将视觉前馈预测从主编码网络分离，并结合meta queries与Diffusion Transformer实现下一状态预测。通过残差连接当前视觉状态，meta queries自动捕捉描述视觉轨迹的潜在动作，以提升动作学习效率，同时主体架构保持对语言的理解与推理能力。

Result: 在大规模人类操作视频、机器人演示和图文对齐数据集上预训练后，Mantis在LIBERO基准上微调取得96.7%的成功率，超越主流基线并收敛速度更快；现实测试中在指令跟随、泛化和推理能力上大幅优于知名开源模型π₀.₅。

Conclusion: Mantis框架通过视觉-语言能力解耦和高效状态预测，在视觉理解、动作推理和任务泛化上大幅提升，展示了下一代高效VLA模型的潜力，并已开放代码和权重以促进研究社区发展。

Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.

</details>


### [48] [Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.16184)
*Nianchang Huang,Yi Xu,Ruida Xi,Ruida Xi,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出一种面向可见光-红外行人重识别(UDA-VI-ReID)的新方式，显著提升跨域识别性能。


<details>
  <summary>Details</summary>
Motivation: 现实中可见光-红外行人重识别模型公有数据和真实场景有差距，已有方法难以无监督适应新领域。作者旨在实现无需额外标注情况下知识迁移，提高实际应用效果。

Method: 分析了UDA-VI-ReID面临的域间模态差异和域内模态差异两大挑战。为此提出了两阶段DSLGA模型：第一阶段采用域共享学习策略（DSLS）减轻预训练时的域间差异；第二阶段用渐进对齐策略（GAS）通过聚类到整体对齐的方式解决可见光与红外间的对齐难题。此外，构建了CMDA-XD测试方法以评估模型。

Result: 大量实验表明，所提方法在多种设置下显著优于现有的域适应方法，甚至在部分条件下超越部分有监督方法。

Conclusion: DSLGA方法在解决UDA-VI-ReID中的模态差异问题具有很强的有效性和泛化能力，有望推动现实场景下的行人重识别落地。

Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.

</details>


### [49] [PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction](https://arxiv.org/abs/2511.16186)
*Deniz Sayin Mercadier,Hieu Le,Yihong Chen,Jiancheng Yang,Udaranga Wickramasinghe,Pascal Fua*

Main category: cs.CV

TL;DR: PrIntMesh提出了一种模板和拓扑保持的方法，实现了更加一致和真实的人体器官重建。


<details>
  <summary>Details</summary>
Motivation: 当前大多数深度学习方法对器官内部各子结构分别处理，导致解剖学上不合理的重建结果，因此需要一种能够整体建模、保持不同子结构空间关系和几何约束的新方法。

Method: PrIntMesh基于连接模板，对所有子结构进行联合变形，同时保持内部边界并确保表面光滑无伪影。它通过拓扑保持策略使器官作为一个统一系统进行重建，并能应对训练数据有限或有噪声的情况。

Result: 在心脏、海马体和肺等器官实验中，PrIntMesh取得了高几何精度、正确的拓扑结构，以及对噪声和数据量不足的鲁棒性。相比基于体素和表面的方法，该方法在重建共享界面和保持结构一致性上表现更好。

Conclusion: PrIntMesh不仅能准确重建器官，还保持了解剖一致性和结构有效性，且数据效率高，临床实用性强，是现有方法的有效补充和改进。

Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.

</details>


### [50] [When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203)
*Yuping Yan,Yuhan Xie,Yinxin Zhang,Lingjuan Lyu,Yaochu Jin*

Main category: cs.CV

TL;DR: 本文提出了VLA-Fool，系统研究了视觉-语言-行动（VLA）模型在现实多模态和黑盒环境下的对抗鲁棒性，并揭示了此类模型在面对多模态扰动时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: VLA模型已在机器人感知、推理和行动中取得显著进展，但其在复杂多模态及黑盒条件下的对抗鲁棒性问题尚未被系统探索。现有工作多关注单一模态扰动，忽视了跨模态失配对系统推理和决策的根本影响。

Method: 作者提出VLA-Fool方法，涵盖三类对抗攻击：1）文本扰动（基于梯度和提示修改），2）视觉扰动（图像修补和添加噪声），3）跨模态失配（故意破坏感知与指令间的语义相关性）；同时引入VLA感知语义空间用于提升自动化和语义引导提示框架。

Result: 在LIBERO基准和经过微调的OpenVLA模型上的实验表明，即使是细微的多模态扰动也会导致机器人行为发生显著偏离，凸显了多模态对齐的易碎性。

Conclusion: VLA-Fool的研究揭露了当前VLA模型在多模态及跨模态扰动下的脆弱性，强调了提升其鲁棒性的重要性，为后续研究提供了基准和方法。

Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

</details>


### [51] [Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles](https://arxiv.org/abs/2511.16213)
*Melih Baydar,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种新的无监督图像分类方法ICCE，通过集成多个聚类结果，显著提升了聚类和分类性能，在多个数据集上达到或超过现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 传统的无监督图像聚类往往需要联合训练特征表示和聚类模型，随着大模型的发展，聚类任务开始直接在预训练特征上运作，但如何进一步提升聚类精度并逼近有监督方法仍是挑战。

Method: ICCE首先在冻结的特征主干网络上，训练多个聚类头获得多样化的聚类结果；然后，通过集成聚类（cluster ensembling）技术对这些分歧的结果融合为一致的聚类标签；最后，利用该一致聚类标签作为伪标签训练分类器。

Result: ICCE在10个主流分类基准数据集上取得了最优或超越的性能：在CIFAR10上准确率99.3%，CIFAR100上准确率89%，ImageNet上达到70.4%准确率，首次在ImageNet上无监督方法超过70%。

Conclusion: ICCE方法通过聚类集成和自监督分类，极大缩小了无监督和有监督分类性能的差距，为无监督图像分类任务树立了新标杆。

Abstract: Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, "Image Clustering through Cluster Ensembles" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.

</details>


### [52] [SwiTrack: Tri-State Switch for Cross-Modal Object Tracking](https://arxiv.org/abs/2511.16227)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出SwiTrack框架用于跨模态目标跟踪，通过引入三种专用流和多项创新机制，实现更鲁棒的特征提取和目标保持，显著提升跟踪精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态（主要为RGB-NIR）目标跟踪方法难以充分提取各模态特有特征，且在遇到不可靠输入时易发生目标漂移，导致跟踪失败。

Method: SwiTrack框架包括三条专用流：RGB帧通过视觉编码器处理，NIR帧通过NIR门控适配器与视觉编码器协同迭代校准特征；对无效模态，引入一致性轨迹预测模块利用时空线索估计目标轨迹，抑制漂移。此外，动态模板重构和相似度对齐损失用于强化特征一致性。

Result: 在最新基准数据上，SwiTrack实现了SOTA（精度提升7.2%，成功率提升4.3%），并保持65帧/秒的实时性能。

Conclusion: 所提框架有效增强了跨模态目标跟踪的鲁棒性和精度，为实际应用提供了高效可靠的解决方案。

Abstract: Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.

</details>


### [53] [Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs](https://arxiv.org/abs/2511.16264)
*Sinan Mutlu,Georgios F. Angelis,Savas Ozkan,Paul Wisbey,Anastasios Drosou,Mete Ozay*

Main category: cs.CV

TL;DR: 该论文提出了一种新型神经网络方法，通过多层感知器（MLP）结合残差连接和创新的Memory-Block，实现基于稀疏传感器数据的高精度全身动作还原。该方法在移动头显上能实时运行，显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前AR/VR系统大多只能追踪头部和手部，无法实现完整的三维全身重建。如何在传感器有限的情况下，利用稀疏数据准确生成全身动作，是提高沉浸感和增强互动体验的关键难题。

Method: 作者设计了以MLP为主干的神经网络，加入残差连接以提升深层信息流动，引入创新的Memory-Block模块用可训练向量补全缺失传感器数据，并结合跨时刻的稀疏输入增强时序一致性。同时，采用多任务学习方案提升网络鲁棒性和输出准确率。

Result: 实验表明，提出的方法较当前最优基线算法有显著降低的预测误差。在移动头戴设备上可实现72 FPS的实时性能，有效改善了准确率与运行时性能间的平衡。

Conclusion: 该文方法不仅提升了基于有限传感器的全身动作还原精度，还可在移动端高效实时运行，具有实际应用和推广价值，对未来AR/VR沉浸式体验的提升具有积极意义。

Abstract: Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.

</details>


### [54] [TetraSDF: Precise Mesh Extraction with Multi-resolution Tetrahedral Grid](https://arxiv.org/abs/2511.16273)
*Seonghun Oh,Youngjung Uh,Jin-Hwa Kim*

Main category: cs.CV

TL;DR: TetraSDF提出了一种新的精确解析式网格提取框架，能高效准确地从神经SDF中提取出与零等值面严格一致的网格。


<details>
  <summary>Details</summary>
Motivation: 此前从神经SDF提取网格存在精度损失（采样误差）或方法局限性（仅适用于简单ReLU MLP结构），影响了SDF应用的准确率与通用性。

Method: 提出TetraSDF：将多分辨率四面体位置编码器与ReLU MLP结合，利用重心插值保持全局CPWA结构，并通过解析式输入预处理减少方向偏置、提升训练稳定性，进而能够解析性追踪ReLU线性区域，实现准确网格提取。

Result: 在多个标准数据集上，TetraSDF的SDF重建精度达到或超越现有基于网格的编码方法，并且其解析式提取器生成的网格一致性极高，忠实还原了目标等值面，且在运行时间和内存消耗上高效实用。

Conclusion: TetraSDF能够为神经SDF提供更精确、高效、鲁棒的网格提取方案，在实际应用中表现出优异的性能，有望推进相关领域的发展。

Abstract: Extracting meshes that exactly match the zero-level set of neural signed distance functions (SDFs) remains challenging. Sampling-based methods introduce discretization error, while continuous piecewise affine (CPWA) analytic approaches apply only to plain ReLU MLPs. We present TetraSDF, a precise analytic meshing framework for SDFs represented by a ReLU MLP composed with a multi-resolution tetrahedral positional encoder. The encoder's barycentric interpolation preserves global CPWA structure, enabling us to track ReLU linear regions within an encoder-induced polyhedral complex. A fixed analytic input preconditioner derived from the encoder's metric further reduces directional bias and stabilizes training. Across multiple benchmarks, TetraSDF matches or surpasses existing grid-based encoders in SDF reconstruction accuracy, and its analytic extractor produces highly self-consistent meshes that remain faithful to the learned isosurfaces, all with practical runtime and memory efficiency.

</details>


### [55] [Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM](https://arxiv.org/abs/2511.16282)
*Gergely Dinya,Péter Halász,András Lőrincz,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Gated Generative Transformers (VGGT)的高效时空场景理解框架，实现了接近实时的3D场景感知，适用于辅助导航等应用。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景理解常常受制于高计算和内存消耗，难以实时应用于辅助导航等需要高效感知的场景，因此亟需一种高效、准确且可实时运行的3D场景理解方法。

Method: 该方法采用VGGT模型，并通过滑动窗口对动态图像流进行处理，将子地图对齐以不断更新3D场景表示，解决了VGGT对内存需求高的问题。利用VGGT的追踪模块，将2D语义实例分割结果聚合为3D目标。同时，系统引入时间戳和实例ID，实现场景变化的检测和时序一致性。

Result: 通过在公开基准数据集和针对辅助导航设计的自定义数据集上的实验，验证了该框架的性能和实用性。结果显示该方法能够在真实环境中有效应用。

Conclusion: 所提出的VGGT场景理解框架不仅能高效处理时空场景，且支持近实时的3D表示更新，具有良好的实际应用前景，尤其适合辅助导航等需要连续环境感知的场景。

Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.

</details>


### [56] [Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability](https://arxiv.org/abs/2511.16294)
*Abishek Karthik,Pandiyaraju V,Sreya Mynampati*

Main category: cs.CV

TL;DR: 本文提出了一种混合深度学习框架，利用CNN、ViT和GNN结合GAN增强和自监督对比学习，实现了高精度杂草检测，助力可持续精准农业。


<details>
  <summary>Details</summary>
Motivation: 传统杂草检测难以应对复杂田间环境且对数据依赖大，杂草检测精度直接影响农药使用和农业可持续发展，因此亟需更加稳健、通用的智能检测方法。

Method: 该方法结合了卷积神经网络（CNN）、视觉Transformer（ViT）和图神经网络（GNN）以提取多模态特征，采用生成对抗网络（GAN）进行数据增强平衡类别分布，并通过自监督对比学习提升有限标注数据下的特征学习能力。

Result: 实验在多个基准数据集上实现了99.33%的准确率、精确率、召回率和F1分数，表明模型优于现有方法，具备很强的泛化和鲁棒性。

Conclusion: 提出的框架能实现本地、全局及关系特征表达，适用于实时边缘设备，大幅减少农药依赖，提升农业智能化和可持续水平，具有很高的实际应用前景。

Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.

</details>


### [57] [Optimizing 3D Gaussian Splattering for Mobile GPUs](https://arxiv.org/abs/2511.16298)
*Md Musfiqur Rahman Sanim,Zhihao Shu,Bahram Afsharmanesh,AmirAli Mirian,Jiexiong Guan,Wei Niu,Bin Ren,Gagan Agrawal*

Main category: cs.CV

TL;DR: 本文提出了一种面向移动GPU的3D Gaussian Splatting（3DGS）高效映射方法Texture3dgs，显著提升了移动设备上的三维场景重建速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统的多视图图像三维重建方法在移动端执行效率低下。3D Gaussian Splatting作为新兴技术具有较高潜力，但在移动GPU上优化尚存在挑战。由于移动端对于数据隐私、离线操作和响应速度有更高需求，急需高效适配3DGS的方法。

Method: 针对3DGS中计算消耗较大的排序过程，设计了针对二维纹理缓存优化的新型排序算法，并基于纹理缓存开销模型分析算法特性。同时通过变量布局优化等手段加速其余3DGS流程，以更好适配移动端GPU的2D内存结构。

Result: 实验表明，Texture3dgs在排序效率上达到4.1倍提升，整体三维重建速度提升1.7倍，内存占用降低1.6倍，取得明显性能提升。

Conclusion: 本文提出的Texture3dgs方法极大提升了移动端3D场景重建的效率和存储利用率，为移动设备高效三维建模提供强有力的技术支撑。

Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.

</details>


### [58] [Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling](https://arxiv.org/abs/2511.16301)
*Minseok Seo,Mark Hamilton,Changick Kim*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练即可高效实现像素级高分辨率输出的轻量级上采样方法Upsample Anything，在多个任务上达到了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型虽泛化能力强，但特征下采样比大，难以直接应用于像素级任务，现有上采样方法依赖重训练或高开销优化，难以扩展和泛化。

Method: 提出基于测试时每幅图像的轻量级优化，学习一个各向异性高斯核，将空间和强度信息结合，实现类Gaussian Splatting和Joint Bilateral Upsampling边缘感知上采样。无需重新训练，能普适于不同架构和模态。

Result: 在224x224的图片上约0.419秒完成一次上采样。方法在语义分割、深度估计、深度/概率图上采样任务实现了最新SOTA精度。

Conclusion: Upsample Anything无需数据集重训练或高耗资源优化，能高效、精准地上采样多种视觉模态特征，具有良好泛化性和实用性。

Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.

</details>


### [59] [Sparse Autoencoders are Topic Models](https://arxiv.org/abs/2511.16309)
*Leander Girrbach,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文将稀疏自编码器(SAEs)重新解释为主题模型，并提出了SAEs作为主题分析工具在文本与图像数据上的有效性。提出了SAEs-TM框架，实现更好的主题一致性与多样性。


<details>
  <summary>Details</summary>
Motivation: SAEs广泛用于分析嵌入向量，但其解释性和实际价值存在争议。作者希望通过新的视角揭示SAEs的本质及其实用作用。

Method: 作者将SAE的学习目标与主题模型（尤其是Latent Dirichlet Allocation, LDA）相联系，推导出SAEs目标实际上等价于对一种扩展LDA的最大后验估计。随后提出SAEs-TM框架：（1）用SAE学习通用的主题原子；（2）将这些原子解释为下游数据上的词分布；（3）无需重新训练即可将这些原子融合为任意数量的主题。

Result: SAE-TM在文本和图像数据集上比强基线获得了更一致、更具多样性的主题。同时，方法可用于分析图像数据的主题结构及追踪日本浮世绘主题随时间演变。

Conclusion: SAE可作为高效的跨模态大规模主题分析工具，比传统方法具有更好性能和解释能力。

Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.

</details>


### [60] [BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks](https://arxiv.org/abs/2511.16315)
*Samuel Stevens*

Main category: cs.CV

TL;DR: ImageNet在科学影像任务上的迁移性能已不再是优秀视觉表示的良好指标。为此，本文提出BioBench生态视觉基准测试，涵盖9项任务、4类生物界、6种数据采集方式，帮助更好地评估和发展针对科学领域的视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型往往在ImageNet上预训练和评估，将其迁移性能作为通用代表。然而，ImageNet性能不再能准确预测模型在生态科学类影像任务中的表现，因此亟需针对科学应用场景的新基准。

Method: 建立了BioBench生态视觉基准，整合了9个公开数据集（涵盖4大类生物界和6种采集方式，计310万张图片），并设计了易用API，实现了冻结特征提取器、轻量级线性分类头训练及结果评估，统一报告类均衡macro-F1指标。支持高效测试（如ViT-L模型在A6000 GPU上6小时完成）。

Result: 实证分析46种现代视觉模型，发现ImageNet迁移性能仅能解释生态任务34%的表现方差，有30%的模型在75%准确率区间被错位排名。BioBench更好地揭示了模型在生态科学任务中的能力。所用代码与预测结果全部开源。

Conclusion: BioBench为生态科学视觉研究提供了更具针对性的新评测信号和开放工具，同时为构建各领域AI科学基准树立了模板，有助于推动AI更好服务于科学研究。

Abstract: ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.

</details>


### [61] [NaTex: Seamless Texture Generation as Latent Color Diffusion](https://arxiv.org/abs/2511.16317)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Xin Yang,Xin Huang,Jingwei Huang,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: 论文提出了NaTex框架，直接在3D空间中生成贴图颜色，克服了传统2D多视角合成方法的局限性，实现了高质量三维贴图自动生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D贴图生成方法多依赖2D多视角图片合成，存在遮挡区域难处理、贴图与网格精细对齐难、视角一致性差等问题。

Method: NaTex通过将贴图视为稠密颜色点云，采用包括几何感知点云VAE和多控制扩散Transformer（DiT）的潜在颜色扩散模型，完全基于3D数据端到端训练。同时引入原生几何控制机制，通过位置嵌入和几何潜变量增强对3D空间信息的感知，实现VAE和DiT架构协同，精细引导贴图生成。

Result: NaTex在贴图一致性和与网格对齐性能上明显优于以往方法，并在贴图生成、细化、分割等下游任务上展现出优异的泛化能力。

Conclusion: NaTex为3D贴图生成提供了全新范式，提升了生成质量与对齐精度，并具备良好的应用拓展性和泛化能力。

Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.

</details>


### [62] [WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement](https://arxiv.org/abs/2511.16321)
*Ching-Heng Cheng,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 该论文提出了一种高效且紧凑的水下图像增强网络WWE-UIE，通过结合三种可解释的先验，提升了图像恢复质量并大幅减少了模型参数量，实现了实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法结合了物理领域先验和深度学习架构，虽然效果突出但计算代价高，难以应用于实时或资源受限的场景。因此，迫切需要一种低计算复杂度、性能依然优秀的增强方法。

Method: WWE-UIE网络集成了三种先验：1）自适应白平衡缓解波长依赖的颜色衰减问题，2）小波增强模块进行多频带分解以兼顾全局结构和细节纹理，3）梯度感知模块结合可学习门控的Sobel算子，显式保留被散射破坏的边缘结构。

Result: 在多个基准数据集上实验，WWE-UIE以更少的参数量和更低的FLOPs实现了与现有最优方法相媲美的恢复质量，并可在资源受限平台上实时运行。消融实验和可视化进一步验证了各子模块的有效性。

Conclusion: WWE-UIE克服了现有方法计算成本高的问题，在保证恢复质量的前提下实现了轻量高效的水下图像增强，具有广泛的实际应用前景。

Abstract: Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.

</details>


### [63] [ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2511.16322)
*Ching-Heng Cheng,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出ChangeDINO框架，融合DINOv3语义特征及空间-光谱变换模块，有效提升遥感变化检测鲁棒性和精度，在四个公开数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习遥感变化检测方法主要依靠变化标注，未充分利用非变化区域的语义信息，导致对光照变化、成像视角和标注稀缺不具鲁棒性。

Method: 提出了端到端多尺度Siamese结构ChangeDINO，结合轻量主干与冻结的DINOv3特征迁移，形成多尺度的语义和上下文特征金字塔；设计空间-光谱差分变换解码器，利用多尺度绝对差值突出真实建筑变化，抑制无关反应；引入可学习形态学模块细化边界。

Result: 在四个公开遥感建筑变化检测基准上的IoU和F1指标均优于最新方法，消融实验验证各模块有效性。

Conclusion: ChangeDINO显著提升了建筑变化检测在多种复杂条件下的表现，每个模块都对整体性能有积极贡献。源码已公开。

Abstract: Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.

</details>


### [64] [Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks](https://arxiv.org/abs/2511.16341)
*Yi Ting Tsai,Yu Wei Chen,Hong-Han Shuai,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的人脸超分辨率方法ARASFSR，可实现任意分辨率和倍率下的人脸图像增强，有效克服现有方法对输入尺寸和放大倍率的限制，并在多种测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸超分辨率方法受限于固定的放大倍率，并且对输入尺寸变化敏感，导致在实际应用中灵活性和鲁棒性不足。作者希望解决现有方法在不同放大倍数和输入尺寸下效果不佳的问题。

Method: 作者提出ARASFSR（Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution）方法。具体包括三点创新：1）利用2D深层特征、局部相对坐标和放大倍率预测每个目标像素的RGB值，实现任意放大倍率的超分辨率；2）引入局部频率估算模块，捕捉高频面部纹理，缓解频谱偏置问题；3）设计全局坐标调制模块，引导模型利用面部结构先验，实现不同分辨率下的自适应增强。

Result: 无论输入尺寸和放大倍率如何变化，ARASFSR都能生成高质量的人脸超分辨率图像。实验结果显示，在定量和定性评估中，该方法的鲁棒性和效果均优于现有最优技术。

Conclusion: ARASFSR在灵活性和性能上大幅提升了人脸超分辨率技术，能够适应任意放大倍率和输入尺寸，在多个指标上超越现有方法，有望为相关领域提供更广泛的应用基础。

Abstract: Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.

</details>


### [65] [Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach](https://arxiv.org/abs/2511.16343)
*Chi-Han Chen,Chieh-Ming Chen,Wen-Huang Cheng,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出通过无人机遥感数据进行地形地貌分类的新方法，结合教师-学生架构和关键帧选择策略，仅用30%的标注数据，实现了时序一致性和识别精度的双提升。


<details>
  <summary>Details</summary>
Motivation: 无人机遥感地形地貌分类与地面车辆巡检任务有较大不同，主要挑战包括数据标注成本高、时序一致性实现难、相关数据稀缺以及技术有效距离受限。现有方法往往难以同时保证标注效率与时序一致性，限制了实际应用效果。

Method: 提出基于教师-学生架构，结合关键帧选择和关键帧更新算法，在弱监督条件下进行时序一致性知识蒸馏。核心思想是在有限的关键标注帧基础上，通过知识迁移和特征提取，提升整体模型表现，同时减少对全量标注数据的依赖。

Result: 实验结果表明，只需使用约30%的标注数据，新方法即可同时提升mIoU与时序一致性指标，实现了地形地貌对象的稳定定位，性能显著优于传统方法。

Conclusion: 该方法有效缓解了数据标注负担，显著增强了无人机遥感任务中的时序一致性与定位稳定性，在地形地貌分类及实际应用中具有良好的推广价值。

Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection

</details>


### [66] [CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering](https://arxiv.org/abs/2511.16349)
*Joni Vanherck,Steven Moonen,Brent Zoomers,Kobe Werner,Jeroen Put,Lode Jorissen,Nick Michiels*

Main category: cs.CV

TL;DR: 本文提出了一种实时相机定位方法，通过将实时图像与高精度有色LiDAR点云进行匹配，实现无漂移、具备比例感知的全局定位，显著优于现有SLAM方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法存在漂移、规模模糊和对信标/回环检测依赖性强等问题，影响机器人和XR等领域中的导航与对齐效果。

Method: 通过渲染点云生成合成视图，建立2D-3D对应关系，并利用神经渲染技术缩小合成图像与真实图像的域差异，改进特征匹配。文中设计了“在线渲染与匹配”和“预生成与本地化”两种实时系统实现。

Result: 在ScanNet++数据集上实验，提出的方法表现出更好的精度和可靠性，显著超过现有SLAM管线。

Conclusion: 文中方法实现了基于LiDAR点云的高精度、无漂移的相机定位，为机器人导航与XR应用带来了更稳健的技术基础。

Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.

</details>


### [67] [Multi-Order Matching Network for Alignment-Free Depth Super-Resolution](https://arxiv.org/abs/2511.16361)
*Zhengxue Wang,Zhiqiang Yan,Yuan Wu,Guangwei Gao,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 現有的深度超分辨方法假設RGB與深度圖嚴格對齊，但實際場景常有對齊偏差。本文提出無需對齊的新方法MOMNet，有效處理現實場景中的RGB-Depth失配，結果表現優異且穩健。


<details>
  <summary>Details</summary>
Motivation: 現實中，硬體限制和校準漂移常導致RGB與深度圖無法嚴格對齊，現有方法在這種場景下效果變差，因此需要無需依賴嚴格對齊的深度超分辨方法。

Method: 提出了Multi-Order Matching Network (MOMNet)。該方法通過多階特徵（零階、一階、二階）匹配機制提取與深度一致的RGB信息，並利用多階聚合（結構檢測器）選擇性地從RGB向Depth傳遞相關特徵。

Result: 實驗結果表明，MOMNet不僅在多項指標上達到領先性能，還在面對RGB-Depth失配場景時展現出卓越的穩健性。

Conclusion: MOMNet作為一種無需嚴格對齊的深度超分辨方法，為實際應用中的非對齊場景提供了新解決方案，並表現出顯著的性能提升和魯棒性。

Abstract: Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.

</details>


### [68] [DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration](https://arxiv.org/abs/2511.16364)
*Meng-Cheng Shih,Tsai-Ling Huang,Yu-Heng Shih,Hong-Han Shuai,Hsuan-Tung Liu,Yi-Ren Yeh,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了一个新的离线签名验证模型DetailSemNet，强调局部结构匹配的重要性，并通过特定结构提升细节判别能力，在多个基准数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有离线签名验证方法多侧重整体特征对比，忽略了精细化的局部差异，而局部结构对签名真伪判别尤为关键。此外，纯transformer骨干网络可能弱化了局部信息，从而影响模型性能，亟需结构创新来补足这一短板。

Method: 提出DetailSemNet，通过Detail Semantics Integrator模块实现特征的解耦与重耦，在增强细节特征表达的同时，扩展判别语义，使局部结构匹配更加有效。此外，针对transformer架构天然易于忽视细节的问题，该模块显著提升了细节捕捉与判别能力。

Result: 在主流离线签名验证基准上，所提模型在精度等指标上显著超越了近年来的方法，实现了领先的性能；在跨数据集实验中也表现出优异的泛化能力。

Conclusion: 通过突出局部结构匹配和语义细节的表达，DetailSemNet不仅提升了签名验证的准确率，还增强了模型的可解释性和泛化能力，显示出在实际应用中的巨大潜力。

Abstract: Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.

</details>


### [69] [CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement](https://arxiv.org/abs/2511.16378)
*Pan Yang,Cheng Deng,Jing Yang,Han Zhao,Yun Liu,Yuling Chen,Xiaoli Ruan,Yanping Chen*

Main category: cs.CV

TL;DR: 本文提出了一种叫做CAMS的新方法，通过对视觉特征进行多维空间的语义解耦，提高了组合零样本学习（CZSL）识别未见属性-物体组合的能力，并在多个数据集上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的组合零样本学习方法主要依赖全局语义表征去区分属性和物体，但该表征能力有限，难以实现完全解耦，导致对新组合的泛化能力不足。

Method: 提出CAMS方法，利用门控交叉注意力机制从CLIP的高层编码块中提取细粒度的语义特征，并通过多空间解耦模块，实现属性和物体语义的有效区分，同时抑制无关背景信息。

Result: 在MIT-States、UT-Zappos和C-GQA三个主流基准上，CAMS在封闭世界和开放世界设置下都达到了最新的性能水平。

Conclusion: CAMS有效解决了属性与物体语义解耦难题，提升了CZSL对未见组合的泛化能力，为后续相关研究提供了更强的基线和新思路。

Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.

</details>


### [70] [End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss](https://arxiv.org/abs/2511.16418)
*Hai Lan,Zongyan Li,Jianmin Hu,Jialing Yang,Houde Dai*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动捕捉（MoCap）基本单元——刚体标记（RBM），并结合深度学习方法，大幅简化光学运动捕捉的流程，同时实现高精度与高效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于标记的光学运动捕捉虽然精准，但存在布置复杂、标记分辨难等实际问题，极大限制了实际应用和规模扩展。

Method: 1. 引入刚体标记（RBM），可提供无歧义的6自由度（6-DoF）数据，显著简化运动捕捉系统搭建。2. 基于深度学习，设计端到端的回归模型，直接用地球测地线损失（geodesic loss）回归SMPL人体参数。3. 在合成数据与真实采集数据上进行训练和验证。

Result: 所提方法在AMASS数据集上的合成数据和Vicon系统采集的真实数据上均达到了与当前优化方法相当甚至更优的姿态估计精度，同时计算效率提高了一个数量级。

Conclusion: 稀疏6-DoF RBM结合流形感知的损失函数，为图形学、虚拟现实和生物力学等领域，提供了可实际应用且高还原度的实时运动捕捉新方案。

Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.

</details>


### [71] [CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation](https://arxiv.org/abs/2511.16428)
*Samer Abualhanud,Christian Grannemann,Max Mehltretter*

Main category: cs.CV

TL;DR: 本文提出了一种全新几何引导的自监督环视深度估计方法，解决了多摄像头重叠区域深度不一致的问题，显著提升了跨视角深度一致性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督环视深度估计方法在多摄像头重叠区域常出现深度结果不一致，影响3D感知和应用效果，需要一种有效解决该问题的方法。

Method: 该方法针对已标定、时间同步的多摄像头系统，首先利用内参和相对外参预测每个视图的初始深度图并构建三维点云；然后将所有点投影到共享的单位圆柱上，建立不同视图间的邻域关系；进一步为每幅图像生成2D位置映射，依据在圆柱上的距离，通过显式的、非学习型空间注意力机制在多个视图间聚合特征，最终输出每张图片的一致性深度图。

Result: 在DDAD和nuScenes数据集上实验表明，该方法在多图像间显著提升了深度估计的一致性和整体深度估计性能，优于当前主流方法。

Conclusion: 该方法实现了密集、低成本、全向的3D感知能力，有效提升了多视角间深度一致性，推动了自监督环视深度估计的实用化。

Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

</details>


### [72] [Graph Neural Networks for Surgical Scene Segmentation](https://arxiv.org/abs/2511.16430)
*Yihan Li,Nikhil Churamani,Maria Robu,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出基于图神经网络的图像分割新方法，提升了腹腔镜胆囊切除术中肝胆区域精确识别的能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在腹腔镜场景分析中面临遮挡、长距离依赖以及罕见复杂结构难以识别等挑战。更精确的肝胆解剖识别对避免术中并发症至关重要，因此需要新的方法增强空间和语义理解能力。

Method: 提出两种将ViT（视觉Transformer）特征编码器与图神经网络结合的分割模型：1）静态k近邻图结合GCNII，实现稳定的长距离信息传播；2）动态图生成器与图注意力网络（GAT）结合，实现自适应拓扑学习。两者在公开腹腔镜分割数据集上评估。

Result: 与现有方法相比，所提方法的平均交并比（mIoU）提升7-8%，Dice系数提升6%。对于薄、罕见以及手术安全关键结构的识别尤为突出，预测结果更符合解剖实际。

Conclusion: 所提基于图的分割方法提升了手术场景分割的性能和解剖一致性，通过ViT的全局上下文和图神经网络的关系推理，提高了解释性和鲁棒性，有助于提高手术安全性和精确性。

Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.

</details>


### [73] [Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation](https://arxiv.org/abs/2511.16435)
*Jin Wang,Bingfeng Zhang,Jian Pang,Mengyu Liu,Honglong Chen,Weifeng Liu*

Main category: cs.CV

TL;DR: 本文提出将语言驱动的属性泛化应用于小样本分割，通过大语言模型获取目标类别的属性描述，并与视觉特征进行多模态匹配和对齐，取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有FSS方法依赖于支持图像挖掘元信息，但由于受限于类内视觉变化，这些信息对未见类别的分割指导不足。作者认为支持集的核心作用在于提供更通用、公正的元指导，而非局限于具体支持图像。

Method: 提出了语言驱动属性泛化（LDAG）架构。首先，用多属性增强（MaE）模块通过大语言模型生成目标类别的多种属性文本描述，并进行多模态匹配形成视觉-文本先验。针对文本与视觉模态的不一致，引入多模态属性对齐（MaA）模块，实现属性文本与视觉特征的跨模态互动。

Result: 新方法在标准小样本分割任务上取得了明显优于现有方法的表现，实现了新的SOTA。

Conclusion: 通过利用属性文本与视觉特征的多模态融合，显著提升了小样本分割中新类的泛化能力，语言属性描述为视觉分割提供了更具鲁棒性的元指导。

Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.

</details>


### [74] [StreetView-Waste: A Multi-Task Dataset for Urban Waste Management](https://arxiv.org/abs/2511.16440)
*Diogo J. Paulo,João Martins,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 本论文提出了StreetView-Waste数据集，专注于城市垃圾管理中的垃圾容器检测、跟踪和溢出分割，填补了现有数据集在真实物流应用中的不足，并通过基线和创新方法提升了相关任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数垃圾检测数据集缺乏对垃圾容器的跟踪标注，且多为静态环境拍摄，难以适应智能城市垃圾管理实际需求。实际场景中，特别是由垃圾车采集的图像中，准确监控垃圾容器溢出是一个被忽视的重要问题。

Method: 构建了StreetView-Waste数据集，包含含垃圾与垃圾容器的城市街景图像，支持容器检测、跟踪以及溢出分割三大任务。对已有检测、跟踪和分割模型进行基线评测，同时提出了基于启发式策略的容器跟踪改进方法和结合几何先验的分割优化框架。

Result: 微调后的目标检测模型在垃圾容器检测上表现良好，但传统跟踪方法对容器数量估算仍有较大误差，所提启发式方法能将平均绝对计数误差降低约79.6%；同时，几何感知的分割方案让轻量级模型的分割mAP@0.5提升了27%。

Conclusion: StreetView-Waste数据集为城市垃圾管理相关的真实视觉感知研究提供了新基准，相关方法显著提升了容器跟踪和垃圾分割任务的性能，有效促进了智能城市实用场景下垃圾管理系统的发展。

Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.

</details>


### [75] [VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/abs/2511.16449)
*Ziyan Liu,Yeqiu Chen,Hongyi Cai,Tao Lin,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为VLA-Pruner的新视觉-语言-动作模型（VLA）中视觉Token剪枝方法，提高了VLA在机器人任务中的实时处理能力和效率，同时保证了理解与动作生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在实际部署时受限于高昂的视觉流处理计算成本。虽然已有Token剪枝方法能加速VLMs，但仅基于语义重要性，忽略了VLA模型还需兼顾低层行动信息，导致动作生成性能下降。因此，亟需一种兼顾语义理解和动作执行的重要性Token剪枝策略。

Method: 提出VLA-Pruner方法，通过引入双层重要性标准：一是基于视觉-语言前填充注意力的语义相关性，二是结合时序平滑预测的动作解码注意力，用于衡量Token的动作层重要性。基于该标准，VLA-Pruner设计了自适应双层Token保留策略，有效压缩冗余数据，同时保证对理解和决策的关键信息保留。该方法为即插即用插件，适用于多种VLA架构。

Result: 实验结果表明，VLA-Pruner在多种VLA架构和机器人任务中均达到了当前最优的加速与性能平衡表现，显著优于仅用语义剪枝的方法。

Conclusion: VLA-Pruner系统性解决了VLA模型中Token剪枝忽略动作信息的问题，实现了低算力、高效率、强任务表现的视觉-语言-动作一体化高效推理，为智能体落地部署提供了有效路径。

Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

</details>


### [76] [LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs](https://arxiv.org/abs/2511.16454)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: 本文提出LLaVA^3方法，无需3D训练数据，仅用多视角2D图像，即可提升多模态语言模型对3D场景的理解能力，且无需微调。实验表明该方法在3D视觉问答和3D语言定位任务上优于已有2D方法。


<details>
  <summary>Details</summary>
Motivation: 由于3D训练数据稀缺，而2D数据丰富，现有多模态语言模型在3D场景理解上表现有限，需探索无需3D数据条件下提升3D理解能力的方法。

Method: 受立体主义画派启发，将3D物体的多视角信息整合为全方位视觉特征，通过对场景进行多视图3D重建，利用2D图像为VLM提供3D表征，无需模型微调。

Result: 在3D视觉问答（VQA）和3D语言定位任务上，LLaVA^3方法在无3D训练数据和不微调情况下，性能超过以往基于2D的多模态语言模型。

Conclusion: LLaVA^3展示了无需3D训练数据，通过创新性利用2D多视角信息即可大幅提升VLM的3D场景理解力，为数据稀缺领域提供了可行方案。

Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.

</details>


### [77] [FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry](https://arxiv.org/abs/2511.16471)
*Clemens Pollak,Kersten Diers,Santiago Estrada,David Kügler,Martin Reuter*

Main category: cs.CV

TL;DR: 本文提出一种高效、全自动的胼胝体形态学分析工具FastSurfer-CC，并验证其在分割和统计分析等方面优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 胼胝体在脑老化和神经疾病研究中具有重要意义，但目前缺乏公开、全面、自动化的分析工具，限制了相关研究和临床应用。

Method: 提出FastSurfer-CC框架，自动识别中矢状切面，分割胼胝体和穹窿，标准化头部定位，生成厚度曲线与分区，提取8项形态学指标，并进行详细统计分析。

Result: FastSurfer-CC在各项任务上的性能超过现有专业工具，并能识别出亨廷顿舞蹈症患者与健康对照组之间的统计学差异，且这些差异无法被主流工具检测到。

Conclusion: FastSurfer-CC为胼胝体的自动化分析提供了高效、精确的新方法，有望促进神经影像学、临床试验和疾病诊断研究。

Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.

</details>


### [78] [Flow and Depth Assisted Video Prediction with Latent Transformer](https://arxiv.org/abs/2511.16484)
*Eliyas Suleyman,Paul Henderson,Eksan Firkat,Nicolas Pugeault*

Main category: cs.CV

TL;DR: 该论文研究通过引入显式的运动（point-flow）和几何结构（depth-maps）信息提升视频预测模型在遮挡情况下的表现，首次系统性评估了相关方法。


<details>
  <summary>Details</summary>
Motivation: 虽然当前视频预测模型在常规场景中表现出色，但在存在遮挡的情况下仍面临挑战。作者认为结合显式的运动流和深度信息有助于解决遮挡带来的难题。

Method: 采用标准的多目标潜变量Transformer架构，并引入depth-maps和point-flow信息，对未来帧进行预测。将改进后的模型在合成和真实世界的数据集上进行对比实验评估。

Result: 引入显式运动和深度信息后，模型在遮挡场景和背景运动的预测上表现优于传统方法，采用Wasserstein距离的度量方式也验证了其对物体运动分布预测的有效提升。

Conclusion: 将point-flow和depth等多模态信息融入视频预测，有效提升了模型对遮挡和背景运动的处理能力，为更复杂应用场景下的视频预测模型设计提供了新思路。

Abstract: Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.

</details>


### [79] [Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation](https://arxiv.org/abs/2511.16494)
*Zongcai Tan,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理的生成对抗网络（GAN）框架，可高效合成高保真显微镜图像，实现光学微型机器人精准位姿估计，并提升数据生成和网络泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前光学微型机器人位姿估计强依赖大规模高质量显微图像数据，而此类数据获取困难且昂贵，因制造复杂及标注需大量人工。亟需高效、低成本的数据增强方法解决该瓶颈。

Method: 提出一种新型物理驱动的深度生成学习方法，将波光学物理渲染和深度对齐过程首度引入生成对抗网络（GAN），以合成更真实、包含复杂光学效应（如衍射与深度依赖）的显微镜图像，用于训练微型机器人姿态估计算法。

Result: 所提方法合成图像在结构相似度指数(SSIM)上较纯AI方法提升35.6%，同时保持实时渲染速度（0.022秒/帧）。基于合成数据训练的位姿估计算法在pitch/roll（俯仰/滚转）精度分别达93.9%/91.9%，仅比完全真实数据训练下降约5%。同时模型能泛化到新姿态，实现数据增强。

Conclusion: 该物理驱动的深度生成框架可高效提升微型机器人位姿估计精度并显著减少真实数据需求，为自动化生物实验和微型机器人领域的高精度任务提供了新思路。

Abstract: Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.

</details>


### [80] [Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI](https://arxiv.org/abs/2511.16498)
*Rui Wang,Yuexi Du,John Lewin,R. Todd Constable,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 本论文提出了一种结合采集时间信息的动态增强磁共振成像（DCE-MRI）乳腺肿瘤分割方法，通过时间调制提升不同采集方案下的自动分割效果。


<details>
  <summary>Details</summary>
Motivation: DCE-MRI广泛用于乳腺癌筛查和治疗评估，但由于采集协议和个体差异，图像外观变化大，导致自动肿瘤分割困难。解决这一挑战，有助于提升乳腺癌自动检测与分割的准确性和泛化能力。

Method: 本文提出在分割模型中引入采集时间信息，采用特征层线性调制（FiLM）层对网络特征进行调节，有效整合时间序列信息。通过不同主干架构、基础模型和时间调制模型在多中心公开DCE-MRI数据集上训练与对比。

Result: 在同域和异域的公开数据集上实验表明，模型结合采集时间信息后，肿瘤分割性能和泛化能力明显提升。

Conclusion: 本文提出的方法，利用采集时间信息显著提升了DCE-MRI乳腺肿瘤分割的效果和模型的泛化能力，对自动医学图像分析具有重要意义。

Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.

</details>


### [81] [YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras](https://arxiv.org/abs/2511.16521)
*Fan Yang,Sosuke Yamao,Ikuo Kusajima,Atsunori Moteki,Shoichi Masui,Shan Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，实现了室内场景的映射与天花板摄像机（CMC）自动注册的联合优化，并建立了相关数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，天花板摄像机需要与场景布局精确对齐，人工注册效率低且成本高，现有自动方法在存在视觉歧义时易出错，因此亟需更高效、鲁棒的联合注册与场景映射方案。

Method: 该方法通过让佩戴头戴RGB-D相机的移动体在室内场景中巡游，记录egocentric世界坐标轨迹和场景布局，并同步让CMC拍摄该移动体，获得伪尺度轨迹和CMC相对位姿。随后将所有轨迹和时间戳对齐，以实现对齐。接着，定制化因子图实现对ego相机位姿、场景布局及CMC位姿的联合优化。

Result: 作者构建了首个协同场景映射与CMC注册的基准数据集，并通过实验证明，该方法能在统一框架下同时高效完成场景映射与摄像机注册，并相互提升两项任务性能。

Conclusion: 本文方法有效提升了室内视觉场景映射与天花板摄像机注册的鲁棒性和效率，为后续基于位置感知的多种应用提供了可靠工具。

Abstract: Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.

</details>


### [82] [BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization](https://arxiv.org/abs/2511.16524)
*Rahul Kumar,Vipul Baghel,Sudhanshu Singh,Bikash Kumar Badatya,Shivam Yadav,Babji Srinivasan,Ravi Hegde*

Main category: cs.CV

TL;DR: 本文介绍了一个面向拳击动作识别（尤其是拳击出拳检测与分类）的高质量视频数据集，包括6915段精准剪辑和注释的出拳片段，涵盖六种拳击出拳类型，目的在于推动计算机视觉在搏击运动分析领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前格斗类运动的视觉分析受限于缺乏高质量、结构良好的数据集，因其动作动态、环境多变，严重制约了动作识别相关算法的发展与评测。

Method: 研究者收集了20场YouTube业余拳击实战影像，汇集18位运动员，通过人工剪辑与标注，构建了包含多风格、不同机位、不同体型运动员的6915段高质量拳击出拳片段，标注了出拳的动作边界和类型。

Result: 研发出一个丰富且多样化的拳击出拳数据集，适用于实时、低资源、受限环境下的动作识别研究和评测。

Conclusion: 该数据集为拳击及相关运动的动作分析、自动化教练与运动表现评估等研究提供了强有力的基线资源，有望推动领域算法和应用进步。

Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.

</details>


### [83] [Contrastive vision-language learning with paraphrasing and negation](https://arxiv.org/abs/2511.16527)
*Kwun Ho Ngan,Saman Sadeghi Afgeh,Joe Townsend,Artur d'Avila Garcez*

Main category: cs.CV

TL;DR: 本文提出了一种增强对否定和重述文本语义理解能力的CLIP变体SemCLIP，在保持原有性能的同时，大幅提升了模型对于语义否定的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前CLIP模型在处理否定或重述（即同义改写）文本时表现不稳定，而这类文本变化对模型检索任务中的语义理解极具挑战性，因此需要提升视觉-语言模型处理语义变换的能力。

Method: 作者提出了一种新的对比损失函数，同时充分利用由大型语言模型生成的原文、重述及否定三元组文本，并用于CLIP类模型训练，推动重述文本靠近原始图像嵌入，同时将否定文本在嵌入空间中推远，实现更精确的语义对齐。

Result: 在CC-Neg基准上，SemCLIP模型的原文优于否定检索准确率从68.1%提升到了78.1%；在Sugarcrepe++基准上，SemCLIP整体优于仅用否定数据训练的模型。多项下游零样本任务上，SemCLIP也广泛优于原始CLIP。

Conclusion: SemCLIP显著提升了视觉-语言模型对否定、重述等语义变换的鲁棒性，能更好地应对复杂语义表述，有利于实际应用中模型的健壮性和泛化能力。

Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.

</details>


### [84] [Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration](https://arxiv.org/abs/2511.16532)
*Fan Yang,Shigeyuki Odashima,Shoichi Masui,Ikuo Kusajima,Sosuke Yamao,Shan Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种稳健的多摄像头体操运动员跟踪方法，并已应用于国际体操锦标赛的裁判系统。通过结合体操领域知识和创新的数据关联策略，有效克服了摄像机数量有限、遮挡和视角失效等问题，实现了更高精度的3D轨迹估计。


<details>
  <summary>Details</summary>
Motivation: 体操比赛受场地和硬件限制，摄像机数量有限，且常因光照、背景、服装和遮挡等原因造成多摄像头检测失败，导致常规三角测量方法难以获得精确的3D轨迹。因此亟需设计能在部分检测缺失情况下仍可稳健跟踪体操运动员的方法，提升体操裁判的精确性和可靠性。

Method: 作者提出了一种结合体操动作先验知识的多摄像头跟踪方法：在跨视角检测信息充足时，采用常规三角测量生成3D轨迹；当检测信息不足时，利用传统动作多在垂直平面内的特点，通过射线-平面相交生成共面轨迹候选，对不确定情况进行补偿。同时，设计了级联式数据关联流程，提高数据关联的鲁棒性。

Result: 通过大量实验验证，本文方法在多种具有挑战性的场景下均优于现有跟踪算法，显著减少了跟踪失败，并成功应用于世界级体操赛事，获得国际体操联合会的肯定。

Conclusion: 结合动作领域知识和灵活的数据关联策略，可以极大提升多摄像头条件不足环境下体操运动员的稳健跟踪效果，为专业体操裁判系统提供了实际可行和高性能的技术方案。

Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.

</details>


### [85] [Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation](https://arxiv.org/abs/2511.16535)
*Haytham Ziani*

Main category: cs.CV

TL;DR: 本文分析了光流计算中的局部与全局方法，重点研究了Horn-Schunck算法，并实现其多分辨率版本提升性能。


<details>
  <summary>Details</summary>
Motivation: 光流估计在计算机视觉应用中非常重要，不同方法在不同图像条件下表现各异，因此需要系统比较和改进。

Method: 作者对局部（如Lucas-Kanade）和全局（如Horn-Schunck）方法进行了理论和实验分析，并实现基于双线性插值和扩展的多分辨率Horn-Schunck算法。

Result: 多分辨率Horn-Schunck算法在不同的图像条件下提升了光流估计的准确性和收敛速度。联合方法效果优于单一局部或全局方法。

Conclusion: 结合多分辨率和双线性插值的Horn-Schunck算法有效提升了运动估计表现，为光流估计提供了更优的实现方案。

Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.

</details>


### [86] [Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution](https://arxiv.org/abs/2511.16541)
*Jaime Álvarez Urueña,David Camacho,Javier Huertas Tato*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段框架，有效提升了对新型生成式AI图像的检测准确性与泛化能力，且明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速进步，合成图像愈发难以辨别，传统检测方法依赖频繁重训，效率低下且难以应对快速演进的生成模型。这需要新方法提升检测的泛化与可扩展性。

Method: 提出两阶段检测框架：第一阶段通过基于监督对比学习的视觉深度学习模型，提取判别性嵌入特征，训练时有意识地屏蔽部分生成器以验证泛化能力；第二阶段在嵌入空间上，利用k近邻分类器并结合少样本学习，仅需极少量新生成器样本即可实现高效检测。

Result: 只需每类150张图像作为新样本，平均检测准确率达91.3%，比现有方法提升5.2个百分点。在归因任务上，AUC和OSCR分别提高14.70%和4.27%，在开放集分类中表现突出。

Conclusion: 该框架无需频繁重训，能适应不断演进的生成式AI图像，提升了检测与归因的泛化性和实用性，为数字媒体取证和内容安全提供了有力手段。

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.

</details>


### [87] [EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering](https://arxiv.org/abs/2511.16542)
*Pierrick Bournez,Luca Savant Aira,Thibaud Ehret,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本文提出了EOGS++方法，通过对卫星遥感数据进行高效三维重建，比现有方法如EOGS和NeRF表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF和EOGS等三维重建方法，在重构质量与训练效率、对高分辨率数据的直接处理能力等方面存在不足，亟需一个既高效又适用于原始高分辨率卫星影像的新方法。

Method: EOGS++针对高分辨率全色卫星影像，无需外部预处理，直接操作原始数据。采用了光流方法，将束调整（bundle adjustment）嵌入训练流程，提升相机位姿估计，避免依赖外部优化器，并引入early stopping与TSDF后处理，提升重建清晰度和几何精度。

Result: 在IARPA 2016和DFC2019等数据集上，EOGS++重建质量与效率达到了当前最优。与原版EOGS比较，建筑物平均MAE误差从1.33降至1.19，优于NeRF等基线方法。

Conclusion: EOGS++不仅提升了三维重建的质量和效率，还进一步增强了系统的稳健性和实用性，证明了高分辨率地球观测数据在无需预处理下实现高效准确重建的可行性。

Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models

</details>


### [88] [Progressive Supernet Training for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.16546)
*Xiaoyue Chen,Yuling Shi,Kaiyuan Li,Huandong Wang,Yong Li,Xiaodong Gu,Xinlei Chen,Mingbao Lin*

Main category: cs.CV

TL;DR: 该论文提出VARiant模型，通过灵活网络深度调节，显著降低内存消耗和推理延迟，同时保持高生成质量。


<details>
  <summary>Details</summary>
Motivation: 经典的Visual Auto-Regressive(VAR)多尺度生成方法虽然推理步数少、速度快，但多层KV缓存导致内存消耗大，影响实际部署。作者观察到，不同尺度对网络深度的依赖不对称，这为降低计算资源消耗提供了新思路。

Method: 作者提出VARiant方法：在30层VAR-d30网络中，等距采样获得多个不同深度的子网络（16至2层），在早期尺度用全量网络，后期尺度采用浅层子网络，子网络和主网络权重共享。为解决权重共享带来的优化冲突，采用渐进式训练策略，实现主网络与子网络生成质量的兼得。

Result: 在ImageNet实验中，VARiant-d16和VARiant-d8在保持近似原网络质量（FID 2.05/2.12 vs 原1.95）的同时，实现40-65%的内存节约；VARiant-d2在质量适度下降（FID 2.97）情况下，推理提速3.5倍，内存降低80%。

Conclusion: VARiant方法支持零成本运行时深度切换，在单模型框架下兼顾高质量和极致效率，提升多场景部署灵活性。

Abstract: Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.
  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.
  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.
  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.

</details>


### [89] [Lite Any Stereo: Efficient Zero-Shot Stereo Matching](https://arxiv.org/abs/2511.16555)
*Junpeng Jing,Weixun Luo,Ye Mao,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的立体匹配模型Lite Any Stereo，兼具小规模模型和强大零样本泛化能力，且在几个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的高准确率立体匹配模型普遍模型庞大、计算量大，而轻量化模型数据泛化能力弱，难以实现零样本推断。如何实现高效且泛化能力强的模型，是该领域的主要难题。

Method: 作者设计了轻量紧凑但表达能力强的骨干网络，结合混合代价聚合模块，并采用三阶段大规模训练策略，以缩小仿真到真实场景的差距。

Result: 模型在四个主流真实世界基准测试中拿到了第一名，同时准确率达到或超过其他主流高精度模型，而计算成本不到1%。

Conclusion: 作者证明了极其轻量级的模型也能具备出色的零样本泛化能力，并保持高效率，为高效立体匹配树立了新标杆。

Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.

</details>


### [90] [NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening](https://arxiv.org/abs/2511.16566)
*Misaal Khan,Mayank Vatsa,Kuldeep Singh,Richa Singh*

Main category: cs.CV

TL;DR: 本文提出了一种新型的基于图注意网络和多模态融合的儿童营养不良筛查系统NutriScreener，能够从儿童图像中自动检测和预测营养不良及人体测量数据，显著提高了检测效率和准确性，特别适用于低资源环境。


<details>
  <summary>Details</summary>
Motivation: 当前全球儿童营养不良问题依然严峻，传统的筛查手段操作繁琐且难以大规模应用，导致无法及时早期发现营养不良儿童，延误干预时机。因此，亟需一种高效、可扩展、操作简单的自动化检测方法。

Method: NutriScreener融合了CLIP视觉嵌入、多姿态图注意网络、知识库增强和上下文感知技术，能够处理样本类别不平衡问题并提升泛化能力。系统先基于儿童图片提取视觉嵌入，再结合知识库信息进行增强，最终实现营养不良分类和人体参数预测。

Result: 系统在AnthroVision、ARAN及自建CampusPose等多个跨洲大样本集上训练与测试，获得了0.79召回率、0.82 AUC和显著低的人体测量RMSE，并可通过人口特征匹配知识库进一步提升召回率25%、RMSE降低3.5cm。医生评价其准确率4.3/5，效率4.6/5。

Conclusion: NutriScreener在无约束、低资源环境下表现优异，是一种可靠、可扩展的儿童营养不良早期筛查工具，有望推动全球范围内儿童健康管理和疾病预防。

Abstract: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.

</details>


### [91] [POMA-3D: The Point Map Way to 3D Scene Understanding](https://arxiv.org/abs/2511.16567)
*Ye Mao,Weixun Luo,Ranran Huang,Junpeng Jing,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 本文提出了POMA-3D，这是首个基于点图(Point Map)的自监督3D表征模型，通过将3D坐标映射到结构化2D网格上，实现3D几何的表达与2D模型兼容，并设计多视角一致性学习和大规模场景数据集，提升3D理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D表征学习面临预训练先验缺失和数据稀缺问题，而点云和体素方式难以与强大的2D基础模型对接。因此，亟需一种既能保持3D几何结构又能利用2D先验的高效3D表示与学习方法。

Method: 1. 提出点图作为3D场景的表达方式，将3D坐标投影到2D有序网格，实现3D结构信息兼容2D模型输入。
2. 设计跨视角场景对齐策略以及POMA-JEPA联合嵌入—预测架构，保证多视角下几何表征一致性。
3. 构建并发布了ScenePoint大规模点图数据集，为自监督预训练提供数据支持。

Result: 实验证明，POMA-3D作为3D理解的主干网络，在仅输入3D几何的情况下，能够有效支持多种3D任务，包括3D问答、智能体导航、场景检索及定位等。

Conclusion: POMA-3D创新性地提出点图方式结合2D模型与3D几何，显著缓解了3D预训练稀缺问题，在多种3D场景理解任务上表现优异，对推动3D表征学习具有重要意义。

Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/

</details>


### [92] [Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks](https://arxiv.org/abs/2511.16574)
*Nirjhor Datta,Md. Golam Rabiul Alam*

Main category: cs.CV

TL;DR: 本文提出了一种医学图像分割中可控遗忘（unlearning）的新框架，能在不完全重训的情况下，有针对性地去除部分知识，同时保留整体性能。通过LoRA和师生蒸馏，系统有效遗忘敏感内容，且性能优异。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规加强和医学数据不断变化，模型需要在保证隐私和伦理性的前提下，有选择地“遗忘”某些知识（如病灶、特定类别），以应对数据删除和持续数据更新需求。传统的彻底重训成本高昂，缺乏高效遗忘机制。

Method: 作者提出“Erase to Retain”框架：采用师生蒸馏结构，引入Low-Rank Adaptation（LoRA），在学生网络解码端进行低秩子空间更新，实现遗忘病灶/类别等特定内容。分为两个阶段：第一阶段利用对抗性训练促使学生的表现在遗忘子集上与教师相悖，从而移除语义信息；第二阶段再通过head-only微调，恢复其在保留数据上的泛化能力。

Result: 在ISIC分割任务中，遗忘数据集IoU由0.875降至0.509，保留和验证集表现无显著下降（0.647到0.677）；在CHASE数据集测评一致有效。分类任务上，遗忘集准确率由87%降至64.1%，保留集准确率甚至提升（83.9%升至90.6%）。

Conclusion: 基于LoRA的子空间遗忘为医学图像分析提供了一条有效、可控且可恢复的模型遗忘路径，实现了在遗忘敏感数据的同时，最大程度地保留有用知识，有助于模型负责任和灵活的应用。

Abstract: The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.
  For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.
  These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.

</details>


### [93] [Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap](https://arxiv.org/abs/2511.16617)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 该论文提出利用生成式AI合成烟雾数据集，并结合无监督领域自适应和多种生成式方法，以提升野火烟雾早期检测模型的泛化和准确性。


<details>
  <summary>Details</summary>
Motivation: 野火早期发现依赖及时识别烟雾羽流，但受限于缺乏大规模标注数据，深度神经网络的性能难以完全发挥。如何解决数据匮乏的问题是提升烟雾检测实际效果的关键。

Method: 论文首先利用生成式AI技术合成大规模标注烟雾数据集作为训练资源。随后，应用无监督领域自适应方法（如风格迁移、生成对抗网络、图像抠图等）优化模型，使其能够更好地适应从合成到真实数据的转换，缩小数据分布差异。

Result: 通过整合合成数据和领域自适应技术，显著提升了烟雾分割模型从合成到真实场景中的检测效果。多种生成式方法进一步增强了合成数据的真实性，提高了模型的泛化能力和实用性。

Conclusion: 生成式AI与领域自适应技术为野火烟雾早期检测模型提供了新的发展路径，可有效改善数据匮乏带来的性能瓶颈，提升模型在真实环境下的检测精度和泛化能力。

Abstract: The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.

</details>


### [94] [SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking](https://arxiv.org/abs/2511.16618)
*Haofeng Liu,Ziyue Wang,Sudhanshu Mishra,Mingqi Gao,Guanyi Qin,Chang Han Low,Alex Y. W. Kong,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了针对手术视频分割的全新数据集和模型，显著提升了现有分割方法在手术场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的交互式视频物体分割（iVOS）模型（如SAM2）在手术应用中存在领域差距和长时追踪能力不足的问题，且缺少大规模带有时空注释的数据集用于发展和评估这些模型。

Method: 作者构建了SA-SV，这是迄今最大规模的手术iVOS基准数据集，包含8类手术类型、61,000帧、1,600个实例级时间片段注释（masklets）。在此基础上，提出了SAM2S模型，包括三大技术创新：1）DiveMem——可训练的多样化记忆机制，用于稳健的长期跟踪；2）时序语义学习用于提升器械理解；3）抗歧义学习减少多源数据集注释不一致性。

Result: 实验显示，SAM2如果在SA-SV上微调，平均$[J$&$[F$提升了12.99分。SAM2S模型进一步将平均$[J$&$[F$提升至80.42，比基础SAM2和微调SAM2分别高17.10和4.11分，且推理速度达到68帧/秒，同时具备很好的零样本泛化能力。

Conclusion: 构建的SA-SV数据集和提出的SAM2S模型有效促进了手术视频分割技术的发展，实现了强大的性能提升和实时性，为未来的手术AI助手等应用奠定基础。

Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.

</details>


### [95] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文针对现实场景中常见的类别长尾分布问题，提出改进方法提升长尾目标检测的性能，在LVISv1数据集上取得新的最好成绩，并探索了特征聚类和度量学习方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的目标检测数据集具有类别分布不均衡特性，许多类别实例很少，造成模型对稀有类别检测性能下降，这是当前主流方法尚未解决的重要难题。

Method: 采用两阶段Faster R-CNN框架，引入并改进了Balanced Group Softmax（BAGS）以缓解类别失衡；另外，引入度量学习优化特征嵌入，通过k-NN辅助推理，提升对稀有类别的判别能力。

Result: 在LVISv1数据集上 mAP 达到24.5%，优于先前的24.0%；实验证明基于度量学习和k-NN的方法对于提升长尾类别的检测表现有效。

Conclusion: 改进的BAGS框架和度量学习方法有效缓解了长尾目标检测中类别不平衡的问题，为提升稀有类别检测能力提供了有效途径。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [96] [Adaptive Guided Upsampling for Low-light Image Enhancement](https://arxiv.org/abs/2511.16623)
*Angela Vivian Dcosta,Chunbo Song,Rafael Radkowski*

Main category: cs.CV

TL;DR: 提出了一种新的低光图像高效放大方法AGU，能同时优化噪声和清晰度，在低光图像增强上性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前低光图像上采样难以同时优化降噪和增强细节，并且已有导向式方法在低光场景下效果不足，需要提升低质量图像的增强能力。

Method: AGU是一种基于导向图像的方法，通过多参数优化学习低光与明亮图像特征之间的关联，只需少量样本对即可训练，能在实时情况下生成高质量图像。

Result: 实验表明AGU在低光条件下，可以远优于现有技术，提升输出图像质量，无论在噪声还是清晰度方面都有明显提升。

Conclusion: AGU为低光图像高效增强提供了新方案，兼具高效性和优越的图像质量提升，实际应用价值高。

Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.

</details>


### [97] [SAM 3D: 3Dfy Anything in Images](https://arxiv.org/abs/2511.16624)
*SAM 3D Team,Xingyu Chen,Fu-Jen Chu,Pierre Gleize,Kevin J Liang,Alexander Sax,Hao Tang,Weiyao Wang,Michelle Guo,Thibaut Hardin,Xiang Li,Aohan Lin,Jiawei Liu,Ziqi Ma,Anushka Sagar,Bowen Song,Xiaodong Wang,Jianing Yang,Bowen Zhang,Piotr Dollár,Georgia Gkioxari,Matt Feiszli,Jitendra Malik*

Main category: cs.CV

TL;DR: 本文提出了一种新型的3D生成模型SAM 3D，可根据单张图片重建物体的几何结构、纹理和布局，特别适用于复杂自然场景，有效应对遮挡和环境杂乱。通过引入‘人-模型协同标注’流程，显著扩展了3D重建标注数据规模，并通过多阶段训练框架结合合成预训练与真实数据校准，突破了3D数据瓶颈。实验结果显示，该方法在人类偏好测试中，实现了对比现有技术的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的3D物体重建面临数据稀缺、场景复杂（如遮挡、杂乱）和现有方法泛化能力弱等挑战。作者旨在攻克这些难题，推动3D重建在真实环境中的应用效果。

Method: 作者提出SAM 3D，通过‘人-模型协同标注’流程，大规模收集带有详细形状、纹理、姿态信息的3D视觉数据。随后，采用多阶段训练策略：先用合成数据进行预训练，再用真实数据做对齐优化，提升模型在自然场景下的泛化与视觉推理能力。

Result: SAM 3D在真实物体和场景上的人类偏好测试中相较最新方法实现了5:1的胜率，在3D重建准确性、自然性和视觉相关性上取得了显著优势。

Conclusion: SAM 3D不仅突破了3D重建数据瓶颈，并在真实世界复杂场景中展现出优越表现。相关代码、模型权重、在线演示与新基准数据集将公开，有望推动三维视觉领域的发展。

Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.

</details>


### [98] [TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming](https://arxiv.org/abs/2511.16642)
*Zeyuan Yin,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本论文提出了TRIM方法，通过减少3D高斯扩散模型生成过程中的冗余轨迹和原语，加速推理过程，提高效率，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 目前3D高斯扩散模型在去噪处理和后处理时由于高斯原语数量庞大，导致生成速度慢、可扩展性差。亟需高效加速推理的方法，以便实际应用。

Method: 提出的TRIM（Trajectory Reduction and Instance Mask denoising）方法，采用基于候选原语质量判断的轻量化选择器模型，对去噪轨迹做早期裁剪，结合实例掩膜去噪，去除冗余背景区域，减少每一步运算负担。该方法可于推理阶段动态调整采样轨迹。

Result: 大量实验和分析显示，TRIM在提升3D生成速度的同时也提升了生成质量，验证了方法的有效性。

Conclusion: TRIM方法能够在不损失输出质量的前提下，大幅提升3D高斯扩散模型的推理效率和可扩展性。

Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.

</details>


### [99] [Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision](https://arxiv.org/abs/2511.16650)
*Shuyu Cao,Chongshou Li,Jie Xu,Tianrui Li,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D分层语义分割（3DHS）框架，有效缓解多层级冲突和类别不平衡问题，实现了更优的三维场景分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DHS方法存在两个主要挑战：1）多标签学习时参数共享引发的跨层级冲突；2）3D场景不同层级之间类别分布不均衡，使模型性能被主流类别主导。

Method: 作者提出了一个由主3DHS分支和辅助判别分支组成的新颖框架。为解决层级冲突，采用多解码器构建后解耦结构，并利用由粗到细的分层引导与一致性，缓解层级间欠拟合与过拟合问题，单独约束各层级类别不平衡。此外，引入基于语义原型的双分支监督机制，相互监督辅助判别与主3DHS分支，提升对小众类别的识别能力。

Result: 在多个数据集和不同网络主干上进行广泛实验，该方法达到了当前最优的3DHS分割性能。同时，框架核心组件还可作为插件用于提升现有方法表现。

Conclusion: 本文方案有效缓解了多层级、类别不均衡对3DHS的影响，分割表现显著提升，具有良好的通用性和可扩展性，能作为提升不同3DHS模型的实用工具。

Abstract: 3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.

</details>


### [100] [Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation](https://arxiv.org/abs/2511.16653)
*Md. Samiul Alim,Sharjil Khan,Amrijit Biswas,Fuad Rahman,Shafin Rahman,Nabeel Mohammed*

Main category: cs.CV

TL;DR: 本文提出了一种高效的无结构剪枝新方法，通过教师网络引导和知识蒸馏，在估算重要性得分时识别和保留关键参数，使神经网络在高稀疏度下依然保持优良性能，且大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统的无结构剪枝依赖多次训练-剪枝-再训练，计算量大且低效，无法很好地满足资源受限环境下的模型部署需求。作者希望在减少计算成本的同时，提升剪枝后网络的性能表现。

Method: 提出一种将知识蒸馏与重要性得分估算紧密结合的教师引导剪枝策略。在剪枝前，利用教师网络的梯度信号计算每个参数的重要性得分，实现一次性全局剪枝。剪枝后，通过考虑稀疏性的再训练（可伴随或不伴随知识蒸馏），恢复模型精度，无需重新激活已剪参数。

Result: 在CIFAR-10、CIFAR-100、TinyImageNet等多个图像分类基准数据集实验中，该方法在实现极高稀疏度的同时仅造成极小性能损失，且在高稀疏度下优于现有EPG、EPSD等方法，并比COLT等迭代剪枝方案更高效。

Conclusion: 该框架明显减少模型稀疏化的计算需求，在保证准确率的同时提升压缩率，非常适用于计算资源受限的实际应用场景。

Abstract: Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.

</details>


### [101] [Solving Spatial Supersensing Without Spatial Supersensing](https://arxiv.org/abs/2511.16655)
*Vishaal Udandarao,Shyamgopal Karthik,Surabhi S. Nath,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.CV

TL;DR: 本文通过对Cambrian-S提出的VSI-Super-Recall (VSR) 和VSI-Super-Counting (VSC)两项基准、及其定制化推理方法的评估，质疑了其空间超感知能力的真实性。结果显示，简单的模型几乎可以完全解决VSR基准，而VSC基准对简单扰动极其敏感，说明现有基准并不能真实反映空间超感知能力。


<details>
  <summary>Details</summary>
Motivation: Cambrian-S提出旨在通过空间超感知提升视频世界模型性能并设立了新的基准测试，但作者怀疑其基准和方法是否真正评估了空间超感知，因而希望通过系统实验证明现有方法的局限。

Method: 作者设计了极简基线模型NoSense，仅用bag-of-words和SigLIP，不考虑时序结构来测试VSR基准，并提出VSC-Repeat实验（视频自拼接），用以检验VSC基准是否存在被简单捷径利用的现象。

Result: NoSense模型在4小时视频的VSR任务上能达到95%的高准确率，几乎解决该任务，无需空间建模；VSC-Repeat实验中，Cambrian-S的准确率从42%骤降到0%，表明Cambrian-S的方法严重依赖于benchmark中的简单规律（如房间不会重复进入），而不是空间超感知。

Conclusion: 现有VSI-Super基准未能有效衡量空间超感知能力，Cambrian-S的推理策略更多是利用了基准测试中的捷径而非真正实现空间超感知。作者发布了代码并附录了Cambrian-S作者对此的回应，以提供平衡视角。

Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity

</details>


### [102] [PartUV: Part-Based UV Unwrapping of 3D Meshes](https://arxiv.org/abs/2511.16659)
*Zhaoning Wang,Xinyue Wei,Ruoxi Shi,Xiaoshuai Zhang,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: PartUV是一种针对3D表面展平（UV展开）的新方法，在处理AI生成的嘈杂、结构复杂的网格物体时，能生成更少、对齐部件的UV区域，有效减少畸变和边界碎片问题，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有的UV展开方法在面对AI生成的、质量差的复杂网格时，常因网格噪声和不规则性导致区域碎片化严重，边界不理想，影响后续任务。需要新的方法处理这类问题，生成更合理区域划分。

Method: 提出了名为PartUV的UV展开流程。它基于最新的学习型零件分解方法（PartField），结合高层次语义分解和创新几何启发式，递归地、顶层拆分3D表面，控制每个UV区域的畸变在给定容许范围内，并极力减少UV区域数量。流程还增强了参数化、打包算法及对非流形/退化网格的处理，并通过并行化提高效率。

Result: 在四个公开数据集（包括人工、CAD、AI生成和常见形状）上测试，PartUV在UV区域数量与缝隙长度上显著优于现有工具与神经网络方法，畸变水平保持可比，尤其在难处理的网格上有很高的成功率，并实现了零件专属多平铺等新应用。

Conclusion: PartUV为UV展开任务带来了部件感知、低畸变、高效率的新范式，特别适合AI生成等复杂网格。其泛用性强、效果好，为相关后处理任务提供了更优基础。

Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.

</details>


### [103] [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/abs/2511.16662)
*Eddie Pokming Sheung,Qihao Liu,Wufei Ma,Prakhar Kaushik,Jianwen Xie,Alan Yuille*

Main category: cs.CV

TL;DR: TriDiff-4D是一种基于扩散模型的新型4D头像生成方法，从文本描述生成高质量、时序一致的可控4D数字人，在效率与表现力上大幅优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 目前从文本描述自动生成高真实感、可控且连贯的4D头像（3D动画序列）仍面临多种挑战，如时空不一致、运动不自然、伪影严重、计算复杂且控制能力有限。针对这些问题，研究者希望提出一种新方法，提升模型的可用性和动画的质量。

Method: TriDiff-4D采用扩散模型与三平面表征相结合的流水线，分两步生成4D内容：先根据文本生成标准姿态下的3D头像和对应动作序列，再用第二个扩散模型驱动3D头像按动作生成完整4D序列。方法通过自回归方式合成任意长度动画，并利用大量3D及动作数据学习骨架驱动的结构与运动先验，提升时间一致性、动作准确性与效率。

Result: 实验表明，TriDiff-4D能在秒级时间内生成视觉保真度高、几何准确的复杂4D动画，表现远优于现有方法，而且显著缩短生成时间（由数小时降至秒级），并解决了传统方法在时空一致性及运动复杂性上的短板。

Conclusion: TriDiff-4D在4D数字人生成领域取得了重要突破，不仅大幅提升了动画质量与生成效率，还为可控、高保真4D内容创作开辟了新方向，推动了相关行业与学术应用的发展。

Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.

</details>


### [104] [SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation](https://arxiv.org/abs/2511.16666)
*Zhenyuan Qin,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出了SceneDesigner，一个能够实现多物体9维（9D）姿态（包括位置、大小、朝向）灵活操控的生成方法，克服了现有方法在多物体9D姿态控制上的限制，提升了生成质量与可控性。


<details>
  <summary>Details</summary>
Motivation: 当前可控图像生成已能处理身份和风格操控，但对多物体的空间位置、大小和朝向（9D姿态）还难以精确控制。现有方法受限于可控性不足和图像质量下降，无法实现全面多物体9D姿态控制。作者旨在突破这些局限，实现更高维度和多对象的精确场景操控。

Method: SceneDesigner在预训练模型基础上引入带分支的网络结构，并提出了编码9D姿态信息的新型表示方式CNOCS map，增强了几何解释能力，提升训练效率与稳定性。为支持模型训练，作者构建了涵盖多来源图像及9D标签的新数据集ObjectPose9D。为克服数据不平衡及少见姿态表现不佳问题，引入两阶段训练与强化学习，第二阶段采用基于奖励的目标对模型进行细化。此外，推理时提出了解耦目标采样，减少复杂场景下物体不足与概念混淆，并通过个性化权重支持定制化姿态控制。

Result: SceneDesigner在多个定性与定量指标上都显著优于现有方法，尤其在多物体9D姿态控制可控性和生成质量方面都有显著提升。

Conclusion: SceneDesigner首次实现了高效、灵活、多物体9D姿态精确可控的图像生成，为多物体场景构建与可控生成提供了新思路，可在用户指定下实现个性化操控。相关代码已开源。

Abstract: Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.

</details>


### [105] [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668)
*Yang Luo,Xuanlei Zhao,Baijiong Lin,Lingting Zhu,Liyao Tang,Yuqi Liu,Ying-Cong Chen,Shengju Qian,Xin Wang,Yang You*

Main category: cs.CV

TL;DR: 本论文提出了V-ReasonBench，这是一个用于系统性评估生成式视频模型推理能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式视频模型的零样本推理能力不断提升，亟需一种系统且可靠的评测工具，弥补当前评估手段的不足。

Method: V-ReasonBench基准涵盖四个推理维度：结构化问题解决、空间认知、基于模式的推断以及物理动态。任务基于合成和真实图像序列，具备可复现性、可扩展性和明确的可验证答案。

Result: 对六种主流视频模型的评测显示，不同模型在四个推理维度上表现差异显著。论文还分析了与图像模型的对比、模型常见“幻觉”现象以及视频长度对链式推理的影响。

Conclusion: V-ReasonBench为定量评估视频推理提供了统一、可复现的框架，将有助于推动更符合人类推理方式的视频生成模型的发展。

Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.

</details>


### [106] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 本文提出了视频作为下一事件预测（VNEP）的新型答案方式，并引入VANS模型，通过融合视觉语言模型（VLM）与视频扩散模型（VDM）及联合强化学习，实现视频方式的更直观预测与展示，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型在实际应用中非常有影响力，但视频生成仍主要集中在娱乐领域。相比文本，视频更直观地展示物理世界的信息，有助于教学等应用，因此探索以视频作为问题答案的新可能性。

Method: 提出了Video-Next-Event Prediction（VNEP）任务，需要输出视频作为应答。VANS模型采用强化学习，将视觉语言模型（VLM）和视频扩散模型（VDM）结合，通过设计Joint-GRPO算法，让两者以共享奖励联合优化。为此还构建了专用数据集VANS-Data-100K。

Result: VANS在程序化和预测性数据集上均取得了当前最优效果，无论是事件预测的正确性还是视频可视化效果均优于已有方法。

Conclusion: VANS表明以视频作为答案方式能显著提升下一事件预测任务的表现，开辟了程序学习和创造性探索的新路径。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


### [107] [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/abs/2511.16670)
*Chenyu Lin,Cheng Chi,Jinlin Wu,Sharon Li,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种可自动切换快慢思维模式的视觉语言模型（DualMindVLM），依据任务难度分配推理资源，实现了高效且准确的视觉推理。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在进行需要推理的任务时，通常追求冗长、详细的推理链路，导致计算成本过高，而人类在简单问题上会快速思考，在复杂问题时则会深入分析。如何让模型也学会类似的人类“快—慢系统”思维分配，是提升推理效率与准确率的关键。

Method: 方法分两步：首先，依据预训练模型在不同问题上输出答案的长度，将数据标记为需要“快思考”或“慢思考”；之后，结合GRPO方法和思考模式标签，把模型训练为能够根据任务难度自动切换快慢思考，实现双模态推理。

Result: DualMindVLM显著优于基础模型，并在视觉推理任务上达到与最新方法相当的效果，同时保持了非常高的token效率。

Conclusion: 提出的双思维模式不仅显著提升了视觉推理模型的效率，也在准确率上达到了业界最高水平，实现了性能与计算资源的有效平衡。

Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.

</details>


### [108] [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/abs/2511.16672)
*Omkat Thawakar,Shravan Venkatraman,Ritesh Thawkar,Abdelrahman Shaker,Hisham Cholakkal,Rao Muhammad Anwer,Salman Khan,Fahad Khan*

Main category: cs.CV

TL;DR: 提出了一种无需人工数据或外部奖励的自进化LMM框架EvoLMM，有效提升多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 主流的大型多模态模型（LMM）训练依赖人工标注数据或外部奖励模型，限制了模型的自动性和扩展能力。为突破该限制，急需开发全自动、无监督的LMM训练方法。

Method: 提出EvoLMM框架，从同一主干模型中构建出合作的Proposer和Solver两个智能体：Proposer生成多样、依赖图像的问题，Solver通过内部一致性机制自我解答，整个过程形成自我奖励的动态反馈，不依赖人工或外部判别。

Result: 以Qwen2.5-VL为基础模型，EvoLMM在ChartQA、MathVista和MathVision等多模态数学推理基准上实现了最高约3%的性能提升，仅使用原始训练图像，无需人工标注。

Conclusion: EvoLMM框架无需人工干预即可提升LMM推理能力，为无监督LMM自进化研究提供了高效新基线。

Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.

</details>


### [109] [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/abs/2511.16673)
*Jing Wen,Alexander G. Schwing,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出无需人体姿态输入即可从单张或少量图像重建可动画3D人体模型的方法NoPo-Avatar。


<details>
  <summary>Details</summary>
Motivation: 现有许多3D人体重建方法在测试时依赖精准的人体和相机姿态信息，但这些输入现实中常常不是很准确，噪声会显著降低重建质量。作者希望去除这一限制，使方法对实际应用更友好。

Method: 提出NoPo-Avatar系统，仅依靠输入图像进行重建，无需在测试时输入人体姿态。通过对比分析传统依赖姿态输入的方法和新方法的鲁棒性。

Result: 在THuman2.0、XHuman及HuGe100K等多个真实和合成数据集上实验，NoPo-Avatar在无姿态输入的实际应用场景中效果优于现有方法，在完美姿态辅助下也能达到可比水平。

Conclusion: NoPo-Avatar摆脱了对姿态输入的依赖，提高了3D人体重建在真实场景下的适用性和实用性。

Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).

</details>


### [110] [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/abs/2511.16674)
*George Cazenavette,Antonio Torralba,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文提出了一种针对预训练视觉模型线性探针的高效数据集蒸馏方法，能生成极小但表现优异的合成数据集，并优于多种真实图像基线，还具备良好的模型泛化与解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法大多关注于支持从随机初始化训练模型，但主流视觉研究已普遍采用大规模自监督预训练模型。因此，需要方法能直接为这些预训练模型“定制”高效的蒸馏数据集，特别是用于线性探针的训练。

Method: 作者提出Linear Gradient Matching，即优化合成图像使其通过预训练特征提取器后，在训练线性分类器时诱导出的梯度能够逼近真实数据产生的梯度，从而提升合成数据集的有效性和可泛化性。

Result: 该方法生成的数据在多个基线真实数据集上性能优越，并展现出跨预训练视觉模型的泛化能力。例如，用DINO主干蒸馏的数据可训练CLIP线性探针获得竞争性结果。此外，蒸馏数据在细粒度分类和模型表征空间相似性测量等任务中表现突出。

Conclusion: 本文方法有效提升了预训练视觉模型下线性探针的训练效率和泛化能力，并兼具模型可解释性，显示其为高效数据集生成和理解视觉模型提供了有力工具。

Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [111] [What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning](https://arxiv.org/abs/2511.15886)
*Jeremias Ferrao,Ezgi Basar,Khondoker Ittehadul Islam,Mahrokh Hassani*

Main category: cs.CL

TL;DR: 本文探讨了多语言大模型在链式思维（CoT）推理过程中的归因模式，发现当前方法在不同语言和干扰条件下存在准确性和可解释性的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多语言任务中的应用，链式思维（CoT）已被证明能提升推理表现，但其产生的推理链条在可解释性和忠实性方面存在疑问。作者想要系统性评估这种归因和解释在多语言环境下的表现及其局限。

Method: 作者对Qwen2.5 1.5B-Instruct模型在MGSM多语言推理基准测试集上，应用了ContextCite（步骤级归因）与Inseq（标记级归因）两种归因工具，系统分析模型生成推理链时不同语言的归因表现。同时，通过插入否定或干扰语句，考察其对模型准确性和归因一致性的影响。

Result: （1）归因分数过度聚焦于最终推理步骤，错误回答尤为明显；（2）结构化CoT提示主要在高资源拉丁文字语言中提升准确性；（3）引入否定和干扰句会降低模型准确性与归因一致性。

Conclusion: 目前CoT提示在多语言环境下的稳健性和可解释性存在瓶颈。尤其在低资源和结构复杂的语言中，这种方法难以保证推理链条的真实、透明，对模型构建和应用提出挑战。

Abstract: This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.

</details>


### [112] [Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language](https://arxiv.org/abs/2511.15887)
*Seungbeen Lee,Jinhong Jeong,Donghyun Kim,Yejin Son,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出了一个新的机器心智理论（ToM）评测框架Motion2Mind，重点在于AI对非语言线索的理解，但实验证明AI系统在这方面还远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 目前主流的ToM评估主要关注于‘信念’等有限心理状态，且多依赖语言或明确信息推理，忽略了丰富的非语言交流在社会认知中的重要作用。为推动AI对更广泛人类心智状态及非语言表达的理解，需要新的评测工具。

Method: 研究建立了Motion2Mind框架与数据集。该数据集包含了222类肢体非语言线索的视频，以及397种心智状态的手工标注解释。通过与专家知识库结合，形成系统性评测方法，检验AI在检测与解释非语言线索能力。

Result: 评估发现，现有AI系统在非语言线索的检测和解释上明显落后于人类：不仅检测表现有显著差距，在解释方面还出现过度解释等偏差。

Conclusion: 当前AI在社会感知、心智理论相关的非语言交流理解方面能力有限。Motion2Mind为细致评估和推动AI在人类社会智能发展上提供了新基线和研究方向。

Abstract: Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.

</details>


### [113] [TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues](https://arxiv.org/abs/2511.15976)
*Sarik Ghazarian,Abhinav Gullapalli,Swair Shah,Anurag Beniwal,Nanyun Peng,Narayanan Sadagopan,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了TOD-ProcBench基准，用于评估大语言模型（LLM）在多轮面向任务对话（TOD）中理解和执行复杂自然语言指令的能力，填补了现有基准过于简化实际场景指令的问题。


<details>
  <summary>Details</summary>
Motivation: 现实任务型对话场景中的指令通常复杂，包含详细操作步骤和约束，而现有基准只覆盖了简化的意图、槽位等结构，无法充分反映真实指令严格性。因此需要一个覆盖复杂指令并可系统测试LLM能力的新基准。

Method: 作者基于高质量的ABCD数据集构建指令文档和对应对话，并将复杂指令抽象为多级条件-动作语句，设计了三个任务：1) 指令相关性检索与下一步动作预测；2) 违背指令响应检测（通过注入不一致改变原示例）；3) 复杂指令条件下的响应生成。还分析了多语言和不同指令格式对模型表现的影响。

Result: 基准可以系统、多维度评估LLM多轮对话中对于复杂指令的理解与执行力，包括遵从性、检错能力以及多语言适应性等。

Conclusion: TOD-ProcBench补足了现有任务型对话基准的短板，为LLM在复杂现实指令环境下的能力评测提供了全方位工具，有助于推动相关模型和应用的发展。

Abstract: In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.

</details>


### [114] [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)
*Kieron Kretschmar,Walter Laurito,Sharan Maiya,Samuel Marks*

Main category: cs.CL

TL;DR: 本论文提出了一个名为LIARS' BENCH的新测试平台，用于系统性评估和推进对大语言模型（LLM）说谎检测方法的研究。


<details>
  <summary>Details</summary>
Motivation: 以往检测LLM说谎的方法多只在有限场景下验证，无法覆盖模型可能生成的多种类型谎言。该领域缺乏多样和具有代表性的测试基准，限制了骗谎检测方法的评价和改进。

Method: 作者构建了LIARS' BENCH测试集，包含72,863条由四个开源模型在七个数据集上生成的谎言与诚实回答例子，涵盖不同类型的谎言和多种说谎动机。基于该平台，系统评测了三种主流的黑盒与白盒说谎检测技术。

Result: 结果显示，现有骗谎检测方法在某些特定类型的谎言下效果很差，尤其是在仅凭输出无法判断模型是否说谎的情况下，检测率明显下降。

Conclusion: LIARS' BENCH揭示了以往方法的不足，为骗谎检测方法的发展和改进提供了多样化、具有挑战性的测试平台，对该领域具有重要推动作用。

Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.

</details>


### [115] [Learning Tractable Distributions Of Language Model Continuations](https://arxiv.org/abs/2511.16054)
*Gwen Yidou-Weng,Ian Li,Anji Liu,Oliver Broadrick,Guy Van den Broeck,Benjie Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法LTLA，将大型语言模型与可解的HMM代理结合起来，提高带有复杂约束的文本生成质量，并克服了效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的受控文本生成方法常用HMM等简化模型，加速推断但上下文感知较差，导致生成文本质量下降，特别是在复杂约束如样式或视觉上下文的条件下。因此需要一种新方法，在兼顾推断效率的同时提升对上下文和约束的感知能力。

Method: 提出了Learning to Look Ahead (LTLA)方法，具体做法是用大语言模型进行前缀编码，同时让HMM代理模型计算精确的续写概率。LTLA通过批量处理下一个token候选配合前缀的唯一HMM状态更新，并仅用语言模型的隐藏状态调整HMM的隐状态先验，保持HMM解码器参数不变，实现前缀间的计算复用，提升推断效率。

Result: LTLA较无条件HMM在条件概率上表现更好，能用于视觉-语言模型的约束生成（如独立HMM编码不了视觉上下文的场景），并且在多个受控生成任务下进一步提升了约束满足率，流畅度无明显下降，且推断延迟低。

Conclusion: LTLA能够在保持高效推断的同时，更好地利用深层语言模型的上下文能力，有效提升了受控文本生成的质量和约束满足能力，未来对于多模态和复杂约束生成具有较大应用前景。

Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.

</details>


### [116] [Early science acceleration experiments with GPT-5](https://arxiv.org/abs/2511.16072)
*Sébastien Bubeck,Christian Coester,Ronen Eldan,Timothy Gowers,Yin Tat Lee,Alexandru Lupsasca,Mehtaab Sawhney,Robert Scherrer,Mark Sellke,Brian K. Spears,Derya Unutmaz,Kevin Weil,Steven Yin,Nikita Zhivotovskiy*

Main category: cs.CL

TL;DR: 本文展示了GPT-5在多个科学领域中帮助研究人员推动进展的实际案例，强调了AI与人类专家协作的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 尽管前沿AI如GPT-5潜力巨大，许多科学家仍不了解其具体能力。作者旨在通过具体案例，让科研人员更好地理解AI在推动科学发现中的作用。

Method: 收集并展示GPT-5参与数学、物理、天文学、计算机科学、生物学和材料科学等领域研究的案例，通过记录和分析人机互动，总结AI协作的特点，以及人类专家介入的必要性。

Result: GPT-5在多领域中协助研究人员提出了新的研究步骤，节省了部分专家的时间。特别是在数学方面，GPT-5帮助人类解决了四个原本未解的问题（经人类专家仔细验证），但也有地方表明人类判断依然不可替代。

Conclusion: 尽管AI辅助的研究成果当前规模有限，但其方法和成效表明，随着AI发展加速，未来在人类与AI协作下有望取得更深远的科学进展。

Abstract: AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.

</details>


### [117] [ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models](https://arxiv.org/abs/2511.16122)
*Qing Zhang,Bing Xu,Xudong Zhang,Yifan Shi,Yang Li,Chen Zhang,Yik Chung Wu,Ngai Wong,Yijie Chen,Hong Dai,Xiansen Chen,Mian Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于集成学习的新型自动提示词优化（APO）框架ELPO，在多个任务上优于现有方法，显著提升了提示词生成与优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在实际应用中高度依赖精心设计的提示词，但手动编写提示词耗时且费力。虽然目前已有自动提示词优化（APO）方法，但大多仅聚焦于单一模型或算法，难以应对复杂任务，成为实际应用的瓶颈。

Method: 论文提出了ELPO框架，引入集成学习思想，通过投票机制联合多种生成策略和搜索算法来优化提示词。此外，ELPO还创新性地提升了提示词生成与搜索的算法效率。

Result: 实验表明，ELPO在多个任务中超过了目前最优的提示词优化方法。例如，在ArSarcasm数据集上的F1分数提升了7.6。

Conclusion: ELPO作为一种基于集成学习的提示词优化框架，为复杂任务提供了更准确和稳健的优化策略，有效推动了自动提示词优化领域的发展。

Abstract: The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.

</details>


### [118] [TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating](https://arxiv.org/abs/2511.16147)
*Dabiao Ma,Ziming Dai,Zhimin Xin,Shu Wang,Ye Wang,Haojun Fei*

Main category: cs.CL

TL;DR: 论文提出了一种新的参数高效微调（PEFT）方式——Token-Selective PEFT（TS-PEFT），通过只选定部分位置索引施加微调，结果显示这种选择性方法比传统全部微调更优。


<details>
  <summary>Details</summary>
Motivation: 传统PEFT方法默认对所有位置索引都进行参数微调，作者质疑这一做法是否真正必要，旨在探索是否可以通过更精细的选择进一步提升微调效率和性能。

Method: 作者提出TS-PEFT范式：引入选择函数S，仅对部分token的位置索引进行参数修改。然后将这种方法应用于NLP和CV领域的大模型，实验对比分析了传统全局PEFT与TS-PEFT的性能差异。

Result: 实验发现，对所有位置索引进行PEFT不仅多余，甚至可能对性能有害。TS-PEFT通过选择性微调，反而带来了更好的下游任务表现。

Conclusion: 作者建议对PEFT方法进行更有针对性的改进，而非一味全局应用，为未来大模型微调提供了新的优化视角和研究方向。

Abstract: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.

</details>


### [119] [SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning](https://arxiv.org/abs/2511.16198)
*Sebastian Haan*

Main category: cs.CL

TL;DR: SemanticCite是一个基于AI的文献引用核查系统，可自动验证学术论文中引用的准确性，并给出细致的理由和相关文本片段，提升科研诚信，实现大规模引用校验。


<details>
  <summary>Details</summary>
Motivation: 当前学术论文引用存在严重问题，比如语义引用错误、AI生成的虚假引用、传统引用格式无法精确定位证据部分，这些都影响了科研交流和文献追溯的精度，亟需更智能的引用验证方案。

Method: SemanticCite系统结合多检索方式，开发了四分类引用判别模型（支持、部分支持、不支持、不确定），能自动分析引用内容与原文的对应关系，并为用户提供详实推理和相关证据片段。该系统采用轻量级语言模型进行微调，在性能与资源消耗之间达到很好的平衡。

Result: 实验表明，经过微调的轻量级模型在引用核查任务上表现与大型商用系统相当，但计算开销更低，实现了大规模引用验证的可行性。研究还发布了包含上千条引用数据、功能分类和语义注释的多学科数据集及开源代码。

Conclusion: SemanticCite系统为高效、透明、可扩展的引用核查提供了工具基础，有助于提升科研诚信，优化同侪评审流程，并为AI成果质量控制提供保障，其全部数据和代码开源，推动学术界引用规范化发展。

Abstract: Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.

</details>


### [120] [SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs](https://arxiv.org/abs/2511.16275)
*Xingtao Zhao,Hao Peng,Dingli Su,Xianghua Zeng,Chunyang Liu,Jinzhi Liao,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型（LLM）不确定性量化方法SeSE（语义结构熵），能更好检测与量化生成过程中的幻觉（错误信息），实验验证其优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要基于概率分布或两两距离，忽视了潜在的语义结构信息，导致LLM在安全关键场景下难以准确识别和规避幻觉。因此需要更精细、更结构化的不确定性衡量工具。

Method: 论文提出了适应性稀疏有向语义图构建算法，提取语义空间中的结构信息，并通过分层抽象生成最优语义编码树，将语义结构熵（SeSE）定义为结构熵，衡量LLM内在语义空间的不确定性。此外，方法还可扩展至长文本中具体事实的细粒度不确定性分析。

Result: 在29组模型与数据集实验中，SeSE在检测LLM幻觉及不确定性量化性能上，显著优于现有先进标准，包括有监督方法与最新KLE方法。

Conclusion: 通过结构化建模语义空间的不确定性，SeSE为LLM幻觉检测和不确定性量化提供了一种理论解释更充分、效果更优的新方法，对安全敏感应用具有重要意义。

Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

</details>


### [121] [SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning](https://arxiv.org/abs/2511.16324)
*Wei Xia,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 本文提出了一种无需重新训练、可广泛适配的LLM对齐方法SDA（Steering-Driven Distribution Alignment），通过在推理阶段动态调整输出概率，提高模型与人类意图的一致性，并在多个开源大模型和任务上取得显著对齐效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）部署日益普及，模型在满足各种实际需求时需要更好地对齐人类意图。但现有方法常需高成本的再训练或大量监督，推理阶段高效对齐仍是难题。

Method: 提出SDA框架，在不重新训练模型的前提下，通过用户自定义指令动态调整模型在推理阶段的输出分布。SDA兼容多种开源LLM，可与训练型对齐手段结合，支持个性化偏好调整。

Result: 在8个不同的开源LLM及3个对齐维度（有用性、无害性、诚实性）上测试，SDA分别带来了64.4%、30%、11.5%的平均性能提升。结果展示了该方法的有效性和通用性。

Conclusion: SDA能够高效、低资源地提升多种LLM在实际中的对齐能力，无需额外训练，支持个性化，是一种具有广泛适用性的对齐新框架。

Abstract: With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.

</details>


### [122] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

TL;DR: 本文提出了一种自重写（self-rewriting）框架，通过让大模型自我重写推理过程，提升其内部思维质量和效率。该方法在多任务、多模型规模实验中有效提升了推理准确率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练的大型推理模型主要关注最终正确性，难以细致监督其内部推理过程，导致出现过度思考、思考不足、冗余或混乱等问题，限制推理质量提升。因此需要更细致的内部推理监督方法。

Method: 提出自重写框架，模型能自我重写推理文本并学习优化推理过程。采用选择性重写策略，仅对“简单”样本（模型多次都做对的部分）执行重写，从而不影响现有奖励信号。在实现上，将重写与传统生成结合在一个批次内，保证算法可扩展且只增加约10%计算开销。

Result: 在多种任务和模型规模下实验证明：自重写方法准确率提升0.6，推理文本长度减少46%；在无需显式缩短提示下效果优于其他强基线方法；内部推理质量得分提升7.2，有效缓解推理过程中的缺陷。

Conclusion: 自重写框架在提升推理质量、效率方面效果显著，对于大推理模型的内部推理优化具有重要意义，拓展了RL奖励机制在细致推理监督方向的应用。

Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [123] [NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345)
*Blake Matheny,Phuong Minh Nguyen,Minh Le Nguyen,Stephanie Reynolds*

Main category: cs.CL

TL;DR: 本文提出了三个新的成语和修辞性语言数据集，用于评估和提升大语言模型处理英语成语、修辞性表达的能力。实验涵盖了大规模候选数据集与两个人工标注的高质量数据集，并在基线模型上进行了广泛测试。


<details>
  <summary>Details</summary>
Motivation: 成语和修辞性语言在日常交流中广泛存在，但现有大语言模型（LLMs）对此类表达的理解依然受限。随着社交媒体的流行，非正式语言数据更易获取，如何让LLMs更好地理解并处理这些表达成为难题。作者希望通过构建更丰富、覆盖面更广的数据集，进一步缩小游戏模型与人类理解之间的差距。

Method: 作者收集并整合了多个成语和修辞性语言的公开数据集，提取成语短语，从大语料库中检索其上下文，最终构建了一个大规模候选数据集和两个由人工严格标注的成语/修辞性语言数据集。随后，数据集经过后处理，具备通用模型训练兼容性，并在预训练语言模型上进行了成语识别、槽位标注和序列标注等任务的评测。

Result: 新构建的数据集能够较好地覆盖多样的成语和修辞性表达，为模型训练提供良好资源。基于这些数据集的实验表明，当前预训练模型在成语和修辞语言识别任务上仍有较大提升空间，新数据集为未来更有效的微调和新方法开发提供了坚实基础。

Conclusion: 面对LLMs在理解成语和修辞表达方面的局限，本文所发布的数据集不仅补充了现有资源，还推动了面向模型泛化与提升理解能力的研究进展。实际应用表明这些数据集对模型训练效果改善具有积极意义。

Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.

</details>


### [124] [Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies](https://arxiv.org/abs/2511.16353)
*Jonathan Kamp,Lisa Beinborn,Antske Fokkens*

Main category: cs.CL

TL;DR: 本文探讨了用于自然语言模型解释的人类推理（rationales）在评估模型是否“学对了理由”时的有效性，并发现目前常用的sufficiency指标的解释力有限。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理任务中，人类标注的rationales被用来评估模型预测对标签的推理是否合理。但当前常用的sufficiency（充分性）指标对于rationales信息对模型表现的影响分析有限，因此需要进一步研究。

Method: 作者将sufficiency与两种建模范式结合：(1) 通过token分类来判断模型能否识别合理的rationales词元；(2) 通过在输入中融入rationales并采用注意力正则化来提升模型性能。

Result: 研究发现，即使rationales提供了高度信息，未必能提升分类准确率；sufficiency反映了非rationales上下文对分类的影响，与token分类能力无关。在跨领域分类任务中，融入rationales有提升效果，但在不同任务和模型中结果不一致。

Conclusion: 当前关于rationales的评价标准（如sufficiency）与token分类等能力关系不大，rationales背后的复杂性需要被更系统的新指标捕捉和进一步研究。

Abstract: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.

</details>


### [125] [AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser](https://arxiv.org/abs/2511.16397)
*Ren Ma,Jiantao Qiu,Chao Xu,Pei Chu,Kaiwen Liu,Pengli Ren,Yuan Qu,Jiahui Peng,Linfeng Hou,Mengjie Liu,Lindong Lu,Wenchang Ning,Jia Yu,Rui Min,Jin Shi,Haojiong Chen,Peng Zhang,Wenjian Zhang,Qian Jiang,Zengjie Hu,Guoqiang Yang,Zhenxiang Li,Fukai Shang,Zhongying Tu,Wentao Zhang,Dahua Lin,Conghui He*

Main category: cs.CL

TL;DR: 论文提出了一种新的HTML内容抽取方法MinerU-HTML，通过语义建模显著提升了结构化元素（如代码、公式、表格）的保留率，证明了高质量抽取对大模型下游任务的重要影响，并发布了相关数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 目前网页数据主要着重过滤和去重，而HTML转文本的抽取仅作为固定预处理，现有基于启发式的工具难以保留复杂结构。作者认为提升抽取质量对于下游模型效果十分关键，因此希望改善内容抽取方式。

Method: 将HTML内容抽取视为序列标注问题，用一个0.6B参数规模的小语言模型代替启发式规则，先按语义进行结构划分再转成Markdown格式。整个流程支持可扩展的模型改进，同时公布了自己的数据集MainWebBench和下游语料AICC。

Result: 在MainWebBench基准上，MinerU-HTML达到了81.8%的ROUGE-N F1（Trafilatura仅为63.6%），代码块和公式的保留率分别高达90.9%、94.0%。用该方法抽取的AICC语料进行预训练，下游13项标准测试准确率比传统方法提升1.08个百分点，并优于RefinedWeb等语料。

Conclusion: HTML内容抽取质提升对下游大模型能力有决定性作用，是Web语料建设不可忽视的环节。仅靠启发式方法已难进一步提升，基于模型的方法为开放语料领域带来大幅跃迁。

Abstract: While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.

</details>


### [126] [Classification of worldwide news articles by perceived quality, 2018-2024](https://arxiv.org/abs/2511.16416)
*Connor McElroy,Thiago E. A. de Oliveira,Chris Brogly*

Main category: cs.CL

TL;DR: 本研究评估了多种传统机器学习和深度学习模型在区分高低感知质量新闻报道方面的有效性。结果显示，深度学习模型特别是ModernBERT-large表现最佳。


<details>
  <summary>Details</summary>
Motivation: 新闻质量参差不齐，自动区分高低质量新闻有助于减少虚假信息传播和提升内容推荐系统的效果。作者希望确定现有机器学习与深度学习方法是否能有效完成该任务。

Method: 作者构建了包含141万条英语新闻的全新数据集，将来源网站根据专家评分分为高低质量，通过194个语言特征表征每篇文章，采用3种传统机器学习（如随机森林）和3种深度学习模型（如ModernBERT、DistilBERT）进行实验对比。

Result: 传统机器学习如随机森林表现良好（准确率0.7355，ROC-AUC 0.8131），但深度学习模型（如ModernBERT-large）表现显著更优（准确率0.8744，ROC-AUC 0.9593）。不同BERT模型和参数设置下结果有所差异，但整体深度学习优于传统模型。

Conclusion: 无论采用传统机器学习还是深度学习方法，均能有效区分高低感知质量新闻，且深度学习模型表现尤为突出，对内容筛选、信息过滤等领域具有参考价值。

Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.

</details>


### [127] [ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports](https://arxiv.org/abs/2511.16438)
*Sherine George,Nithish Saji*

Main category: cs.CL

TL;DR: 本文提出了ESGBench，这是一个用于评估企业可持续发展报告问答系统的基准数据集和评测框架，旨在推动ESG领域可解释AI的发展。


<details>
  <summary>Details</summary>
Motivation: 可持续发展和ESG披露在企业治理中日益重要，但现有AI问答系统缺乏透明性和可解释性，因此需要一个专门的评测基准来推动该领域研究。

Method: 作者构建了一个覆盖多个ESG主题的问题集，并配有人类标注的答案和证据，能够支持细粒度的模型推理评估。同时分析了当前先进大模型在该数据集上的表现。

Result: 实验表明，现有大模型在事实一致性、可溯源性和领域对齐等方面存在显著不足，ESGBench能有效揭示这些挑战。

Conclusion: ESGBench为开发透明且负责任的ESG人工智能系统提供了重要工具，有助于加速相关领域可信AI的研究进展。

Abstract: We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.

</details>


### [128] [Anatomy of an Idiom: Tracing Non-Compositionality in Language Models](https://arxiv.org/abs/2511.16467)
*Andrew Gomes*

Main category: cs.CL

TL;DR: 本文提出使用一种新颖的电路发现与分析方法，揭示了transformer模型处理成语时的特殊计算模式，包括发现‘Idiom Heads’和‘增强接受’现象。结果展示transformer如何应对非组合性语言并兼顾效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 成语等非组合性语言的处理一直是自然语言处理领域的难题。本文旨在揭示transformer类模型内部是如何处理这些特殊语言现象的，提升对模型机制的理解。

Method: 采用改进的路径修补算法(path patching)进行电路发现，定位出与成语处理高度相关的注意力头(Idiom Heads)以及‘增强接受’现象，即成语各词之间更强关联性，并系统分析这些机制。

Result: 发现transformer在处理成语时，会有特定的注意力头被频繁激活，并且成语内部token之间的注意力增强。分析指出这些是模型在非组合性语言下实现效率和鲁棒性的机制。

Conclusion: 论文揭示transformer可通过特殊注意力分布与电路结构，有效处理成语等复杂句法现象。这对于解释模型如何理解复杂、非直接推理的语言现象提供了新路径，也有助于未来研究更复杂语法结构的处理机制。

Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.

</details>


### [129] [Arctic-Extract Technical Report](https://arxiv.org/abs/2511.16470)
*Mateusz Chiliński,Julita Ołtusek,Wojciech Jaśkowski*

Main category: cs.CL

TL;DR: Arctic-Extract 是一个高效的结构化数据抽取模型，适用于资源有限的硬件，在文档理解方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着业务文档的电子化和多样化，如何高效从扫描件或原生数字文档中提取结构化数据（如问答、实体、表格）成为一个重要问题，尤其是在资源有限的设备上部署高性能模型需求提升。

Method: 提出了一种体积仅6.6 GiB的深度学习模型 Arctic-Extract，结合有效的训练协议和模型设计，使其能够在如A10 GPU等有限显存的硬件上高效处理多达125页A4文档。

Result: 实验表明，Arctic-Extract 在文档结构化数据抽取任务中表现出色，支持对长文档的处理，且符合资源受限场景部署需求。

Conclusion: Arctic-Extract兼具高效、轻量、易部署等优点，为在资源受限环境下的业务文档结构化处理提供了优异的解决方案。

Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.

</details>


### [130] [TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval](https://arxiv.org/abs/2511.16528)
*Özay Ezerceli,Mahmoud El Hussieni,Selva Taş,Reyhan Bayraktar,Fatma Betül Terzioğlu,Yusuf Çelebi,Yağız Asker*

Main category: cs.CL

TL;DR: 该论文提出了TurkColBERT，这是一个针对土耳其语信息检索（IR）系统，比较密集型编码器和后期交互模型的全新基准。研究发现，体积更小的后期交互模型在保持较高检索性能的同时，大大提升了参数效率与响应速度。


<details>
  <summary>Details</summary>
Motivation: 神经信息检索系统在高资源语言上表现出色，但对形态复杂且低资源的语言如土耳其语的研究有限。当前土耳其语IR大多采用密集双编码器，尚未对后期交互模型（如ColBERT）做系统性评估。缺乏基准和模型阻碍了土耳其语IR技术进步。

Method: 提出了两阶段自适应流程，先对英/多语种编码器在土耳其NLI/STS任务上微调，再转化为ColBERT风格的后期交互检索器。利用PyLate工具在MS MARCO-TR上训练，并基于五个土耳其BEIR数据集对10种模型进行全面评测。

Result: 后期交互模型（如colbert-hash-nano-tr）参数量远小于密集编码器（如turkish-e5-large），但在mAP指标上能保持71%以上性能。更小参数量的ColmmBERT-base-TR在特定领域任务上mAP提升最大达13.8%。MUVERA+Rerank索引算法检索速度提升3.33倍，且mAP增加1.7%。ColmmBERT-base-TR在MUVERA架构下查询延迟仅0.54毫秒。

Conclusion: 后期交互模型在土耳其语检索中展现出极好的参数效率、性能与延迟优势，优于传统密集编码器。相关模型与工具公开，便于社区复现。现有局限包括依赖中等规模及翻译数据集，实际应用仍需更大规模评估。

Abstract: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.

</details>


### [131] [Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks](https://arxiv.org/abs/2511.16540)
*Éloïse Benito-Rodriguez,Einar Urdshals,Jasmina Nasufi,Nicky Pochinkov*

Main category: cs.CL

TL;DR: 本文探讨了能否仅依据大模型内部激活（而非输出文本）预测提示词的文本体裁，首次实现了这一方向近乎可行的验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）难以解释且其输出难以全人工评估，因此开发能从模型内部机制直接理解其行为的工具成为研究热点。体裁作为提示词核心特性，能否通过LLM激活模式预测，为解释性理解打开了新思路。

Method: 作者基于Mistral-7B和两个数据集，收集模型对不同体裁文本提示的中间激活表征，用scikit-learn中的浅层分类器进行体裁预测，并设有对照实验。

Result: 在两个数据集上，准确率表现分别高达98%和71%，且均优于对照任务，说明体裁在LLM激活空间可以被有效捕获。

Conclusion: 研究证明了即便使用浅层学习模型，也能根据LLM内部激活有效区分文本体裁，为模型可解释性和行为预测工具的开发提供了新方向。

Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.

</details>


### [132] [WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544)
*Zachary Ellis,Jared Joselowitz,Yash Deo,Yajie He,Anna Kalygina,Aisling Higham,Mana Rahimzadeh,Yan Jia,Ibrahim Habli,Ernest Lim*

Main category: cs.CL

TL;DR: 本文发现通用ASR评价指标（如WER）在临床领域并不能有效反映实际临床风险，因此提出利用LLM模拟专家评估，实现更贴合实际的ASR临床安全性评价。


<details>
  <summary>Details</summary>
Motivation: 随着自动语音识别（ASR）在医疗领域（如医生与病人对话）广泛应用，现有的评估标准——例如词错误率（WER）——可能不足以衡量ASR误差对临床实际的影响，因此需要更好反映临床安全风险的评价标准。

Method: 作者组织专家对话医生与病人真实对话和ASR转录结果，标记其中错误对临床的影响（无、轻微、重大）；分析常见指标与人工标注风险的一致性；提出并优化一个大模型自动评审框架（LLM-as-a-Judge），并用GEPA方法来进行参数优化，使其效仿专家的评价标准。

Result: 结果显示，无论是WER还是其它现有指标，都与专家的临床风险评判一致性较差。而优化过的LLM评审者（Gemini-2.5-Pro）则能够以90%的准确率、Cohen’s κ为0.816，达到接近人类专家的表现。

Conclusion: 该研究为临床ASR评估提供了自动化、可扩展且更安全相关的评价方法，有助于推动ASR系统在医疗场景下更好落地。

Abstract: As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.

</details>


### [133] [Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation](https://arxiv.org/abs/2511.16577)
*Kexin Zhao,Ken Forbus*

Main category: cs.CL

TL;DR: 该论文提出了一种无需人工标注数据的新方法，利用统计语言模型作为歧义消解的“判官”，有效地改进了丰富语义表示的自动消歧能力。


<details>
  <summary>Details</summary>
Motivation: 现有词义消歧方法多针对粗粒度表示且依赖人工标注数据。而对于需要复杂推理的丰富知识库（如OpenCyc），这类方法难以自动化，大幅限制了NLP系统的表现与应用范围。

Method: 作者提出将符号自然语言理解系统生成的多个候选词义转换为可区分的自然语言表达，再利用大型语言模型根据上下文选择合适的含义，最后将结果反馈到符号系统中，全程无需人工数据标注。

Result: 通过与人工标注的黄金答案进行对比，实验验证了所提出方法的有效性，即在不依赖人工标注训练数据的前提下也能获得良好的消歧性能。

Conclusion: 该方法能够自动、高效地对细粒度丰富语义表示进行消歧，减少人工成本，提升基于符号的NLP系统的推理能力，有望拓展复杂知识库的实际应用范围。

Abstract: Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.

</details>


### [134] [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](https://arxiv.org/abs/2511.16654)
*Elias Lumer,Alex Cardenas,Matt Melich,Myles Mason,Sara Dieter,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文对多模态RAG系统中两种检索方法进行对比分析，证明直接多模态嵌入检索在金融文档问答任务中显著优于将图片转文本的方式。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态RAG系统为了处理图片信息，倾向于用大模型生成图片摘要转化为文本再做知识检索，但这种方法在预处理阶段丢失了图片中的关键信息，影响后续的检索和问答质量。

Method: 提出并实验对比两种检索方式：1）将图片转为文本摘要再嵌入做文本检索；2）利用多模态嵌入模型直接将图片与文本内容共同存入向量数据库，通过多模态检索直接利用图片原始信息。利用新构建的金融财报基准数据集（40组问答，每组含1图片1文本），基于6种大语言模型及2种多模态嵌入模型全面评测检索及问答表现。

Result: 实验表明，多模态嵌入直接检索方法在mAP@5和nDCG@5指标上，分别比文本摘要嵌入提升了绝对13%和11%（相对提升32%和20%）。且通过模型判分，直接多模态检索生成的答案事实性和准确性更高。

Conclusion: 多模态RAG系统中，直接多模态嵌入检索能更好保留和利用视觉信息，显著提升文图混合场景的问答准确性，优于传统的图片转文本摘要检索方案。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.

</details>


### [135] [Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs](https://arxiv.org/abs/2511.16664)
*Ali Taghibakhshi,Sharath Turuvekere Sreenivas,Saurav Muralidharan,Ruisi Cai,Marcin Chochowski,Ameya Sunil Mahabaleshwarkar,Yoshi Suhara,Oluwatobi Olabiyi,Daniel Korzekwa,Mostofa Patwary,Mohammad Shoeybi,Jan Kautz,Bryan Catanzaro,Ashwath Aithal,Nima Tajbakhsh,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本论文提出了Nemotron Elastic框架，实现了在单一大型语言模型中嵌套多种不同规模的子模型，这些子模型可以零样本（zero-shot）提取并直接部署，无需额外训练，极大降低多模型家族训练成本。


<details>
  <summary>Details</summary>
Motivation: 多规模和多配置需求的LLM通常需分别训练，耗费巨额资源。虽有剪枝和知识蒸馏等压缩技术，但每压缩一个模型依然成本高昂。亟需新方法能同时高效训练适配多部署场景的模型。

Method: 该方法通过权重共享，将多个子模型嵌套于父模型中，并使用端到端训练的路由器模块结合两阶段针对推理任务的训练策略。同时，提出分组感知SSM弹性化、异构MLP弹性化、归一化MSE层重要性判别和多预算知识蒸馏等技术，共同优化多配置模型性能。

Result: Nemotron Elastic应用于Nemotron Nano V2 12B的实验中，同时生成效果优异的9B与6B子模型，仅需110B训练tokens，训练成本相比从零开始分别下降360倍和比最先进的模型压缩技术下降7倍，各子模型推理准确率优于现有压缩SOTA。

Conclusion: Nemotron Elastic允许在模型部署时以多合一形式灵活调用不同规模子模型，并大幅压缩所需资源，为多需求、低成本LLM部署提供了创新解决方案，超越现有压缩方法。

Abstract: Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [136] [Gimballed Rotor Mechanism for Omnidirectional Quadrotors](https://arxiv.org/abs/2511.15909)
*J. Cristobal,A. Z. Zain Aldeen,M. Izadi,R. Faieghi*

Main category: cs.RO

TL;DR: 本文提出了一种万向节转子机构作为模块化高效的全向四旋翼解决方案，实现了无需大幅结构修改即可获得全自由度六维独立控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有全向四旋翼通常需要复杂且重量大的结构改造，且传统四旋翼本质上为欠驱动，难以实现六自由度独立运动。因此需要一种简单易集成的全向四旋翼设计。

Method: 作者提出在四旋翼的每个转子平台上集成舵机，实现每个转子独立倾斜调节，无需大改机体结构。同时，针对该非传统结构，作者开发了新的控制分配算法并集成进PX4自动驾驶仪进行飞行控制。

Result: 通过实际飞行测试，验证了所提设计和控制系统的有效性，四旋翼能够实现全向运动和精确操控。

Conclusion: 新型万向节转子方案可在保持结构轻量和模块化的基础上，实现四旋翼全方位驱动与控制，具备广泛应用前景。

Abstract: This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.

</details>


### [137] [I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration](https://arxiv.org/abs/2511.15914)
*Debasmita Ghose,Oz Gitelson,Ryan Jin,Grace Abawe,Marynel Vazquez,Brian Scassellati*

Main category: cs.RO

TL;DR: 该论文提出了一种机器人在任务中主动检测并适应人类动态变更目标的新方法，显著提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中人类常在协作过程中动态变更目标，而现有方法多假定目标固定，难以适应实际需求。

Method: 该方法通过跟踪多个候选动作序列并用策略集合进行可行性验证，检测人类目标变更。在检测到变更后，机器人更新对相关历史动作的信念，并构建递推式视野规划（RHP）树，主动选择既能协助人类又能区分新目标的动作。

Result: 在包含最多30个独特菜谱的协作烹饪环境中进行评测，提出的方法在目标切换后能快速收敛到正确目标，优于其他三种主流人类目标预测算法，表现为完成任务所需时间更短、协作效率更高。

Conclusion: 该方法有效提升了机器人对人类动态目标的识别与适应能力，有助于实现更高效的人机协作。

Abstract: For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.

</details>


### [138] [The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces](https://arxiv.org/abs/2511.15956)
*Aliyah Smith,Monroe Kennedy*

Main category: cs.RO

TL;DR: 本研究探讨了机器人在日常环境下通过声音与人类互动的方式，尤其关注功能性和结果性声音及空间定位声音对人类感知和行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在生活中的普及，如何通过声音有效与人沟通成为关键问题。现有研究多关注视觉与语言，声音设计尚未被充分利用，因此有必要探究声音（自然噪音和设计提示音）的影响。

Method: 作者以Kinova Gen3机械臂为实验对象，分析了其结果性和功能性声音对人类的影响，并通过空间定位与接手任务，系统测试了空间声音对感知和行为的作用。

Result: 结果发现，Kinova Gen3的结果性噪音未对用户感知造成负面影响；空间声音在横向定位上极为准确，但在前方定位表现较差；空间声音既能传达任务信息，也能提升温暖感、降低不适。

Conclusion: 功能性和创新的听觉设计可提升人机协作体验，未来应更多应用声音设计优化人机交互策略。

Abstract: As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.

</details>


### [139] [PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization](https://arxiv.org/abs/2511.15995)
*Zili Tang,Ying Zhang,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展且高效的多机器人协作推送任意形状物体的优化方法，适用于复杂且存在障碍物的环境，并通过仿真和实际实验验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，许多机器人没有夹爪，许多物体也不适合抓取方式操作（如大箱子、圆柱体），推动成为一种重要的非抓取操作方式。然而，目前的研究通常假设推动方式和物体形状固定，无法应对实际环境中对象形状多样、协同要求高、障碍复杂等挑战。

Method: 基于组合混合优化的方法，动态分配任务、优化推动模式和执行切换，方案主要包括三部分：（1）将复杂推动任务分解、排序，再分配给不同机器人子组；（2）通过“关键帧”引导的混合搜索优化每个子任务的推动方式序列；（3）通过混合控制稳定执行及切换各推动方式，并采用基于扩散模型的加速器预测关键帧与优先推动模式，提高规划效率。

Result: 所提框架在仿真和真实硬件实验中进行了广泛验证，能够高效地适应不同数量机器人、任意形状物体的协作推动任务，表现出优良的效率与通用性，并可推广至异构机器人、平面装配以及6自由度推动等任务。

Conclusion: 本方法为多机器人协作非抓取推动提供了高效泛化的系统性解决方案，能有效适应现实复杂环境中的推送任务，提升了多机器人系统的智能化水平。

Abstract: Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.

</details>


### [140] [Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud](https://arxiv.org/abs/2511.16048)
*Qing Zhang,Jing Huang,Mingyang Xu,Jun Rekimoto*

Main category: cs.RO

TL;DR: 本文提出了一种有意采用“低保真”（lo-fi）方式的软体飞行机器人艺术装置，该机器人结合了数字考古的物理外观，与仅基于大语言模型的语义导航，探讨“不完美”机器人伴侣的创造潜力。


<details>
  <summary>Details</summary>
Motivation: 主流机器人追求高精度和完美性能，鲜有关注“有缺陷”或“低保真”的伙伴型机器人。本研究旨在发掘低精度和富有个性的机器人如何在艺术和人机互动中展现独特价值。

Method: 研究团队设计了一款3D像素风格云朵形态的软体机器人，完全摒弃LiDAR、SLAM等传统传感器，仅依赖多模态大语言模型进行语义导航和人格塑造。通过自然语言提示赋予机器人生物拟态的个性，从而形成叙事性“心智”。随后通过自主飞行日志分析和统计验证，评估不同性格设置下的表现。

Result: 实验揭示了该框架下的自主导航表现具有明显“性格”特征，包括基于地标的导航、执行计划的偏差等，并表现出不可预期但合理的行为。统计分析进一步确认了人格定制方法的稳健性及显著差异。

Conclusion: 本文证明了一种以性格和不完美为核心评价标准的低保真机器人设计方法，展示了“有缺陷的伙伴型”机器人在艺术与互动体验上的创新潜力，为机器人设计带来新的可能性。

Abstract: While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.

</details>


### [141] [Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers](https://arxiv.org/abs/2511.16050)
*Takeru Tsunoori,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: 提出了Bi-AQUA，这是首个结合仿射学习与照明感知视觉处理的水下机器人操作框架，在不同光照条件下显著提升了控制与操作的稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前水下机器人受到极端光照变化、颜色失真、能见度降低等影响，导致操作任务表现不佳，缺乏能自适应光照变化的水下仿射学习方法。

Method: Bi-AQUA框架引入了三级照明自适应机制：1）Lighting Encoder用于从RGB图像中提取光照特征且无需人工标签，并通过模仿学习目标隐式监督；2）FiLM调制对视觉主干特征进行自适应增强，实现光照感知特征提取；3）将显式的lighting token加入Transformer编码器输入，实现任务相关的自适应。

Result: 在真实水下抓取和放置任务，以及多种静态与动态光照环境下，Bi-AQUA表现出强鲁棒性，且明显优于未建模光照的双边基线方法。消融实验显示所有光照自适应组件都至关重要。

Conclusion: 该方法打通了陆地仿射学习与水下操作的桥梁，在复杂环境下实现了对力敏感的自主水下操作。

Abstract: Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua

</details>


### [142] [MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics](https://arxiv.org/abs/2511.16158)
*Lara Bergmann,Cedric Grothues,Klaus Neumann*

Main category: cs.RO

TL;DR: 该论文提出了MagBotSim，一个用于磁悬浮系统的物理仿真平台，旨在支持群体磁机器人在制造自动化中的高效运输与操作。


<details>
  <summary>Details</summary>
Motivation: 目前磁悬浮技术可实现灵活的工业物料运输，但其未被充分挖掘用于‘运输+操作’的智能群体制造环境。缺乏相应智能算法开发和仿真工具，限制了其应用潜力。

Method: 作者提出将磁悬浮系统视作机器人群体（MagBots），并开发了MagBotSim仿真环境，实现对大型自主移动装置（shuttles/movers）的物理级仿真与算法测试。

Result: MagBotSim为磁悬浮系统的智能算法研究与开发提供了统一的仿真平台，相关文档及代码开放，支持视频与实验。

Conclusion: 此工作为基于磁悬浮的群体机器人制造系统奠定了基础，有望极大提升工业自动化的效率、灵活性与空间利用率。

Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/

</details>


### [143] [PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks](https://arxiv.org/abs/2511.16200)
*Kewei Chen,Yayu Long,Mingsheng Shang*

Main category: cs.RO

TL;DR: 针对多机器人系统中高带宽数据通讯带来的“共享大脑困境”，提出了PIPHEN框架，通过语义蒸馏将高维感知数据压缩为结构化表达，实现高效协作和控制。


<details>
  <summary>Details</summary>
Motivation: 多机器人合作任务需要传递大量高维感知数据（如视频流），但带宽受限和决策时延高，成为系统效率提升的瓶颈。

Method: 提出PIPHEN框架，核心包括：1）基于模型蒸馏的物理交互预测网络（PIPN），在边缘侧将原始数据转为紧凑语义表达；2）基于能量守恒的哈密顿网络控制器（HEN），将语义表达精确转换为协作动作。

Result: 相比传统方法，PIPHEN将信息表达压缩到原始数据的5%以下，将决策延迟从315ms降低到76ms，同时提升任务成功率。

Conclusion: PIPHEN为带宽受限多机器人系统中的高效感知与协作提供了新范式，有效缓解了“共享大脑困境”。

Abstract: Multi-robot systems in complex physical collaborations face a "shared brain dilemma": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace "raw data communication" with "semantic communication" by performing "semantic distillation" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the "shared brain dilemma" in resource-constrained multi-robot systems.

</details>


### [144] [DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks](https://arxiv.org/abs/2511.16223)
*Vincenzo Pomponi,Paolo Franceschi,Stefano Baraldo,Loris Roveda,Oliver Avram,Luca Maria Gambardella,Anna Valente*

Main category: cs.RO

TL;DR: DynaMimicGen (D-MG) 是一个用于机器人操作策略学习的数据集生成框架，只需极少人工演示，即可为动态环境中的任务生成多样化、高质量数据，显著减少人工数据采集成本。


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作策略学习依赖于大量多样的数据集，人工采集过程耗时、费力且难以覆盖动态环境，限制了实际应用。该文旨在解决数据集采集效率与泛化能力之间的矛盾。

Method: D-MG框架通过极少量人类示范，先对任务进行分段，并利用动态运动基元（DMP）对示范行为进行适应和泛化，动态生成能够实时响应环境变化的平滑、合理任务轨迹，适用于多种场景和动态变化的操作任务。

Result: 基于D-MG生成的数据，机器人通过模仿学习在长时序、高接触任务（如立方体堆叠、杯子进抽屉等）上表现优秀，面对环境变化时依然能保持强鲁棒性和任务完成率。

Conclusion: D-MG显著降低了对大量人工演示的需求，实现了高效、可扩展的数据自动生成，并提升了机器人在动态环境中的泛化能力，为自动化机器人学习开辟了新方向。

Abstract: Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.

</details>


### [145] [FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models](https://arxiv.org/abs/2511.16233)
*Kewei Chen,Yayu Long,Shuai Li,Mingsheng Shang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的数据蒸馏框架FT-NCFM，通过高价值样本的生成，显著提升了视觉-语言-动作（VLA）模型的训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型对大规模且冗余数据集过度依赖，影响其实用性。现有的模型压缩或策略蒸馏方法要么损失性能，要么无法脱离特定模型，并未从根本上解决数据高效性问题。

Method: 提出了FT-NCFM数据蒸馏框架，包括一个Fact-Tracing引擎，结合因果归因和程序式对比验证，评价样本本身价值。依据样本评估结果，利用对抗NCFM过程生成模型无关且信息密集的新数据资产，用于训练。

Result: 在多个主流VLA基准测试上，使用仅占原始数据5%的蒸馏数据训练模型，即可达到85-90%的原始成功率，同时训练时间减少80%以上。

Conclusion: 智能化的数据蒸馏可显著提升VLA模型效率和性能，是实现高效VLA模型的重要新路径。

Abstract: The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

</details>


### [146] [How Robot Dogs See the Unseeable](https://arxiv.org/abs/2511.16262)
*Oliver Bimber,Karl Dietrich von Ellenrieder,Michael Haller,Rakesh John Amala Arokia Nathan,Gianni Lunardi,Marco Camurri,Mohamed Youssef,Santos Miguel Orozco Soto,Jeremy E. Niven*

Main category: cs.RO

TL;DR: 本论文提出了一种受动物启发的机器人视觉方法——peering（探视性侧移），通过合成孔径成像，有效应对场景中的部分遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 在传统机器人视觉系统中，由于相机光圈较小和景深较大，前景遮挡物和背景信息常常同时清晰可见，造成遮挡物严重阻碍对场景关键信息的获取。这一问题在复杂、混杂环境下对机器人感知与决策造成了巨大挑战。因此亟需一种既高效又能适应多种光谱条件的遮挡抑制新方法。

Method: 本文借鉴动物探视运动原理，通过让机器人做左右探视移动，利用合成孔径成像技术采集多帧图像并进行计算整合，生成极浅景深的合成图像，实现对前景遮挡物的虚化并清晰还原背景信息。该方法无需依赖特征匹配或主动传感器，计算高效，理论上可快速部署于任意移动机器人。

Result: 实验证明，该方法不仅能够恢复传统视觉无法获取的基本场景信息，还能辅助大模型实现更高级的视觉推理。此外，其性能优于传统多视图三维视觉和LiDAR等主动传感手段，对遮挡具有更高的鲁棒性和适用性。

Conclusion: 该研究建立了动物行为与机器人合成孔径感知的联系，表明通过peering运动实现的合成孔径成像是解决复杂环境下遮挡问题、提升机器人场景理解能力的关键手段。

Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.

</details>


### [147] [Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist](https://arxiv.org/abs/2511.16265)
*Haru Fukatsu,Ryoji Yasuda,Yuki Funabora,Shinji Doki*

Main category: cs.RO

TL;DR: 本文提出了一款名为Funabot-Upper的可穿戴触觉服，可让用户感受14种上半身运动，包括躯干、肩部、肘部和手腕的动作。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴触觉设备多只关注于单个身体部位，且多点位刺激时会出现感知混合问题，因此需要一种能在多个部位独立诱发本体感觉的方案。

Method: 基于人工肌肉三维变形技术，设计了一种新触觉服，可分别刺激多个关节和肌肉，独立诱发本体感觉。通过实验测试了刺激与感知之间的关系，并与前代产品进行了对比。

Result: 新方案显著减少了感知混合，识别准确率从68.8%提高到94.6%。证明了新设计在多关节感知诱发上的有效性与优越性。

Conclusion: Funabot-Upper能有效独立诱发多关节本体感知，优于过往设计，具有良好的未来触觉应用前景。

Abstract: This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.

</details>


### [148] [InEKFormer: A Hybrid State Estimator for Humanoid Robots](https://arxiv.org/abs/2511.16306)
*Lasse Hohmeyer,Mihaela Popescu,Ivan Bergonzani,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 该论文提出了一种结合扩展卡尔曼滤波（InEKF）和Transformer网络的混合状态估计算法InEKFormer，用于提升人形机器人动态行走时的状态估计精度。实验在RH5机器人数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人需要在复杂环境下稳定行走，对状态估计精度与反馈速度要求很高；传统方法如卡尔曼滤波对噪声参数依赖较强，调优困难。深度学习的新进展激发了将其引入状态估计任务的需求。

Method: 提出InEKFormer，融合了不变扩展卡尔曼滤波（InEKF）与Transformer神经网络，利用Transformer强大的特征建模能力提升状态估计精度，并与传统InEKF及KalmanNet方法进行对比实验。

Result: 实验结果表明，Transformer在高维人形机器人状态估计中具备巨大潜力，但在高维自回归建模中仍需要提升训练的鲁棒性。

Conclusion: InEKFormer能够提升人形机器人状态估计的精度，为机器人动态行走等应用奠定基础，但未来需进一步加强对高维时序的自回归建模能力。

Abstract: Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.

</details>


### [149] [Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning](https://arxiv.org/abs/2511.16330)
*Shreyas Kumar,Ravi Prakash*

Main category: cs.RO

TL;DR: 本文提出了一种强化学习新方法C-GMS，实现了DMP和可变阻抗控制的安全、稳定学习，适用于机器人复杂交互任务，并在仿真和真实机器人上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的DMP+VIC方法存在探索不稳定、容易产生危险行为等安全隐患，亟需一种能保证稳定性和可实现性的学习框架。

Method: 提出Certified Gaussian Manifold Sampling（C-GMS）框架，将策略探索过程限制在一个由数学定义的、所有增益调度都保证稳定性的流形上，通过设计该流形确保每一次策略采样都稳定且可实现。该方法无需传统的奖励惩罚或后验验证，并能在存在一定模型误差和部署不确定性时保证有界追踪误差。

Result: 在仿真和真实机器人实验中，C-GMS展现了其在复杂环境下安全可靠地自主交互的能力，证实该理论和方法的有效性。

Conclusion: C-GMS为强化学习下机器人DMP+VIC的安全高效策略学习提供了理论基础和实际手段，为机器人在复杂场景中的可靠自主交互奠定了基础。

Abstract: Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

</details>


### [150] [Flow-Aided Flight Through Dynamic Clutters From Point To Motion](https://arxiv.org/abs/2511.16372)
*Bowen Xu,Zexuan Yan,Minghao Lu,Xiyu Fan,Yi Luo,Youshen Lin,Zhiqiang Chen,Yeke Chen,Qiyuan Qiao,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种基于单一LiDAR传感的强化学习方法，实现无人机在动态杂乱环境下的高效避障飞行，无需传统的目标检测与跟踪，直接从激光点云到运动决策，显著提升了自主飞行的成功率与适应性。


<details>
  <summary>Details</summary>
Motivation: 传统应对动态障碍的路径规划方法依赖于显式建模障碍物运动，计算量大且在高动态和有遮挡场景下效果并不理想。本文动机在于设计无需目标检测、跟踪和预测的轻量级方案，提升避障效率及应用的实际可行性。

Method: 方法上，利用单一LiDAR，通过点云编码生成固定形状、低分辨率但细节安全的距离图。引入环境变化感知的点流作为运动特征，用于捕获动态环境信息，并与距离图融合，形成简练、易于学习的环境表示。策略训练中，通过相对运动调制的距离场优化避障决策，采用无动力学建模的加速度控制。

Result: 所提出系统在仿真与真实四旋翼飞行任务中展现出了优于现有方法的成功率和适应性，能够安全实现动态场景下的自主飞行。

Conclusion: 本文证明了无须复杂检测与跟踪，仅凭单LiDAR感知和巧妙策略设计，即可高效实现动态避障，具有显著的实际部署与推广潜力。

Abstract: Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.

</details>


### [151] [Robot Metacognition: Decision Making with Confidence for Tool Invention](https://arxiv.org/abs/2511.16390)
*Ajith Anil Meera,Poppy Collis,Polina Arbuzova,Abián Torres,Paul F Kinghorn,Ricardo Sanz,Pablo Lanillos*

Main category: cs.RO

TL;DR: 该论文提出了一种以“信心”为核心的机器人元认知架构，从神经科学获得灵感，提升机器人自主决策与适应能力，重点应用于自动化工具发明任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人通常缺乏对自身认知过程与决策进行反思的能力，而人类的元认知（自我监控）在学习、决策、问题解决中至关重要。研究动机在于为机器人赋予类似人类的元认知能力，提升其智能与适应性。

Method: 作者提出以“信心”作为机器人的元认知度量，并将其整合进机器人决策流程中，使其能够对自身决策的可靠性做出二阶判断。通过借鉴神经科学概念，搭建元认知体系，并在自动化工具发明任务中进行了验证。

Result: 机器人通过引入基于信心的元认知能力，能够在实际物理部署中更好地评估自身行为的可靠性，提高了鲁棒性和决策质量。

Conclusion: 以信心为基础的机器人元认知有助于机器人更好地监控和调整自身行为，提升自主性与智能性。该方法具备实际应用潜力，为机器人元认知的进一步研究指明了方向。

Abstract: Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.

</details>


### [152] [Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators](https://arxiv.org/abs/2511.16406)
*Luis Luna,Isaac Chairez,Andrey Polyakov*

Main category: cs.RO

TL;DR: 提出了一种同类齐次PID（hPID）控制策略，用于提升移动机器人机械臂（MRM）的鲁棒性和协调运动控制，实验显示优于传统PID控制。


<details>
  <summary>Details</summary>
Motivation: 移动机器人机械臂由于非线性、欠驱动和底盘与机械臂耦合，控制难度大。现有经典PID难以应对动态不确定性和外部扰动，亟需更高效、鲁棒的控制方法。

Method: 基于齐次控制理论设计hPID控制器，将传统PID增益推广为非线性、状态依赖函数。通过分级齐次方式增强跟踪误差收敛性，并用Lyapunov方法进行稳定性分析，证明闭环系统的全局渐近稳定与有限时间收敛。

Result: 在典型MRM模型上开展实验，hPID在运动基座和机械臂的轨迹跟踪方面都实现了更快响应、更小稳态误差和更强抗不确定性，优于线性PID控制器。

Conclusion: 该方法为下一代移动操作系统提供了可扩展、理论扎实的控制框架，在结构化和非结构化环境下均可提升自主性与可靠性。

Abstract: Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.

</details>


### [153] [LAOF: Robust Latent Action Learning with Optical Flow Constraints](https://arxiv.org/abs/2511.16407)
*Xizhou Bu,Jiexi Lyu,Fulei Sun,Ruichen Yang,Zhiqiang Ma,Wei Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于光流约束的鲁棒潜在动作学习方法（LAOF），能够有效抑制与动作无关的干扰因素，并在极少动作标签的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视频潜在动作预训练方法容易受到与动作无关信息的干扰，而动作标注数据稀缺限制了有监督方法的应用，因此需要一种在动作标签少甚至无标签下依然鲁棒的动作特征学习框架。光流天然突出运动物体、抑制背景干扰，从而启发作者用光流作为伪监督信号来辅助学习鲁棒潜在动作特征。

Method: 提出LAOF（Latent Action learning with Optical Flow constraints）框架，以agent的光流作为引导信号，对潜在动作特征加以约束，实现对干扰项的鲁棒学习。该方法在极少或无动作标签的情况下，利用光流信号提升动作特征的质量和稳定性，并可灵活融合有限的动作标签以进一步优化表现。

Result: 实验显示，LAOF方法在下游模仿学习和强化学习任务中，学到的潜在动作表示明显优于已有方法。在动作标签占比极低（甚至无标签）的情况下，也能达到或超越使用1%动作标签的有监督方法，并保持对标签比例变化的鲁棒性和优越性。

Conclusion: LAOF通过引入光流约束，有效增强了动作特征学习的鲁棒性和泛化性，突破了动作标注数据稀缺的瓶颈，为可扩展的实体基础模型预训练提供了新的思路和方法。

Abstract: Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.

</details>


### [154] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: 提出了一种新的3D模型生成框架SEG，通过在生成阶段直接优化结构，使3D打印时所需支撑材料大幅减少。


<details>
  <summary>Details</summary>
Motivation: 目前的3D打印切片技术多关注在生成模型后优化支撑结构，而不是在模型生成阶段就减少对支撑的需求。这样导致了材料浪费和生产效率低下。

Method: SEG将直接偏好优化（DPO）和Offset技术整合到3D生成流程中，在训练阶段模拟支撑结构，以激励模型生成天然更容易打印、支撑需求更少的几何体。

Result: 在Thingi10k-Val和GPT-3DP-Val两个数据集上，SEG相较于TRELLIS、DPO和DRO等方法显著减少了支撑体积，同时保持输入提示的高保真度。

Conclusion: SEG能在生成阶段直接优化模型，明显降低打印所需支撑，实现高效、可持续的数字制造，对3D打印领域具有重要意义。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>


### [155] [MiMo-Embodied: X-Embodied Foundation Model Technical Report](https://arxiv.org/abs/2511.16518)
*Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue,Shuhao Gu,Hao Tian,Yuannan Shen,Jianwei Cui,Wen Zhang,Shaoqing Xu,Bing Wang,Haiyang Sun,Zeyu Zhu,Yuncheng Jiang,Zibin Guo,Chuhong Gong,Chaofan Zhang,Wenbo Ding,Kun Ma,Guang Chen,Rui Cai,Diyun Xiang,Heng Qu,Fuli Luo,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: MiMo-Embodied是首个覆盖自动驾驶和具身智能的跨领域基础大模型，表现优异并创下多项基准纪录，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶和具身智能（Embodied AI）研究大多各自为政，缺少统一的基础大模型来融合并提升两大领域的表现。因此，作者希望通过构建一个能跨越这两个领域的模型，实现能力互补和性能提升。

Method: 提出MiMo-Embodied模型，通过多阶段学习、精心设计的数据构建流程，以及利用Chain-of-Thought（CoT）推理与强化学习（RL）微调，实现自动驾驶与具身智能任务的知识共享与正迁移。

Result: MiMo-Embodied在17个具身智能和12个自动驾驶权威基准测试上均大幅优于公开、闭源及专用模型，在任务规划、可供性预测、空间理解、环境感知、状态预测和驾驶规划等任务上取得新纪录。

Conclusion: 跨领域大模型的设计及多样性训练方法可以实现自动驾驶与具身智能领域的能力互补和大幅性能提升，为后续研究提供了可复现开源平台。

Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.

</details>


### [156] [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/abs/2511.16651)
*Yang Tian,Yuyin Yang,Yiman Xie,Zetao Cai,Xu Shi,Ning Gao,Hangxu Liu,Xuekun Jiang,Zherui Qiu,Feng Yuan,Yaping Li,Ping Wang,Junhao Cai,Jia Zeng,Hao Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文首次证明，仅利用大规模合成数据可以在预训练视觉-语言-动作（VLA）模型时达到最强真实机器人数据集的效果，并实现强大的零样本仿真到真实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作发现真实数据对VLA模型泛化很有帮助，但大规模合成数据尚未展现同等效益。论文希望探究仅用合成数据是否可以等效甚至替代真实数据进行大模型预训练。

Method: 作者构建了包含63万+条轨迹、4种体现、18种技能、70项任务、227个场景的大规模合成数据集InternData-A1，通过高度自动化与解耦的仿真数据生成管线得到。再采用与现有π_0机器人模型相同的网络架构，仅用合成数据进行预训练，并评估其任务表现。

Result: 模型在49项仿真任务、5项真实世界任务和4项复杂灵巧操纵任务上，与使用真实数据集π_0训练的模型表现相当，且展现显著的零样本仿真到真实迁移能力。

Conclusion: 大规模高质量合成数据可独立支持VLA大型模型的有效预训练，效果可与真实数据相媲美。数据集及生成管线的开源将促进机器人智能体领域的数据获取和算法研究。

Abstract: Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.

</details>


### [157] [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/abs/2511.16661)
*Irmak Guzey,Haozhi Qi,Julen Urain,Changhao Wang,Jessica Yin,Krishna Bodduluri,Mike Lambeta,Lerrel Pinto,Akshara Rai,Jitendra Malik,Tingfan Wu,Akash Sharma,Homanga Bharadhwaj*

Main category: cs.RO

TL;DR: 本文提出了一种新的人类到多指机器人政策学习框架AINA，利用可穿戴设备（Aria Gen 2眼镜）采集的人体自然环境下数据，使机器人能无需机器人自身采集数据便习得通用的操控能力。


<details>
  <summary>Details</summary>
Motivation: 目前多指机器人要学习日常任务，往往需要大量机器人自身的数据采集和训练，过程繁琐且不易泛化。若能直接从人类日常自然任务的视频数据学习，可大幅降低门槛和泛化能力。不过，人与机器人的身体结构差异和自然环境下的视频信息提取均是主要障碍。

Method: 作者提出利用配备高分辨率RGB和3D头手姿态检测以及广角立体摄像头的Aria Gen 2眼镜来收集任何人、任何地点、自然环境下的人类操作数据，再通过AINA框架提取3D点云特征并学习对多指手可泛化的操控策略。该方法无需机器人自身采集数据或额外的在线校正、强化学习与仿真。

Result: 本工作在九个日常任务上测试，表明AINA框架能在无机器人数据参与下让机器人习得多任务、多指灵巧操作能力，并且对背景变化具有较强鲁棒性，实验证明优于以往的类人到机器人策略模仿方法。

Conclusion: AINA框架结合高性能人类数据采集硬件和端到端泛化策略学习，推动了从人类自然示范中直接学习机器人多指操控的进步，为实现真正可泛化的机器人操作奠定了基础。

Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

</details>
