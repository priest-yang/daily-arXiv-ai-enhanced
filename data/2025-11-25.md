<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 321]
- [cs.CL](#cs.CL) [Total: 77]
- [cs.RO](#cs.RO) [Total: 65]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

TL;DR: 本文提出了一种利用AI模型根据人体正面图片和基础人体测量数据（身高、体重、脖围、踝围、腕围）估算体脂率的方法，具有较高准确率，为大众提供低成本的体脂检测方案。


<details>
  <summary>Details</summary>
Motivation: 目前体脂率的金标准检测方法如DEXA扫描价格昂贵、难以普及。为满足大众对健康管理的需求，作者希望探索AI在普通条件下提供准确、廉价体脂估算的可行性。

Method: 研究自建了一个535例的数据集：包括253例有详尽人体测量数据的样本，以及282例自Reddit爬取、带有自报（有些为DEXA检测）体脂率及正面全身图的样本。采用两种方法：一是基于ResNet的人像图像模型，二是基于回归的人体测量模型。还提出了未来可用配对数据扩展的多模态融合模型框架。

Result: 基于图片的AI模型在体脂率预测上取得了4.44%的RMSE、R^2为0.807，显示出较高的准确性。

Conclusion: AI辅助的体脂率估算方法可作为现有高成本检测方法的低价替代方案，有望应用于消费者端的健康与健身领域。

Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [2] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

TL;DR: 提出了一种多模态自编码器（MMAE），可统一处理和理解文本、音频和视觉数据，实现内容元数据提取与语义聚类的自动化，验证了其在多模态基准集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统通常只处理单一模态（如视频、音频或文本），难以理解广播媒体中不同模态间的复杂关联，这限制了广播内容管理的自动化和智能化水平。

Method: 设计并提出了MMAE模型，可以学习文本、音频和视觉等多模态统一表征。MMAE在真实代表性多模态数据集LUMA上训练，通过最小化多模态的联合重建损失，无须大量成对或对比数据集，发现模态无关的语义结构。

Result: 与线性基线方法相比，MMAE在聚类和对齐等指标（如Silhouette、ARI、NMI）上显著提升，表明基于重建的多模态嵌入适用于可扩展的元数据生成和跨模态检索任务。

Conclusion: 基于重建的多模态学习方法有助于提升广播内容的自动化处理、检索能力和管理效率，显示出在现代广播工作流中强大的应用潜力。

Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [3] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 作者发布了一个覆盖25年、每天分辨率的野火数据集，涵盖2.4亿公顷区域，以及38种影响因素，并基于该数据集评估了多种时间序列预测模型，推进了野火风险预测领域的数据基础和方法研究。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险预测任务受限于复杂驱动因素（如燃料、气象、地形、人的活动）交互，且缺乏支持长时间序列和大规模空间覆盖的公开基准数据集，导致研究进展受阻。

Method: 作者收集并整理了覆盖不列颠哥伦比亚及周边2.4亿公顷区域、为期25年（每日分辨率）的野火相关数据，包含38种变量（如活跃火点检测、气象、燃料、地形和人为因素），并用此数据集评估多种时间序列预测模型（包括CNN、线性模型、Transformer和Mamba架构），还分析了位置编码和不同驱动因子的作用。

Result: 通过基准评测，各种时间序列模型在该数据集上性能有详细比较，作者还报告了位置编码和各种驱动因素对模型预测效果的重要性。

Conclusion: 该工作为野火风险预测研究提供了高质量的数据集和基线模型，并为后续数据驱动的方法创新和领域进一步发展奠定了基础。数据和代码已经开源。

Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [4] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

TL;DR: 本文分析了多模态大模型（如Gemini-1.5-pro）在处理含有旋转和透视畸变的文档图像进行OCR数据提取时的表现，发现畸变对结构识别准确率影响显著，并且简单的旋转校正可提升这一任务表现。


<details>
  <summary>Details</summary>
Motivation: 现实生活中的文档图像不仅存在旋转，还常有透视畸变，但当前多模态大模型在此类复杂失真场景下的OCR能力和表现并未被系统研究。了解这些影响对推动多模态LLM在实际OCR应用中的落地具有重要意义。

Method: 作者观察并总结了文档常见的透视失真，发现它们可以简化为等腰梯形变换（从8个参数降至2个关键参数：旋转角和畸变比）。通过合成样本文档，分别控制这两个参数，提取特定实体并评估字符识别与结构识别（订单正确性）准确率。

Result: 实验发现：透视畸变极大降低了结构识别的准确率（文本的阅读顺序正确性），而简单的字符识别受影响较小。此外，简单的旋转校正方法可显著提升模型的结构识别准确率。

Conclusion: 多模态大模型在遭遇透视和旋转畸变的文档OCR任务中，结构识别准确率影响较大。结合简单旋转矫正措施，可较好提升多模态大模型的实用性，对实际部署具有指导意义。

Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [5] [3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF](https://arxiv.org/abs/2511.17609)
*Linh Van Ma,Unse Fatima,Tepy Sokun Chriv,Haroon Imran,Moongu Jeon*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，利用Unscented Kalman Filter（UKF）融合多摄像头2D标注，生成高精度的3D地面真实数据。方法具备自动化、可扩展性强、能输出完整3D物体形状等特点，并在多个公开数据集上验证了其高精度。


<details>
  <summary>Details</summary>
Motivation: 在自动导航、监控与机器人领域，精准的3D地面真实数据非常关键。目前很多方法只能提供2D或简单的地面平面信息，难以获得完整的3D物体形状和位置。因此，亟需一种能够从简单2D标注自动生成高质量3D信息的方法。

Method: 作者提出利用UKF，结合多摄像头的2D边框或关键点标注，通过单目标多摄像机跟踪，将2D图像坐标经单应投影与UKF融合为3D世界坐标。该算法能够处理多视角下的数据融合，估计目标的空间位置与完整形状，并有效应对遮挡等难题。

Result: 在CMC、Wildtrack和Panoptic等数据集上，所提方法在3D定位精度方面优于当前对比方法，不仅精准还可重建完整物体3D形状。

Conclusion: 本文方法能够实现全自动、多摄像头下，仅需2D标注即生成高精度、可扩展的3D地面真实数据，突破了现有方法只能提供简单地面信息的限制，推动了3D视觉领域进步。

Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.

</details>


### [6] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

TL;DR: 本文提出了一种全新的全无监督多阶段深度学习框架，用于提升低光照交通图像的视觉质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低光照的交通场景由于照明不均、噪声、运动模糊和光斑等问题，严重影响自动驾驶、智能交通和城市监控系统的感知效果，急需提升低光照图像的可用性。

Method: 模型采用全无监督学习，通过三大模块分阶段处理：1）亮度适应模块全局和局部调整图像亮度；2）反射率复原模块结合空间-通道注意力机制抑制噪声、恢复结构细节；3）过曝补偿模块重建饱和区域，均衡整体亮度。训练采用自监督重建、反射平滑感知一致性和领域正则损失，无需真实成对训练数据。

Result: 在通用和交通领域数据集上，此方法在PSNR、SSIM、LPIPS、NIQE等量化指标和视觉质量上全面超越现有先进算法。增强后图像更清晰、结构保留更完整，下游感知任务表现更佳。

Conclusion: 所提出的多阶段无监督增强框架能有效提升低光照交通图像的可见性和结构保留，助力实际交通应用中自动感知系统的可靠性。

Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [7] [HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2511.17614)
*Danyang Sun,Fadi Dornaika,Nagore Barrena*

Main category: cs.CV

TL;DR: 该论文提出了HSMix，一种用于医学图像分割的全新局部图像编辑数据增强方法，通过‘硬混合’和‘软混合’提升分割性能，在多种医学数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割领域由于标注成本高或疾病稀有，常面临数据稀缺与过拟合问题。自监督和半监督学习可部分缓解但实现复杂，因此作者希望通过更简单的数据增强方式有效提升分割表现。

Method: 提出HSMix增强方法，首先从两张原图像通过超级像素提取同质区域，进行‘硬混合’组合生成增强图像；随后采用‘软混合’，依据局部像素显著性系数对亮度进行混合调整。分割掩码也同步做同样混合，使标签与增强图像一致。该方法能充分利用轮廓与显著性信息，同时丰富样本多样性。

Result: 在多种医学分割任务和数据上，HSMix方法有效提升了分割性能，并具备模型无关、易用等特点。

Conclusion: HSMix是一种高效、简单、模型无关的数据增强方法，能显著改善医学图像分割中的数据稀缺问题。

Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.

</details>


### [8] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

TL;DR: 本文提出了一种无需调参的新方法（PnP-MIX），能高保真地将多个个性化概念整合到单张图片中，显著提升了多对象场景下文本到图像生成的表现。


<details>
  <summary>Details</summary>
Motivation: 现有T2I方法在复杂多对象场景下表现欠佳，常常导致个性化和非个性化区域出现意外改动，损害了生成图片的语义一致性。如何在不影响非个性化区域的情况下，实现多概念的高精度融合，是一个亟待解决的问题。

Method: PnP-MIX方法主要包括：1）引导外观注意力以精确反映每个个性化概念；2）基于掩模的噪声混合策略，保护非个性化区域完整性并精准融合个性化对象；3）提出background dilution++，有效降低概念特征泄漏，提升个性化区域的精确定位。方法无需对基础模型进一步调整或微调。

Result: 大量实验结果显示，PnP-MIX在单概念和多概念个性化任务中都超越了现有方法，展现了无须调参的强鲁棒性和优越性能。

Conclusion: PnP-MIX能高效、无缝地整合多个个性化概念，针对多对象T2I生成难题，显著提高了生成质量和区域语义一致性，是相关任务的新进展。

Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [9] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频问答（VQA）基础性问答生成框架（FIQ），通过从视频中抽取描述性信息并生成Q&A对，提高模型对视频内容的理解和推理能力，并取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法主要依赖事件驱动的问题回答对，缺乏基础的场景描述信息（如对象类别、空间结构、视觉属性），造成模型对环境理解不完整，影响泛化和推理能力。

Method: 提出FIQ框架，从视频直接抽取描述性信息生成Q&A对，丰富训练数据集的基础场景属性。同时，提出VQ-CAlign模块，将任务相关的问题嵌入与视觉特征对齐，增强模型对场景上下文的把握及适应不同下游任务的能力。

Result: 在SUTD-TrafficQA数据集测试下，所提方法显著超越现有基线，达到了最新最优（SOTA）性能。

Conclusion: 通过补充基础性场景属性的Q&A对，并引入嵌入对齐模块，显著提升了VQA模型对复杂视频内容的综合理解与推理能力，对提升VQA模型的泛化性能效果显著。

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [10] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于角点对齐的回归方法来提升LiDAR点云3D目标检测的准确性，解决了传统基于中心回归因稀疏点云导致不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于中心对齐的回归依赖点云中心，但LiDAR点云受前表面偏置影响，导致目标中心常落在稀疏甚至空白的区域，增加了预测误差，因此需要新的更稳定的边界框回归方式。

Method: 作者提出将预测目标从不稳定的中心点迁移到几何上信息更丰富、观测更密集的角点。这种角点回归结合角点的几何约束与2D目标框，使部分3D框参数仅需角点标注即可恢复，从而实现弱监督。并设计了一种易于集成角点感知检测头。

Result: 在KITTI数据集上，所提方法相比中心回归基线提升了3.5%的AP，且仅用BEV角点弱标签即可取得全监督准确度的83%。

Conclusion: 角点对齐回归能显著提高3D目标检测效果，并降低对完整3D标签的依赖，验证了该策略的有效性。

Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [11] [BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?](https://arxiv.org/abs/2511.17633)
*DoYoung Kim,Jin-Seop Lee,Noo-ri Kim,SungJoon Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 提出了一种1.58-bit卷积和前BN残差连接的新方法，解决了BNN中极限量化导致的表达能力下降和训练不稳定问题，实现了深度可分离卷积的成功二值化，显著提升了轻量模型在多个数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统BNN（Binary Neural Networks）虽具备极高效率，但极端量化会降低表达能力并导致训练不稳定，尤其在使用深度可分离卷积的轻量模型中更为突出，因此亟需方法提升其表现和训练稳定性。

Method: 提出1.58-bit卷积以增强BNN的表达能力，并设计了预批归一化（pre-BN）残差连接结构来优化Hessian条件数，从而稳定网络优化。这些方法首次实现深度可分离卷积的成功二值化。

Result: 在ImageNet上基于MobileNet V1实现仅33M OPs，并在各类主流数据集（CIFAR-10/CIFAR-100、STL-10、Tiny ImageNet、Oxford Flowers 102）上取得最高可达9.3个百分点的精度提升，超越同等OPs下的所有现有方法。

Conclusion: 所提方法显著提升了BNN的性能和训练稳定性，并首次实现了BNN中深度可分离卷积的可行性，为后续超高效低比特神经网络设计树立了新标杆。

Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.

</details>


### [12] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的加速score-based扩散模型的框架，通过数值线性代数方法，显著减少了求解线性系统的时间成本，有效提升在资源受限场景下的图像生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（如stable diffusion）在高质量图像生成方面表现优异，但对计算资源消耗极大。每张图片需求解大型线性系统，导致在需要处理大量图片时计算代价过高，因此迫切需要高效的加速方法。

Method: 作者将标准stable diffusion转化为Fokker-Planck形式，将图像生成问题转化为线性系统的求解，并提出了跨矩阵Krylov投影方法。该方法通过“种子”矩阵构建共享子空间，并快速用于“目标”矩阵的求解，从而大幅提升求解速度。

Result: 与常规稀疏求解器相比，该方法在实际实验中实现了15.8%至43.7%的时间减少。在图像去噪任务中相比DDPM基线模型，速度提升最高可达115倍。在固定计算预算下，该模型生成高质量图像，而DDPM无法生成可识别结果。

Conclusion: 提出的方法能广泛提升扩散模型在受限资源下的生成效率和质量，实验证明了其实用性，有望在实际应用中推广。

Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [13] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

TL;DR: 该论文提出了一种名为UPMI的新颖数据增强策略，能有效提升小样本多模态儿童胰腺炎的影像诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 儿童胰腺炎属于渐进性炎症性疾病，诊断难度高，现有机器学习方法受限于样本量和多模态成像数据的复杂性，因此需要创新的处理方式来提升诊断效果。

Method: 作者提出UPMI方法：在低维的meta特征空间（由多模态MRI影像经过特定逻辑回归输出的概率形成的7维向量）进行高斯混合模型采样生成合成样本，然后与真实样本共同训练随机森林分类器。

Result: 在67例配对T1W/T2W MRI的儿童病人数据上，UPMI策略的诊断AUC平均为0.908±0.072，相较于只用真实样本的基线（AUC 0.864±0.061）提升了约5%。

Conclusion: UPMI是一种轻量、高效的数据增强方法，在小样本多模态医学诊断中可提升模型表现，对于儿童胰腺炎等难诊疾病具有应用前景。

Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [14] [TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection](https://arxiv.org/abs/2511.17636)
*Weijun Gao,Rundong He,Jinyang Dong,Yongshun Gong*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于通道可判别性和活跃度的典型集修正方法，并结合偏度修正提升了分布不匹配下的OOD检测性能，在多个基准上均取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前激活基方法在检测OOD时，往往忽视了通道特性和分布偏度，导致对典型集估计不准，进而影响了OOD检测的准确性。本文旨在解决激活通道间的固有特性被忽视带来的检测误差。

Method: 提出了通道感知的典型集修正策略，基于每个通道的可判别性与活跃度调整激活，同时引入偏度修正策略，避免分布偏斜带来的典型集界定不准，最终利用修正后的激活计算能量分数进行OOD检测。

Result: 本文方法在ImageNet-1K和CIFAR-100等主流基准上，超过了现有方法，展现了较强的跨主干网络与分数函数的泛化能力。

Conclusion: 对激活的细粒度通道修正与偏度补偿能有效提升OOD检测的准确性，本文方法具备良好的适应性与扩展性。

Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.

</details>


### [15] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

TL;DR: 本文提出了SWITCH基准，用于测试自主智能体在现有环境和基础设施中的控制与交互能力，特别关注实际场景下的视频感知、语义推理、动作生成和结果验证。


<details>
  <summary>Details</summary>
Motivation: 当前的智能体评测体系多集中在感知和推理，对物理环境中真实可操作接口（如电器、面板、嵌入式GUI等）的交互能力评测不足，且很少关注基于视频的部分可观测性与结果验证。出现交互失误时可能导致安全隐患，因此有必要设立相关基准。

Method: 作者构建了一个名为SWITCH的基准体系，其迭代的首版SWITCH-Basic覆盖5种能力测试：任务感知问答、语义界面定位、动作生成、状态转移预测及结果验证。输入为自我视角RGB视频，覆盖98种真实设备/器具共351项任务，并设计可复现的数据、代码、测试分组。

Result: 商业大模型和多模态模型在SWITCH-Basic上的表现不一致，即使是简单单步交互任务也常常失败。模型普遍过度依赖文本线索，忽视了视觉或视频中的关键信息，高汇总分数可能掩盖细节上的失败。

Conclusion: SWITCH推动了智能体对现实物理世界接口的综合交互、推理和验证能力评测，填补了现有基准的空白。为后续高难度基准和相关训练集的建立提供了基础资源，并鼓励社区持续改进。

Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [16] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 本文提出了一个用于MRI图像脑肿瘤自动分类的端到端深度学习系统，涵盖六种主流网络结构，并实现了高精度和可部署性。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤的MRI自动分类方法存在评估流程不统一、模型可解释性和部署适用性不足的问题，限制了其在临床尤其是低资源环境中的实际应用。

Method: 系统性比较五种ImageNet预训练模型（VGG-16、Inception V3、ResNet-50、Inception-ResNet V2、Xception）与一个自研轻量级CNN（仅1.31M参数），所有流程（预处理、训练、评测、优化策略等）严格统一；采用Grad-CAM与GradientShap分析模型关注区域，提高可解释性；多维指标（IoU、Hausdorff距离、精确率-召回率曲线、混淆矩阵等）全面评估模型性能。

Result: Inception-ResNet V2获得最高测试精度99.53%、精确率/召回率/F1均不低于99.5%。自研轻量CNN测试准确率96.49%，模型比Inception-ResNet V2小100倍，可在边缘设备上375ms推理，适合资源受限场景。

Conclusion: 本工作在模型性能、可解释性与真实临床部署可行性三者之间取得了平衡，为推进脑肿瘤AI筛查工具在多种医疗体系中的实际应用提供了框架和参考。

Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>


### [17] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了一种高效的连续学习方法MedPEFT-CL，有效解决了医学视觉语言分割模型在适应新任务时遗忘旧任务的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言分割模型在适应新解剖结构时，面临灾难性遗忘，需要完全重训练，限制了实际应用。尽管连续学习方法已被研究，但针对医学视觉语言任务的方案仍然不足。

Method: 提出了基于CLIPSeg的双阶段结构，包括自适应学习阶段（利用语义相似性分配adapter并高效微调）和知识整合阶段（通过双向Fisher-memory协调防止遗忘），在新任务学习和旧知识保留之间形成强化循环。

Result: 在多个医学数据集上进行了大量实验，与现有方法相比，显著减少遗忘并保持性能，同时训练参数增量极小。

Conclusion: 该方法能在医学视觉语言场景下，实现高效的连续学习，有效缓解遗忘问题，提高模型临床部署的可行性。

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [18] [Person Recognition in Aerial Surveillance: A Decade Survey](https://arxiv.org/abs/2511.17674)
*Kien Nguyen,Feng Liu,Clinton Fookes,Sridha Sridharan,Xiaoming Liu,Arun Ross*

Main category: cs.CV

TL;DR: 本文对过去十年150多篇有关以人为中心的空中监控任务论文进行了综述，系统分析了利用无人机等空中平台进行人的检测、识别及再识别等方面的研究现状、数据集、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来无人机、空中平台及成像传感器迅速发展，使得大规模、高机动、隐蔽的空中监控成为可能。针对此前以地面监控为主的研究局限，本文动机是梳理和总结当前空中背景下以人为对象的监控技术进展、挑战及改进方法，为研究人员提供全面参考。

Method: 本文综述了近十年150余篇相关论文，将以人为中心的空中监控任务分为检测、识别与再识别，从技术角度系统分析这些任务在空中环境中遇到的独特挑战，并归纳总结了用于每项任务的公开空中数据集；此外，比较并评述了现有文献中的方法如何应对上述挑战并提出优化思路。

Result: 本文揭示了空中平台人类监控任务与地面监控存在明显不同的技术挑战，如尺度变化、视角极端、背景复杂等，总结了当前应对这些挑战的方法和缺陷，同时系统整理了相关公开数据集。通过大量文献分析，总结出各主流算法的优劣和未来可改进之处。

Conclusion: 本文不仅全面梳理了空中平台下人的检测、识别与再识别的研究现状，还指出了目前领域的主要空白与未解决问题，为未来开展相关研究提供了方向和建议。

Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.

</details>


### [19] [Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models](https://arxiv.org/abs/2511.17681)
*Weiyi Lv,Ning Zhang,Hanyang Sun,Haoran Jiang,Kai Zhao,Jing Xiao,Dan Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种用于Referring Multi-Object Tracking (RMOT)的新框架VMRMOT，通过引入运动模态和多模态大语言模型（MLLMs），提升了视觉和参考信息的动态对齐能力，实现了更高性能的多目标追踪。


<details>
  <summary>Details</summary>
Motivation: 现有RMOT方法只能根据静态的目标外观、相对位置和初始运动状态进行描述，无法捕捉目标运动过程中的速度变化和方向转变，导致参考信息与动态视觉信息之间存在时序差异，影响多模态融合跟踪性能。

Method: 本文提出VMRMOT，融合运动模态、视觉模态和语言参考信息。利用MLLMs的时序推理能力，从目标动态行为中生成运动感知描述并提取运动特征。设计了VMRA模块分层对齐视觉、运动和参考信息，并引入MGPH进一步用运动信息增强预测。

Result: 在多个RMOT基准上实验，VMRMOT方法性能优于现有最先进的方法。

Conclusion: VMRMOT首次在RMOT中引入MLLM，解决了动态运动信息与静态参考之间的时序不一致性，大幅提升了多模态融合跟踪准确性。

Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.

</details>


### [20] [Understanding Counting Mechanisms in Large Language and Vision-Language Models](https://arxiv.org/abs/2511.17699)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 论文分析了大型语言模型（LLMs）和视觉-语言模型（LVLMs）在计数任务中，对数值信息的表征和计算方式。


<details>
  <summary>Details</summary>
Motivation: 深入理解大模型如何在不同层次、不同模态下处理和推理数值（特别是计数）信息，并找出其内部的机制。

Method: 利用文本与视觉上重复元素的对照实验，结合因果中介分析和激活修补方法，并提出了专用工具CountScope进行机制可解释性分析。

Result: 模型能在单个token或视觉特征中内隐编码位置计数信息，且该信息可跨场景抽取和迁移。计数表征在模型内部逐层递进产生，有内部计数器机制，主要存储在最终token或region。在LVLM中，数值信息与视觉embedding空间位置相关。模型也利用分隔符等结构性线索辅助计数。

Conclusion: LLM和LVLM中的计数表征呈现有结构、分层次的演变过程，与视觉编码器属性相关，且相关内部机制可迁移并受输入组织结构影响。

Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.

</details>


### [21] [Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions](https://arxiv.org/abs/2511.17722)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型（VLMs）在图像计数任务中对固有偏见的依赖性，并构建了合成数据集和评价框架，系统评测了不同视觉和文本特性变化下的表现。研究发现，通过一定的注意力机制干预，可在一定程度上提升计数性能，但整体而言，VLM计数仍具挑战性，特别是在复杂场景下。


<details>
  <summary>Details</summary>
Motivation: 此前有研究指出，VLMs在处理涉及图像属性的具体问题时，易受训练时习得的偏见影响，尤其是在计数等需要聚焦特定区域的任务中。该文旨在系统性评估和揭示影响VLMs计数性能的视觉与文本因素及其注意力分配机制。

Method: 作者开发了一个合成基准数据集和评价框架，系统改变图像中物体数量、颜色、背景、纹理及文本提示的具体性等参数。基于开源VLM，分析其在不同输入参数下的注意力分配变化，并实施基于注意力的干预以调控模型在不同层级上的视觉关注，评估其对计数性能的影响。

Result: 实验显示，VLM在高复杂度的视觉或语言条件下计数表现依旧不佳；但某些注意力机制调整可以带来计数性能的有限提升。

Conclusion: 尽管针对注意力分布的特殊干预能在部分条件下改善VLM的计数能力，但整体来看，VLM在复杂环境下的计数任务依然具有较大挑战性。

Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.

</details>


### [22] [AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography](https://arxiv.org/abs/2511.17724)
*Mohammad Atwany,Mojtaba Lashgari,Robin P. Choudhury,Vicente Grau,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 本文提出了一种新方法AngioDG，通过通道正则化促进冠状动脉血管分割模型的泛化能力，在6个X光血管造影数据集上取得业界最佳的分布外性能。


<details>
  <summary>Details</summary>
Motivation: 现有血管分割模型在不同数据集上的泛化能力较差，主要因为影像协议和患者人群的差异导致领域转移，且缺乏标注数据使得领域泛化非常困难。

Method: 提出AngioDG方法，通过分析早期特征通道对任务指标的贡献，并对通道加权以增强领域无关的特征，提高模型泛化能力，同时提升可解释性。

Result: 在6个冠状动脉X光血管造影分割数据集上，AngioDG方法在分布外测试中表现优于其它已对比方法，并保持了域内测试性能一致。

Conclusion: AngioDG能够有效缓解领域转移带来的泛化难题，是解决X光血管分割泛化问题的有效方法。

Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.

</details>


### [23] [The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation](https://arxiv.org/abs/2511.17727)
*Victor Li,Naveenraj Kamalakannan,Avinash Parnandi,Heidi Schambra,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 本文探讨了视觉-语言模型（VLMs）在中风康复视频分析中的应用，但现有VLMs在精细动作理解方面仍有明显不足，对具体康复量和障碍衡量的自动化水平有限。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型已在计算机视觉领域表现出色，研究者希望将其应用于数字健康领域，特别是通过视频自动量化中风康复的剂量和障碍，这是数据驱动康复的重要挑战。

Method: 将康复量和障碍评估问题转化为动作识别任务，借助VLMs进行无任务特定训练或微调的实验，并在29名健康对照者和51名中风患者的数据上进行框架评估。

Result: 当前VLMs难以实现康复剂量和障碍的精确量化，估算结果与不使用视觉信息的基线模型相当，障碍分数无法可靠预测。但经过优化提示和后处理，VLMs可有效分类高级活动，适度检测动作和抓握，并在部分对象中粗略估算康复次数。

Conclusion: VLMs现阶段在中风康复视频分析中存在局限，但也展现出无需专门训练即可分析临床视频的潜力，未来有望通过进一步优化在数字健康领域发挥更大作用。

Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.

</details>


### [24] [VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.17731)
*Lingxiao Li,Yifan Wang,Xinyan Gao,Chen Tang,Xiangyu Yue,Chenyu You*

Main category: cs.CV

TL;DR: 本文提出了VisReason，一个大规模视觉链式思维（CoT）推理数据集，显著提升了多模态大语言模型的视觉推理与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维提示在语言模型中已展现推理能力，但在多模态大语言模型（MLLMs）中的应用受限，主要由于缺乏具有人类分步推理结构的大规模视觉数据集。

Method: 作者构建了VisReason数据集，包含48.9万条带有人类多轮推理标注的多领域视觉推理样本，以及VisReason-Pro子集（16.5万条，含更强专家模型详细标注和3D空间信息）。并在Qwen2.5-VL上进行微调实验。

Result: 在VisReason和VisReason-Pro上微调后的模型在视觉推理准确率、可解释性和跨基准任务泛化性上均取得明显提升。

Conclusion: VisReason为多模态大模型赋予更系统且具广泛适用性的视觉推理能力，未来可作为推动人类化视觉推理和多模态智能发展的关键资源。

Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.

</details>


### [25] [Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders](https://arxiv.org/abs/2511.17735)
*Samuel Stevens,Jacob Beattie,Tanya Berger-Wolf,Yu Su*

Main category: cs.CV

TL;DR: 本文提出利用稀疏自编码器（SAE）对科学领域的基础模型进行特征无监督发现的方法，并通过生态影像数据实验验证其能够在无需标注的情况下发现细粒度结构。结果显示SAE有助于揭示未被预设标签覆盖的新模式，有助于科学发现。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型的内部表示蕴含诸多结构和模式，但现有方法多数只针对预设目标提取特征，适用于验证已知假设，缺乏开放式、面向未知模式的特征发现能力。科学数据中蕴藏尚未了解的新规律，因此需一种自动揭示数据结构的方法。

Method: 作者将稀疏自编码器应用于已有基础模型（如视觉模型）的表征层，通过无监督方式在不依赖标签的情况下挖掘潜在特征。在受控重发现实验中，作者评估SAE学到的特征与标准分割基准中的语义概念对齐情况，并与其他无标签方法进行比较。此外，通过生态影像案例，展示无标注情况下发现解剖结构的能力。

Result: SAE学到的特征在与概念对齐的评估指标上优于或媲美其他无监督方法，并在生态影像中成功发现细粒度的结构，且无需依赖分割或部件标签，实际揭示了科学基础模型中所蕴含的表示结构。

Conclusion: 稀疏分解（SAE）为科学基础模型的知识探索提供了实用工具，可在不依赖人工标签的前提下推进从确认已知到发现未知的科学研究，对蛋白质、基因组、气候等多领域模型具有通用性和推广潜力。

Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.

</details>


### [26] [AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations](https://arxiv.org/abs/2511.17747)
*Dawid Wolkiewicz,Anastasiya Pechko,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 本文提出了AEGIS框架，实现了对3D高斯人脸头像的身份隐私保护，在不损失感知质量和头像功能完整性的前提下，彻底去身份化。


<details>
  <summary>Details</summary>
Motivation: 随着拟真3D人脸头像的广泛应用，尤其是基于高效3D高斯球渲染的技术，网络身份盗用风险增加，特别是在依赖生物认证的系统中。目前针对2D图片的对抗掩码较为成熟，但对动态3D头像的鲁棒、视角一致的身份保护还存在重大技术空缺。

Method: 提出AEGIS框架，通过在3D高斯头像的颜色系数中加入对抗扰动（由预训练的人脸验证网络引导），实现多视角一致的身份隐私保护，无需重新训练模型或修改头像几何信息。

Result: AEGIS框架可将人脸检索、验证准确率降至0%，实现完全去身份化，同时保持高感知质量（SSIM为0.9555，PSNR为35.52dB），且关键面部属性如年龄、种族、性别、情感基本不变。

Conclusion: AEGIS首次实现了对3D高斯头像的强隐私保护，兼顾多视角一致性与高感知保真度，保护个人身份信息，具备良好的实用价值与推广潜力。

Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.

</details>


### [27] [SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration](https://arxiv.org/abs/2511.17750)
*Zhimin Shao,Abhay Yadav,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: 本论文提出了SPIDER，一个适用于不同场景的通用图像特征匹配框架，能够针对大尺度、不同视角和跨领域环境实现更高效和精准的匹配，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图像特征匹配是视觉空间感知的基础，但在不同领域（如航拍、室内外）受外观、尺度、视角的巨大变化影响，传统特征匹配方法效果有限，尤其在大视角变化和细致几何结构检测方面存在不足。

Method: 作者首先通过linear probe实验分析多种视觉基础模型用于图像匹配的表现；接着提出SPIDER框架，融合共享特征提取主干网络，并设计两个专门的子头，分别处理基于2D和3D的粗到细特征对应关系；最后构建了专注大基线和无约束场景的图像匹配评测基准。

Result: SPIDER在作者提出的评测基准和多场景下均显著优于当前最优方法（SoTA），表现出强大的通用性和鲁棒性，无论是对主平面还是几何细节都具备优异匹配能力。

Conclusion: SPIDER框架兼顾2D与3D信息，可为各种场景下的视觉空间感知提供通用、鲁棒和更精准的图像匹配方案，有望成为未来视觉任务的基础方法。

Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.

</details>


### [28] [CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation](https://arxiv.org/abs/2511.17755)
*Prantik Howlader,Hoang Nguyen-Canh,Srijan Das,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了CORA框架，通过结合少量标注数据和大量未标注图像，实现对复杂指令的推理分割任务，并在低标注条件下取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理分割方法在处理复杂、隐式指令时对像素级标注和丰富的语言指令配对要求高，导致泛化性差且在数据分布变化时表现不稳定。高质量注释的获取成本也严重限制了这些方法的实际应用。

Method: CORA框架包含三大模块：1) 使用条件视觉指令编码目标间的空间和语境关系；2) 基于多模态大模型对于语义等价查询输出一致性的伪标签噪声筛选器；3) 标注样本和伪标注样本之间的token级对比对齐，以提升特征一致性。该方法联合有标注和无标注数据训练。

Result: 在Cityscapes和PanNuke两大数据集上，CORA只需极少的标注样本（如Cityscapes上仅100张标注图像），即分别比基线提升了2.3%和2.4%的分割性能，且达到当前最优水平。

Conclusion: CORA实现了在极低标注条件下复杂指令推理分割的高性能，较现有方法在多数据集上均有显著提升，展示了鲁棒性和泛化能力强的优势。

Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.

</details>


### [29] [Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers](https://arxiv.org/abs/2511.17757)
*Giancarlo Giannetti,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 本文提出了一种新型的高光谱图像解混模型LDVAE-T（Latent Dirichlet Transformer Variational Autoencoder），结合Transformer的全局建模能力与Dirichlet先验的物理约束，在多个基准数据集上取得优越表现。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像可以实现像素级的材料识别，但实际中由不同物质混合导致的光谱混合问题会掩盖纯净材料的光谱特征。解决如何准确解混、还原各物质的比例和性质，是高光谱分析中的核心挑战。

Method: LDVAE-T将Transformer用于编码端以获取全局特征，再引入Dirichlet先验限制以保证丰度分布满足物理约束（非负且和为1）。在解码端，将材料作为端元集合，针对每个端元和图像块同时预测均值谱和可解释的段内协方差结构，从而表征材料内部的光谱变化。最终丰度分布由Transformer编码器与Dirichlet分布结合得出，有效提升了解混的准确性和物理解释性。

Result: 在Samson、Jasper Ridge和HYDICE Urban三个公开数据集上，LDVAE-T在丰度估计和端元提取任务上，分别以均方根误差（RMSE）和光谱角距离（SAD）两个指标超越了现有主流方法。

Conclusion: LDVAE-T模型利用Transformer与Dirichlet先验的互补优势，能更好地解决高光谱图像解混中的物理约束与材料变异问题，具备较强的实用性和先进性。

Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.

</details>


### [30] [Deepfake Geography: Detecting AI-Generated Satellite Images](https://arxiv.org/abs/2511.17766)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: 本文比较了CNN与ViT在检测AI生成卫星影像中的表现，发现ViT在准确率和鲁棒性上均优于CNN，并通过可解释性方法验证模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成模型如StyleGAN2和Stable Diffusion的进步威胁到卫星影像的真实性，这对科研和安全等领域的决策产生影响。目前针对卫星影像的AI检测相关研究较少，亟需高效方法来自动区分真伪。

Method: 作者构建了包含超过13万张标注卫星影像的数据库，比较了卷积神经网络（CNN）和视觉Transformer（ViT）两类主流架构在检测AI生成影像方面的表现，并结合Grad-CAM（针对CNN）和Chefer's attention attribution（针对ViT）开展模型可解释性分析。

Result: ViT在检测AI生成卫星影像中表现更佳，准确率达95.11%，高于CNN的87.02%。ViT尤其在识别结构不一致性和重复纹理等合成痕迹方面展现出优势。

Conclusion: ViT优于CNN，适合用于检测AI生成卫星影像。未来将对多光谱、SAR影像与频域分析拓展研究，以增强高风险场景下的图像真实性保障能力。

Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.

</details>


### [31] [Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/abs/2511.17792)
*Dingrui Wang,Hongyuan Ye,Zhihao Liang,Zhexiao Sun,Zhaowei Lu,Yuchen Zhang,Yuyu Zhao,Yuan Gao,Marvin Seegert,Finn Schäfer,Haotong Qin,Wei Li,Luigi Palmieri,Felix Jahncke,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: 本文提出了Target-Bench数据集，用于首次系统评估世界模型在无地图、语义目标下机器人的路径规划能力。实验揭示了当前主流世界模型在机器人规划方面的不足，并通过微调显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管世界模型在合成逼真视频方面已取得显著进步，但其在机器人路径规划领域的应用与能力尚未被量化和系统评估。缺乏专门的基准和数据集阻碍了相关研究的发展。

Method: 作者构建了Target-Bench数据集，包含450段真实环境下机器收集的视频与45类语义目标及精准轨迹。提出一套评估流程，从模型生成视频中恢复相机轨迹，并基于五项指标系统评估路径规划能力。并对主流模型进行了对比实验。

Result: 现有表现最佳的模型Wan2.2-Flash的综合得分仅为0.299，表现有限。开源5B参数模型经仅325个场景微调后，相较其原始版本得分提升逾4倍（0.066→0.345），也优于所有即用型模型。

Conclusion: 当前世界模型在机器人路径规划任务上还有大量提升空间。为推动该领域发展，作者将开放数据集和代码，促进后续研究。

Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.

</details>


### [32] [Attention Guided Alignment in Efficient Vision-Language Models](https://arxiv.org/abs/2511.17793)
*Shweta Mahajan,Hoang Le,Hyojin Park,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文分析高效视觉-语言模型（VLMs）的注意力模式，发现常用的拼接架构难以有效区分语义匹配与不匹配的图文对，导致物体幻觉问题。提出了AGE-VLM，通过插入交叉注意力层并结合空间知识，有效减轻这一问题，并在多个基准上表现优越或相当。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs往往依赖拼接式视觉编码器和大语言模型来实现多模态对齐，但拼接架构缺乏足够的细粒度视觉-语言融合能力，导致模型在复杂视觉推理任务中出现幻觉（即错误描述或想象不存在的物体）。改进对齐机制和视觉信息利用方式，提升模型在视觉理解和描述的可靠性，有着迫切的研究必要性。

Method: 提出一种新型架构——AGE-VLM，它在预训练的小型语言模型中插入交错的交叉注意力层，使模型能够通过关注正确的图像区域进行有效的视觉定位。该方法利用Segment Anything Model (SAM)提取的空间知识对注意力分布进行引导，从而增强模型的视觉对齐能力。

Result: 实验证明，AGE-VLM在多个以视觉为中心的基准任务上，其表现优于或可与现有高效VLMs媲美，且显著降低了幻觉现象的发生率。

Conclusion: AGE-VLM有效提升了模型对视觉信号的理解能力，缓解了物体幻觉问题，为未来多模态理解模型的设计与优化提供了重要参考。

Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.

</details>


### [33] [Pillar-0: A New Frontier for Radiology Foundation Models](https://arxiv.org/abs/2511.17803)
*Kumar Krishna Agrawal,Longchao Liu,Long Lian,Michael Nercessian,Natalia Harguindeguy,Yufu Wu,Peter Mikhael,Gigin Lin,Lecia V. Sequist,Florian Fintelmann,Trevor Darrell,Yutong Bai,Maggie Chung,Adam Yala*

Main category: cs.CV

TL;DR: 这篇论文介绍了Pillar-0，一个针对放射科医学影像的基础模型，在大量腹盆腔CT、胸部CT、头部CT和乳腺MRI数据集上预训练，并提出了RATE标注框架，显著提升了影像诊断的准确率和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 现代放射科工作量激增，成像数量远超专业人员增长速度，现有AI模型在数据处理、信息保真和临床评测等方面存在重大不足，需要开发更先进且接近真实临床需求的医学影像基础模型。

Method: 提出Pillar-0基础模型，利用来自大学医学中心的逾15万份CT和MRI数据进行预训练；同时开发了基于大语言模型的RATE标注框架，能高效提取366类结构化影像诊断标签；对多个内部和外部数据集进行对比实验，评估其多任务表现。

Result: Pillar-0在腹部、胸部、头部CT及乳腺MRI领域，分别获得86.4、88.0、90.1和82.9的平均AUROC，较主流模型高出7.8-15.8分，87.2%的任务拿到第一名；外部验证同样领先所有基线；在肺癌预测等延展任务及脑出血检测中，亦展现出更高的效率和准确率。

Conclusion: Pillar-0和RATE建立了高性能、开源且具临床可用性的放射科AI基础平台，极大突破了此前影像AI的算力、数据和评测壁垒，为构建新一代放射学AI系统奠定了坚实基础。

Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.

</details>


### [34] [A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking](https://arxiv.org/abs/2511.17805)
*Chengan Che,Chao Wang,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: 提出了一种新的自监督学习方法PL-Stitch，通过利用视频帧的时间顺序，提升了程序性活动（如烹饪、手术）的视频理解能力，并在多个基准任务上取得显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法虽然在静态图片和短视频片段上表现优良，但在处理具有明确顺序和流程的视频（如手术、烹饪）时，缺乏对‘程序性’顺序的建模能力。实验表明，现有模型在处理正序与逆序视频时特征差别很小，说明其对顺序信息不敏感。

Method: 提出PL-Stitch自监督框架，利用Plackett-Luce（PL）模型设计了两个目标：（1）主目标让模型学会按时间顺序对帧排序，学习全局工作流程的进展；（2）辅目标是时空拼图损失，捕捉帧间的细粒度对象关联。这样可以逼迫模型理解和建模过程的先后顺序和内部细节。

Result: PL-Stitch在五个手术及烹饪数据集上均取得领先表现。具体如手术分期识别任务中，Cholec80数据集的k-NN准确率提升了11.4个百分点；烹饪动作分割任务Breakfast数据集线性探测准确率提升5.7个百分点。

Conclusion: PL-Stitch显著增强了程序性视频的表示学习能力，模型能更好理解和利用时序顺序，为相关视频分析任务带来大幅性能提升。

Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.

</details>


### [35] [REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion](https://arxiv.org/abs/2511.17806)
*Ryoma Yataka,Pu Perry Wang,Petros Boufounos,Ryuhei Takahashi*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视角室内雷达感知方法REXO，提升了雷达多视图三维目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视角室内雷达目标检测方法隐式进行特征关联，易导致特征匹配不明确，复杂场景下检测性能下降。作者希望解决此交叉视图关联中的信息不清问题。

Method: 提出REXO方法，将DiffusionDet的二维边界框扩展到三维雷达空间，通过噪声3D边界框引导明确的跨视图雷达特征关联，并利用人物与地面接触的先验知识简化扩散参数。

Result: 在HIBER数据集上提升4.22 AP，在MMVR数据集上提升11.02 AP，超过现有方法。

Conclusion: REXO显著提升了多视角室内雷达目标检测的准确性，验证了其在实际室内复杂场景下的有效性。

Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.

</details>


### [36] [Importance-Weighted Non-IID Sampling for Flow Matching Models](https://arxiv.org/abs/2511.17812)
*Xinshuang Liu,Runfa Blark Li,Shaoxiu Wei,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种针对流动匹配模型（flow-matching models）的非独立同分布（non-IID）重要性加权采样方法，以在采样预算受限时更有效地估计期望。方法实现多样性采样同时保证无偏估计，并提出基于score的正则机制与残差速度场，获得了更准确的期望估计。


<details>
  <summary>Details</summary>
Motivation: 流动匹配模型可有效表示复杂分布，但在有限采样预算下估计输出函数的期望仍然困难，特别是当稀有但高影响样本决定期望时，独立采样方差较大，因此需要改进采样方法。

Method: 提出一种非IID的联合采样方法，通过重要性加权来保证无偏估计，并设计基于score函数（概率对数梯度）的多样性正则，用于提升采样多样性且防止脱离数据流形。进一步地，提出用学习到的残差速度场实现非IID样本的重加权。

Result: 实验证明，该方法获得了多样且高质量的样本，重要性权重和期望的估计也更准确。

Conclusion: 本工作提升了流动匹配模型输出的可靠性与样本多样性，为分布式生成建模带来了新的采样与估计框架，代码将在GitHub公开。

Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.

</details>


### [37] [QAL: A Loss for Recall Precision Balance in 3D Reconstruction](https://arxiv.org/abs/2511.17824)
*Pranay Meshram,Yash Turkar,Kartikeya Singh,Praveen Raj Masilamani,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: 本文提出了一种新的损失函数Quality-Aware Loss (QAL)，用于提升三维视觉任务中点云覆盖率和质量，优于常用的Chamfer Distance（CD）及Earth Mover's Distance（EMD）。


<details>
  <summary>Details</summary>
Motivation: 现有三维视觉任务常用的评价/训练方法（CD或EMD）难以兼顾‘回忆率’与‘精确率’，导致细结构或稀疏区域重建效果不佳。提出新方法QAL以解决这一短板。

Method: QAL由两个部分组成：加权覆盖的最近邻损失和未覆盖真实点的吸引项，分别对应回忆率和精确率，并可调节两者比重。无需大幅修改网络结构即可替代传统损失。

Result: 在不同3D生成和补全管线中，QAL比CD在点覆盖率指标上平均提高4.3分，比最优对比方法也提升2.8分。能显著改善细结构与低频区域的还原，泛化性和稳定性经过多个数据集和参数消融实验验证。在机器人抓取GraspNet评测中，QAL的补全结果取得更高抓取评分。

Conclusion: QAL为3D视觉及机器人等安全敏感应用提供了更稳健、可解释且实用的优化目标，改善了传统损失忽视结构细节和稀疏区域的问题。

Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines

</details>


### [38] [Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations](https://arxiv.org/abs/2511.17828)
*Guilherme J. Cavalcante,José Gabriel A. Moreira,Gabriel A. B. do Nascimento,Vincent Dong,Alex Nguyen,Thaís G. do Rêgo,Yuri Malheiros,Telmo M. Silva Filho,Carla R. Zeballos Torrez,James C. Gee,Anne Marie McCarthy,Andrew D. A. Maidment,Bruno Barufaldi*

Main category: cs.CV

TL;DR: 本研究利用BiomedCLIP基础模型处理乳腺成像中的BI-RADS乳腺密度自动分类，比较单一与多模态训练方法，展示了在大规模数据集上的优异泛化能力和良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像领域潜力巨大，尤其是在尚未广泛探索的乳腺成像任务中。现有模型在泛化能力和处理多种成像方式时存在局限，因此需要验证基础模型在乳腺图像辅助诊断中的性能和适用性。

Method: 本研究将BiomedCLIP基础模型针对乳腺密度自动分级任务进行调整，利用96,995张不同模态的乳腺影像（合成2D、数字乳腺摄影、数字乳腺断层成像），比较单一模态与多模态训练，采用加权对比学习方法应对类别不平衡问题，并使用GradCAM进行可视化解释。

Result: 单一模态与多模态模型准确率相当（多模态0.74，单一模态0.73），多模态模型在不同成像方式中表现更为通用，AUC始终大于0.84。外部验证（RSNA及EMBED数据集）显示优异泛化能力（AUC 0.80-0.93）。GradCAM结果证实模型关注区域与临床相关，具较强可解释性和鲁棒性。

Conclusion: 研究证明基础模型可有效应用于乳腺成像多模态自动分类，具有良好泛化、准确性及可解释性，为后续乳腺疾病诊断等应用拓展了基础。

Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.

</details>


### [39] [Show Me: Unifying Instructional Image and Video Generation with Diffusion Models](https://arxiv.org/abs/2511.17839)
*Yujiang Pu,Zhanbo Huang,Vishnu Boddeti,Yu Kong*

Main category: cs.CV

TL;DR: 本文提出了ShowMe，一个通过统一激活视频扩散模型的空间和时间组件，融合图像操作与视频预测双任务的生成框架，实现更一致和真实的视觉任务生成。


<details>
  <summary>Details</summary>
Motivation: 以往针对上下文视觉指令生成的研究，将文本引导的图像操作与视频预测这两类任务分开处理，导致模型各自有局限：图像操作方法忽视了动作的时间展开过程，视频预测模型则忽略了目标结果。因此作者希望通过一个统一的框架，同时解决这两个问题。

Method: 提出了ShowMe框架，通过在视频扩散模型中有选择地激活空间和时间组件，同时引入结构和运动一致性奖励，提升结构保真度和时间连贯性。此外通过视频预训练获得空间认知能力，结合指令引导的编辑提升面向目标的视频预测能力。

Result: 在多个主流基准测试上，ShowMe在指令性图像和视频生成任务上均优于已有专家模型。

Conclusion: 视频扩散模型在兼顾图像操作和视频预测任务时，具有作为统一行动-对象状态转换器的显著优势。

Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.

</details>


### [40] [JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception](https://arxiv.org/abs/2511.17843)
*Chenyi Wang,Zhaowei Li,Ming F. Li,Wujie Wen*

Main category: cs.CV

TL;DR: 该论文提出JigsawComm框架，通过联合语义特征编码和通信策略实现多智能体协同感知，在有限带宽下，有效提升了数据传输效率和感知准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知（CP）可突破单一智能体（如自动驾驶）感知范围和遮挡的局限，但其实际应用受限于通信带宽，现有方法未充分考虑语义相关性和跨智能体数据冗余。为最大化每一比特的信息价值，有必要研发能提取并传输语义关键、非冗余数据的新系统。

Method: 作者提出了联合语义特征编码与传输的问题建模，设计了端到端可训练的JigsawComm框架。该框架包含一个正则化编码器提取稀疏且语义相关的特征，以及轻量级特征效用评估器预测各智能体特征对整体感知任务的贡献。各智能体交换并综合效用图，最终按最优策略选择高效用的特征进行传输，消除冗余并保证通信成本不会随智能体数量增加。

Result: 在OPV2V和DAIR-V2X基准上，JigsawComm在传输数据量减少高达500倍的同时，感知准确率达到甚至优于现有最新方法。

Conclusion: JigsawComm实现了在有限通信带宽下多智能体高效、冗余消除的协同感知，极大提升了系统实用性和扩展性，对未来自动驾驶等多智能体协作领域具有重要意义。

Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.

</details>


### [41] [Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation](https://arxiv.org/abs/2511.17844)
*Shihan Cheng,Nilesh Kulkarni,David Hyde,Dmitriy Smirnov*

Main category: cs.CV

TL;DR: 本文提出了一种高效的数据微调策略，用于在大规模文本到视频扩散模型中添加物理相机参数控制，仅需稀疏低质量合成数据即可实现，效果优于传统高保真数据微调。


<details>
  <summary>Details</summary>
Motivation: 现有模型若要引入例如快门速度、光圈等相机参数控制，通常需要大量高质量数据，而获取这些数据成本高、难度大。

Method: 作者提出了一种只需稀疏、低质量合成数据即可微调模型的方法，并通过在模型中添加相机参数控制，实现了所需的生成能力。

Result: 实验结果显示，用低质量合成数据微调不仅能实现目标控制能力，甚至效果优于用高保真“真实”数据微调的模型。

Conclusion: 使用简单合成数据进行微调可高效实现模型控制能力，并可能获得更优结果，论文还通过理论框架对该现象进行了解释。

Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.

</details>


### [42] [MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use](https://arxiv.org/abs/2511.17881)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: 提出了MGA-VQA，一个结合多模态信息进行文档视觉问答的新模型，模型在六个公开数据集上取得了更高的准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 当前DocVQA方法在处理空间关系建模、高分辨率文档、复杂推理及可解释性方面存在不足。因此，需要一种能够更好融合文本语义、空间布局和视觉特征的新方法，以提升推理能力和效率。

Method: 作者提出了MGA-VQA框架，核心包括：1）基于token层的编码，2）空间图谱推理，3）记忆增强推理，4）问题引导的信息压缩。该方法通过图结构与有组织的记忆访问，实现透明的推理和更好的可解释性。

Result: MGA-VQA在FUNSD、CORD、SROIE、DocVQA、STE-VQA和RICO六个主流基准数据集上进行了评测。实验结果在答案预测准确率和空间定位效率上均显著优于现有方法。

Conclusion: MGA-VQA显著提升了文档VQA任务的准确性和效率，同时改善了推理过程的可解释性。该方法有潜力成为DocVQA领域的有影响力的解决方案。

Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.

</details>


### [43] [ArticFlow: Generative Simulation of Articulated Mechanisms](https://arxiv.org/abs/2511.17883)
*Jiong Lin,Jinchen Ruan,Hod Lipson*

Main category: cs.CV

TL;DR: 本文提出ArticFlow框架，实现对可控、多样的三维关节体生成，兼具高运动精度和形态质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在静态三维形状方面取得了很大进展，但对于包含动作依赖性变形的关节式三维生成仍具挑战。这主要由于运动带来的复杂形变以及数据集的有限性导致。作者希望开发一种能针对动作做出响应，并能高质量生成多样关节形体的模型。

Method: 提出ArticFlow：一个两阶段流匹配框架。1）先用潜在流将噪声映射到形状先验码；2）再用点流在动作和形状先验条件下将点集从噪音转移到目标形状。该方法允许单一模型泛化到多类别和多动作。

Result: 在MuJoCo Menagerie数据集上，ArticFlow既作为生成模型，也可以作为神经模拟器，能从紧凑先验预测动作条件下的运动学，且通过潜在空间插值合成新形态。相比于针对具体物体的模拟器和动作条件下的点云生成器，ArticFlow在运动学精度和形态质量上均表现更好。

Conclusion: 本文验证了动作条件下流匹配框架在生成可控且高质量三维关节体方面的有效性，为关节机制的高质量生成提供了实用的新途径。

Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

</details>


### [44] [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Wei Chen,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CV

TL;DR: 提出了FastMMoE方法提升多模态大语言模型在视觉高分辨率输入下的推理效率，通过减少视觉token的冗余以降低计算和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在高分辨率视觉输入下会产生大量视觉token，导致推理延迟和资源消耗过高，亟需在保持模型性能的前提下有效减少无用的视觉token以适应资源有限或低延迟的场景。

Method: 提出了一种无需重新训练、适用于MoE架构的加速框架FastMMoE，包括两点：(1)减少不必要的视觉token对专家模块的激活；(2)基于token路由概率的相似性进行token裁剪，去除冗余视觉token。

Result: 在DeepSeek-VL2和InternVL3.5等大规模MoE-MLLMs上，FastMMoE可在保持95.5%原始性能的条件下将FLOPs减少最高达55%，大幅优于现有的FastV、SparseVLM等密集模型裁剪方法。

Conclusion: FastMMoE为MoE多模态大模型带来了显著的加速效果，能高效减轻视觉token冗余，在多种保留率下均优于现有方法，促进了大模型在资源受限和低延迟场景下的应用。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.

</details>


### [45] [When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA](https://arxiv.org/abs/2511.17886)
*Pume Tuchinda,Parinthapat Pengpun,Romrawin Chumpu,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CV

TL;DR: 本文系统研究了知识蒸馏（KD）在多种CLIP风格视觉语言模型（VLMs）中的效果，发现更强的教师模型并不一定培养出更强的学生模型，现有KD方法在扩展到大型VLM时经常失效。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏在纯视觉或语言任务中可以有效地压缩模型，但在VLM（特别是CLIP类模型）中的应用尚不充分，且多为小规模教师和有限任务。作者希望系统性分析KD在不同规模CLIP式教师模型中的表现。

Method: 选取不同规模和性能水平的CLIP风格教师VLM，对其进行系统的知识蒸馏到较小模型，并在广泛下游任务（如分类、检索、视觉问答）中评估蒸馏效果。

Result: 实验发现，和NLP、视觉领域不同，使用更强的教师模型进行知识蒸馏不总能提升学生模型性能，部分情况下甚至导致下游多模态任务表现恶化。现有蒸馏框架在面对大规模模型时效果不理想。

Conclusion: 当前关于KD的许多假设可能不适用于多模态VLM。未来应探索新型方法，以设计更高效的多模态参数模型。

Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.

</details>


### [46] [MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization](https://arxiv.org/abs/2511.17888)
*Seulgi Jeong,Jaeil Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为MINDiff的新方法，通过引入负注意力机制，缓解了大规模文本到图像模型个性化过程中的过拟合问题，且无须增加训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法如DreamBooth在训练时通过类别特定的先验保持损失缓解过拟合，但带来了计算成本高和推理阶段用户控制力弱等问题。

Method: 提出Mask-Integrated Negative Attention Diffusion（MINDiff），通过修改推理阶段的交叉注意力机制，引入负注意力来抑制无关区域受主体影响，并允许用户通过调整参数lambda灵活平衡主体还原和文本对齐。该方法无需重新训练或更改模型结构，可直接应用于已训练的DreamBooth模型。

Result: 在DreamBooth模型上的定性和定量实验表明，MINDiff比类别特定的先验保持损失更有效地缓解了过拟合问题，并提升了语义控制和文本对齐能力。

Conclusion: MINDiff为现有大型文本到图像模型个性化带来了一种高效有效的过拟合缓解方式，推理灵活、易于集成，对学术和应用场景有良好实践价值。

Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.

</details>


### [47] [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/abs/2511.17890)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种新的音视（音频-视觉）数据集蒸馏方法DAVDD，通过解耦和预训练机制提升蒸馏效果，有效压缩数据集但保持原始性能，在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有音视数据集蒸馏方法无法有效处理跨模态对齐，且直接模态交互会损伤模态专有信息，导致蒸馏数据质量下降。因此需要新方法来平衡跨模态对齐和模态信息保护。

Method: 提出框架DAVDD，利用多样化的预训练编码器获取稳定特征，通过轻量解耦器将特征分为公共和私有表示。创新性地引入了跨模态匹配与样本-分布联合对齐，保证共享特征跨模态对齐，同时隔离私有特征避免损伤专有信息。

Result: 在多个IPC（图像每类样本数）设置与基准测试上，DAVDD方法均取得了最优的SOTA性能，蒸馏后的数据既紧凑又高效。

Conclusion: 通过预训练+特征解耦，DAVDD有效解决了跨模态对齐与模态信息保护的难题，为高质量音视数据集蒸馏提供了新范式。

Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.

</details>


### [48] [CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation](https://arxiv.org/abs/2511.17904)
*Yuhang Ming,Chenxin Fang,Xingyuan Yu,Fan Zhang,Weichen Dai,Wanzeng Kong,Guofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种称为CUS-GS的高效三维场景表示方法，将语义特征与结构化三维几何结合，通过创新机制大幅度提升表示能力同时降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射3D场景表示主要分为强调语义或结构，但两者难以兼得。语义导向方法缺少明确结构信息，而结构导向方法语义抽象有限。需要一种兼具语义和结构的新方法，提高三维场景理解与效率。

Method: CUS-GS使用体素锚点结构搭建三维空间框架，由多种基础模型（CLIP、DINOv2、SEEM等）提取多模态语义特征。提出多模态潜特征分配机制，将外观、几何和语义统一到不同模型的特征空间中。同时引入特征显著性评估策略，动态生长或裁剪锚点，剔除冗余无效部分，保持语义完整。

Result: CUS-GS与先进方法相比，参数量仅为6M（对比最近的35M），性能仍然具有竞争力。实验证明该方法展现了优异的性能与模型效率权衡。

Conclusion: CUS-GS有效融合了三维结构与多模态语义特征，实现了表现力强、效率高的三维场景紧凑表示，对高效场景建模和理解具有重要意义。

Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.

</details>


### [49] [Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation](https://arxiv.org/abs/2511.17914)
*Chenyang Jiang,Hang Zhao,Xinyu Zhang,Zhengcen Li,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出了一种用于长尾数据集蒸馏的新方法——自适应软标签对齐（ADSA），显著提升了长尾类别的模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据集蒸馏技术在实际应用中面临长尾分布数据上的性能瓶颈，主要由于软标签处理不当导致尾部类别表现不佳。论文旨在解决现有蒸馏方法对数据不平衡的不健壮性。

Method: 作者分析了长尾分布下软标签偏置的两个来源，并引入了ADSA模块，通过对软标签偏置的自适应校准，有效缓解数据不平衡引发的性能下降。ADSA为轻量级插件，能无缝集成到已有的蒸馏框架中。

Result: 在ImageNet-1k-LT实验中，搭配EDC算法和IPC=50，ADSA使尾部类别准确率提升了11.8%，整体准确率达到41.4%。在多种蒸馏技术和标签预算有限的情况下，ADSA也展现出鲁棒且优异的泛化表现。

Conclusion: ADSA能够高效缓解长尾分布数据下的软标签偏置问题，为实际蒸馏任务中的性能提升提供了通用且有效的新思路。

Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.

</details>


### [50] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文针对3D Gaussian Splatting在少样本新视角合成时泛化能力不足的问题，提出了基于频率自适应的锐度正则化方法（FASR），显著提升了新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在场景稀疏观测时容易过拟合，导致面对新视角泛化能力弱。现有的锐度正则化方法如SAM直用于3DGS会模糊高频细节。因此，亟需能兼顾细节重建与防止过拟合的正则化方法。

Method: 作者重新审视3DGS优化过程，将新视角合成视为泛化问题，提出频率自适应锐度正则化（FASR）。其核心在于根据图像局部的频率信息动态调整正则化强度和估计邻域半径，从而在提升泛化能力的同时保留高频细节。

Result: 在多个数据集和不同实验配置下，FASR均显著提升了3DGS相较于多种基线方法的新视角合成效果，有效防止了新视角下的浮点伪影，并能更好地还原细节。

Conclusion: 本文提出的FASR方法能有效缓解3DGS在少样本新视角场景下的过拟合问题，在保持细节的同时提升了模型对新视角的泛化能力，对三维重建和新视角合成具有推广价值。

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [51] [PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning](https://arxiv.org/abs/2511.17927)
*Yingjie Ma,Xun Lin,Yong Xu,Weicheng Xie,Zitong Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PA-FAS的新颖方法，通过扩展推理路径和答案扰动机制提升人脸反欺骗检测中的多模态融合、泛化与可解释性，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前人脸反欺骗检测(FAS)在多模态融合、跨域泛化和可解释性等方面取得了一定进展，但现有利用强化学习的多模态推理方法存在推理路径有限和推理混淆等问题，导致模型无法充分利用多模态信息，泛化能力不足。

Method: 作者提出PA-FAS方法：一方面，通过从有限注释中构建高质量、扩展的推理序列，丰富模型可能采用的推理路径，拓展探索空间；另一方面，在有监督微调阶段引入答案扰动机制，迫使模型在多模态信息间进行更全面和深入分析，减少投机取巧行为，提高推理深度。

Result: PA-FAS在多模态推理准确率和跨域泛化能力等方面均取得了显著提升，相较传统方法有更佳表现，并能更好实现多模态融合、泛化与可解释性的一体化。

Conclusion: PA-FAS有效应对了多模态FAS推理路径有限和推理混淆难题，提升了模型的多模态推理能力和可信度，为FAS发展提供了新思路。

Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

</details>


### [52] [MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection](https://arxiv.org/abs/2511.17929)
*Hui Lu,Yi Yu,Shijian Lu,Deepu Rajan,Boon Poh Ng,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: MambaTAD提出了一种新颖的结构化状态空间方法，通过创新模型结构，有效提升了长跨度动作的识别和定位能力，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前结构化状态空间模型虽然在动作检测任务中有潜力，但存在信息衰减和全局建模冲突等挑战，尤其在识别长跨度动作实例时效果不佳，因此需要新的方法增强全局建模能力并降低复杂度。

Method: 本文提出了MambaTAD模型，主要包含两个新设计：（1）对角掩码双向状态空间（DMBSS）模块，实现全局特征融合和动作检测；（2）多粒度全局特征融合检测头，结合全局感知，逐步优化检测结果。模型采用端到端一阶段策略，并引入线性复杂度的时序适配器SSTA，有效降低参数量和计算成本。

Result: 在多个公开数据集上的实验显示，MambaTAD在动作检测任务上取得了优于现有方法的性能，尤其在长跨度动作实例的检测表现突出。

Conclusion: MambaTAD通过创新状态空间结构和全局特征融合方法，有效提升了时序动作检测的准确性和效率，为处理长跨度动作带来新的思路。

Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.

</details>


### [53] [UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection](https://arxiv.org/abs/2511.17930)
*Yuan Qu,Zhipeng Zhang,Chaojun Xu,Qiao Wan,Mengying Xie,Yuzeng Chen,Zhenqi Liu,Yanfei Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种统一的遥感变化检测框架UniRSCD，能够适应不同粒度的输出需求，并在多任务和多数据集上获得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法为不同任务设计特定解码器，需大量专家经验，且模型泛化性差，难以应对突发性场景（如灾害），制约了应用推广。

Method: 提出基于状态空间模型骨干，并引入频率变化提示生成器作为统一编码器，实现双时相全局上下文扫描，将高频细节与低频全局信息融合，无需专门解码器补偿特征；统一解码与预测头通过分层特征交互和任务自适应输出映射，整合不同任务于同一架构。

Result: 实验表明，所提架构可适配多种变化检测任务，并在LEVIR-CD、SECOND、xBD等五个数据集上取得领先性能。

Conclusion: UniRSCD框架打破了传统方法对任务特定解码器的依赖，实现了多任务、不同输出粒度下的高效遥感变化检测，展现出良好的泛化能力和实际应用前景。

Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.

</details>


### [54] [Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion](https://arxiv.org/abs/2511.17932)
*Yan Xu,Yixing Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于预训练视频扩散模型的稀疏输入新视角合成方法，可以在只有少量视角下，生成自然且高质量的连贯场景视图，无需针对场景的训练或微调。


<details>
  <summary>Details</summary>
Motivation: 在新视角合成任务中，当输入视角非常稀疏时，传统方法在空间补全和逼真渲染方面存在较大挑战。因此，研究如何利用强大的生成模型先验，实现基于极少输入的高质量场景生成具有重要意义。

Method: 作者提出了一个零样本、生成引导的视图合成框架，将稀疏输入的新视角合成任务重构为测试时的自然视频补全。具体做法是利用预训练的视频扩散模型，在空间上生成可信的中间视图，并通过不确定性感知机制增强空间一致性。合成的新视图用于密集监督3D Gaussian Splatting（3D-GS）重建，并通过3D几何与2D视图生成的交替反馈迭代优化。

Result: 在多个公开数据集（LLFF, DTU, DL3DV, MipNeRF-360）上，不经过场景特定训练的该方法在极度稀疏输入下，显著优于传统的3D-GS基线方法。

Conclusion: 该方法证明了在极稀疏视角条件下，通过预训练生成模型和不确定性反馈引导，可取得空间连贯、高保真场景渲染，提升了少样本场景重建与新视点合成的能力。

Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

</details>


### [55] [V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction](https://arxiv.org/abs/2511.17941)
*Xiangyan Kong,Xuecheng Wu,Xiongwei Zhao,Xiaodong Li,Yunyun Shi,Gang Wang,Dingkang Yang,Yang Liu,Hong Chen,Yulong Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为V2X-RECT的高密度交通场景下的轨迹预测框架，通过改善目标数据关联一致性、减少冗余交互及复用历史信息，实现更高效且准确的预测。


<details>
  <summary>Details</summary>
Motivation: 在高密度交通环境中，传统V2X（车到一切）预测面临目标身份频繁变换、冗余信息交互和历史轨迹特征重复等问题，这些问题导致了关联困难和推理效率低下，影响交通安全与效率。

Method: 提出了多源身份匹配与校正模块，通过多视角时空关系提升目标关联的稳定性和一致性；设计了信号灯引导交互模块，将信号灯状态趋势编码为特征，有效筛选关键交互对象；并引入局部时空坐标编码，实现历史信息特征复用和并行解码，从而提升整体推理效率。

Result: 在V2X-Seq和V2X-Traj公开数据集上进行大量实验，结果显示V2X-RECT在准确性、健壮性以及推理效率方面均优于当前最新方法（SOTA），表现出更好的适应不同交通密度的能力。

Conclusion: V2X-RECT框架有效解决了高密度交通中目标身份切换和冗余交互问题，在提升预测精度的同时显著加快了推理速度，为V2X场景下的交通安全与效率提供了有力支持。

Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.

</details>


### [56] [SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System](https://arxiv.org/abs/2511.17943)
*Zhiyu Xu,Weilong Yan,Yufei Shi,Xin Meng,Tao He,Huiping Zhuang,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出SciEducator，一种用于科学视频理解与教育的自进化多智能体系统，并且构建了相关基准测试集SciVBench，实验验证其性能优于现有主流模型和视频智能体。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型和视频智能体在通用视频理解上已取得进展，但面对需要外部专业知识和严密推理步骤的科学视频教育场景时，现有方法表现不足。因此需要专门的方法，提升科学视频的理解与教育效果。

Method: 作者借鉴戴明管理循环思想，设计了迭代自进化的多智能体架构SciEducator，结合自我进化推理及反馈机制，实现对复杂科学视频活动的解释。系统可生成多模态教育内容，如文字、图片、音频和互动参考内容。为评测系统，作者还构建了涵盖五大类科学问题、共500对科学问答的SciVBench基准。

Result: 实验表明，SciEducator系统在SciVBench基准上显著优于主流闭源MLLMs（如Gemini、GPT-4o）和最新视频智能体，展现了更强的科学视频理解与教育能力。

Conclusion: SciEducator开拓了科学视频理解与教育领域的新范式，为相关应用提供了更强大和专业的支持。

Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.

</details>


### [57] [Test-Time Temporal Sampling for Efficient MLLM Video Understanding](https://arxiv.org/abs/2511.17945)
*Kaibin Wang,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出了一种无需训练、即插即用的推理方法T3S，可高效处理长视频的数据冗余，实现更快且准确的视频理解。


<details>
  <summary>Details</summary>
Motivation: 长视频输入到多模态大模型时，因自注意力计算复杂度随视频token长度急剧增加，导致计算开销大、推理慢。现有方法各有弊端，如准确性损失、需额外训练、推理速度下降。

Method: 作者提出Test-Time Temporal Sampling (T3S)方法：在推理时将长视频分割为多个短且多样化的子序列，每次前向传播同时打包多个子序列，汇总其预测结果。这种方法充分利用时空冗余，扩展视觉覆盖，同时大幅降低自注意力的计算量。

Result: 在多个长视频理解基准数据集上，T3S推理精度提升最高可达3.1%，首token推理延时降低2.04倍，且整合成本极低。

Conclusion: T3S全程在推理阶段工作，无需模型结构改动或微调，适配各种预训练大模型，把视频冗余转变为计算优势，为长视频理解提供了可扩展的新方案。

Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m α_i^2L^2)$, where $\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.

</details>


### [58] [Multi-speaker Attention Alignment for Multimodal Social Interaction](https://arxiv.org/abs/2511.17952)
*Liangyang Ouyang,Yifei Huang,Mingfang Zhang,Caixin Kang,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出了一种改进多模态大模型在视频中的多说话人社交推理表现的方法，通过引入多说话人注意力对齐机制，实现了更好的人物-语句对齐，并在多个任务和模型上取得了SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在处理多说话人视频社交任务时，视觉和文本之间对齐效果差，特别是在谁在说、说什么和相应的视觉表现（如注视和手势）方面联系不紧密，导致表现不理想。

Method: 提出了多说话人注意力对齐方法，包括动态跨模态head选择和自适应社交注意力偏置（根据已有注意力模式和说话人位置计算），增强说话人视觉表征与其言语的对齐，无需新增可训练参数或结构变更，可集成到现有模型中。

Result: 该方法集成到LLaVA-NeXT-Video、Qwen2.5-VL、InternVL3三种MLLM，并在TVQA+、MMSI和OnlineMMSI三个基准数据集上进行了实验证明。在四项社交任务中均提升了性能，实现了新的SOTA。注意力可视化显示新方法能更好聚焦于相关说话人区域。

Conclusion: 本方法提升了多模态大模型在多说话人社交推理中的对齐能力，实现了更鲁棒的社会场景理解，简单高效且支持开源。

Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

</details>


### [59] [HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2511.17958)
*Yulong Shi,Jiapeng Li,Lin Qi*

Main category: cs.CV

TL;DR: 本文提出了HEAL框架，通过分层去噪、边缘引导选择、尺寸感知融合和无监督方法，有效提升了在无源数据和无标签场景下的领域自适应能力，在多模态实验中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 临床数据的隐私保护和存储压力使得源数据无法访问，落地应用中常常无法获得目标域标签，传统领域自适应方法受限。因此迫切需要在无源且无标签的环境下解决领域偏移问题。

Method: 提出HEAL框架，包括分层去噪、边缘引导选择、尺寸感知融合和无监督特性，专为源数据不可用、目标域无标签的严格场景设计。该方法通过结构化特征处理，有效适应未知目标域。

Result: 在大规模跨模态实验中，HEAL方法优于现有SFUDA方法，取得了SOTA结果。

Conclusion: HEAL有效应对了SFUDA中的关键挑战，是领域自适应领域的重要进展，为实际医学应用提供了新方案。

Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.

</details>


### [60] [VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment](https://arxiv.org/abs/2511.17962)
*Ziheng Jia,Linhan Cao,Jinliang Han,Zicheng Zhang,Jiaying Qian,Jiarui Wang,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了一种以视觉编码器为中心的生成式预训练方法和VITAL系列多模态大模型，用于视觉质量评估，同时建立了迄今为止最大的数据集，模型在多任务、零样本等情境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉质量评估多模态大模型（LMM）一般只针对单一任务，并依赖全参数微调，容易过拟合，泛化与迁移能力有限。

Method: 1. 构建超过450万对视觉-语言数据集，由机器完成注释和质量复查；2. 多任务训练，兼顾图像、视频的定量评分和解读质量能力；3. 基于视觉编码器实现模型管理，仅需极少量额外数据即可高效扩展新任务。

Result: 训练得到的模型在零样本任务中表现突出，不同解码器只需极少量数据热启动即可接近全量训练水平。同时提升了图像和视频的质量评估精度和解释能力。

Conclusion: 工作打下了视觉质量评估领域基础多模态大模型的基础，对提升模型通用性和扩展性意义重大。

Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.

</details>


### [61] [X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.17964)
*Chenyang Yu,Xuehu Liu,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一个新的跨模态特征学习框架X-ReID，用于视频中可见光-红外跨模态行人重识别任务，有效缩小模态差异并提升时空特征建模能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型如CLIP在检索任务上表现优异，但其在视频可见-红外跨模态行人重识别（VVI-ReID）上的潜力尚未被充分挖掘，主要面临模态差异大和缺乏有效序列建模的挑战。

Method: 提出Cross-modality Prototype Collaboration (CPC)来对齐和融合不同模态特征，减少模态差异。设计了Multi-granularity Information Interaction (MII)模块，结合帧间信息交互、长短期时序建模与模态对齐，提升时空特征建模能力。最终综合多粒度信息，提升序列级表征的鲁棒性。

Result: 在HITSZ-VCM和BUPTCampus两个大规模VVI-ReID数据集上的实验表明，该方法在准确率等指标上均优于现有先进方法。

Conclusion: X-ReID有效缩小了可见光和红外图像之间的模态差距，并提升了序列建模能力，为VVI-ReID领域提供了新的解决思路与强有力的技术支撑。

Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.

</details>


### [62] [Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification](https://arxiv.org/abs/2511.17965)
*Yangyang Liu,Yuhao Wang,Pingping Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态目标重识别（ReID）方法Signal，通过对特征选择和对齐进行创新设计，实现更强判别力和多模态一致性，并在三个公开数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态目标重识别方法存在两个主要问题：一是多模态特征融合时忽视了背景干扰，二是融合方法多集中于模态对的对齐，缺乏多模态全局一致性对齐机制。因此，论文旨在提出更有效的特征选择与全局/局部对齐策略，提升ReID性能。

Method: 论文提出Signal框架。首先，设计选择性交互模块（SIM），用于在不同模态之间选择重要的patch token，并与类token交互，提取判别性强的特征。其次，提出全局对齐模块（GAM），通过最小化gram空间中3D多面体体积，实现多模态特征全局一致对齐。最后，提出局部对齐模块（LAM），以平移感知的方式对局部特征对齐。三者结合提升多模态ReID效果。

Result: 作者在RGBNT201、RGBNT100、MSVR310三个多模态目标ReID基准数据集上进行了大量实验。结果表明，所提出方法在识别准确率等指标上优于现有主流方法，证明了其有效性。

Conclusion: Signal框架通过创新的特征选择与全局/局部对齐机制，有效克服了传统方法在多模态背景干扰和一致性对齐方面的不足，显著提升了目标重识别的判别性和准确度。

Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.

</details>


### [63] [Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments](https://arxiv.org/abs/2511.16091)
*Renxiang Xiao,Wei Liu,Yuanfan Zhang,Yushuai Chen,Jinming Chen,Zilu Wang,Liang Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Rad-GS的4D雷达-相机SLAM系统，解决了大规模户外环境的重建与定位问题，并通过多项实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机或激光雷达的SLAM方法在大规模户外环境中的适应性有限，而4D毫米波雷达能够在恶劣条件下提供有价值的信息。如何有效结合雷达数据，提升大规模环境下的鲁棒重建和定位能力，是亟需解决的问题。

Method: Rad-GS系统利用3D高斯作为可微分空间表示，将原始雷达点云的多普勒信息和几何增强点云结合，引导图像中的动态物体掩码处理，以减少渲染伪影并提升定位精度。同时，利用非同步图像帧对高斯表示进行全局优化，提升纹理一致性和新视角合成效果。此外，采用八叉树结构和高斯管理策略来抑制噪声并大幅节省内存。

Result: 通过大量实验与消融分析，Rad-GS在雷达数据驱动下，在定位与重建性能上达到了与传统基于相机或激光雷达3D高斯方法相当的水平，并显著提升了大规模户外环境下的鲁棒性和效率。

Conclusion: Rad-GS方法展示了在基于4D毫米波雷达进行大规模场景重建中的可行性和优越性，为复杂户外环境下的鲁棒SLAM系统提供了有效思路。

Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.

</details>


### [64] [CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking](https://arxiv.org/abs/2511.17967)
*Hao Li,Yuhao Wang,Xiantao Hu,Wenning Hao,Pingping Zhang,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种用于RGB-热红外(RGBT)目标跟踪的新型框架CADTrack，通过上下文聚合和可变形对齐，有效提升了在复杂环境下的跟踪鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有RGBT跟踪器因模态差异难以实现有效跨模态信息融合，导致特征表达能力受限，影响跟踪准确性。

Method: 1) 提出基于马尔巴机制的特征交互模块(MFI)，高效实现信息交互并降低计算量；2) 利用基于稀疏门控的混合专家结构的上下文聚合模块(CAM)，编码跨层互补上下文信息；3) 设计可变形对齐模块(DAM)，结合可变形采样和时序传播，缓解空间失配和漂移。

Result: 在五个RGBT跟踪基准数据集上，CADTrack展现出优越的鲁棒性和高准确性，超越现有方法。

Conclusion: CADTrack能有效融合跨模态信息，提升RGBT跟踪性能，为复杂环境下的多模态目标跟踪提供了新思路。

Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.

</details>


### [65] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: 本论文提出了一种无需存储旧数据的类增量学习方法，通过对新任务图像进行对抗性扰动，在线合成伪重放图像，从而缓解灾难性遗忘问题，并取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 类增量学习需要在不存储历史数据的情况下学习新知识，但容易遗忘旧知识（灾难性遗忘），尤其是在存储空间有限或隐私受限时，现有方法难以兼顾新旧知识。

Method: 提出对抗性伪重放（APR）方法：在新任务训练时，将新类图像通过对抗性攻击扰动，使其特征尽量靠近旧类均值原型，用作知识蒸馏，且利用伪重放样本学习转移矩阵，校正各类别的协方差矩阵以补偿语义漂移。整个过程无需存储任何历史任务图像。

Result: 在标准的EFCIL基准尤其是冷启动等挑战性场景下，该方法取得了最先进的性能。

Conclusion: APR方法有效平衡了类增量学习中的稳定性和可塑性，证明了其在无示例存储前提下依然能显著减缓遗忘、提升新任务适应能力。

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [66] [FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning](https://arxiv.org/abs/2511.17979)
*Bo Yin,Xiaobin Hu,Xingyu Zhou,Peng-Tao Jiang,Yue Liao,Junwei Zhu,Jiangning Zhang,Ying Tai,Chengjie Wang,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了FeRA框架，通过频率驱动的微调策略，提升扩散模型在新任务上的适应性，显著增强了适配效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成建模方面取得了成功，但如何高效地将大规模预训练模型适配到新任务仍然面临挑战。为此，作者重新审视了扩散模型的反噪过程，试图理解其中的频率能量机制，以寻求更有效的适配方法。

Method: 作者提出了FeRA（频率驱动微调框架），其核心是根据扩散模型内在的频率能量变化来指导参数更新。该框架包含三个关键部分：（1）频率能量指示器，用于刻画潜在特征的频带能量分布；（2）软频率路由器，自适应地融合多个特定频率的adapter专家；（3）频率能量一致性正则化，用于优化过程稳定和多频带协同。训练和推理阶段都采用频率路由，且可与adapter类微调方法无缝结合，兼容不同的扩散模型骨干和分辨率。

Result: FeRA框架能够有效对齐和利用扩散模型中的频率能量规律，显著提升模型在不同任务和分辨率下的适应能力，表现出更高的稳定性和泛化性能。

Conclusion: 将模型适配与扩散过程中的频率能量机制对齐，实现了简单、稳定、通用的扩散模型微调新范式，有望推动大型生成模型高效适应多样化新任务。

Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.

</details>


### [67] [Plan-X: Instruct Video Generation via Semantic Planning](https://arxiv.org/abs/2511.17986)
*Lun Huang,You Xie,Hongyi Xu,Tianpei Gu,Chenxu Zhang,Guoxian Song,Zenan Li,Xiaochen Zhao,Linjie Luo,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Plan-X 是一个结合语言模型和扩散模型的视频生成新框架，有效减少幻觉和错配，实现更精细、指令对齐的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前扩散型Transformer在视觉生成表现出色，但难以处理高层次语义推理和长时规划，导致视觉幻觉和指令偏离，尤其在复杂场景和交互中尤为突出。

Method: 提出了Plan-X框架，通过“语义规划器”，利用多模态语言模型对文本和视觉上下文做推理，生成与文本对齐的时空语义token序列。这些token为视频扩散模型提供结构化语义“草图”，强化高层指令引导。扩散模型则聚焦于高保真渲染。两者结合实现规划与细节双提升。

Result: 大量实验证明，Plan-X 较大幅度减少了视觉幻觉，并提升了与多模态上下文一致、精细、指令对齐的视频生成能力。

Conclusion: Plan-X 有效融合了语言模型的语境推理和规划能力与扩散模型的视觉细节合成优势，为复杂视频生成任务带来了更高水平的表现。

Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

</details>


### [68] [HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.17988)
*Haodong Chen,Xianfei Han,Qwen*

Main category: cs.CV

TL;DR: 该论文提出了结合CNN和Mamba的新型分割网络HyM-UNet，显著提升了医学图像分割的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN结构受限于局部感受野，难以捕获复杂的整体解剖结构，限制了自动医学分割的性能。

Method: 提出一种混合架构HyM-UNet，浅层采用CNN提取细节，高层用Visual Mamba捕获全局语义信息，并设计了Mamba-Guided Fusion跳跃连接，用语义特征动态过滤背景噪声。

Result: 在ISIC 2018数据集上进行实验，HyM-UNet在Dice系数和IoU指标上均优于现有最佳方法，并且模型参数量和推理延迟更低。

Conclusion: HyM-UNet有效提升了复杂医学图像分割的性能与效率，证明了该方法的有效性和鲁棒性。

Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.

</details>


### [69] [SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining](https://arxiv.org/abs/2511.17993)
*Jiayu Wang,Haoyu Bian,Haoran Sun,Shaoning Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于物理机制的图像去雨方法（SD-PSFNet），通过多阶段恢复和点扩散函数(PSF)机制，显著提升了复杂场景下的去雨效果，在多个数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像去雨方法难以应对雨滴的多尺度物理特性及其与场景的耦合，导致去雨效果有限。本文希望通过物理建模结合深度学习方法，揭示更真实的图像退化过程，实现更高质量的雨噪去除。

Method: 提出了SD-PSFNet模型，采用三阶段级联的顺序恢复架构，每阶段通过引入具有学习能力的PSF机制，动态模拟雨滴光学过程，分离雨与背景信息。通过自适应门控融合模块实现跨阶段特征优化，从粗到细逐步提升去雨和细节恢复能力。

Result: 在Rain100H、RealRain-1k-L和RealRain-1k-H等数据集上，SD-PSFNet模型在PSNR/SSIM指标上均达到当前最优（如Rain100H为33.12dB/0.9371，RealRain-1k-L为42.28dB/0.9872，RealRain-1k-H为41.08dB/0.9838），展现出优异的综合去雨性能。

Conclusion: SD-PSFNet通过引入动态物理建模和创新的特征融合机制，能够更有效地解决复杂场景和密集降雨下的图像退化问题，为基于物理的图像去雨方法提供了新思路。

Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.

</details>


### [70] [ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models](https://arxiv.org/abs/2511.18082)
*Wencheng Ye,Tianshi Wang,Lei Zhu,Fengling Li,Guoli Yang*

Main category: cs.CV

TL;DR: 本文提出ActDistill框架，通过蒸馏方法将大型视觉-语言-动作（VLA）模型的动作预测能力转移到轻量级模型上，大幅减少推理计算量和延迟，同时保持甚至超越原模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型在灵活性和泛化方面表现出色，但由于计算开销大、推理延迟高，难以实际应用于机器人操作任务。该工作旨在解决这一低效瓶颈。

Method: 作者提出了ActDistill蒸馏框架，采用VLA模型作为教师，通过图结构封装的动作预测分层信息引导知识转移到学生模型。学生模型通过动态路由机制自适应选用计算路径，保证高效动作预测。在推理阶段，删除图相关辅助模块，仅保留必要路由层，极大提升效率。

Result: 在多项具身智能基准测试中，ActDistill学生模型与全量VLA模型相比实现超过50%的计算减少与最高1.67倍的推理速度提升，同时保持或超越原模型性能。

Conclusion: ActDistill建立了高效VLA模型的新范式，可广泛推进高效具身智能系统的发展，在实际机器人任务中具有重要应用前景。

Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

</details>


### [71] [RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale](https://arxiv.org/abs/2511.18005)
*Shengyuan Wang,Zhiheng Zheng,Yu Shang,Lixuan He,Yangcheng Yu,Fan Hangyu,Jie Feng,Qingmin Liao,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了RAISECity系统，用于高质量、真实感且可扩展的城市级3D世界生成，在各项指标显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前城市规模的3D生成在智能体研究和世界模型构建中具有重要意义，但现有方法在质量、真实度和可扩展性方面存在严重不足。

Method: 提出RAISECity——现实对齐智能合成引擎。该方法采用基于智能体的框架，整合多模态基础工具来获取真实知识，并通过动态数据处理、自我反思与迭代，以及高级多模态工具调用，优化中间表示，减少误差，提升3D场景构建总体质量。

Result: 实验显示RAISECity在真实对齐、形状精度、纹理保真和美学水准上具有明显优势，感知质量超90%的胜率超越现有基线方法。

Conclusion: RAISECity兼具高质量3D生成、真实性、可扩展性、以及与图形学流程的兼容性，有望促进沉浸式媒体、具身智能和世界模型等多领域应用。

Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.

</details>


### [72] [PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation](https://arxiv.org/abs/2511.18570)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Dinesh Manocha*

Main category: cs.CV

TL;DR: 本论文提出PhysGS方法，结合贝叶斯推断与3D高斯喷溅技术，可在视觉和语言先验的辅助下，从视觉线索中推断密集的点云物理属性（如摩擦、硬度等），并具备不确定性建模能力，在多个真实数据集上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 机器人与环境安全、有效交互需要理解摩擦、硬度等物理属性，但现有3D重建方法仅关注几何和外观，不能推断这些底层物理性质，因此有必要开发能估算物理属性的3D重建框架。

Method: 提出PhysGS，将物理属性估算建模为对高斯点云的贝叶斯推断，在视觉观测和视觉-语言先验指导下，迭代优化每个点的材料与物理属性估计，同时显式建模测量不确定性（包括随机性和模型不确定性）。

Result: 在ABO-500、室内与室外真实数据集上，PhysGS大幅提升了质量估计（最多提升22.8%）、肖氏硬度误差（最多降低61.2%）、动摩擦误差（最多降低18.1%）等指标的准确度。

Conclusion: PhysGS首次实现了3D重建、物理推理与不确定性建模的统一，能以空间连续方式对物体密集地估算物理属性，为机器人感知和场景理解提供了新工具。

Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.

</details>


### [73] [Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging](https://arxiv.org/abs/2511.18007)
*Siteng Ma,Honghui Du,Prateek Mathur,Brendan S. Kelly,Ronan P. Killeen,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: 本文提出了一种适用于纵向医学影像主动学习的新方法LMI-AL，能在极少标注的情况下实现与全标注方法相当的变化检测效果。


<details>
  <summary>Details</summary>
Motivation: 纵向医学影像的变化检测对于疾病跟踪和治疗效果评估非常关键，但其标注过程极为繁琐且耗时高，常规主动学习方法难以直接应用，因此需要一种新的、面向变化检测的高效主动学习策略。

Method: 提出了Longitudinal Medical Imaging Active Learning（LMI-AL）框架。该方法对基线与随访的3D图像进行2D切片配对与差分，通过主动学习迭代选择信息量最大的切片对进行标注，以最少的人工干预训练深度模型。

Result: 实验证明，LMI-AL在仅标注不到8%数据的情况下，就能获得与全数据标注训练模型相当的性能。同时，作者还对方法的表现进行了细致分析，为后续研究提供参考。

Conclusion: LMI-AL能显著减少纵向医学影像中变化检测所需的标注成本，同时保证模型性能，为该领域的主动学习研究提供了有效的新途径。

Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.

</details>


### [74] [Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents](https://arxiv.org/abs/2511.18685)
*Dayong Liu,Chao Xu,Weihong Chen,Suyu Zhang,Juncheng Wang,Jiankang Deng,Baigui Sun,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出CFG-Bench基准，系统评估多模态大模型（MLLMs）在复杂物理环境中作为智能体细粒度动作智能能力，目前主流MLLMs在该基准下表现有限，针对性训练可显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型基准主要聚焦高层规划和空间推理，忽视了智能体在实际物理交互中所需的细粒度动作智能。为填补这一评估空白，作者设计了更具针对性的基准。

Method: 作者构建了CFG-Bench，包含1368个精选视频及19562组三模态问答，覆盖物理互动、时序因果、意图理解和评判推理四种认知能力，并用其系统评测主流MLLMs能力及其在细致动作描述方面的表现，通过监督微调提升模型能力并验证效果。

Result: 主流MLLMs在物理交互细致指令和高阶推理上表现不佳，通过在CFG-Bench上进行监督微调后，模型在多项既有体态基准上取得了显著性能提升。

Conclusion: 现有MLLMs在具体物理动作和高阶认知能力上存在明显短板，CFG-Bench揭示了这一问题，通过针对性训练可有效提升模型操作性智能，是未来进一步发展可落地智能体能力的重要方向。

Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.

</details>


### [75] [RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios](https://arxiv.org/abs/2511.18011)
*Jun Zhang,Jie Feng,Long Chen,Junhui Wang,Zhicheng Liu,Depeng Jin,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了RoadBench基准，用于全面评估多模态大模型（MLLMs）在城市道路标线等细粒度空间元素上的理解和推理能力。通过六个任务和9,121个严密测试案例，揭示了现有MLLMs在细粒度空间推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在一般空间理解和推理表现强大，但在复杂城市场景下的细粒度空间理解（如道路标线）却缺乏深入研究和工业应用。作者希望弥补这一领域的空白，推动MLLMs在真实城市交通系统中的应用。

Method: 作者提出RoadBench基准，包括六类任务，涉及9,121个人工严格验证的测试案例，采用鸟瞰视角（BEV）和第一人称视角（FPV）图片输入系统，全面测试模型对局部到全局空间信息的识别、理解、推理及领域知识整合能力。对14种主流MLLMs进行了系统实验评测。

Result: 实验结果表明，RoadBench对现有MLLMs是一项极具挑战性的基准。多数模型在某些细分任务上的表现甚至低于基于规则的方法或随机选择水平，暴露出其在城市复杂场景下空间推理与理解的显著不足。

Conclusion: RoadBench填补了细粒度城市空间推理评测的空白，并凸显出MLLMs当前的局限性。这一成果及其评测体系将有助于推动MLLMs在细粒度空间理解能力上的进一步发展。

Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.

</details>


### [76] [Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks](https://arxiv.org/abs/2511.19198)
*Ann-Sophia Müller,Moonkwang Jeong,Meng Zhang,Jiyuan Tian,Arkadiusz Miernik,Stefanie Speidel,Tian Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理器官模型和深度学习的自动化3D解剖数据生成流程，解决现实中获取海量高质量3D医疗数据的难题，特别针对成像对比度低的软组织器官。使用人工前列腺模型，通过超声成像与神经网络分割，实现高效3D重建，并借助3D GAN扩展数据多样性。


<details>
  <summary>Details</summary>
Motivation: 机器学习驱动的外科规划和训练需大量3D解剖模型数据，实际采集遇到法律、伦理与技术（尤其软组织成像难）障碍，亟需新的高效自动化数据生成方式。

Method: （1）制备仿生水凝胶人工前列腺模型，具备多区成像对比。（2）使用超声成像于手术模拟前后对模型扫描。（3）用神经网络对超声图像分割，重建3D结构；与传统方法对比评估分割性能。（4）基于分割结果重建3D网格，并利用3D生成对抗网络（GAN）扩展模型多样性，用于下游任务。

Result: 神经网络分割超声图像的IoU优于传统计算机视觉方法。成功实现高质量3D模型重建，3D GAN丰富了数据空间，流程对软组织器官如前列腺验证有效。

Conclusion: 提出的流程能高效自动生成高质量3D解剖数据，突破传统采集难题，尤其适用于成像对比度低的软组织，可为ML驱动的手术规划和训练任务提供坚实的数据基础。

Abstract: Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.

</details>


### [77] [State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2511.18012)
*Jiaying Zhou,Qingchao Chen*

Main category: cs.CV

TL;DR: 本文提出两种新方法，针对弱监督开放词汇目标检测（WS-OVOD）中的语义原型不足与语义失配问题，实现了更丰富、更精准的目标检测能力。


<details>
  <summary>Details</summary>
Motivation: 目前WS-OVOD领域存在两个核心难题：其一，语义原型静态、单一，无法覆盖物体的多样视觉状态（如同一只猫的不同姿态）；其二，视觉区域和文本嵌入之间存在语义失配，影响检测性能。

Method: 作者提出两种互补的原型增强策略：1）状态增强语义原型（SESP），通过生成状态感知的文本描述（如“正在睡觉的猫”），捕捉同类物体多样化的外观；2）场景增强伪原型（SAPP），融合上下文语义（如“猫躺在沙发上”）并引入软对齐机制，提高视觉与文本的匹配度。两者结合提升了语义表达和视觉文本对齐能力。

Result: 整合SESP与SAPP后，实验结果表明该方法在语义原型丰富度和视觉-文本对齐方面显著提升，且在目标检测性能上实现优越表现。

Conclusion: 本文方法有效解决了WS-OVOD中的语义原型单一与视觉-文本失配问题，推动了开放词汇目标检测的实用能力。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.

</details>


### [78] [Modeling Retinal Ganglion Cells with Neural Differential Equations](https://arxiv.org/abs/2511.18014)
*Kacper Dobek,Daniel Jankowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 本文比较了LTC与CfC两种连续时间网络在模拟虎斑蝾螈视网膜神经节细胞活动上的表现，相较于卷积基线和LSTM拥有更低MAE、更快收敛速度和更小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有用于模拟视网膜神经元活动的神经网络模型存在模型复杂、训练慢、数据需求大等问题，特别在需要轻量、灵活、快速再训练的边缘视觉应用中有所不足。本文旨在探索更高效、适合低数据场景的模型架构。

Method: 作者在三个关于虎斑蝾螈视网膜神经节细胞的数据集上，分别采用LTC与CfC两种连续时间网络架构，与卷积网络和LSTM进行对比，评估其MAE、收敛速度、模型体积和查询延迟等表现。

Result: LTC和CfC在MAE、收敛速度、模型规模和查询时间方面全面优于卷积和LSTM基线模型，但其皮尔逊相关系数略低。

Conclusion: LTC和CfC因高效和适应性强，特别适用于数据有限且需频繁再训练的场景（如视觉假体的边缘部署），在这些应用中展示出明显优势。

Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.

</details>


### [79] [MambaX: Image Super-Resolution with State Predictive Control](https://arxiv.org/abs/2511.18028)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Naoto Yokoya,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文提出了一种称为MambaX的非线性状态预测控制模型，显著提升了图像超分辨率任务的性能，尤其是在多模态任务和复杂场景中。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法主要提升最终分辨率，忽视了中间阶段误差的有效控制和累积问题，特别是在高精细图像处理时受限于感受野和灵活性。因此，需提出更能动态控制和抑制误差传播的新模型。

Method: 提出MambaX，将连续光谱带映射至潜在状态空间，并通过动态学习控制方程的非线性状态参数，实现对状态空间模型非线性微分系数的拟合。融合新颖的状态交叉控制范式和递进性迁移学习，以适应不同领域与模态的异质性。

Result: 实验表明MambaX在单幅图像超分辨率和多模态融合超分辨率任务中均取得了优异的性能，优于现有相关序列模型。

Conclusion: MambaX的提出推动了跨任意维度与模态的光谱广义表征建模，为超分辨率相关研究和应用带来新的可能性。

Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.

</details>


### [80] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出首个同时针对APS与EVS像素的统一噪声模型，并基于统计学方法详细校准与分析噪声行为，还开发了真实统计特性的仿真器HESIM，实验验证其在多种任务上的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 事件帧混合传感器（集成APS与EVS）虽提升了成像性能，但复杂电路结构带来的噪声模式难以理解与建模，限制了该类传感器的分析与应用。

Method: 该研究建立了一个统计驱动的统一噪声模型，涵盖光子散粒噪声、暗电流噪声、固定图案噪声和量化噪声，并分析了噪声与照度等变量的关系。制定了噪声参数的标定流程，最终开发了噪声统计驱动的仿真器HESIM，可用于合成现实噪声条件下的RAW帧与事件数据。

Result: 在两款实际混合传感器上验证，模型能准确描述和模拟APS及EVS像素的噪声特性，仿真器生成的数据在视频插帧、去模糊等任务上具有很好的现实数据迁移效果。

Conclusion: 提出的方法不仅揭示并量化了混合传感器的噪声来源和行为，还通过精确噪声建模和仿真，为后续相关图像处理算法的开发和真实场景应用提供了坚实基础。

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [81] [UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios](https://arxiv.org/abs/2511.18050)
*Tian Ye,Song Fei,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种新型扩散变换器UltraFlux，能够原生在4K分辨率和多种宽高比（AR）下进行高质量文生图生成，显著超过现有开源方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器多在1K分辨率表现优良，但在原生4K和多种宽高比时，受限于位置编码、VAE压缩和优化的相互影响，导致生成质量下降。单独优化这些因素效果有限，需要系统性改进。

Method: 文章基于数据与模型协同设计思想，提出UltraFlux方法：（1）开发MultiAspect-4K-1M数据集，包含100万张4K多宽高比图片及多语言和丰富元数据；（2）在模型端，采用Resonance 2D RoPE与YaRN组合实现4K下对宽高比敏感的位置编码；（3）设计非对抗性VAE后训练方法提升4K重建质量；（4）引入SNR-Aware Huber Wavelet损失平衡训练梯度；（5）利用阶段式审美课程学习策略，加强高审美监管。

Result: UltraFlux在Aesthetic-Eval at 4096基准和多宽高比4K设置下，在保真度、美学和对齐等指标上，持续超越主流开源基线；结合LLM prompt refiner后，效果达到或超过专有模型Seedream 4.0。

Conclusion: UltraFlux通过系统性创新，实现了原生4K、多宽高比下高质量文生图生成，推动扩散变换器在高分辨率和实际应用中的发展。

Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.

</details>


### [82] [IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment](https://arxiv.org/abs/2511.18055)
*Bowen Qu,Shangkun Sun,Xiaoyu Liang,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了IE-Bench套件和IE-Critic-R1新方法，用于更好地评估文本驱动的图像编辑质量，更贴近于人类感知标准。


<details>
  <summary>Details</summary>
Motivation: 传统的文本驱动图像编辑评价方法往往只关注文本与图像的一致性，或与人类的主观感受不够吻合。文本驱动图像编辑还需考虑原始图像、编辑文本与编辑结果三者之间更复杂的联系，因此亟需更科学、更全面的评价标准。

Method: 作者构建了IE-Bench基准套件，包含多样化源图像、编辑文本和不同方法生成的编辑结果，并收集了约4,000组样本及15人给出的主观评分（MOS）。同时提出IE-Critic-R1评测模型，利用可验证奖励的强化学习（RLVR）机制，使评测更全面并可解释。

Result: 实验表明，IE-Critic-R1在主观一致性方面优于现有评价指标，更贴合人类的图像编辑质量评判标准。

Conclusion: IE-Bench和IE-Critic-R1为文本驱动图像编辑提供了全面的评测工具和方法，有助于推动该方向的研究和实际应用，相关数据与代码已公开。

Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.

</details>


### [83] [Hierarchical Semi-Supervised Active Learning for Remote Sensing](https://arxiv.org/abs/2511.18058)
*Wei Huang,Zhitong Xiong,Chenying Liu,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种结合半监督学习和分层主动学习的新方法HSSAL，能高效利用少量标注实现遥感场景分类，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感领域的表现依赖于大量高质量标注数据，而标注耗时且昂贵，大量未标注图像未被充分利用。因此需要设计高效利用未标注数据并减少标注负担的新方法。

Method: 提出了层次化半监督主动学习框架（HSSAL），在每次迭代中，半监督学习结合有标注和无标注数据自训练提升特征表达与不确定性估计，利用分层聚类引导主动学习模块选择最具代表性且高不确定性的样本进行标注和再训练，提高标注效率。

Result: 在UCM、AID和NWPU-RESISC45三个遥感场景分类基准数据集上，HSSAL在仅用了很少标注（分别为8%、4%、2%）时，准确率可以达到完全监督方法的95%以上，优于传统半监督或主动学习方法。

Conclusion: HSSAL显著提高了遥感场景分类任务的标注利用率和效率，具备更强的代表性样本选取能力，能有效减少标注需求，推动遥感智能解译发展。

Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.

</details>


### [84] [A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)](https://arxiv.org/abs/2511.18063)
*Gabriela Fernandes*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的虚拟病理助手，可自动区分子宫颈腺癌原位病变（AIS）与正常宫颈腺体组织。该模型在CAISHI数据集上训练和测试，取得了较好的分类效果，对未来病理诊断的辅助具有实际意义。


<details>
  <summary>Details</summary>
Motivation: AIS是重要的癌前病变，但其病理诊断难度较大。早期发现有助于预防宫颈腺癌，然而准确判断依赖于经验丰富的病理医生。科研人员希望借助AI辅助提升诊断效率和准确性。

Method: 使用含2240张专家标注H&E病理图像的CAISHI数据集，结合Macenko染色归一化和patch切割，提升图像特征表达。采用EfficientNet-B3卷积神经网络，配合类别均衡抽样和focal loss解决数据不均衡问题。最后利用Grad-CAM可视化模型关注的区域，将模型集成到基于Gradio的虚拟诊断助手中。

Result: 模型在测试集上的整体准确率为0.7323，异常（AIS）类别F1分数为0.75，正常类别为0.71。可视化热图显示模型关注的病理特征与AIS一致。

Conclusion: 研究证明轻量且可解释的AI病理系统在宫颈腺体病理筛查等场景具备应用潜力，可用于临床预筛查、医生培训以及低资源地区的病理辅助。

Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.

</details>


### [85] [VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection](https://arxiv.org/abs/2511.18075)
*Jianhang Yao,Yongbin Zheng,Siqi Lu,Wanying Xu,Peng Sun*

Main category: cs.CV

TL;DR: 该论文提出了VK-Det，无需额外监督，实现了视觉知识引导的开放词汇航空目标检测，显著提升了对新类别目标的检测能力，在主流数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测方法依赖于文本监督，导致语义偏差，仅能检测文本指定的概念，限制了新类别的扩展能力。

Method: 提出VK-Det框架：1）挖掘并利用视觉编码器原生的区域感知能力，实现更细粒度的定位和自适应蒸馏；2）创新性引入原型感知伪标签策略，通过特征聚类建立类别决策边界，并采用原型匹配将检测区域映射到潜在类别。方法完全不依赖额外监督，泛化性强。

Result: 在DIOR和DOTA数据集上的新类别检测（mAP^N）分别达到30.1和23.3，优于现有的甚至带有额外监督的方法，验证了其有效性。

Conclusion: VK-Det有效解决了文本依赖带来的语义偏差问题，无需额外监督，增强了对新类别目标的检测能力，为开放词汇目标检测提供了更具扩展性和实用性的方案。

Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

</details>


### [86] [Less Is More: An Explainable AI Framework for Lightweight Malaria Classification](https://arxiv.org/abs/2511.18083)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CV

TL;DR: 提出了EMFE特征工程管道，利用简单显著的形态学特征和轻量机器学习模型，在CPU上实现了与深度学习模型相近的疟疾细胞二分类性能，大大降低了计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然准确但计算代价高、可解释性差。作者想探讨在疟疾这类形态特征明显、任务简单的二分类问题上，是否真的需要复杂的神经网络，而不是更透明、轻量的传统方法。

Method: 采用NIH疟疾细胞图像数据集，提取两个形态学特征（非背景像素数与细胞内部空洞数），分别用逻辑回归、随机森林建模，并与ResNet18等深度学习模型在准确率、模型大小、CPU推理速度上对比。最终还将逻辑回归和随机森林集成提高性能。

Result: 单变量逻辑回归模型在CPU上达到了94.8%的测试准确率，模型仅1.2kB，推理延迟极低。集成模型准确率提升到97.15%。相比之下，深度学习模型体积13.6-44.7MB，推理时间明显更长（68ms）。

Conclusion: 对于简单形态学任务，透明、轻量的特征工程方法能实现与深度学习相当的临床分类效果，并在部署效率、可解释性等方面有巨大优势，适合在计算资源匮乏的实际环境中应用。

Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.

</details>


### [87] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态生存分析方法，称为Together-Then-Apart（TTA），既强调模态对齐，也注重保留各模态的独特信息，从而提升了模型性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生存分析方法多依赖注意力机制进行模态对齐，导致不同模态间的独特特征易丢失，表现为特征坍塌和多样性下降。因此，作者提出同时关注对齐与区分性，以获得更鲁棒、可解释的表示。

Method: TTA方法包括两个阶段：Together阶段利用共享原型和不平衡最优传输目标对模态嵌入进行对齐；Apart阶段通过模态锚点和对比正则项提升不同模态表达的多样性，从而保留独特信息。核心思想是以最小-最大优化联合建模公共和模态特有表示。

Result: 在五个TCGA基准测试中，TTA方法在生存预测的准确性方面均优于现有先进方法，展现了模型的优越性能和有效性。

Conclusion: TTA方法能够有效兼顾多模态对齐与区分性，不仅具备较强的预测能力，还提高了模型解释性，并为多模态数据整合提供了新的理论视角。

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [88] [Versatile Recompression-Aware Perceptual Image Super-Resolution](https://arxiv.org/abs/2511.18090)
*Mingwei He,Tongda Xu,Xingtong Ge,Ming Sun,Chao Zhou,Yan Wang*

Main category: cs.CV

TL;DR: 该论文提出一种面向感知的超分辨率（SR）与多种压缩联合感知的方法VRPSR，利用扩散模型模拟压缩，对训练流程进行优化，实现SR后可适应下游多种压缩编码方案，提升最终压缩图片质量并节省码率。


<details>
  <summary>Details</summary>
Motivation: 现有感知超分辨率方法未考虑下游重新压缩，往往只关注提升重建图像的视觉质量，但实际应用中，输出结果经常需要经过压缩存储或传输，压缩过程可能产生额外的失真和伪影。目前SR与压缩联合优化存在挑战，如编解码器的不可微分性与参数多样性。本文旨在解决如何让SR方案同时面向不同压缩方式优化，兼顾超分辨率与压缩联动下的图像视觉体验与码率成本。

Method: 将压缩过程表述为条件文本到图像生成任务，利用预训练扩散模型作为通用可泛化的编解码器模拟器。针对感知超分训练流程，提出优化技巧：一、用感知损失优化模拟器；二、用轻度压缩图像作为超分训练目标，从而提升模型对后续多种压缩方式的鲁棒性。

Result: 基于Real-ESRGAN和S3Diff两大SR方案，在标准H.264/H.265/H.266三种主流视频编码格式下，所提VRPSR方法平均节省超过10%的码率，且强化了SR与压缩后处理的联合优化能力。

Conclusion: VRPSR能够高效对齐感知超分辨率与多种压缩的实际链路，显著提升经压缩后的图像主观质量并降低码率，对广泛实际场景的图像/视频增强与压缩存储传输有应用前景。

Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.

</details>


### [89] [Spotlight: Identifying and Localizing Video Generation Errors Using VLMs](https://arxiv.org/abs/2511.18102)
*Aditya Chinchure,Sahithya Ravi,Pushkar Shukla,Vered Shwartz,Leonid Sigal*

Main category: cs.CV

TL;DR: 该论文提出了Spotlight任务，通过对文本生成视频中的细粒度错误进行标注和解释，推动了T2V模型的精细化评估。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型虽能生成高质量视频，但仍会产生更精细和局部的错误，而现有评估方法无法定位和描述这些具体错误。作者希望填补细粒度视频错误定位与解释的评估空白。

Method: 作者构建了Spotlight任务，收集了3个主流T2V模型（Veo 3, Seedance, LTX-2）生成的600个视频，基于200个文本提示，手工标注了超过1600个细粒度错误，分为6类（如运动、物理、文本遵循等），并分析错误的分布、持续时长等。随后用当前主流视觉语言模型（VLMs）在Spotlight任务上评测其错误定位与解释能力，并尝试推理策略提升VLMs准确率。

Result: 实验发现，文本遵循和物理错误最常见且持续时间长，出现-消失和体态类错误表现为短暂片段。VLMs在视频错误识别和定位上远落后于人类，通过新推理策略，VLMs性能提升了近2倍。

Conclusion: Spotlight任务为T2V模型的细粒度错误评估建立了新基准，有望推动更完善的评测工具和奖励模型发展，提升视频生成技术的可靠性。

Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

</details>


### [90] [Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning](https://arxiv.org/abs/2511.18104)
*Xiaohong Liu,Xiufeng Song,Huayu Zheng,Lei Bai,Xiaoming Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种针对扩散模型生成视频的伪造检测新方法MM-Det++，采用了空间-时间和多模态双分支结构，并构建了大规模检测数据集，实现了对伪造视频的高效检测。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成视频数量激增，信息安全问题日益突出，现有检测方法主要集中在图像级别，视频级别的通用伪造检测技术仍较为欠缺，因此亟需有效的视频检测手段。

Method: 1) 提出MM-Det++算法，包含两个创新分支：
- 空间-时间（ST）分支，采用新型Frame-Centric Vision Transformer（FC-ViT）用于时空特征聚合，通过FC-token捕捉视频帧整体伪造痕迹。
- 多模态（MM）分支，利用多模态大模型的理解与推理能力获得伪造表示，从语义角度灵活检测伪造。
2) 设计统一多模态学习（UML）模块融合多模态特征。
3) 构建大规模扩散视频取证数据集（DVF）。

Result: 大量实验结果表明，MM-Det++在检测扩散生成视频伪造方面优于现有方法，验证了统一多模态伪造学习方式的有效性。

Conclusion: 本文方法能高效检测出扩散模型生成的视频伪造，推动了视频取证领域的发展，提出的大规模数据集亦有助于今后的相关研究。

Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

</details>


### [91] [AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens](https://arxiv.org/abs/2511.18105)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,Yung-Hsiang Lu,James C. Davis*

Main category: cs.CV

TL;DR: AdaPerceiver是一种具备统一自适应能力（深度、宽度、tokens）的新型transformer架构，通过高效联合训练支持多种配置，显著提升了推理灵活性并兼顾性能和吞吐率。


<details>
  <summary>Details</summary>
Motivation: 现有transformer在推理时对计算资源分配缺乏灵活性，动态计算方法大多仅针对某一个维度（如token数），无法适应实际部署对不同硬件和时延的需求。因此需要一种能在多轴上灵活调整的模型架构。

Method: 提出AdaPerceiver架构，可在深度、宽度和token数三轴实现自适应，并设计高效联合训练策略，使各配置下模型表现均衡。

Result: 在图像分类任务中，AdaPerceiver在准确率-吞吐率上超越了FlexiViT-L，能够以36%更高的吞吐率达到85.4%准确率；在密集预测任务中，以大幅减少FLOPs（约26倍于ViT-H/14）下匹配其表现，在ImageNet1K上只损失极小准确率即可节省24-33% FLOPs。

Conclusion: AdaPerceiver首创统一多轴灵活自适应能力，为transformer模型在多场景高效部署提供了新的技术路径，同时保证了性能和资源利用的双重优化。

Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

</details>


### [92] [Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training](https://arxiv.org/abs/2511.18115)
*Wenyu Li,Sidun Liu,Peng Qiao,Yong Dou,Tongrui Hu*

Main category: cs.CV

TL;DR: Muskie 是一种新型原生多视图视觉骨干网络，用于3D视觉任务，能同时处理多个视角，并在预训练阶段引入多视图一致性，无需3D监督即可获得很强的几何理解能力。该模型在多项下游3D任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉骨干网络通常采取逐帧处理方式，多视图一致性差。为了解决这一问题，提升3D空间理解能力，论文提出能原生支持多视图处理、自动引入视图一致性的骨干架构。

Method: Muskie 通过设计可同时处理多视角输入的结构，并采用激进的遮挡掩码预训练任务，让模型学习在缺失大量信息的情况下，通过其它视角建立几何对应关系进行内容重建，期间不依赖3D监督。

Result: Muskie 在多视图对应准确率上优于当前最先进的逐帧骨干（如DINO）；用于下游如相机位姿估计、点云重建等任务时也表现优越。

Conclusion: Muskie 能有效学习到视图不变和几何相关特征，实现无监督3D理解，其作为骨干网络可广泛提升各类3D视觉任务表现。

Abstract: We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/

</details>


### [93] [PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures](https://arxiv.org/abs/2511.18116)
*Yuheng Shao,Lizhang Wang,Changhao Li,Peixian Chen,Qinyuan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法PromptMoE用于零样本异常检测（ZSAD），通过视觉引导的专家混合机制动态组合多种提示词，有效提高了对未知类别异常的检测与定位能力，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP等视觉-语言模型的ZSAD方法依赖于单一或固定的提示词工程，存在表达瓶颈和易过拟合的问题，难以处理复杂多样的异常模式。为提升泛化能力，需要能灵活组合和动态适应的提示学习方法。

Method: 作者提出PromptMoE方法，建立一组专家提示词池，作为可组合的语义基础，并引入视觉引导的专家混合机制(VGMoP)，针对每个实例动态稀疏地选用并组合不同专家提示词，实现更具表现力和适应性的文本表达。

Result: 在工业和医疗领域共15个数据集上进行实验，PromptMoE方法在识别和定位未知类别异常方面取得了优异和先进的性能。

Conclusion: 通过动态图文结合和专家提示混合，PromptMoE极大增强了ZSAD的泛化能力，有效克服了现有方法在提示工程上的瓶颈，为零样本异常检测提供了新思路。

Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.

</details>


### [94] [MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning](https://arxiv.org/abs/2511.18120)
*Hannuo Zhang,Zhixiang Chi,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的多视图立体（MVS）推理时适应方法MVS-TTA，提高了现有数据驱动MVS模型在新场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前学习型MVS方法依赖大规模数据训练，但模型一旦训练好后泛化能力有限。与之相对，优化型MVS可做场景自适应但效率低下。如何兼具高泛化和高效率，成为亟待解决的问题。

Method: 提出了一种测试时自适应框架MVS-TTA，通过自监督、跨视角一致性损失实现无监督辅助学习，在模型推理时进行适应。采用元辅助学习策略训练模型，使其在辅助任务更新下能获得更大受益。该方法与具体MVS架构无关，适配性强。

Result: 在标准数据集DTU和BlendedMVS，以及跨数据集泛化测试中，MVS-TTA在各类主流MVS模型上均显著提升了效果。

Conclusion: MVS-TTA首次将优化型推理时自适应与学习型MVS结合，推动了MVS模型的广泛适应性与性能提升。

Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.

</details>


### [95] [VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging](https://arxiv.org/abs/2511.18121)
*Ming Zhong,Yuanlei Wang,Liuzhou Zhang,Arctanx An,Renrui Zhang,Hao Liang,Ming Lu,Ying Shen,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了VCU-Bridge框架和HVCU-Bench基准，旨在使多模态大模型（MLLMs）具备更接近人类的分层视觉理解和推理能力，并验证了低层能力提升对高层推理和通用任务的促进作用。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在基准测试上表现出色，但其处理视觉信息的方式与人类不同，通常将细节与抽象概念孤立对待，缺乏层次化的整合。现有评测方法也未有效检测低层感知与高层推理间的联系，难以诊断模型瓶颈。该工作旨在填补这一空白，推动MLLMs获得更“类人”的视觉理解能力。

Method: 作者提出了VCU-Bridge分层视觉内涵理解框架，模拟人类由感知到推理的多层次语义桥接；构建了分层级、可诊断的基准集HVCU-Bench，并开发了基于MCTS的数据生成流程进行指令微调训练。同时通过实验系统研究了各层级任务之间的依赖和能力转移。

Result: 实验显示，现有MLLMs在推理层级升高时性能持续下降。通过强化低层感知能力，不仅层级推理效果显著提升，在通用多模态基准上也有明显增益（整体均值提升2.53%，MMStar提升7.26%）。

Conclusion: 分层视觉理解和推理对于提升MLLMs能力至关重要。通过类人分层学习和训练，能有效增强模型对复杂视觉-语义关系的建模和泛化能力，为后续多模态AI研究提供了方法和评测工具。

Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .

</details>


### [96] [Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models](https://arxiv.org/abs/2511.18123)
*Dachuan Zhao,Weiyue Li,Zhenda Shen,Yushu Qiu,Bowen Xu,Haoyu Chen,Yongchao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种称为Subspace Projection Debiasing（SPD）的视觉-语言模型去偏方法，系统地识别和移除隐含偏见的子空间，有效提升公平性同时保持下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）在多模态推理任务中表现卓越，但其隐式表征常包含并放大了性别、种族等人口统计学偏见，影响公平性和模型泛化能力。既有基于特征坐标置零的去偏方法存在特征缠结、跨数据集泛化差和偏见去除不彻底等根本性问题。

Method: 作者通过系统分析发现偏见分布于若干线性子空间中，而非少数独立特征坐标。为此，提出了一种新的几何方法SPD，自动识别和移除可被线性解码的偏见子空间，并将中性均值分量重新嵌入以维持语义信息。

Result: SPD方法在零样本分类、文本-图像检索、图像生成等多种任务和四项公平性指标上展示出平均18.5%的去偏提升，且与现有最佳去偏方法相比，对原任务性能影响极小。

Conclusion: SPD是一种高效且通用的去偏框架，能够在不显著牺牲模型表现的情况下，大幅提升视觉-语言模型的公平性与去偏鲁棒性。

Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.

</details>


### [97] [SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation](https://arxiv.org/abs/2511.18127)
*Ruicong Liu,Yifei Huang,Liangyang Ouyang,Caixin Kang,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了SFHand，首个支持语言引导的实时3D手部动作预测系统，能流式预测手部未来动作，并引入了同步手势与语言大数据集EgoHaFL，在准确性及下游任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D手部动作预测方法对人机交互应用（如增强现实和辅助机器人）不适用，因为它们需要离线处理，并且无法根据自然语言任务指令调整预测。

Method: 作者提出了流式自回归（autoregressive）预测架构SFHand，结合ROI增强记忆层，用于捕捉时序上下文并关注手部区域。模型输入持续的视频流与语言指令，输出包括手部类型、二维框、三维姿势和运动轨迹等详细未来状态。此外，作者同时发布了大规模同步3D手部与语言数据集EgoHaFL。

Result: SFHand在3D手部动作预测任务中达到了新的SOTA，超越以往方法最高达35.8%。并且，其学习的表示在下游实体操纵任务中的成功率提升了13.4%。

Conclusion: SFHand作为首个流式并能接受语言指令的3D手部预测框架，不仅在精度及任务泛化能力上大幅领先，还促进了该领域数据和方法的发展。

Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

</details>


### [98] [Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824)
*Alvin Wei Ming Tan,Jane Yang,Tarun Sepuri,Khai Loong Aw,Robert Z. Sparks,Zi Yin,Virginia A. Marchman,Michael C. Frank,Bria Long*

Main category: cs.CV

TL;DR: 本研究利用CLIP模型，自动分析婴儿视角日常视频中视觉与语言的对齐情况，发现理想的词物对齐场景在真实生活中非常罕见，与现有机器学习数据集存在差异。


<details>
  <summary>Details</summary>
Motivation: 幼儿在自然环境中如何将词语与对应的物体或概念关联起来，是语言学习中的核心难题，但此前关于他们视觉与语言经历时间同步性的了解有限，因为需要大量人工标注。

Method: 使用对比语言-图像预训练模型（CLIP），在大量家庭环境下婴儿视角的视频中，对视觉与语言的对齐进行自动评估，并将CLIP模型得分与人工标注进行比对验证。

Result: 理想化的词物对齐时刻（如说“看球”的同时视野中有球出现）在婴儿的日常经历中非常罕见，且对齐程度在不同婴儿间及婴儿自身不同时间点差异很大。

Conclusion: 早期词汇学习模型需要考虑现实中词物对齐机率低的限制。这项方法为考察儿童多模态学习环境提供了新工具。

Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

</details>


### [99] [Video4Edit: Viewing Image Editing as a Degenerate Temporal Process](https://arxiv.org/abs/2511.18131)
*Xiaofan Li,Yanpeng Sun,Chenming Wu,Fan Duan,YuAn Wang,Weihao Bo,Yumeng Zhang,Dingkang Liang*

Main category: cs.CV

TL;DR: 该论文提出通过视频时序建模的观点，利用视频预训练迁移先验，实现极高数据效率的图像编辑方法，相比主流方法，监督数据需求降至1%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型推动了跨模态的图像生成与编辑，但主流方法需要大量高质量审校的数据和成本高昂的模型训练，同时高度依赖精确的指令描述，限制了实用性。

Method: 作者将图像编辑问题视为视频的简化时序过程，借助视频预训练得到的单帧演化先验，迁移应用于图像编辑，仅需极少的标注数据即可微调模型。

Result: 实验证明，所提方法只用主流编辑模型约1%的监督数据，效果已能达到同类开源基线领先水平。

Conclusion: 将时序建模思想引入图像编辑，极大提升了训练数据效率，减少了成本，为指令驱动图像生成与编辑提供更高性价比的解决方案。

Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.

</details>


### [100] [From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149)
*Moazzam Umer Gondal,Hamad Ul Qudous,Daniya Siddiqui,Asma Ahmad Farhan*

Main category: cs.CV

TL;DR: 本论文提出了一种结合多服饰检测、属性推理和大语言模型提示的检索增强式自动服饰描述与标签生成框架。该方法突破了端到端模型在描述准确性和领域泛化性上的不足，能生成更具视觉基础且风格多样的文本描述与标签。


<details>
  <summary>Details</summary>
Motivation: 传统端到端自动服饰描述系统在属性准确还原和新领域泛化方面存在瓶颈，难以为时尚图片生成既具备事实依据又有趣味性的描述和标签。

Method: 本方法结合YOLO检测器进行服饰定位，利用k-means提取主色，CLIP-FAISS模块推断面料与性别属性，并通过结构化商品索引检索风格样本。这些证据共同输入大语言模型，指导其生成描述与标签，并用微调BLIP模型作为对比基线。

Result: YOLO检测器在九类服饰上获得0.71的mAP@0.5。RAG-LLM框架在属性齐全性上达到0.80，标签生成在50%阈值下完全覆盖，且描述丰富具事实基础。对比BLIP模型，RAG-LLM表现出更好的事实性与泛化能力。

Conclusion: 检索增强生成范式能高效、可解释地实现自动化、视觉基础鲜明的服饰内容生成，具有较强的实际应用推广潜力，并有效减少了虚假描述。

Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

</details>


### [101] [SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation](https://arxiv.org/abs/2511.18136)
*Chunming He,Rihan Zhang,Longxiang Tang,Ziyun Yang,Kai Li,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 本文提出了一种联合利用一致性约束和SAM伪标注的新框架SCALER，用于提升标签稀缺下的遮掩物体分割性能，实现了技巧互补和模型双向提升。


<details>
  <summary>Details</summary>
Motivation: 现有面向遮掩物体分割的少标签学习方法效果受限于目标的高度遮掩性和标注稀缺。一致性约束和SAM伪标注各有优劣，如何充分结合其优势以进一步提升分割结果，是该研究关注的主要问题。

Method: 提出SCALER协同训练框架，包括两个交替阶段：第一阶段通过加权机制在SAM监督下优化分割器；第二阶段则借助增强不变性和抗扰动损失，利用任务特性反过来优化SAM，使二者互为监督，实现互补增强。

Result: 在8个半监督和弱监督COS任务上取得了一致的性能提升，验证了SCALER的有效性和普适性。

Conclusion: SCALER不仅提升了小型分割网络，也有助于基础大模型的训练，适用于标签稀缺情形，可作为通用训练范式。

Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.

</details>


### [102] [Compact neural networks for astronomy with optimal transport bias correction](https://arxiv.org/abs/2511.18139)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li*

Main category: cs.CV

TL;DR: WaveletMamba是一种结合小波分解、状态空间建模、数学正则化和多层次偏差校正的新型天文图像分析框架，在高效率下实现高分辨率表现，并有效提升分类与红移预测。


<details>
  <summary>Details</summary>
Motivation: 天文图像存在分辨率和计算效率的权衡，这限制了大规模图像形态学分类与红移预测的研究进展。现有方法难以兼顾高分辨与高效率，且偏差问题未能充分解决。

Method: 提出了WaveletMamba框架，理论驱动地融合了小波分解、状态空间建模、数学正则化和多层级的偏差校正。引入分布级最优传输（HK距离）与样本级加权校正的方法，无需显式建模选择函数。

Result: 在64x64低分辨率下以仅354万参数达到了81.72%的分类准确率，并在高分辨率输入下（244x244）实现80.93%的准确率，同时计算效率提升9.7倍。通过多层次偏差校正，Log-MSE提升22.96%，异常值减少26.10%。还发现了分辨率多稳态现象。

Conclusion: 该框架理论严谨，实现了高效、全面偏差校正的科学AI，有望促进计算机视觉与天体物理的交叉创新，推动科学发现。

Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

</details>


### [103] [UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors](https://arxiv.org/abs/2511.18152)
*Chunming He,Rihan Zhang,Zheng Chen,Bowen Yang,CHengyu Fang,Yunlong Lin,Fengyang Xiao,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出UnfoldLDM，将深度展开网络与潜空间扩散模型相结合，有效解决盲图像复原中的退化特异性依赖和过度平滑问题，性能领先并可作为插件集成到现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络（DUNs）在处理盲图像复原（BIR）任务时，存在优化过程依赖已知退化模型、以及对图像细节复原不佳（过度平滑）的问题，限制了其实用性。

Method: 提出UnfoldLDM，每一阶段采用多粒度退化感知（MGDA）模块对未知退化进行建模和去除，在近端步骤中引入抗退化潜空间扩散模型（DR-LDM）提取退化不变先验，并通过过度平滑修正变换器（OCFormer）增强高频细节。该方法同时兼容现有DUN架构，支持插件式集成。

Result: UnfoldLDM在多种盲图像复原任务上表现领先，并提升了相关下游任务的效果。

Conclusion: UnfoldLDM有效解决了DUN在盲图像复原中的核心困难，是一种性能优越、可扩展性的通用框架。

Abstract: Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.

</details>


### [104] [Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design](https://arxiv.org/abs/2511.18163)
*Pasquale De Marinis,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 本文提出了首个专用于解释匹配式小样本语义分割（FSS）模型的方法——Affinity Explainer，能直观展示支持集像素对查询分割结果的贡献，并在多个基准上验证优越性。


<details>
  <summary>Details</summary>
Motivation: 虽然小样本语义分割在未知类分割中表现优异，但其决策过程难以解释。既有可解释性方法主要集中在传统视觉任务，并未针对FSS的问题、结构特性进行专门设计。缺乏可解释方法使得理解模型行为、选取支持样本困难，尤其在数据稀缺场景更为突出。

Method: 作者利用FSS模型“匹配”特性，设计Affinity Explainer。方法依据支持-查询特征之间的多层次匹配分数，生成突出显示支持集像素对查询分割决策贡献的归因图。并对现有可解释性指标进行扩展，提出适用于FSS的新评价标准。

Result: 在多个FSS基准数据集和多种模型上实验表明，Affinity Explainer显著压倒现有通用归因法，生成的解释图结构化、连贯，能反映模型关注区域并便于诊断。

Conclusion: 本 work 为FSS模型的解释性研究奠定基础，使得小样本分割系统更易理解及诊断，从而提升实际可靠性。

Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.

</details>


### [105] [Nested Unfolding Network for Real-World Concealed Object Segmentation](https://arxiv.org/abs/2511.18164)
*Chunming He,Rihan Zhang,Dingming Zhang,Fengyang Xiao,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度展开网络（NUN）框架，用于真实场景下的隐蔽物体分割，解决了现有方法目标冲突和恢复依赖先验的问题，并在多个基准测试上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度展开网络（DUN）的隐蔽物体分割方法存在图像恢复与背景估计目标相互冲突且依赖预定义退化类型的局限，这些假设在现实场景中并不适用。为了解决这些问题，需要一种能够自适应各种图像退化且分割恢复过程互不干扰的新框架。

Method: 文中提出嵌套展开网络（NUN），采用DUN-in-DUN结构，将抗退化的恢复网络（DeRUN）嵌入每个分割展开网络（SODUN）的阶段，实现恢复与分割的解耦及相互优化。通过融合视觉语言模型（VLM），DeRUN可动态理解退化语义并实现高质量图像恢复，SODUN则进行前景/背景的可逆估计。同时引入多阶段图像质量评估和自一致性损失，提升网络鲁棒性。

Result: 大量实验结果表明，NUN方法无论在干净还是退化数据集上均取得了优异的分割性能，超越了现有主流方法。

Conclusion: NUN实现了分割与恢复的解耦建模，对实际场景下的各种未知退化具有良好适应性和强鲁棒性，为隐蔽物体分割问题提供了更有效的解决方案。

Abstract: Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.

</details>


### [106] [EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses](https://arxiv.org/abs/2511.18173)
*Enrico Pallotta,Sina Mokhtarzadeh Azar,Lars Doorenbos,Serdar Ozsoy,Umar Iqbal,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的自我视角（egocentric）视频生成方法，能够通过3D身体姿态实现精确的运动控制，并生成高质量、一致性很强的视频。


<details>
  <summary>Details</summary>
Motivation: 当前自我视角视频生成面临难以实现细粒度（如身体姿势）控制的挑战。模拟和预测未来动作对于具身智能体在感知和规划等任务中至关重要，因此需要能精确操控身体动作的视频生成模型。

Method: 作者提出了EgoControl模型，基于扩散视频模型，结合自我视角数据训练。该方法通过显式的3D身体姿态序列引导未来帧生成，并提出了能同时捕捉相机和身体动态的新型姿态表征，作为控制信号嵌入扩散过程，从而实现精准运动控制和视频生成。

Result: 模型能够根据短暂观测帧和目标姿态序列，生成与给定姿态高度一致、时间连贯和视觉逼真的后续自我视角视频。实验表明，EgoControl在控制一致性和视频质量方面都表现优秀。

Conclusion: EgoControl为可控的具身视频模拟与理解提供了新的方法基础，有助于未来智能体在复杂环境中的动作预测和规划。

Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.

</details>


### [107] [Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera](https://arxiv.org/abs/2511.18174)
*Mukai Yu,Mosam Dabhi,Liuyue Xie,Sebastian Scherer,László A. Jeni*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的球面前端（USF）框架，能够高效地处理各种广角相机（如鱼眼、全景等）获取的图像，并在空间域实现球面卷积，解决了传统CNN应对球面图像的多项局限。


<details>
  <summary>Details</summary>
Motivation: 现代计算机视觉任务越来越依赖于鱼眼、全景等广角相机，但大多数深度学习模型依然用为针孔相机设计的平面CNN，导致物理空间邻域失真且对全局旋转敏感。频域球面CNN虽能部分缓解，但极为依赖高耗的球谐变换，限制了效率与分辨率。因此，寻找一种可泛化到各种镜头、能高效直接在球面空间运算的新方法十分迫切。

Method: 作者提出了Unified Spherical Frontend（USF）——一个镜头无关的通用前端，将任何已标定相机采集的图像通过射线方向映射到单位球体上，并直接在空间域进行球面重采样、卷积和池化。该方法的各模块（投影、取样、插值、分辨率）相互解耦，卷积核只依赖球面距离，从而可配置旋转等变性，并彻底摈弃了繁琐的球谐变换。

Result: 在Spherical MNIST、PANDORA、Stanford 2D-3D-S等多个合成和真实数据集上的分类、检测与分割任务中，USF相比普通平面CNN和频域球面CNN具有更好的旋转鲁棒性与跨镜头泛化能力。即使在极端镜头畸变、大视场和随意旋转的压力测试下，模型高分辨率处理下性能损失低于1%，且能实现“零样本”泛化到新镜头。

Conclusion: USF打破了传统平面CNN非物理邻接和不具旋转等变的瓶颈，高效实现镜头无关、空间域的球面特征学习和推理，为未来多视场、广角感知系统提供了通用高效的基础架构。

Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

</details>


### [108] [Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching](https://arxiv.org/abs/2511.18185)
*Yutong Wu,Yifan Wang,Qining Zhang,Chuan Zhou,Lei Ying*

Main category: cs.CV

TL;DR: 本文提出了一种生成虚拟随访CT扫描的新方法，有助于早期肺癌的诊断，并在实际临床数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期诊断对生存率影响极大，但早期信号和良性病变不易区分，且临床上患者往往需多次随访才能确诊，容易错失最佳治疗时机。现有AI方法大多只分析单个早期CT，难以解决随访诊断迟缓的问题。

Method: 作者提出CorrFlowNet，一种以扩散生成模型为基础的方法，可在仅有基线CT扫描的情况下生成虚拟的一年后随访CT，模拟结节随时间的动态变化。训练过程中，利用相关性自编码器将基线与随访CT共同编码到潜在空间，捕捉结节演变规律及其相关性，并在该潜在空间内采用流匹配算法（基于神经常微分方程）进行生成建模，结合辅助分类器提升诊断准确率。

Result: 在真实临床数据集上的评估显示，CorrFlowNet显著提升了肺结节风险评估的表现，优于现有基线模型。该方法的诊断准确率接近实际随访CT，展示了虚拟随访的临床应用潜力。

Conclusion: CorrFlowNet能有效生成高质量的虚拟随访CT扫描，提高肺结节恶性/良性早期判别能力，为临床早期干预提供新途径，有望优化肺癌诊断流程。

Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.

</details>


### [109] [ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization](https://arxiv.org/abs/2511.18192)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: 提出了一种新框架ARIAL，结合大型语言模型调度多种专用工具，实现高精度的文档视觉问答和答案定位，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文档VQA需要模型不仅准确抽取文本答案，还要精确定位答案在图像中的位置。这对高风险场景下的可解释性尤为关键，但现有系统没有很好平衡这两点。

Method: ARIAL采用模块化结构，将文档VQA划分为结构化子任务：利用TrOCR执行OCR文本抽取，用语义检索进行文档上下文选择，通过微调的Gemma 3-27B生成答案，最后通过文本与区域对齐显式定位答案。所有子任务由LLM为核心的规划Agent协调，实现每一步可追溯和工具级可审计。

Result: 在DocVQA、FUNSD、CORD、SROIE四个数据集上，ARIAL在文本准确率（ANLS）和空间精度（mAP）指标上都达到SOTA，其中DocVQA上分别获得88.7 ANLS和50.1 mAP，FUNSD上90.0/50.3，CORD上85.5/60.2，SROIE上93.1（与主流最优方法相比有显著提升）。

Conclusion: 通过智能调度多种专用工具，ARIAL同时提升文档VQA任务的性能和可解释性，为构建可信赖、可解释的文档AI系统提供新路径。

Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

</details>


### [110] [InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity](https://arxiv.org/abs/2511.18200)
*Haoming Wang,Qiyao Xue,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了InfiniBench，一个可以自动生成无限多样化、可自定义三维场景基准的工具，专为检验视觉-语言模型在复杂空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型评测基准缺乏充足的可定制性，无法针对不同场景复杂度和特定空间条件下有效分析模型的空间推理能力和失效模式。

Method: InfiniBench能够通过自然语言场景描述自动生成高真实感的3D视频，核心方法包括：（1）基于大语言模型的智能体框架，迭代细化场景约束；（2）集群式布局优化器，能生成高密度、杂乱复杂的场景；（3）基于任务的相机运动轨迹优化，实现全对象覆盖的视频渲染。

Result: 实验显示InfiniBench在提示符合度和物理可实现性方面，尤其在高复杂度场景下，优于现有程序化和LLM驱动的3D生成方法。此外，还能根据不同空间推理任务生成有代表性的测试基准。

Conclusion: InfiniBench为视觉-语言模型空间推理能力的全面、细致评估提供了强有力的工具，并推动了此类模型在复杂场景下的研究和应用。

Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.

</details>


### [111] [Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading](https://arxiv.org/abs/2511.18204)
*Pavan Narahari,Suraj Rajendran,Lorena Bori,Jonas E. Malmsten,Qiansheng Zhan,Zev Rosenwaks,Nikica Zaninovic,Iman Hajirasouliha*

Main category: cs.CV

TL;DR: 本论文提出了一种名为DIA的扩散模型系统，可以生成高保真、可控的人工囊胚图像，有效提升AI对试管婴儿囊胚形态的评估准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前囊胚形态评估依赖于主观的人为判断，且AI模型训练又受限于数据稀缺、类别不平衡和隐私保护等问题，导致难以标准化和提升准确性。

Method: 作者开发了基于潜在空间的扩散模型（DIA），根据Gardner分类和z轴焦点条件生成日5囊胚图像，并通过FID、记忆度量、“胚胎学家图灵测试”和多种分类任务严谨验证模型表现。

Result: DIA生成的图像能够以高真性欺骗专业胚胎学家，并且用于扩充不平衡或本已平衡的数据集时，AI分类准确率显著提升，部分场景下使用高达40%的合成数据也不会显著损失性能。

Conclusion: DIA模型为胚胎图像数据稀缺、类别不均衡提供了有效解决方案，可助力AI评估工具的性能提升和标准化应用。

Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.

</details>


### [112] [Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI](https://arxiv.org/abs/2511.18208)
*Ahmed Gomaa,Annette Schwarz,Ludwig Singer,Arnd Dörfler,Matthias Stefan May,Pluvio Stephan,Ishita Sheth,Juliane Szkitsak,Katharina Breininger,Yixing Huang,Benjamin Frey,Oliver Schnell,Daniel Delev,Roland Coras,Daniel Höfler,Philipp Schubert,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Dieter H Heiland,Udo S. Gaipl,Andrea Wittig,Rainer Fietkau,Christoph Bert,Stefanie Corradini,Florian Putz*

Main category: cs.CV

TL;DR: 本研究提出了一种利用自监督学习的大规模预训练深度学习方法，通过T1CE MRI和分割掩模，有效分辨脑转移瘤SRS术后放疗坏死与肿瘤进展，实现高准确率，并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 放射性坏死和肿瘤进展在影像学上极难区分，传统依赖活检，但活检具有侵入性且样本不足。深度学习模型又因缺乏足够生物学标注数据性能受限。自监督学习可利用大量无标注MRI数据，有望突破此瓶颈。

Method: 分两阶段：第一阶段，使用自监督的Vision Transformer（ViT）在10,167例无标注多中心T1CE MRI数据上进行预训练；第二阶段，将预训练模型用T1CE MRI与分割掩模的双通道输入，在公开MOLAB数据集（109例）微调，并选用20%原中心数据做测试，另用第二中心（28例）做外部验证。

Result: 自监督ViT在本中心测试集AUC达0.916，外部验证集AUC为0.764，均显著优于全监督ViT（AUC 0.624/0.496）和放射组学模型（AUC 0.807/0.691），p值均有统计学意义。多模态融合后表现进一步提升。注意力可视化显示模型关注临床相关病灶区域。

Conclusion: 基于无标注大规模影像预训练的自监督深度学习方法，能显著提升放疗坏死与肿瘤进展的鉴别准确性，且结果具可解释性、接近临床实际流程，值得进一步临床推广与验证。

Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.

</details>


### [113] [Using MLIR Transform to Design Sliced Convolution Algorithm](https://arxiv.org/abs/2511.18222)
*Victor Ferrari,Marcio Pereira,Lucas Alvarenga,Gustavo Leite,Guido Araujo*

Main category: cs.CV

TL;DR: 本文提出SConvTransform，一种用于MLIR的2D卷积优化转换方言扩展，通过切片分析和参数化仿射等式实现声明式的分块与打包高效转换，提升了在多种平台上的卷积性能。


<details>
  <summary>Details</summary>
Motivation: MLIR生态对2D卷积等关键算子的优化需求强烈，现有方法难以在不同硬件平台间复用高效策略，因此需要更模块化、声明式且可分析的转换流程。

Method: 设计SConvTransform扩展，核心为SConvOp操作，通过卷积切片分析，根据输入、卷积核形状以及目标硬件参数，自动判定分块和数据布局，运用参数化仿射方程实现统一的打包和分块；特殊情况（如分块不规则边缘）则自动拆分及调整仿射映射。

Result: 在ARM SME平台达峰值60%、Intel AVX512平台达67%峰值性能，展示良好性能。生成代码性能稳定适用于不同架构。

Conclusion: SConvTransform结合静态形状分析和结构化切分打包，适配多平台，结构可拓展、易集成，为MLIR卷积优化提供了有效新工具，后续将继续提升性能和平台适配性。

Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.

</details>


### [114] [Parallel qMRI Reconstruction from 4x Accelerated Acquisitions](https://arxiv.org/abs/2511.18232)
*Mingi Kang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端深度学习框架，可从4倍加速下的稀疏采样k空间数据中同时估计线圈敏感图并重建MRI图像，实现更流畅的重建效果。


<details>
  <summary>Details</summary>
Motivation: MRI扫描时间长，影响病人流通与造成运动伪影。现有加速并行MRI方法需高质量重建，但依赖预先计算的线圈敏感图，流程复杂，易受误差影响，因此需要更自动和鲁棒的重建方案。

Method: 提出一种两模块的端到端深度学习结构：一模块估计线圈敏感图，另一基于U-Net进行图像重建。输入仅需稀疏采样（4x）的k空间数据。实验证明在多线圈脑MRI数据（8回波，10被试）上优于传统SENSE重建。

Result: 与2x SENSE重建作为对照，本文方法能够产生视觉上更平滑的图像，虽在PSNR/SSIM指标略低，但视觉质量相当甚至优于SENSE。

Conclusion: 该方法简化了传统流程，实现了高质量自动MRI重建，发现空域错位等挑战，并提出未来改进方向。

Abstract: Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.

</details>


### [115] [EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning](https://arxiv.org/abs/2511.18242)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: 本文提出了EgoVITA，一个增强型的多模态大模型推理框架，通过结构化的计划与验证流程，有效提升了第一视角（egocentric）视频下推理意图和动作的能力，并大幅超过现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 第一人称视频与第三人称视频相比，因视角随主体不断变化，存在部分可观测性、视野有限和自相关运动等难题。现有多模态大模型难以处理上述挑战，缺乏对自我视角下未来意图与行动的有效推理。

Method: 提出EgoVITA框架，基于Group Relative Policy Optimization（GRPO）方法，分为两个阶段：1）第一视角计划阶段，从主体视角推理并逐步规划未来动作；2）第三视角验证阶段，转为观察者视角，检验计划的视觉和逻辑一致性。通过重复两阶段，模型可基于因果推理预测未来视觉观察，提升推理连贯性和视觉关联性。

Result: 在EgoBlind和EgoOrient等第一视角推理任务上，EgoVITA分别优于当前主流基线Qwen2.5-VL-7B 7.7 和 4.4个点，并在第三视角任务上保持良好泛化能力。

Conclusion: EgoVITA框架显著增强了多模态大模型在第一视角视频下的推理与规划能力，推动了该领域的发展，可为更多实际应用中的复杂自主推理任务提供支持。

Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.

</details>


### [116] [UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization](https://arxiv.org/abs/2511.18254)
*Siyi Li,Qingwen Zhang,Ishan Khatri,Kyle Vedder,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 本文提出了一种通用LiDAR场景流方法UniFlow，通过在多个多样化的LiDAR数据集上联合训练，显著提升了三维点云运动估计的泛化能力和效果。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR场景流方法通常只在单一传感器数据集上训练和评估，难以适应不同传感器和场景。已有语义分割和目标检测等任务表明，简单多数据集训练反而可能导致性能下降，因此需要新方法提升跨数据集的泛化能力。

Method: 提出了UniFlow模型家族，采用前馈神经网络结构，对来自不同传感器布局和点云密度的多个大型LiDAR场景流数据集进行统一联合训练，分析不同场景对模型泛化能力的影响。

Result: UniFlow在Waymo和nuScenes数据集上表现优异，性能分别超越以往方法5.1%和35.2%；在未见过的新数据集TruckScenes上也达到SOTA水平，超过专用模型30.1%。

Conclusion: 多数据集联合训练对场景流任务有效，运动估计算法对传感器变化的敏感性低，UniFlow模型能极大提升跨数据集的表现，适合广泛应用于多种LiDAR场景。

Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.

</details>


### [117] [Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization](https://arxiv.org/abs/2511.18255)
*Sina Mokhtarzadeh Azar,Emad Bahrami,Enrico Pallotta,Gianpiero Francesca,Radu Timofte,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAVi-DNO的新方法，可持续优化视频预测扩散模型中的噪声，从而提升对连续视频流未来帧的预测能力，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 面对越来越多连续视频流的应用场景，现有扩散模型虽然预训练效果好，但难以在持续变化的新数据流中自适应更新，且直接参数微调计算开销大。本文希望让模型能高效适应新视频流信息，从而提升预测性能。

Method: 提出Sequence Adaptive Video Prediction with Diffusion Noise Optimization（SAVi-DNO）方法，在不微调原有大模型参数的情况下，通过推理时自适应优化扩散噪声，让模型针对新观测到的视频流自适应调整采样噪声。

Result: 在Ego4D、OpenDV-YouTube、UCF-101和SkyTimelapse等长视频数据集上，SAVi-DNO在FVD、SSIM、PSNR等指标上性能优于基线方法，能够更精确地预测未来视频帧。

Conclusion: SAVi-DNO无需高昂的模型参数微调成本，通过优化扩散噪声显著提升了扩散模型对连续视频流的预测能力，在长视频预测任务上取得了实质性进步。

Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.

</details>


### [118] [MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2511.18262)
*Tao Shen,Xin Wan,Taicai Chen,Rui Zhang,Junwen Pan,Dawei Lu,Fanding Lei,Zhilin Lu,Yunfei Yang,Chen Cheng,Qi She,Chang Liu,Zhenbang Sun*

Main category: cs.CV

TL;DR: 本文提出了MammothModa2（Mammoth2）模型，一种统一的自回归-扩散（AR-Diffusion）架构，实现了高语义理解与高保真图像生成的结合，在多个公开基准上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 目前统一多模态模型在整合语义推理与高保真视觉合成方面仍面临挑战，尤其是如何将离散的语义推理与连续的图像生成有效结合。本文的研究动机是寻求一种架构，既能执行强大的语义理解，又能进行高质量的生成和编辑，提升多模态统一模型的性能和实用性。

Method: Mammoth2采用串行设计，将自回归路径（AR）用于全局语义建模，结合扩散Transformer（DiT）解码器用于高保真图像生成，并通过特征对齐模块将AR表征与扩散连续表征稳定结合。模型端到端训练，采用联合下一步预测和流匹配目标，并辅以监督微调及生成与编辑任务上的强化学习。

Result: 在无预训练生成器、使用约6000万条监督生成样本的情况下，Mammoth2在GenEval、DPGBench、ImgEdit等多个公开生成与编辑任务基准上取得了领先成绩（0.87、87.2、4.06），同时在多模态理解方面与理解型大型模型相竞争。

Conclusion: AR-Diffusion耦合架构可以在保持参数和数据高效的基础上，兼顾高保真生成/编辑能力与强大的多模态理解能力，是实现统一多模态模型的一种有效方案。

Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

</details>


### [119] [SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors](https://arxiv.org/abs/2511.18264)
*Ruijie Fan,Junyan Ye,Huan Chen,Zilong Huang,Xiaolei Wang,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出了一种零样本卫星视频目标跟踪方法SatSAM2，通过引入KFCMM和MCSM两大核心模块，提升了跟踪的鲁棒性和泛化能力，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前卫星视频跟踪方法泛化能力差、需要特定场景训练，并且易受遮挡影响导致丢失目标，限制了其在遥感领域的应用。

Method: 提出基于SAM2的SatSAM2算法，引入基于卡尔曼滤波的约束运动模块（KFCMM）抑制漂移、提升时间连贯性，以及运动约束状态机（MCSM）提高对跟踪状态的动态调控能力。此外，提出了MVOT基准数据集，用于大规模评测。

Result: SatSAM2在两个卫星跟踪基准和MVOT数据集上均取得优于传统和基础模型方法的性能，在OOTB数据集上AUC提升5.84%。

Conclusion: SatSAM2显著提升了卫星视频跟踪的鲁棒性和通用性，为遥感领域提供了更强大的跟踪工具，相关代码与数据集将公开推动后续研究。

Abstract: Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.

</details>


### [120] [Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models](https://arxiv.org/abs/2511.18271)
*Tianyang Han,Junhao Su,Junjie Hu,Peizhen Yang,Hengyu Shi,Junfeng Luo,Jialin Gao*

Main category: cs.CV

TL;DR: 本文提出了PicWorld，一个全面评估文本到图像（T2I）模型对隐性世界知识和物理因果推理能力的基准，并发现现有模型在这些方面均存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 虽然T2I模型可以生成高质量且符合指令的图像，但它们在需要隐性世界知识的任务中常常失败。现有评估方式未能充分检测模型在知识落地、多物理交互、可审计证据等多维度能力的不足，因此有必要设计更全面的评测体系。

Method: 作者提出了PicWorld基准，包含1100个涵盖三大核心类别的提示，并提出PW-Agent多智能体评估器，通过分解提示、证据验证等方式，从物理真实感和逻辑一致性多个层次对模型生成的图像进行细粒度评测。作者还对17个主流T2I模型进行了系统分析。

Result: 分析显示，所有主流T2I模型在隐性世界知识和物理因果推理能力上都普遍存在基本性的局限，且表现有一定差异。

Conclusion: 作者认为，未来T2I系统需要更加关注推理和知识整合，以补足当前模型在理解和展现世界知识与因果性上的不足。

Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.

</details>


### [121] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: 本文首次系统性评价了在医疗文档OCR中利用视觉token遮罩保护隐私的有效性，发现只用视觉层遮罩难以完全防止敏感信息泄露，需结合NLP等后处理方案。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉-语言模型（VLMs）被广泛应用于医疗OCR，敏感健康信息（PHI）在文档处理中易被泄露，需探索有效的隐私保护机制以符合HIPAA等合规要求。

Method: 本文以DeepSeek-OCR为基础，提出了七种在不同视觉模型结构层级实施的遮罩（V3-V9），并在100份带精确标注的合成医疗账单上评估PHI去除效果。进一步通过消融实验分析不同空间覆盖半径对遮罩效果的影响，并测试与NLP后处理结合的混合方案。

Result: 所有视觉遮罩策略对总PHI实现了42.9%的去除，能100%抑制长文本空间型标识如姓名、地址，但对短结构化标识如医保号、社保号完全无效。扩大遮罩半径无助提升抑制效果。结合NLP后处理的混合策略可达88.6%的PHI去除。

Conclusion: 单靠视觉层遮罩无法彻底实现医疗文档中的PHI保护，对混合结构化短标识需引入NLP级后处理。本文明确了视觉隐私干预的能力边界，为未来医疗OCR安全防护的设计和方向提供建议。

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [122] [Point-to-Point: Sparse Motion Guidance for Controllable Video Editing](https://arxiv.org/abs/2511.18277)
*Yeji Song,Jaehyun Lee,Mijin Koo,JunHoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种利用锚点令牌（anchor tokens）的新型运动表示方法，有效提升了视频编辑时的动作与内容保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在动作和编辑保真度间通常需要权衡，主要由于其运动特征要么对原始布局过度拟合，要么是隐式定义，难以兼顾多样性和可控性。如何在多样场景下，在编辑主体的同时精准保留运动，一直是视频编辑难题。

Method: 作者创新性地提出了“锚点令牌”这一新的运动表示方法，通过少量信息量大的点轨迹编码视频动态，并结合视频扩散模型的丰富先验知识。这些锚点令牌可灵活迁移，有助于与新主体对齐。基于此，作者设计了Point-to-Point方法。

Result: 实验表明，用锚点令牌的方法可以获得更可控且语义对齐度更高的视频编辑结果，在编辑与运动保真度上优于现有方法。

Conclusion: 研发的Point-to-Point方法克服了以往运动表示的关键局限性，实现了跨多场景高质量的视频编辑，并提升了控制性和保真度。

Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.

</details>


### [123] [Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation](https://arxiv.org/abs/2511.18281)
*Yara Bahram,Melodie Desbos,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种将蒸馏和自适应统一为单阶段过程的新方法Uni-DAD，能以极少采样步数生成高质量、具多样性的图像，超越当前主流两阶段训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采样生成高质量图像但速度慢，蒸馏模型虽较快但难以高效适应新领域。目前主流的两阶段训练流程（如先适应再蒸馏或先蒸馏再适应）会增加复杂度并影响生成质量和多样性。因此需要一种高效、简单、且在新领域依然表现优秀的生成方法。

Method: 作者提出Uni-DAD单阶段训练流程，训练时结合：1）双域分布匹配的蒸馏目标，将学生模型引导至源领域教师和目标领域教师的分布；2）多头生成对抗网络（GAN）损失，提高目标域的真实感并抑制过拟合。源域蒸馏保持多样性，多头GAN稳定训练以适应小样本场景，同时通过目标教师实现对结构差异大的新领域适应。

Result: 在小样本图像生成和主体个性化等任务上，Uni-DAD不仅以极少（4步以下）采样步生成高质量图像，还在质量和多样性上超越现有主流的两个阶段的方法。

Conclusion: Uni-DAD消除了两阶段训练的繁琐流程，在新领域实现了更快、更优质、更具多样性的图像生成，有望成为高效且通用的扩散模型域自适应新范式。

Abstract: Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.

</details>


### [124] [RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System](https://arxiv.org/abs/2511.18286)
*Runwei Guan,Rongsheng Hu,Shangshu Chen,Ningyuan Xiao,Xue Xia,Jiayang Liu,Beibei Chen,Ziren Tang,Ningwei Ouyang,Shaofeng Liang,Yuxuan Fan,Wanjie Sun,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出了一个针对路测场景的视觉问答（VQA）数据集RoadSceneVQA，创新性地结合多模态大模型和人类类场景锚定机制，提升了交通行为推理和自然语言交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有路测感知系统侧重于实例级识别，难以支持自然语言交互及交通行为语境推理。为填补此空白，需要设计能挑战模型常识推理和语境理解能力的新型数据集与方法。

Method: 1. 构建RoadSceneVQA数据集，包含34736个多样化QA对，涵盖不同天气、光照和交通状况，任务覆盖对象属性、意图、合法性及交互模式。
2. 提出CogniAnchor Fusion（CAF）模块，以人类类情景锚定机制提升视觉-语言融合能力。
3. 引入Assisted Decoupled Chain-of-Thought（AD-CoT），用CoT提示和多任务学习增强推理。
4. 基于上述方法建立基线模型RoadMind。

Result: 实验表明，RoadMind在RoadSceneVQA和CODA-LM两个基准上，在推理准确性和计算效率方面均实现了持续提升，达到结构化交通感知和推理任务的最新水平。

Conclusion: RoadSceneVQA数据集及所提方法显著提升了多模态大模型在实际交通场景下的推理与感知能力，为推动路测智慧感知系统的发展提供了有力基础。

Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.

</details>


### [125] [SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes](https://arxiv.org/abs/2511.18290)
*Jungho Lee,Minhyeok Lee,Sunghun Yang,Minseok Kang,Sangyoun Lee*

Main category: cs.CV

TL;DR: 该论文提出了SwiftVGGT方法，实现了高质量且高效的大规模三维重建，无需外部VPR模型，重建速度极快，准确性优异。


<details>
  <summary>Details</summary>
Motivation: 大规模三维重建任务在精度与计算效率之间难以兼顾。传统方法要么牺牲速度换取重建质量，要么重建速度快但质量差，为此需要一种兼顾速度和质量的创新方案。

Method: SwiftVGGT是一种无需训练的三维重建方法，不依赖外部视觉定位（VPR）模型即可完成环闭检测，有效减少冗余计算，在大规模（千米级）场景下依然保持重建的全局一致性。同时，采用一种基于Sim(3)-SVD的点采样对齐方法，成功取代了以往常用的IRLS优化，大幅提升运算速度。

Result: 实验证明，SwiftVGGT在多个数据集上实现了当前最优的重建质量，且推理时间仅为现有VGGT方法的1/3左右。

Conclusion: SwiftVGGT显著提升了大规模三维重建的效率和质量，为实际应用奠定基础。

Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.

</details>


### [126] [DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition](https://arxiv.org/abs/2511.18305)
*Raja Kumar,Arka Sadhu,Ram Nevatia*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练方法DiVE-k，通过利用模型自身top-k预测生成多项选择题，并用强化学习进行训练，从而显著提升大规模视觉语言模型（LVLMs）在细粒度图像识别中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LVLMs虽然具有丰富文本知识，但在细粒度图像识别、区别相似类别时表现不佳。现有基于强化学习的精调方法依赖准确匹配奖励，容易导致模型记忆训练类别，缺乏泛化能力，不能有效实现差异化推理。

Method: DiVE-k利用每张训练图片对应模型top-k预测结果生成多项选择题，并采用强化学习指导模型选择正确答案。这种方式促使模型在多个合理候选中进行差异化推理，并提供了简单且有效的奖励信号，抑制死记硬背行为，提升泛化能力。

Result: 在五个主流细粒度识别数据集上，DiVE-k显著优于现有方法。在标准的base-to-novel泛化设置下，DiVE-k对比QWEN2.5-VL-7B和ViRFT在Harmonic Mean指标上分别提升10.04%和6.16%。混合领域和小样本设置下也取得了类似效果。

Conclusion: DiVE-k通过差异化多项选择训练方式，有效改善了LVLMs在细粒度识别任务中的泛化与推理能力，为模型解决同类细分问题提供了新思路。

Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.

</details>


### [127] [ScriptViT: Vision Transformer-Based Personalized Handwriting Generation](https://arxiv.org/abs/2511.18307)
*Sajjan Acharya,Rajendra Baskota*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉Transformer风格编码器和交叉注意力机制的端到端手写风格生成系统，能够更好捕捉和还原作者独特的书写风格。


<details>
  <summary>Details</summary>
Motivation: 现有GAN、Transformer和扩散模型虽然在手写体风格生成上取得进展，但在捕捉如倾斜度、曲率、笔压等全局书写风格特征上存在不足，导致生成文本缺乏作家独有的风格一致性。因此，亟需一种能更好反映长距离结构特征的方法。

Method: 该方法提出了基于Vision Transformer的风格编码器，通过多个参考图像学习手写全球风格特征，再借助交叉注意力机制将目标文本与风格特征结合。此外，引入SSAA（显著笔画注意力分析）用于解释模型在风格迁移时关注的关键笔画，使生成过程更具可解释性。

Result: 实验表明，该方法能更准确生成具有作家独特风格且结构一致的手写文本，同时提供了对生成过程的更好可解释性。

Conclusion: 综上所述，本文提出的框架在手写风格转移中提升了风格一致性和可解释性，有望成为手写体个性化合成领域的新方向。

Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.

</details>


### [128] [Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification](https://arxiv.org/abs/2511.18316)
*Subhajeet Das,Pritam Paul,Rohit Bahadur,Sohan Das*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer（ViT）和转移学习的方法，通过结合Bi-GRU神经网络，实现了对脑卒中CT影像的早期识别，准确率达94.06%。


<details>
  <summary>Details</summary>
Motivation: 脑卒中致残致死率高，早期识别对于治疗至关重要。传统CT人工分析效率低且易出错，因此亟需自动化、高效且准确的检测方法。

Method: 本研究采用了预训练的ViT模型，部分编码器层权重冻结，仅微调部分层以适应脑卒中特征提取。提取的特征输入到单层Bi-GRU进行分类，并通过数据增强方法缓解类别不平衡问题。

Result: 所提模型在Stroke Dataset上实现了94.06%的脑卒中分类准确率。

Conclusion: 基于ViT与Bi-GRU的自动分类系统能够有效提升CT影像脑卒中早期识别的准确率和效率，有望辅助临床快速诊断。

Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

</details>


### [129] [Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement](https://arxiv.org/abs/2511.18317)
*Dongcai Tan,Shunkun Liang,Bin Li,Banglei Guan,Ang Su,Yuan Lin,Dapeng Zhang,Minggang Wan,Zibin Liu,Chenglong Wang,Jiajian Zhu,Zhang Li,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新的立体标定框架，通过自动生成最优拍摄姿态，实现更高精度和更高效率的三维变形测量校准。


<details>
  <summary>Details</summary>
Motivation: 当前三维数字图像相关（DIC）等立体测量方法的标定步骤缺乏高效、直观的最优姿态指导，既影响效率也影响测量精度，因此亟需改进校准流程。

Method: 提出了一种集成相对和绝对外参联合优化的姿态优化方法，以协方差矩阵迹最小化为损失函数自动计算下一步最优标定姿态，并配合开发了指导性图形用户界面，引导用户采集高质量校准图片。

Result: 所提方法较随机姿态采集标定，能以更少的图片获得更优的标定精度，具有更强鲁棒性。实际热变形测量中变形量及其演化趋势与有限元仿真高度一致。

Conclusion: 新方法可大幅提升三维变形测量系统的标定效率和精度，具有较强应用潜力，适用于学术与工程实际三维变形测量场景。

Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation

</details>


### [130] [General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification](https://arxiv.org/abs/2511.18326)
*Helia Abedini,Saba Rahimi,Reza Vaziri*

Main category: cs.CV

TL;DR: 本文比较了医学领域特定预训练模型和通用大数据集预训练模型在小样本脑肿瘤MRI分类任务中的表现，发现以通用大数据集为基础的现代深度CNN表现更佳。


<details>
  <summary>Details</summary>
Motivation: 随着深度卷积神经网络（CNN）在医学图像分析中的广泛应用，预训练模型成为提升性能的关键，但目前尚不清楚在小数据集场景下，采用领域特定医学预训练模型是否优于通用大数据集模型。因此，需要系统评估两者在脑肿瘤分类任务中的有效性。

Method: 作者选取了RadImageNet DenseNet121（医学领域预训练）、EfficientNetV2S和ConvNeXt-Tiny（通用模型）三种CNN架构，在同一小规模MRI脑肿瘤数据集上以完全一致的设置进行训练和微调，对比其分类性能。

Result: ConvNeXt-Tiny获得最高分类准确率，EfficientNetV2S次之，而虽然RadImageNet DenseNet121经过医学领域预训练，但泛化性能较差，表现为准确率低且损失高。

Conclusion: 在数据有限的情况下，基于大规模通用数据集预训练的现代深度CNN模型相比医学领域特定预训练模型，在医疗影像迁移学习任务中表现更优。医学领域特定预训练的模型未必具备更好的泛化能力。

Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.

</details>


### [131] [SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters](https://arxiv.org/abs/2511.18329)
*Shohei Tanaka,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 本文提出了SciPostLayoutTree数据集，并针对学术海报的结构分析开发了高效算法，提升了对空间复杂关系的解析能力。


<details>
  <summary>Details</summary>
Motivation: 学术海报在科研交流中作用重要，但其结构分析尚不充分，尤其是在阅读顺序和父子关系的自动识别方面。现有研究大多聚焦于论文文本，忽略了海报的特殊结构及可视化特点，导致界面和辅助工具难以精准支持海报信息的获取。

Method: 作者构建了包含约8,000份学术海报的SciPostLayoutTree数据集，对每份海报标注了阅读顺序和父子结构关系。提出了Layout Tree Decoder模型，将视觉特征与位置和类别等包围盒特征结合，通过beam search提升关系预测的连贯性和准确性，特别关注空间上更具挑战性的结构关系。

Result: 实验结果显示，所提模型在空间复杂关系的预测准确率上优于现有方法，为学术海报结构分析建立了新基线。公开的数据集和代码推动了学术海报结构化分析研究。

Conclusion: 本研究补足了学术海报结构分析领域的空白，为后续相关界面开发与学术交流自动化工具奠定了基础。SciPostLayoutTree数据集和模型为进一步研究提供了丰富资源和有效方法。

Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.

</details>


### [132] [ConsistCompose: Unified Multimodal Layout Control for Image Composition](https://arxiv.org/abs/2511.18333)
*Xuanke Shi,Boxuan Li,Xiaoyang Han,Zhongang Cai,Lei Yang,Dahua Lin,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了ConsistCompose，一个新的统一多模态模型，可通过在语言提示中直接嵌入布局坐标，实现多实例、可布局控制的图像生成，并推出了大规模数据集ConsistCompose3M，显著提升了空间精度和多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前多模态模型虽然在视觉和语言结合方面取得进步，但大多聚焦于视觉定位，缺少对语言嵌入的布局可控多实例生成（LELG）的研究，导致对生成内容的精细组合和空间控制能力有限。作者希望解决这一空白，实现对生成图像实例的精确空间控制。

Method: 作者提出ConsistCompose框架，将布局坐标直接嵌入语言提示，通过“实例-坐标绑定提示”和“坐标感知无分类引导”将语言中的空间信息传递到生成模型，无需为各任务设计专用分支。同时，构建了包含3.4M配有布局和身份标注的数据对的大型数据集，用于大规模训练。

Result: 在COCO-Position和MS-Bench两个基准数据集上的实验表明，ConsistCompose在保持身份一致性和通用理解能力的同时，大幅提升了布局控制下的空间准确率，并优于其他可布局控制的主流基线方法。

Conclusion: ConsistCompose通过统一的多模态建模方式，显著提升了多实例、可空间布局控制的图像生成准确性和多模态理解表现，为布局可控的多模态生成建立了新的范式。

Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.

</details>


### [133] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文针对低空无人机多目标视觉跟踪任务，提出了首个大规模多模态无人机跟踪数据集MM-UAV，并基于此设计了新的多模态跟踪框架，在30余复杂场景中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单一视觉模态（如仅用RGB）在复杂环境中难以应对低照度、遮挡、快速运动等挑战，而现有缺乏公开、多模态（RGB、红外、事件）无人机跟踪数据集限制了技术发展。

Method: 1）发布集成RGB、红外和事件信号的MM-UAV多模态数据集，涵盖1321段同步多模态序列与280万标注帧；2）提出双创新模块的UAV多模态跟踪框架：一是offset-guided自适应对齐模块处理多传感器空间误差，二是自适应动态融合模块优化多模态信息互补性；3）引入事件增强关联机制，提高多目标身份保持的可靠性。

Result: 实验结果表明，所提出的框架在MM-UAV数据集及多种复杂场景下均显著优于现有多目标跟踪方法，取得更高的鲁棒性和跟踪准确度。

Conclusion: MM-UAV数据集和配套新方法为多模态无人机目标跟踪研究提供了基础平台和新的技术基线，将推动该方向的进一步研究与发展。

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [134] [FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement](https://arxiv.org/abs/2511.18346)
*Wenshuo Gao,Junyi Fan,Jiangyue Zeng,Shuai Yang*

Main category: cs.CV

TL;DR: 本文提出FlowPortal，无需训练即可实现视频重光照和背景更换，兼顾时间一致性、结构保真和照明自然。通过残差修正流机制、解耦条件设计和高频传递，有效提升重光照真实感和细节。实验证明方法高效并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照与背景更换方法难以同时保证时间一致性、空间结构完整性和照明自然度，制约了在电影制作和创意媒体中的实际应用。作者旨在解决这些痛点，提升生成视频质量和实用性。

Method: 1) 核心提出残差修正流（Residual-Corrected Flow）机制，将常规流模型转变为可编辑的模型，实现完全重构和真实重光照。2) 解耦条件设计，便于精准控制光照变化。3) 高频信息传递，增强细节保留。4) 分离前景重光照与背景生成，通过掩码策略提升整体效果。

Result: FlowPortal在时间连贯性、结构保留、光照真实感等方面优于现有方法，且无需专门训练，效率高。

Conclusion: FlowPortal为无需训练下的视频重光照与背景替换任务提供了有效、高效且真实感强的解决方案，为影视与创意媒体应用提供了新的技术路径。

Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.

</details>


### [135] [MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference](https://arxiv.org/abs/2511.18352)
*Zitong Xu,Dake Shen,Yaosong Du,Kexiang Hao,Jinghan Huang,Xiande Huang*

Main category: cs.CV

TL;DR: 本文提出了UniPrefer-100K大规模用户偏好数据集，并基于此提出了MagicWand生成与评估代理，可更好地根据用户偏好生成和优化AIGC内容。还推出了UniPreferBench基准，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管AIGC模型在图像和视频生成方面取得巨大进展，但用户难以通过复杂提示词获得理想内容，且缺乏保留和反映用户偏好的机制。因此需要新的方法帮助模型理解并对齐用户偏好。

Method: 构建包含图片、视频和用户偏好描述的UniPrefer-100K数据集。在此基础上，提出MagicWand代理，自动增强提示词、利用先进生成模型生成内容、并据用户偏好进行评价与优化。同时提出UniPreferBench大规模基准，用以评测各方法在用户偏好对齐上的表现。

Result: 实验表明，MagicWand在UniPreferBench各类任务中均能更好地生成并评价出符合用户偏好的内容，效果优于现有方法。

Conclusion: 该方法在AIGC内容生成和评估上显著提升了与用户偏好的对齐能力，为后续相关研究和应用提供了有价值的工具和数据集。

Abstract: Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.

</details>


### [136] [TRANSPORTER: Transferring Visual Semantics from VLM Manifolds](https://arxiv.org/abs/2511.18359)
*Alexandros Stergiou*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务L2V（logits-to-video），以及与模型无关的方法TRANSPORTER，用于生成反映视觉语言模型（VLM）预测背后规则的视频，为模型可解释性提供高保真度的新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管当前的视觉语言模型能够处理复杂场景，但其内部推理过程难以理解和控制。受到文本到视频生成模型研究进展的启发，作者希望用可视化手段加深对VLM决策机制的理解。

Method: 提出L2V任务，并设计TRANSPORTER方法：通过最优传输将VLM的高语义嵌入空间与高视觉保真度的视频生成模型（T2V）耦合，用VLM输出的logit分数指导条件视频生成，从而展现不同语义（如属性、动作、场景变化）对生成结果的影响。

Result: 定量和定性评测表明，TRANSPORTER生成的视频能够细致反映VLM在多对象属性、动作副词和场景上下文的语义变化。实验结果验证了该方法在提升VLM可解释性方面的有效性。

Conclusion: L2V搭配TRANSPORTER方法为分析VLM工作机制、解释其预测提供了高保真、创新性的新途径，推动了视频理解模型可解释性研究的进展。

Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.

</details>


### [137] [Alias-free 4D Gaussian Splatting](https://arxiv.org/abs/2511.18367)
*Zilong Chen,Huan-ang Gao,Delin Qu,Haohan Chi,Hao Tang,Kai Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的4D高斯溅射采样方法，通过自适应滤波和损失函数消除因分辨率变化引起的高频伪影，并提升多视角场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯溅射的动态场景重建虽可实时渲染，但在调整焦距或高斯与相机距离以改变渲染分辨率时，常因高斯采样频率和滤波错误导致明显伪影，降低渲染质量。

Method: 作者推导了4D高斯溅射的最大采样频率理论，提出一种4D可调尺度滤波器及相应尺度损失函数，能灵活调节采样频率，适配不同分辨率渲染需求。

Result: 新方法在单目和多视角视频重建实验中，显著消除了高分辨率渲染时的高频伪影，同时能有效减少冗余高斯，提高多视重建效率。

Conclusion: 提出的尺度自适应过滤和损失约束，能够提升4D高斯溅射用于动态场景重建的图像质量及资源利用，具备实际应用前景。

Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/

</details>


### [138] [MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer](https://arxiv.org/abs/2511.18370)
*Zenghao Chai,Chen Tang,Yongkang Wong,Xulei Yang,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: MimiCAT是一种全新的级联Transformer模型，能够实现不同行为体类别间的3D姿态转移，在多结构角色下表现优异，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D姿态转移方法只适用于结构相似的角色，难以通用于如类人到四足动物等类别自由的场景。主要挑战是不同类型角色的结构和变换多样性导致区域对应关系不准确，迁移效果差。

Method: 作者首先构建了一个包含百万级姿态、涵盖数百种角色的大型数据集。提出了MimiCAT模型：通过利用语义关键点标签学习柔性多对多映射，避免严格的一一对应。整体采用级联Transformer架构，将转移过程建模为条件生成：先通过柔性对应将源姿态投影到目标，然后结合目标形态对结果精细调整。

Result: 实验表明，MimiCAT能够在不同类型角色之间迁移真实合理的3D姿态，无论定性还是定量实验均显著优于仅支持窄类别（如类人到类人）迁移的现有方法。

Conclusion: MimiCAT实现了类别无关的3D姿态转移，在结构和转化多样性大的场景下表现优异，为三维动作建模等应用带来新可能。

Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).

</details>


### [139] [MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models](https://arxiv.org/abs/2511.18373)
*Xiyang Wu,Zongxia Li,Jihui Jin,Guangyao Shi,Gouthaman KV,Vishnu Raj,Nilotpal Sinha,Jingxi Chen,Fan Du,Dinesh Manocha*

Main category: cs.CV

TL;DR: 本文提出了一种增强视觉语言模型（VLMs）物理推理能力的方法，并配套发布了MASS-Bench物理推理数据集，显著提升了VLMs在物理相关视频理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽然在常规视频任务上表现良好，但在涉及物理推理（如运动动力学、空间交互）的问题上能力有限，导致其难以理解和生成物理一致性的视频内容。因此亟需提升VLM对物理世界动态的理解和推理能力。

Method: 1）提出MASS-Bench数据集，包含4,350个真实和AI生成视频，以及8,361条物理推理相关的问题-答案对，并提供丰富的标注信息（视觉检测、子片段定位和3D运动追踪）；2）提出MASS方法，通过深度驱动的3D编码和视觉绑定将空间-时间信号注入VLM语言空间，结合运动追踪器刻画物体动态，并用强化微调增强跨模态对齐和推理能力。

Result: 实验结果表明，所提出的方法在MASS-Bench上比同等规模及更大规模的模型和其它最新方法分别提升了8.7%和6.0%，在物理推理和理解任务上达到接近主流闭源模型（如Gemini-2.5-Flash）的表现。

Conclusion: 该工作证实了通过增强空间-时间建模与跨模态对齐，可以有效提升VLMs的物理世界动态理解和推理能力，为生成更符合物理规律的视频内容提供了基础。

Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

</details>


### [140] [Synthetic Curriculum Reinforces Compositional Text-to-Image Generation](https://arxiv.org/abs/2511.18378)
*Shijian Wang,Runhao Fu,Siyi Zhao,Qingqin Zhan,Xingjian Wang,Jiarui Jin,Yuan Lu,Hanqian Wu,Cunjian Chen*

Main category: cs.CV

TL;DR: 提出了一种新的组合式课程强化学习方法CompGen，通过更有针对性的训练提高文本到图像生成模型在复杂场景下的组合生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统文本到图像生成方法在处理包含多个对象及其复杂属性和关系的场景时存在表现不佳的问题，主要体现在对对象布局和对象间相互作用的理解和表现不足。

Method: 作者提出了CompGen框架，创新性地利用场景图进行构图难度评判，并通过自适应的MCMC图采样算法为模型生成有难度分级的训练数据，然后结合课程强化学习进一步优化模型。该方法被集成到GRPO优化方法中，并尝试了多种课程进度安排策略。

Result: 实验结果显示，CompGen在不同课程调度策略下展现出不同的提升曲线，其中“由易到难”和高斯采样策略优于随机采样。此外，CompGen对基于扩散和自回归的T2I模型的组合生成能力均有显著提升。

Conclusion: CompGen显著提升了文本到图像生成系统的组合生成能力，尤其适用于需要处理复杂多对象场景的T2I任务。

Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.

</details>


### [141] [RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models](https://arxiv.org/abs/2511.18380)
*Timing Yang,Guoyizhe Wei,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: 本文系统性分析了Mamba在视觉任务中的表现和机制，并提出三项主要贡献：理论剖析、评估新方法、性能验证。


<details>
  <summary>Details</summary>
Motivation: 尽管Mamba在视觉任务中表现突出，但其如何有效建模和解释视觉表征仍不清楚。作者希望填补Mamba在视觉领域机制理解的空白。

Method: （1）理论上分析了Mamba与Softmax和线性注意力的关系，将其解释为Softmax Attention的低秩近似；（2）提出用于二值分割的新度量方法，量化激活图并评测长程建模能力；（3）利用DINO自监督预训练，增强激活图的解释性并比较与监督方法的差异。

Result: 通过上述分析，作者发现Mamba能有效建模长程依赖，自监督预训练下激活图更清晰。在ImageNet上，模型线性探针准确率达到78.5%。

Conclusion: Mamba不仅表现优异，还兼具良好解释性，理论与实验分析为后续基于Mamba的视觉架构研究提供了参考与新工具。

Abstract: Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.

</details>


### [142] [ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access](https://arxiv.org/abs/2511.18382)
*Timing Yang,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: 该论文提出了ViMix-14M，这是一个包含约1400万对视频-文本对的大规模、高质量、可直接下载的视频文本数据集，用于解决开放领域文本生成视频模型训练中缺乏优质数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 目前开源文本生成视频模型面临的数据瓶颈主要是缺乏大规模且高质量、易用的视频文本语料。现有公共数据集需要大量手动爬取，且数据体量有限、获取困难，并存在版权和可用性等问题。

Method: 作者汇集了多个公开视频源的数据，通过统一的去重和质量过滤流程，辅以多粒度、基于真实标签的重新描述（re-caption）管道，提升描述与视频中动作、场景、时间结构的紧密度，从而构建出ViMix-14M数据集。

Result: ViMix-14M在多模态检索、文本生成视频和视频问答等任务上，在一致性和性能上超越了同类数据集。

Conclusion: ViMix-14M为开源视频基础模型的训练和微调扫清了数据障碍，也为构建高质量、强泛化性的视频文本数据集提供了经验和方法。

Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.

</details>


### [143] [Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.18385)
*Chuang Peng,Renshuai Tao,Zhongwei Ren,Xianglong Liu,Yunchao Wei*

Main category: cs.CV

TL;DR: 该论文提出了DualXrayBench多视角X射线检测基准，并创新性地将第二视角作为类语言模态来提升违禁品检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前X射线违规品检测主要依赖视觉模态，对于复杂威胁检测效果有限。虽然已有研究引入语言模态辅助，但实际检查常用多视角图像，因而探索第二视角能否提供类似语言约束成为研究动机。

Method: 作者提出DualXrayBench，这是首个包含多视角和多模态的X射线检测基准，并设计了8个跨视角推理任务。基于此，提出了几何（跨视角）-语义（跨模态）推理器（GSR）模型，将第二视角视为类语言模态，通过GSXray数据集中的结构化链式推理序列<top>, <side>, <conclusion>进行联合学习。

Result: 在DualXrayBench数据集上的综合评测结果显示，GSR模型在所有X射线检测任务上均显著优于现有方法。

Conclusion: 第二视角可等效为类语言模态，联合多视角与多模态推理可有效提升X射线安检检测性能，为实用安检系统提供了新思路。

Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.

</details>


### [144] [SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation](https://arxiv.org/abs/2511.18386)
*Peter Siegel,Federico Tombari,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: SegSplat是一种新颖的3D重建框架，能够快速实现带有丰富开放语义理解的3D场景重建，实现高几何精度与开放语义分割，无需针对每个场景单独优化。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建方法要么缺乏快速性，要么无法支持复杂的开放语义识别，且常需为每个场景单独优化语义，限制了其实用性。在机器人、增强现实等场景下，迫切需要高效兼容丰富语义的3D环境生成技术。

Method: 论文提出SegSplat，通过多视角2D基础模型特征构建语义记忆库，在3D高斯点中一次性预测语义索引、几何及外观，实现可查询的语义3D重建。无需对单个场景进行额外的语义优化。

Result: SegSplat在保持和先进快速3D高斯Splatting几何保真度的同时，实现了鲁棒的开放集语义分割。实验显示其无需针对每个场景的专门优化操作。

Conclusion: SegSplat推动了语义感知3D环境的快速生成，为机器人、增强现实等智能系统提供了高实用性的解决方案。

Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.

</details>


### [145] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种在视觉-语言模型（如CLIP）中通过弱监督模型提升强模型能力的方法，并引入了类原型学习方法，实现了比强基线方法高3.67%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大规模商业模型的复杂度提升，单纯依赖人类监督校准模型输出已不现实，且当模型超越人类知识时，很难高效提供反馈。因此，需要更高效、不依赖强人类监督的新方法，弱模型监督强模型被提出以减轻人力负担。

Method: 作者提出了“类原型学习”（CPL）方法，旨在为每个类别学习更具代表性的原型，通过简单的损失函数和弱监督信号，提升CLIP模型在分类任务上的表现。该方法扩展了此前仅在语言模型中的“弱监督-强泛化”思想到多模态视觉-语言模型。

Result: 在CLIP基础上采用CPL，在有限预训练等弱监督场景下，通过大量实验证明，该方法能带来有力提升，在目标任务上性能比强基线高3.67%。

Conclusion: 弱模型对强模型的监督在视觉-语言多模态任务上同样有效，提出的CPL方法可增强CLIP等模型的分类能力，在弱监督情形下尤其显著，具有实际应用推广价值。

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [146] [ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering](https://arxiv.org/abs/2511.18399)
*Yuxiang Nie,Han Wang,Yongjie Ye,Haiyang Yu,Weitao Jia,Tao Zeng,Hao Feng,Xiang Fei,Yang Li,Xiaohui Lv,Guozhi Tang,Jingqun Tang,Jinghui Lu,Zehui Dai,Jiacong Wang,Dingkang Yang,An-Lan Wang,Can Huang*

Main category: cs.CV

TL;DR: 本文提出了ChineseVideoBench，这是首个专为中文视频问答领域多模态大模型（MLLMs）评测设计的基准数据集及评估体系。


<details>
  <summary>Details</summary>
Motivation: 当前对视频分析需求日益增长，但缺乏能充分考虑中文及其文化背景的多模态问答评测体系。为此，作者开发了针对性强、覆盖面广的评测标准，以推动该领域发展。

Method: 作者构建了包含8个主类和12个子类的中文视频问答数据集，难度涵盖对视频深度理解和对中文语言文化的细致把握，并设计了相应的评估指标对现有多模态大模型进行测试。

Result: 实验发现ChineseVideoBench对现有大模型提出了显著挑战。Gemini 2.5 Pro取得了最高分（77.9%），而InternVL-38B则是最佳的开源模型。

Conclusion: ChineseVideoBench建立了中文视频多模态问答评测的新范式，极大推动了此领域的基准标准建设，对评估和提升相关大模型能力将产生重要影响。

Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.

</details>


### [147] [4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation](https://arxiv.org/abs/2511.18416)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法4D-VGGT，用于动态场景几何估计，有效区分和融合空间与时间特征，在多个基准上效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景几何估计方法通常将空间和时间特征统一到同一潜空间中，但空间和时间特征本质异质，统一表达可能导致表现不佳，需要新的分治策略。

Method: 提出4D-VGGT模型，从三个方面创新：（1）多输入设置：自适应视觉网格支持任意视角和时间步的输入序列；（2）多层次特征表达：空间通过跨视角全局融合，时间通过跨时间局部融合；（3）多任务预测：在时空特征上增加多个任务头，支持不同几何视觉任务。

Result: 整合多个几何数据集进行训练，在多个动态场景几何基准和任务上实验，结果表明本方法在特征区分性和应用普适性方面优于现有方法。

Conclusion: 所提4D-VGGT方法利用分治的时空表达方式，提升了动态场景几何估计的性能与泛化能力，为复杂场景建模提供了新思路。

Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.

</details>


### [148] [NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI](https://arxiv.org/abs/2511.18422)
*Mohammad Jafari Vayeghan,Niloufar Delfan,Mehdi Tale Masouleh,Mansour Parvaresh Rizi,Behzad Moshiri*

Main category: cs.CV

TL;DR: 本文提出并验证了一种名为NeuroVascU-Net的新型深度学习架构，可高效、精准地对T1CE MRI中的脑血管进行3D分割，兼具高精度与低计算成本，适用于神经外科手术规划。


<details>
  <summary>Details</summary>
Motivation: 现有T1CE MRI脑血管分割依赖人工或效率低的自动化方法，且精度与计算成本难以兼顾。既有方法多以TOF-MRA为主，T1CE方案尚缺专用深度学习工具。而在神经肿瘤患者的术前规划中，精确分割极为重要。

Method: 基于扩张U-Net架构，创新性地引入了多尺度上下文特征融合（MSC^2F）和跨域自适应特征融合（CDA^2F）两个模块。MSC^2F通过多尺度扩张卷积获取全局与局部特征，CDA^2F在深层自适应整合特定领域特征，从而提升表示能力并降低计算消耗。模型在137例神经肿瘤患者的T1CE数据集上进行训练与验证，所有数据由专业神经外科医生标注。

Result: NeuroVascU-Net实现了Dice得分0.8609，精度0.8841，能准确分割主要及细小的脑血管结构。总参数量仅为12.4M，远低于基于transformer的模型如Swin U-NetR。

Conclusion: NeuroVascU-Net在保持极高分割准确率的同时显著降低了模型复杂度与计算成本，非常适合计算机辅助神经外科手术规划，具备实际临床应用前景。

Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.

</details>


### [149] [CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images](https://arxiv.org/abs/2511.18424)
*Avishka Perera,Kumal Hewagamage,Saeedha Nazar,Kavishka Abeywardana,Hasitha Gallella,Ranga Rodrigo,Mohamed Afham*

Main category: cs.CV

TL;DR: 提出了CrossJEPA，一种高效、精简的跨模态联合嵌入预测架构，用于提升3D表征学习，显著减少模型参数和训练时间，并在多个基准测试上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前3D表示学习受限于大规模3D数据集的稀缺，且现有基于2D数据的跨模态方法常导致模型复杂、训练慢，不适合资源受限环境。因此，需设计高效、紧凑的架构。

Method: 作者提出CrossJEPA，借鉴JEPA理念，将2D图像基础模型的知识迁移到3D点云，通过预测特定视角的2D嵌入实现无掩码的预训练方式。引入跨域投影信息进行条件化、使用冻结教师+缓存目标嵌入机制提升效率。

Result: CrossJEPA仅用1410万参数（其中编码器850万），单卡预训练6小时，在ModelNet40和ScanObjectNN上刷新了线性探测SOTA（分别为94.2%和88.3%）。

Conclusion: CrossJEPA实现了高性能、低内存占用、训练快速的3D表示学习新框架，有望在资源有限场景广泛应用，并为跨模态无掩码预训练提供了新思路。

Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

</details>


### [150] [LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection](https://arxiv.org/abs/2511.18425)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: 本文提出了一种新型的深度学习模型LungX，通过融合EfficientNet、CBAM注意力机制和Vision Transformer，有效提升了肺炎检测的性能。


<details>
  <summary>Details</summary>
Motivation: 肺炎高发且致死率高，及时诊断对改善预后至关重要。现有方法在准确性及可解释性上有待提升，因此需要新的AI工具辅助高效诊断。

Method: 提出了LungX架构，结合EfficientNet的多尺度特征、CBAM注意力机制和Vision Transformer的全局建模能力，并在RSNA和CheXpert两个大型胸片数据集上进行了评估。

Result: LungX在两大数据集上取得了86.5%的准确率和0.943的AUC，相较EfficientNet-B0提升了6.7%的AUC，且可解释性更强，能更好地定位病灶区域。

Conclusion: LungX在肺炎检测中表现优异且拥有良好的可解释性，未来计划进行多中心验证及架构优化，目标作为临床辅助诊断工具部署。

Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.

</details>


### [151] [DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation](https://arxiv.org/abs/2511.18434)
*Yongkun Du,Pinxuan Chen,Xuye Ying,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文提出了DocPTBench，一个针对真实拍摄文档解析与翻译的全新基准，揭示了主流多模态大模型在真实拍摄场景下性能大幅下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析与翻译评测数据集主要基于高质量扫描或电子文档，忽略了在现实拍摄条件下（如几何畸变、光照变化）的解析难题，导致对实际使用场景的泛化能力欠缺，因此亟需新的、更具挑战性的评测基准。

Method: 作者构建了DocPTBench，包括1300多张高分辨率、多领域的拍摄文档，覆盖8种翻译场景，并配有人工高质量标注，系统评测主流多模态大模型与专用文档解析模型在数字文档与拍摄文档上的表现变化。

Result: 实验显示，从数字文档切换到拍摄文档后，多模态大模型端到端解析准确率平均下降18%，翻译准确率下降12%，专用文档解析模型平均降幅更高达25%。

Conclusion: 真实拍摄文档带来显著性能挑战，现有模型在该场景下鲁棒性较低，有必要关注和提升模型在复杂现实条件下的解析与翻译能力。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.

</details>


### [152] [When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection](https://arxiv.org/abs/2511.18436)
*Hao Shen,Jikang Cheng,Renye Yan,Zhongyuan Wang,Wei Peng,Baojin Huang*

Main category: cs.CV

TL;DR: 本文提出面向增量伪造检测中的生成式回放问题，提出了域感知相对加权（DARW）方法，有效缓解了生成样本领域混淆导致的性能下降，显著提升了增量学习伪造检测的效果。


<details>
  <summary>Details</summary>
Motivation: 随着人脸生成技术的发展，伪造手段也日益多样化，因此伪造检测模型需要不断使用新型伪造样本进行增量更新。但传统的样本回放方法存在样本多样性不足和隐私风险，生成式回放作为替代方案，其在伪造检测中的可行性尚不明确。

Method: 作者系统性分析了生成式回放下的两种典型场景（一是回放生成器与新伪造相似，二是差异大），发现相似时会带来领域混淆风险。为此提出域感知相对加权（DARW）策略：对领域安全样本直接监督，对领域风险样本引入相对分离损失，以动态平衡监督与混淆。同时利用领域混淆分数，动态调整不同样本的学习权重。

Result: 大量实验表明，在不同生成式回放设置下，DARW策略显著提升了增量伪造检测的性能，并有效减轻了领域重叠带来的负面影响。

Conclusion: 域感知相对加权方法能有效利用生成式回放，应对领域混淆问题，对提升增量伪造检测鲁棒性具有重要意义。

Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.

</details>


### [153] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: 该论文提出PEARL方法，通过在多模态推理任务中引入感知证据核验，显著提升了视觉-语言模型的推理能力，并在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）仅核查视觉-语言模型最后的文本输出，忽略了视觉感知这一关键基础。如果视觉感知出错，会导致推理失真、视觉幻觉和奖励欺骗。因此，需要方法确保推理基于真实可信的视觉感知。

Method: 作者提出PEARL方法，采用双分支结构。它针对每个推理型问答样本，先生成一组可验证的感知子问题，作为核查清单。模型在训练中需通过这些清单来获取感知奖励，并以此筛查和引导后续的推理训练，确保模型推理建立在检验合格的感知基础上。该方法可与GRPO、DAPO等主流RL方法结合。

Result: 在多模态推理基准测试上（如MathVerse），PEARL相较于原始方法提升9.7%，相较于GRPO方法提升6.6%，显示出显著的优势。

Conclusion: PEARL能够有效防止基于错误视觉信息的推理，提升VLM多模态推理的可靠性和准确性，是增强视觉-语言模型推理能力的有效方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [154] [ReCoGS: Real-time ReColoring for Gaussian Splatting scenes](https://arxiv.org/abs/2511.18441)
*Lorenzo Rutayisire,Nicola Capodieci,Fabio Pellacini*

Main category: cs.CV

TL;DR: 本文提出了一种针对高斯洒点场景的区域重着色编辑方法，具备高效用户交互体验，并公开了相应代码。


<details>
  <summary>Details</summary>
Motivation: 虽然高斯洒点技术已在新视角合成领域表现突出，但在具体编辑任务，如重着色方面仍缺乏高效且精细的解决方案，现有方法依赖2D扩散模型，存在视角不一致、操作不便和计算资源消耗大的问题。

Method: 作者提出了一个面向用户、便捷的管线，用户可在预训练高斯洒点场景中精确选择和重着色目标区域。此外，开发了实时互动工具，演示该方法的高效性能。

Result: 实验和工具展示了所提管线在精确选择与重着色的效率和效果，证明了其实用性和实时性能。

Conclusion: 该方法为高斯洒点场景编辑，尤其是区域重着色任务，提供了高效易用的解决方案，有望推动3D重建与编辑能力的发展。

Abstract: Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.

</details>


### [155] [SineProject: Machine Unlearning for Stable Vision Language Alignment](https://arxiv.org/abs/2511.18444)
*Arpit Garg,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大语言模型知识“遗忘”方法，能在最大程度保留模型性能的情况下忘记敏感信息。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型中存在需要忘记、删除安全或隐私相关信息的实际需求。但现有的unlearning方法容易破坏视觉-语言对齐关系，导致模型拒绝回答无害甚至有用的问题。

Method: 作者分析了现有遗忘方法失败的机理，发现问题主要出在投影网络的Jacobian病态。为此，提出“SineProject”——将正弦调制的可训练参数加入冻结的投影层，提高Jacobian的谱条件数，维持模态对齐并稳定训练过程。

Result: 在LLaVA v1.5 7B和13B模型上，SineProject在常规安全与隐私遗忘基准上表现优越：能实现对有害信息的彻底遗忘，同时大幅减少无害问题的拒答率。计算开销极低，效果达到当前最优的‘遗忘—保留’平衡。

Conclusion: SineProject是一种简单高效的方法，能在不大幅损失模型原有知识的前提下，实现有针对性的信息“遗忘”，兼顾安全性和实用性，对未来MLLM可控安全应用意义重大。

Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.

</details>


### [156] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出了EventBench，一个面向多模态大语言模型（MLLMs）事件视觉能力的综合评测基准，并对主流模型进行了系统性能评估。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在事件视觉领域取得了进展，但缺乏统一、全面的评测基准，难以系统性比较和推进模型能力，因此亟需开发一个标准化的多任务评测平台。

Method: 作者设计并公开了EventBench评测基准，涵盖八类评测任务，包括理解、识别、空间推理等，含有丰富的数据集（超过一百万对“事件-文本”数据），并对开源和闭源主流模型（如GPT-5、Gemini-2.5 Pro、Qwen2.5-VL等）进行了实测。

Result: 大量实验表明，现有事件型MLLMs在事件流理解任务上表现优异，但在细粒度识别和空间推理任务上仍然存在较大不足。

Conclusion: EventBench为事件视觉多模态大模型的研究提供了标准化评测工具，有助于未来模型的全面提升；同时也指出当前模型在细粒度和空间能力上亟需突破。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [157] [NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering](https://arxiv.org/abs/2511.18452)
*Loick Chambon,Paul Couairon,Eloi Zablocki,Alexandre Boulch,Nicolas Thome,Matthieu Cord*

Main category: cs.CV

TL;DR: 本文提出了一种新的上采样方法，名为Neighborhood Attention Filtering (NAF)，它实现了无需针对具体视觉基础模型（VFM）重新训练即可进行高效、准确的特征上采样，相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 主流视觉基础模型输出的特征图是下采样后的，对于诸如分割、重建等需要像素级信息的任务不够精准。现有上采样方法要么速度快但形式固定（难以适应复杂特征），要么需要针对每个VFM单独训练，牺牲了通用性和便利性。因此，亟需一种既高效又能兼容多种VFM的、无需额外训练的上采样方法。

Method: 提出Neighborhood Attention Filtering (NAF)方法，通过Cross-Scale Neighborhood Attention与RoPE位置编码，基于输入的高分辨率图像自适应地学习空间和内容权重。NAF可以直接对任何VFM的输出特征实现零样本（zero-shot）上采样，无需重新训练，且结构高效。

Result: NAF无需针对特定VFM训练，即可在多个下游任务上超过专用上采样器，达到了SOTA（水准）。在2K特征图上也有很高的效率（18帧/秒），除了特征上采样，在图像修复等任务上也表现强劲，显示出其通用性与高性能。

Conclusion: NAF方法打破了上采样领域在效率、通用性和准确性间的传统权衡，首次实现了无需重新训练就能优于定制方法的性能，并具备广泛的适用性和高效率，对VFM及高分辨率视觉任务有重要意义。

Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.

</details>


### [158] [RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading](https://arxiv.org/abs/2511.18454)
*Ming-Jhe Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多任务学习模型RegDeepLab，用于试管婴儿（IVF）胚胎碎片化程度的自动分级，兼顾视觉可解释性和分级精度。


<details>
  <summary>Details</summary>
Motivation: 现有胚胎碎片化分级方法主要依赖人工，存在主观差异大、效率低的问题。自动深度学习方法要么缺乏可解释性，要么难以直接获得临床所需分级，无法满足实际需求。

Method: 提出RegDeepLab，一个结合DeepLabV3+语义分割和多尺度回归的双分支MTL框架。为解决多任务训练中的梯度冲突和负迁移问题，引入“两阶段解耦训练策略”。并通过“特征注入”机制和“Range Loss”，提升半监督学习能力和训练效果。

Result: 标准端到端多任务训练可通过特征注入降低分级均方误差（MAE=0.046），但牺牲了分割精度；而解耦训练策略在保证高分级精度的同时，分割准确率也达到Dice=0.729。

Conclusion: 该方法实现了高精度、可解释的胚胎碎片化分级，满足临床应用需求，推动了辅助生殖领域的智能化发展。

Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.

</details>


### [159] [Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463)
*Bowei Pu,Chuanbin Liu,Yifan Ge,Peichen Zhou,Yiwei Sun,Zhiyin Lu,Jiankang Wang,Hongtao Xie*

Main category: cs.CV

TL;DR: 现有视频推理大模型在感知阶段存在缺陷，作者提出了新的循环感知推理框架（PLR）和抗幻觉机制，有效提升了感知充分性和推理准确性，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 目前视频推理大模型普遍采用单步感知方法，即一次性描述视频内容后再进行推理，这容易导致信息不充分和幻觉现象，影响推理的可靠性。作者旨在解决这一核心问题。

Method: 提出了一种循环感知推理（PLR）范式：模型不是一次性描述全部视频内容，而是每个循环只分析一个精确时间段的视频片段，描述、分析，并决定下一个动作。同时加入Factual-Aware Evaluator（FAE）作为抗幻觉奖励，评估感知结果的真实性，鼓励模型给出充分、精确依据。FAE通过大规模幻觉偏好数据集（AnetHallu-117K）微调，能力媲美GPT-4o。

Result: 提出的Video-PLR在3B和7B参数规模下，均取得了最先进（SOTA）的表现，数据使用效率也最佳。FAE评测器性能接近GPT-4o。

Conclusion: 循环感知推理与抗幻觉奖励相结合显著提高了视频理解与推理的效果，为视频推理LLM的感知鲁棒性和推理可靠性提供了新路径。算法、模型及数据集均已开源。

Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.

</details>


### [160] [Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span](https://arxiv.org/abs/2511.18470)
*Heeseung Yun,Joonil Na,Jaeyeon Kim,Calvin Murdock,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新方法EgoSpanLift，实现了从二维向三维环境中预测人类下一个视觉关注区域（视觉范围）的能力，并创建了大型评测基准。方法在3D预测准确性方面超越了传统2D方法。


<details>
  <summary>Details</summary>
Motivation: 现有以自我视角为主的用户和场景理解研究侧重于运动和接触行为，对视觉感知的预测关注较少，但视觉感知对指导行为、增强现实（AR/VR）和辅助技术有着重要意义。

Method: 提出EgoSpanLift方法，将SLAM生成的关键点转为与视线兼容的三维几何，提取三维体积视觉范围。结合3D U-Net和单向transformer进行时空融合，在三维网格上高效预测未来视觉关注区域。同时建立了包含36.46万条样本的多模态测试基准。

Result: 该方法在三维预测和定位精度上，优于主要二维注视预测和三维定位的基线方法。在无针对性二维训练的情况下，若将结果投射回二维图像平面，也取得了可比的性能。

Conclusion: EgoSpanLift有效地实现了自我视角下三维视觉关注预测，在增强现实、辅助技术及相关领域具有应用前景，并为该方向的研究提供了高质量的实验基准。

Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

</details>


### [161] [Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale](https://arxiv.org/abs/2511.18471)
*Liav Hen,Tom Tirer,Raja Giryes,Shady Abu-Hussein*

Main category: cs.CV

TL;DR: 本文提出了一种自适应似然步长策略（AdaPS），用于优化扩散模型在逆问题中的重建效果，从而在不需超参数调整的情况下超越现有基线并提升成像质量。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型用于成像等逆问题时，如何平衡扩散先验和数据一致性项是关键挑战。步长设置不合理会导致重建伪影或收敛缓慢、质量下降。

Method: 作者针对逆问题中的扩散采样过程，提出了基于观测依赖的自适应权重机制，通过比较两种对中间似然梯度的近似，实现对步长的动态调整。该方法可适应扩散调度、时间重定和噪声注入，全流程无需手动调超参数。

Result: 在高分辨率人脸（CelebA-HQ）和ImageNet-256等数据集的超分辨率、高斯去模糊和运动去模糊实验中，AdaPS在感知质量上优于现有扩散基线，失真指标持平或更优，且对扩散步数、噪声和随机性表现出鲁棒性。

Conclusion: AdaPS无需特定任务调优即可提升扩散模型在多种成像逆问题上的重建质量，是稳健、通用的扩散采样新方法，有望推广至更广泛的生成建模任务。

Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.

</details>


### [162] [Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements](https://arxiv.org/abs/2511.18473)
*Juan Romero,Qiang Fu,Matteo Ravasi,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯推断的高光谱图像重建方法HSDiff，采用无条件训练的扩散先验和后验抽样，有效提升了重建效果并实现了对不确定性的量化。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据驱动的高光谱图像重建方法因数据集谱域多样性不足，容易出现幻觉误差，尤其在检验同色现象（metamerism）时效果不佳。本文旨在提升模型对不确定性和谱多样性的建模能力。

Method: 作者将高光谱图像重建建模为贝叶斯推断问题，引入HSDiff框架。该方法利用像素级扩散模型作为无条件先验，并采用后验抽样生成与多种成像模型相符的高光谱样本。创新之处包括基于区域的同色增强和谱域上采样扩展训练集谱多样性，从而提升先验多样性和不确定性校准能力。此外，比较了不同前向模型与有效谱编码引导下的后验分布形态。

Result: HSDiff在重建质量和不确定性量化方面表现优越。有效的光谱编码能够增强不确定性信息的校准，优于未编码模型。相关实验结果也凸显了在快拍高光谱成像中光谱编码的重要性。

Conclusion: HSDiff为不确定性感知的高光谱图像重建提供了完整、高性能的贝叶斯解决方案，并验证了有效光谱编码的重要作用。

Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.

</details>


### [163] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: 本文提出了两个创新的模型压缩技术，有效提升了视觉-语言模型在受限边缘设备上的实时性能，同时大幅降低了算力和存储需求。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI需求增长，尤其是在视觉-语言任务上，如何在低功耗、低存储的小设备上实现高效的AI推理成为亟需解决的问题。传统的压缩方法无法在动态场景和异构硬件上实现最佳的性能和适应性，因此需要新的自适应方法。

Method: 作者提出Sparse Temporal Token Fusion（STTF）和Adaptive Neural Compression（ANC）：前者通过事件驱动检测场景变化，动态复用视觉token以减少冗余，后者通过学习路由器有选择地激活编码器分支，根据场景复杂度调整计算强度。两种方法都结合了算法创新与硬件感知优化。

Result: 3B参数的TinyGPT-STTF模型在COCO 2017测试集上，超越了更大模型LLaVA-1.5 7B，高效提升了CIDEr等指标并显著减少了计算量和参数；在事件视觉任务上，STTF降低了token数量84%但保持95.6%准确率，ANC在低动态场景下将FLOPs降至原来的10%。总的来看，新方法兼顾了准确率、延迟和资源消耗的优化。

Conclusion: 新提出的STTF和ANC使视觉-语言模型能够高效适用于现实中的边缘设备，显著减少资源消耗且保持模型性能，有望推动视觉-语言AI模型在资源受限场景的应用落地。

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [164] [Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives](https://arxiv.org/abs/2511.18507)
*Kai Jiang,Siqi Huang,Xiangyu Chen,Jiawei Shao,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文针对多模态大型语言模型（MLLMs）在视觉连续学习中灾难性遗忘的问题，提出了新的多场景多视角数据集MSVQA，并提出UNIFIER方法，有效缓解了场景切换下的遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 部署在设备上的MLLMs常常遇到场景变化（如背景、视角）导致灾难性遗忘，影响复杂视觉任务的执行。论文旨在提升MLLMs面对动态下游场景时的适应与知识积累能力。

Method: 构建覆盖高空、水下、低空和室内四种场景的新型多模态视觉问答数据集MSVQA。提出UNIFIER框架，将不同场景的视觉信息在模型中分别处理，并映射到同一特征空间，通过特征一致性约束保持不同分支间视觉表征的稳定性。

Result: 在MSVQA数据集上，UNIFIER方法显著缓解了跨场景任务中的灾难性遗忘现象，同时在同一场景内实现了知识的积累。

Conclusion: UNIFIER方法能有效提高MLLMs对于多变现实场景下的适应能力和连续学习能力，对未来MLLMs在实际环境部署具有实际价值。

Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

</details>


### [165] [LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging](https://arxiv.org/abs/2511.18513)
*He Huang,Yujun Guo,Wei He*

Main category: cs.CV

TL;DR: 论文提出低秩深度展开网络（LRDUN），将低秩分解融入压缩感知光谱成像重建中，显著降低计算成本并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络（DUN）直接在高维光谱图像（HSI）空间操作，导致计算冗余，且2D到3D映射病态不适定，影响重建效果并效率低。

Method: 提出两种新的成像模型：基于光谱基和子空间图像，通过将低秩分解与感知模型结合，仅重建低维分量，缓解问题不适定性。基于此，开发低秩深度展开网络（LRDUN），通过PGD框架联合优化两个子问题，同时引入广义特征展开机制（GFUM），解耦数据保真项和先验模块的特征维度，增强表达能力和灵活性。

Result: 在模拟和真实数据集上实验，LRDUN在重建质量上达到最新水平（SOTA），且计算成本明显降低。

Conclusion: LRDUN结合低秩分解和深度展开方法，突破现有HSI重建的效率和性能瓶颈，为光谱压缩成像领域带来新的高效方案。

Abstract: Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.

</details>


### [166] [Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging](https://arxiv.org/abs/2511.18514)
*Abishek Karthik,Sreya Mynampati,Pandiyaraju V*

Main category: cs.CV

TL;DR: 本论文提出一个多功能的集中平台，利用深度学习方法实现太阳能电池板的灰尘和故障检测，在准确率和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 太阳能作为重要的可再生能源，其发电效率容易受到灰尘、故障、环境等多种因素影响。传统的检测和维护手段效率有限，需要一种自动化、准确、高效的检测方案以提升运维效率，降低维护成本。

Method: 作者构建了一个综合平台，集成灰尘检测和故障检测两大功能。方法上，首先对采集到的图片进行伽马校正、Gaussian滤波和归一化等预处理。灰尘检测方面，依据阴影、杂物、污染等多种指标，利用CNN、ResNet结合自注意力机制（KerNet模型）进行分类。故障检测则结合热成像识别太阳能电池的物理损伤及故障现象。

Result: 通过大量实验和对比，作者表明该模型在灰尘和故障检测的准确率、效率等指标上均优于现有同类方法。

Conclusion: 该多功能平台有效提升了太阳能电池板的自动检测能力，能广泛适用于小型家庭到大型太阳能电站的运维，具有现实应用价值。

Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.

</details>


### [167] [Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516)
*Haidong Kang,Ketong Qian,Yi Lu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的训练自由的少样本增量学习方法，通过扩散模型代替传统的梯度优化，实现无梯度、低代价的持续学习，并结合视觉和大语言模型自动生成的文本描述，缓解样本稀缺，取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有少样本增量学习主要依赖梯度优化方法，但这会随着新类别增多导致训练成本急剧上升，数据稀缺情况下还会加剧遗忘，并影响新类的适应能力。迫切需要摆脱对梯度训练的依赖，探索高效新范式。

Method: 作者首先发现在FSCIL场景下，可用条件扩散过程替代梯度优化，提出了无训练、基于条件扩散生成的增量学习框架（CD-FSCIL）。此外，采用多模态学习，将视觉特征与由大语言模型自动生成的自然语言描述融合，提升表示能力和泛化性。

Result: 在主流少样本增量学习基准数据集上，所提出方法不仅达到SOTA性能，同时大幅降低了计算和内存消耗。

Conclusion: 本工作实现了少样本增量学习领域从依赖梯度优化到训练自由的范式转变，为后续持续学习研究提供了全新思路，并用多模态方法有效缓解了样本不足问题。

Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.

</details>


### [168] [DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation](https://arxiv.org/abs/2511.18533)
*Md Mizanur Rahman Mustakim,Jianwu Li,Sumya Bhuiyan,Mohammad Mehedi Hasan,Bing Han*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的牙齿分割方法DE-KAN，并在两大基准牙科X光数据集上取得了优异成绩，在准确率和Dice系数方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在全景牙科X光图像中难以准确分割单独牙齿，主要原因在于解剖变异、牙齿形状不规则以及结构重叠等问题，影响了模型的表现。本研究旨在克服这些挑战，提高分割精度。

Method: 提出了DE-KAN（Dual Encoder Kolmogorov Arnold Network），框架将ResNet-18编码器应用于增强输入，定制CNN编码器应用于原始输入，从而互补提取全局和局部特征。随后通过基于Kolmogorov Arnold表示定理的KAN瓶颈层进行特征融合，引入非线性可学习激活函数以提升模型能力和可解释性。

Result: 在两个牙科X光基准数据集上，DE-KAN模型达到mIoU为94.5%，Dice系数为97.1%，精度为98.91%，召回率为97.36%，Dice系数相比其他方法提高了4.7%。

Conclusion: DE-KAN方法有效提升了牙齿分割的精度，在处理解剖变异与复杂结构时展现出更优性能，有望为口腔医学影像智能分析提供更强有力的工具。

Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.

</details>


### [169] [HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2511.18534)
*Pengcheng Fang,Hongli Chen,Guangzhen Yao,Jian Shi,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: cs.CV

TL;DR: 该论文提出了HiFi-MambaV2模型，用于从欠采样的k-space数据中重建高质量的MRI图像，通过结合频率分解与内容自适应计算，有效提升高频细节的恢复和整体结构一致性，并在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 磁共振（MRI）图像的重建常受限于采样不足，导致高频细节丢失和结构失真。现有方法难以兼顾细节恢复与解剖结构一致性，亟需一种兼顾高频恢复和全局结构的方法。

Method: 作者提出了HiFi-MambaV2模型，包括两个核心组件：（1）可分离频率一致的拉普拉斯金字塔（SF-Lap），提供低/高频别的稳定特征流。（2）分层共享路由的稀疏专家混合结构（MoE Mamba），通过逐像素稀疏分派实现专家模块的有效特化，同时保证跨层稳定。模型还融合了轻量级全局上下文路径，并结合数据一致性正则进行端到端重建。

Result: 在fastMRI、CC359、ACDC、M4Raw、Prostate158等公开数据集上，HiFi-MambaV2在PSNR、SSIM、NMSE指标上均超越CNN、Transformer及此前基于Mamba的重建方法，在单线圈和多线圈、多加速倍数下均有一致提升，特别在高频细节和结构保真度上表现突出。

Conclusion: HiFi-MambaV2在MRI图像重建任务上实现了高效、稳定和准确的性能，能够可靠地恢复高频细节并保持解剖结构一致性，在实际MRI重建场景中具有广泛应用前景。

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.

</details>


### [170] [Zero-Shot Video Deraining with Video Diffusion Models](https://arxiv.org/abs/2511.18537)
*Tuomas Varanka,Juan Luis Gonzalez,Hyeongwoo Kim,Pablo Garrido,Xu Yao*

Main category: cs.CV

TL;DR: 本论文提出了一种无需合成数据及模型微调的零样本视频去雨方法，利用预训练的文本到视频扩散模型，通过反向提示和注意力切换机制，有效去除动态场景中的雨，并保持内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频去雨方法依赖于合成数据或静态摄像头数据，难以泛化到真实世界的动态场景。此外，基于扩散模型的微调虽有进展，但易削弱泛化能力。本论文旨在克服这些局限，实现更强泛化能力的视频去雨方法。

Method: 作者设计了一种零样本去雨方法，核心依赖预训练的文本到视频扩散模型。首先对输入视频进行扩散空间反演，利用negative prompting将重建过程从扩散模型中的"雨"概念中推离。同时引入注意力切换机制，以保证动态背景和结构一致性，有效减小负向提示带来的伪影。

Result: 通过在多个真实世界雨景数据集上的大量实验，本文方法在无需监督训练的情况下，显著优于现有方法，展现了优良的泛化能力和去雨效果。

Conclusion: 本论文方法首次实现了对复杂动态场景的零样本视频去雨，无需合成数据和模型微调，具备较强实用性和拓展性，为后续相关研究提供了新思路。

Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.

</details>


### [171] [C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction](https://arxiv.org/abs/2511.18559)
*Kuan Wei Huang,Brandon Li,Bharath Hariharan,Noah Snavely*

Main category: cs.CV

TL;DR: 本论文提出了C3数据集，专为地面照片与平面图之间的对应关系任务设计，并显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的几何模型在处理视角或模态变化很大的场景时表现不佳，尤其是地面照片与建筑平面图之间的对应任务。现有数据集也存在模态单一或缺失精确对应的问题，限制了该领域的发展。

Method: 作者首先通过网络收集图片并应用结构光束法重建3D场景，然后手动将其与互联网上的平面图进行配准，生成了包含详细像素级对应和相机位姿的C3数据集。该数据集规模大，涵盖多场景、多模态，为训练和评估跨模态几何对应模型提供了支持。

Result: 在该数据集上，现有最先进的对应模型表现不佳。作者在C3数据集上训练模型后，最佳方法的RMSE提升了34%。

Conclusion: C3数据集弥补了现有数据集的局限，为跨模态几何推理提供了高质量资源，并推动了地面照片与平面图对应这一难题的发展。

Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.

</details>


### [172] [Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation](https://arxiv.org/abs/2511.18591)
*Wei Dong,Han Zhou,Junwei Lin,Jun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉自回归（VAR）生成模型的新方法，结合视觉-语言模型（VLM）感知先验，解决真实暗光图像恢复中的可见性差、复杂噪声和模糊等难题，且无需配对数据，实验表现领先。


<details>
  <summary>Details</summary>
Motivation: 现实中的暗光图片常常不仅亮度和对比度低，还伴随复杂噪声和模糊，现有方法依赖成对数据或难以建模动态光照和模糊特性，导致泛化能力差。因此需要一种能够更好适应真实场景且泛化性更强的无监督图像恢复方法。

Method: 作者提出利用视觉自回归（VAR）生成模型，将视觉-语言模型（VLM）作为感知先验。具体方法包括：1）采用自适应曲线估计方案，用VLM推断的可见度评分调节光照信息，为VAR提供条件线索；2）引入动态及空间频率感知的旋转位置编码（SF-RoPE），提升VAR对模糊退化结构的建模能力；3）设计递归相位域调制策略，通过VLM模糊评分引导有限步相位域精修，减轻模糊伪影。

Result: 该框架在公开基准数据集上的无监督图像恢复任务中取得了领先的性能，超过了现有方法。

Conclusion: 所提方法无需监督配对数据，能有效提升暗光图像的可见性、降噪和去模糊能力，为复杂真实场景下图像恢复提供了新方案。

Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.

</details>


### [173] [Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI](https://arxiv.org/abs/2511.18595)
*Wenhao Guo,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: 本研究对多种深度学习模型在胶质母细胞瘤（GBM）治疗后随访MRI上区分真性肿瘤进展（TP）和伪进展（PsP）的能力进行了系统基准测试，提出了分阶段的性能评估。


<details>
  <summary>Details</summary>
Motivation: TP和PsP在影像上表现相似，早期随访中两者区分非常困难，而准确区分对临床治疗路径决策至关重要。目前缺乏系统、按随访时点分阶段的深度学习基准评价，这对实际应用和进一步改进模型有指导意义。

Method: 利用Burdenko GBM Progression队列（n=180），对多种代表性深度学习架构（CNN、LSTM、混合模型、transformer和状态空间模型）分别按放疗后不同随访时点进行训练和评价。全部采用统一的数据质量控制和患者级交叉验证流程，并评估模型精确度、AUC、F1分数和计算效率等性能指标。

Result: 不同模型在两个阶段表现出的总体准确率相近（约0.70-0.74），但在第二次随访时模型的区分能力（F1和AUC）有所提升。Mamba+CNN混合模型在准确率与效率之间表现最佳，transformers虽AUC高但计算成本大，轻量级CNN效率高但稳定性欠佳。模型性能还受批大小影响。总体上，模型区分TP与PsP的绝对效能仍有限。

Conclusion: 本研究首次建立了基于随访阶段的深度学习基准，指出了当前模型区分TP与PsP的局限与挑战，强调了构建更大样本、纵向及多序列MRI与多中心数据等未来研究方向。

Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.

</details>


### [174] [NeAR: Coupled Neural Asset-Renderer Stack](https://arxiv.org/abs/2511.18600)
*Hong Li,Chongjie Ye,Houyuan Chen,Weiqing Xiao,Ziyang Yan,Lixing Xiao,Zhaoxi Chen,Jianfeng Xiang,Shaocong Xu,Xuhui Liu,Yikai Wang,Baochang Zhang,Xiaoguang Han,Jiaolong Yang,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种神经资产与神经渲染器耦合的新型渲染堆栈（NeAR），通过端到端联合优化资产与渲染过程，实现更高的保真度、一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经资产与神经渲染的研究各自独立，忽略了彼此结合带来的潜力。作者希望通过耦合设计资产与渲染器，实现端到端可学习的图形堆栈，提升渲染质量和效率。

Method: 资产端：基于Trellis风格结构化3D潜变量，设计能够去除光照影响的神经资产（SLAT），结合几何和材质信息。渲染端：开发感知光照的神经渲染器，结合视角、HDR环境贴图，实现实时可重光渲染。

Result: NeAR在四个任务上进行了验证：基于G-buffer的前向渲染、任意光线单图重建、未知光照单图重光、新视角重光。在定量和感知质量上均超越了最新基线方法。

Conclusion: 耦合神经资产与渲染器能提升渲染栈整体性能，建议未来将资产生成与渲染作为协同设计的整体推进，而非彼此独立。

Abstract: Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.

</details>


### [175] [RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data](https://arxiv.org/abs/2511.18601)
*Wenchao Ma,Dario Kneubuehler,Maurice Chu,Ian Sachs,Haomiao Jiang,Sharon Xiaolei Huang*

Main category: cs.CV

TL;DR: 该论文提出了RigAnyFace (RAF)，一个适用于各种面部几何拓扑结构的可扩展神经自动绑定框架，能够自动为静态中性人脸重建标准FACS姿态，实现高保真表情混合形变控制，并支持多组件（如眼球等）绑定。其方法提升了准确性与泛化能力，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 面部绑定（rigging）是高质量表情动画与数字人制作的重要步骤，传统依靠艺术家手工操作，费时费力，尤其针对具有多样布线及连通性的面部3D模型，自动化处理难度大。提升自驱动、通用化和细节支持（如眼球分离绑定）电动化绑定框架成为亟需。

Method: 提出了基于神经网络的表面学习方法，不依赖三角化拓扑结构，用网络对标准FACS表情参数进行条件控制，同时设计能处理多不连续部件（如眼球/上下牙齿）的网络结构；训练时结合有手工绑定标注的3D数据集与自设计的2D无标注监督法提升数据多样性，实现更大规模训练。

Result: 实验证明，RAF对各种不同几何与拓扑的面部3D模型（包括现成资产与真实采集数据）都能生成高质量的FACS绑定形变。RAF不仅精度优于现有方法，并具备更强泛化性和对多组件（如眼球）支持能力。

Conclusion: RAF大幅推进了三维面部绑定技术的自动化与适应性，能够适用于更广泛、复杂的数据与场景，提升行业生产效率，有望在数字人、动画等应用领域带来实际价值。

Abstract: In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io

</details>


### [176] [Functional Localization Enforced Deep Anomaly Detection Using Fundus Images](https://arxiv.org/abs/2511.18627)
*Jan Benedikt Ruhland,Thorsten Papenbrock,Jan-Peter Sowa,Ali Canbay,Nicole Eter,Bernd Freisleben,Dominik Heider*

Main category: cs.CV

TL;DR: 本研究系统性评估了Vision Transformer（ViT）在多种数据增强与增强策略下对多源视网膜疾病数据集的检测表现，并提出ViT优于卷积基线。还开发了基于重构解释的异常检测器与支持无阈值决策的校准机制。


<details>
  <summary>Details</summary>
Motivation: 视网膜疾病自动检测受限于图像质量变异、早期症状隐匿以及数据域转移，造成检测准确性和泛化能力不足。本文旨在系统探索ViT及增强策略对这些挑战的应对能力，并推动高质量数据集与更实用的异常检测模型的发展。

Method: 以ViT为基础模型，结合多种几何和色彩增强、直方图均衡等策略，在多个公开与自建高质量数据集（AEyeDB）上系统测试，并比较与传统卷积模型。在Papila数据集中，融合增强策略，另辅以GANomaly异常检测器，并通过GUESS做概率校准。

Result: ViT在各数据集与疾病类别中表现稳定，准确率0.789-0.843。糖尿病视网膜病变和老年黄斑变性检测效果优秀，青光眼误分类较多。几何和色彩增强整体提升最大，直方图均衡针对结构细微数据集有效，拉普拉斯增强反而降低表现。Papila集上ViT+AUG  AUC 0.91，优于ensemble卷积基线（AUC 0.87）。GANomaly异常检测AUC 0.76，泛化能力强且具解释性。GUESS实现无阈值决策支持。

Conclusion: ViT结合多数据集训练与合理的数据增强在视网膜疾病检测上优于传统方法。此外，异常检测与校准框架提升了系统在临床应用中的实用性和安全性。

Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.

</details>


### [177] [Health system learning achieves generalist neuroimaging models](https://arxiv.org/abs/2511.18640)
*Akhil Kondepudi,Akshay Rao,Chenhui Zhao,Yiwei Lyu,Samir Harake,Soumyanil Banerjee,Rushikesh Joshi,Anna-Katharina Meissner,Renly Hou,Cheng Jiang,Asadur Chowdury,Ashok Srinivasan,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 针对现有AI模型难以利用神经影像临床私有数据的问题，作者提出了“健康系统学习”新范式，通过大量实际临床影像训练NeuroVFM模型，在多个临床任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前前沿AI模型（比如GPT-5、DINOv3）由于缺乏大规模可用的脑部影像临床数据，在医学领域特别是神经影像相关任务表现不佳。主要原因是医学影像中含有可识别的个人面部特征，导致公开数据稀缺。这限制了AI在临床医学中的应用和性能。

Method: 作者提出“健康系统学习”范式，即从实际健康系统中收集非精细整理（uncurated）的临床MRI、CT影像，训练了NeuroVFM视觉基础模型。模型采用可扩展的体积联合嵌入预测架构，利用5.24百万组临床影像进行训练，实现了全面的脑解剖与病理表征学习。

Result: NeuroVFM在多个神经影像临床任务（如放射学诊断、报告生成）上表现出业界领先水平，具备新兴的解剖结构理解与可解释的诊断定位能力。将模型与开源语言模型结合后，生成的放射学报告在准确性、分诊、专家偏好等方面均超越现有前沿模型，同时减少了幻觉内容和关键性错误。

Conclusion: 健康系统学习为通用医学AI模型的构建提供了新范式，NeuroVFM展示了基于临床大数据训练基础模型的可行性与优越性，为创建规模化、实用的临床基础模型提供了框架。

Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.

</details>


### [178] [From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis](https://arxiv.org/abs/2511.18654)
*Nayu Dong,Townim Chowdhury,Hieu Phan,Mark Jenkinson,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Tumor Fabrication (TF)的新颖两阶段框架，能够在MRI肿瘤标注数据稀缺的情况下，自动合成3D脑肿瘤图像及其标注，提高后续肿瘤分割任务性能。


<details>
  <summary>Details</summary>
Motivation: MRI肿瘤图像及其精确标注非常稀缺，这严重限制了自动化肿瘤分割的准确性。现有的数据合成方法存在人工建模周期长、专业性强、深度生成模型又需要大量配对数据等问题。因此，亟需在只有健康图像和少量标注的情况下，可自动合成高质量肿瘤数据的方法。

Method: 作者提出了一个名为TF的两阶段3D脑肿瘤合成框架，首先以粗粒度方式进行肿瘤合成，随后通过生成式模型进行精细化，这一过程完全自动，仅需健康扫描和少量实际注释数据，即可批量合成高质量的成对图像及标注。

Result: 实验表明，使用TF框架合成的数据对下游肿瘤分割任务进行数据增强，可在训练数据稀缺时显著提升分割模型性能。

Conclusion: TF框架能够为医学图像分割任务提供高效、可靠且可扩展的数据增强手段，在临床AI应用中为缓解标注数据匮乏问题作出了有价值的贡献。

Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.

</details>


### [179] [Robust Physical Adversarial Patches Using Dynamically Optimized Clusters](https://arxiv.org/abs/2511.18656)
*Harrison Bagley,Will Meakin,Simon Lucey,Yee Wei Law,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于超像素（superpixel）的正则化方法，用于生成更具缩放鲁棒性的物理对抗补丁，使其即使在尺寸变化或距离变动时也能保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的物理对抗补丁在缩放（如距离变化、下采样/上采样）时，由于像素插值导致颜色混合和高频信息丢失，攻击效果显著减弱。过去很少有工作针对这种“缩放变异性”进行改进。

Method: 作者提出利用SLIC超像素算法，在补丁优化过程中动态聚类像素，并使用隐函数定理（Implicit Function Theorem）实现能反向传播的超像素边界和颜色优化，从而得到结构稳定、缩放鲁棒性强的补丁。

Result: 提出的方法在数字域提升了对抗补丁的性能，并且在物理世界中的效果也得到保留。论文通过创新的实体评测协议（利用屏幕和纸板系统性地变化场景条件）客观验证了补丁的实际表现。

Conclusion: 基于超像素的正则化技术能有效提升对抗补丁的缩放鲁棒性，使其在物理应用中更具威胁性，为物理攻防研究和安全防护提供了新思路。

Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

</details>


### [180] [Data Augmentation Strategies for Robust Lane Marking Detection](https://arxiv.org/abs/2511.18668)
*Flora Lian,Dinh Quang Huynh,Hector Penades,J. Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 该论文提出了一种利用生成式AI的数据增强流程，解决车道检测模型受摄像头视角变化影响导致泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的车道检测模型多以公开数据集（如CULane）训练，这些数据集主要基于前视摄像头，导致模型对侧面摄像头等实际部署场景的适应能力较差（域偏移问题）。为提升模型在不同摄像头位置下的鲁棒性，亟需定制化的增强方法。

Method: 论文提出结合几何透视变换、AI修复（inpainting）和车辆车身叠加，生成符合部署环境的图片，同时保证车道连贯性，从而增强训练数据。随后在SCNN和UFLDv2两种主流模型上进行训练与测试。

Result: 采用增强数据训练后，两个模型在多样化视角（如侧视摄像头）与光照条件（如阴影）下的精度、召回率和F1分数均显著提升，相比原始预训练模型表现更优。

Conclusion: 该方法有效弥补了公开数据集与真实部署场景之间的数据鸿沟，为提升车道检测系统在侧装摄像头等实际应用下的可靠性和泛化能力，提供了可扩展、实用的解决方案。

Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.

</details>


### [181] [Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement](https://arxiv.org/abs/2511.18672)
*Yuchen Xia,Souvik Kundu,Mosharaf Chowdhury,Nishil Talati*

Main category: cs.CV

TL;DR: Sphinx是一种无须训练的混合推理框架，将回归方法的高效率与扩散模型的高质量结合，实现了新颖视角生成（NVS）中质量与推理速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前高质量新视角生成依赖于计算量大的扩散模型，而高效率回归模型则质量逊色，如何低计算实现高质量NVS成为亟需解决的问题。

Method: Sphinx利用回归模型进行快速初始化，引导并减少扩散模型的去噪计算工作量。同时，引入自适应噪声调度和选择性细化，对不确定区域/帧分配更多计算资源。整个流程无需再训练。

Result: Sphinx比常规扩散模型推理平均提速1.8倍，且感知质量损失不到5%，可根据需求灵活权衡推理速度和图像质量。

Conclusion: Sphinx实现了NVS任务质量与推理速度间前所未有的平衡，为动态场景下的新颖视角合成设立了新的质量-效率标杆。

Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.

</details>


### [182] [Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers](https://arxiv.org/abs/2511.18673)
*Yiqing Shi,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了Edit2Perceive，一个基于编辑扩散模型的统一框架，在深度、法线和抠图等密集感知任务中取得了全面领先的性能。相较于传统的文本到图像生成器，编辑扩散模型更强调图像一致性，结构保留能力更强。


<details>
  <summary>Details</summary>
Motivation: 当前大多数密集感知任务仍依赖主要用于随机生成的文本到图像扩散模型，这些方法在结构和几何一致性上存在不足。作者希望利用具有更强图像一致性能力的图像编辑扩散模型，提升密集感知（如深度、法线、抠图）效果。

Method: 作者提出Edit2Perceive，基于FLUX.1 Kontext架构，采用参数全量微调，加入像素空间一致性损失，对中间步的去噪状态做结构保留优化。此外，该方法使用单步确定性推断，实现加速和对小数据集友好训练。

Result: Edit2Perceive在深度、法线和抠图三个任务上均取得了全面的最新最优结果，运行速度提升显著，且对小规模数据集有良好适应性。

Conclusion: 面向图像编辑的扩散模型能为感知任务提供更优异的几何与结构一致性基础，融合相关方法可在多密集视觉识别任务中取得领先表现，具有强大应用潜力。

Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

</details>


### [183] [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/abs/2511.18676)
*Yongcheng Yao,Yongshuo Zong,Raman Dutt,Yongxin Yang,Sotirios A Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: 本文提出了MedVision，一个专为医学影像量化分析设计的大规模数据集与基准，用于提升和评测视觉-语言模型（VLMs）的量化推理能力。通过在22个公开数据集上的3千万多对图像和注释，涵盖结构检测、肿瘤/病变大小估计及角度/距离测量等任务，经MedVision微调后模型性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM主要针对分类或定性任务，缺乏临床常见的量化测量能力，如肿瘤大小和关节角度。临床决策急需支持量化推理的VLM，因此有必要开发相关数据集和评测方法。

Method: 作者收集并整理了22个公开医学影像数据集，构建了包括3080万对图像和注释的数据集MedVision，定义并评测了结构检测、肿瘤/病变定量以及角度/距离测量三类量化任务。对现有VLM进行监督微调以提升其量化能力。

Result: 当前主流VLM在医学影像量化任务上表现欠佳。使用MedVision进行监督微调后，这些模型在结构检测、肿瘤/病变估算及角度/距离测量三项任务上的误差率显著降低，精度获得有效提升。

Conclusion: MedVision为医学影像量化分析领域的发展提供了基础，有助于推动具备强量化推理能力的医学视觉语言模型的研究。

Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.

</details>


### [184] [A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification](https://arxiv.org/abs/2511.18677)
*Yunpeng Gong,Yongjie Hou,Jiangming Shi,Kim Long Diep,Min Jiang*

Main category: cs.CV

TL;DR: 本论文提出KTCAA框架，通过理论驱动与创新方法，实现少样本情境下素描与RGB图像跨模态行人重识别，并在多个基准上取得最先进效果。


<details>
  <summary>Details</summary>
Motivation: 素描-行人重识别受到模态差异大和标注数据稀缺的挑战，现有方法难以充分解决这两个核心问题。因此亟需一种兼顾泛化性和对小样本适应能力的解决方案。

Method: KTCAA框架以泛化理论为基础，针对目标域风险提出两大影响因素：域分布差异和扰动不变性，并基于此设计了两个核心模块：（1）Alignment Augmentation（对齐增强），通过局部素描风格变换来模拟目标域分布、促进渐进对齐；（2）Knowledge Transfer Catalyst，通过引入最坏情况扰动并强制一致性，提高模型对模态转变的鲁棒性。两模块在元学习范式下联合优化，实现从RGB向素描域的对齐知识迁移。

Result: KTCAA在多个跨模态、少样本素描行人重识别基准上实现了当前最优表现，尤其在数据匮乏情况下效果显著优于现有方法。

Conclusion: 通过理论与方法创新，KTCAA框架有效缓解了素描-行人识别中模态差异和数据稀缺的难题，具有较强的跨模态泛化能力和实际应用潜力。

Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.

</details>


### [185] [Neural Geometry Image-Based Representations with Optimal Transport (OT)](https://arxiv.org/abs/2511.18679)
*Xiang Gao,Yuanpeng Liu,Xinmu Wang,Jiazhi Li,Minghao Guo,Yu Guo,Xiyun Song,Heather Yu,Zhiqiang Lao,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 本文提出一种以几何图像为基础的3D网格神经表示方法，实现了无需解码器的紧凑存储和高效处理，显著提升了恢复精度和压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格神经表示方法多依赖神经网络过拟合与多步解码，虽能还原高质量表面，但计算成本高，且网格结构不规则限制了直接应用高效的图像处理技术。

Method: 作者将不规则的3D网格通过几何图像（geometry image）映射到规则的图像网格，实现图像化神经处理。提出的表示方法无需解码器，采用多分辨率mipmap技术，结合最优传输（Optimal Transport, OT）算法自适应解决平坦与特征区的采样不均问题，实现连续细节层级控制。

Result: 实验结果表明，该方法在压缩比（CR）、Chamfer距离（CD）和Hausdorff距离（HD）等指标上达到目前最佳的存储效率与还原精度。

Conclusion: 本文的几何图像神经表示将三维重建转化为高效的图像处理问题，通过单次前向推理即可恢复高质量网格，具有解码器自由、存储高效、适合神经网络直接处理等优势，有望推动3D网格存储与处理新进展。

Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).

</details>


### [186] [Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework](https://arxiv.org/abs/2511.18682)
*Xiang Gao,Xinmu Wang,Zhou Zhao,Junqi Huang,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的3D结构光相位展开算法，大幅提升速度并保持高精度，适用于实时动态面部捕捉等应用。


<details>
  <summary>Details</summary>
Motivation: 现有相位展开方法在速度和精度之间存在权衡：快速方法精度不高，精确方法速度过慢，难以满足需要实时性的3D扫描应用的需求。作者旨在解决这一速度与精度兼顾的问题。

Method: 作者首次将基于GraphCut的相位展开问题重新表述为像素标注问题。其创新地结合了共形和最优传输映射生成多个微分同胚变换域，在每个域中用分层GraphCut进行标注，最终通过多数投票融合各域标签，提升相位展开准确性和鲁棒性。

Result: 实验显示，该方法在真实与仿真测试中速度提升45.5倍，且L2误差低于对比方法，兼具高速与精度。

Conclusion: 提出的多变换-GraphCut结合多数投票的相位展开框架兼顾了精度和实时性，对于结构光三维扫描中动态人脸等高要求场景，具备实际应用前景。

Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.

</details>


### [187] [Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation](https://arxiv.org/abs/2511.18684)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出了一种名为ICE（Instant Concept Erasure）的创新方法，在无需重新训练和推理开销的情况下，实现了对文本生成图像（T2I）及视频（T2V）模型中敏感内容或概念的精准、永久、安全移除。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像/视频模型的内容移除方法存在成本高、效率低、容易被攻击、影响生成质量等问题，尤其是无法处理目标概念与其他内容的语义重叠，严重时会误伤相关内容。此外，现有方法难以同时适用于T2I和T2V模型。安全且泛化性强的内容移除需求亟待满足。

Method: ICE是一种训练无关、模态无关的一次性权重修改方法。其基本思路是通过各向异性能量加权缩放构建“擦除-保留子空间”，并通过显式、闭式重叠投影算子正则化二者交集。提出了凸且有Lipschitz界限的谱去习目标函数，实现抹除与保护性之间的稳健平衡，并能解析求解。最终产生的解被应用于模型的文本条件层，实现永久且零推理开销的编辑。

Result: ICE能有效且高效地从T2I和T2V模型中移除艺术风格、物体、身份、敏感内容等目标概念。实验证明，ICE在擦除准确性、鲁棒性（例如面对红队测试）、模型原始生成能力保护等方面均优于现有方法。

Conclusion: ICE为文本到图像/视频生成模型提供了一种训练无关、永久、精准、高效的概念移除工具，大幅提升了模型内容安全控制能力和适应性，为实际安全部署奠定了坚实基础。

Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

</details>


### [188] [EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification](https://arxiv.org/abs/2511.18691)
*Kazi Reyazul Hasan,Md Nafiu Rahman,Wasif Jalal,Sadif Ahmed,Shahriar Raj,Mubasshira Musarrat,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 本论文提出了EVCC，一种结合Transformer和CNN优点的多分支视觉架构，在提升分类准确率的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前混合视觉模型虽然在图像分类上取得了进步，但通常伴随高计算成本。本文旨在设计精度更高且计算效率更优的混合视觉架构，适应真实应用需求。

Method: 提出EVCC新架构，整合Vision Transformer、ConvNeXt和CoAtNet，并创新性引入：自适应Token剪枝、双向门控交叉注意力、多任务辅助分类头，以及上下文相关的动态路由门机制，从而动态调节计算量，提升特征利用效率。

Result: 在CIFAR-100、Tobacco3482、CelebA和脑癌等多个数据集上，EVCC模型在准确率上相比DeiT-Base、MaxViT-Base和CrossViT-Base等强力模型提升最多2个百分点，同时计算量（FLOPs）降低25-35%。

Conclusion: EVCC实现了精度和计算效率的有效平衡，通过整合全局与局部特征及层次信息，展现出良好的实际应用潜力。

Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.

</details>


### [189] [Exploring Surround-View Fisheye Camera 3D Object Detection](https://arxiv.org/abs/2511.18695)
*Changcai Li,Wenwei Lin,Zuoxun Hou,Gang Chen,Wei Zhang,Huihui Zhou,Weishi Zheng*

Main category: cs.CV

TL;DR: 本文研究了如何实现适用于全景鱼眼摄像头系统的端到端三维目标检测，并提出了两种基于主流3D检测框架且适配鱼眼几何特性的改进方法。


<details>
  <summary>Details</summary>
Motivation: 传统三维目标检测方法多基于针孔型摄像头，当直接应用于鱼眼影像时性能下降明显，因此有必要专门为鱼眼影像设计适配算法。

Method: 一方面实验对比了现有针孔模型目标检测器在鱼眼场景的性能损失，另一方面提出了FisheyeBEVDet（鸟瞰视角方案）和FisheyePETR（query-based方案）两种新方法，这些方法采用球面空间表征以更好支持鱼眼几何属性，并开发了合成数据集Fisheye3DOD作为评测基准。

Result: 在Fisheye3DOD数据集上的实验表明，所提出的鱼眼适配检测方法在准确率方面比基线提升了最高6.2%。

Conclusion: 针对鱼眼影像场景的三维目标检测，结合独特的图像几何结构进行方法改进能显著提升检测性能，并推进了相关基准数据集的发展。

Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.

</details>


### [190] [Dendritic Convolution for Noise Image Recognition](https://arxiv.org/abs/2511.18699)
*Jiarui Xue,Dongjian Yang,Ye Sun,Gang Liu*

Main category: cs.CV

TL;DR: 提出一种基于神经元树突结构的抗噪卷积，显著提升了噪声环境下的视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前图像识别场景中，噪声干扰严重影响识别效果。现有方法以调整网络结构或训练策略为主，提升效果有限，缺乏从神经元结构角度探索抗干扰方案。

Method: 设计了一种模仿神经元树突结构的抗噪卷积操作（Dendritic Convolution，DDC）,将树突的邻域交互和XOR逻辑预处理思想融入卷积底层设计，通过特征间的非线性交互，实现对传统特征提取范式的重构。

Result: 在YOLOv11-cls、VGG16、EfficientNet-B0等分类任务和YOLOv11、YOLOv8、YOLOv5等目标检测任务中，DDC卷积分别让EfficientNet-B0在噪声数据集上准确率提升11.23%，YOLOv8的mAP提升19.80%。

Conclusion: 借鉴神经元树突结构的卷积方法在噪声复杂场景下有显著抗干扰效果，优于传统卷积方案。

Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.

</details>


### [191] [ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction](https://arxiv.org/abs/2511.18701)
*Mustafa Munir,Harsh Goel,Xiwen Wei,Minkyu Choi,Sahil Shah,Kartikeya Bhardwaj,Paul Whatmough,Sandeep Chinchali,Radu Marculescu*

Main category: cs.CV

TL;DR: ObjectAlign 是一个专门用于检测、验证和修复视频编辑中对象一致性问题的新颖框架，有效提升视频感知质量。


<details>
  <summary>Details</summary>
Motivation: 视频编辑常常引入如帧闪烁、身份漂移等对象不一致问题，严重影响视频观感。现有方法无法有效检测并自动修正此类高层次和时序一致性问题，亟需更强大且自动化的解决方案。

Method: ObjectAlign 结合可学习的感知一致性指标（包括 CLIP 语义相似度、LPIPS 感知距离、直方图相关性和 SAM 掩码 IoU）并引入神经-符号验证方法，通过 SMT 检查和概率时序逻辑校验进行对象身份和时序一致性的严格判定。检测出异常后，方案利用神经网络插帧，根据损坏片段自适应调整插帧深度，实现高质量帧恢复。

Result: 在 DAVIS 和 Pexels 视频数据集上，ObjectAlign 相较于现有最先进方法，CLIP Score 提升最高 1.4 分，warp error 降低最高 6.1 分，表明其具备显著的改善效果。

Conclusion: ObjectAlign 能有效检测与修复视频编辑引入的对象不一致问题，同时兼顾底层稳定性和高层次时序正确性，显著提升视频的一致性和观感质量。

Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.

</details>


### [192] [CoD: A Diffusion Foundation Model for Image Compression](https://arxiv.org/abs/2511.18706)
*Zhaoyang Jia,Zihan Zheng,Naifu Xue,Jiahao Li,Bin Li,Zongyu Guo,Xiaoyi Zhang,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新型的以压缩为导向的扩散基础模型CoD，以提升现有扩散编解码器在超低码率下的压缩效率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散编解码器多数基于如Stable Diffusion等文本-图像扩散基础模型，但文本条件限制了其在超低码率下的压缩能力，因此研究者希望开发针对压缩任务从零训练的扩散基础模型。

Method: 提出并训练了CoD（Compression-oriented Diffusion）基础模型，实现压缩与生成的端到端优化。CoD不是某一具体编解码器，而是可广泛用于扩散型编解码器的通用基础模型。同时，训练CoD仅依赖开放的图片数据集，并极大地降低了所需计算资源。

Result: 将CoD应用于下游扩散编解码器如DiffC，尤其在0.0039 bpp等超低码率下，获得了当前最优压缩效率；CoD训练速度较Stable Diffusion快约300倍；像素空间的扩散模型可达到VTM级别的PSNR，并在较少参数下超越了基于GAN的编解码器。

Conclusion: CoD首次实现了面向压缩任务的扩散基础模型，显著提升了超低码率下的编解码表现和训练效率，促进未来扩散编解码器的研究。

Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.

</details>


### [193] [Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation](https://arxiv.org/abs/2511.18711)
*Yuyang Wanyan,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了Modality-Collaborative LowRank Decomposers (MC-LRD)新框架，有效提升了小样本视频领域自适应的多模态对齐能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往小样本视频领域自适应方法忽略了视频多模态特征的耦合及不同模态间协作带来的领域转移差异，导致多模态特征融合效果不佳，跨领域泛化能力受限。

Method: 提出MC-LRD框架，通过多个低秩分解器及多模态分解路由器，将不同领域迁移程度的模态特征分解为独有和共享部分，并采用正交去相关约束提升分解多样性，引入跨域激活一致性损失增强领域对齐能力。

Result: 在三个常用数据集上实验，MC-LRD在各项指标上显著超越现有多模态小样本领域自适应方法。

Conclusion: MC-LRD框架创新性解决多模态视频领域自适应的特征耦合和对齐难题，提升了小样本情况下的对齐与泛化能力，在多个基准数据集上验证了其有效性。

Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.

</details>


### [194] [DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2511.18713)
*Hongbin Lin,Yiming Yang,Chaoda Zheng,Yifan Zhang,Shuaicheng Niu,Zilu Guo,Yafeng Li,Gui Gui,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法DriveFlow，通过频率分解和无噪声编辑路径，提升自动驾驶中基于视觉的3D目标检测模型在分布外（OOD）场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于3D目标检测数据标注成本高，且自动驾驶场景多变，训练集很难覆盖所有测试环境，导致模型在分布外场景下表现不佳。现有利用预训练扩散模型进行图像增强的方法各有局限，因此需要新方法提升数据增强质量，从而提高模型泛化能力。

Method: 提出DriveFlow，基于Text-to-Image预训练flow模型，通过频率分解引入对前景高频、背景双频的两种适应策略：1）前景高频对齐损失，保持3D目标精确几何结构；2）背景双频优化，平衡编辑灵活性和语义一致性，无需微调扩散模型。

Result: 实验表明，DriveFlow在各类分布外场景下的3D目标检测性能全面提升，验证了其高效性和有效性。

Conclusion: DriveFlow显著提升了训练数据增强对自动驾驶3D检测模型的帮助，尤其在分布外情况，增强了模型的鲁棒性和泛化能力。

Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.

</details>


### [195] [Seeing What Matters: Visual Preference Policy Optimization for Visual Generation](https://arxiv.org/abs/2511.18719)
*Ziqi Ni,Yuanzhi Liang,Rui Li,Yi Zhou,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种名为ViPO的新方法，通过像素级奖励优化视觉生成模型，使其更好地符合人类偏好，比传统GRPO表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的强化学习仅对每个图像或视频整体分配一个标量奖励，忽略了视觉内容丰富的空间和时间结构，难以纠正局部瑕疵和捕捉细粒度的感知特征。

Method: 提出ViPO方法，将标量反馈提升为结构化、像素级的优势信号。利用感知结构化模块，借助预训练视觉模型，生成时空感知的优势图，将优化压力集中于感知上重要的区域，兼顾训练稳定性。

Result: 在图像和视频基准测试中，ViPO在域内人类偏好对齐和跨域泛化能力上均优于传统GRPO方法。

Conclusion: ViPO方法架构无关、轻量，能与现有GRPO训练流程无缝兼容，为视觉生成提供了更丰富和有效的学习信号。

Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.

</details>


### [196] [GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.18729)
*Lin Liu,Caiyan Jia,Guanyi Yu,Ziying Song,JunQiao Li,Feiyang Jia,Peiliang Wu,Xiaoshuai Hao,Yandan Luo*

Main category: cs.CV

TL;DR: 本文提出了GuideFlow，一种新颖的自动驾驶路径规划框架，能够直接在生成过程中施加物理和安全约束，同时提升轨迹多样性，并通过实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶规划方法主要存在两类问题：仿真模仿型方法无法生成多样化轨迹（轨迹模式坍缩），生成型方法难以直接融入安全和物理约束，通常需后续优化修正，效率和精度受限。作者希望设计一个能整合约束且具备轨迹多样性的规划框架。

Method: 本文提出GuideFlow，通过显式的Constrained Flow Matching框架，在生成阶段直接引入各种物理和安全约束，并与能量模型（EBM）联合训练，提升模型自我优化能力。GuideFlow还引入驾驶激进性参数，实现生成过程中的轨迹风格可控。

Result: GuideFlow在Bench2Drive、NuScenes、NavSim和ADV-NuScenes等重要驾驶基准上进行了评测，显著优于现有方法。在NavSim的困难测试集（Navhard）上，取得了SOTA成绩（EPDMS得分达43.0）。

Conclusion: GuideFlow有效缓解了轨迹模式坍缩问题，并能直接灵活地在生成阶段实现多约束融合，提升路径多样性和可控性，在多项主流基准测试中达到了最优水平。

Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.

</details>


### [197] [Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion](https://arxiv.org/abs/2511.18734)
*Keyang Lu,Sifan Zhou,Hongbin Xu,Gang Xu,Zhifei Yang,Yikai Wang,Zhen Xiao,Jieyi Long,Ming Li*

Main category: cs.CV

TL;DR: 本文提出了Yo'City，一个允许用户自定义并可无限扩展的3D城市生成系统，能够通过推理和组合大型模型能力实现个性化城市，且在多项评测指标上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成方法依赖单一扩散模型，无法生成个性化且无边界的城市场景，限制了其在虚拟现实和数字孪生等应用中的潜力。

Method: Yo'City引入代理式框架，基于'城市-区块-网格'分层结构，分为全局规划（城市整体与功能区布局）与局部设计（区块详细网格描述），通过“生成-优化-评估”循环合成等距图像并生成3D模型，并设计用户交互和语义关系引导的扩展优化机制，保持城市成长的空间一致性；同时建立新数据集和六项多维评价指标。

Result: Yo'City在多个方面（语义、几何、纹理、布局）六项指标上，均显著优于现有最优方法，并支持个性化、无限扩展与用户互动。

Conclusion: Yo'City实现了高质量、可扩展、定制化3D城市生成，并为虚拟现实与数字孪生系统带来更强的生成能力和扩展性，推动了该领域的发展。

Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.

</details>


### [198] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 本文提出了Foresight Intelligence（预见智能）的新概念，并构建了用于评估该能力的视觉问答数据集FSU-QA。实验表明，当前主流视觉-语言模型（VLMs）在预见未来事件方面表现有限，FSU-QA数据集能显著提升模型的预见能力。


<details>
  <summary>Details</summary>
Motivation: 当前智能体（如自动驾驶）需要具备预见未来的能力，但现有视觉-语言模型相关研究对此关注不足。论文试图弥补这一研究空白。

Method: 作者提出了FSU-QA视觉问答数据集，专门设计用于激发和评估模型的预见智能能力。然后，利用FSU-QA对现有视觉-语言模型进行了系统性测试，并探索将世界模型产出的语义信息与VLM融合以提升推理。

Result: 实验显示，目前先进的VLM在基于预见的任务上表现不佳，而基于FSU-QA微调的小型模型甚至能超过大体验证模型。此外，FSU-QA可用于评估和提升世界模型的预测一致性。

Conclusion: FSU-QA数据集为发展具备真正预见能力的下一代智能模型提供了理论和实践基础，对推动该领域有重要意义。

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [199] [ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion](https://arxiv.org/abs/2511.18742)
*Zhenghan Fang,Jian Zheng,Qiaozi Gao,Xiaofeng Gao,Jeremias Sulam*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于反向离散化的文本到图像扩散模型ProxT2I，用条件近端算子替代得分函数，并结合强化学习优化采样器；同时发布了1,500万张高质量人像-文本对的数据集。结果显示在采样效率与人类偏好一致性上优于现有模型，并在计算量与模型规模更小的前提下，达到主流开源模型同等性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型大多基于正向离散化和从数据学得的得分函数，采样过程计算量大且不稳定。研究动力是提高采样效率、稳定性，并提升结果与人类偏好的契合度，同时降低算力和模型规模需求。

Method: 1）提出使用基于条件近端算子的反向离散化扩散网络（ProxT2I）替代传统得分函数方法；2）采样器通过强化学习与策略优化针对特定任务奖励进行优化；3）构建并开放大型高质量人像-文本配对数据集LAION-Face-T2I-15M用于训练和评估。

Result: 与基线得分方法比较，ProxT2I在提升采样效率及与人类偏好的一致性方面均表现更为出色，且算力开销与模型参数规模更低；在人类主观和客观评估上，达到与现有公开主流T2I模型相当的水平。

Conclusion: 基于反向离散化、近端算子的扩散建模可显著提升T2I生成效率与人类偏好一致性，在更低计算与参数规模下实现主流水平，是一个更轻量但高效的人类文本-图像生成方案。

Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

</details>


### [200] [Any4D: Open-Prompt 4D Generation from Natural Language and Images](https://arxiv.org/abs/2511.18746)
*Hao Li,Qiao Sun*

Main category: cs.CV

TL;DR: PEWM提出通过限制视频生成到更短的原始动作（primitive）时段，结合视觉-语言模型规划和热图引导机制，提高了视频生成型具身世界模型在数据效率、推理延迟和多任务泛化上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于视频生成的具身世界模型在实际应用中受制于对大规模、难以采集且高维的具身数据的依赖，使得语言-动作对齐和长时序视频生成变得困难，影响了通用具身智能的发展。

Method: PEWM限制视频生成的时长至较短的原始动作，通过精粒度对齐语言和动作并减小学习复杂度，提升数据使用效率。它采用模块化的视觉-语言规划器和起止点热图引导机制，支持复杂任务中原始动作级策略的组合和泛化。模型结合视频模型的时空视觉先验和视觉-语言模型的语义理解，加强物理互动与高层推理之间的衔接。

Result: PEWM实现了更高的数据采集和使用效率，更低的推理延迟，能够灵活控制原始动作粒度的任务执行，并在复杂多任务泛化上展现出更强能力。

Conclusion: PEWM方法为提升具身世界模型的可扩展性、可解释性和通用性提供了新路径，有望推动具身智能领域实现突破性进展。

Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

</details>


### [201] [From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving](https://arxiv.org/abs/2511.18757)
*Yongqi Zhu,Morui Zhu,Qi Chen,Deyuan Qu,Song Fu,Qing Yang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量、可解释的自动驾驶多车协作感知框架RefPtsFusion，通过只交换对象的参考点（如位置、速度、尺寸等），在大幅降低通信量的同时，保持感知准确性并增强系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多车协作感知主要依赖大体量特征映射或嵌入的共享，导致带宽占用高且不同传感器或模型间难以通用。因此，亟须一种高效、普适并兼顾精度的方法来解决带宽瓶颈与异构适配问题。

Method: RefPtsFusion建议车辆间仅交换对象的参考点（如位置、速度、尺寸），实现“关注何处”而非“看到了什么”。同时引入Top-K高置信度查询融合，即仅发送最有用的高置信对象，极大减少所需通信量，并使系统不依赖具体传感器或感知模型。

Result: 在M3CAD数据集上实验表明，RefPtsFusion能在感知性能几乎无损的前提下，将通信开销由每秒数百MB降至仅几KB，强于传统特征级融合方法。该方法在多种实验设定下均展现出鲁棒性和稳定性。

Conclusion: RefPtsFusion展现出高扩展性与实时性，是未来多车协作自动驾驶体系的有力候选，可解决现有方法在通信和异构适配上的难题。

Abstract: We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from "what is seen" to "where to see", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.

</details>


### [202] [VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement](https://arxiv.org/abs/2511.18763)
*Xuanzhao Dong,Wenhui Zhu,Yujian Xiong,Xiwen Chen,Hao Wang,Xin Li,Jiajun Cheng,Zhipeng Wang,Shao Tang,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无配对视网膜图像增强方法，旨在减少图像增强过程中对血管结构的破坏。方法在改善图像质量的同时，能更好地保持血管的完整性和端点信息。


<details>
  <summary>Details</summary>
Motivation: 传统基于GAN的无配对视网膜图像增强方法容易导致关键血管结构形变，影响疾病诊断的准确性。因此，亟需能同时提升图像质量并保护血管拓扑结构的方法。

Method: 提出名为Vessel-Aware Optimal Transport (VAOT) 的方法，在最优传输目标下，结合两种结构保持正则项：（1）基于骨架的损失以维持血管整体连通性；（2）端点感知损失以稳定局部血管端点。该方法适用于无配对增强场景，有助于降噪同时保持关键血管结构。

Result: 在合成退化基准数据集和实际下游血管、病灶分割任务上，所提方法在去噪和血管结构保持方面均优于多种现有主流方法。

Conclusion: VAOT方法能有效提升视网膜图像的增强质量，在保持血管结构方面显著优于现有技术，具有广泛的实际应用前景。

Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT

</details>


### [203] [NI-Tex: Non-isometric Image-based Garment Texture Generation](https://arxiv.org/abs/2511.18765)
*Hui Shan,Ming Li,Haitao Yang,Kai Zheng,Sizhe Zheng,Yanwei Fu,Xiangru Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过非等距图像生成高质量且空间一致的3D服装PBR（基于物理渲染）材质纹理，解决现有方法在复杂衣物姿态和拓扑不一致下的局限。


<details>
  <summary>Details</summary>
Motivation: 现有3D服装网格虽然几何覆盖广，但缺乏丰富的真实纹理，仅通过产生同拓扑和需精确匹配姿态的方法获取纹理，限制了服装设计的真实感与灵活性。

Method: 1) 构建了3D Garment Videos数据集，提供丰富变形下的几何与材质监督。
2) 使用Nano Banana实现高质量的非等距图像编辑，支持跨拓扑间的纹理生成。
3) 提出基于不确定性引导的多视图融合烘焙方法，将多视角预测无缝融合为高质量PBR材质。

Result: 大量实验表明，该方法的前馈双分支架构能够生成多样且空间对齐的高质量PBR服装材质，适用于工业级3D服装设计。

Conclusion: 新提出的数据集和方法显著提升了基于图像的3D服装材质生成的质量，拓展了在复杂姿态和拓扑下的实用性，为产业级3D服饰建模提供更真实丰富的纹理方案。

Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.

</details>


### [204] [Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment](https://arxiv.org/abs/2511.18766)
*Xintao Chen,Xiaohao Xu,Bozhong Zheng,Yun Liu,Yingna Wu*

Main category: cs.CV

TL;DR: 本文提出了一种用于多视角图像的无监督视觉异常检测新方法ViewSense-AD（VSAD），通过建模几何一致性，取得了比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 多视角视觉异常检测需要区分真实缺陷与由于视角变化引起的表观变化，现有方法多为单视角设计，容易导致特征不一致，误报率高。

Method: 提出了ViewSense-AD（VSAD）框架，引入了多视角对齐模块（MVAM），利用单应性将特征区块对齐，集成到潜变量扩散模型（VALDM）中，实现多阶段渐进式特征对齐。引入轻量级融合细化模块（FRM）提升特征一致性。异常检测通过与正常原型特征库的多层特征比对实现。

Result: 在RealIAD和MANTA两个具有挑战性的数据集上，VSAD在像素、视角与样本级指标均显著超越现有方法，表现出强大的对大视角变化和复杂纹理的鲁棒性。

Conclusion: VSAD通过建模视角一致性、有力对齐特征，显著降低了误报，提升了检测性能，为多视角无监督视觉异常检测领域设立了新标杆。

Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.

</details>


### [205] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的单一UNet（Re-CatVTON）模型用于虚拟试衣（VTON），在保持较低计算和内存开销的同时，实现了与双UNet模型相媲美甚至更优的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的VTON模型通常采用双UNet架构以获得更高画面保真度，但代价是较高的计算与内存消耗。为提升效率与实用性，作者希望探索单一UNet结构是否也能实现高性能。

Method: 通过可视化和理论分析，作者提出了三个关于利用上下文特征调控去噪过程的假设，并据此设计了高效的单一UNet模型（Re-CatVTON）；进一步融合了针对空间拼接条件化的改良分类器自由引导策略，以及通过注入真实服装潜码防止误差累积。

Result: Re-CatVTON在FID、KID和LPIPS等指标上较前作（CatVTON）和高性能的Leffa（双UNet）模型有显著提升，且大幅降低了计算和内存消耗，仅SSIM略有下降。

Conclusion: Re-CatVTON为单一UNet VTON模型确立了新的效率-性能平衡，证明无需复杂重结构也可实现高质量虚拟试衣效果。

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [206] [ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780)
*Ruize Ma,Minghong Cai,Yilei Jiang,Jiaming Han,Yi Feng,Yingshui Tan,Xiaoyong Zhu,Bo Zhang,Bo Zheng,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了ConceptGuard框架，实现了对多模态视频生成中安全风险的主动检测和缓解，并在两个新基准上达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态（文本+图片）驱动的视频生成模型实现了高质量、可控的视频输出，但带来了新的安全风险，现有方法主要是文本单一、需已知风险类别或仅做后审计，难以主动应对多模态、组合型的风险。

Method: 提出了ConceptGuard统一安全框架，包括两个阶段：对融合后的图像-文本输入在结构化概念空间中的对比检测以发现潜在安全风险；然后通过语义抑制机制在生成环节主动规避危险概念。此外，作者还构建了ConceptRisk大规模多模态风险数据集和T2VSafetyBench-TI2V评测基准用于训练和评测。

Result: ConceptGuard在ConceptRisk和T2VSafetyBench-TI2V两个基准上在风险检测和安全视频生成上均明显优于现有方法，达到了最新的性能水平。

Conclusion: ConceptGuard能够有效地识别和消除多模态视频生成中的安全风险，为该领域的安全防护提供了强有力的方案。

Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.

</details>


### [207] [A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data](https://arxiv.org/abs/2511.18781)
*Haotian Yan,Bocheng Guo,Jianzhong He,Nir A. Sochen,Ofer Pasternak,Lauren J O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合dMRI和fMRI的双流新方法，用于更准确地对神经纤维束流线进行分类，并在解剖上细分功能区，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于流线的分类方法侧重于几何特征，难以区分出空间轨迹类似但功能差异明显的神经纤维束，影响了功能相关脑区的准确识别与研究。作者希望结合结构和功能成像数据，提高分类的功能相关性。

Method: 提出了一种双流网络结构，一方面利用预训练的主网络对流线轨迹进行整体分类，另一方面增加辅助网络对流线端点的fMRI信号进行分析，并将两者信息联合用于流线分类。实验中以皮质脊髓束（CST）为例，进行了分区实验。

Result: 通过消融实验和与主流方法的对比，结果表明新方法在流线分类、功能一致性和分区精度方面均优于现有技术。

Conclusion: 该方法能够联合结构与功能成像信息，更精确地实现神经纤维束的功能性细分，对未来神经影像学的研究和相关疾病分析具有重要意义。

Abstract: Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.

</details>


### [208] [STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution](https://arxiv.org/abs/2511.18786)
*Junyang Chen,Jiangxin Dong,Long Sun,Yixin Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练视频扩散模型的视频超分辨率方法STCDiT，能应对复杂摄像机运动下的视频恢复，提升视频的结构真实性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频超分辨率技术在遇到复杂摄像机运动时，难以兼顾结构保真和时间稳定性，现有方法在这些方面存在明显短板。

Method: 提出了一种运动感知的VAE重建方法，将视频按运动特征分段分别重建，并提出锚帧引导机制，利用每段首帧提取的特征在生成过程中提升结构保真。两者结合增强了基于扩散模型的视频超分辨率效果。

Result: 实验结果表明，STCDiT在结构保真度和时间一致性等关键指标上都超过了现有一流方法。

Conclusion: 通过运动感知分段重建与锚帧结构信息引导，STCDiT显著提升了复杂运动场景下的视频超分辨率重建质量，证实了所提方法有效。

Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

</details>


### [209] [Understanding Task Transfer in Vision-Language Models](https://arxiv.org/abs/2511.18787)
*Bhuvan Sachdeva,Karan Uppal,Abhinav Java,Vineeth N. Balasubramanian*

Main category: cs.CV

TL;DR: 本文系统性研究了视觉-语言模型（VLMs）在视觉感知任务中的迁移能力及微调带来的影响，提出了新的评估指标，并揭示了任务间的正负迁移关系。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在多模态基准表现出色，但在特定视觉感知任务（如深度估计、目标计数）上表现不佳，同时对某一任务微调会不可预测地影响其它任务。因此需要系统地量化和理解这种任务间的迁移与干扰。

Method: 作者在3个开源VLMs上，针对13个视觉感知任务进行实验，通过引入"Perfection Gap Factor (PGF)"新指标，定量分析单任务微调对其它任务零样本性能的影响，并构建任务迁移图揭示任务关系。

Result: 实验构建了任务转移图，发现了感知任务之间未曾观测到的正迁移和负迁移关系，发现部分任务间存在互相关联效应，并能将任务按迁移特征归类。

Conclusion: PGF指标和任务关系图为多任务高效训练提供了实用建议，有助于规避负向干扰、利用正向迁移，推动VLMs感知能力提升。

Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.

</details>


### [210] [StereoDETR: Stereo-based Transformer for 3D Object Detection](https://arxiv.org/abs/2511.18788)
*Shiyi Mu,Zichong Gu,Zhiqi Ai,Anqi Liu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了StereoDETR，一个高效的基于DETR的双目3D目标检测框架，实现了实时推理，同时在准确率和速度上都超过了当前的单目和双目方法，在KITTI数据集上取得了新的SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 当前双目3D检测方法虽然准确率高于单目方法，但推理速度较慢，难以满足实时性需求。单纯依赖单目或双目也各有局限，因此需要一种兼顾准确率和速度的高效方案。

Method: StereoDETR架构包含两部分：单目DETR分支（负责2D目标检测并预测尺度、朝向和采样点）和双目分支（利用低成本多尺度视差特征估计目标级深度图）。二者通过可微分的深度采样策略耦合。为解决遮挡问题，设计了约束式监督策略，无需额外标注。

Result: StereoDETR在KITTI数据集上达到了新的行人和骑自行车者子集SOTA，在实时推理速度下首次实现了双目方法超越单目方法。

Conclusion: StereoDETR兼顾了实时性与高准确率，为双目3D目标检测提供了更优解决方案，有望推动实际应用。

Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.

</details>


### [211] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: 本文提出应用MAE风格的基础模型对大规模多样化的Wi-Fi CSI数据进行预训练，有效提升了Wi-Fi感知在不同领域中的泛化性能。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知作为一种隐私保护的替代方案，其在不同场景下泛化能力差，源于“领域偏移”和公共数据集有限。因此，需探索能提升跨域泛化能力的技术路径。

Method: 收集了14个数据集、超130万样本，涵盖4款设备和不同频段/带宽，使用MAE方法对数据进行预训练，并系统评估数据多样性和模型规模对跨域能力的影响。

Result: 实验显示，预训练数据的增加能显著提升未见域的性能，而模型容量增大带来的提升有限；在多个人体行为识别等任务上，预训练带来2.2%-15.7%的跨域准确率提升。

Conclusion: 未来Wi-Fi感知泛化能力的提升应侧重于数据规模和多样性扩展，基础大模型配合大数据预训练，有望推动其现实部署。

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [212] [PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion](https://arxiv.org/abs/2511.18801)
*Yichen Yang,Hong Li,Haodong Zhu,Linin Yang,Guojun Lei,Sheng Xu,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出PartDiffuser，一个用于点云到三维网格生成的半自回归扩散模型，提升了细节表现与整体结构一致性，效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自回归方法在三维网格生成时，难以兼顾全局一致性和局部高精细度，还容易积累错误。

Method: PartDiffuser采用“先分割、再生成”的方式，将网格分为多个语义部分，部分间用自回归，部分内用离散扩散过程，以加强全局拓扑和局部几何。核心架构基于DiT，引入了部分感知的跨注意力机制，并以点云作为动态生成的条件。

Result: 实验证实PartDiffuser在三维网格生成精细度上较其他先进方法有显著提升，尤其表现在细节丰富性上。

Conclusion: PartDiffuser有效解耦了全局和局部生成任务，能够生成具有真实应用价值的高细节三维网格，优于当前SOTA。

Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

</details>


### [213] [TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://arxiv.org/abs/2511.18806)
*Qinglei Cao,Ziyao Tang,Xiaoqin Tang*

Main category: cs.CV

TL;DR: 本论文提出了一种利用目标先验信息促进隐式学习的3D CT重建新方法，在超稀疏投影视角下实现了更高效、更高质量的三维重建。


<details>
  <summary>Details</summary>
Motivation: 传统的基于NeRF及其变体的隐式CT重建方法，在超稀疏视角下往往忽略了解剖先验信息，导致重建精度和学习效率受限。因此，提升隐式3D CT重建在极少投影下的表现，成为亟需解决的问题。

Method: 作者提出将从物体投影数据自动估算的“目标先验”引入隐式建模，同时结合位置编码与结构编码，并利用CUDA加速算法高速获取高质量的三维先验。该策略用于指导体素采样和丰富结构表达，从而提升重建效率和质量。

Result: 在复杂腹部数据集上实验表明，该方法学习效率超越主流NAF模型10倍，重建质量优于SOTA模型NeRP。具体在仅用10、20、30个投影时，分别提升PSNR 3.57 dB、5.42 dB和5.70 dB。

Conclusion: 引入基于投影数据的目标先验能显著提升稀疏视角下的隐式CT 3D重建效率和精度。所提方法有望在医疗成像等领域广泛应用。

Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.

</details>


### [214] [Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache](https://arxiv.org/abs/2511.18811)
*Yuqiu Jiang,Xiaozhen Qiao,Tianyu Mei,Haojian Huang,Yifan Chen,Ye Zheng,Zhe Sun*

Main category: cs.CV

TL;DR: 本文提出了一个无需训练的模块ADC，可以有效缓解人-物交互检测任务中的长尾分布问题，显著提升了罕见类别的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用视觉-语言模型（VLMs）的人-物交互检测方法，虽然表现提升明显，但过于依赖额外训练或提示调优，导致计算开销大、扩展性差，尤其在样本稀缺的长尾类别上表现有限。

Method: 提出了Adaptive Diversity Cache（ADC）模块。ADC在推理过程中针对每一类别动态维护一个“缓存”，收集置信度高且多样化的特征表示。通过类别频率感知的机制，特别照顾到罕见类别，无需额外训练或微调即可完成预测校准。

Result: 在HICO-DET与V-COCO数据集上评测，ADC能为现有HOI检测器带来持续提升，罕见类别mAP提升可达8.57%，整体提升4.39%，尤其提升了长尾类别的检测能力。

Conclusion: ADC模块有效缓解了长尾分布带来的类别不均衡问题，无需增加训练负担即可集成到现有方法中，对实际HOI检测任务具有广泛适用性。

Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

</details>


### [215] [DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video](https://arxiv.org/abs/2511.18814)
*Jiawei Hou,Shenghao Zhang,Can Wang,Zheng Gu,Yonggen Ling,Taiping Zeng,Xiangyang Xue,Jingbo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的4D目标检测方法（DetAny4D），并构建了一个大规模4D检测数据集（DA4D），显著提升了4D目标检测的准确性和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有4D目标检测方法要么逐帧处理，缺乏时序一致性建模，要么依赖多阶段复杂流程，容易产生错误累积。此外，缺乏大规模、连续的高质量3D标注数据集也限制了该领域的进步。

Method: 1）首先采集、标注并发布了包含28万多段高质量3D边界框标注的大规模4D检测数据集（DA4D）；2）提出了DetAny4D框架，能直接端到端从时序多模态输入预测3D目标框，使用预训练模型融合多模态特征，并设计了几何感知的时空解码器以精准建模空间和时间动态；3）采用多任务学习和专门的训练策略保证长序列全局一致性。

Result: 大量实验表明，DetAny4D在检测准确度上具有竞争力，并且在提升4D检测的时间稳定性和全局一致性方面效果显著，尤其解决了抖动和不一致等历史性难题。

Conclusion: DetAny4D结合了大规模数据和创新网络设计，从根本上提升了4D目标检测的表现并推动了该领域发展；数据和代码将在论文接收后公开。

Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.

</details>


### [216] [SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2511.18816)
*Nimeshika Udayangani,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: 本文提出了SupLID方法，利用像素级语义空间的几何结构，提升语义分割中的异常检测能力，并实现了领先的检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割中的异常检测主要借鉴图像级置信度方法，但其易于过度自信，对多样异常场景检测效果有限。因此，亟需新方法提升像素级OOD检测精度。

Method: 作者提出SupLID框架，利用线性内在维数(LID)分析高维数据的局部结构，通过构建几何核心集并在超像素级别计算OOD分数。这结合了传统置信度与空间几何特征，实现高效、平滑的异常检测。SupLID为部署友好的后处理方法，可集成至任意分割模型。

Result: 实验表明，SupLID提升了现有分类器的OOD检测性能，在AUR、FPR、AUP等指标上达到SOTA表现。

Conclusion: SupLID能够提供传统置信度方法的有力补充，显著改善语义分割中的异常检测能力，实用性强且易于集成。

Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.

</details>


### [217] [Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring](https://arxiv.org/abs/2511.18817)
*Siyuan Wei,Chunjie Wang,Xiao Liu,Xiaosheng Yan,Zhishan Zhou,Rui Huang*

Main category: cs.CV

TL;DR: 提出了一种全自动管道，将原始3D扫描数据转化为无歧义、高质量的对话数据，大幅降低生成多模态3D大模型训练数据的成本，并显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D多模态大语言模型（MLLM）发展落后于2D模型，主要瓶颈在于缺乏大规模高质量的3D场景-对话数据。现有数据集高度依赖人工标注且存在视角和物体指代歧义，导致模型能力受限。

Method: 设计了一个全自动流水线，结合规则约束、2D MLLM及LLM，无需人工即可规模化生成有控制的高质量对话数据。流程包含四步：元注释采集、场景图构建与关系校正、判别式物体指代生成、和多任务对话数据合成。

Result: 生成了Disc3D数据集——涵盖2.5万混合3D场景、超过200万样本，支持场景/视角/物体描述、视觉定位、5类基于物体的问答。实验表明，基于Disc3D训练的模型在公开基准与自有任务上均取得了显著和一致的性能提升。

Conclusion: 该管道显著提升了3D多模态大语言模型的数据规模、质量和训练效果，为领域发展奠定了基础。代码、数据和模型计划对外开放。

Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.

</details>


### [218] [DiP: Taming Diffusion Models in Pixel Space](https://arxiv.org/abs/2511.18822)
*Zhennan Chen,Junwei Zhu,Xu Chen,Jiangning Zhang,Xiaobin Hu,Hanzhen Zhao,Chengjie Wang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文提出DiP框架，将扩散模型的生成过程分为全局和局部两个阶段，实现了无需VAE的高效高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成质量与计算效率之间存在权衡。VAE+LDM有信息损失和非端到端训练问题，而像素空间方法在高分辨率时计算开销巨大。

Method: 提出DiP框架，将生成过程分解为两部分：一个基于扩散Transformer（DiT）的大块全局结构生成和一个轻量Patch Detailer Head补充局部细节，两者协同训练。整个框架无需VAE，提升效率和质量。

Result: DiP在保有LDM级别计算效率的同时，实现了比以往方法快10倍的推理速度，参数量仅增加0.3%。在ImageNet 256×256任务上，获得了1.90的FID分数。

Conclusion: DiP有效突破了原有扩散模型的质量与效率矛盾，提供了高效、端到端、无需VAE的像素空间扩散新方案。

Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

</details>


### [219] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: 该论文提出了VideoPerceiver，一种新的视频多模态大模型，解决了现有模型对短暂动作和罕见事件感知能力不足的问题，并通过创新训练方法显著提升了细粒度视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频多模态大模型在识别短时动作或长视频中的罕见、瞬时事件时表现较弱，很难进行细致的时空推断。因此，需要提升模型对细粒度视频信息的感知能力。

Method: 提出了两阶段训练框架：第一阶段（有监督微调）通过构造“关键信息缺失”的视频，并联合编码视频文本与原始视频，使用对比损失增强模型对细粒度动态线索的敏感性；第二阶段（强化学习）输入完整与降质版本视频，对比生成描述，并引入相对奖励机制让模型更精确恢复运动细节。此外，作者构建了包含8万个细粒度动作与瞬时事件的视频数据集。

Result: 在细粒度动作理解和罕见事件描述的基准测试中，VideoPerceiver表现远超当前主流模型，在标准任务上也保持了良好表现。

Conclusion: 通过关注任务相关的视觉特征，VideoPerceiver有效提升了细粒度视频理解能力，为视频-语言模型细粒度感知开辟了新路径。

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [220] [Q-Save: Towards Scoring and Attribution for Generated Video Evaluation](https://arxiv.org/abs/2511.18825)
*Xiele Wu,Zicheng Zhang,Mingtao Chen,Yixian Liu,Yiming Liu,Shushi Wang,Zhichao Hu,Yuhong Liu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 本文提出了Q-Save，一个用于AI生成视频质量评估的新数据集和模型，支持可解释性分析。该数据集包含约10000条视频，具备多维细致标注，并配套开发了统一的评分和解释模型，显著提升了评估精度和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频（AIGV）质量评估方法缺乏多维度细致标注和可解释性，无法充分支持对视频内容的全面评价和分项溯因，因此亟需高质量数据集与解释性模型。

Method: 1）建立包含近1万条视频及三大维度精细标注（视觉质量、动态质量、文本-视频对齐）的新数据集。2）提出基于SlowFast的统一模型，用高分辨率处理慢帧、低分辨率处理快帧，实现评估精度与效率平衡。3）采用COT格式数据、多阶段训练（监督微调-SFT、分组相对策略优化-GRPO、再SFT）提升模型能力和稳定性。

Result: 模型在视频质量预测任务上达到了当前最优表现，并给出了与人工一致、可解释的评分说明。

Conclusion: Q-Save数据集和模型为生成视频质量的可解释性评估提供了先进基础，有助于多模态生成和可信AI领域的发展。

Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

</details>


### [221] [Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification](https://arxiv.org/abs/2511.18826)
*Aakash Gore,Anoushka Dey,Aryan Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种考虑教师模型预测不确定性的双学生知识蒸馏框架，使学生模型更有效地学习高质量知识，并实现了更优性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法未区分教师模型预测的置信度，简单地对所有预测信息一视同仁，可能让学生模型学习到低置信度的误导性知识，因此需要一种基于不确定性的方法来提升蒸馏效率。

Method: 提出了基于教师模型预测不确定性的双学生知识蒸馏方法，引入ResNet-18和MobileNetV2两种结构的学生模型，二者通过与教师模型及彼此协作进行互相学习，结合不确定性对教师输出进行筛选，有针对性地引导学生模型训练。

Result: 在ImageNet-100数据集上，所提方法显著超越了传统单学生知识蒸馏基线，ResNet-18和MobileNetV2的Top-1准确率分别达到83.84%和81.46%，比基线提升2.04%和0.92%。

Conclusion: 充分利用教师模型不确定性并结合不同结构学生的协同学习，可有效增强知识蒸馏效果，提高学生模型表现，优于传统知识蒸馏方案。

Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.

</details>


### [222] [Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection](https://arxiv.org/abs/2511.18827)
*Mohammadreza Amiri,Monireh Hosseini*

Main category: cs.CV

TL;DR: 本研究提出了一种结合深度学习与群体智能优化的新模型，用于通过可穿戴多模态数据更客观、高效地检测焦虑障碍。


<details>
  <summary>Details</summary>
Motivation: 当前焦虑障碍的检测依赖主观评估，如临床访谈和问卷，效率低且依赖评估者，缺乏自动化和一致性。人工智能发展为自动检测提供了新可能，亟需新的方法来克服传统不足。

Method: 模型融合深度学习架构与受群体智能启发的优化策略，利用多模态和可穿戴传感器数据，分析生理、情感和行为信号。采用遗传算法和粒子群优化对特征空间和超参数进行优化，深度学习部分则负责从多源序列数据中提取分层判别特征。

Result: 与单独使用深度网络相比，融合两种计算范式显著提高了检测准确率，并表现出更好的泛化能力。

Conclusion: 结合元启发式优化与深度学习，有望开发出可扩展、客观且临床上有意义的焦虑障碍检测方案。

Abstract: Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders

</details>


### [223] [VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction](https://arxiv.org/abs/2511.18831)
*Shaobo Wang,Tianle Niu,Runkang Yang,Deshan Liu,Xu He,Zichen Wen,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 文章提出了一种视频数据合成与压缩的新方法VideoCompressa，通过压缩视频关键帧极大提升视频数据的利用效率和学习性能，在大幅减少数据存储和计算成本的同时还提升了模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型的扩展性受限于大规模视频数据集的存储和算力消耗。现有图像领域的数据合成已提升数据效率，但视频领域受时序冗余与时空动态复杂影响，合成挑战更大。作者发现视频数据效率低下的关键在于样本内部的帧级冗余，而非不同样本之间的重复。

Method: 提出VideoCompressa，将视频数据合成表述为一种动态潜在变量压缩问题。其方法包括一个可微关键帧选择器(轻量级ConvNet+Gumbel-Softmax采样)来筛选信息量高的帧，并用预训练VAE对关键帧进行潜在编码压缩，再通过压缩网络实现端到端训练。关键帧选择器和潜在编码联合优化以保最大任务相关信息。

Result: 在UCF101数据上，VidCompressa仅使用0.13%的原始数据量即可比全数据训练提升2.34个百分点，效率提升超过5800倍；在HMDB51数据集上微调Qwen2.5-7B-VL模型，用0.41%数据可达全数据性能，比零样本高10.61%。

Conclusion: VideoCompressa极大提升了视频数据集的数据效率，显著减少了存储和算力消耗，同时模型效果反而更好，为今后大规模视频理解应用与合成提供了更高效的解决方案。

Abstract: The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.

</details>


### [224] [FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories](https://arxiv.org/abs/2511.18834)
*Lei Ke,Hubery Yin,Gongye Liu,Zhengyao Lv,Jingcai Guo,Chen Li,Wenhan Luo,Yujiu Yang,Jing Lyu*

Main category: cs.CV

TL;DR: 本文提出了FlowSteer方法，通过引导学生模型沿着教师模型的真实生成轨迹，显著提升了基于ReFlow的蒸馏技术性能，并修复了推理质量下降的关键调度器缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然流匹配（flow matching）技术在视觉生成任务中表现出色，但采样效率仍是实际应用的瓶颈。已有的ReFlow加速方法理论上与流匹配一致，但实际效果不如其它主流蒸馏方法，很少被关注。作者希望解决ReFlow在实际应用中的性能不足问题。

Method: 作者分析了ReFlow存在的性能障碍，提出FlowSteer，通过引导学生模型学习并沿用教师模型的真实生成轨迹提升效果。具体做法包括发现和修正ReFlow训练中的分布不匹配问题，提出在线轨迹对齐（OTA）方法。此外，引入了一种直接作用于ODE轨迹上的对抗蒸馏目标，并修复了当前常用的离散调度器在推理中步骤较少时的重大缺陷。

Result: 在稳定扩散3（SD3）模型上的实验表明，FlowSteer能够显著提升基于ReFlow蒸馏的性能，且修正后的调度器提高了少步推理的效果。

Conclusion: 该方法解锁了ReFlow蒸馏的潜力，实现了更高效的视觉生成采样与更好的模型性能，为实际应用中流模型的高效采样和蒸馏提供了新思路和可行方案。

Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.

</details>


### [225] [FVAR: Visual Autoregressive Modeling via Next Focus Prediction](https://arxiv.org/abs/2511.18838)
*Xiaofan Li,Chenming Wu,Yanpeng Sun,Jiaming Zhou,Delin Qu,Yansong Qu,Weihao Bo,Haibao Yu,Dingkang Liang*

Main category: cs.CV

TL;DR: 本文提出FVAR模型，通过模仿相机对焦过程（由模糊到清晰）来改善视觉自回归模型因尺度下采样而导致的混叠伪影问题。FVAR用物理一致的离焦模糊代替常规下采样，结合高频残差学习，极大提升了细节质量与文本可读性，并与现有VAR兼容。


<details>
  <summary>Details</summary>
Motivation: 传统视觉自回归（VAR）模型采用统一尺度下采样建立金字塔，但这种方式易引入混叠伪影和细节损失，影响生成图像质量。本文旨在从根本上解决因下采样导致的细节失真和伪影问题。

Method: 作者提出三大创新：（1）引入“下一个对焦”预测范式，以逐步去模糊代替传统下采样；（2）采用物理一致的离焦核递进构建金字塔，多层模糊平滑过渡消除混叠；（3）高频残差学习，借助教师网络学习并迁移混叠残差信息，部署阶段保持简单网络结构。

Result: 在ImageNet上，FVAR显著减少混叠伪影，提升了细节保留和文本可读性，实验效果优于传统方法，同时可无缝兼容现有VAR系统。

Conclusion: FVAR有效解决了多尺度下采样带来的混叠与细节损失问题，通过更自然的模糊—清晰转变和高频残差信息的引入，提升了视觉生成模型的质量与适用性。

Abstract: Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.

</details>


### [226] [Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification](https://arxiv.org/abs/2511.18839)
*Yasiru Laksara,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文在胸部疾病诊断中，通过引入深度集成方法，显著提升了模型预测的不确定性量化和置信度校准能力，使深度学习辅助诊断系统更加可靠和可解释。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型如CheXNet在临床高风险场景下缺乏对预测置信度的可靠度量，这限制了其临床应用价值。因此迫切需要为此类模型加入稳健的不确定性量化机制。

Method: 作者首先尝试采用Monte Carlo Dropout (MCD)实现不确定性量化，但模型性能和校准表现较差（ECE高达0.7588）。随后，作者转而采用包含9个多样化子模型的深度集成（Deep Ensemble, DE）方法，提升了诊断平台的稳定性和鲁棒性，同时支持对不确定性来源（Epistemic和Aleatoric）进行分解。

Result: 该集成方法在NIH ChestX-ray14数据集上取得了SOTA的AUROC（0.8559）和较高的平均F1分数（0.3857），校准指标（平均ECE 0.0728，NLL 0.1916）显著优于MCD，并能够有效分离和量化Aleatoric与Epistemic不确定性。

Conclusion: 深度集成方法显著提升了胸部X光多病种诊断的可靠性与可解释性，可作为值得信赖的临床辅助决策支持系统，推动深度学习模型在实际医疗场景的落地应用。

Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.

</details>


### [227] [Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration](https://arxiv.org/abs/2511.18847)
*Ishmam Tashdeed,Md. Atiqur Rahman,Sabrina Islam,Md. Azam Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种新的个性化联邦学习方法，用于跨器官的肿瘤分割，通过引入解耦跨注意力机制和边界感知损失函数，有效提升了分割准确率，并在多个任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割中，不同客户端拥有不同器官的数据，现有个性化联邦学习方法未充分利用客户端间的共享特征，从而无法最优地解决数据异质性与分割一致性的问题。

Method: 作者提出FedOAP方法，核心包含：1）解耦跨注意力模块（DCA），允许各客户端在保持本地特征的同时，关注全体客户端共享的关键特征，以建模长距离依赖；2）设计扰动边界损失（PBL），专注于提升预测掩码边界的一致性与精度。

Result: 在多个涵盖不同器官肿瘤分割的数据集上进行实验，FedOAP在准确率和一致性上均优于现有最先进的联邦及个性化分割方法。

Conclusion: FedOAP有效结合了全局共享特征与本地个性化信息，提升了联邦医疗分割的表现，并为跨器官分割任务提供了更优的解决方案。

Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.

</details>


### [228] [Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization](https://arxiv.org/abs/2511.18851)
*Yilin Wen,Kechuan Dong,Yusuke Sugano*

Main category: cs.CV

TL;DR: 本文提出了一种新的在线测试时自适应方法，用于3D人体姿态估计，有效缓解了自监督过程中因预测不准导致的误差累积问题。通过运动离散化和软重置机制，提升了长时间自适应过程中的稳健性和准确性。实验表明该方法优于以往的自适应方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体姿态估计算法在训练和测试时会遇到领域间差异（domain gap），且在无监督情况下进行在线自适应时，依赖自身预测来作为监督信号，容易导致误差逐步累积，最终降低模型性能。作者为了解决这一普遍且重要的问题，提出新的自适应方案。

Method: 1. 通过运动潜表示空间中的无监督聚类，离散得到一组锚点（anchor motions），这些锚点具有较强的规律性，可用于辅助姿态估计器的自监督和自回放；2. 在连续自适应过程中，设计了一种高效的软重置机制，通过回退至指数滑动平均的参数，避免模型陷入错误积累。

Result: 提出的方法在长时间自适应的不断变化场景下，能够抑制误差积累，显著提升了模型对个人体型和运动特征的捕捉能力。实验结果显示该方法在准确率和鲁棒性上优于以往其他在线测试自适应方法。

Conclusion: 通过运动离散化和软重置机制，有效解决了3D人体姿态估计在线自适应过程中的误差积累问题，从而实现了更高的准确性和稳定性。方法也验证了设计思路的有效性和先进性。

Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

</details>


### [229] [Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos](https://arxiv.org/abs/2511.18856)
*Sana Alamgeer*

Main category: cs.CV

TL;DR: 本项目提出了一种新的模型，用于预测360度视频中的感兴趣区域（ROI），通过构建混合显著性模型提升ROI的识别效果，并与公开数据集标注进行了对比。


<details>
  <summary>Details</summary>
Motivation: 360度视频中的ROI对于带宽优化和提升用户体验非常关键。提前识别ROI能够辅助视窗预测、智能剪辑视频、减少带宽消耗并提升观看流畅性。

Method: 预处理视频获得帧图像，设计并训练一个混合显著性（saliency）模型用于预测ROI，最终对模型输出进行后处理获得每帧的ROI，并与360RAT数据集的人为标注进行性能对比。

Result: 该方法能够有效预测360度视频中的ROI，且在与360RAT数据集标注的对比中表现出不错的性能。

Conclusion: 混合显著性模型在预测360度视频ROI任务中表现良好，有潜力应用于实际流媒体优化和提升VR观看体验。

Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

</details>


### [230] [Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling](https://arxiv.org/abs/2511.18858)
*Xiao Cui,Yulei Qin,Xinyue Li,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种面向长尾分布的新型数据集蒸馏方法，显著提升了模型在不平衡数据上的表现，在多个基准上大幅超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法在均衡数据集上有效，但在类别极度不平衡（长尾分布）情况下表现不佳，主要由于模型偏差和统计估计失准，例如BN参数被破坏。急需解决长尾数据集蒸馏场景中的偏差与监督失衡问题。

Method: 方法从统计对齐角度改进传统蒸馏：（1）增强专家模型（观察者模型用于恢复、教师模型用于软标签）以获得可靠统计与标签；（2）动态调整Momentum，通过整体前向传播重校BatchNorm统计，减少特征偏斜；（3）引入多轮高置信度多样性选择机制，逐步初始化合成样本，提升覆盖与多样性。

Result: 在四个长尾数据集基准上进行实验，提出的方法相比最优现有方法在各种类别不均衡设置下均取得了显著提升，并在CIFAR-100-LT和Tiny-ImageNet-LT等数据集下，在IPC=10和IF=10设置下，Top-1准确率分别大幅提升15.6%和11.8%。

Conclusion: 通过专家模型增强、BN统计重校和多轮高多样性初始化，结合统计对齐视角的方法能有效缓解长尾分布下的数据集蒸馏难题，对提升不平衡数据场景下的小样本优化和公平性具有重要意义。

Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

</details>


### [231] [DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection](https://arxiv.org/abs/2511.18865)
*Yu Zhang,Haoan Ping,Yuchen Li,Zhenshan Bing,Fuchun Sun,Alois Knoll*

Main category: cs.CV

TL;DR: 该论文提出了一种简单但高效的纯Transformer架构DualGazeNet，显著提升了显著性目标检测（SOD）任务的准确率和效率，同时减少了模型复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前SOD方法通过引入语义增强、边界优化、辅助任务监督和多模态融合等技术，虽提升了性能，但导致模型结构愈加复杂，出现特征冗余和组件间干扰，从而限制了性能提升。该论文受到人类视觉系统高效识别机制的启发，探索能否用生物学原理简化SOD模型结构的可能性。

Method: 提出了一种名为DualGazeNet的纯Transformer框架，模拟人类视觉系统中并行的M-通路（动态/粗略）与P-通路（静态/细节）处理，以及皮层注意力调控机制，强调鲁棒表征和高效推理。该方法用较为生物本真的路径设计代替传统复杂的多阶段、专门融合、边缘引导等策略。

Result: 在五个主流RGB SOD数据集上的实验结果表明，DualGazeNet在整体性能上优于25种当前主流的CNN和Transformer方法。相比相似规模的四种主流Transformer baseline（VST++、MDSAM、Sam2unet、BiRefNet），DualGazeNet平均推理速度提升约60%，FLOPs减少53.4%。其还在伪装和水下SOD等跨领域任务表现出领先或具竞争力的性能，并无需额外模态支持。

Conclusion: DualGazeNet证明了以人类视觉为灵感的简单架构可以兼顾高精度、高效率和良好的泛化性，为SOD领域模型设计带来了新的思路和范式。

Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.

</details>


### [232] [HunyuanVideo 1.5 Technical Report](https://arxiv.org/abs/2511.18870)
*Bing Wu,Chang Zou,Changlin Li,Duojun Huang,Fang Yang,Hao Tan,Jack Peng,Jianbing Wu,Jiangfeng Xiong,Jie Jiang,Linus,Patrol,Peizhen Zhang,Peng Chen,Penghao Zhao,Qi Tian,Songtao Liu,Weijie Kong,Weiyan Wang,Xiao He,Xin Li,Xinchi Deng,Xuefei Zhe,Yang Li,Yanxin Long,Yuanbo Peng,Yue Wu,Yuhong Liu,Zhenyu Wang,Zuozhuo Dai,Bo Peng,Coopers Li,Gu Gong,Guojian Xiao,Jiahe Tian,Jiaxin Lin,Jie Liu,Jihong Zhang,Jiesong Lian,Kaihang Pan,Lei Wang,Lin Niu,Mingtao Chen,Mingyang Chen,Mingzhe Zheng,Miles Yang,Qiangqiang Hu,Qi Yang,Qiuyong Xiao,Runzhou Wu,Ryan Xu,Rui Yuan,Shanshan Sang,Shisheng Huang,Siruis Gong,Shuo Huang,Weiting Guo,Xiang Yuan,Xiaojia Chen,Xiawei Hu,Wenzhi Sun,Xiele Wu,Xianshun Ren,Xiaoyan Yuan,Xiaoyue Mi,Yepeng Zhang,Yifu Sun,Yiting Lu,Yitong Li,You Huang,Yu Tang,Yixuan Li,Yuhang Deng,Yuan Zhou,Zhichao Hu,Zhiguang Liu,Zhihe Yang,Zilin Yang,Zhenzhi Lu,Zixiang Zhou,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanVideo 1.5是一款轻量级且高性能的开源视频生成模型，以仅8.3亿参数实现了业界领先的视频质量与运动连贯性，并可在消费级GPU上高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有高质量视频生成模型参数量大、推理受限，难以在普通硬件上高效运行，视频创作门槛高。该研究旨在降低硬件和算法门槛，推动先进视频生成技术的普及。

Method: 模型采用先进的DiT架构，并引入SSTA（选择性与滑动块注意力）机制，结合字形感知的双语文本编码、渐进式预训练与微调，以及高效的视频超分辨率网络，实现了高质量的文本生成视频与图像生成视频。

Result: 模型在视觉质量和运动连贯性方面均达到了开源模型中的最新水平。实验结果显示其性能优异，且在多时长和分辨率场景下表现出色。

Conclusion: HunyuanVideo 1.5为开源社区带来了高性能、低门槛的视频生成基础，降低了创作难度，支持更广泛的研究和应用。所有代码与模型已开放，促进了学术与产业的进一步发展。

Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.

</details>


### [233] [Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction](https://arxiv.org/abs/2511.18873)
*Yiming Wang,Shaofei Wang,Marko Mihajlovic,Siyu Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种改进3D高斯点渲染的新方法Neural Texture Splatting（NTS），以提升在多种3D和4D重建任务中的表现，特别是在稀疏和密集输入下的效果均有优势。


<details>
  <summary>Details</summary>
Motivation: 虽然3D高斯点渲染（3DGS）在新视角合成等任务中取得巨大成功，但其表达能力受到局部高斯核建模方式的限制。现有提升表现的方法（如每点纹理）对点数减少有效，但在更多通用重建场景中的表现有限。作者希望提升3DGS在各种任务下的泛化能力和表达效果。

Method: 作者提出Neural Texture Splatting（NTS），采用了一个以三平面与神经解码器混合形式构成的全局神经场，对每个原语预测局部外观和几何属性。通过全局共享的神经场学习所有局部纹理，实现了高效的信息共享，同时可以建模视角依赖和时间依赖的复杂纹理变化。

Result: NTS在多个基准数据集和任务上均超越了现有3DGS方法，在新视图合成、几何和动态重建等任务中表现突出，同时显著降低了模型参数量。

Conclusion: Neural Texture Splatting不仅提升了3DGS模型的表现和泛化能力，还支持高效且精细的视角/时序依赖渲染，在各类3D/4D重建任务中具有广泛应用前景。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

</details>


### [234] [Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference](https://arxiv.org/abs/2511.18875)
*Wengyi Zhan,Mingbao Lin,Zhihang Lin,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出了一种名为ParVTS的训练无关视觉token调度方法，在保持多模态大模型（MLLMs）性能的同时，大幅减少计算量和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 在多模态大模型中，高分辨率图片带来的大量视觉token导致计算量和推理延迟显著增加。直接剪枝不重要的token虽可减轻负担，却会失去对背景或细节问题至关重要的信息，影响准确率。因此，需要一种在不影响性能的前提下高效减 token 的方案。

Method: 提出ParVTS框架，将视觉token分为主体组和非主体组并行处理，分别将语义迁移到问题token后，在中途丢弃非主体组，仅保留主体信息进行后续推理，从而减少不必要计算，不依赖额外训练和模块，适配多种主流MLLM架构。

Result: 在多种MLLM骨干网络上实验，ParVTS可以剪除高达88.9%的视觉token，几乎不影响模型表现，实现了1.77倍推理加速和70%的FLOPs减少。

Conclusion: ParVTS有效提升了MLLM的推理效率，减少了计算资源消耗，又能保证精度下降最小，且具良好泛化性与兼容性，是提升多模态模型实际应用潜力的实用方案。

Abstract: Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.

</details>


### [235] [Facade Segmentation for Solar Photovoltaic Suitability](https://arxiv.org/abs/2511.18882)
*Ayca Duran,Christoph Waibel,Bernd Bickel,Iro Armeni,Arno Schlueter*

Main category: cs.CV

TL;DR: 本论文提出了一种自动识别建筑立面适用光伏的位置和估算光能潜力的机器学习流程，为城市建筑一体化光伏（BIPV）部署提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有针对屋顶光伏规划的机器学习方法较成熟，但针对建筑立面的自动化方法仍然缺乏且简单化。因此，亟需针对立面的精确、自动化BIPV选址方法，以推动城市减碳。

Method: 作者基于SegFormer-B5，在CMP Facades数据集上进行微调，通过语义分割获得建筑立面表面的结构信息。然后将预测结果转化为BIPV适用性掩模和面板布局，考虑光伏板尺寸和间隙要求，并在含有373个、来自10座城市的已知尺寸立面数据集上进行验证。

Result: 结果显示，实际可安装的BIPV潜力远低于理论潜力，这为城市能源规划带来了更现实的参考。方法能够自动高效识别立面合适的光伏区域。

Conclusion: 随着立面图像数据日益丰富，所提流程可扩展至全球城市BIPV规划，助力城市低碳发展和更准确的能源评估。

Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.

</details>


### [236] [MagicWorld: Interactive Geometry-driven Video World Exploration](https://arxiv.org/abs/2511.18886)
*Guangyuan Li,Siming Zheng,Shuolin Xu,Jinwei Chen,Bo Li,Xiaobin Hu,Lei Zhao,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本论文提出MagicWorld，一种结合3D几何先验和历史检索的新型交互式视频世界模型，有效提升了场景结构稳定性和互动连续性。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式视频世界模型在面对用户指令驱动场景变化时，存在难以保持3D结构稳定性以及多步互动中历史信息遗失，导致场景语义和结构漂移等问题。

Method: MagicWorld从单帧图像开始，通过用户操作驱动动态场景演化，利用自回归方式连续生成场景。提出了Action-Guided 3D Geometry Module（AG3D），基于初始帧和用户行动构建点云，实现显式几何约束提升结构一致性。同时，设计History Cache Retrieval（HCR）机制，在生成过程中检索关键历史帧并作为条件信息输入，减少误差积累，增强历史信息利用。

Result: MagicWorld在场景结构稳定性和多轮交互连续性方面表现显著优于现有方法，实验结果显示其在保留结构和语义一致性上有较大提升。

Conclusion: 该方法能够有效缓解现有交互式视频世界模型中的结构不稳定和信息遗忘问题，为以后相关研究和实际应用提供了新的思路。

Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

</details>


### [237] [MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model](https://arxiv.org/abs/2511.18888)
*Qian Jiang,Qianqian Wang,Xin Jin,Michal Wozniak,Shaowen Yao,Wei Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的多功能模型MFmamba，能够在仅有高分辨率全色（PAN）图像输入的情况下，实现超分辨率（SR）、光谱恢复以及联合SR与光谱恢复三项任务。所提模型在多个实验任务、评估指标和视觉结果中均展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 遥感领域单一传感器受限，通常只能获得高空间分辨率的灰度全色图像和低空间分辨率的彩色多光谱图像，难以直接得到高空间高光谱分辨率的彩色图像。现有方法（SR和着色、融合）各有短板，不能同时提升空间和光谱分辨率，且多需多个已配准输入。为解决上述限制，作者希望提出一种集成式新模型，实现单一输入下的多任务输出。

Method: 本文构建了一个多功能模型MFmamba，以UNet++为主干网络，并结合了Mamba Upsample Block（MUB）模块；引入了Dual Pool Attention（DPA）替换UNet++中的跳跃连接；提出了Multi-scale Hybrid Cross Block（MHCB）进行初始特征提取。通过三种不同输入，模型可完成超分辨率、光谱恢复及其联合任务。

Result: 实验结果表明，MFmamba在仅输入PAN图像的情况下，于超分辨率、光谱恢复及联合任务三项上，在评价指标和视觉效果上均取得了优良、竞争性的性能。

Conclusion: MFmamba模型有效解决了现有方法无法在单一输入下兼顾空间与光谱分辨率提升的问题，为遥感图像的高质量恢复和增强提供了新方案。

Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.

</details>


### [238] [MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting](https://arxiv.org/abs/2511.18894)
*Chenyu Mu,Guihai Chen,Xun Yang,Erkun Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出了一种新颖的医学图像分割方法MetaDCSeg，通过动态像素权重和边界不确定性建模，显著提升了有噪声标注情况下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常受噪声标注和模糊边界影响，现有方法对于边界区的噪声鲁棒性不足，对分割性能造成影响。

Method: 设计了MetaDCSeg框架，动态为每像素分配权重以抑制噪声标签影响，并通过动态中心距离机制，显式建模边界不确定性，重点关注难以分割的边界像素。

Result: 在四个不同噪声级别的公开数据集上，MetaDCSeg在性能上均优于现有主流分割方法。

Conclusion: MetaDCSeg有效缓解了边界噪声对医学图像分割的负面影响，在高噪声场景下也能获得稳健的分割效果。

Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.

</details>


### [239] [Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation](https://arxiv.org/abs/2511.18919)
*Ruiying Liu,Yuanzhi Liang,Haibin Huang,Tianshu Yu,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的后训练生成模型优化方法BPGO，通过引入贝叶斯语义先验，在图像和视频生成任务中，显著提升了语义对齐、感知质量及收敛速度，优于现有GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO框架用于生成模型优化时，受限于文本与视觉数据的多对多关系，导致奖励信号不可靠、优化效果有限。需要更好地处理文本与图像间的歧义和奖励信号的不确定性。

Method: 作者提出BPGO（Bayesian Prior-Guided Optimization），在GRPO框架基础上融入贝叶斯语义先验。具体做法为：1）组间层面通过贝叶斯信任分配，强化和先验一致的更新，弱化歧义组的影响；2）组内基于先验进行归一化，突出置信得分，压缩不确定样本分数。该方法能自适应调节优化过程中对奖励不确定性的信任。

Result: BPGO在图像和视频生成任务上，均表现出比标准GRPO及其变体更强的语义对齐能力、更高的感知保真度，以及更快的收敛速度。

Conclusion: BPGO有效解决了GRPO中奖励模型信号不稳定的问题，提升了模型对语义和感知的把控，为后训练生成模型优化提供了更强和更稳定的方法。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

</details>


### [240] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 该论文提出了一个无训练、高效的视频时空理解方法EventSTU，大幅降低了计算量，并提升了理解性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大模型虽然具备强大理解能力，但面对长视频时计算量和推理成本极高。受事件视觉启发，作者探索如何通过筛选更有用的信息点，实现高效处理。

Method: 提出了EventSTU框架：1）时间维度上，设计了基于事件触发的粗到细关键帧采样算法，去除冗余帧；2）空间维度上，基于事件显著性，引入自适应token剪枝算法，实现视觉token的高效筛减；3）整体上，通过任务相关性动态分配token预算。还构建了第一个涵盖真实场景事件数据的人类标注多模态基准EventBench。

Result: 在多个实验中，EventSTU相比最强基线可实现3.01倍FLOPs减少、3.10倍输入阶段加速，并且性能有所提升。

Conclusion: EventSTU方法极大提升了视频大模型在推理速度与计算效率方面的表现，在保证甚至提升理解准确率的同时，显著减少了计算开销。并且能适用于真实与模拟事件，具有宽广的应用前景。

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [241] [BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models](https://arxiv.org/abs/2511.18921)
*Juncheng Li,Yige Li,Hanxun Huang,Yunhao Chen,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了BackdoorVLM，这是首个系统性评测视觉-语言模型（VLMs）后门攻击的综合基准，包括图像描述、视觉问答等多任务，并揭示了VLMs对文本后门极为敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管后门攻击在单模态机器学习系统中已有较多研究，但在多模态基础模型（尤其是视觉-语言模型）上的影响尚未被充分探索。随着VLMs应用广泛，其潜在安全隐患亟待系统性揭示与评估。

Method: 作者设计了BackdoorVLM基准，将多模态后门威胁细分为5类（如目标拒绝、恶意注入、越狱等），并覆盖文本、图像及双模触发方式，测试于2个开源VLMs和3个多模态数据集。在各任务下系统比较12种代表性后门攻击方法的表现。

Result: 分析发现：VLMs对于文本类触发最为敏感，在多模后门中，文本触发往往主导模型行为。即便投毒比例极低（1%），针对文本的后门在大部分任务中也能获得超过90%的攻击成功率。

Conclusion: 当前VLMs存在显著的安全漏洞，尤其是面对多模态特别是文本驱动的后门威胁。BackdoorVLM为后续后门攻击检测、防御等研究提供了系统的评测平台。

Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

</details>


### [242] [One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control](https://arxiv.org/abs/2511.18922)
*Zhenxing Mi,Yuxin Wang,Dan Xu*

Main category: cs.CV

TL;DR: One4D是一个通用的4D生成与重建框架，可实现同步的RGB帧与点云的联合作为4D输出，通过独特的统一掩码调控机制和创新的模型结构，实现高质量4D数据的生成与重建。


<details>
  <summary>Details</summary>
Motivation: 当前4D内容的生成与重建在任务灵活性、数据稀疏性适应以及高质量输出等方面存在较大挑战，尤其是联合RGB与点云生成时，现有方法常常遭遇模型退化，无法兼顾多模态输出。

Method: 提出统一掩码调控（UMC）机制，使模型能动态应对不同数量的输入帧，支持单图像4D生成、完整视频4D重建以及稀疏帧混合生成-重建。另设计分离的LoRA控制（DLC），通过两个针对不同模态（RGB和点云）的LoRA模块，并以零初始化轻连接实现互相一致性学习，解决联合生成时的退化难题。网络在合成和真实的4D数据集上进行训练。

Result: One4D在单图生成、视频重建以及稀疏帧任务中表现出高质量的RGB与点云输出，展现了强大的通用性和适应性。

Conclusion: One4D推动了基于视频扩散模型的4D世界建模，能够在有限算力下实现高质量的4D内容生成与重建，是向通用高质量4D建模的重要一步。

Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D

</details>


### [243] [AttenDence: Maximizing Attention Confidence for Test Time Adaptation](https://arxiv.org/abs/2511.18925)
*Yash Mali*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时自适应（TTA）方法，通过最小化transformer中CLS token对图像patch的注意力分布熵，提高模型在测试时分布变化下的鲁棒性，且只需单张测试图像即可生效。


<details>
  <summary>Details</summary>
Motivation: TTA在模型推理时能适应分布变化，现有方法多利用输出分布的熵最小化，但transformer还可从注意力机制获取额外无监督信号，利用这一点可进一步提升适应能力。

Method: 在TTA过程中，提出不仅最小化输出分布的熵，还最小化CLS token对图像patch注意力分布的熵，促使模型在分布变化下更加专注于相关图像区域；新方法无需额外标注，也适用于单样本测试。

Result: 实验表明，在各种类型的图像扰动下，注意力熵最小化能提升模型鲁棒性，且在干净数据上的性能没有下降，该方法也适合测试时只提供单张样本的场景。

Conclusion: 将注意力分布熵最小化作为TTA目标，能进一步提升transformer模型面对分布变化的适应性和鲁棒性，是TTA技术的有效补充。

Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

</details>


### [244] [FineXtrol: Controllable Motion Generation via Fine-Grained Text](https://arxiv.org/abs/2511.18927)
*Keming Shen,Bizhu Wu,Junliang Chen,Xiaoqin Wang,Linlin Shen*

Main category: cs.CV

TL;DR: 提出了一种名为FineXtrol的新方法，用于通过细粒度、可控性强的文本信号高效生成动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的动作生成方法，要么使用大语言模型生成更详细文本（带来细节错配及时间线索不足），要么用全局3D坐标辅助（计算成本高）。作者希望解决细节错配、缺乏时序信息及高计算成本的问题，提高动作生成的精度和易用性。

Method: 提出FineXtrol框架，设计了层次对比学习模块，让文本编码器能更好地理解描述具体身体部位时序运动的细粒度控制信号，并生成更具区分度的特征表示。

Result: FineXtrol在可控动作生成方面表现优异。定量实验验证了效果，定性分析证明其在精细控制具体身体部位动作时具有极强灵活性。

Conclusion: FineXtrol能够以高效、易用且精细的方式，实现对动作生成过程的精确控制，尤其适合需要细粒度身体部位动作指令的场景。

Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.

</details>


### [245] [Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929)
*Zijian Song,Xiaoxin Lin,Tao Pu,Zhenlong Yuan,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了人本开放未来任务发现（HOTD）的问题与评测基准，通过新方法提升机器人与多模态模型在发现有助于人类的任务能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LMMs在机器人和具身AI领域进展显著，但在不确定未来场景下，如何发现真正能帮助人类减轻负担的任务依然是挑战。文中希望解决LMMs如何发现与人类动态意图密切相关的任务。

Method: 作者提出了HOTD-Bench评测基准，包括2千余个真实世界视频、半自动标注流程和面向开放未来场景的仿真评测协议。同时，提出了CMAST（协作多智能体搜索树）框架，利用多智能体系统和可扩展的搜索树来分解与结构化复杂推理。

Result: CMAST在HOTD-Bench平台上取得了最佳表现，显著优于现有LMMs，并能与这些模型集成，持续提升整体性能。

Conclusion: CMAST为机器人和具身AI在人为中心、开放未来场景中任务发现能力提供了新的方法基础，并通过实验验证了其显著优越性与泛化性。

Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.

</details>


### [246] [VeCoR - Velocity Contrastive Regularization for Flow Matching](https://arxiv.org/abs/2511.18942)
*Zong-Wei Hong,Jing-lun Li,Lin-Ze Li,Shen Zhang,Yao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种增强Flow Matching（FM）的方法——Velocity Contrastive Regularization（VeCoR），通过对速度场进行对比式正负监督，有效改善了FM在生成模型中的稳定性与泛化能力，取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的FM方法在生成过程中容易累计误差，使样本偏离数据流形，尤其在轻量级或低步数设置下会导致生成结果感知质量下降。

Method: 作者将FM扩展为一种平衡的吸引-排斥机制，提出了VeCoR训练方案，在FM原有目标的基础上，增加了对速度的双向（正负）对比监督。具体而言，既要吸引预测速度沿着参考方向（正监督），也要排斥其偏离或不一致的方向（负监督）。

Result: 在ImageNet-1K 256x256数据集上，VeCoR使SiT-XL/2和REPA-SiT-XL/2骨干的FID分别相对降低22%和35%；在MS-COCO文本到图像生成任务上也达到了32%的相对FID提升，显示出在多数据集和主干上的稳定性能改进。

Conclusion: VeCoR通过对比式正负训练信号提升了FM生成模型的稳定性、收敛性和生成图像质量，特别适用于低步数和轻量配置。

Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both "where to go" and "where not to go." To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.
  On ImageNet-1K 256$\times$256, VeCoR yields 22\% and 35\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/

</details>


### [247] [Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining](https://arxiv.org/abs/2511.18946)
*José Teixeira,Pascal Klöckner,Diana Montezuma,Melis Erdal Cesur,João Fraga,Hugo M. Horlings,Jaime S. Cardoso,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 作者开发了一种新的虚拟染色GAN模型，提升了图像的病理保真度，并质疑了现有评估标准的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统免疫组化染色（IHC）用于检测组织中特异蛋白，但存在成本高、操作繁琐的问题。虚拟染色作为一种图像到图像翻译任务，为替代引入了新方向，但研究中存在方法与评估上的不足。

Method: 本文提出CSSP2P GAN模型，对虚拟染色过程中的对抗损失进行了详细研究，采用盲法病理专家评估验证其在病理保真度上的提升，并与现有主流方法进行了比较。

Result: CSSP2P GAN在病理专家盲评中表现优异，提升了虚拟染色的真实度；同时展示了对抗损失对图像质量的重要作用，并指出传统评估指标SSIM、PSNR的局限性。

Conclusion: CSSP2P GAN模型在提升虚拟染色质量方面优于现有方法，强调了合适损失函数与专业评估对虚拟染色发展及应用的重要性。

Abstract: In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.

</details>


### [248] [Eevee: Towards Close-up High-resolution Video-based Virtual Try-on](https://arxiv.org/abs/2511.18957)
*Jianhao Zeng,Yancheng Bai,Ruidong Chen,Xuanpu Zhang,Lei Sun,Dongyang Jin,Ryan Xu,Nannan Zhang,Dan Song,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了一个高分辨率视频虚拟试穿数据集和新的评测指标，提升服饰纹理和细节还原度，有效改进电子商务时尚视频生成。


<details>
  <summary>Details</summary>
Motivation: 目前视频虚拟试穿主要使用单一服装图像，无法准确表现真实质感细节。此外，现有方法仅生成全身视频，未满足电商对细节特写视频的需求。

Method: 作者构建了高分辨率数据集，包含高保真服装图片、特写和全身视频及文本描述。提出了VGID（Video Garment Inception Distance）指标，用于度量视频中服饰纹理与结构的一致性。采用该数据集评估现有方法及改进后的视频生成模型。

Result: 新数据集和指标提升了现有模型对纹理特征的提取与还原，生成的虚拟试穿视频在真实感与细节保留方面大幅提高。通过基准测试发现现有方法在质感和结构保留上存在不足。

Conclusion: 高分辨率视频试穿数据集与VGID指标有助于推动视频虚拟试穿技术发展，尤其是在细节还原和业务应用上具有重要意义。

Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.

</details>


### [249] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: 本文提出了一种基于人工智能的新型白内障手术并发症检测方法，并基于新建的数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 白内障手术虽然常见，但部分术中并发症严重影响患者术后效果，目前缺乏有效的自动化检测工具，限制了早期预警和外科医生培训反馈。

Method: 作者提出CataractCompDetect框架，集成了手术阶段感知定位、基于SAM 2的目标跟踪、并发症风险评分及视觉-语言推理，用于识别并分类术中并发症。为验证方法，作者构建并标注了首个白内障手术并发症视频数据集CataComp。

Result: 在CataComp数据集上，CataractCompDetect平均F1得分70.63%，各并发症检测F1得分分别为虹膜脱垂81.8%、后囊破裂60.87%、玻璃体脱垂69.23%。

Conclusion: 结合结构化手术先验知识和视觉-语言推理可有效提升术中罕见高危并发症的自动检测能力，工具和数据集将在论文接收后开放。

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [250] [Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs](https://arxiv.org/abs/2511.18976)
*Huaming Ling,Ying Wang,Si Chen,Junfeng Fan*

Main category: cs.CV

TL;DR: 论文针对同态加密（FHE）推理中的卷积神经网络（CNN）部署问题，提出低阶多项式替代激活函数和新加密打包方案，实现高效、高分辨率的安全推理。


<details>
  <summary>Details</summary>
Motivation: FHE可实现对加密数据的神经网络推理，但现有CNN中的非线性激活函数与高分辨率图像处理受限于FHE计算能力和密文容量，导致准确率下降和实际应用受到阻碍。

Method: 提出单阶段微调（SFT）策略，将预训练CNN的激活函数直接转化为FHE友好的低阶多项式函数，并通过微调减少精度损失。同时，提出通用交错打包（GIP）方案和配套同态算子，可支持任意分辨率的特征图高效加密运算。

Result: 在CIFAR-10、ImageNet和MS COCO等数据集上，转换后的FHE友好型CNN在准确率上可与传统的ReLU/SiLU版本媲美。首次实现了低阶多项式激活下的YOLO目标检测在FHE平台上的推理。

Conclusion: 本文的SFT策略和GIP方案使多种CNN结构在FHE下实现端到端、高效且精度损失极小的加密推理，推动了深度学习模型的安全部署和实际应用。

Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

</details>


### [251] [Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models](https://arxiv.org/abs/2511.18978)
*Santiago Moreno,Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ZEUS的零样本视觉-语言分割框架，可以实现无需像素级标签的肿瘤区域高精度分割，显著减轻了组织病理图像标注负担。


<details>
  <summary>Details</summary>
Motivation: 皮肤肿瘤活检的准确标注因形态多变、组织学模式重叠及良恶性区别微妙而极具挑战。现有视觉-语言大模型在病理学中的应用大多只能达到粗粒度的分类，无法实现对超大组织切片的细致分割。

Method: 提出了ZEUS零样本视觉-语言分割流程，将WSI分割为重叠小块，提取每块视觉特征，利用与类别相关的文本提示及冻结的视觉-语言模型编码器，计算视觉和文本特征的余弦相似度，拼合生成最终高分辨率肿瘤分割掩膜。

Result: 在两个自建数据集上（原发性梭形细胞肿瘤和皮肤转移灶）实现了竞争性分割性能，并探讨了提示设计、领域偏移和医疗机构间差异对模型结果的影响。

Conclusion: ZEUS能够极大节省人工标注时间，同时为病理诊断流程提供可扩展、可解释的高分辨率肿瘤分割，显示出良好的实用前景。

Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.

</details>


### [252] [UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983)
*Ching-Yi Lai,Chih-Yu Jian,Pei-Cheng Chuang,Chia-Ming Lee,Chih-Chung Hsu,Chiou-Ting Hsu,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 本文提出了一种适用于跨压缩率的深度伪造检测新方法，能在不同视频压缩水平下保持高检测精度，并且特征一致性强，具有很好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在深度伪造检测任务中，社交媒体平台上多变的视频压缩程度导致模型性能下降。现有单模态方法在数据压缩下特征丢失严重，难以泛化。多模态方法虽有提升但数据采集和标注成本高，且在真实场景中常因模态质量不一致带来问题。因此急需一种高效、泛化性强且抗压缩的检测方法。

Method: 论文提出'单模生成多模对比学习(UMCL)'框架，在训练阶段将单一视觉模态转换为三类互补特征（抗压缩rPPG信号、时序地标动态、视觉-语言预训练模型语义嵌入）。通过亲和语义对齐（ASA）策略，将多模态特征关联建模并用对比学习优化一致性。同时引入跨质量相似性学习（CQSL）以提升特征的多压缩率鲁棒性。

Result: 实验证明该方法在多种压缩率与伪造类型下均显著优于现有方法，成为鲁棒深度伪造检测领域新基线。即使在部分特征退化的情况下，也能保持高检测准确率并具备良好解释性。

Conclusion: 提出的方法不仅能有效对抗社交媒体上的各种压缩变换，还能通过显式特征对齐获得可解释性，为深度伪造检测提供了高效且具有实际应用价值的新途径。

Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.

</details>


### [253] [Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning](https://arxiv.org/abs/2511.18989)
*Wassim Benabbas,Mohammed Brahimi,Samir Akhrouf,Bilal Fortas*

Main category: cs.CV

TL;DR: 本文探讨了深度学习模型在叶片图像植物病害分类任务中，从学术数据集到真实田间应用的泛化能力问题。通过比较CNN、Vision Transformer和CLIP这三类模型，发现基于zero-shot的CLIP模型在无需特定任务训练的情况下具有更强的适应性和可解释性，为实际农业诊断提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 传统研究多依赖PlantVillage等理想化数据集，模型虽在此类数据集上表现良好，但在真实田间图片上的泛化能力不足，导致科研成果难以落地实际应用。研究动机是缩小学术图像数据与实际生产数据之间的性能差距，提升病害诊断的实用性。

Method: 本文对比评估了三类模型：传统卷积神经网络（CNN）、Vision Transformer和基于CLIP的zero-shot模型，分别考查它们在标准数据集与真实农民提交田间图片之间域转移条件下的表现。CLIP模型仅以自然语言描述，无需专门训练即可实现分类。

Result: CNN在域转移下鲁棒性有限，Vision Transformer因能捕捉全局上下文特征表现更佳。CLIP模型则展现出极强泛化能力和适应性，可通过自然语言描述准确分类不同环境下的植物病害。

Conclusion: zero-shot学习，尤其是CLIP类模型，为跨域适应、多场景下的农作物病害诊断提供了现实、可扩展的解决方案，有望显著推进相关技术在实际农业中的应用。

Abstract: Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.

</details>


### [254] [View-Consistent Diffusion Representations for 3D-Consistent Video Generation](https://arxiv.org/abs/2511.18991)
*Duolikun Danier,Ge Gao,Steven McDonagh,Changjian Li,Hakan Bilen,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: 本文提出ViCoDR方法，通过提升视频扩散模型的多视图一致性，从而显著改善视频生成中的三维一致性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成模型进步明显，但生成视频仍易出现三维不一致带来的视觉瑕疵（如随相机姿态变化物体形变），影响用户体验和应用效果。为此，作者希望提升扩散模型表示的多视图一致性，以实现更真实的三维一致视频生成。

Method: 作者分析了多个主流相机可控视频扩散模型，发现三维一致的扩散表示与视频质量高度相关。在此基础上，提出ViCoDR方法，通过学习多视图一致的扩散表示增强视频生成模型的三维一致性。

Result: ViCoDR被应用于图像/文本到视频及多视角视频生成模型，在提升三维一致性方面取得显著效果。

Conclusion: 通过提升扩散模型的多视图一致性，ViCoDR有效缓解了三维不一致问题，提升了生成视频的真实感和应用价值。

Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.

</details>


### [255] [AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization](https://arxiv.org/abs/2511.18993)
*Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于音频-视觉语音表示重构（AuViRe）的方法，用于对深度伪造视频的篡改时刻进行精准定位，并在多项数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着音视频合成技术的进步，恶意篡改内容愈发隐蔽，数字媒体的真实性和完整性面临严峻考验，因此需要高效准确地检测并定位深度伪造内容。

Method: 作者提出AuViRe方法，通过从一种模态（如唇动画面）重构另一种模态（如音频波形）的语音表示，利用跨模态重建的困难性在被篡改片段会表现出更大差异，从而实现精确时域定位深度伪造。

Result: 在LAV-DF数据集上AP@0.95提升8.9，AV-Deepfake1M数据集上AP@0.5提升9.6，真实场景实验中AUC提升5.1，均超过当前最优方法。

Conclusion: AuViRe方法利用跨模态表征重建提升了深度伪造时域定位的精度和鲁棒性，为音视频篡改检测提供了更高效的解决方案。

Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.

</details>


### [256] [A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation](https://arxiv.org/abs/2511.19004)
*Wentao Qu,Guofeng Mei,Yang Wu,Yongshun Gong,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种用于场景生成的文本到LiDAR扩散模型T2LDM，并利用自条件表示引导提升生成质量与几何细节。还建立了新的基准与测评指标，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在文本到LiDAR数据生成领域，受限于高质量文本-LiDAR配对数据的稀缺，现有方法容易生成结构过于平滑的3D场景，而且文本描述质量的不足会进一步影响生成效果的可控性与细致度。为了提升生成质量与结构丰富性，亟需改进训练和控制策略。

Method: 作者提出了带有自条件表示引导（SCRG）的文本到LiDAR扩散模型T2LDM。SCRG通过和真实表示对齐，在训练期为去噪网络提供细节的软监督，但推理时与主流程解耦。提出了可内容组合的新基准T2nuScenes和可控性度量指标。同时引入了方向位置信息以降低街景畸变，并通过学习条件编码支持稀疏-稠密、稠密-稀疏及语义-激光等多条件生成任务。

Result: 无条件和多条件生成实验表明，T2LDM在结构细腻度、可控性及场景真实性上均大幅超越现有文本到LiDAR生成方法，达到了最新水平。

Conclusion: T2LDM有效解决了文本-激光生成中的结构细节和可控性难题，建立的新基准和测评体系推动了领域进步，具有广泛应用前景。

Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

</details>


### [257] [Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting](https://arxiv.org/abs/2511.19021)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉Transformer方法Grc-ViT，能根据图像复杂度动态调整处理粒度，提高细粒度特征的表达能力，兼顾精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有ViT虽善于捕捉全局信息，但细粒度本地信息表达不足，已有多尺度方法依赖固定块大小并带来冗余计算，因此需要更灵活高效的粒度调整机制。

Method: Grc-ViT引入了动态由粗到细的两阶段框架：第一阶段是粗粒度评估模块，利用边缘密度、熵、频域等特征估算合适的patch与window大小；第二阶段是细粒度精炼模块，根据选择的粒度优化Attention计算。此外，通过α和η两个可学习参数，端到端适应全局与局部信息平衡。

Result: 实验证明Grc-ViT能更好地区分细粒度特征，在维持甚至提升精度的同时，有效降低了计算开销，在多项评测中优于现有方法。

Conclusion: Grc-ViT在提升细粒度判别力和计算效率方面取得优越效果，实现了精度与效率的良好权衡。

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.

</details>


### [258] [Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling](https://arxiv.org/abs/2511.19024)
*Long Tang,Guoquan Zhen,Jie Hao,Jianbo Zhang,Huiyu Duan,Liang Yuan,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种新颖的无参考图像质量评价（BIQA）方法Life-IQA，通过GCN增强的层间交互和基于MoE的特征解耦，有效提升了评估精度，并在多个基准上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA方法多只融合了骨干网络中的浅层和深层特征，却忽视了它们对质量预测贡献的不均衡性。同时，虽然采用了多种视觉编码器骨干结构，缺乏有效的质量特征解码架构研究。

Method: 提出GCN增强的层间交互模块，使用最深层GCN特征作为Query，次深层特征作为Key和Value，通过Cross-Attention实现特征互动。同时，设计了基于MoE（专家混合）的特征解耦模块，将融合特征交由不同专家处理，分别适用于不同失真或质量维度。

Result: 在多个无参考图像质量评价基准数据集上，Life-IQA在评估精度和计算成本之间取得了更优平衡，且性能达到最新最优水平。

Conclusion: Life-IQA通过创新的特征交互和解耦机制，有效提升了BIQA的准确性和效率，为相关领域提供了性能更佳的方法。

Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \underline{l}ayer\underline{i}nteraction and MoE-based \underline{f}eature d\underline{e}coupling, termed \textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\texttt{Life-IQA}}.

</details>


### [259] [Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric](https://arxiv.org/abs/2511.19032)
*Xiangjie Sui,Songyang Li,Hanwei Zhu,Baoliang Chen,Yuming Fang,Xin Sun*

Main category: cs.CV

TL;DR: 本论文提出了Bench-C数据集和RAS指标，用于更全面地评估大规模视觉-语言模型（LVLMs）在图像干扰下的鲁棒性。作者指出现有评测存在样本区分度低和评测维度单一等问题。通过新方法和详细实验，揭示了模型在干扰下的多种表现和鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在受视觉干扰时表现出鲁棒性差，但相关研究和评测工具不完善：一是很多数据集以辨别度较低的样本为主，掩盖了实际鲁棒性差距，二是主流评测仅靠准确率，无法反映预测结构衰退。本研究旨在设计更科学的基准和指标，客观反映模型在多种干扰下的真实鲁棒性。

Method: 作者提出Bench-C基准，特意强调高辨别度样本，通过同时考虑模型受干扰下预测的不一致性和语义多样性来筛选样本。同时，作者提出Robustness Alignment Score（RAS）指标，结合预测不确定性变化与校准对齐度，统一度量模型在logit层面结构性衰退。

Result: 实验发现：（1）模型在不同干扰下表现出有区别的信心问题和犹豫行为；（2）即使干扰很小，准确率略有提升，预测结构却整体退化；（3）通过将鲁棒性分解为破坏性和纠正性两个部分，可刻画不同模型失效及恢复方式的差异。

Conclusion: Bench-C与RAS为LVLMs鲁棒性评测提供了更科学的工具，可充分揭示不同模型在多种视觉干扰下的异同，为后续鲁棒性提升研究提供了新方向。

Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.

</details>


### [260] [ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay](https://arxiv.org/abs/2511.19033)
*Gengyuan Zhang,Mingcong Ding,Jingpei Wu,Ruotong Liao,Volker Tresp*

Main category: cs.CV

TL;DR: 提出了一种名为ReEXplore的新方法，通过回顾性经验重放和分层前沿选择，提升了多模态大模型（MLLMs）驱动的智能体在新环境下的自主探索效率和成功率，无需额外训练。性能显著优于现有强基线。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM驱动的智能体在探索新环境时存在：依赖预训练旧知识、训练代价高、动作空间复杂导致决策不可靠等问题。本工作旨在解决这些探索过程的核心挑战，提升泛化及决策能力。

Method: 提出了无需训练的新框架ReEXplore。方法包括：1）回顾性经验重放，在推理过程中引入抽象经验，丰富决策基础；2）分层前沿选择，将探索空间的排名决策分解为由粗到细的过程，使动作决策更可控、更高效。

Result: 在多个标准测试环境下，ReEXplore在开源MLLM基础上，成功率和导航效率均达到基线的3倍，提高了探索的鲁棒性、可追踪性与效率。

Conclusion: ReEXplore显著提升了MLLM驱动智能体的探索能力，尤其是在无需重新训练情况下，体现了方法的高效性和普适性。为智能体自主探索范式提供可靠新路径。

Abstract: Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.

</details>


### [261] [CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones](https://arxiv.org/abs/2511.19035)
*Kai Zhenga,Zhenkai Wu,Fupeng Wei,Miaolan Zhou,Kai Lie,Haitao Guo,Lei Ding,Wei Zhang,Hang-Cheng Dong*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv3预训练模型的多尺度跨注意力Siamese网络(MC-DiSNet)，用于冲突区域遥感图像中的快速精确损毁检测，解决了数据有限、标注困难和细粒度识别等难题，并发布了Gaza-change像素级数据集。


<details>
  <summary>Details</summary>
Motivation: 冲突区域建筑风格相似、破坏面积小且边界模糊，导致遥感损毁检测数据稀缺、标注难度大和识别挑战（如高类内相似与语义变化不明）。因此，亟需高效且鲁棒的方法提升人道救援与地区安全的自动化损毁评估能力。

Method: 采用视觉表征能力强的DINOv3作为主干网络，设计多尺度跨注意力差分Siamese网络（MC-DiSNet）处理双时相遥感图像特征。此外，提出change semantic detection (CSD)任务，仅关注像素级变化区域，无需以往大规模语义标注。并发布了高分辨率Gaza-change数据集（2023-2024年）。

Result: 在Gaza-change和SECOND数据集上的实验结果表明，该方法在CSD任务下表现优异，能有效应对传统语义变化检测的不足，实现对冲突区损毁的快速精准感知。

Conclusion: 所提方法和CSD新任务为遥感损毁检测提供新范式，极大简化了数据需求，提升识别精度和速度，为冲突地区的快速损毁评估和人道干预提供有力工具。

Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.

</details>


### [262] [MedSAM3: Delving into Segment Anything with Medical Concepts](https://arxiv.org/abs/2511.19046)
*Anglin Liu,Rundong Xue,Xu R. Cao,Yifan Shen,Yi Lu,Xiang Li,Qianqian Chen,Jintai Chen*

Main category: cs.CV

TL;DR: 本文提出了MedSAM-3，这是一种基于文本提示的医学图像和视频分割模型，通过结合多模态大语言模型实现了更高的泛化能力和分割精度，在多种医学成像模式下表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割方法普遍存在泛化能力差、需要大量手工标注的缺点，限制了其在新临床场景下的应用。研究动机是开发一种通用性更强、依赖更少人工标注的医学图像分割模型。

Method: 该方法在Segment Anything Model (SAM) 3的基础上，针对带有语义标签的医学图像进行微调，支持基于自然语言的开放词汇文本提示进行概念分割，并引入了融合多模态大语言模型的MedSAM-3 Agent，实现更精细的推理和迭代分割优化。

Result: 在X光、MRI、超声、CT和医学视频等多种类型的医学图像数据上，MedSAM-3显著优于现有的专家模型和通用大模型，在分割准确性和泛化能力方面均有提升。

Conclusion: MedSAM-3通过引入文本可提示和多模态推理显著提升了医学图像分割的适应性和精度，为不同医疗场景下的自动分割提供了高效可扩展的新方案。

Abstract: Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.

</details>


### [263] [Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation](https://arxiv.org/abs/2511.19049)
*Ruojun Xu,Yu Kai,Xuhua Ren,Jiaxiang Cheng,Bing Ma,Tianxiang Zheng,Qinhlin Lu*

Main category: cs.CV

TL;DR: 本文分析了直接偏好优化（DPO）在扩散模型中的瓶颈，发现训练过程中出现似然度下降问题。基于对DPO损失的深入剖析，提出了新方法PG-DPO，有效提升了视频生成任务中的模型表现。


<details>
  <summary>Details</summary>
Motivation: DPO已在自动回归模型中展现了对齐生成内容与人类偏好的优越性，但该方法在扩散模型中普遍存在生成样本似然度下降的现象。该问题极大影响了视频生成等任务的性能，因此需系统分析并提出解决思路。

Method: 作者从扩散模型角度，系统分析了DPO损失对于训练样本间相互影响的本质机理，找出了两大失效模式（优化冲突和次优最大化）。基于这些发现，提出了PG-DPO新架构，结合自适应拒绝缩放（ARS）和隐式偏好正则化（IPR）来缓解上述问题。

Result: PG-DPO经定量和定性实验验证，在视频生成任务中优于现有DPO及扩展方法，无论指标评价还是实际生成质量均有明显改善。

Conclusion: PG-DPO为扩散模型中偏好对齐提供了有效思路，对未来高质量生成尤具启发意义。其策略可望推广到更广泛的生成模型任务中。

Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

</details>


### [264] [LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space](https://arxiv.org/abs/2511.19057)
*Hai Wu,Shuai Tang,Jiale Wang,Longkun Zou,Mingyue Guo,Rongqin Liang,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 本文介绍了LAA3D数据集，这是一个专为低空飞行器三维感知而设计的大规模数据集，并提出了相关基线模型和评测基准。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对低空飞行器（如eVTOL、无人机、直升机）三维感知的公开数据集，限制了该领域的研究和应用发展。

Method: 作者建立了LAA3D数据集，包括1.5万幅真实图像和60万帧合成数据，涵盖城市及郊区多种场景，包含多类飞行器的3D框、类别、身份等标注；建立了完整的评测基准，支持3D检测、MOT、6-DoF位姿估计等任务；此外，提出了单目3D检测基线方法MonoLAA，并验证了合成数据的迁移能力。

Result: MonoLAA能够在不同焦距的变焦摄像头下实现稳健的3D定位；合成数据预训练模型通过微调可有效迁移到真实数据上，表现出良好的仿真-现实泛化能力。

Conclusion: LAA3D为低空三维目标感知研究提供了坚实基础，填补了领域数据空白，有望推进低空飞行器感知任务的学术与应用研究。

Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.

</details>


### [265] [Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation](https://arxiv.org/abs/2511.19062)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min,Yi Zhang*

Main category: cs.CV

TL;DR: 本文针对现有无提示图像分割模型（如SAM）在区域定位和高分辨率细粒度建模方面的局限，引入了基于粗细粒度计算的新框架Grc-SAM，显著提升了分割精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有无提示图像分割方法，如SAM，依赖单一粒度生成提示，导致对目标区域定位不准和高分辨率下建模能力不足，限制了分割精度和泛化能力。

Method: 提出了Grc-SAM，一个借鉴粗细粒度计算的两阶段框架。首先，粗粒度阶段自适应地从特征中提取高响应区域，实现前景准确定位，减少对外部提示的依赖。随后，细粒度阶段通过更精细的区域划分及稀疏的局部注意力机制，提升细节建模和高分辨率分割能力。最后，将细化后的分割掩码编码为潜在提示嵌入，自动引导SAM解码器，无需手工提示。

Result: Grc-SAM通过整合多层次粒度注意力机制，充分结合粗细粒度计算与视觉Transformer。在多个实验中，Grc-SAM在分割精度和可扩展性方面均优于现有主流方法。

Conclusion: Grc-SAM以全新粒度计算视角，有效缓解了现有无提示分割方法的局限，推动了自动化图像分割技术的发展。

Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.

</details>


### [266] [Understanding, Accelerating, and Improving MeanFlow Training](https://arxiv.org/abs/2511.19065)
*Jin-Young Kim,Hyojun Go,Lea Bogensperger,Julius Erbach,Nikolai Kalischek,Federico Tombari,Konrad Schindler,Dominik Narnhofer*

Main category: cs.CV

TL;DR: 本文提出了一种改进的MeanFlow训练方案，通过更好地协调瞬时速度和平均速度的学习，显著提升了图像生成模型的性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: MeanFlow 作为一种新颖的生成模型，因其能够在极少步数条件下生成高质量样本而备受关注。但其训练机制及瞬时速度与平均速度间的作用机制尚不明确，亟需深入剖析，以提升模型潜力。

Method: 作者系统分析了瞬时速度与平均速度的相互影响，并据此设计了一套新训练流程：先加速瞬时速度的学习，再逐步从短间隔的平均速度过渡到长间隔平均速度的学习。

Result: 新方案在ImageNet 256x256的1-NFE条件下，使用相同DiT-XL骨干网络，FID从3.43（基线）降至2.87；或用更小的DiT-L骨干亦可达到同等效果。此外，训练时间可缩短2.5倍。

Conclusion: 通过理顺训练内部动态并合理分配学习阶段，作者的增强版MeanFlow显著提升了生成质量和效率，为Few-step生成设定了新基准。

Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.

</details>


### [267] [DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling](https://arxiv.org/abs/2511.19067)
*Timur Mamedov,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: 本文提出了一种名为DynaMix的新方法，通过动态混合多摄像头人工标注数据和大规模单摄像头伪标注数据，大幅提升了跨摄像头、跨场景的人体重识别（Re-ID）泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前人体重识别方法对有限的多摄像头标注数据依赖较大，难以泛化到新环境和新摄像头，因此需要充分利用单摄像头伪标注等规模更大的数据。

Method: DynaMix包含三个核心模块：1）重标注模块，动态优化单摄像头伪标注的数据标签；2）高效中心模块，用于在超大身份空间下维持稳定的身份表示；3）数据采样模块，智能组合混合数据mini-batch，兼顾学习难度和平衡多样性，所有模块都针对大规模场景高效设计。

Result: 在大规模数据集上实验，DynaMix在泛化性和准确率上均显著超过了现有最优方法。

Conclusion: DynaMix有效解决了泛化人体重识别的数据依赖和规模化训练难题，对实际应用中大规模跨摄像头ReID系统有重要借鉴意义。

Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

</details>


### [268] [DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation](https://arxiv.org/abs/2511.19071)
*Fangda Chen,Jintao Tang,Pancheng Wang,Ting Wang,Shasha Li,Ting Deng*

Main category: cs.CV

TL;DR: 本论文提出了一种全新的DEAP-3DSAM方法，有效提升了3D医学图像分割的性能，通过增强解码器和自动提示机制克服了现有SAM模型的局限性，达到了最新水平。


<details>
  <summary>Details</summary>
Motivation: 当前SAM模型虽在医学图像分割中展现潜力，但用于3D场景时常因伪3D处理带来空间特征丢失，且过于依赖人工提示，实际应用不便。因此，需要提高模型在3D分割上的空间感知能力和自动化能力。

Method: 提出了DEAP-3DSAM方法，包含增强的特征解码器（融合图像原始与丰富空间信息），以及双重注意力自动提示器（通过空间和通道注意力自动生成提示信息），减小对人工提示的依赖。

Result: 在四个公开腹部肿瘤分割数据集上进行了大量实验，DEAP-3DSAM在3D图像分割中取得了领先或相当于现有手动提示方法的性能。消融实验（定量+定性）也验证了所提模块的有效性。

Conclusion: DEAP-3DSAM方法有效克服了SAM模型在3D医学图像分割中的局限，实现了无需人工提示的高精度分割，对实际医学应用具有重要意义。

Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.

</details>


### [269] [Graph-based 3D Human Pose Estimation using WiFi Signals](https://arxiv.org/abs/2511.19105)
*Jichao Chen,YangYang Qu,Ruibo Tang,Dirk Slock*

Main category: cs.CV

TL;DR: 本文提出了一种基于图网络的WiFi人体姿态估计算法，相较于以往直接回归关节点坐标的方法，显式建模了人体骨骼拓扑结构，极大提升了估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi姿态估计主要面临对关节拓扑关系建模缺失的问题，这限制了估计算法的准确性和泛化能力。因此，亟需一种方法能有效利用人体自身的骨骼结构信息。

Method: 提出GraphPose-Fi框架，包括：1）用于不同天线子载波-时间特征提取的卷积编码器，2）轻量级注意力模块对天线和时间维度的特征自适应加权，3）结合GCN和自注意力的关节回归头以建模关节局部拓扑与全局依赖关系。

Result: 在MM-Fi数据集上，在多种实验设置下均显著优于现有WiFi姿态估计方法。

Conclusion: 通过引入骨骼拓扑信息，GraphPose-Fi大幅提升了WiFi人体三维姿态估计的准确度和鲁棒性，为后续相关研究提供了新方向。

Abstract: WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.

</details>


### [270] [HABIT: Human Action Benchmark for Interactive Traffic in CARLA](https://arxiv.org/abs/2511.19109)
*Mohan Ramesh,Mark Azer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 本文提出了HABIT（人类行为交互交通基准），针对自动驾驶仿真中对真实、多样的人类行为建模不足的问题，构建了高拟真、高多样性的仿真测试集。通过引入真实人类动作数据，能更好评估自动驾驶系统在复杂行人场景下的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶仿真系统对人类行为的还原十分有限，尤其在行人动态行为和复杂交互上表现简化，这导致无法准确暴露自动驾驶系统的安全隐患与弱点。因此需要一个能真实反映人类行为多样性和交互复杂性的仿真基准。

Method: 作者提出了HABIT基准，通过将真实世界中的人体动作捕捉和视频数据，经标准化及动力学一致的动作重定向流程，整合到CARLA仿真平台。最终筛选得到4,730条适用于交通环境的标准化行人动作序列。该基准兼容CARLA Leaderboard，支持自动生成复杂场景和精细评测。

Result: 利用HABIT基准，对三种主流自动驾驶系统（InterFuser、TransFuser、BEVDriver）进行了严格评测。尽管这些系统在CARLA原生测试集上几乎零碰撞，但在HABIT基准上每公里碰撞数最高达7.43，AIS 3+伤害风险高达12.94%，过度制动比例高达33%。同时，HABIT揭示了之前测试集中未暴露的系统弱点。

Conclusion: HABIT显著提升了自动驾驶测试中对真实人类行为的建模和测评能力。通过更逼真的行人场景，HABIT能有效发现自动驾驶系统在现实复杂交通中的潜在风险和失效模式，为行人安全感知的AI研究提供了公开、复现的平台。

Abstract: Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.

</details>


### [271] [DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection](https://arxiv.org/abs/2511.19111)
*Hai Ci,Ziheng Peng,Pei Yang,Yingxin Xuan,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了DiffSeg30k数据集，专注于对扩散模型修改的图像进行像素级编辑检测，并验证了分割模型在定位AI生成内容方面的有效性和挑战性。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的AI内容生成技术日益成熟，局部地区的真实感编辑让检测AI生成内容变得更加困难。现有AIGC检测方法大多只关注整个图片的真假分类，忽略了定位具体被编辑区域的重要性。

Method: 作者构建了DiffSeg30k数据集，包括3万张扩散模型本地编辑的图像，并配有像素级标注。该数据集特点包括：多样的真实图片、八个最先进的扩散模型参与编辑、多轮次（最多三轮）编辑模拟真实流程，以及依赖视觉语言模型自动生成有意义的区域与语境化编辑提示。同时，作者基于该数据集基准测试了三种分割方法。

Result: 基准实验显示，图像语义分割任务本身在应对图像失真时有较大挑战，但训练好的分割器在对整图判别AI编辑（而不仅仅是像素级检测）时，效果优于现有伪造检测器，并展现了良好的生成器泛化能力。

Conclusion: DiffSeg30k推动了AIGC检测从二元分类向细粒度语义分割转变，为AI生成内容的定位检测研究提供了新方向，也揭示了基于分割方法的潜力与局限性。

Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k

</details>


### [272] [3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion](https://arxiv.org/abs/2511.19117)
*Minchong Chen,Xiaoyun Yuan,Junzhe Wan,Jianing Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种无需校准的热成像超分辨率方法3M-TI，通过创新的扩散模型和跨模态注意力机制，有效提升了移动端热成像分辨率和细节。


<details>
  <summary>Details</summary>
Motivation: 现有移动平台的热成像传感器受限于体积和硬件限制，导致其图像模糊且细节缺失，单一热图提升方法能力有限，而依赖RGB引导的方法又需要复杂的跨相机校准，实际应用受限，因此有迫切需求开发无需繁琐校准的高性能热成像增强方法。

Method: 3M-TI引入了跨模态自注意力模块（CSM）进扩散式UNet网络，用于在去噪过程中自适应对齐和融合热成像与RGB信息，无需依赖精确的相机校准，充分利用扩散模型生成先验，实现高效的超分辨率增强。

Result: 实验结果表明，3M-TI在真实移动热成像设备及公开基准数据集上，在视觉质量和量化指标两方面均优于现有方法，达到了最先进性能。此外，该方法显著提升了目标检测和分割等下游任务的效果。

Conclusion: 3M-TI为移动热成像超分辨率提供了一种实用且高效的新方案，克服了传统RGB引导方法对繁琐校准的依赖，具备较强的实用价值和应用前景。

Abstract: The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.

</details>


### [273] [MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images](https://arxiv.org/abs/2511.19119)
*Qirui Wang,Jingyi He,Yining Pan,Si Yong Yeo,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 本文提出MonoSR大规模单目空间推理数据集，覆盖室内外与多种任务，并评估现有视觉语言模型在这一挑战中的表现，推动实际环境下单目空间推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理研究多集中于室内环境且依赖多视角输入，难以适用于实际中以单目相机为主、场景多样的真实世界应用。因此，亟需支持多场景、多任务的单目空间推理基准。

Method: 作者构建了MonoSR单目空间推理数据集，涵盖室内、室外、以物体为中心等多样场景，并支持多种问题类型。同时，利用当前先进的视觉-语言模型在数据集上进行系统性能评估和分析。

Result: 实验显示，现有视觉-语言模型在MonoSR数据集上的表现存在显著不足，尤其在面对复杂、开放世界的单目空间推理任务时表现有限。分析同时揭示辅助信息对提升单目空间推理能力的重要性。

Conclusion: MonoSR数据集为未来广泛、真实世界下的单目空间推理研究奠定了基础。作者建议未来模型需充分利用辅助信息，并为模型设计提供了实际建议。

Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

</details>


### [274] [When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP](https://arxiv.org/abs/2511.19126)
*Beilin Chu,Weike You,Mengtao Li,Tingting Zheng,Kehan Zhao,Xuan Xu,Zhigao Lu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CV

TL;DR: 本文针对AI生成图像检测中的一般化问题，提出了一种新的基于CLIP的微调范式，通过抑制语义偏置显著提升跨域检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的检测器虽具备一定泛化能力，但其依赖于语义信息而忽略生成器伪影，导致在分布变化下性能不稳定。研究者需改善CLIP检测器对AI生成图像的鲁棒性。

Method: 作者引入Patch Shuffle技术，打乱图像整体语义连贯性，但保留生成伪影线索，从而减少语义熵和同质化自然与合成图像的特征分布。此外，通过层级分析，研究CLIP结构，在此基础上提出SemAnti框架：冻结语义子空间，只对对伪影敏感的层在乱序语义下进行适配微调。

Result: SemAnti方法在AIGCDetectBenchmark和GenImage等跨域检测任务上取得了新的效果SOTA，验证了语义调控对提升CLIP在AI图像检测领域的关键作用。

Conclusion: 通过限制CLIP对全局语义的依赖，并关注生成伪影信息，可以显著改善AI生成图像检测的泛化能力，SemAnti提供了一种简单有效的微调新路径。

Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.

</details>


### [275] [MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery](https://arxiv.org/abs/2511.19134)
*Shuyu Cao,Minxin Chen,Yucheng Song,Zhaozhong Chen,Xinyou Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对无人机图像中小目标检测的新方法MambaRefine-YOLO，并在两个数据集上实现了精度和效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的小目标检测因图像低分辨率和背景杂乱而面临很大挑战。虽然RGB与红外融合有望提升性能，但现有方法难以兼顾模态交互效果和计算效率。因此，作者希望提出一种有效且高效的多模态融合检测方法。

Method: 1. 提出Dual-Gated Complementary Mamba融合模块（DGC-MFM），通过基于光照感知和差异感知的门控机制，自适应平衡RGB与IR模态特征。
2. 设计分层特征聚合颈部模块（HFAN），采用“先精炼，后融合”的策略强化多尺度特征表达。
3. 在双模态（DroneVehicle）和单模态（VisDrone）数据集上进行广泛实验。

Result: 在DroneVehicle双模态数据集上，完整模型mAP达到83.2%，比基线提升7.9%；在VisDrone单模态数据集上，仅使用HFAN的变体模型同样获得显著提升。

Conclusion: 所提方法在检测精度和推理速度间取得较优平衡，适用于实际无人机小目标检测场景，对多模态与单模态任务均有普适性和显著效果。

Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

</details>


### [276] [FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation](https://arxiv.org/abs/2511.19137)
*Zhifeng Xie,Keyi Zhang,Yiye Yan,Yuling Guo,Fan Yang,Jiting Zhou,Mengtian Li*

Main category: cs.CV

TL;DR: 本文提出了一套自动化电影场景生成系统FilmSceneDesigner，利用自然语言描述自动生成符合电影美学的三维场景，并引入新数据集以提升真实感和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统电影布景建模依赖人工操作，流程繁琐且耗时。为提高效率并降低专业门槛，亟需自动化、智能化的场景生成方法。

Method: 作者设计了一个agent链式框架，根据自然语言描述（包括场景类型、历史时期和风格）生成结构化参数，遵循专业电影场景设计逻辑，辅以prompt策略确保参数准确和一致。利用过程生成流水线，根据这些参数自动完成地面结构、材质、门窗、物品布局等，完整构建场景。此外，构建了包含6,862个3D资产和733种材质的SetDepot-Pro数据集，丰富资产多样性和真实性。

Result: 实验和人工评估表明，该系统能生成结构合理、具有强烈电影感的三维场景，能够支持虚拟预演、施工图和情绪板等下游任务。

Conclusion: FilmSceneDesigner实现了电影场景设计流程的自动化，为电影美术场景生成提供高效、真实、灵活的新方案，可在多种影视制作环节落地应用。

Abstract: Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.

</details>


### [277] [ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation](https://arxiv.org/abs/2511.19145)
*Dongha Lee,Jinhee Park,Minjun Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: 本文提出ABM-LoRA，一种优化低秩适配器(LoRA)初始化的方法，通过与预训练模型的激活边界对齐，加快模型收敛速度，并在多项任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法通过随机初始化，将下游任务的梯度限制在与主模型不匹配的切线空间，导致信息损失和收敛缓慢。为提升LoRA适配效率，需要一种能够减少初始化时信息损失的方法。

Method: ABM-LoRA在下游训练前，通过使适配器的激活边界与预训练模型对齐，提升适配器子空间对主模型梯度的投影，从而优化信息保留并改善初始化。

Result: ABM-LoRA在多种架构和任务中有效加快收敛速度，包括自然语言理解（T5-Base+GLUE）、对话生成（LLaMA2-7B+WizardLM）、视觉识别（ViT-B/16+VTAB-1K），在VTAB-1K结构化推理任务上取得最高准确率。

Conclusion: 通过对齐激活边界，ABM-LoRA能有效降低信息损失，提高低秩适配器的收敛速度和性能，优于现有LoRA方法，适用于多领域任务。

Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

</details>


### [278] [Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation](https://arxiv.org/abs/2511.19147)
*Huisoo Lee,Jisu Han,Hyunsouk Cho,Wonjun Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种协同多基础模型自适应（CoMA）框架，通过联合利用两种属性互补的基础模型（如CLIP和BLIP），提升了无源域自适应（SFDA）任务中的适应能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法通常仅利用单一基础模型引导自适应，但单一模型的语义覆盖有限，难以应对领域迁移中复杂多样的上下文信息，因此需要新的方法获取更丰富的外部知识以改进目标域模型。

Method: 提出了CoMA框架，联合两种基础模型（如CLIP和BLIP），通过双向适应机制：一方面保持基础模型间的语义差异性并对齐至目标模型，另一方面将它们的互补知识传递给目标模型。同时提出分解式互信息（DMI）机制，选择性增强真实依赖关系、抑制虚假依赖，提高小批量训练下的适应稳定性。

Result: 在Office-31、Office-Home、DomainNet-126和VisDA等四个基准的封闭集设定下，CoMA在所有实验中均显著优于现有最先进SFDA方法，在部分集和开放集场景中也取得了最佳结果。

Conclusion: 多基础模型协作有助于提升SFDA任务中的模型泛化与鲁棒性，所提方法在多种基准上验证了有效性和优越性，显示了进一步利用外部多源知识进行无监督域自适应的巨大潜力。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.

</details>


### [279] [Test-Time Preference Optimization for Image Restoration](https://arxiv.org/abs/2511.19169)
*Bingchen Li,Xin Li,Jiaqi Xu,Jiaming Guo,Wenbo Li,Renjing Pei,Zhibo Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时偏好优化（TTPO）范式，用于图像复原任务，可以在不重新训练模型和无需人工偏好数据收集的情况下，提升图像感知质量并更好地符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的图像复原模型（包括预训练和零样本方法）虽然能还原高质量图像，但输出结果常常不能很好地满足人类主观偏好。因此，需要一种无需额外数据和训练即可提高复原图像与人类偏好一致性的方法。

Method: 提出了训练自由的三阶段TTPO流程：（1）利用扩散反演和去噪手段，基于初步复原图像生成一组备选偏好图像；（2）用自动化指标或人工反馈，在这些图像间挑选偏好和非偏好图片；（3）将“偏好样本”作为奖励信号，指导扩散去噪过程，优化得到更符合人类偏好的复原图像。该方案对各类IR模型均兼容。

Result: 在多种图像复原任务和模型上进行了大量实验，验证了此流程能够有效提升复原图像的感知质量，并表现出良好的灵活性和普适性。

Conclusion: 所提出的TTPO范式无需模型再训练和大规模人工偏好数据，仅借助测试时即时生成和筛选的偏好图像信号，便能普适显著改善复原图像的人主观满意度，为图像复原任务提供了一种高效易用的新思路。

Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.

</details>


### [280] [MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes](https://arxiv.org/abs/2511.19172)
*Kehua Chen,Tianlu Mao,Zhuxin Ma,Hao Jiang,Zehao Li,Zihan Liu,Shuqi Gao,Honglong Zhao,Feng Dai,Yucheng Zhang,Zhaoqi Wang*

Main category: cs.CV

TL;DR: 本文提出了MetroGS，一种新型的高效鲁棒3D高斯点分布场景重建框架，特别适用于复杂城市环境，可显著提升重建的精度和渲染效果。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯点分布（Gaussian Splatting）虽然在大规模场景重建上取得突破，但在高质量几何还原和效率、稳定性方面仍有核心挑战，特别是在城市等复杂大场景中。

Method: 1. 基于分布式2D高斯点分布表示为统一骨干。
2. 提出结构化密集增强方案，结合SfM先验与点图模型，实现更致密的初始化，并引入稀疏补偿提升完整性。
3. 设计渐进式混合优化策略，将单目和多视图优化有机结合，实现高效准确的几何优化。
4. 提出深度引导的外观建模方法，有效分离几何与外观，实现大规模场景下的一致渲染。

Result: 在大规模城市数据集上的实验显示，MetroGS在几何准确性和渲染质量上均优于现有方法，表现出更好的适应性和统一性。

Conclusion: MetroGS为大规模高保真场景重建提供了一个高效、鲁棒、统一的方案，推动了复杂环境下3D重建技术的发展。

Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.

</details>


### [281] [Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification](https://arxiv.org/abs/2511.19180)
*Mansur Ozaman*

Main category: cs.CV

TL;DR: 该论文分析了三种主流图像来源设备识别方法，并比较了其设备分类准确率与实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 准确识别图像拍摄设备对后续图像分析具有重要意义，有助于取证、安全等多种场景。

Method: 使用三种方法进行设备识别：光响应非一致性（PRNU）、JPEG压缩伪影分析、卷积神经网络（CNN），并比较其分类准确率。

Result: 论文对比了三种方法在设备分类准确率上的表现，并分析了其优缺点。

Conclusion: 三种方法各有优势与局限，并对其在实际场景落地应用所需的科学进展进行了讨论。

Abstract: One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.

</details>


### [282] [nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation](https://arxiv.org/abs/2511.19183)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jaeger,Fabian Isensee,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 本文提出了nnActive，一个面向3D生物医学成像语义分割的开源主动学习（AL）框架，通过大规模实验证明，尽管不同AL方法普遍优于标准随机采样，但在引入前景敏感的随机采样后，效果未必更优，AL效果依赖具体任务参数。


<details>
  <summary>Details</summary>
Motivation: 生物医学图像语义分割常需大量人工标注成本高、专业性强。主动学习（AL）可减少标注需求，但在3D医学图像领域，其是否优于随机采样没有共识，目前评价方法存在四大漏洞，需更全面规范的研究框架。

Method: 提出nnActive框架，覆盖四大改进：（1）基于四个数据集和三种标注场景进行大规模实验；（2）基于nnU-Net扩展，能利用3D局部块和局部标注；（3）提出针对前景-背景不均衡的前景敏感随机采样；（4）设计前景效率指标，衡量只标注背景时的低成本。

Result: a) 所有AL方法均优于传统随机采样，但均无法稳定优于前景敏感随机采样；b) AL的收益强依赖特定任务参数；c) 预测熵法表现最佳但需更多标注；d) 通过更高计算投入可提升AL效果。

Conclusion: nnActive作为一个结构化、开源的AL框架，为3D医学影像主动学习领域的研究及实际应用提供了可靠基线和新工具，能加速相关研究和产业落地。

Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive

</details>


### [283] [SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection](https://arxiv.org/abs/2511.19187)
*Nithira Jayarathne,Naveen Basnayake,Keshawa Jayasundara,Pasindu Dodampegama,Praveen Wijesinghe,Hirushika Pelagewatta,Kavishka Abeywardana,Sandushan Ranaweera,Chamira Edussooriya*

Main category: cs.CV

TL;DR: 本文提出了一种基于EfficientNet-B6的轻量级深度伪造图像检测模型，并通过多种技术提升准确率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造（Deepfake）图像威胁信息安全，亟需通用且易用的检测手段帮助大众识别伪造内容。

Method: 采用EfficientNet-B6作为基础结构，结合数据增强、过采样、优化策略等方式，专门处理类别严重不平衡问题，并尝试引入傅里叶变换相位和幅度特征（但作用不大）。

Result: 模型在准确率、稳定性和泛化能力上均表现优异。傅里叶变换特征贡献有限。

Conclusion: 该框架易于非专业人士使用，有望显著提升深度伪造检测的普及性和可靠性。

Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.

</details>


### [284] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文提出了CLASH基准，用于评估多模态输入之间矛盾检测能力，并分析了当前模型对此能力的不足。经实验证明，通过CLASH进行针对性微调能显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态输入经常存在矛盾，但目前的基准假设输入一致，缺乏评估跨模态矛盾检测（对防止幻觉和提升可靠性至关重要）。为弥补这一空白，作者设计了新的基准。

Method: 构建CLASH基准，用COCO图片与包含物体或属性矛盾的描述性标题配对，设置了多项选择和开放式回答问题，数据经自动质量检测和人工验证，包含微调集和诊断集。利用此基准评测主流多模态模型并进行有针对性的微调实验。

Result: 实验发现现有先进模型对跨模态冲突的识别能力有限，表现出模式偏见和部分种类的弱点。通过CLASH进行微调后，模型在矛盾检测方面获得了显著提升。

Conclusion: CLASH弥补了评测多模态矛盾检测的空白，有助于相关技术进步。针对性训练能显著提升模型的可靠性和冲突识别能力，对实际多模态应用具有重要意义。

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [285] [Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?](https://arxiv.org/abs/2511.19200)
*Itay Cohen,Ethan Fetaya,Amir Rosenfeld*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（如CLIP）是否能够区分'真实物体'和'相似但非真实物体'（lookalike）。作者基于此构建了新的RoLA数据集并提出相关方法。


<details>
  <summary>Details</summary>
Motivation: 虽然计算机视觉模型已在识别任务上取得了显著进展，但它们在模仿人类细致感知方面仍有差距。特别是，模型能否区分'像某物但并非其实例（lookalike）'这一人类能力尚未被充分探索。

Method: 作者建立了RoLA数据集，其中包含多个类别的真实和相似（lookalike）示例，如玩具、雕像、绘画、空想成像等。首先以'真实/相似'为提示词，评估基线模型表现。然后，在CLIP的嵌入空间中估算从'真实'到'相似'的方向，将该方向应用于图像和文本嵌入，提高跨模态检索及文本生成能力。

Result: 在大型数据库Conceptual12M上，应用该方向调整后的CLIP嵌入后，跨模态检索性能得到提升。同时，在CLIP前缀生成的图像描述任务上，描述质量也有提升。

Conclusion: CLIP等视觉-语言模型能够一定程度上捕捉'真实'与'相似'的语义差异，所提出的方法有效增强了模型区分和生成能力。

Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

</details>


### [286] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于MLP的视点相关遮挡剔除方法，有效提升了3D Gaussian Splatting在复杂场景下的渲染效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽然可以通过视锥裁剪和细节层级加速渲染，但由于高斯半透明的特性，无法利用遮挡剔除进一步提升效率，限制其处理大规模复杂场景能力。

Method: 作者设计了一个小型的、共享的MLP（多层感知机），学习所有高斯原语在不同视点下的可见性函数，在光栅化前对视锥内的高斯进行可见性查询，提前剔除被遮挡的原语，并将这一查询过程与实例化的软件光栅器相结合，同时利用Tensor Core提升运算效率。

Result: 该方法在合成场景渲染中，相较现有技术在显存占用和图像质量上均有提升，支持大规模高效实例化渲染，而且能与已有的细节层级（LoD）方法互补协作。

Conclusion: 通过引入基于MLP的遮挡剔除神经查询与定制光栅器，本文方法显著提升了3D Gaussian Splatting技术在复杂场景下的实际应用价值，有效改善了渲染效率与效果。

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


### [287] [ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/abs/2511.19217)
*Wanjiang Weng,Xiaofeng Tan,Junbo Wang,Guo-Sen Xie,Pan Zhou,Hongsong Wang*

Main category: cs.CV

TL;DR: 文本到动作的生成任务面临文本与动作语义对齐不足的问题，作者提出了奖励引导对齐方法（ReAlign），提升了动作生成的对齐性和质量。


<details>
  <summary>Details</summary>
Motivation: 虽然基于扩散模型的方法能生成多样且真实的动作，但存在生成结果与文本描述语义不匹配的问题，导致动作质量下降。为解决这一扩散模型中的对齐分布偏差，提出了新的对齐方法。

Method: 提出Reward-guided sampling Alignment（ReAlign）方法，包括一个步态感知奖励模型，在降噪采样过程中动态评估对齐质量，以及使用奖励引导分布采样，推动采样向最优对齐方向。奖励模型融合步态感知token，结合语义一致性（文本对齐模块）和动作真实性（动作对齐模块），在每一步细化噪声动作，兼顾概率密度与对齐。

Result: 在动作生成和动作检索任务的大量实验中，提出的方法在文本-动作对齐和动作质量上都显著优于当前最先进方法。

Conclusion: Reward-guided采样对齐（ReAlign）方案能有效提升扩散模型下文本到动作生成的对齐性和质量，具有良好的实际应用和推广价值。

Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

</details>


### [288] [Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering](https://arxiv.org/abs/2511.19220)
*Federico Felizzi,Olivia Riccomi,Michele Ferramola,Francesco Andrea Causio,Manuel Del Medico,Vittorio De Vita,Lorenzo De Mori,Alessandra Piscitelli Pietro Eric Risuleo,Bianca Destro Castaniti,Antonio Cristiano Alessia Longo,Luigi De Angelis,Mariapia Vassalli,Marcello Di Pumpo*

Main category: cs.CV

TL;DR: 本研究测试了四种顶尖大视觉语言模型（VLMs）在意大利医学视觉问答中的视觉依赖性，发现模型对视觉信息的依赖程度差异显著，一些模型主要依赖文本而非真正进行视觉分析。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在医学视觉问答取得了优异表现，但其是否真正利用视觉信息仍然不明晰。作者希望通过实验明确各模型对视觉内容的依赖程度，为模型评估和临床部署提供参考。

Method: 选择欧洲医学问答意大利语数据集中的60道需图像解读的题目，将正确医学图像替换为空白，占位符后让Claude Sonnet 4.5、GPT-4o、GPT-5-mini和Gemini 2.0 flash exp四个模型回答，并比较准确率变化，同时分析模型的作答逻辑。

Result: GPT-4o的准确率下降最明显，从83.2%降至55.3%，显示较强视觉依赖，GPT-5-mini、Gemini、Claude的准确率下降幅度较小，依赖主要为文本推断。此外，所有模型在无真实图像时均能给出自信但虚构的视图解析。

Conclusion: VLMs在视觉医疗问答中对视觉信息的依赖性强弱不一，有些模型更依赖文本推理。临床应用之前，需要对其视觉理解能力进行更严格评估。

Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

</details>


### [289] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: 本文提出Percept-WAM模型，将2D/3D场景理解无缝整合入单一视觉-语言模型（VLM），显著提升自动驾驶中对复杂场景的空间感知和稳定性，特别在长尾和复杂互动情形下有重大突破。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在空间感知上准确性和鲁棒性不足，尤其在长尾和复杂场景下易失败。已有视觉-语言模型空间理解能力有限，严重影响了其在自动驾驶中的感知和定位效果。

Method: 提出Percept-WAM模型，通过World-PV和World-BEV token统一建模2D/3D感知任务，结合编码空间坐标和置信度。创新性采用基于网格的预测机制，实现IoU感知评分和平行自回归解码。利用预训练VLM参数增强通用智能，模型可端到端输出感知结果和运动规划。

Result: 在COCO 2D检测和nuScenes BEV 3D检测等感知基准上，Percept-WAM表现与经典检测器/分割器持平或更优，分别获得51.7/58.9 mAP。结合轨迹解码器后，规划性能（如NAVSIM上的PMDS）也超越现有DiffusionDrive（高2.1分）。

Conclusion: Percept-WAM有效提升了视觉-语言模型在自动驾驶空间感知、复杂场景理解和运动规划能力，展现出强大的开放词汇、长尾泛化能力，对推进具备世界感知和智能推理的自动驾驶系统具有重要意义。

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [290] [Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229)
*Selena Song,Ziming Xu,Zijun Zhang,Kun Zhou,Jiaxian Guo,Lianhui Qin,Biwei Huang*

Main category: cs.CV

TL;DR: 本论文提出DiT-Mem，一种为Diffusion Transformer（DiT）视频生成模型增强物理规律与世界知识的方法，利用可插拔记忆编码器提升视频生成质量和物理合规律性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Diffusion Transformer的视频生成模型虽然具有优秀的视觉质量和时序一致性，但常常缺乏对基本物理定律和常识动态的遵循。这表明模型缺少显式的世界知识，亟需补充此能力，以生成更合理、更真实的视频。

Method: 作者受到基于Transformer的大语言模型上下文记忆机制启发，首先通过对DiT隐藏状态的干预实验，发现可以利用embedding空间的低通和高通滤波，自然分离外观信息和物理/语义线索，并实现有针对性的指导。基于此，作者设计了可学习的记忆编码器DiT-Mem，包括堆叠3D CNN、低/高通滤波器和自注意力层，将参考视频映射为紧凑的记忆token，并在DiT的自注意力层内作为记忆插入。训练时，仅优化记忆编码器，冻结扩散主干网络，提升效率并实现即插即用。

Result: 在训练参数仅1.5亿、数据量约1万的条件下，所提方法有效提升了最先进扩散视频生成模型的物理规律遵循性和视频真实感。大量实验数据表明方法显著提升了生成视频的合规律性与视觉质量。

Conclusion: DiT-Mem能够以高效、即插即用方式为DiT类视频扩散模型注入有用的世界知识，明显提高其生成视频的合规律性和真实性，有望促进物理知识增强的视频生成应用。

Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

</details>


### [291] [IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2511.19235)
*Carl Lindström,Mahan Rafidashti,Maryam Fatemi,Lars Hammarstrand,Martin R. Oswald,Lennart Svensson*

Main category: cs.CV

TL;DR: 本文提出IDSplat，一种无需人工标注、具备实例分解能力的自监督3D高斯点云动态场景重建方法，能高效还原具备独立运动轨迹的动态驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有高保真动态场景重建方法存在依赖人工标注或无法显式分离动态与静态元素的问题，导致场景成分交织，难以用于真实感仿真和自动驾驶训练。

Method: 作者提出了IDSplat，通过自监督方式在不需人工标注的前提下，将动态对象建模为刚体实例。方法包含：1）利用基于零样本、语言引导的视频追踪算法，与激光雷达结合，实现3D实例分割与跟踪；2）提取特征对应关系确定一致位姿；3）提出协调转弯平滑机制以优化运动轨迹，减少姿态误差和跟踪失败；4）联合优化对象位姿与高斯参数。

Result: 在Waymo Open Dataset上实验表明，IDSplat在保持对象实例级分解的同时，动态场景重建质量优异，并能在不同序列和观测密度间泛化，无需重新训练。

Conclusion: IDSplat能够高效、实用地为大规模自动驾驶提供实例可控的动态场景重建仿真，推动无标注自动驾驶场景数字化发展。

Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.

</details>


### [292] [Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation](https://arxiv.org/abs/2511.19254)
*Mohamed Rissal Hedna,Sesugh Samuel Nder*

Main category: cs.CV

TL;DR: 本论文研究了物流场景下基于视觉的货物占用率估计系统对物理对抗补丁攻击的脆弱性，利用全三维仿真环境优化补丁贴图，实现了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉系统广泛应用于现代物流的规划、路由及计费等环节，其面临物理对抗性攻击（如可打印并贴上的对抗补丁）的安全隐患。本研究动机在于评估和量化此类系统在真实物理环境下遭受补丁攻击的可行性和威胁程度。

Method: 作者在全仿真的3D环境下，基于Mitsuba 3差分渲染技术，针对不同几何、光照和视角条件，优化对抗补丁的纹理，并与常规2D合成贴图方法进行对比。

Result: 3D优化得到的对抗补丁在“空货到满载”的拒绝服务场景下可达84.94%的攻击成功率，在“满载到空货”的隐蔽攻击场景下也有30.32%的成功率。分析了影响补丁攻击效果的主要因素。

Conclusion: 货物占用率视觉系统在物理补丁攻击下存在较大安全隐患。为保障自动化物流管道的安全，需提升其物理鲁棒性。本工作首次在真实物理3D仿真场景下验证了此类攻击的威胁。

Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

</details>


### [293] [LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models](https://arxiv.org/abs/2511.19261)
*Shuai Wang,Daoan Zhang,Tianyi Bai,Shitong Shao,Jiebo Luo,Jiaheng Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新方法LAST，使得通用视觉-语言模型（VLMs）能够更好地理解3D空间和长视频，仅依赖于一组2D图片作为输入，并显著提升相关任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在3D空间和长视频理解方面表现不佳，尽管在常规视觉-语言任务中非常强大，而且针对3D或视频任务通常需要专门的结构设计。论文旨在通过一个统一方法弥补这一不足。

Method: 作者提出了LAST（LeArn to Think in Space and Time）框架，使VLMs在给出最终答案前能在空间与时间维度进行“视觉思考”，通过建立3D空间和时间上的思考轨迹，帮助模型更全面地理解输入内容。方法可直接对闭源模型进行zero-shot测试，也可通过包含思考轨迹的数据对通用VLMs进行微调。

Result: LAST在多个基准测试上表现优异，如在EgoSchema数据集上，GPT-4o的zero-shot提升了15.8%；在VSI-Bench上，相比Qwen2.5-VL-7B提升了8.3分。此外，在空间、视频和图像理解任务上均取得了显著进步。

Conclusion: LAST为提升VLMs对3D空间和长视频的理解能力提供了一种简洁且有效的框架，无需专门架构即可实现大幅性能提升，有助于通用视觉-语言模型的进一步发展应用。

Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

</details>


### [294] [BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment](https://arxiv.org/abs/2511.19268)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yu Lu,Yunqiu Xu,Zhizhong Wang,Zeyi Huang,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于文本-条件图像生成中冲突处理的新框架BideDPO，有效提升了文本匹配率和条件遵循度。


<details>
  <summary>Details</summary>
Motivation: 现有带结构、空间或风格先验的条件图像生成方法，在输入矛盾或模型自身产生偏差时，难以兼顾文本与条件的对齐，传统监督微调难以解决这一类冲突。

Method: 提出Bidirectionally Decoupled Direct Preference Optimization（BideDPO）框架，将条件和文本分别生成成解耦的偏好对，采用自适应损失平衡，两者联合优化；自动化采样生成富含冲突的数据，用于迭代训练模型。还提出DualAlign基准评测对抗冲突。

Result: BideDPO显著提升了文本成功率（如提升35%）和条件遵循度。在COCO等数据集上验证了方法有效性。

Conclusion: BideDPO框架能够更好地解决文本与条件对齐冲突问题，为多约束图像生成提供了高效的偏好优化新思路。

Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

</details>


### [295] [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](https://arxiv.org/abs/2511.19274)
*Mingyang Chen,Jiawei Du,Bo Huang,Yi Wang,Xiaobo Zhang,Wei Wang*

Main category: cs.CV

TL;DR: 该论文提出利用扩散模型，通过重构误差估算数据似然，从而指导更有效的核心样本集选择，实验证明方法在ImageNet上优于以往方法，用50%数据表现接近全量训练。


<details>
  <summary>Details</summary>
Motivation: 现有核心集筛选方法大多依赖启发式得分（如模型不确定性），未直接建模数据的概率分布，可能遗漏数据分布中的关键细节，影响模型训练效果。

Method: 提出基于扩散模型的重构误差来估算数据似然，并通过ELBO理论建立重构误差与数据似然之间的关系，引入信息论方法确定最佳重构步数，作为数据选择的得分标准。

Result: 在ImageNet大规模实验中，该方法作为得分信号选取核心集，优于此前基线方法，并且只用50%的数据即可获得和全量数据训练相近的效果。

Conclusion: 基于数据似然的核心集选择更能反映数据分布特性和模型偏好，为高效数据选取提供理论与实践支持，结果优异且具有解释性。

Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

</details>


### [296] [ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278)
*Qianying Liu,Xiao Liang,Zhiqiang Zhang,Yibo Chen,Xu Tang,Zhongfei Qing,Fengfan Zhou,Yao Hu,Paul Henderson*

Main category: cs.CV

TL;DR: ReMatch提出了一套利用多模态大模型生成能力进行多模态检索的新框架，通过生成式匹配阶段增强判别与泛化能力，在多个基准数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 以往多模态检索方法将多模态大模型（MLLM）视作简单的编码器，忽略了其生成性、组合推理与世界知识，无法充分发挥模型能力。该工作旨在充分挖掘与利用MLLM的这些特性，实现更强大和泛化性更好的多模态检索。

Method: 核心方法是端到端训练MLLM，通过对话式生成式匹配阶段进行实例判别，使用同一MLLM自回归判断查询与文档的相关性。结合标准对比损失与实例判别监督，提升模型在困难负例上的表现，同时保留生成模型的组合表达能力。并引入可学习token，实现语义更丰富、低成本的多模态嵌入。最后整合多种创新训练方法，形成有力的训练方案。

Result: 在Massive Multimodal Embedding Benchmark (MMEB)上取得了新的SOTA，在五个数据集上展现出极强的零样本泛化能力，验证了方法的鲁棒性与可迁移性。

Conclusion: ReMatch能够更好地利用MLLM的生成与推理能力，实现多模态检索的性能提升与更好的泛化迁移能力，为多模态理解领域提供了新的思路与工具。

Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

</details>


### [297] [DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting](https://arxiv.org/abs/2511.19294)
*Phurtivilai Patt,Leyang Huang,Yinqiang Zhang,Yang Lei*

Main category: cs.CV

TL;DR: 本文提出了一种在3D高斯泼洒(3DGS)方法中，利用稀疏LiDAR数据与单目深度联合先验稠密化的方式，改善初始化点云质量，提升视觉效果并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法严重依赖自适应密度调控，造成浮动伪影和资源浪费，亟需改进初始化策略，以提升效率及场景还原质量。

Method: 方法在场景初始化时，通过稀疏LiDAR和RGB图像单目深度估计相结合，采用关注ROI（感兴趣区域）的采样方案，直接产生稠密点云，无需后续密度自适应控制。同时优化3D高斯属性，提升视觉质量和效率。

Result: 该方法在四个新采集的数据集上，通过对比与消融实验，表现出与先进方法相当的视觉质量，但显著降低了资源消耗和训练时间，特别是在复杂场景中的ROI区域还原方面效果突出。

Conclusion: 先验稠密化方式在3DGS框架下有效解决了传统方法的冗余与伪影问题，在保证或提升视觉质量的同时，提高了计算和训练效率，适用于复杂三维场景的重建。

Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.

</details>


### [298] [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](https://arxiv.org/abs/2511.19301)
*Johannes Meier,Florian Günther,Riccardo Marin,Oussema Dhaouadi,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了一种用于单目3D目标检测的新型主动学习方法IDEAL-M3D，通过实例级选择和多样性提升模型，显著提升了标注效率和检测性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测易于部署但对标注依赖重且3D标注成本极高。为在有限标注预算下提升性能，需要优先选择对性能提升贡献最大的样本进行标注，即主动学习。然而，以往方法存在只选整个图片且偏向远距离目标的问题，导致效率低和样本偏差。

Method: 提出IDEAL-M3D（实例级主动学习管线），创新地采用多异构主干网络、任务无关特征、损失权重扰动和时序bagging等技术，形成多样性突出、易于训练的模型集，支持实例级的样本选择，避免了全局不相关标注和远距离目标偏差。

Result: 在KITTI数据集上，IDEAL-M3D标注60%数据即可达到等同或更高的AP3D性能，相较于全量标注可显著节省标注人力和资源。

Conclusion: IDEAL-M3D为单目3D目标检测领域率先实现实例级主动学习，显著提升了样本利用率和检测表现，具备广泛部署价值。

Abstract: Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.
  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.

</details>


### [299] [Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection](https://arxiv.org/abs/2511.19306)
*Zixuan Wang,Haoran Sun,Jiaming Lu,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Xuelin Qian,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了一种新的端到端语言提示驱动的红外小目标检测方法（DGSPNet），通过引入多层次的文本语义提示和文本引导的注意力机制，显著提升了小目标检测的准确率，并在多个数据集上达到了最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测面临特征表达受限和背景干扰严重的问题。尽管受CLIP启发的方法尝试引入文本指导，但往往依赖不准确的文本描述和手工注释，限制了检测效果。因此，需要一种无需注释，且能充分利用文本精确性的新方法。

Method: 作者提出DGSPNet框架，集成了粗粒度（如“红外图像”、“小目标”）和细粒度（通过视觉到文本映射获得的个性化描述）的双层次语义提示。模型中还设计了文本引导的通道注意力（TGCA）和空间注意力（TGSA）机制，用以提升对潜在目标的检测敏感性，无需依赖人工标注。

Result: 在三个公开基准数据集上进行的实验表明，该方法在检测准确率上取得了显著提升，实现了当前最优的检测表现。

Conclusion: DGSPNet通过结合多层次语义提示和创新的注意力机制，有效提升了红外小目标检测的性能，并证明了语言提示在此任务中的应用潜力。

Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.

</details>


### [300] [Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach](https://arxiv.org/abs/2511.19316)
*Xincheng Wang,Hanchi Sun,Wenjun Sun,Kejun Xue,Wangqiu Zhou,Jianbo Zhang,Wei Sun,Dandan Zhu,Xiongkuo Min,Jun Jia,Zhijun Fang*

Main category: cs.CV

TL;DR: 本文对数据集水印技术在扩散模型微调中的有效性进行了系统评估，提出统一的评测框架，并揭示现有方法在实际威胁下的脆弱性。作者还提出了一种有效去除水印的方法，指出未来研究挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型的微调可带来个性化合成能力，但随之而来的版权和安全风险需溯源手段。现有水印技术虽可嵌入训练集但评估体系不统一、实际鲁棒性未明。因此有必要科学系统评估水印方法的性能与短板。

Method: 作者建立了统一的威胁模型和全新的评估框架，以普适性（Universality）、可传播性（Transmissibility）和鲁棒性（Robustness）为核心指标，对现有水印技术进行了实验性综合评测。同时提出一种实用且完全去除水印的新方法以揭示防御的薄弱环节。

Result: 实验发现，当前水印方法在普适性和可传播性表现良好，对常规的图像处理操作有一定的鲁棒性，但在实际威胁场景下存在明显短板。新提出的去水印技术能完全移除水印且不影响微调效果。

Conclusion: 当前主流水印方法尚难应对真实威胁，急需更强鲁棒性的技术。本文的评测框架和去除攻击方法为未来数据集水印的安全性和可靠性研究指明了方向。

Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.

</details>


### [301] [SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis](https://arxiv.org/abs/2511.19319)
*Lingwei Dang,Zonghan Li,Juntong Li,Hongwen Zhang,Liang An,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: 提出SyncMV4D，实现多视角手与物体交互视频与4D动态运动的同步生成，在视觉真实感、运动合理性和多视角一致性等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视角的视频方法无法完整感知3D几何，且存在失真和动作不自然的问题；3D方法需依赖高质量实验室数据，难以适用于真实场景。因此需要一个能结合多视角、视觉先验和运动学的新方法以实现更真实且具有泛化能力的HOI生成。

Method: 提出SyncMV4D框架，包括Multi-view Joint Diffusion (MJD)联合生成视频与中间运动、Diffusion Points Aligner (DPA)对中间运动进行4D全局对齐。通过闭环机制实现2D视频与4D动态的耦合，在去噪过程中视频与4D运动互为条件、增强生成效果。

Result: 实验表明，SyncMV4D在视觉真实感、动作合理性和多视角一致性等指标上均优于当前最先进方法。

Conclusion: SyncMV4D创新性地实现了多视角HOI视频与4D动态同步生成，有效结合2D与4D特征，对动画、机器人等领域具有潜在重要应用价值。

Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

</details>


### [302] [SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation](https://arxiv.org/abs/2511.19320)
*Jiaming Zhang,Shengming Cao,Rui Li,Xiaotong Zhao,Yutao Cui,Xinglin Hou,Gangshan Wu,Haolan Chen,Yu Xu,Limin Wang,Kai Ma*

Main category: cs.CV

TL;DR: 提出SteadyDancer新方法，实现了高保真、精准控制的人像动画，首次稳健保持首帧身份，实现视觉质量与动作控制双优。


<details>
  <summary>Details</summary>
Motivation: 当前主流的人像动画R2V方法在转换时易出现首帧身份漂移和视觉伪影，原因在于忽视了动态时空错位。如何同时精准控制动作又保持高保真首帧身份，是该领域的核心难题。

Method: 1. 提出Condition-Reconciliation Mechanism，协调输入状态与动作控制条件，兼顾准确性与还原度。
2. 设计Synergistic Pose Modulation Modules，自适应生成更加匹配输入图像的动作表示。
3. 采用分阶段、任务解耦的训练策略，层次化优化动作保真、视觉质量和时序一致性。

Result: SteadyDancer在外观保真和运动控制方面都达到了SOTA水平，并且所需训练资源显著低于现有同类方法。

Conclusion: SteadyDancer首次稳健实现了首帧身份保持，兼得动作精准控制和高视觉质量，性能与资源消耗兼优，推动了人像动画生成技术的发展。

Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.

</details>


### [303] [MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation](https://arxiv.org/abs/2511.19326)
*Farnoosh Koleini,Hongfei Xue,Ahmed Helmy,Pu Wang*

Main category: cs.CV

TL;DR: MonoMSK是一种结合数据驱动和物理仿真的新框架，能从单目视频中高精度重建人体运动和动力学（运动学和力学）。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D人体动作重建方法多采用简化的人体模型（如SMPL），忽略了生物力学与物理规律，难以真实还原人体运动及受力；而基于标记点的方法则受限于实验室环境且效率低。为解决这些局限，作者提出更为真实、物理一致的重建方案。

Method: 提出MonoMSK，结合了变换器网络的逆向动力学与可微分的前向动力学和动力学仿真。该框架采用解剖学级肌肉骨骼模型，通过前后物理一致性和新颖损失项提升运动/用力推断的准确性和合理性。

Result: 在BML-MoVi、BEDLAM和OpenCap数据集上，MonoMSK在运动学准确性上明显优于已有方法，并首次实现了通过单目视频精确估算运动动力学参数（力与力矩）。

Conclusion: MonoMSK在确保物理合理性和生物力学一致性的同时，实现了更准确的三维人体运动学和动力学重建，推动了单目视频下高保真人体运动分析的发展。

Abstract: Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.

</details>


### [304] [POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse](https://arxiv.org/abs/2511.19339)
*Anjie Le,Can Peng,Yuyuan Liu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出了一种针对表征(Representation)级别的高效机器遗忘方法，通过几何投影达到最优遗忘效果，并用新的评估指标验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法大多只改变分类器部分，未有效消除内部表征中的记忆，导致遗忘不彻底。因此作者试图在表征层面实现真正的遗忘。

Method: 作者借助Neural Collapse理论，将Simplex Equiangular Tight Frame(ETF)做正交投影，实现在低维空间中也具有最优遗忘的投影操作，提出了POUR（Provably Optimal Unlearning of Representations）方法，包括闭式投影（POUR-P）和蒸馏下的特征级遗忘（POUR-D）。同时提出RUS指标量化遗忘和保留效果。

Result: 在CIFAR-10/100和PathMNIST等数据集上的实验表明，POUR能高效遗忘指定信息，并保留剩余知识，在分类及表征两方面指标上均超越现有最佳方法。

Conclusion: POUR方法在表征级遗忘方面理论上最优且实证有效，为机器遗忘提供了新思路，可解决遗忘不彻底等实际难题。

Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.

</details>


### [305] [Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning](https://arxiv.org/abs/2511.19343)
*Qihan Huang,Haofei Zhang,Rong Wei,Yi Wang,Rui Tang,Mingli Song,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了Syn-GRPO方法，通过在线数据生成器合成高质量多样响应训练数据，提升多模态大模型（MLLM）感知能力的强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法虽然可提升多模态大模型的泛化能力，但数据样本质量低、响应单一，限制了模型探索能力，亟需切实提升训练数据多样性与质量的新方法。

Method: 提出Syn-GRPO，由数据服务器和GRPO流程两部分组成。数据服务器用生成模型从已有样本中在线合成新样本，实现高效多样的数据生成；GRPO流程根据多样性奖励引导MLLM生成多样描述，进一步丰富样本响应。

Result: 在三类视觉感知任务中，Syn-GRPO显著提升了数据质量，模型感知任务表现优于现有方法。

Conclusion: Syn-GRPO大幅扩大了数据多样性与质量，强化了MLLM感知能力，为长期自进化强化学习提供了良好基础。

Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

</details>


### [306] [CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting](https://arxiv.org/abs/2511.19351)
*Abdurahman Ali Mohammed,Catherine Fonder,Ying Wei,Wallapak Tavanapong,Donald S Sakaguchi,Qi Li,Surya K. Mallapragada*

Main category: cs.CV

TL;DR: 该论文提出了一个包含3023幅图像和超过43万个手工标注细胞位置的大规模免疫细胞计数数据集，并对多种细胞计数算法进行了系统评测，提出的方法在准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动细胞计数受限于小规模且难以获得的高质量标注数据集，现有公开数据集普遍图像数量少于500，导致深度学习方法难以充分训练和评估。因此亟需一个规模大、标注精细的细胞计数数据集推动领域进步。

Method: 作者构建了一个高难度、多样化的大规模细胞计数数据集，并系统评测了回归型、人群计数型与传统细胞计数三大类主流方法。此外，探索了Segment Anything Model（SAM）对仅点注释数据集的适用性，并提出基于密度图的SAM-Counter新方法。

Result: SAM-Counter方法在测试集上取得了22.12的平均绝对误差（MAE），显著优于现有方法（次优为27.46）。数据集包含高密度、重叠、异质形态细胞以及标注协议多样性，对算法有较高挑战性。

Conclusion: 作者提供了首个大规模、高难度免疫细胞计数公开数据集，并基于该数据集建立了系统评测基准，推动了细胞计数自动化方法的进步，为未来细胞计数领域的研究奠定了坚实基础。

Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.

</details>


### [307] [Growing with the Generator: Self-paced GRPO for Video Generation](https://arxiv.org/abs/2511.19356)
*Rui Li,Yuanzhi Liang,Ziqi Ni,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种自适应奖励机制的Group Relative Policy Optimization（GRPO）方法，用于提升视频生成模型的训练效果，取得了优于传统静态奖励GRPO的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法使用静态奖励模型，训练过程中奖励不变，这导致分布偏差、奖励容易饱和、生成器提升受限，影响模型稳定性和性能。

Method: 提出“Self-Paced GRPO”框架，引入了渐进式奖励机制。奖励反馈会根据生成器能力不断调整，从强调粗略视觉保真度逐步过渡到时序连贯性和细粒度文本-视频语义对齐，形成自适应难度课程学习。

Result: 在VBench数据集和多种视频生成主干上实验，Self-Paced GRPO在视觉质量和语义对齐方面均超过了使用静态奖励的GRPO基线方法。

Conclusion: Self-Paced GRPO能够有效减少奖励和策略的不匹配，提高视频生成的质量和对齐效果，具有较强的通用性和有效性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.

</details>


### [308] [DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation](https://arxiv.org/abs/2511.19365)
*Zehong Ma,Longhui Wei,Shuai Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 该论文提出了frequency-DeCoupled（DeCo）像素扩散框架，通过将高频和低频部分的生成解耦，有效提升了像素扩散生成图像的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的像素扩散模型训练与推理速度慢，是因为高频信号与低频语义信息都被同一个扩散模型处理，导致计算冗余。为了解决这一效率瓶颈，需要一种能够分离处理不同频率成分的新方法。

Method: 作者提出了frequency-DeCoupled pixel diffusion架构，用轻量级像素解码器生成高频细节，而将低频语义建模任务交给扩散Transformer（DiT）。同时设计了frequency-aware flow-matching loss，专注对视觉显著频率的优化，抑制不重要的频率。

Result: 在ImageNet数据集上，DeCo模型在256x256和512x512分辨率下分别获得了1.62和2.22的FID，优于以往的像素扩散模型，并逐步缩小了与潜空间扩散方法的性能差距。在GenEval基准的文本到图像生成任务中，预训练模型系统级领先，取得总分0.86。

Conclusion: DeCo模型通过频率解耦、大大提升了像素扩散在效率与性能上的表现，在多个基准上取得了领先结果，是像素扩散方向的重要进步，且代码已公开，方便社区复现和后续研究。

Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.

</details>


### [309] [An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification](https://arxiv.org/abs/2511.19367)
*Saniah Kayenat Chowdhury,Rusab Sarmun,Muhammad E. H. Chowdhury,Sohaib Bassam Zoghoul,Israa Al-Hashimi,Adam Mushtak,Amith Khandakar*

Main category: cs.CV

TL;DR: 本文提出了一种结合医学知识的深度学习管道，通过分割和定量分析肿瘤与邻近解剖结构的关系，实现对肺癌肿瘤分期的高准确度自动化判别。


<details>
  <summary>Details</summary>
Motivation: 现有端到端深度学习在肺癌分期时易忽视对空间及解剖关系的显式建模，而肿瘤分期高度依赖肿瘤尺寸及其与邻近解剖结构的空间关系，因此需要更医学导向的方法来提升分期准确性和可解释性。

Method: 方法上，作者设计了专用的编码-解码网络分割肺部及相关解剖结构（如肺叶、肿瘤、纵隔、膈肌），据分割结果定量测量肿瘤最大尺寸与距关键解剖结构的距离，最后依据医学指南采用基于规则的分期判定。

Result: 在Lung-PET-CT-Dx数据集上，方法整体准确率91.36%，分期F1-score分别为T1:0.93、T2:0.89、T3:0.96、T4:0.90，优于传统深度学习基线且实现了细粒度评测。

Conclusion: 本工作首次将临床分期知识显式嵌入自动分期模型，实现了高性能与决策透明性的兼备，对医学影像肿瘤分期应用具有重要价值。

Abstract: Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.

</details>


### [310] [UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval](https://arxiv.org/abs/2511.19380)
*Maroun Ayli,Youssef Bakouny,Tushar Sharma,Nader Jalloul,Hani Seifeddine,Rima Kilany*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图的全新UI表示方法，将UI截图转化为带有属性的图，实现了视觉、结构和语义多层级的相似度建模，并应用在UI检索系统中，效果优于当前主流视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 企业软件公司存在产品和版本间大量UI界面，导致设计一致性、模式发现和合规性检查极具挑战。现有方法多依赖视觉或文本，忽视了UI结构属性的表达。

Method: 作者将UI截图转化为带有层级关系和空间信息的属性图，提出对比图自动编码器学习UI的多层次嵌入表示。这种方法可扩展到文档布局、建筑图等结构化视觉领域。

Result: 实验表明，结构性嵌入在判别能力上优于最先进视觉编码器。在20,396个金融软件UI上，UISearch系统实现了Top-5准确率0.92，中位延迟47.5ms（P95: 124ms），可扩展到2万+界面。

Conclusion: 基于结构的UI表示方法显著提升了UI检索与区分的能力，支持复杂查询和细粒度区分，为UI分析和搜索提供了更强的表达力和效率。

Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.

</details>


### [311] [BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation](https://arxiv.org/abs/2511.19394)
*Rachit Saluja,Asli Cihangir,Ruining Deng,Johannes C. Paetzold,Fengbei Liu,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: 提出了一种名为BackSplit的方法，通过对背景类进行细分标签，显著提升了医学图像中小病灶的分割性能。该方法无需增加推理开销，且实验中表现出较强的鲁棒性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的小病灶分割方法通常把所有非病灶像素归为单一的背景类，忽略了背景内丰富的人体解剖信息，这限制了分割模型对小病灶复杂环境的感知和识别能力。

Method: 本文提出BackSplit方法，通过使用细化的背景标签（如组织、器官及结构等），而不是将全部背景合为一类。实现方式可以人工标记或利用现有预训练分割模型自动生成。这种方法在训练阶段不增加额外推理成本。

Result: 理论上，该方法提升了信息量（Fisher信息），优化更稳定；实验证明在多个数据集和网络结构下，均显著提升小病灶分割性能。即使采用自动生成或交互式分割框架的辅助标签时，依然有效。

Conclusion: 细分背景标签是一种简单且有效的手段，其对小病灶分割的提升明显，并且具有较强的鲁棒性和通用性，可广泛应用于医学图像分析中。

Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.

</details>


### [312] [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的可控图像到视频生成范式——In-Video Instruction，通过在视频帧内嵌入视觉信号（如文本、箭头、轨迹等）作为指令，指导视频生成，并验证了当前主流视频生成模型对此新型指令的识别与执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视频生成模型在视觉表现上取得了显著进步，但现有的控制方法主要基于文本描述，表达能力有限且不够精细。因此作者探索是否可借助视觉域中的直观信号，实现更细粒度和空间感知的可控视频生成。

Method: 作者提出In-Video Instruction范式，将用户指导直接嵌入到视频帧的视觉元素中（如叠加文本、箭头、路径），为不同对象分配独立指令，继而驱动视频生成过程，并在Veo 3.1、Kling 2.5和Wan 2.2等生成器上进行实验评估。

Result: 实验证明，大型视频生成模型能够准确理解并执行帧中嵌入的视觉指令，尤其是在多对象复杂场景下，也表现出良好的可控性和一致性。

Conclusion: In-Video Instruction为可控视频生成提供了更直观、精确的用户交互方式，展现出比传统文本提示更强的表达能力，未来有望扩展视频内容生成和编辑的应用范围。

Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.

</details>


### [313] [Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens](https://arxiv.org/abs/2511.19418)
*Yiming Qin,Bomin Wei,Jiaxin Ge,Konstantinos Kallidromitis,Stephanie Fu,Trevor Darrell,Xudong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Chain-of-Visual-Thought（COVT）的新框架，为视觉-语言模型（VLMs）引入连续视觉token，使模型能有效进行空间与几何推理，从而提升其多模态感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型在语言推理上表现优异，但因难以捕捉空间维度上的稠密视觉特征，导致感知理解和空间/几何推理能力有限。此问题制约了模型在需要视觉细致理解的应用场景中的表现。

Method: COVT框架通过引入连续视觉token，将2D外观、3D几何、空间布局以及边缘结构等感知知识以紧凑的token形式融入VLMs中。在训练时，VLM结合COVT预测这些视觉token以重构包含深度、分割、边缘、DINO特征等多种稠密感知信号，实现知识迁移与融合；推理时则在连续视觉token空间内进行高效推理，并可选实现密集可解释性预测。

Result: 在超过十个感知基准（如CV-Bench, MMVP, RealWorldQA等）上，将COVT集成进主流VLM（包括Qwen2.5-VL、LLaVA）后，模型性能得到3%~16%的显著提升。

Conclusion: COVT能够让VLM具备更精确且可解释的多模态推理与感知能力，通过融合连续视觉思维，实现了视觉信息的高效利用，为多模态智能发展提供了新方向。

Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.

</details>


### [314] [SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation](https://arxiv.org/abs/2511.19425)
*Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang*

Main category: cs.CV

TL;DR: 本论文提出了SAM3-Adapter，这是首个专为最新的SAM3模型设计的适配器框架，显著提升了其在细粒度分割任务中的表现并刷新了多项下游任务的精度记录。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模基础模型（如Segment Anything和其继任者SAM2）在通用分割任务表现卓越，但在细致、低层级分割（如隐蔽物体检测、医学和细胞图像分割、阴影检测）等领域依然存在明显短板。因此作者希望通过改进适配器机制，更好地利用升级后的SAM3架构，解决这些难题。

Method: 作者提出了SAM3-Adapter，继承了原SAM-Adapter的模块化设计思想，并针对SAM3的重构架构和改进训练流程进行了优化。该框架不仅降低了计算资源消耗，同时能灵活地应用于多种分割场景，并在下游任务中实现了与原生SAM/SAM2模型及其它改编方法的对比提升。

Result: 实验表明，将SAM3与SAM3-Adapter结合后，在医学影像、隐蔽物体分割和阴影检测等多个任务上均取得了新的最优结果。同时在精度、鲁棒性和效率方面全面优于所有SAM相关已有方法。

Conclusion: SAM3-Adapter框架大幅拓展了SAM3的下游分割适应力和泛化能力，是今后研究和实际应用的坚实基础。相关代码和模型已同步开放，为社区创新提供了便利。

Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.

</details>


### [315] [Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction](https://arxiv.org/abs/2511.19426)
*Yun Zhou,Yaoting Wang,Guangquan Jie,Jinyu Liu,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出Ref-SAM3D方法，使3D重建模型能够利用文本描述实现单张RGB图像的特定对象三维重建，在零样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的SAM3D虽有强大的3D重建能力，但无法依据文本描述指定要重建的特定目标，限制了其实用性，如3D编辑、游戏开发等场景中对指定对象的需求。

Method: 作者扩展了SAM3D，提出Ref-SAM3D，将文本描述作为高层先验信息，指导3D重建流程，实现基于自然语言和单张2D图片的3D目标重建。

Result: 通过大量定性实验，Ref-SAM3D仅依赖一张2D图片和文本描述，即可获得竞争性的高保真零样本三维重建效果。

Conclusion: Ref-SAM3D有效弥合了2D视觉信息与3D几何理解的鸿沟，为参考引导的3D重建提供了更灵活和易用的新范式。

Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.

</details>


### [316] [Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430)
*Dingkang Liang,Cheng Zhang,Xiaopeng Xu,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了基于运筹学知识的3D任务调度新任务（ORS3D），并构建了大规模数据集ORS3D-60K，用以促进行为体AI在自然语言、三维环境和效率任务中的进步；还提出了GRANT多模态模型，能有效理解、计划并执行任务。


<details>
  <summary>Details</summary>
Motivation: 现有任务调度数据集过于简化，缺乏对运筹学知识和三维空间落地的结合，无法反映实际3D物理世界中智能体高效完成复杂任务的需求。

Method: 1) 提出ORS3D任务，需要智能体结合自然语言理解、三维空间定位与任务效率优化。2) 构建ORS3D-60K数据集，包括60,000个复合任务与4,000个真实世界场景。3) 提出具备调度标记机制的多模态大语言模型GRANT，实现高效任务计划和动作生成。

Result: 在ORS3D-60K数据集上，大量实验表明GRANT模型在任务语言理解、3D定位和调度效率方面均表现优越，显著超过基线方法。

Conclusion: 通过融合运筹学知识与3D空间感知，智能体可生成更高效的任务计划。ORS3D-60K和GRANT为研究复杂场景下的智能体调度提供了有力工具，为未来体能AI发展铺平道路。

Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT

</details>


### [317] [Cloud4D](https://arxiv.org/abs/2511.19431)
*Jacob Lin,Edward Gryspeerdt,Ronald Clark*

Main category: cs.CV

TL;DR: 本论文提出Cloud4D框架，使用地面摄像头重建高分辨率的四维云状态，显著提升空间和时间分辨率，并准确估算风向风速。


<details>
  <summary>Details</summary>
Motivation: 目前气象和气候模型多为公里级分辨率，难以刻画云、极端降水、阵风等小尺度现象。要提升分辨率，需高分辨率实测数据，但现有仪器难以满足需求，因此需要创新性的解决方案。

Method: 提出Cloud4D，首个基于学习的方法，仅依靠同步地面摄像头，通过homography引导的2D-to-3D Transformer网络，重建物理一致、四维（空间+时间）云状态，包括25米空间分辨率和5秒时间分辨率的液态水含量分布，并可估算三维风矢量。

Result: 在为期两个月、六组相机的部署实验中，Cloud4D较现有卫星技术在时空分辨率上提升一个数量级，对比雷达测量的相对误差低于10%。

Conclusion: Cloud4D大幅提升了小尺度天气现象的观测能力，为高分辨率气象建模和预测带来新可能。代码和数据已开放。

Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.

</details>


### [318] [Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts](https://arxiv.org/abs/2511.19434)
*Yasin Esfandiari,Stefan Bauer,Sebastian U. Stich,Andrea Dittadi*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型采样过程中，通过在去噪轨迹中的不同阶段切换两个预训练“专家”模型的方法，能够同时提升生成图像的质量和数据似然性，无需重新训练；实验在CIFAR-10和ImageNet32上展示了优异效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成时常面临样本质量与数据似然性之间的权衡：关注高噪声去噪阶段能生成真实感强的图片但似然性指标差，注重低噪声阶段则损害视觉质量。如何突破这一固有限制，是当前研究的难题。

Method: 作者提出一种简单的采样方法：在去噪过程中，先用注重图像质量的专家（模型）处理高噪声阶段，塑造整体结构；再在低噪声阶段切换到偏重似然性的专家，精细化像素统计。这一方法无需对基础模型重新训练或微调，只需选定切换时机。

Result: 在CIFAR-10和ImageNet32数据集上，采用专家切换的合并模型在图像质量和似然性上都优于或至少持平于各自的基础专家，突破了以往的指标权衡。

Conclusion: 通过在去噪不同阶段切换专家，能有效解决扩散模型中样本质量和似然性指标互相妨碍的问题，为相关模型开发和部署提供了新思路。

Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.

</details>


### [319] [Are Image-to-Video Models Good Zero-Shot Image Editors?](https://arxiv.org/abs/2511.19435)
*Zechuan Zhang,Zhenyuan Chen,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了IF-Edit框架，将大规模图像转视频扩散模型用于基于指令的零样本图像编辑，通过创新方法解决了关键挑战，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视频扩散模型展现了强大的世界模拟与时序推理能力，但其作为零样本图像编辑器的潜力尚未被深入研究。作者旨在系统性解决扩散模型用于图像编辑时的若干实际难题，并统一视频与图像生成推理的应用。

Method: 提出无需微调的IF-Edit框架，包含：1）链式思考提示增强模块，将静态编辑指令转为具有时序推理能力的提示；2）时间潜变量dropout策略，在模型的专家切换节点后压缩帧潜变量以加速去噪并保持语义一致性；3）自一致后期优化步骤，通过短静态视频轨迹对后期帧进行锐化。

Result: 在四个公开基准上评测，包括非刚性编辑、物理及时间推理以及通用指令编辑。IF-Edit在推理相关任务上有很强表现，并在通用编辑任务上也具备竞争力。

Conclusion: 本研究系统展示了视频扩散模型作为图像编辑器的能力，并提出了一种简单有效的统一视频-图像生成推理方法。

Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.

</details>


### [320] [VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection](https://arxiv.org/abs/2511.19436)
*Qiang Wang,Xinyuan Gao,SongLin Dong,Jizhou Han,Jiangyang Li,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: VDC-Agent 是一个无需人工标注和大型教师模型的自进化视频详细描述生成系统，通过自我生成描述、评分和提示优化，实现高质量视频字幕，显著提升性能并超越当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频详细字幕生成任务依赖大量人工标注或大型教师模型，成本高、效率低，且难以推广。作者希望开发一种无需人工干预、可自我进化的自动化系统来提升视频字幕生成的准确性和效率。

Method: 提出 VDC-Agent 框架，闭环包括生成字幕、基于原则的评分（提供得分与文本建议）、提示词改进；通过自我反思修正低质量更新。使用无标注视频，获得一系列（字幕、得分）对，转化为偏好元组并过滤异常样本，构建 18886 对样本的 VDC-Agent-19K 数据集。随后基于易到难的课程学习，对基础多模态大模型（MLLM）进行直接偏好优化微调。

Result: 基于 Qwen2.5-VL-7B-Instruct 构建的 VDC-Agent-7B 在 VDC 基准上获得 49.08% 平均准确率和 2.50 分的成绩，超越了现有专用视频字幕模型，同时比基础模型准确率提升 5.13%，得分提升 0.27，且推理成本相近。

Conclusion: VDC-Agent 能在无需人工标注和额外大型模型的前提下，有效推动视频详细字幕生成任务的发展，自动构建高质量数据并优化模型性能，达到甚至超越当前最优水平。

Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.

</details>


### [321] [LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context](https://arxiv.org/abs/2511.19437)
*Jingzhi Bao,Hongze Chen,Lingting Zhu,Chenyu Liu,Runze Zhang,Keyang Luo,Zeyu Hu,Weikai Chen,Yingda Yin,Xin Wang,Zehong Lin,Jun Zhang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了LumiTex方法，实现了更加真实的基于物理的渲染材料分解和无缝一致的纹理填充，并在纹理质量上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的纹理生成方法在材料分解和纹理的无缝、视角一致填充方面表现有限，特别是在照明信息有限的图片输入下。作者希望解决材料属性难以分离和纹理拼接不一致这两个难题。

Method: 提出一个端到端的LumiTex框架，包含三个关键部分：1）通过多分支生成方案，在共享光照先验下解耦反照率和金属-粗糙度，提高材料理解力；2）在解码阶段引入照明感知材料注意力机制，将照明上下文融入反照率、金属及粗糙度图的生成，确保物理真实性；3）基于大型视图合成模型的几何引导修复模块，实现丰富的纹理覆盖和UV空间的无缝、视角一致补全。

Result: 大量实验表明，LumiTex在纹理质量上达到了当前最先进水平，效果优于现有的开源以及商业方案。

Conclusion: LumiTex有效解决了材料分解和无缝、视角一致纹理补全难题，为基于物理的渲染纹理生成设定了新的高标准。

Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [322] [SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering](https://arxiv.org/abs/2511.17559)
*Gyubok Lee,Woosog Chay,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了SCARE基准，专为电子健康记录（EHR）中的自然语言提问与SQL查询系统设计，用于评估SQL生成后的安全验证机制，是针对临床环境中SQL自动生成安全性的首个统一评测基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）推动了EHR文本到SQL问答的发展，但由于自动生成SQL的错误风险，直接应用于临床系统存在严重安全隐患。目前缺乏评估SQL后验证机制的统一基准，这限制了系统的安全落地。

Method: 作者设计了SCARE基准，包含4200组基于MIMIC-III、MIMIC-IV和eICU数据库的问题、SQL查询和期望输出样本。覆盖七种主流text-to-SQL模型自动生成的查询，要求系统对问题能回答性分类，并对SQL查询进行验证或修正。随后，作者系统性评测了多种后验验证方法，包括两阶段方法与自主代理框架。

Result: 实验表明：在问题可回答性分类与SQL错误修正之间存在重要权衡，解决方案难以兼顾两方面的最优效果。此外，现有方法在多样化真实数据上的表现揭示了尚待解决的关键技术瓶颈。

Conclusion: SCARE为EHR问答系统安全机制的研究提供了基准，并指出后期SQL验证与修正仍面临挑战。未来研究需在提升分类准确性和SQL修正能力之间寻求更优平衡。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.

</details>


### [323] [$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving](https://arxiv.org/abs/2511.17560)
*Yuechi Zhou,Yi Su,Jianxin Zhang,Juntao Li,Qingrong Xia,Zhefeng Wang,Xinyu Duan,Baoxing Huai*

Main category: cs.CL

TL;DR: 本文提出一种新的KV Cache融合算法（A^3），用于提升大语言模型在长文本处理中的效率和性能。实验表明，其在保证任务表现的同时，将首token延迟减半。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然能处理长上下文，但解码延迟和内存开销较大，严重影响实际部署。现有KV Cache重用方法在降低开销时存在性能下降问题，因此需要更优方法平衡效率与性能。

Method: 作者提出了Attention-Aware Accurate KV Cache Fusion（A^3）算法。该方法基于文本片段与问题的相关性，预先计算并有选择地融合KV Cache，实现关键上下文信息的高效整合，从而减少不必要的计算和性能损耗。

Result: 在多个基准数据集和不同LLM上的实验显示，A^3优于四个现有方法，在保持最佳任务性能的同时，将“首token生成时间”缩短至原方法的一半。

Conclusion: A^3算法在降低算力和延迟成本的同时显著提升了长文本任务的模型表现，证明了其在实际应用中的潜力。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.

</details>


### [324] [LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models](https://arxiv.org/abs/2511.17561)
*Huimin Ren,Yan Liang,Baiqiao Su,Chaobo Sun,Hengtong Lu,Kaike Zhang,Chen Wei*

Main category: cs.CL

TL;DR: 本文提出了LexInstructEval基准与评测框架，用于客观、细致地评估大语言模型对复杂词汇指令的遵循能力，并开源相关数据集与工具。


<details>
  <summary>Details</summary>
Motivation: 目前评估大语言模型精确遵循复杂词汇指令的能力存在挑战，人工评估成本高且主观，自动化方法则存在偏差和不可靠性。现有自动基准也表达力不足，难以细致评测模型在复杂指令下的表现。

Method: 作者提出LexInstructEval框架，通过正式、基于规则的文法，将复杂指令拆解为<Procedure, Relation, Value>三元组，实现细粒度的基准测试。数据集通过多阶段、融合人工审查的流程自动生成，并用透明可编程引擎进行客观验证。

Result: LexInstructEval能够系统、客观地评估大语言模型的精细化指令遵循能力，并已发布相关数据集与源代码工具，支持社区进一步研究。

Conclusion: 该框架弥补了当前评测工具在表达力、效率与客观性方面的不足，可助力提升大模型可控性和实用性研究。

Abstract: The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.

</details>


### [325] [ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector](https://arxiv.org/abs/2511.17562)
*Wei Tian,YuhaoZhou*

Main category: cs.CL

TL;DR: 本文提出了ChineseErrorCorrector3-4B，一个基于Qwen3-4B的中文拼写与语法纠错统一模型，在多个权威数据集上纠错效果优异，整体性能居于业界前列。


<details>
  <summary>Details</summary>
Motivation: 随着中文文本快速增长，自动拼写与语法纠错需求日益增强。现有方法在准确性与通用性方面仍有提升空间，且很少有统一模型同时对拼写和语法错误做出高效纠正。

Method: 作者基于Qwen3-4B大模型，构建了ChineseErrorCorrector3-4B，并在多个中文拼写（CSC）和语法（CGC）纠错数据集上进行了统一训练和优化。模型采用端到端方式，实现拼写与语法错误的综合识别与修正。

Result: 在SIGHAN-2015、EC-LAW、MCSC和NaCGEC等权威数据集上，模型的F1和F0.5得分大幅超过现有公开模型，在拼写和语法纠错任务中均排名第一。

Conclusion: ChineseErrorCorrector3-4B实现了中文拼写与语法纠错的新突破，展现了统一模型在复杂文本纠错场景中的应用潜力和行业领先水平。

Abstract: This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.

</details>


### [326] [Generative Caching for Structurally Similar Prompts and Responses](https://arxiv.org/abs/2511.17565)
*Sarthak Chakraborty,Suman Nath,Xuchao Zhang,Chetan Bansal,Indranil Gupta*

Main category: cs.CL

TL;DR: 提出了一种支持结构相似提示缓存的生成式缓存方法，解决了结构性相似但内容略有不同提示的缓存效率与准确性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在实际应用中常有结构相似但内容不同的重复提示，现有的精确匹配或语义缓存方法在这类场景下要么缓存命中率低，要么返回错误结果。需要一种兼顾准确率和缓存效果的新方法。

Method: 作者提出了一种新的生成式缓存机制，能够识别结构相似的提示中的可复用响应模式，对新请求进行自适应、变化敏感的生成，从而支持对变化提示的高效缓存与召回。

Result: 该方法在缓存命中率上达到83%，并在没有重复提示的数据集上错误命中极少。在agentic workflow场景下，缓存命中率提升约20%，端到端执行延时降低约34%。

Conclusion: 结构变异感知的生成式缓存能够较大幅度提高多样化LLM应用中的缓存效率和交互性能，优于传统提示匹配缓存方式。

Abstract: Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.

</details>


### [327] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

TL;DR: 本文提出一种新方法，探究大型语言模型（LLMs）对社区观点的泛化能力，即使删除关键事件知识，模型是否仍能反映社区对不确定性的有机反应模式。实验表明，模型会保留特定社区的行为特征，展示了超越表面模仿的深层次行为偏好。


<details>
  <summary>Details</summary>
Motivation: 主要动机在于了解LLMs对特定网络社区对齐后，是否只是在机械地复现训练数据，还是能够泛化社区独特的行为和观点，尤其是在面对新不确定性时。该研究旨在揭示LLMs对社区对齐是否真正内化了社区的行为风格和偏见。

Method: 作者提出“认知立场迁移”实验框架，先有针对性地删除模型知识库中的特定事件信息（事件事实删除），并通过多种检测手段验证删除效果，最后观察模型在“不知道事实”的情形下，是否依然展现出社区固有的应对模式。研究对象为俄乌军事话语和美国党派Twitter数据。

Result: 实验证明，即便在广泛删除事实知识后，LLMs依然保持与对齐社区高度一致的不确定性应对行为。这种稳定且社区特定的行为模式说明了LLMs的对齐不仅仅流于表面模仿。

Conclusion: 该研究表明，LLMs通过对齐过程内化了结构化、可泛化的行为模式，而非仅仅机械复刻训练数据。这为识别和理解LLMs在无知情境下的行为偏见提供了系统化工具，助力更安全、透明的模型部署。

Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [328] [Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models](https://arxiv.org/abs/2511.17575)
*Vladimir Berman*

Main category: cs.CL

TL;DR: 本文提出了一个完全非语言学的文本模型，通过字母和空格的独立采样，系统性地推导出词长分布、词类型变化点和Zipf定律等现象，说明部分自然语言统计特征可仅由随机组合和分段产生。


<details>
  <summary>Details</summary>
Motivation: 探究自然语言中词频、词长等统计规律（如Zipf定律）是否需要复杂的语言机制，或仅凭简单的符号组合和分段也能产生类似模式，为理解和判别语言统计规律的成因提供基线模型。

Method: 构建一个由有限字母和空格组成的符号序列模型，在此基础上，利用概率和组合学工具推导出词长的几何分布、指定长度词的数量公式，找出词类型由高频变为低频的临界长度，并严格推导出基于模型参数的Zipf型词频分布。

Result: 得到词长几何分布的闭式表达、词数量和不同词类型数量的计算公式、决定词统计切分点的临界长度，以及用字母表大小和空格概率控制的Zipf类词频分布，统合出词表和频率结构间的数学联系。

Conclusion: 无需引入语言学优化或复杂结构，仅凭随机符号串联及简单分割，就可得出部分自然语言统计特征。该模型可视为词统计的零假设，有助于识别哪些模式需要更深层的解释，哪些可归因于基本统计结构。

Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.

</details>


### [329] [Computational frame analysis revisited: On LLMs for studying news coverage](https://arxiv.org/abs/2511.17746)
*Sharaj Kunjar,Alyssa Hasegawa Smith,Tyler R Mckenzie,Rushali Mohbe,Samuel V Scarpino,Brooke Foucault Welles*

Main category: cs.CL

TL;DR: 本研究系统性评估了生成型大型语言模型（如GPT、Claude）、传统袋装词模型、只编码Transformer模型，以及手动编码在新闻议题框架分析任务中的表现。结果显示生成型LLM虽有潜力，但整体落后于人工和部分小型模型，强调了方法互补和多元方法结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式LLM（如GPT、Claude）越来越多被用作内容分析工具，但其在新闻议题框架检测方面的有效性和限制尚未系统评价。本研究希望明确其性能边界及应用价值。

Method: 研究研发出一套新颖金标准数据集，聚焦2022年美国Mpox疫情半年新闻报道内容。用此数据集，实验对比了生成式LLM、传统NLP模型和人工编码器在议题框架检测任务上的表现，并考察这些工具在分析工作流不同环节的应用差异。

Result: 生成型LLM在某些应用场景表现有潜力，但在整体框架识别性能上，始终被人工编码员以及部分小型自然语言模型超越。人工评估和干预仍是不可或缺的，模型选择依赖具体任务特点。

Conclusion: 研究建议采用方法论多元的策略，将不同方法结合，实现优势互补。为研究者提供了一条多元化、实践友好的计算型框架分析路径图。

Abstract: Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.

</details>


### [330] [PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://arxiv.org/abs/2511.17808)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: 本文为葡萄牙语提出了迄今为止最全面的大语言模型（LLMs）评测，基于新构建的PoETa v2基准，评估了20余款模型在40多项任务上的能力，并对不同资源投入及专门适配对模型性能的影响进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在不同语言和文化场景下表现显著差异，特别是葡萄牙语缺乏系统性评测，难以指导模型优化。因此亟需针对葡萄牙语的严格、大规模评估体系。

Method: 作者构建了包含40余项葡萄牙语任务的PoETa v2评测基准，全面收集并测试20多款主流大语言模型，涵盖多种训练规模和算力资源。同时，将葡萄牙语任务表现与对应英语任务对比，以分析模型跨语种差异。

Result: 不同模型和算力投入下，在葡萄牙语任务表现出显著差异；专门为葡萄牙语适配过的模型性能优于通用模型。跨语言比较显示，葡萄牙语上仍存在一定性能差距。

Conclusion: PoETa v2为葡萄牙语大模型能力评测提供了系统化工具，有助于后续针对葡萄牙语的模型优化和研究，推动多语种NLP发展。

Abstract: Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.

</details>


### [331] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本研究提出了一个可以将Zoom会议录音转为带有说话人属性和行为标签的文本数据流程，并发布了三个地方政府多方会谈的数据集。通过使用这些“行为感知”数据集对大语言模型进行微调，可大幅提升模型模拟真实发言者和会谈过程的准确性和逼真度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在模拟多人会谈时，受限于缺乏带说话人标识的数据，无法准确还原发言者的行为和身份一致性。ASR自动生成的文本只用匿名标签，如Speaker_1，无法支持深入的发言者行为建模。提升会谈模拟的真实性和实用性，需要建立有说话人和行为属性的数据集。

Method: 作者开发了一个完整流程，将公开的Zoom录音自动处理成带有说话人身份、话语元数据（如人物画像、行为动作标签）的文本，并整理成用于科研的数据集。发布的数据涵盖上诉法庭、学区董事会、城市议会三类地方政府会谈。作者进一步基于这些数据对大语言模型进行以“发言人+行为”微调。

Result: 基于带说话人标签和行为标签的数据进行微调后，模型困惑度降低了67%，在说话人真实感和一致性的分类评估中，相关指标几乎提升了一倍。人类评判结果显示，微调后的模型模拟出的会谈过程在不少场合下与真实会谈难以区分。

Conclusion: 本研究提出的方法可有效提升大语言模型在模拟多方公共讨论时的真实性和参与者一致性，为大规模、复杂的现实主义式公民话语仿真提供了实用且可扩展的技术方案。

Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [332] [A superpersuasive autonomous policy debating system](https://arxiv.org/abs/2511.17854)
*Allen Roush,Devin Gonier,John Hines,Judah Goldfeder,Philippe Martin Wyder,Sanjay Basu,Ravid Shwartz Ziv*

Main category: cs.CL

TL;DR: DeepDebater是一个全新的自动化系统，能独立参与和赢得完整的两队制政策辩论，其多智能体架构在各个环节协同工作，并在多次评估中优于人类撰稿。所有代码和资料已开源。


<details>
  <summary>Details</summary>
Motivation: 人工智能在复杂、基于证据且需策略应变的说服能力上依然存在巨大挑战，现有系统如IBM Project Debater多局限于简化的辩论形式且面向大众。作者旨在突破AI在真实、复杂、高水平政策辩论中的应用瓶颈。

Method: DeepDebater采用层次化多智能体架构，多个由大语言模型驱动的智能体在检索、分析、合成和自我修正等环节分工协作，利用庞大的政策辩论证据库。系统还集成实时语音合成与动画展示，支持全自动、AI与人类混合作战多种模式。

Result: 在与人类案例的初步对抗评测中，DeepDebater在论证内容上普遍表现更优，常在AI裁判判决下获胜，人类专家教练也更偏好其输出的论点和证据。

Conclusion: DeepDebater展示了AI在复杂辩论环境中的优越表现，不仅提升了自动化辩论的水平，也为AI-human协作辩论形式提供了新工具。相关技术与资源已全部开源，推动该领域进一步研究与应用。

Abstract: The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main

</details>


### [333] [Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction](https://arxiv.org/abs/2511.17908)
*Debashish Chakraborty,Eugene Yang,Daniel Khashabi,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 本文提出利用保覆盖率的conformal prediction方法过滤RAG检索得到的内容，使大模型处理的上下文更加精炼、相关，验证其确实能有效提升事实准确率并减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法虽然通过检索增强了生成内容的事实性，但处理过长或噪音过多的上下文时准确率会下降。现有的过滤方法主要依赖启发式或未经校准的模型置信度，缺乏对保留证据的统计控制。作者希望找到一种可以兼顾相关性与置信度且有统计保证的过滤方法。

Method: 作者提出结合conformal prediction理论开发过滤框架，对RAG检索出的内容集进行覆盖率设定下的相关性过滤。实现上采用embedding和LLM两种评分机制，并在NeuCLIR和RAGTIME数据集上评估，重点考察相关内容保留比例和上下文压缩比。

Result: 实验表明，conformal filtering 总能达到预设的相关内容覆盖率目标，相比不过滤，能够将输入长度缩减2-3倍。在NeuCLIR数据集下启用严格过滤时，事实准确率显著提升，在适度覆盖率时准确率亦保持稳定，说明大多数被过滤内容为冗余或无关信息。

Conclusion: conformal prediction为RAG提供了一种可靠、模型无关且有理论保证的上下文工程方法，实现了有覆盖率控制的信息过滤，在保障事实依据的前提下，有效提升处理效率与准确率。

Abstract: Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

</details>


### [334] [L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention](https://arxiv.org/abs/2511.17910)
*Yuliang Zhan,Xinyu Tang,Han Wan,Jian Li,Ji-Rong Wen,Hao Sun*

Main category: cs.CL

TL;DR: 该论文提出L2V-CoT方法，无需训练地将大语言模型（LLMs）的推理能力迁移到视觉-语言模型（VLMs），显著提升了VLMs的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通过Chain-of-Thought（CoT）推理取得显著进步，但VLMs因多模态推理数据有限，在多步推理任务上表现不足。现有推理迁移方法通常代价高或需结构调整。因此，亟需一种更高效、更通用的迁移方式，促进VLMs的推理能力提升。

Method: 通过线性人工层析成像（LAT）实证发现，即使架构不同，LLMs和VLMs在CoT推理上仍共享相似的低频潜在表征。基于此，提出L2V-CoT方法：在频域中提取与重采LLMs的低频CoT表征，使其维度与VLMs匹配，并在推理时注入至VLMs的潜在空间，无需训练与架构调整。

Result: 大量实验表明，L2V-CoT方法在零训练的条件下持续优于其他无需训练的基线方法，甚至超过部分有监督迁移方法。

Conclusion: 本文提出的L2V-CoT展示了CoT推理能力可通过频域潜在表征迁移，高效提升视觉-语言模型在多步推理任务上的表现，并为多模态模型间推理迁移提供了新视角和低成本方案。

Abstract: Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.

</details>


### [335] [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)
*Wenda Li,Tongya Zheng,Shunyu Liu,Yu Wang,Kaixuan Chen,Hanyang Yuan,Bingde Hu,Zujie Ren,Mingli Song,Gang Chen*

Main category: cs.CL

TL;DR: 该论文提出了一个高效的异构图大模型感知（ELLA）框架来建模复杂的关系语义，同时显著降低了推理复杂度，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前异构图中的节点和关系类型多样，语义复杂。传统方法受限于预定义语义依赖和监督信号稀缺，且预训练与微调任务间存在语义断层。大量语言模型（LLMs）可提升异构图建模，但因计算复杂度高而难以直接应用。

Method: 该方法核心包括：1）提出LLM感知的关系分词器，用大模型预先编码多跳、多类型关系；2）引入跳级关系图变换器，将LMM推理复杂度从指数级降低到线性级；3）采用细粒度任务感知的Chain-of-Thought提示词，弥合预训练与微调间的差异。

Result: 在4个异构图数据集上，ELLA在性能和效率上均超过现有最优方法；尤其是在与现有LLM方法对比，最高加速可达4倍，并能扩展到参数规模13亿的大模型。

Conclusion: ELLA框架可高效捕获异构图的复杂关系语义，兼顾了性能与推理效率，显著推进了大模型在异构图领域的实际应用。

Abstract: Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.

</details>


### [336] [SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization](https://arxiv.org/abs/2511.17938)
*Jianghao Wu,Yasmeen George,Jin Ye,Yicheng Wu,Daniel F. Schmidt,Jianfei Cai*

Main category: cs.CL

TL;DR: 提出了一种新的测试时增强学习方法SPINE，通过只对推理分叉点的高熵token进行更新，显著改善了语言模型和多模态大模型在多个推理评测中的表现，避免了常见的输出长度塌缩和表现下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维推理的大模型在测试时面临分布偏移和缺少可验证监督，最近的无标签增强学习方法容易导致输出短、表现变差。作者分析原因在于普遍性序列更新策略未能关注到决定分支的高熵token。

Method: SPINE是一种选择性token的测试时增强学习框架，仅对前向传播统计中识别出的高熵分叉token进行更新，并在这些位置应用熵带正则化，以在熵太低时促进探索，在熵太高时抑制噪声。SPINE可集成进主流GRPO类目标，无需标签或奖励模型。

Result: 在10个涵盖多模态视觉问答、一般和专家问答、数学推理及医学问答的评测中，SPINE在LLM和MLLM骨干上整体提升了Pass@1，避免了常规方法中的输出长度塌缩，并带来了更稳定的训练结果。

Conclusion: SPINE通过将模型更新对齐至关键推理分支点，为推理模型提供了一种简单、无监督且高效的测试时自适应机制，有望广泛提升各类任务中的大模型推理能力。

Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.

</details>


### [337] [Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://arxiv.org/abs/2511.17946)
*Shuo Zhang,Fabrizio Gotti,Fengran Mo,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文探讨了预训练语料覆盖度在大语言模型幻觉检测中的作用，发现其与置信度信号结合可提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在开放域问答中存在幻觉（生成虚假内容）的问题，现有幻觉检测方法大多依赖模型内部信号，对预训练数据覆盖度与幻觉的关系研究较少，而长尾知识准确率下降现象说明预训练覆盖或许能检测幻觉。

Method: 作者通过构建可扩展的后缀数组，对RedPajama 1.3万亿token语料进行n-gram统计，分别统计问题和模型生成内容的词汇覆盖度，并在三个QA基准数据集上，结合对数概率等信号，评估其对幻觉检测的效果。

Result: 词汇出现频率特征单独用作幻觉检测时效果有限，但与对数概率等内部信号结合时，在模型存在更大不确定性的数据集上能带来一定提升。

Conclusion: 词汇覆盖度特征是一种有益的补充幻觉检测信号，可以与现有检测指标结合共同提升大语言模型幻觉检测性能。

Abstract: Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.

</details>


### [338] [MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok](https://arxiv.org/abs/2511.17955)
*Dat Thanh Nguyen,Nguyen Hung Lam,Anh Hoang-Thi Nguyen,Trong-Hop Do*

Main category: cs.CL

TL;DR: 本文提出了MTikGuard，一个用于抖音（TikTok）实时有害内容检测的多模态系统，并扩展了数据集，采用视觉、音频和文本特征融合，取得了先进的检测效果，并在大规模流媒体平台上实现了实时部署。


<details>
  <summary>Details</summary>
Motivation: 短视频平台如TikTok在青少年中极具影响力，但隐藏或隐蔽的有害内容对青少年的认知和行为带来风险。由于上传量巨大和内容实时性，传统内容审核难以应对。因此，亟需高效、准确且可扩展的有害内容自动检测方法。

Method: 提出了MTikGuard系统，包括：（1）扩展TikHarm数据集至4723条真实标注视频；（2）基于视频视觉、音频和文本特征的多模态融合分类架构；（3）基于Apache Kafka与Spark实现可扩展的实时流式架构，支持实时大规模内容检测。

Result: 多模态融合模型在TikHarm数据集上实现了89.37%的准确率和89.45%的F1值，显示出优于以往方法的检测效果。系统架构支持大规模实时应用，验证了其实用性与可扩展性。

Conclusion: 通过结合数据集扩展、先进的多模态模型与流式实时架构，MTikGuard系统可以高效、准确地检测TikTok中的有害内容，为大规模社交媒体内容管理提供了有效的解决方案。

Abstract: With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.

</details>


### [339] [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)
*Gowtham,Sai Rupesh,Sanjay Kumar,Saravanan,Venkata Chaithanya*

Main category: cs.CL

TL;DR: 本论文提出了Blu-WERP数据预处理流水线，用以提高LLM训练数据质量，并在多个基准和模型规模下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型训练数据预处理流程难以高效去除网络大规模数据中的噪声和非结构化内容，导致模型效果受限。为了解决这一问题，作者设计了新的数据预处理方法。

Method: 本文开发了Blu-WERP预处理流程，针对Common Crawl WARC文件，融合了先进的数据筛选及质量评估机制，并在不同规模（150M-1B参数）的模型及九项标准任务上进行了系统评测。

Result: 实验结果显示，Blu-WERP在所有模型规模和评价指标下均优于主流基线（如DCLM、Fineweb），在1B参数模型下整体提升4.0%/9.5%，且在相关类别上有2.4%-6.2%的显著提升，并实现了更高的质量-效率。

Conclusion: Blu-WERP成为当前数据预处理先进方法，大幅提升了LLM训练数据质量与下游模型性能，同时降低计算成本。数据预处理设计对LLM能力影响显著，该方法为数据优化提供了实用方案。

Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.

</details>


### [340] [GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set](https://arxiv.org/abs/2511.18146)
*Yomal De Mel,Nisansa de Silva*

Main category: cs.CL

TL;DR: 该研究提出了GeeSanBhava——一个高质量的僧伽罗语歌曲评论情感数据集，并建立了相应的情感分析模型。


<details>
  <summary>Details</summary>
Motivation: 现有僧伽罗语（Sinhala）的音乐评论情感数据集稀缺，亟需为自然语言处理及音乐情感识别领域提供高质量资源。

Method: 研究团队从YouTube手动采集并根据Russell情感二维模型由三位专家进行标注，验证标注一致性后，探索不同歌曲的评论情感分布。同时，研究选择多种机器学习和深度学习方法，以僧伽罗语新闻评论大数据集为预训练基础，对新采集的歌曲评论数据集进行零样本学习。通过超参数调优，最终得到一套三层（256,128,64单元）的MLP神经网络模型。

Result: 数据集的多位标注者间一致性达到Fleiss kappa 84.96%。优化的MLP模型在该数据集上ROC-AUC得分为0.887，展现较强的情感识别能力。

Conclusion: GeeSanBhava数据集为僧伽罗语自然语言处理及音乐情感识别研究提供了宝贵资源，对相关领域的未来研究具有推动作用。

Abstract: This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.

</details>


### [341] [Vector Arithmetic in Concept and Token Subspaces](https://arxiv.org/abs/2511.18162)
*Sheridan Feucht,Byron Wallace,David Bau*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在预测下一个词时，如何在模型激活空间中区分语义信息和表层词汇信息，并提出了通过不同类型注意力头定位这些信息子空间的方法。


<details>
  <summary>Details</summary>
Motivation: 理解和操控LLM中隐藏层如何分别表征词汇的语义信息和表层信息，有助于解释模型内部机制，改进下游任务的表现。

Method: 作者基于Llama-2-7b，利用前期工作识别出的两类注意力头：一类负责复制语义（concept induction head），一类复制词面表征（token induction head）。他们通过这些注意力头的权重变换隐藏状态，分别提取出更纯粹的语义或表层信息，并测试经典的词向量算术任务。

Result: 对比直接使用隐藏状态（47%）和经过语义头变换（80%）的最近邻准确率，证明注意力头变换极大提升了模型对语义操作的表达力。同时，表层信息通过token head变换也能被有效提取，实现“coding”-“code”+“dance”≈“dancing”等操作。

Conclusion: 本文发现并验证了不同类型注意力头可以区分并操作LLM中的语义与表层信息子空间，为模型可解释性和结构理解提供了新视角和工具。

Abstract: In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".

</details>


### [342] [Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models](https://arxiv.org/abs/2511.18177)
*Elias Lumer,Matt Melich,Olivia Zino,Elena Kim,Sara Dieter,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah,James A. Burke,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文系统比较了两类RAG架构（向量型和非向量型）在金融文档问答中的表现，并测试了多种增强技术对检索精度、答案质量、延迟和成本的影响。结果显示，先进的RAG技术显著提升了检索和回答效果。


<details>
  <summary>Details</summary>
Motivation: 当前RAG被广泛用来增强大模型对金融文档的问答能力，但不同架构及其改进方法在实际金融应用中表现如何仍不明确，也缺乏细致的对比。作者希望为金融Q&A系统选型和部署提供数据支持。

Method: 本文选取向量型agentic RAG（融合混合检索与元数据过滤）与层级节点式（基于文档结构，非嵌入）架构进行系统性对比。针对向量型架构，还分别测试cross-encoder重排序和small-to-big chunk检索两种增强手段。评测覆盖了1,200份SEC财报及150道问答基准，采用MRR、Recall@5、LLM判分、延迟及预处理成本等多项指标。

Result: 向量型RAG在答案质量上68%胜率优于层级结构，延迟相近。cross-encoder重排序在最佳参数下能让MRR@5提升59%。small-to-big检索对比标准块切割方式，检索效果提升65%，代价是0.2秒延迟。

Conclusion: 进阶RAG技术（如cross-encoder重排序与small-to-big检索）在金融文档Q&A中带来更高精度和答案质量，但需考虑成本与部署效率权衡，适用于实际生产场景。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.

</details>


### [343] [Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems](https://arxiv.org/abs/2511.18194)
*Faheem Nizar,Elias Lumer,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的Agent-as-a-Graph检索方法，通过知识图谱将多智能体系统中的代理与工具关系建模，并优化检索机制，在LiveMCPBenchmark基准测试上显著超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多智能体系统中，智能体及其附属工具的能力细粒度检索不足，导致智能体选择亚最优，难以充分利用工具和智能体的协作能力。

Method: 提出了Agent-as-a-Graph方法，将工具和父代理作为图中的节点和边建模。检索时，首先通过向量搜索找到相关节点，然后利用加权倒数排序融合(wRRF)重排序，最后通过知识图谱遍历获取最终代理集合。

Result: 在LiveMCPBenchmark上，该方法在Recall@5和nDCG@5指标上分别比现有最好系统提升了14.9%和14.6%，在wRRF优化上提升2.4%。

Conclusion: Agent-as-a-Graph能够更好地刻画和检索多智能体系统中细粒度的工具与智能体关系，为后续多智能体系统的高效调度和利用提供了有效方案。

Abstract: Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.

</details>


### [344] [From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation](https://arxiv.org/abs/2511.18259)
*Xiaochen Zheng,Alvaro Serra,Ilya Schneider Chernov,Maddalena Marchesi,Eunice Musvasva,Tatyana Y. Doktorova*

Main category: cs.CL

TL;DR: 本论文提出了DiscoVerse，一种多智能体系统，用于制药领域历史数据的语义检索和知识复用，在罗氏公司的海量历史制药数据上实现了高召回率和较高准确性，支持复杂决策和逆向知识转化。


<details>
  <summary>Details</summary>
Motivation: 制药研发中积累了大量异构历史数据，许多重要信息来自已终止的项目，若能对这些数据进行复用和逆向知识转化，将为新药研究提供巨大价值，但目前现实中对这些档案的复用存在较大障碍。

Method: 提出并实现了DiscoVerse多智能体共研平台，集成语义检索、跨文献关联和可审计综合功能。基于罗氏4十年历史的药物档案（包含0.87亿BPE tokens的180个分子），采用专家盲评对系统在真实世界规模下的输出进行广泛评估。

Result: DiscoVerse在七项基准查询中，对180个分子实现了接近完美的召回率（≥0.99），精确率为0.71-0.91；并能对药物终止原因和器官毒性等复杂问题，给出有据可查的综合答案。

Conclusion: DiscoVerse突破了制药数据复用的实践障碍，展示了大规模自动化、多智能体系统在药物研发知识提炼和逆向翻译中的可行性与实用价值，提升了数据复用效率，能为科学决策提供可靠支持。

Abstract: Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.

</details>


### [345] ["AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa](https://arxiv.org/abs/2511.18301)
*Harsh Rathva,Pruthwik Mishra,Shrikant Malviya*

Main category: cs.CL

TL;DR: 本文提出了一种数据驱动的方法，通过整合和均衡五个数据集，极大丰富了多语言科学文本幻觉检测的训练数据，使得在9种语言上，LLM生成文本的幻觉检测效果显著提升，在部分低资源语种表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多语言科学文本生成中常出现“幻觉”问题（即生成虚假内容），而相关检测任务由于训练数据匮乏且分布不均，尤其在低资源语言上难以可靠检测。作者为提升检测准确性，解决数据稀缺和不均的问题。

Method: 作者采用数据中心而非模型结构创新的策略，将五个现有数据集进行统一与均衡，最终形成包含124,821条样本（正负各半），大幅扩充原训练数据，并基于XLM-RoBERTa-Large模型进行微调以检测多语言下的幻觉文本。

Result: 该方法在所测9种语言中均表现出有竞争力的成绩，尤其在古吉拉特语（零样本设置）中获得第2名（Factuality F1 0.5107），其余8种语言亦取得第4-6名的成绩。

Conclusion: 系统性的数据整理和扩充策略可以显著提升多语言幻觉检测的表现，特别是在低资源语言或零样本场景下，优于依赖模型结构创新的做法。

Abstract: The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

</details>


### [346] [Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning](https://arxiv.org/abs/2511.18306)
*Mohammad Aqib,Mohd Hamza,Ying Hei Chui,Qipei Mei*

Main category: cs.CL

TL;DR: 本论文针对建筑规范中的表格数据提取难题，比较了直接输入图片和转换为LaTeX输入两种信息抽取方法，并通过LoRA微调VLM模型实现性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 建筑规范包含大量结构化表格信息，对于安全、合规和决策极为重要，但当前NLP和VLM很难准确处理复杂的表格布局及语义关系。提高对表格数据的自动问答能力，有助于提升工程效率、减少人为错误。

Method: 作者提出并比较了两种基于VLM模型的信息提取方法：1）直接法，将规范图片输入VLM直接问答；2）间接法，将图片转为LaTeX后输入再问答。随后，采用LoRA技术在领域表格数据集对各VLM进行微调。

Result: 实验表明，直接输入图片的方式准确率普遍优于间接法。经过LoRA微调后，所有模型问答表现显著提升，其中Qwen2.5-VL-3B-Instruct模型准确率提升超100%。

Conclusion: 论文证明参数高效微调（如LoRA）能极大增强VLM对复杂表格数据的理解和问答能力，展示了其在建筑规范等专业领域自动化合规解读中的广阔应用前景。

Abstract: Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.

</details>


### [347] [Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search](https://arxiv.org/abs/2511.18313)
*Joseph Oladokun*

Main category: cs.CL

TL;DR: 本文提出了路径约束检索（Path-Constrained Retrieval, PCR），提升大语言模型在知识库中检索信息时的结构一致性和推理连贯性。实验显示，PCR在结构一致性和相关性方面都优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型检索知识库内容时，常会获得与其推理状态结构不一致的信息，导致推理链条不连贯，因此需要提升检索时的信息结构一致性。

Method: 提出PCR方法，通过将结构化的图约束和语义检索结合，只在锚点节点可达的范围内进行检索，避免引入结构上无关的信息。

Result: 在包含6个领域的PathRAG-6基准上，PCR方法结构一致性100%，远高于基线的24-32%，同时能保持强相关性。PCR在技术领域rank-10下同样表现最佳，并将检索上下文的平均图距离降低了78%。

Conclusion: 路径约束检索能显著提升大语言模型推理系统的信息结构一致性与推理可靠性，是提升检索增强型LLM系统的有效方法。

Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.

</details>


### [348] [Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection](https://arxiv.org/abs/2511.18324)
*Syed Mohaiminul Hoque,Naimur Rahman,Md Sakhawat Hossain*

Main category: cs.CL

TL;DR: 本文针对BLP-2025的孟加拉语仇恨言论识别任务，提出了一种基于集成式微调的混合模型，分别用于仇恨类型和目标群体分类，方法在基准模型上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前孟加拉语的低资源背景下，亟需有效的方法应对仇恨言论及其类型、对象的精确识别，尤其是在社交媒体等复杂场景中。

Method: 作者提出了一种集成式微调策略，基于Bangla Language Model（孟加拉语预训练模型），融合多模型结果，并通过大量实验与其他语言模型变体进行了对比分析。

Result: 在BLP-2025任务1A中取得第6名（微平均F1得分73.23%），任务1B取得第3名（F1得分73.28%），整体表现超过基准模型。

Conclusion: 实验充分证明了混合集成方法在孟加拉语低资源仇恨言论识别任务中的有效性，且通过细致误判分析揭示了模型在实际应用中的局限与潜力。

Abstract: This paper introduces the approach of "Gradient Masters" for BLP-2025 Task 1: "Bangla Multitask Hate Speech Identification Shared Task". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.

</details>


### [349] [OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas](https://arxiv.org/abs/2511.18335)
*James Y. Huang,Wenxuan Zhou,Nan Xu,Fei Wang,Qin Liu,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: 本文提出OmniStruct基准，用于评估和促进大语言模型在各种文本到结构生成任务（包括信息抽取、表格生成、函数调用）上的能力，提出合成训练数据以实现高效小模型的无监督结构化输出。实验结果表明小模型能与GPT-4o相媲美。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言生成上表现优异，但其在严格结构化输出任务（如信息提取、表格生成、函数调用）上的能力尚不明晰。现有基准局限性阻碍了对模型结构化输出能力的全面评价和提升。

Method: 1. 构建OmniStruct基准：收集多类适合结构化输出的数据集并统一设为文本到结构任务。2. 通过合成任务自动生成高质量训练数据，无需人工标注。3. 在无监督数据下对小规模模型进行微调。4. 对比不同模型在该基准下的表现。

Result: 在没有使用OmniStruct监督数据的情况下，微调得到的小型模型在结构化输出任务中性能与GPT-4o接近，显示合成数据和统一基准的有效性。

Conclusion: 提出的OmniStruct基准和合成训练方法为低资源环境下结构化输出模型的开发提供了新路径，可推动通用、强大且高效的大语言模型结构化输出能力的发展。

Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.

</details>


### [350] [Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle](https://arxiv.org/abs/2511.18369)
*Manon Berriche*

Main category: cs.CL

TL;DR: 论文研究了在没有新闻规范的社交媒体环境下，假新闻传播有限及政治极化加剧的悖论。通过在Twitter和Facebook上的混合方法研究，发现假新闻的传播集中在少数高度政治化用户，这些用户设定议程影响同阵营；大多数用户对假新闻有批判性距离，但讨论常流于表面、并未促进真正的协商或多元对话。


<details>
  <summary>Details</summary>
Motivation: 尽管社交媒体缺乏新闻编辑和规范，但假新闻在用户获取、分享的信息中占很小比例；同时，政治极化却在加剧。论文旨在解释这一矛盾，并揭示用户对假新闻的实际态度和行为。

Method: 采用混合方法：一是在法国Twitter上定量分析分享过假新闻的用户特征；二是分析Facebook上的用户标记假新闻内容后的互动，包括线上观察与访谈，并结合用户社会人口属性分析不同互动场景下的行为。

Result: 1. 假新闻传播高度集中于少数政治化、批评体制的用户，这些人不一定受教育水平或认知能力低，但活跃度高，对同阵营议程有显著影响；2. 多数用户面对假新闻展现不同程度的批判性（如谨慎发言或直接纠正）；3. 这种批判性很少带来真正的协商讨论，常沦为少数活跃者间的“聋子对话”。

Conclusion: 社交媒体假新闻泛滥的印象被夸大，实际上多数用户对其保持距离，极化更多源自少数极端活跃、政治化的用户影响力溢出。批判性互动并未转化为有效的多元协商，反而加剧隔阂与极化。

Abstract: This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence énonciative) or interventions ('points d'arrêt') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.

</details>


### [351] [Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://arxiv.org/abs/2511.18393)
*Heejoon Koo*

Main category: cs.CL

TL;DR: 本文系统性研究了当前最先进的大语言模型（LLM）在面对临床文本降质（噪声、错误）情形下的诊断预测表现，并提出方法提升其鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI和LLM在临床辅助决策中表现突出，但临床文本普遍存在噪声（如人为错误或自动化流程失误），影响AI的可靠性和公平性，且不同人群受影响程度可能不同。目前相关影响研究不足。

Method: 对现有优秀LLM在多种文本降质环境下系统测试，对诊断标签空间过大问题提出临床驱动的标签精简方案及模拟医生分层推理的层级链式推理（CoT）策略。

Result: 提出的新方法提升了大语言模型在降质输入下的鲁棒性，显著降低了不同人口亚组间的不稳定性。

Conclusion: 本文工作推进了LLM在临床决策支持系统中可靠、公平应用的进程，所提方法在实际复杂输入情况下具有较好效果，并公开了相关代码。

Abstract: A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.

</details>


### [352] [Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models](https://arxiv.org/abs/2511.18409)
*Dana Arad,Yonatan Belinkov,Hanjie Chen,Najoung Kim,Hosein Mohebbi,Aaron Mueller,Gabriele Sarti,Martin Tutek*

Main category: cs.CL

TL;DR: 该论文介绍了面向语言模型可解释性的新基准MIB，并基于此举办了BlackboxNLP 2025可解释性方法对比任务，旨在推进可解释性方法的标准化评测和社区交流。


<details>
  <summary>Details</summary>
Motivation: 在理解和解释大型语言模型的内部机制过程中，如何量化和评测可解释性研究进展一直是难题。为此，社区需要标准化和可重复的评测框架。

Method: 该工作基于最新发表的MIB基准，组织了Shared Task，分别设有电路定位和因果变量定位两个赛道，吸引多支团队采用集成、正则化、低维投影等多种方法参赛，并对方法效果进行对比。

Result: 在电路定位项目中，团队通过集成与正则化方法取得了显著进展；在因果变量定位项目中，低维和非线性投影方法带来了性能提升。

Conclusion: MIB基准的开放榜单将推动可解释性领域的持续进步，社区鼓励更多研究基于统一标准不断提升模型可解释性。

Abstract: Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.

</details>


### [353] [SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data](https://arxiv.org/abs/2511.18411)
*Sultan Alrashed,Chadi Helwe,Francesco Orabona*

Main category: cs.CL

TL;DR: 本文介绍了SmolKalam数据集，这是一个高质量阿拉伯语多轮推理及工具调用翻译数据集，采用多模型集成翻译与质量过滤，并针对译后微调需求进行了方法研究。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语的高质量预训练数据已经有进展，但针对多轮对话、推理和工具调用的大规模阿拉伯语数据集依然稀缺。尤其是在模型微调阶段，对数据集质量要求更高，仅靠朴素翻译无法满足需求，因此需要更精细的数据集构建方法。

Method: 提出SmolKalam数据集，通过ensemble多模型联合翻译管道，结合质量过滤手段，并通过消融实验研究不同翻译技巧在decoder-only模型上的效果。

Result: 通过这些技术，获得了更高质量的阿拉伯语多轮对话数据集，并分析了不同翻译策略对数据集质量和后续模型表现的影响。

Conclusion: 高质量阿拉伯语多轮对话数据的构建需要复杂的多模型翻译和严格的质量控制，本文方法提升了数据质量，为后续阿拉伯语对话模型研究提供了数据基础。

Abstract: Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.

</details>


### [354] [Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations](https://arxiv.org/abs/2511.18413)
*Yu Xia,Sungchul Kim,Tong Yu,Ryan A. Rossi,Julian McAuely*

Main category: cs.CL

TL;DR: 本文提出了多智能体协同过滤（MACF）框架，将大语言模型（LLM）智能体用于推荐系统，并通过多智能体协同提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的推荐系统缺乏针对推荐任务的特殊设计，无法充分利用用户-物品交互中的协同信息，导致推荐表现不理想。

Method: MACF框架将传统的协同过滤思想与LLM多智能体协作结合。针对目标用户和查询，系统将相似用户和相关物品分别实例化为具有独特画像的LLM智能体，每个智能体可检索信息、推荐候选物品，并与其他智能体交互。与传统方法静态偏好聚合不同，MACF通过中央调度智能体动态地管理多智能体协作、招募智能体并进行个性化协同。

Result: 在三个不同领域的数据集上，与当前先进的智能体推荐基线相比，MACF框架在推荐效果上表现更优。

Conclusion: MACF框架有效结合了多智能体协同与LLM能力，能更好地利用协同信号，提升了智能体推荐系统的表现。

Abstract: Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.

</details>


### [355] [General Agentic Memory Via Deep Research](https://arxiv.org/abs/2511.18423)
*B. Y. Yan,Chaofan Li,Hongjin Qian,Shuqi Lu,Zheng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新型AI记忆框架GAM，通过类似即时编译（JIT）的机制，实现任务时上下文优化。GAM采用记忆器和研究者双组件设计，提升了大模型在多种记忆依赖任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的静态记忆体系会在预先建立可用记忆时大幅丢失信息，显著限制了AI的能力。为解决此问题，作者希望实现一种能够在任务执行时动态优化记忆上下文的新型记忆系统。

Method: 方法上，GAM框架分为两个核心部分：1）记忆器，利用轻量级记忆突出历史关键信息，但同时在底层完整存储全部历史；2）研究者，在需要时依据预先构建的轻量记忆，有针对性地检索和整合完整历史中的有用信息。此外，GAM可结合强化学习端到端优化性能。

Result: 实验表明，GAM在多种需要记忆支撑的任务中，性能明显超过传统记忆体系。

Conclusion: GAM框架能够更高效地发掘和利用大语言模型在记忆相关任务中的能力，实现更优的任务完成与动态扩展能力。

Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.

</details>


### [356] [MindEval: Benchmarking Language Models on Multi-turn Mental Health Support](https://arxiv.org/abs/2511.18491)
*José Pombal,Maya D'Eon,Nuno M. Guerreiro,Pedro Henrique Martins,António Farinhas,Ricardo Rei*

Main category: cs.CL

TL;DR: 本文提出了MindEval框架，用于更真实、全面地评估用于心理健康干预的AI聊天机器人系统。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康AI聊天机器人的需求增加，现有系统在真实治疗交流中存在奉承、过度认同及强化不良信念等局限。目前评测这类AI的基准缺乏，无法体现治疗对话的复杂性，阻碍了更优质系统的开发。

Method: 作者联合临床心理学专家开发了MindEval框架，通过患者行为模拟及自动化多轮对话评价，以量化和模拟真实心理咨询场景，并比对AI评测和人类专家评测的一致性。

Result: MindEval评测了12个主流大模型，结果显示所有模型平均得分不足6分中的4分，明显存在AI特有的表达弱点。推理能力和大模型规模未必能带来更好表现，且长对话及重症患者情景下表现更差。

Conclusion: 目前的AI心理健康对话系统距离真实、多变的心理干预需求还有很大差距。MindEval促进了更高标准和更切实际的模型评估，为未来更高质量的AI心理健康助手开发奠定基础。所有代码和评估数据均已开源。

Abstract: Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.

</details>


### [357] [For Those Who May Find Themselves on the Red Team](https://arxiv.org/abs/2511.18499)
*Tyler Shoemaker*

Main category: cs.CL

TL;DR: 本文主张文学学者应参与大语言模型（LLM）可解释性研究，尽管这会带来意识形态斗争，但这种参与是必要的。作者建议可以在'红队'这一场域进行探索。


<details>
  <summary>Details</summary>
Motivation: 当前主流的LLM可解释性方法强调实用性，但这种标准未能覆盖全部解释性的意义。文学学者具备独特的诠释学视角，有必要参与、扩展如何理解与解释LLM。

Method: 作者采用立场论文写作方式，倡议文学研究者介入LLM可解释性方向，并提出通过'红队'实践等具体方式参与讨论和研究。

Result: 论文并无新实验结果，而是提出了一种跨学科合作和争论的理论框架，为文学与AI研究搭建沟通桥梁。

Conclusion: 虽然介入LLM可解释性研究不可避免地涉及意识形态冲突，文学学者的参与仍然必不可少，有助于拓宽我们对AI解释的标准和理解方式。

Abstract: This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.

</details>


### [358] [Dealing with the Hard Facts of Low-Resource African NLP](https://arxiv.org/abs/2511.18557)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Panga Azazia Kamaté,Madani Amadou Tall,Emmanuel Élisé Koné,Aymane Dembélé,Michael Leventhal*

Main category: cs.CL

TL;DR: 本文介绍了如何针对低资源语言Bambara，收集、注释大规模自然语音数据，训练紧凑型语音模型，并进行了自动和人工评测，同时分享了相关资源。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在语音数据集、模型和评测体系方面严重缺乏，导致相关语音技术进展缓慢。作者希望通过实际案例为该领域的问题提供经验和解决方案。

Method: 作者实地采集了612小时Bambara语言的自然语音，采用半自动化方式进行转录注释。使用此数据集训练了多种超紧凑和小型单语语音模型，并结合机器与人工方式对模型输出进行评测。

Result: 构建出了高质量大规模Bambara语音数据集，开发了多种紧凑模型，并对其性能进行了全面评估。同时提出了数据采集、注释及模型设计的实际建议。

Conclusion: 为低资源语言语音研究提供了丰富的实证经验、详细的实践建议和开源数据资源，有助于进一步推动该领域的发展。

Abstract: Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.

</details>


### [359] [Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks](https://arxiv.org/abs/2511.18597)
*H. M. Shadman Tabib,Jaber Ahmed Deedar*

Main category: cs.CL

TL;DR: 论文对比了LLM（如GPT-4o）与基于特征的传统模型（LightGBM）在编程题难度判定上的表现，发现LLM远逊于传统方法，且存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言和代码生成上表现卓越，但其在结构化任务（如难度判定）上的能力尚未得到充分研究。了解LLM能否胜任此类任务对于其在教育、自动评分等领域的应用非常关键。

Method: 作者系统比较了GPT-4o（仅基于题目描述的自然语言分析）和LightGBM（利用数值与文本特征训练）的表现，分析手段包括准确率统计、混淆矩阵和SHAP解释。同时，通过生成合成难题来探查GPT-4o的判别机制。

Result: 在1825道LeetCode题目（带有难度标签）上，LightGBM模型准确率达86%，而GPT-4o仅为37.75%。进一步分析发现，模型是否考虑如输入规模、通过率等数值约束，是区分难易的关键。GPT-4o经常忽略这些要素，倾向于低估难度。此外，GPT-4o对自己生成的“难题”也普遍定为中等难度，暴露出其判别标准的不一致。

Conclusion: 当前LLM，尤其是GPT-4o，尚不能胜任编程题自动难度判定任务，存在显著失效模式。在真正应用于竞赛、教育和相关系统前，需针对这些缺陷进行改进。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.

</details>


### [360] [A Benchmark for Zero-Shot Belief Inference in Large Language Models](https://arxiv.org/abs/2511.18616)
*Joseph Malone,Rachith Aiyappa,Byunghwee Lee,Haewoon Kwak,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 研究提出了一个系统性的基准，用于评估大语言模型（LLMs）在零样本设置下预测个人多领域立场的能力，并揭示其泛化性能与局限。


<details>
  <summary>Details</summary>
Motivation: 尽管人类信念对推理、交流和社会联系非常重要，现有的计算方法大多局限于特定的社会政治情境，且依赖微调。LLMs广泛应用，但它们在不同信念领域中的泛化能力尚不清楚。

Method: 作者设计了一套可复现的基准，利用在线辩论平台数据，系统评估LLMs在零样本设置下预测个人在各种主题上的立场能力。基准设置了多种信息条件，以区分人口统计背景和已知信念对预测的影响，并在多种小到中型LLMs上进行测试。

Result: 结果发现，提供更多关于个体的背景信息能提高模型预测准确率，但在不同信念领域表现差别较大。

Conclusion: 当前LLMs在模拟人类推理方面既有能力也存在明显局限，该研究为机器行为研究和非社会政治信念建模提供了可扩展的框架。

Abstract: Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.

</details>


### [361] [A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News](https://arxiv.org/abs/2511.18618)
*Mirza Raquib,Munazer Montasir Akash,Tawhid Ahmed,Saydul Akbar Murad,Farida Siddiqi Prity,Mohammad Amzad Hossain,Asif Pervez Polok,Nick Rahimi*

Main category: cs.CL

TL;DR: 本研究提出了一种结合BERT-CNN-BiLSTM的混合迁移学习模型，用于孟加拉语新闻标题的分类与情感分析，并在一个首次同时用于标题与情感分类的新闻数据集上取得了最新的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 随着新闻信息量的激增，公众难以有效浏览并理解不同新闻媒体的内容和情感倾向，尤其是在低资源语言如孟加拉语中缺乏相关研究背景。因此，开发能够自动实现新闻内容和情感分类的模型，能够提升用户对新闻全貌与情绪倾向的快速感知。

Method: 作者提出了一种融合BERT、CNN和BiLSTM的混合迁移学习模型，并在BAN-ABSA的9014条孟加拉新闻标题数据集上进行实验。针对不平衡数据，设计了两种采样策略：1）在训练集划分前采样；2）在训练集划分后采样，并分别对模型性能进行比较。

Result: 在采样策略1中，过采样带来的头条与情感分类准确率分别为78.57%和73.43%；在策略2中，直接在原始不平衡数据上训练，头条和情感分类准确率分别为81.37%和64.46%。总体上，所提BERT-CNN-BiLSTM模型在所有分类任务上优于基线模型，达到了最新的最佳性能。

Conclusion: BERT-CNN-BiLSTM混合模型显著提升了孟加拉语新闻标题与情感分析的准确性，此研究为低资源孟加拉语文本分类提供了有力的基线和方向。

Abstract: In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\% and 73.43\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\% and 64.46\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.

</details>


### [362] [Prompt Optimization as a State-Space Search Problem](https://arxiv.org/abs/2511.18619)
*Maanas Taneja*

Main category: cs.CL

TL;DR: 本文提出将提示词优化视为状态空间搜索问题，并运用图结构和搜索算法系统探索提示词空间，提升自然语言处理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型对提示词的微小变化极为敏感，可能导致性能大幅下降，现有方法如DSpy虽有所缓解但有局限，因此需要新的、更系统的提示词优化方法。

Method: 将提示词空间建模为图结构，节点为不同提示状态，边为有意的提示转化（如简化、添加示例、调整顺序）。采用beam search和随机游走算法探索该空间，并在开发集上评估候选提示、剪枝低效路径。

Result: 在五项NLP任务中，哪怕使用浅层（宽度=2，深度=2）搜索也能显著提升开发集上性能（如推理任务答对率从0.40提升到0.80），但测试集提升较小（0.20到0.50），说明存在过拟合。常见的优化方式是让提示更简洁，而不是冗长。

Conclusion: 将提示词优化视为搜索问题是有效的，当前浅层搜索已提升表现，未来若有更强算力和更优评价指标，深度探索有望获得更具泛化性的提示词。

Abstract: Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].

</details>


### [363] [OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph](https://arxiv.org/abs/2511.18622)
*Michael J. Bommarito*

Main category: cs.CL

TL;DR: OpenGloss 是一个融合词典与百科知识的大型英文词汇与语义知识图谱，由多智能体 LLM 通过自动化流程低成本高效生成，数据量丰富，支持 NLP 与教育领域公开使用。


<details>
  <summary>Details</summary>
Motivation: 现有词汇知识库如 WordNet、Open English WordNet 等虽然经典，但定义数量有限，内容多为人工整理，扩展慢且成本高，且往往缺乏多维度的整合内容（如词源、用法、百科信息等），无法满足 NLP 及教育中日益增长的需求。

Method: 作者设计了一套多智能体的自动化生成流程，通过大模型输出并用结构化 schema 校验生成结果，配合自动质量控制机制，快速批量生成词条、释义、例句、搭配、语义关系及百科上下文，同时记录全部生成数据，保证结构一致和高自动化。

Result: OpenGloss 覆盖150K词条、537K义项，释义数量为 WordNet 系列的4倍，含 9.1M 语义关系、1M 例句、3M 搭配及6千万字百科内容，且全部生成仅用时一周、成本不到$1000。

Conclusion: OpenGloss 证明了结构化生成方法可低成本、高速地打造高质量词汇与语义知识库，为 NLP 与教育领域带来实用的新资源，也暴露并反映了大模型在语言知识生成上的能力与局限。数据开源，为后续研究与应用提供了丰富基底。

Abstract: We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.
  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.
  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.

</details>


### [364] [No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases](https://arxiv.org/abs/2511.18635)
*Shireen Chand,Faith Baca,Emilio Ferrara*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）去偏方法的跨类别副作用，发现针对某类偏见的消除常常会在其他类别上引发新的偏见或降低模型连贯性。


<details>
  <summary>Details</summary>
Motivation: 目前多数去偏技术只关注单一偏见维度，但忽视了去偏过程对其他维度的影响，有可能造成新的问题。作者希望系统调查这些去偏技术的副作用。

Method: 作者在10个模型（来自7个模型家族）上，应用了4种去偏技术，针对种族、宗教、职业及性别偏见进行实验，并利用StereoSet基准测试，量化去偏后模型的连贯性和刻板印象变化。

Result: 结果显示，去偏方法虽然在目标偏见维度有效，但通常会在其他未关注的维度上引入新的负面影响，比如模型连贯性下降或其他偏见增加。

Conclusion: 结论指出，去偏应采取多维度评估，单一维度的考量有可能掩盖甚至加剧整体偏见，需要开发更健壮、全面的评价工具和策略。

Abstract: Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.

</details>


### [365] [Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting](https://arxiv.org/abs/2511.18649)
*Goun Pyeon,Inbum Heo,Jeesu Jung,Taewook Hwang,Hyuk Namgoong,Hyein Seo,Yerim Han,Eunbin Kim,Hyeonseok Kang,Sangkeun Jung*

Main category: cs.CL

TL;DR: 本文通过2026年韩国高考数学试题，系统评估了24种主流大语言模型（LLMs）的数学推理能力，严格避免数据泄漏，全面分析模型在不同输入方式和提示语言下的表现。GPT-5 Codex获得唯一满分。


<details>
  <summary>Details</summary>
Motivation: 目前针对数学推理能力的评测数据普遍存在数据泄漏隐患，难以真实反映大语言模型的实际能力。为此，作者希望建立一个完全无污染的评测标准，以更准确评判各类LLMs的数学推理水平并揭示其局限与成本效益。

Method: 作者在2026年韩国高考数学考试宣布后2小时内，将全部46道题数字化，确保试题绝不可能被模型训练见过。借助真实考试环境，评估24种先进LLMs（含不同体量与结构），覆盖文本、图片、文本+图片输入方式，以及韩文与英文提示词。重点跟踪满分率、高难题表现、输入方式与推理深度对效率的影响。

Result: GPT-5 Codex以韩文文本输入获得唯一100分满分；Grok 4、GPT-5与Deepseek R1也表现优异（>95分）。小规模模型gpt-oss-20B性能突出，性价比高。针对几何题的表现普遍较弱（平均77.7%），高难度题明显拉低模型整体分数。文本输入优于图片输入，提示语语言的效果依模型规模有所不同。增加推理强度能极大提升分数，但显著增加消耗，影响实际应用效率。

Conclusion: 本研究首次实现了无数据污染的真实数学考试LLMs评估环境，并建立了兼顾性能、成本与效率的实用评测视角。结论表明，少量推理即可满足实际需求，且高效率的模型更具应用价值，对后续大模型数学能力评测与优化具重要参考价值。

Abstract: This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).
  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.
  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).

</details>


### [366] [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659)
*Jie He,Richard He Bai,Sinead Williamson,Jeff Z. Pan,Navdeep Jaitly,Yizhe Zhang*

Main category: cs.CL

TL;DR: 该论文提出了CLaRa框架，通过连续嵌入空间实现检索增强生成（RAG）的压缩与联合优化，显著提升了RAG在长文本和检索生成分离优化下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法（RAG）虽然能提升LLM的知识能力，但依然面临上下文过长和检索/生成分离优化的问题，急需更有效的联合建模和信息压缩方法。

Method: 作者提出了CLaRa，即连续潜在推理框架，它基于嵌入做信息压缩，并在连续空间中实现了检索和生成的联合优化。为获得有语义且可检索的压缩向量，还引入了SCP（重要信息保持的数据合成框架），并通过问答与同义句监督进行训练。最终，使用端到端的语言模型损失，通过可微分的top-k估算器优化重排器和生成器。

Result: 在多个问答基准测试中，CLaRa在信息压缩和重排器性能上达到或超越了当前最优的文本微调基线。

Conclusion: CLaRa能够在共享空间内有效联合优化检索与生成任务，提升了RAG架构在长文本和复杂推理任务的表现。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.

</details>


### [367] [Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models](https://arxiv.org/abs/2511.18696)
*Wangjiaxuan Xin*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段的同理心级联网络（ECN）框架，用于提升大语言模型的共情与包容对话能力，并在实验中证明其EQ测评分数优异。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在对话中往往缺乏情感共鸣与包容性，难以满足需情感理解的应用需求。因此，作者希望提升模型的同理心表达能力。

Method: 提出Empathetic Cascading Networks（ECN）框架，将对话响应过程分为四个阶段：视角采纳、情感共鸣、反思理解、整合综合，引导模型逐步生成具有情感共鸣和情境感知的回复。

Result: ECN在GPT-3.5-turbo和GPT-4上均取得了最高的同理心量表（EQ）分数，同时在Regard和困惑度（Perplexity）指标上也具备竞争力。

Conclusion: ECN能显著提高对话式AI的共情和包容能力，显示出其在需情感理解和包容性强的应用场景中的潜力。

Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.

</details>


### [368] [RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context](https://arxiv.org/abs/2511.18743)
*Yu Lei,Shuzheng Si,Wei Wang,Yifei Wu,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了RhinoInsight框架，通过引入“可验证清单模块”和“证据审核模块”提升大语言模型在深度研究任务中的准确性、可追溯性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前的深度研究系统多为线性流程，易受误差积累和上下文腐烂影响，且缺乏对模型行为和上下文的显式控制，导致输出不鲁棒、不够可追溯。

Method: RhinoInsight框架包括两个新机制：（1）可验证清单模块，将用户需求拆解为可追踪、可验证的子目标，并由人类或LLM批判员细化并生成分层大纲，避免不可执行的计划；（2）证据审核模块，对搜索内容结构化处理，不断更新并清理大纲上下文，引入批判员对证据进行排序、绑定，提升可验证性并减少幻觉。

Result: 实验表明RhinoInsight在深度研究任务中表现达到最新水平，在深度搜索任务中也有竞争力。

Conclusion: 通过引入控制机制，无需参数更新即可显著提升大语言模型在深度研究任务中的整体表现、鲁棒性和可追溯性。

Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

</details>


### [369] [Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search](https://arxiv.org/abs/2511.18749)
*Matthew R. DeVerna,Kai-Cheng Yang,Harry Yaojun Yan,Filippo Menczer*

Main category: cs.CL

TL;DR: 本论文系统评估了15个主流大型语言模型在事实核查任务中的表现，比较了常规模型、具推理能力模型和结合网页搜索模型，并对比了基于高质量人工摘要的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被广泛应用于事实核查、信息验证等领域，且越来越多用户将其作为事实查证工具，急需对其在现实公开数据上的验证能力进行系统性和严格的评估。

Method: 作者选取了OpenAI、Google、Meta、DeepSeek等公司最新的15个大型语言模型，并以PolitiFact的6000余条已核查声明为测试集，分别评估常规模型、带推理模型、集成网页搜索模型的表现，并与使用PolitiFact人工摘要和RAG（Retrieval-Augmented Generation）系统的结果进行对比。

Result: 实验证明：常规模型在事实核查任务上表现较差，单一推理能力提升有限，结合网页搜索虽有改善但收益不大。相比之下，使用高质量人工摘要结合RAG技术能够带来极大性能提升，F1分数平均提升233%。

Conclusion: 相比扩展推理或网页检索，赋予模型高质量、经过整理的专业上下文（如权威摘要）是实现自动化事实核查的更有前景的路径。

Abstract: Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.

</details>


### [370] [Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion](https://arxiv.org/abs/2511.18751)
*Daiqing Wu,Dongbao Yang,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种分布式特征恢复与融合（DRF）方法，以实现图文对情感分析的稳健性，能够有效应对低质量或缺失模态的问题。


<details>
  <summary>Details</summary>
Motivation: 在现实场景下，社交媒体中的图文数据常常存在模态低质量或缺失，导致现有情感分析方法性能下降。因此需要能稳健应对这些问题的多模态情感分析方法。

Method: 作者提出DRF方法，通过对每种模态维持特征队列，近似其特征分布，对于低质量模态，通过量化模态质量调整其在融合中的权重；对于缺失模态，通过样本与分布监督构建模态间映射关系，从可用模态恢复缺失模态特征。

Result: 在三个公开图文数据集上，采用两种破坏策略（模拟低质量及缺失）进行实验，相比SOTA方法，DRF在各种情况下表现出普遍提升。

Conclusion: DRF方法能有效提升多模态情感分析在面对低质量和缺失模态时的鲁棒性，对实际应用具有重要意义。

Abstract: As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.

</details>


### [371] [Context-Aware Whisper for Arabic ASR Under Linguistic Varieties](https://arxiv.org/abs/2511.18774)
*Bashar Talafha,Amin Abu Alhassan,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了多种面向语境的提示策略，无需重新训练即可提升低资源阿拉伯语ASR（自动语音识别）的性能，显著降低了单词错误率（WER）和幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 面对阿拉伯语语音识别，特别是在方言多样且标注数据有限的场景下，现有低资源ASR模型表现不佳，容易出现识别错误和模型幻觉。因此，作者希望探索不依赖复杂再训练的新方法，以提升ASR模型在不同阿拉伯语条件下的实用性和准确率。

Method: 作者提出了上下文感知的提示策略，包括：利用首次转录或检索到的语句进行解码器提示；对编码器引入用目标说话人声音合成的语音前缀。他们还提出了提示重排序、说话人感知的前缀合成，以及分词法（词汇、语义、声学）检索等技巧，用于改善零样本场景下的转录表现。

Result: 在九种阿拉伯语语言条件下评估，上述方法使现代标准阿拉伯语WER降低了22.3%，方言语音WER降低了9.2%，同时有效减少了幻觉和说话人不匹配的问题。

Conclusion: 所提上下文提示方法无需重新训练，即可实质提升低资源和多方言环境下的阿拉伯语ASR表现，为应对这类问题提供了高效的新途径。

Abstract: Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.

</details>


### [372] [HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations](https://arxiv.org/abs/2511.18808)
*Cao Linxiao,Wang Ruitao,Li Jindong,Zhou Zhipeng,Yang Menglin*

Main category: cs.CL

TL;DR: 本文提出了一种基于双几何空间的新型检索增强生成（RAG）方法——HyperbolicRAG，在各类文本问答任务中显著提升了结构推理与层级表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统RAG尽管能缓解LLM幻觉并增强知识应用，但基于欧氏嵌入的方法难以捕获知识图谱中的层级与抽象关系，仅能表达语义相似性，限制了复杂知识结构的利用。

Method: HyperbolicRAG融合了超球几何与欧氏空间，核心方法包括：1）深度感知的Poincare流形节点表示，将语义与层级同时纳入嵌入；2）无监督对比正则约束，保持不同抽象层级间的几何一致性；3）检索阶段引入欧氏与超球信号匹配的互斥排序融合机制，强化推理中的跨空间一致性。

Result: 在多个问答基准数据集上，HyperbolicRAG在准确性和层级结构推理能力上均优于常规RAG及图增强RAG等主流基线。

Conclusion: 将超球几何引入RAG可以更充分地结合层级和语义信息，为复杂知识结构的利用和推理提供有效技术支持。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.

</details>


### [373] [Concept than Document: Context Compression via AMR-based Conceptual Entropy](https://arxiv.org/abs/2511.18832)
*Kaize Shi,Xueyao Sun,Xiaohui Tao,Lin Li,Qika Lin,Guandong Xu*

Main category: cs.CL

TL;DR: 本文提出了一种通过AMR图实现无监督上下文压缩的新方法，有效提升了长文本处理中的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 在RAG等任务中，大模型读取长文本会受到信息冗余影响，推理准确率下降且计算开销上升，因此亟需一种能够高效过滤、压缩关键信息的方法。

Method: 方法基于AMR(抽象意义表达)图，对原始文本构建AMR图后，计算每个节点的概念熵，通过熵值筛选信息量大的节点，去除无关内容，仅保留核心语义，最终形成精简且信息集中的上下文。

Result: 在PopQA与EntityQuestions数据集上的实验结果显示，该方法比原始与现有基线方法表现更好，不仅提高了问答准确率，还大幅缩短了上下文长度。

Conclusion: 首次引入基于AMR的概念熵用于上下文压缩，展示了其在稳定语义特征与上下文工程中的潜力。

Abstract: Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.

</details>


### [374] [A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis](https://arxiv.org/abs/2511.18843)
*Heger Arfaoui,Mohammed Iheb Hergli,Beya Benzina,Slimane BenMiled*

Main category: cs.CL

TL;DR: 本文提出了一个基于神经主题建模（BERTopic）的高效、可复现的焦点小组讨论自动分析框架，系统解决了模型超参数敏感性、稳定性及结果可解释性等方法学难题，并提供了实际可操作的指南和开放源码，实现了高质量的人类验证和稳定性评估。


<details>
  <summary>Details</summary>
Motivation: 传统焦点小组定性分析依赖人工标注，既耗时又难以扩展，且缺乏可复现性。因此，亟需将自动化、可扩展的计算方法用于焦点小组文本分析，并探索和克服主题模型在此应用中的主要方法学挑战，如超参数敏感性、模型输出的稳定性、以及主题解释的合理性。

Method: 本文以突尼斯HPV疫苗认知为例，应用BERTopic对共10个焦点小组、1076句发言文本进行主题建模。系统评估了27种超参数配置、每种采用30次bootstrap重抽样以测试稳定性，并通过3位领域专家的人类评估验证了主题可解释性。提出以细粒度主题稳定性为基础，采用层级合并法平衡主题的稳定性和可解释性。

Result: 实验表明，主题模型对超参数非常敏感，不同评估指标的选择需与分析目标匹配。层级合并方法有效提升了主题的可解释性与稳定性，主题内聚度由直接提取的0.539提升至0.558。专家标注一致性良好（ICC=0.79，Cohen's kappa=0.578）。

Conclusion: 提出的自动化分析框架大幅提升了焦点小组定性分析的效率、可扩展性和复现性，为相关研究提供了详细实用的流程和方法建议。同时，全部代码和流程公开，便于后续复现和拓展。

Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.

</details>


### [375] [Large Language Models for the Summarization of Czech Documents: From History to the Present](https://arxiv.org/abs/2511.18848)
*Václav Tran,Jakub Šmíd,Ladislav Lenc,Jean-Pierre Salmon,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型（如Mistral和mT5）以及基于翻译的方法，提升捷克语文本摘要的效果，尤其关注历史文献的处理，并构建了新的历史捷克数据集。


<details>
  <summary>Details</summary>
Motivation: 目前捷克语摘要研究不足，尤其在历史文档领域。其主要原因是捷克语的复杂性和高质量标注数据集的缺乏。

Method: 使用多语言大型语言模型（Mistral和mT5）直接对捷克文摘要任务建模；还提出了先将捷克文翻译成英文，利用英文模型做摘要，再翻译回捷克文的流程。同时，创建了针对19世纪文献的全新历史捷克摘要数据集，并用现代LLM进行了基线实验。

Result: （1）LLM在现代捷克语摘要基准数据集SumeCzech上取得了新的最佳性能，证明了LLM在中等资源、形态复杂语言上的有效性；（2）发布了全新历史文档摘要数据集Posel od Čerchova，为后续研究提供基础。

Conclusion: 本研究首次系统性推动了捷克语摘要任务的发展，在现代与历史数据集上验证了先进模型的有效性，并为后续捷克语及其他低资源历史文献处理工作提供了重要资源和基线。

Abstract: Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.
  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.
  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.

</details>


### [376] [Cognitive Alpha Mining via LLM-Driven Code-Based Evolution](https://arxiv.org/abs/2511.18850)
*Fengyuan Liu,Huang Yi,Sichun Luo,Yuqi Wang,Yazheng Yang,Xinye Li,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 本文提出一种新的Alpha信号挖掘框架CogAlpha，结合大语言模型和进化算法，能在高维金融数据中有效发现更具预测力和稳健性的Alpha信号。实验证明其性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在高维、低信噪比的金融数据中发现有效的预测性交易信号（Alpha）一直非常困难。现有深度学习、遗传规划及大语言模型等方法所探索的Alpha空间有限，并存在可解释性差、泛化能力弱等问题。需要一种能兼顾创造性与逻辑性的、更高效的Alpha挖掘方法。

Method: 提出CogAlpha框架，将Alpha信号用代码表示，并借助大语言模型（LLM）作为认知代理，结合进化搜索，通过多阶段提示和金融反馈迭代改进、变异和重组信号。该框架强化LLM逻辑推理能力与结构化创新，极大拓展Alpha搜索空间。

Result: 在A股市场验证中，CogAlpha能稳定发现比传统深度学习、遗传规划等方法更具预测准确性、稳健性及泛化能力的Alpha信号。

Conclusion: 将进化优化与LLM推理结合，可实现更加自动化、可解释且效果领先于现有技术的Alpha信号探索。该框架为未来量化投资信号挖掘提供了新方向。

Abstract: Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.

</details>


### [377] [FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models](https://arxiv.org/abs/2511.18852)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 该论文提出了一种名为FanarGuard的双语内容审核过滤器，能够同时评估安全性和文化契合度，尤其关注阿拉伯语和英语场景，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的内容审核过滤器主要关注语言模型的一般安全问题，忽视了文化语境下的契合性，容易导致在不同文化背景下的审查不当或误伤，尤其是在阿拉伯语等文化脉络下表现不足。

Method: 作者构建了大型数据集（涵盖46.8万对提示与回复），由大模型评审打分安全性和文化认知；利用这些数据训练两个版本的过滤器。为评测文化契合性，还开发了首个针对阿拉伯语文化语境的基准测试集（1千多个涉及文化规范的问题，回复经真人标注）。

Result: FanarGuard与人工标注的一致性高于不同标注者之间的一致性，并且在通用安全性评测上与当前最优的过滤器持平。

Conclusion: 将文化认知整合进内容审核至关重要。FanarGuard为更具文化敏感度的内容安全防护提供了实用方案。

Abstract: Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.

</details>


### [378] [Generating Reading Comprehension Exercises with Large Language Models for Educational Applications](https://arxiv.org/abs/2511.18860)
*Xingyu Huang,Fei Jiang,Jianli Xiao*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的英语阅读理解习题自动生成框架RCEG，可生成高质量、个性化的练习题，并显著提升其相关性和认知适切性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育领域应用广泛，尤其是在自动文本生成上有很大潜力。为解决智能、适应性学习内容自动化生成问题，本文旨在提升英语阅读理解习题的生成质量和个性化。

Method: RCEG框架首先用微调后的LLM生成多个内容候选项，然后通过判别器（discriminator）筛选出最优候选，进一步提升生成内容的质量。为评估RCEG性能，作者构建了英语阅读理解专用数据集，并采用多维度指标（内容多样性、事实准确性、语言毒性、教学契合度）进行分析。

Result: 实验结果表明，RCEG框架显著提升了生成阅读理解习题的相关性和认知适切性。

Conclusion: RCEG有效提升了英语阅读习题自动生成的质量，尤其在相关性和适切性方面有突出表现，为基于LLM的教育内容生成提供了新工具。

Abstract: With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.

</details>


### [379] [Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models](https://arxiv.org/abs/2511.18864)
*Yang Xiang,Yixin Ji,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文是针对大型推理模型（LRM）剪枝的首次实证研究，提出采用具有挑战性的自生成推理数据进行校准，显著提升了剪枝后模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管剪枝已被证明能有效降低大模型的计算成本，但此前主要集中在语言模型（LLM）领域，而针对推理模型（LRM）的剪枝尚未被探索。此外，传统剪枝技术直接应用于LRM上效果不佳，因此作者希望寻找更有效的LRM剪枝方案。

Method: 作者首先实证分析了现有剪枝方法直接用于LRM的弊端，随后引入自生成推理数据用于校准，研究了数据难度和长度对剪枝效果的影响。基于发现，提出了有选择性的自生成推理数据构建策略（SSGR）。

Result: 实验在DeepSeek-R1-Distill系列模型上验证，所提出的SSGR策略提升了剪枝后LRM的推理能力，比通用方法提升了10%-13%。

Conclusion: 采用有挑战性的、适度长度的自生成推理数据进行校准，是提升LRM剪枝效果的关键。SSGR策略为高效推理模型的部署提供了新思路。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.

</details>


### [380] [CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation](https://arxiv.org/abs/2511.18889)
*Jingqian Zhao,Bingbing Wang,Geng Tu,Yice Zhang,Qianlong Wang,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出一种新的数据评测方法，针对大语言模型（LLM）评测中的数据污染问题，提高评测的公平性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测任务中，训练过程中可能无意间暴露了测试数据，导致数据污染，进而模型的表现被高估。已有方法修改或新建数据集，但效果有限，无法彻底清除污染或保持语义复杂度。

Method: 提出CoreEval方法：首先提取原始数据中的实体关系，利用GDELT数据库检索最新相关知识，对知识进行语境重构并整合进原始数据，保证数据语义统一和任务相关性。通过反复验证和细化标签机制，确保新旧数据一致性。

Result: 在大量更新后的数据集上进行实验证明，CoreEval能有效缓解因数据污染带来的模型性能高估问题，评测更加健壮。

Conclusion: CoreEval为数据污染问题提供了创新、有效的评测方案，有助于提升LLM评测结果的公平性和可信度。

Abstract: Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \textbf{CoreEval}, a \textbf{Co}ntamination-\textbf{re}silient \textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.

</details>


### [381] [Reproducibility Study of Large Language Model Bayesian Optimization](https://arxiv.org/abs/2511.18891)
*Adam Rychert,Gasper Spagnolo,Evgenii Posashkov*

Main category: cs.CL

TL;DR: 本研究复现了LLAMBO框架，在保持原有评测协议的情况下，将其文本编码模型由GPT-3.5替换为Llama 3.1 70B，并验证其主要结论依然成立。


<details>
  <summary>Details</summary>
Motivation: 评估LLAMBO框架是否具有通用性，并检验是否可以用开源大模型（Llama 3.1 70B）替换专有模型（GPT-3.5）而依然保持性能。

Method: 复现LLAMBO论文的关键实验（如Bayesmark和HPOBench），在不同大模型（Llama 3.1 70B、Gemma 27B、Llama 3.1 8B）下，比较其超参数优化、早期收敛和候选采样等表现，同时进行消融实验去评估文本上下文的作用。

Result: 用Llama 3.1 70B代替GPT-3.5仍能验证LLAMBO的核心优势。文本上下文增强了早期优化和结果稳定性，去除文本上下文会显著降低预测精度和校准。LLAMBO采样器生成的候选比TPE或随机采样更优质多样。较小模型则容易出现不稳定或错误预测。

Conclusion: LLAMBO框架对语言模型骨干网络的变更表现出鲁棒性，只要模型容量足够大（如Llama 3.1 70B），其方法依然有效。

Abstract: In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.
  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.
  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.

</details>


### [382] [Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs](https://arxiv.org/abs/2511.18931)
*Sahil Kale*

Main category: cs.CL

TL;DR: 该论文评估了大型语言模型集成网页搜索能力的有效性及其调用时机，但发现其在必要时调用搜索仍有不足，准确率和检索策略尚需提升。


<details>
  <summary>Details</summary>
Motivation: 尽管现代大语言模型可通过网页搜索提供实时答案，但目前尚不清楚模型是否能在真正需要时高效调用搜索功能，因此有必要量化和评估其检索决策和实际收益。

Method: 作者设计了一个包含静态和动态两部分的新基准测试集，静态部分为783个可用已有知识回答的问题，用于考察模型是否因低置信度而调用搜索；动态部分为288个知识截止时间后的新问题，用于分析模型能否识别需要检索并获取最新的信息，然后对主流商业模型（如GPT-5-mini和Claude Haiku 4.5）进行测试评估。

Result: 集成网页搜索可显著提升部分模型的静态准确率，但模型的信心校准性反而下降。在动态问题中，虽然两款模型都频繁发起检索，但准确率仍低于70%，主要受限于查询表达能力。无限制调用搜索带来的收益递减，首次检索失败后表现进一步下滑。

Conclusion: 集成网页搜索有助于提升模型事实性准确率，更适合作为低延迟的结果快速核查层，而非可靠的深度分析工具；模型检索策略还需优化，包括提升识别何时需检索和检索失败后的降级机制，有较大的改进空间。

Abstract: Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.

</details>


### [383] [Skeletons Matter: Dynamic Data Augmentation for Text-to-Query](https://arxiv.org/abs/2511.18934)
*Yuchen Ji,Bo Xu,Jie Shi,Jiaqing Liang,Deqing Yang,Yu Mao,Hai Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: 该论文提出了一种统一多种查询语言的Text-to-Query任务新范式，并通过骨架结构作为通用优化目标，实现高效泛化，取得了多项基准测试的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到查询语言的语义解析方法一般只针对单一查询语言，缺乏跨语言通用性。因此需要构建方法能够适应多查询语言场景。

Method: 作者提出将Text-to-Query任务统一建模，并首次引入“查询骨架”作为不同查询语言的共性结构。设计了动态数据增强框架，根据模型对骨架结构的薄弱环节合成针对性训练数据，从而提升泛化能力。

Result: 在4个公开Text-to-Query基准任务上，只用少量合成数据模型即取得了最新最优的结果，证明了方法的高效率和强泛化性。

Conclusion: 论文为Text-to-Query任务定义了统一范式，并验证了基于骨架的动态数据增强方法的有效性，为该领域后续统一研究提供了坚实基础。

Abstract: The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.

</details>


### [384] [Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials](https://arxiv.org/abs/2511.18937)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 本文提出了一种基于图形和知识的自动化方法，用于在临床试验中高效、直观地审查和解释治疗引发的不良事件（AEs）。通过增强MedDRA，利用隐藏的医学知识层（Safeterm）实现自动聚类和信号检测。


<details>
  <summary>Details</summary>
Motivation: 在临床试验中，不良事件的审查和解释常受术语繁杂、手动分组不一致、信号识别不明显等问题困扰。作者希望通过增加语义知识支持，提升AE数据的清晰性和信号检测的准确性。

Method: 方法上，作者通过为MedDRA添加一个可捕捉语义关系的Safeterm知识层，将AE术语在二维地图中重新自动聚类，并能量化这些术语与试验疾病的关联。基于ClinicalTrials.gov数据，计算各治疗组的收缩发生率比，并用精确加权汇总聚类水平的EBGM值，通过两个可视化图形展示信号——语义地图和预期性-异常性图。

Result: 在三个历史试验的应用中，该方法能够自动回溯并正确识别所有已知安全性信号，表现出较高的自动化和准确性。

Conclusion: 通过为MedDRA引入知识层，可有效提升AE解释的清晰度、效率和准确性，为临床试验不良事件信号检测和解读提供了有力的支持。

Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.

</details>


### [385] [Logic of Montage](https://arxiv.org/abs/2511.19063)
*Hayami Takahashi,Kensuke Takahashi*

Main category: cs.CL

TL;DR: 本文提出了一种独立于自然语言的情感表达形式，即“矛盾结构效应”，并通过“蒙太奇”操作、多重叠加等方式，构建了包含“强度（intensity）”与“结构效应”的理论框架，以补充和丰富情感的表达。


<details>
  <summary>Details</summary>
Motivation: 现有情感表达主要依赖自然语言，难以反映情绪复杂动态变化，作者希望探索和建立一种超越自然语言，能够更加细腻和动态地表达复杂情感状态的新表达形态。

Method: 作者提出“矛盾结构效应”这一表达形式，分析其动态属性及愉快/不愉快取向，将多个“效应”叠加（称为“蒙太奇”），并引入“强度（intensity）”作为模型参数。同时通过“系统（模型）间词汇迁移”框架，借助奥斯汀的“力”概念为“强度”引入正当性，最后用“升学”举例演示理论流程。

Result: 建立了理论模型，将‘矛盾结构效应’与‘结构效应’形式化归纳，并用‘升学’情境展示了该模型在实际情绪表达中的适用性。

Conclusion: 论文证明了独立于自然语言、引入强度参数的情感表达理论在建模人类情绪上有理论创新意义，丰富了表达情绪的方式，有助于更好地表达和解释复杂的情感状态。

Abstract: In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form "Effect of Contradictory Structure." "Effect of Contradictory Structure" is not static but dynamic. Effect in "Effect of Contradictory Structure" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, "Effect of Contradictory Structure" can be overlapped with each other. This overlapping operation is called "montage." A broader "Structure" that includes related "Effect of Contradictory Structure" and "Effect of Structure" are set up. Montage produces "Effect of Structure". In montage, it is necessary to set something like "strength," so we adopted Deleuze and Deleuze/Guattari's word "intensity" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of "intensity" through Austin's use of the word "force." "Effect of Structure" process is demonstrated using the example of proceeding to the next level of education.

</details>


### [386] [GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning](https://arxiv.org/abs/2511.19078)
*Yutong Li,Yitian Zhou,Xudong Wang,GuoChen,Caiyan Qin*

Main category: cs.CL

TL;DR: 提出了GraphMind，将GNN与LLM结合，用动态图机制提升多步推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型在多步推理时，缺乏对中间推理状态结构化和动态演化的显式机制，导致定理选择和结论生成的上下文感知能力有限。

Method: 设计了GraphMind框架，将推理过程表示成异构动态图，节点为条件、定理和结论，边表示逻辑依赖。利用GNN对推理状态编码，并结合语义匹配进行定理选择，与LLM闭环协作，动态生成中间结论。

Result: 在多个QA数据集上，GraphMind在多步推理任务中表现稳定，并且显著优于现有基线方法，验证了其有效性和泛化能力。

Conclusion: GraphMind能够实现可解释、结构化和上下文感知的推理过程，对多步推理场景有较强的应用前景。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.

</details>


### [387] [A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis](https://arxiv.org/abs/2511.19083)
*Wenxuan Mu,Jinzhong Ning,Di Zhao,Yijia Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个新的多智能体框架KDR-Agent，实现了低资源和多领域场景下的大语言模型命名实体识别效果的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型ICL（in-context learning）的命名实体识别方法，受限于对带注释示例的依赖、对新领域泛化性差以及难以引入外部知识和消歧义，这些问题在低资源场景下尤为突出。

Method: 作者提出了KDR-Agent框架，结合知识检索、消歧和反思分析三个模块。该方法通过中央规划者协调多个专家agent，利用维基百科检索实体知识、基于上下文推理进行消歧，并通过结构化自评纠正模型预测，减少对动态注释实例和大规模标注数据的依赖。

Result: 在五个领域的十个数据集上的实验表明，KDR-Agent在多种大语言模型（LLM）主干上，都显著优于现有的零样本和小样本ICL基线方法。

Conclusion: KDR-Agent可以有效结合外部知识和自我纠错机制，在低资源和多领域NER任务中提升大语言模型的性能，具有良好的实用价值和推广前景。

Abstract: In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.

</details>


### [388] [DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF](https://arxiv.org/abs/2511.19097)
*Ziyuan Gao,Di Liang,Xianjie Wu,Philippe Morel,Minlong Peng*

Main category: cs.CL

TL;DR: 本文提出了DeCoRL框架，实现了推理子任务的并行协作，从而大幅提升了推理速度、能效和结果解释性，同时保持高解题质量。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在Chain-of-Thought推理中，存在奖励信号颗粒度粗、难以诊断具体步骤错误，以及推理串行导致速度慢、实时性差等问题。

Method: 提出DeCoRL框架，将推理流程解耦成多个子任务，并通过轻量级专用模型并行生成推理子步骤。每步独立打分，采用级联式DRPO优化以协调这些分数，同时保持步骤间依赖关系。

Result: 在RM-Bench、RMB、RewardBench等基准上，DeCoRL推理速度提升3.8倍，能耗下降72.4%，吞吐量提升68%，可解释性提升22.7%，整体表现超越大规模模型。

Conclusion: DeCoRL显著解决了推理流程中的速度瓶颈和可解释性难题，为复杂推理系统的实时部署提供了高效、可控的落地方案。

Abstract: Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\% reduction in energy consumption and a 68\% increase in throughput, make real-time deployment of complex reasoning systems a reality.

</details>


### [389] [A symbolic Perl algorithm for the unification of Nahuatl word spellings](https://arxiv.org/abs/2511.19118)
*Juan-José Guzmán-Landa,Jesús Vázquez-Osorio,Juan-Manuel Torres-Moreno,Ligia Quintana Torres,Miguel Figueroa-Saavedra,Martha-Lorena Avendaño-Garrido,Graham Ranger,Patricia Velázquez-Morales,Gerardo Eugenio Sierra Martínez*

Main category: cs.CL

TL;DR: 本文提出了一种符号化模型，实现了纳瓦特语（Nawatl）文本的自动正字法统一。该模型基于正则表达式和前人算法，并通过人类评估验证了人工统一句子的质量。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特语存在多种不同的正字法（拼写体系），导致同一种语言的文本难以统一处理，影响语料库整理、语言技术开发等工作。作者希望通过自动化的方法，统一处理不同正字法的文本，以促进语言资源建设和相关研究。

Method: 提出了以符号正则表达式为基础的自动正字法统一算法，结合此前针对纳瓦特语句子的分析算法和$π$-yalli多正字法纳瓦特语语料库。并设计并实施了一套人工评估协议，在句子语义任务上检验了统一算法生成的句子的质量。

Result: 通过人工评估，大多数经过统一的纳瓦特语句子在预定特性上获得了积极的评价，说明算法在正字法统一和语义保留方面表现良好。

Conclusion: 符号化的自动正字法统一方法能高效地实现纳瓦特语多正字法文本的一体化处理，有助于纳瓦特语相关的语言技术发展和语料库建设。

Abstract: In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences

</details>


### [390] [On the Optimality of Discrete Object Naming: a Kinship Case Study](https://arxiv.org/abs/2511.19120)
*Phong Le,Mees Lindeman,Raquel G. Alhama*

Main category: cs.CL

TL;DR: 本文提出了一种信息论框架，用于分析自然语言命名系统在信息丰富性与复杂性之间的权衡，特别针对以往假设最优听众及普世交际需求的简化作出了改进。通过实验证明，在近亲关系等实际语义领域中，理论上的最优权衡可通过贝叶斯解码实现，并可在模型学习中自然出现。


<details>
  <summary>Details</summary>
Motivation: 以往关于自然语言命名系统的研究常依赖于信息论，但普遍假设所有听众皆为最优解码者，且各语言有共同的交际需求，这些前提与现实存在偏差。本文旨在提出更贴近日常语言使用实际的新理论框架，以准确描述命名系统的权衡机制。

Method: 本文基于信息论，建立了离散对象命名系统的数学框架。证明了只有当听者的解码方式等价于说者的贝叶斯解码时，命名系统才能达到信息量与复杂性的最优权衡。同时，采用语言涌现中的指称博弈实验，在亲属词语义领域对理论结果进行实证分析。

Result: 理论上证明了贝叶斯解码能够实现命名系统的信息与复杂性最优权衡，并通过模拟学习的通信系统验证了该最优解答在实际中可达，特别是在亲属语义范畴具备普适性。

Conclusion: 摒弃最优听者和普世需求假设后，语言命名系统在信息传播与复杂性之间仍能达成最佳平衡，且这一平衡在模型学习和实际语义场景中均具备实现可能，对理解自然语言符号系统的本质提供了新视角。

Abstract: The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.

</details>


### [391] [Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2511.19122)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 该论文提出了一种结合情感维度增强的多任务方面类别情感分析方法，不仅分析情感极性，还学习与方面类别相关的基本情感，实验证明效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在方面类别情感分析中主要关注情感极性，忽视了支撑情感表达的底层情感维度，导致难以捕捉细粒度的情感信号。为解决这一局限，作者希望将情感维度引入ACSA任务。

Method: 提出情感增强多任务学习框架，联合学习情感极性和基于Ekman六大基本情感的类别特定情感。利用大语言模型生成每个方面类别的情感描述，并通过基于VAD（效价-唤起-主导性）模型的情感精炼机制保证情感标注的准确性和一致性，将LLM预测的情感投射到VAD空间，对不一致的情感用结构化LLM策略重新精标。

Result: 在各基准数据集上的实验结果表明，所提方法在性能上显著优于强基线模型，证明了引入情感维度的有效性。

Conclusion: 整合情感维度能丰富ACSA任务中的情感表达，提升模型捕捉细粒度情感信号的能力，方法具有推广价值。

Abstract: Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.

</details>


### [392] [Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization](https://arxiv.org/abs/2511.19131)
*Zijian Wang,Yanxiang Ma,Chang Xu*

Main category: cs.CL

TL;DR: 本论文提出了一种新的基于概率条件生成的隐藏状态操控方法，以提升基础大语言模型的链式思维（CoT）推理能力，并在多个推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基础大语言模型虽然预训练于通用文本，但在复杂、多步推理任务上表现较弱，因为缺乏专门的推理训练。已有研究表明模型潜在地具有推理能力，存在于其隐藏状态中。但现有的隐藏状态操控方法刚性大、受限，容易引起分布漂移，导致文本质量降低。因此需要一种更有效并兼顾语言和推理能力的方法。

Method: 作者提出将 CoT 隐藏状态操控建模为一个平衡似然和先验正则的优化问题，通过概率条件生成方法引导模型隐藏状态向更具推理能力的方向发展，同时保持语言流畅与连贯。

Result: 通过在数学、常识和逻辑推理等多项基准测试上评估，该方法在提升推理能力和文本质量方面，表现出较以往线性激活引导等方法更为优越的效果。

Conclusion: 本文方法理论上更为严谨，能有效增强基础大语言模型的推理能力，为隐藏状态操控和推理能力提升提供了新的解决思路。

Abstract: Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.

</details>


### [393] [Representational Stability of Truth in Large Language Models](https://arxiv.org/abs/2511.19166)
*Samantha Dies,Courtney Maynard,Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）在处理真假信息时内部表示的稳定性，提出并检验了“表征稳定性”指标，发现模型对陌生知识的真假边界较为敏感，对熟悉虚构内容则更稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被广泛用于事实性任务，但其内部如何区分真假信息尚不清楚，尤其在面对未在训练数据中出现的信息或虚构情境时，真假判断的稳定性值得分析。

Method: 作者提出“表征稳定性”概念，通过训练线性探测器区分真与非真陈述，再有控制地改变标签，测量判别边界的变化。研究用16个开源模型、3个事实领域，并区分了“陌生的未见事实”（训练数据中没有的事实性宣称）和“熟悉的虚构内容”。

Result: 模型在面向陌生事实时真/假判断的边界变动最大，最高导致40%判断翻转；而面对熟悉的虚构内容判断更为稳定，边界变化较小（≤8.2%）。

Conclusion: 表征稳定性更多源于知识（语义）熟悉度，而非语言形式。该方法可用于诊断和训练LLM在语义不确定下保持一致真假判断，拓展了对模型内部语义处理机制的理解。

Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.

</details>


### [394] [In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations](https://arxiv.org/abs/2511.19232)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文分析了transformer模型（phi-2）在检测句子语义异常时的表征过程，发现模型中层对异常检测最为敏感，表征维度也随检测阶段变化。


<details>
  <summary>Details</summary>
Motivation: 理解transformer结构中的不同层如何以及何时对语义异常（句子语义脱轨）做出反应，以揭示其内部表征机制，并探讨其与人类阅读心理语言学处理过程的一致性。

Method: 1. 构建一个包含语义合理结尾和不合理结尾的句子语料库。
2. 用phi-2因果语言模型进行实验，逐层采样隐藏状态。
3. 使用线性探测器进行逐层异常检测，测试各层对语义异常的辨别能力。
4. 分析异常信息在不同层的表征子空间的有效维度变化。

Result: 线性探测器在模型最下三分之一层的区分能力较弱，中层准确率迅速上升，在接近顶层时达到峰值。维度分析显示，从“异常”开始，表征子空间扩展，但在模型中部瓶颈后急速收敛。

Conclusion: transformer内部对语义异常的感知主要集中在中层，与心理语言学观点一致——即语义异常的检测在句法分析之后，是较晚发生的过程。这揭示了深度语言模型在处理语义异常时与人类在线理解过程的某种对齐。

Abstract: How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.

</details>


### [395] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 本研究建立了一个包含5万多篇多领域孟加拉语文章与摘要的新型数据集，并用多种深度学习模型进行了实验，旨在提升孟加拉语文本摘要系统的实用性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数孟加拉语摘要研究仅聚焦于新闻领域，缺乏对更广泛真实文本的适应性。随着数字时代孟加拉语内容激增，需要更全面的数据资源和系统缓解信息过载，提高用户阅读效率。

Method: 收集来自博客和报纸等多来源的孟加拉语文章及其摘要，最终汇集超5.4万条多领域数据集，涵盖不同写作风格；并基于该数据集，利用LSTM、BanglaT5-small和MTS-small等深度学习和迁移学习模型进行了基线性能评估。

Result: 实验结果表明用该数据集训练的模型在摘要任务上具备良好表现，显示数据集作为孟加拉语自然语言处理基准的潜力。

Conclusion: 该多领域高质量数据集为孟加拉语摘要与NLP研究提供了坚实基础，有助于推动低资源语种的自然语言处理发展。

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


### [396] [Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333)
*Shaltiel Shmidman,Asher Fredman,Oleg Sudakov,Meriem Bendris*

Main category: cs.CL

TL;DR: 本文研究将大型语言模型在推理时生成的中间推理轨迹，用作中小规模语言模型的监督微调数据，对其数学问题表现的提升效果进行比较。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型能够通过推理轨迹提升推理和解题能力，这类推理轨迹也可被用于小型模型的训练，提升其推理能力，且节约人力标注成本。因此，有必要系统评估不同来源推理轨迹对中等模型的提升作用。

Method: 选用DeepSeek-R1和gpt-oss两种具有推理能力的前沿LLM，收集它们在解决数学问题时生成的推理轨迹，作为微调监督信号，分别对中等规模语言模型进行微调，并比较其解题准确率和推理效率。

Result: 实验对比了中等规模语言模型基于两类推理轨迹微调后的数学题准确率和推理效率，并得出不同来源推理数据对小模型能力提升的差异影响。

Conclusion: 通过对比分析不同推理轨迹数据的效果，为中小模型如何借助高质量自监督推理数据高效提升推理能力提供了实证参考。

Abstract: Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.

</details>


### [397] [DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research](https://arxiv.org/abs/2511.19399)
*Rulin Shao,Akari Asai,Shannon Zejiang Shen,Hamish Ivison,Varsha Kishore,Jingming Zhuo,Xinran Zhao,Molly Park,Samuel G. Finlayson,David Sontag,Tyler Murray,Sewon Min,Pradeep Dasigi,Luca Soldaini,Faeze Brahman,Wen-tau Yih,Tongshuang Wu,Luke Zettlemoyer,Yoon Kim,Hannaneh Hajishirzi,Pang Wei Koh*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方法RLER，并据此训练开发出首个专为开放性、长文本深度研究回答设计的开源模型DR Tulu-8B，其在多个基准测试中优于现有公开模型，并接近或超过专有系统，且体积更小、成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型多依赖易验证的短问答任务进行训练，无法很好地推广到要求推理和多步骤回答的真实长文本研究任务。因此，亟需针对这一需求提升研究型模型表现的方法。

Method: 作者提出了RLER（Reinforcement Learning with Evolving Rubrics）方法，在模型训练过程中动态维护与模型共同进化的评分标准（rubrics），使评分更能反映模型新学到的知识，并能给出更有区分力的反馈。基于此方法，作者训练了名为DR Tulu-8B的新模型，并建立了配套基础设施。

Result: DR Tulu-8B在科学、医疗保健及通用领域的4项长文本深度研究基准任务上，相较于现有开源模型表现出显著优越性，且在多项指标上可与甚至优于业界专有同类系统，同时模型规模和推理成本更低。

Conclusion: RLER为训练深度研究型大模型提供了更有效的方法，DR Tulu-8B在保持开源的同时实现了高性能，推动了研究社区在深度问答系统领域的进步，对未来的研究和应用具有重要价值。

Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.

</details>


### [398] [Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration](https://arxiv.org/abs/2511.19417)
*James Y. Huang,Sheng Zhang,Qianchu Liu,Guanghui Qin,Tinghui Zhu,Tristan Naumann,Muhao Chen,Hoifung Poon*

Main category: cs.CL

TL;DR: 该论文提出了一种模块化多智能体框架BeMyEyes，通过让小型视觉-语言模型(VLMs)作为感知者与大型语言模型(LLMs)作为推理者协作，实现高效的多模态推理，避免了对大规模多模态模型的训练需求。实验表明，该方法性能优于当前主流大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽强大但难以便捷地扩展至视觉等新模态；而小型VLMs虽高效但缺乏全面知识与推理能力。作者希望兼顾两者优点，有效实现多模态推理。

Method: 提出BeMyEyes方法，将高效VLMs与强大LLMs通过对话协作组建多智能体系统。设计了数据合成与有监督微调流程，训练感知智能体与推理智能体高效协作。

Result: 将普通的文本LLM DeepSeek-R1和Qwen2.5-VL-7B视觉感知器结合，超越了GPT-4o等大规模专有VLMs，在多种知识密集型多模态任务上表现优异。

Conclusion: 多智能体框架BeMyEyes有效实现了轻量、高适应性的多模态推理系统，具备开源、易扩展与强性能特性，对未来多模态系统构建具有重要意义。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [399] [AUTOSAR AP and ROS 2 Collaboration Framework](https://arxiv.org/abs/2511.17540)
*Ryudai Iwakami,Bo Peng,Hiroyuki Hanyu,Tasuku Ishigooka,Takuya Azumi*

Main category: cs.RO

TL;DR: 本文提出了一种协作框架，实现了AUTOSAR AP与ROS 2之间通过DDS和SOME/IP协议的无缝通信，解决了研究和开发平台间的差距，并验证了桥转换器的功能和性能。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶领域研究使用ROS 2居多，而实际开发多用AUTOSAR AP，二者间协议和平台的差异导致研究成果难以快速转化和落地。因此需要一种解决两者互通的方法。

Method: 提出了一个桥接框架，使得AUTOSAR AP和ROS 2能够通过DDS与SOME/IP协议通讯，并自动生成桥接配置文件，简化集成。用实验证明了桥转换器的性能和集成性。

Result: 实验证明，该桥接框架能够高效地进行协议转换，易于与ROS 2工具集成，并可自动生成所需配置文件。

Conclusion: 该协作框架成功实现了AUTOSAR AP与ROS 2间的互通，为研究与实际开发平台间的融合提供了技术基础，有助于推进自动驾驶科研成果的产业化。

Abstract: The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.

</details>


### [400] [Implicit Neural Field-Based Process Planning for Multi-Axis Manufacturing: Direct Control over Collision Avoidance and Toolpath Geometry](https://arxiv.org/abs/2511.17578)
*Neelotpal Dutta,Tianyu Zhang,Tao Liu,Yongxue Chen,Charlie C. L. Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于隐式神经场的多轴制造工艺规划框架，将层生成与刀具路径设计集成在同一可微分流程中，实现了更直接和有效的优化与防碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有多轴制造工艺规划方法在防碰撞方面仅间接处理，且在刀具路径生成上采用后处理方式，导致刀具路径在优化过程中难以受控。为解决这一问题，该研究希望设计一种联动的分层与刀具路径设计方法，提升工艺的安全性与质量。

Method: 作者利用正弦激活神经网络，将制造层与刀具路径均以隐式场的方式表达。该方法允许在任意空间点直接评估场值及其导数，使得碰撞检测与工艺层—刀具路径的联合优化得以实现。此外，作者还分析了网络超参数和目标定义对于奇异性行为和拓扑变换的影响，并提出了内建的正则化与稳定机制。

Result: 该方法在增材和减材制造的多个案例中进行了验证，结果显示提出的神经场方法兼具通用性和有效性，显著突出其实用价值。

Conclusion: 基于隐式神经场的工艺规划方法克服了传统方法在碰撞处理与刀具路径优化上的不足，实现了一体化、直接可控的工艺流程，对多轴制造领域具有较强理论创新与应用前景。

Abstract: Existing curved-layer-based process planning methods for multi-axis manufacturing address collisions only indirectly and generate toolpaths in a post-processing step, leaving toolpath geometry uncontrolled during optimization. We present an implicit neural field-based framework for multi-axis process planning that overcomes these limitations by embedding both layer generation and toolpath design within a single differentiable pipeline. Using sinusoidally activated neural networks to represent layers and toolpaths as implicit fields, our method enables direct evaluation of field values and derivatives at any spatial point, thereby allowing explicit collision avoidance and joint optimization of manufacturing layers and toolpaths. We further investigate how network hyperparameters and objective definitions influence singularity behavior and topology transitions, offering built-in mechanisms for regularization and stability control. The proposed approach is demonstrated on examples in both additive and subtractive manufacturing, validating its generality and effectiveness.

</details>


### [401] [Translating Cultural Choreography from Humanoid Forms to Robotic Arm](https://arxiv.org/abs/2511.17603)
*Chelsea-Xi Chen,Zhe Zhang,Aven-Le Zhou*

Main category: cs.RO

TL;DR: 该研究提出了一种结合符号化姿态转译与机构兼容记谱法的新方法，实现了机器人手臂文化语境下的语义复现，并证明了跨不同机构的通用性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人手臂编舞往往还原的是轨迹，而忽略了文化语义，导致无法真实复现人类文化动作的深层含义。为解决语义保真度与可移植性之间的难题，需开发新的编码与重现方法。

Method: 研究提出了ROPERA三阶段流程：首先将昆曲《牡丹亭》的姿态语料编码为文化记谱符号；随后将姿态序列进行符号化编排；最后将符号解码为关节执行命令，并结合视觉层如光绘和服饰色彩增强其表达。

Result: 结果表明，该方法能准确复现目标姿态序列，无论时序或文化意义均获得了专家与观众的认可，具备良好的再现性和文化可读性。

Conclusion: 研究证明了采用符号化记谱与兼容关节空间的转译方式，能有效实现非以人为中心的文化动作遗产保存，并为跨平台创作提供新的可行流程。未来将扩展至舞蹈动作连续、运动、触觉等多模态，并验证其在不同平台的移植性。

Abstract: Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.

</details>


### [402] [Robot joint characterisation and control using a magneto-optical rotary encoder](https://arxiv.org/abs/2511.17608)
*Yunlong Guo,John Canning,Zenon Chaczko,Gang-Ding Peng*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的磁光旋转编码器，用于测量机器人旋转关节的运动，实现了高精度和高速旋转检测。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人旋转编码器存在成本高、体积大或易受干扰等缺点，亟需一种更小巧、可靠、低成本的解决方案。

Method: 作者设计了一个采用磁场诱导的光学衰减结构，利用非均匀磁体围绕光环行器旋转，实现光信号的变化测量旋转角度，采用双通道反射配置提升灵敏度。

Result: 该系统可连续追踪360°旋转，扫描速度范围达到135°/s至370°/s，角分辨率为0.3°，整体性能优异。

Conclusion: 所提编码器在保持性能竞争力的同时，相比传统编码器具有更低成本与更高可靠性，适合机器人应用。

Abstract: A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360° rotation with rotation sweep rates from ν = 135 °/s to ν = 370 °/s, and an angular resolution of Δθ = 0.3°. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.

</details>


### [403] [Vision-Guided Optic Flow Navigation for Small Lunar Missions](https://arxiv.org/abs/2511.17720)
*Sean Cowan,Pietro Fanti,Leon B. S. Williams,Chit Hong Yam,Kaneyasu Asakuma,Yuichiro Nada,Dario Izzo*

Main category: cs.RO

TL;DR: 该论文提出了一种轻量级、基于CPU的自主导航方法，结合光流和激光测距用于月球着陆过程中的自运动估计，在合成月球影像验证下取得了优异的速度估算精度。


<details>
  <summary>Details</summary>
Motivation: 当前私人月球任务受限于航天器的质量、能耗和算力资源瓶颈，迫切需要无需高性能硬件、仍然可靠的自主导航方法，尤其是在月面复杂地形下实现精准降落。

Method: 作者将传统光流法与场景深度建模相结合，提出了一种运动场反演框架。使用金字塔Lucas-Kanade算法提取稀疏光流特征，通过测距仪参数化地形平面或球面模型，并采用最小二乘法对运动场进行反演，从而实现对航天器自运动的高效估计。该方法完全基于CPU实现，计算资源需求低。

Result: 在合成的月球南极复杂地形影像上测试时，该方法实现了接近1%的速度估算误差（典型地形），在极端复杂地形下误差低于10%，并且计算速度满足实时性要求。

Conclusion: 该工作证明了基于稀疏光流与深度建模的运动场反演方法有望为小型、低成本月球任务提供准确、可靠且高效的导航能力，促进其自主安全着陆。

Abstract: Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.

</details>


### [404] [LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation](https://arxiv.org/abs/2511.17765)
*Darren Chiu,Zhehui Huang,Ruohai Ge,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: 本文提出了一个名为LEARN的轻量级强化学习框架，能让多架纳米级无人机（Nano-UAV）高效安全地在复杂环境中自主导航，大幅降低了对高性能传感和计算的需求，同时超过了主流导航方案的表现。


<details>
  <summary>Details</summary>
Motivation: 纳米级无人机体积极小，机载传感、通信和算力极为有限，导致其在复杂环境中自主导航变得极具挑战。现有方法多依赖高分辨率视觉或大量计算资源，无法实际应用于这种平台。因此，急需一种低成本、高效且安全的智能导航方法。

Method: 作者提出LEARN框架，采用两阶段、以安全为导向的强化学习策略。方法集成了低分辨率ToF（飞行时间）传感器、简易运动规划器和紧凑的基于注意力机制的RL策略。系统设计旨在最大限度减少对资源消耗的依赖，并通过模拟和实际多机飞行实验进行验证。

Result: 在仿真实验中，LEARN比两种当前先进的导航规划器性能提升10%，资源占用却大幅降低。在实物验证中，6台Crazyflie四旋翼无人机可完全自主飞行，支持不同室内外环境，速度达2.0米/秒，并成功穿越0.2米窄缝。

Conclusion: LEARN框架实现了在资源极度受限的纳米无人机平台上高效、安全、可靠的多机自主导航。该工作大大推动了纳米级无人机在实际复杂场景中应用的可行性，为相关领域带来重要进展。

Abstract: Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.

</details>


### [405] [Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty](https://arxiv.org/abs/2511.17774)
*Salma Mozaffari,Daniel Ruan,William van den Bogert,Nima Fazeli,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: 该论文研究了扩散策略学习在建筑尺度下面向复杂接触装配任务中的表现和鲁棒性，尤其应对构件制造不确定性（如榫卯结构位置误差），证明其在含10mm扰动下仍能取得75%的成功率，显示出该方法提升不确定环境下机器人装配能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人装配任务在面对制造误差和材料缺陷时，表现出的精度和鲁棒性不足，尤其是在涉及大量接触的结构装配任务（如建筑榫卯结构）中，这类不确定性严重影响装配效率与安全。因此，亟需开发能适应制造偏差的智能装配方法。

Method: 本文以榫卯结构为对象，在机器人装配任务中采用扩散策略学习。研究分为两步：第一步评估决策策略的装配表现和适用性；第二步通过扰动榫眼位置（最大10mm）来评估在制造不确定性条件下的鲁棒性。

Result: 实验发现，在最大10mm的扰动情况下，所学习的最优扩散策略平均装配成功率为75%，未扰动时成功率达100%。该策略在模拟制造不确定性下表现出较强的泛化能力和鲁棒性。

Conclusion: 扩散策略学习方法能够提升机器人在不确定环境下进行复杂接触装配的能力，对建筑和制造领域具有广泛的可推广性，有助于提高建筑自动化的安全性与效率。

Abstract: Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.

</details>


### [406] [See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance](https://arxiv.org/abs/2511.17777)
*Ravi Prakash,Vincent Y. Wang,Arpit Mishra,Devi Yuliarti,Pei Zhong,Ryan P. McNabb,Patrick J. Codd,Leila J. Bridgeman*

Main category: cs.RO

TL;DR: 本文提出了一种OCT引导的智能机器人平台RATS，实现自主体积软组织切除，具备高精度和闭环反馈。


<details>
  <summary>Details</summary>
Motivation: 当前机器人激光手术系统缺乏体积规划和术中反馈，难以满足高精度软组织切除需求。

Method: RATS结合RGB-D成像、OCT、光纤耦合激光，并通过多阶段校准，实现高精度OCT-激光对准。基于激光与组织作用模型，并采用采样式MPC方法，直接在OCT体素上更新切除轨迹并实现闭环反馈。

Result: 平台在组织仿体和离体猪组织上，OCT与激光的配准误差为0.161mm，切除轨迹RMSE为0.842mm，交并比提升64.8%，且能识别并保护亚表层结构。

Conclusion: RATS平台实现高精度自主软组织切除，兼具术中反馈和对重要结构的保护，展现临床应用可行性。

Abstract: Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.

</details>


### [407] [SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs](https://arxiv.org/abs/2511.17781)
*Kristy Sakano,Jianyu An,Dinesh Manocha,Huan Xu*

Main category: cs.RO

TL;DR: 提出一种基于监管机构驱动的后验安全评估方法，用于学习型黑盒自主移动机器人，持续确保其满足不断变化的人为安全要求，并通过定量指标指导模型迭代优化。


<details>
  <summary>Details</summary>
Motivation: 传统自主移动机器人的安全性难以评估，且难以适应不断变化的人为安全标准。尤其对于黑盒模型，如何在其部署后持续保证对最新安全规范的合规性，是一个迫切需要解决的问题。

Method: 将人类安全需求通过监管机构转化为STL(信号时序逻辑)规范，通过外部工具对黑盒模型的运行轨迹进行合规性验证，得到总鲁棒值(TRV)和最大鲁棒值(LRV)两个定量安全指标。利用这些指标反馈指导模型设计人员定向微调和迭代优化模型。

Result: 该方法在虚拟驾驶场景和现实复杂环境中的自主机器人上测试，均取得显著改进：如虚拟驾驶场景中，合规轨迹数量提升177%-1138%；真实TurtleBot3实验中，进一步提升了带有安全缓冲的障碍物规避能力。

Conclusion: 提出的方法可有效提升学习型黑盒自主系统在不断变化人类安全规范下的合规性和安全性能，在仿真和真实环境验证均表现优异。

Abstract: We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.

</details>


### [408] [SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control](https://arxiv.org/abs/2511.17798)
*Francesco D'Orazio,Sepehr Samavi,Xintong Du,Siqi Zhou,Giuseppe Oriolo,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文提出了一种结合任务分层模型预测控制与人类动态交互预测的新方法，实现了移动操作机器人在动态人类环境下的安全高效任务执行。实验显示，该方法超越了传统加权法和开环人类模型，提升了人与机器人协作安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多任务优化控制方法在机器人操作任务中主要应用于静态或结构化场景，难以适应动态的人类环境，因缺乏对人与机器人动态交互的预测与建模。

Method: 提出了SM$^2$ITH框架，将分层任务模型预测控制（HTMPC）与交互式人类动作预测结合，通过双层优化方法同时建模机器人与人的动态行为，指导机器人在复杂动态任务中安全运行。

Result: 在两个不同类型的移动操作机器人（Stretch 3 和 Ridgeback-UR10）及三种实验场景下进行验证，包括任务递送、拾取放置以及对抗性人类行为等，所提方法在安全性和任务效率上均优于只用加权目标函数或简单人类模型的基线方法。

Conclusion: SM$^2$ITH能显著提升移动操作机器人在动态人类环境中的互动预测能力，实现了更安全且高效的人—机协作，展现了在现实复杂环境中的应用前景。

Abstract: Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.

</details>


### [409] [MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)
*Ting Huang,Dongjian Li,Rui Yang,Zeyu Zhang,Zida Yang,Hao Tang*

Main category: cs.RO

TL;DR: 提出了一种新的视觉-语言-动作框架MobileVLA-R1，将自然语言指令有效转换为四足机器人连续控制动作，实现了更强的推理和实地泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将自然语言高层语义有效对接至机器人底层动作控制，导致实地表现弱、泛化不佳。

Method: 构建了多粒度思维链（CoT）数据集MobileVLA-CoT，为推理与对齐提供监督。框架采用两阶段训练：以CoT进行监督对齐，结合基于GRPO的强化学习提升推理一致性、控制稳定性及长期任务能力。

Result: 在视觉语言导航（VLN）与视觉语言动作（VLA）任务上领先当前主流方法约5%，实现了更优的推理和控制表现。

Conclusion: MobileVLA-R1极大提升了机器人对自然语言理解到运动控制的稳定性与泛化能力，并在真实复杂环境下验证了系统的有效性和鲁棒性。

Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.

</details>


### [410] [L1 Sample Flow for Efficient Visuomotor Learning](https://arxiv.org/abs/2511.17898)
*Weixi Song,Zhetao Chen,Tao Xu,Xianchao Zeng,Xinyu Zhou,Lixin Yang,Donglin Wang,Cewu Lu,Yong-Lu Li*

Main category: cs.RO

TL;DR: 本文提出了一种结合去噪模型和L1回归优势的新方法（L1 Flow），在机器人操作领域实现了更快的训练和推理速度的同时，保留了捕捉多模态分布的能力。


<details>
  <summary>Details</summary>
Motivation: 去噪模型（如扩散模型与流匹配）在分布拟合和可扩展性方面表现突出，但训练和推理效率较低；而L1回归任务简单，效率高，但易丧失多模态分布捕捉能力。因此，作者希望兼得二者优点。

Method: 方法将v-prediction流匹配重构为采样预测，并采用L1损失优化。主要设计为L1 Flow，两步采样过程：先通过单步积分生成次优动作序列，再通过一次预测重构精确序列。该方法仅需两次神经网络评估，兼具流匹配多模态优势和L1训练高效性。

Result: 作者在MimicGen（8任务）、RoboMimic & PushT Bench（5任务）以及一个真实机器人任务上系统评测，L1 Flow在训练效率、推理速度与总体性能方面均优于多种基线方法。

Conclusion: L1 Flow有效融合了流匹配的分布拟合能力和L1回归的高效性，是高效且表现卓越的机器人操作策略生成方法。

Abstract: Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}

</details>


### [411] [Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game](https://arxiv.org/abs/2511.17925)
*Jeonghwan Kim,Wontaek Kim,Yidan Lu,Jin Cheng,Fatemeh Zargarbashi,Zicheng Zeng,Zekun Qi,Zhiyang Dou,Nitish Sontakke,Donghoon Baek,Sehoon Ha,Tianyu Li*

Main category: cs.RO

TL;DR: 本文提出了Switch-JustDance，一个利用任天堂Switch的舞蹈游戏Just Dance对机器人全身控制能力进行低成本和可复现性评测的新管道。作者分析了该平台的评测可靠性和有效性，并用它对三种最先进的类人机器人控制器进行了对比测试。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能够在真实世界、人类和机器人之间进行公平对比的标准化测试基准。现有方法多基于预采集的人体动作数据或仿真，难以考虑硬件因素、保证可复现性，也难做到公平对比。

Method: Switch-JustDance 利用Switch上的Just Dance游戏作为评测平台。其流程包括游戏动作转为机器人可执行动作（动作重构、动作重定向），再借助游戏内评分系统评估机器人控制器表现。作者评估了该方法的可靠性、有效性、灵敏性及潜在偏差，并基于该方案对三种顶级类人机器人控制器进行了硬件实测对比。

Result: 实验表明，Just Dance平台能够提供一致且易于解释的性能评估结果，可作为类人体态AI的基准测试工具。三种控制器的对比也揭示了各自优缺点。

Conclusion: Switch-JustDance为机器人全身控制提供了一种低成本、方便对比且可复现的新型测试标准，有助于推动机器人与人体动作能力的公平评测和研究。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.

</details>


### [412] [RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement](https://arxiv.org/abs/2511.17961)
*Hao Wang,Xiaobao Wei,Ying Li,Qingpo Wuwu,Dongli Wu,Jiajun Cao,Ming Lu,Wenzhao Zheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人臂数字资产建模方法RoboArmGS，通过贝塞尔曲线矫正URDF运动误差，实现了更真实的运动与更高质量渲染，并发布了新数据集RoboArm4D。


<details>
  <summary>Details</summary>
Motivation: 现有机器人臂数字资产建模方法普遍依赖URDF运动绑定静态3D高斯模型，难以应对实际运动的噪声和不确定性，导致渲染伪影，影响真实感。

Method: 提出混合表示法RoboArmGS，创新性地引入可学习的贝塞尔曲线对机器人每个关节实时修正，弥合URDF和真实动作之间残差，实现高质量的3D高斯绑定。开发并发布高质量RoboArm4D数据集用于评测和研究。

Result: 在RoboArm4D数据集上，RoboArmGS在真实运动建模和渲染质量方面达到了目前最新最优的效果。

Conclusion: RoboArmGS显著缓解了URDF理想化运动带来的渲染问题，实现了高质量、高真实感的机器人臂数字资产建模，为真实仿真一体化流程提供了实用工具，并推动相关领域发展。

Abstract: Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.

</details>


### [413] [Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation](https://arxiv.org/abs/2511.17992)
*Chungeng Tian,Fenghua He,Ning Hao*

Main category: cs.RO

TL;DR: 本文针对视觉-惯性导航系统（VINS）中的不一致性问题，提出了一种新的理论分析方法以及相应的高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前VINS估计器在实现过程中存在不一致性问题，影响了其精度和实用性。现有分析多为理论化简，仅考虑部分标准步骤，未涵盖实际中用到的如MSCKF校正、延迟初始化等非标准步骤，导致对不一致性成因认识有限，难以提出有效应对措施。

Method: 提出了不可观子空间演化（USE）分析框架，系统追踪整个估计流程中不可观子空间的变化，理解各估计步骤对不一致性的具体影响。在此基础上，提出不可观子空间对齐（USA）解决范式，并设计了基于变换和重评估的两种具体方法，对特定引起不一致的步骤进行有针对性的调整。

Result: 通过大量仿真和真实实验验证，所提出的USA方法在保持计算开销低的同时，有效提升了估计器的一致性和精度。

Conclusion: USE分析框架完善了VINS一致性理解，USA方法为不一致性问题提供了高效实用的解决思路，有助于推动VINS在实际中的更广泛应用。

Abstract: The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.

</details>


### [414] [Continually Evolving Skill Knowledge in Vision Language Action Model](https://arxiv.org/abs/2511.18085)
*Yuxuan Wu,Guangming Wang,Zhiheng Yang,Maoqing Yao,Brian Sheil,Hesheng Wang*

Main category: cs.RO

TL;DR: 该论文提出了Stellar VLA，一种知识驱动的持续学习框架，有效提升了机器人在开放环境下的技能持续学习能力，并在实验中显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型在多任务操作中表现强劲，但高度依赖于任务特定微调，缺乏持续学习能力。此外，现有持续学习方法在VLA模型扩展时资源消耗大。为解决这些挑战，作者开发了一种高效、可扩展的方法。

Method: 作者提出了Stellar VLA，包括T-Stellar（任务中心的知识空间建模）和TS-Stellar（捕获任务-技能分层结构）两种变体。核心方法是结合任务潜表示与知识空间的联合自监督学习，实现知识进化，并通过知识引导的专家路由，在不增加额外网络参数的前提下实现任务特化，从而降低训练开销。

Result: 在LIBERO基准和真实世界任务实验中，Stellar VLA相比基线平均提升超过50%的最终成功率。TS-Stellar在复杂动作推理任务上表现尤为优异，并通过消融分析验证了知识保持和发掘效果。

Conclusion: Stellar VLA实现了高效、资源友好的持续学习，在保持与发掘知识方面具有突出表现，为开放环境下泛化与持续学习机器人智能提供了新路径。

Abstract: Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead.Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.

</details>


### [415] [Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior](https://arxiv.org/abs/2511.18086)
*Miguel Lourenço,António Grilo*

Main category: cs.RO

TL;DR: 本文提出一种结合遗传算法、监督学习和强化学习的统一优化框架，提升无人机蜂群在抗干扰环境下的通信与任务效率。


<details>
  <summary>Details</summary>
Motivation: 无人机蜂群任务高度依赖无线通信，易受干扰（如干扰攻击）影响，影响任务协调和成功。因此，研究如何让蜂群在遭受干扰时保持通信和协同能力，对无人系统应用有重要意义。

Method: 提出以任务时隙与轮次为基础的调度模型，集成GA、SL、RL算法进行路径规划、天线定向及队形调整，并通过null-steering天线对准干扰源以提升抗干扰能力，采用PPO训练的RL以适应动态环境。提出自适应运动模型，支持任意方向的无人机运动。

Result: GA得到了稳定且避碰的航迹但计算复杂度高，SL能模仿GA但泛化性不足，RL则具备实时决策和较低计算开销，能维持通信与队形稳定。自适应运动模型证实了方案的可扩展性。

Conclusion: 搭配null-steering天线及智能优化算法的无人机蜂群在面对干扰时能有效维持通信、队形和安全，所提统一框架为蜂群通信抗干扰研究奠定了灵活且可复现实验基础。

Abstract: Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.
  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.
  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.
  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.

</details>


### [416] [A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots](https://arxiv.org/abs/2511.18088)
*Ibrahim Alsarraj,Yuhao Wang,Abdalla Swikir,Cesare Stefanini,Dezhen Song,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 该论文提出了一种多动力学统一建模框架，通过整合电机、卷线器和连续体本体的动力学模型，实现了对腱驱连续体机器人（如Spirob）的内部信号感知，用以取代对外部传感器的依赖，并成功实现了接触感知与物体尺寸估计。


<details>
  <summary>Details</summary>
Motivation: 腱驱连续体机器人因其结构柔顺和多余自由度，适合安全、复杂的接触任务，但传统依赖外部传感器，导致系统复杂、难以拓展。因此需要一种基于内部动力学信号实现环境感知的新方法。

Method: 作者提出并实现了一个多层级动力学建模框架，将电机电气动力学、卷线器动力学和本体动力学有机融合，对腱驱机器人的内部电机信号进行建模。通过模拟和实物实验，探究电机信号（如电流、角位移）承载的接触和物体信息。

Result: 实验表明，该模型能够准确捕捉和验证真实系统关键物理行为（如驱动滞后、自接触等），在无外部传感器条件下实现了被动和主动的接触检测以及物体尺寸估计，并且仿真学习策略可直接迁移到真实硬件。

Conclusion: 该框架为腱驱连续体机器人提供了基于内部动力学的环境感知新思路，不依赖外部传感器，降低系统复杂度、提升可拓展性，为感知和控制策略的物理实现提供了坚实基础。

Abstract: Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.

</details>


### [417] [EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation](https://arxiv.org/abs/2511.18112)
*Min Lin,Xiwen Liang,Bingqian Lin,Liu Jingzhi,Zijian Jiao,Kehan Li,Yuhan Ma,Yuecheng Liu,Shen Zhao,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.RO

TL;DR: 本文提出了EchoVLA，一种具有记忆意识的视觉-语言-动作（VLA）模型，用于长期移动操作任务。该模型通过模拟人脑的宣言性记忆机制，显著提升了机器人在复杂环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的VLA模型主要应用于短时、桌面范围的操作，缺乏解决长期、动态环境下移动操作任务所需的记忆与推理能力。作者希望解决这种能力短板，使装备VLA的机器人能够更好地处理实际的长期任务。

Method: 作者提出EchoVLA模型，在系统中引入类人脑宣言性记忆机制，包括场景记忆（记录空间-语义地图）和情节记忆（储存任务历史及多模态上下文）。在训练和推理阶段独立存储、更新和检索，并通过多粒度注意力融合用于移动臂扩散策略。此外，提出大规模基准MoMani自动生成长期操作轨迹，用于模型训练和评测。

Result: 在仿真和现实环境中，EchoVLA在长期操作任务上表现优异，操控/导航任务成功率达0.52，移动操作任务为0.31，均显著超过对比方法（分别提升0.08和0.11）。

Conclusion: EchoVLA通过融合类人记忆机制，有效提升了机器人在长期移动操作中的推理与记忆能力，实验结果验证了方法的有效性，可为实际机器人应用提供有力支撑。

Abstract: Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.

</details>


### [418] [Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting](https://arxiv.org/abs/2511.18140)
*Yilong Wang,Cheng Qian,Ruomeng Fan,Edward Johns*

Main category: cs.RO

TL;DR: 本文提出了Observer Actor (ObAct) 框架，通过主动视觉模仿学习，利用观察者机械臂动态选择最优视觉位置，有效提升了策略执行的鲁棒性，并在双机械臂系统上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统静态摄像头下的模仿学习在策略执行时易因物体或夹爪遮挡，导致观测质量下降，影响鲁棒性。本研究旨在通过引入主动移动摄像的观察者臂，提高视觉观测的有效性，从而提升学习到的操作策略的泛化能力。

Method: 提出ObAct框架：在双机械臂机器人系统中，某一臂作为观察者，利用腕部摄像头采集图像，并构建3D Gaussian Splatting表达，虚拟探索寻找最优视角后，物理移动至该位置；另一臂（执行者）则利用该最优视角下的观测来执行策略。该方法可与多种模仿学习算法结合（如轨迹迁移、行为克隆）。

Result: 在有无遮挡两种场景，通过轨迹迁移和行为克隆两种模仿学习方法，ObAct框架均大幅提升了性能：轨迹迁移方法分别提升145%和233%，行为克隆方法分别提升75%和143%，相比于静态摄像头方案有显著优势。

Conclusion: ObAct主动视觉框架有效缓解了因遮挡导致的观测分布变化问题，保持了训练与测试分布的一致性，提升了策略的鲁棒性和泛化能力。该方案为高效模仿学习及机器人操作提供了新思路。

Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.

</details>


### [419] [A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies](https://arxiv.org/abs/2511.18153)
*Shreyas Kumar,Barat S,Debojit Das,Yug Desai,Siddhi Jain,Rajesh Kumar,Harish J. Palanthandalam-Madapusi*

Main category: cs.RO

TL;DR: 本文提出了一个轻量级神经网络SnapNet和基于动态系统的双臂协调框架，实现对精密卡扣装配过程的实时检测和力控制。核心创新点在于利用关节速度瞬变信号（内部传感，非外部传感），显著提升装配精度并减少组件损伤风险。


<details>
  <summary>Details</summary>
Motivation: 精密卡扣装配如眼镜片安装或电子产品装配时，因过度用力或检测不及时易导致组件损坏或装配失败，急需高效精准的装配状态检测与力控制方法。

Method: 1. 提出SnapNet神经网络，实时分析关节速度瞬变来检测装配卡扣的完成，仅依赖本体感受信号，无需外部传感器。2. 设计了基于动态系统的双臂协作框架，将SnapNet事件与阻抗控制策略协同，实现准确对准与柔顺插入。

Result: 在多种卡扣几何结构及双臂异构平台上实验，SnapNet检测精度高（召回率超96%），并比传统阻抗控制方法将装配过程中峰值冲击力降低约30%。

Conclusion: 新方法显著提升了精密卡扣装配的安全性与成功率，无需外部传感器即能实现高效检测和力控制，为自动化复杂装配任务提供了通用且便捷的解决方案。

Abstract: Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.

</details>


### [420] [Time-aware Motion Planning in Dynamic Environments with Conformal Prediction](https://arxiv.org/abs/2511.18170)
*Kaier Liang,Licheng Luo,Yixuan Wang,Mingyu Cai,Cristian Ioan Vasile*

Main category: cs.RO

TL;DR: 本文提出了两种结合共形预测（CP）的方法，实现了在动态环境下具分布无关安全保证的运动规划，并通过自适应量化机制进一步提升轨迹可行性。


<details>
  <summary>Details</summary>
Motivation: 动态环境下的安全导航依然具有挑战性，主要因为障碍物行为存在不确定性且缺乏正式的预测保证。现有方法难以兼顾长期安全与实时响应。

Method: 作者提出了两套基于共形预测的新运动规划框架：1) 基于安全区间路径规划（SIPP）的全局规划器，实现了不依赖分布的轨迹安全性保证；2) 本地规划器采用自适应CP进行在线反应规划，应对障碍物轨迹预测的不准。此外引入自适应量化机制，根据环境不确定性自动调整置信区间，优化轨迹的可行性和安全裕度。

Result: 通过数值实验验证，所提框架能够在动态且复杂的环境中实现安全、健壮的运动规划，并能自适应调整安全边界，提升轨迹可行性。

Conclusion: 文章证明了结合CP和自适应量化机制能够有效提升动态环境中运动规划的安全性和鲁棒性，为不确定环境下的导航提供了分布无关的理论保证。

Abstract: Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io

</details>


### [421] [Off-Road Navigation via Implicit Neural Representation of Terrain Traversability](https://arxiv.org/abs/2511.18183)
*Yixuan Jia,Qingyuan Li,Jonathan P. How*

Main category: cs.RO

TL;DR: 本文提出了一种新的越野自主导航方法——TRAIL（Traversability with an Implicit Learned Representation），利用隐式神经网络对地形可通行性进行建模，结合新颖的基于梯度的轨迹优化方法，实现对路径几何形状和速度的动态调整，以提高越野环境下的运动表现。


<details>
  <summary>Details</summary>
Motivation: 现有越野导航方法大多采用采样式规划器（如MPPI），这些方法虽然响应快，但仅能在短时视野内优化，对路径整体几何的考虑有限，且无法针对复杂地形动态调整速度，不利于应对多变的越野道路环境。

Method: 本文提出TRAIL框架，利用隐式神经表征连续参数化地形可通性，获得空间梯度信息，并将该表征与新颖的基于梯度的轨迹优化方法结合，实现路径几何和速度轮廓的自适应优化。

Result: 该方法可根据地形难易和崎岖状况调整路径和速度，有效提高了越野环境下导航的平滑性和安全性。

Conclusion: TRAIL框架通过深度学习与优化的结合，突破了传统采样式规划的局限，在复杂越野环境下实现了更高效、更平滑的自主导航。

Abstract: Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.

</details>


### [422] [SkillWrapper: Generative Predicate Invention for Skill Abstraction](https://arxiv.org/abs/2511.18203)
*Ziyi Yang,Benned Hedegaard,Ahmed Jaafar,Yichen Wei,Skye Thompson,Shreyas S. Raman,Haotian Fu,Stefanie Tellex,George Konidaris,David Paulius,Naman Shah*

Main category: cs.RO

TL;DR: 本文提出了一种基于生成谓词发明的技能抽象方法SkillWrapper，可用仅含RGB图像输入、无需低层状态知识，实现机器人黑盒技能在新任务中的抽象表达与规划应用。该方法利用大模型主动收集数据，并学习可供人类理解及规划的高层符号表示，实验验证其能在真实环境中解决长远未知任务。


<details>
  <summary>Details</summary>
Motivation: 长时序任务的自动规划要求智能体能够将低层技能泛化为高层抽象，现有方法虽能抽象出技能谓词，但缺乏对抽象表示所需形式性质的理论解释，且如何学习可保证规划完整性和可靠性的抽象表示仍不明确。解决这些基础理论与方法难题，是实现机器人自主解决复合长时任务的关键。

Method: 作者提出SkillWrapper框架，基于大模型生成与主动数据采集，挖掘并学习可人类解释的技能谓词与符号操作符。该框架给出了生成谓词发明的形式理论，保证抽象出来的符号操作符可用于可证明的完备且可靠的规划。SkillWrapper仅依赖RGB图像观察数据，在仿真及真实机器人平台上进行端到端训练与评估。

Result: 实验结果显示，SkillWrapper能够从图像观测中抽象出对应技能的高层符号结构，并利用这些抽象在长时任务规划中优于传统方法，能成功解决仿真和现实环境下未见过的新任务。

Conclusion: 文章证明了基于生成谓词发明和大模型的技能抽象不仅形式上具备规划的充分性和健壮性，还在实际数据和真实机器人中展现了泛化与可用性，为自主智能体的复杂任务规划与执行提供了理论与实践基础。

Abstract: Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.

</details>


### [423] [AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots](https://arxiv.org/abs/2511.18215)
*Shangyuan Yuan,Preston Fairchild,Yu Mei,Xinyu Zhou,Xiaobo Tan*

Main category: cs.RO

TL;DR: 本文提出了一种无需标记物、无需训练数据的视觉方法，通过利用软体机器人的自然表面特征实现形状重建，能够实时、高精度地进行软体机器人形状追踪，且适应性强、部署成本低。


<details>
  <summary>Details</summary>
Motivation: 目前视觉方法在软体机器人形状重建中虽然具有成本低、部署简单等优势，但普遍依赖复杂的相机系统、特定背景或大规模训练数据，限制了其在实际场景中的应用。作者旨在突破这些局限，提升形状重建方法的实用性和可扩展性。

Method: 提出基于自然表面特征的视觉重建框架，不依赖标记点或任何训练步骤。方法核心为：利用机器人表面的细节作为隐形视觉标记，通过分层匹配策略，将局部分区对齐与全局运动学优化解耦。只需初步3D建模和运动学对齐后，即可在多个环境中实时、稳健地进行形状跟踪。

Result: 在连续型软体机器人上实验验证，实时操作过程中平均末端误差为2.6%，且在应用于实际闭环控制时表现稳定。

Conclusion: 该方法具备鲁棒、低成本、易部署等优点，非常适合动态、复杂的实际应用场景，展示出广阔的实际应用前景。

Abstract: Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.

</details>


### [424] [APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs](https://arxiv.org/abs/2511.18236)
*Nuno Soares,António Grilo*

Main category: cs.RO

TL;DR: 本文提出APULSE算法，有效解决大规模、稠密图上的资源约束最短路径问题（RCSPP），在复杂环境下显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有的RCSPP求解器在处理复杂、稠密和大规模图（如无人地面车辆任务规划问题）时，因可扩展性差和计算效率低，难以满足实时规划需求。

Method: APULSE是一种混合标签设定算法，将A*启发式引导的优先搜索与Pulse算法激进剪枝机制以及时间分桶策略结合，实现高效的状态空间压缩与搜索。

Result: 在大规模UGV规划场景的算例测试中，APULSE较现有顶尖算法，能在更短时间内找到近似最优解，表现出数量级上的速度提升与更强的鲁棒性，尤其在其他方法失效的超大问题实例上更为突出。

Conclusion: APULSE具备卓越的可扩展性与实用性，适用于复杂大规模环境下的RCSPP求解，为交互式决策支持和动态重规划等应用提供了有力工具。

Abstract: The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.

</details>


### [425] [Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters](https://arxiv.org/abs/2511.18243)
*Eashan Vytla,Bhavanishankar Kalavakolanu,Andrew Perrault,Matthew McCrink*

Main category: cs.RO

TL;DR: 本文尝试将物理知识融入Dreamer框架中的世界模型，以提升无人机在动态环境下的控制表现，但实验发现依然存在泛化性不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有飞行机器人控制算法在动态或恶劣环境下的鲁棒性有限，而基于模型的强化学习（RL）具备优良的泛化能力与样本效率，Dreamer算法又在此方向表现突出，但应用到无人机系统后遇到采样效率低与动力学泛化差等实际难题。

Method: 作者提出利用物理知识指导的世界模型，将四旋翼作为自由体，预测其受力和力矩，然后通过6自由度的Runge-Kutta积分（RK4）预测未来状态演化。与传统RNN型世界模型进行性能对比。

Result: 实验表明，物理引导和RNN的世界模型在训练数据上都能表现良好，但在新轨迹或测试场景的泛化能力较差，状态预测很快发散，导致策略难以收敛。

Conclusion: 论文工作显示，单独依赖物理引导或RNN进行世界建模，尚无法解决无人机在复杂环境下长期状态预测和策略收敛的挑战，泛化性依然是主要瓶颈。

Abstract: Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.

</details>


### [426] [Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search](https://arxiv.org/abs/2511.18270)
*Zhongkai Chen,Yihao Sun,Chao Yan,Han Zhou,Xiaojia Xiang,Jie Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种名为Skypilot的框架，将大语言模型（LLM）与Monte Carlo树搜索（MCTS）结合，显著提升了自主无人机在覆盖和搜索任务中的空间推理与决策能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在智能增强方面表现优异，但其缺乏物理现实的感知，导致在空间推理和决策上易出现幻觉和不可复现性问题。针对自主无人机在复杂任务中的实际需求，这一研究旨在弥补LLM物理基础的不足。

Method: 该方法采用两阶段框架：第一阶段通过丰富动作空间（包括生成、再生、微调、评估）及物理约束奖励函数，提升轨迹可行性。第二阶段将Qwen3-4B模型在23000条MCTS生成样本上进行微调，在保证方案质量的同时加快推理速度。

Result: 通过大量数值仿真和实地飞行实验，验证了该框架在效率和性能上均优于传统方法。

Conclusion: Skypilot框架能够将LLM有效嵌入物理任务环境中，为自主无人机提供更可靠和高效的决策方案，对复杂空间任务的智能体系统有重要的推进作用。

Abstract: Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.

</details>


### [427] [AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization](https://arxiv.org/abs/2511.18293)
*Shuai Zhang,Jingsong Mu,Cancan Zhao,Leiqi Tian,Zhijun Xing,Bo Ouyang,Xiang Li*

Main category: cs.RO

TL;DR: 本论文提出了一种结合超声声阻抗感知的神经辐射场（AIA-UltraNeRF）和机器人超声系统（RUSS），加速超声重建和定位过程，实现了更快、更精确的新视角合成和局部化。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法忽视了超声成像中声阻抗的作用，以及姿态初始化带来的定位局部极小值问题。因此，需发展能有效处理声阻抗信息且对扫描与诊断过程进行解耦的新方法。

Method: 本文设计了结合声阻抗建模的AIA-UltraNeRF，通过hash编码存储超声3D地图，实现无须密集采样条件下的高效重建和推理。提出了双监督网络，用教师-学生模型编码重建图像的哈希值，实现无需重新渲染即可离线检索初始定位。构建了带球面遥控旋转中心的RUSS，保证操作员无关的独立扫描。

Result: AIA-UltraNeRF在体模与人体实验中有效利用声阻抗表征图像特征，重建和定位推理速度比常规NeRF快9.9倍。

Conclusion: 结合声阻抗信息的UltraNeRF网络与新型机器人扫描平台显著提升超声图像重建与定位效率，推动了超声影像从被动诊断向自动化、高效采集的转变。

Abstract: Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.

</details>


### [428] [MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing](https://arxiv.org/abs/2511.18299)
*Steven Oh,Tai Inui,Magdeline Kuan,Jia-Yeu Lin*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖且低成本的声学传感方法MicCheck，将市售蓝牙领夹麦克风作为机器人夹爪的触觉传感器，实现了对材料类别和接触相关操作的感知和控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习大多依赖视觉，忽略了关键的接触信号（如刚度、滑动等），而触觉传感器又成本高、集成复杂。因此，亟需既经济又易集成的感知手段来提升机器人操作能力。

Method: MicCheck采用一个无需自制电子元件或特殊驱动的廉价蓝牙麦克风，安装于3D打印夹爪内，通过USB无线接收声波并直接输出音频信号，用于材料识别和接触操作；论文将其与模仿学习算法组合，并与现有触觉传感器进行性能比较。

Result: 在10类材料4种交互方式基准测评下，MicCheck实现了92.9%的材料分类准确率。在夹取与倒料任务中，集成此方法后任务成功率由0.40提升至0.80，并能稳定完成如拔插、基于声音分拣等高接触性操作。与高分辨率触觉传感器相比，麦克风方案大幅降低成本并简化集成。

Conclusion: MicCheck为机器人感知带来了易部署、极具成本优势的方案。尽管较高端触觉传感器牺牲了空间分辨率，但凭借方便及价格，其方法极具推广价值，可促进声学触觉方案在低成本机器人平台上的实际落地。

Abstract: Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.

</details>


### [429] [Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video](https://arxiv.org/abs/2511.18322)
*Henrik Krauss,Johann Licher,Naoya Takeishi,Annika Raatz,Takehisa Yairi*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于注意力的解码器（ABCD），用于学习和解释软体连续体机器人（SCR）从高维观测数据中的动力学。它不仅提高了动力学模型的预测精度，还赋予模型物理可解释性，适用于控制任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的SCR动力学学习方法虽然灵活，但缺失物理可解释性；而基于物理建模的方法则依赖先验知识且计算代价高。因此，亟需开发既具备解释性又高效的数据驱动方法。

Method: 本文提出Attention Broadcast Decoder（ABCD），集成到自动编码器框架中学习潜在动力学，通过生成像素级的注意力图定位每个潜在维度的贡献，并去除静态背景。将注意力图与2D振荡网络耦合，实现对学习到的动力学（质量、刚度、力）直观可视化，无需先验知识。方法在单段和双段SCR上进行了验证。

Result: 与标准方法相比，ABCD模型在两段式机器人数据上使Koopman算子多步预测误差减少5.7倍，振荡网络减少3.5倍。学习到的振荡网络还自动发现了振荡器链式结构。ABCD模型能实现训练数据范围外的平滑潜空间外推。

Conclusion: ABCD方法发展了一种全数据驱动、紧凑且具物理可解释性的动力学建模方式，能够有效支持软体机器人的控制应用。

Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.

</details>


### [430] [Enhancing UAV Search under Occlusion using Next Best View Planning](https://arxiv.org/abs/2511.18353)
*Sigrid Helene Strand,Thomas Wiedemann,Bram Burczek,Dmitriy Shutin*

Main category: cs.RO

TL;DR: 本文提出了一种用于高遮挡环境搜索与救援任务的无人机视角优化算法，通过新颖的几何启发和可见性启发，提高了对被遮挡目标的识别效率。实验表明，可见性启发在模拟和真实环境下均优于几何启发。


<details>
  <summary>Details</summary>
Motivation: 在地形复杂、高遮挡的自然灾害救援场景中，无人机部署虽能提高搜索效率，但其搜索效果受限于获取地面清晰视野的能力，因此需要更加智能的视角规划和定位方法。

Method: 作者提出并实现了“下一个最佳视角（next best view）”优化策略，通过两种启发式算法——几何启发和可见性启发——决定无人机拍摄的最佳位置与角度，从而提升被遮挡环境下的搜索性能。

Result: 实验在仿真和真实环境中进行。结果显示，可见性启发能在模拟森林中检测出90%以上的被遮挡物体，且比几何启发的检测率高出10%。实地实验也证实了可见性启发在林冠下有更好的覆盖和目标发现能力。

Conclusion: 该研究的方法特别适用于高遮挡环境下的无人机搜索行动，能显著提升目标发现率和覆盖效率，为今后的自然灾害救援和类似场景的无人机部署提供了更优解。

Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.

</details>


### [431] [Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates](https://arxiv.org/abs/2511.18374)
*Jiaxun Sun*

Main category: cs.RO

TL;DR: 本文首次给出了一个关于截断极小鲁棒正不变（mRPI）集合与其无穷水平极限之间Hausdorff距离的显式、封闭形式的上界表达式。


<details>
  <summary>Details</summary>
Motivation: 现有对mRPI集的逼近方法虽能保证收敛，但未能提供可计算的截断误差表达式，实际工程中难以确定截断水平下的误差大小。

Method: 推导了截断mRPI集合与极限集合之间Hausdorff距离的解析上界，公式为 $d_H(\mathcal{E}_N,\mathcal{E}_\infty) \le r_W\,γ^{N+1}/(1-γ)$，其中参数均有明确物理意义，实现无需迭代求解。

Result: 得到了完全解析的误差上界，论证向量范数的选择可以作为收敛速度的设计参数。实验显示该界限在实际中具有很高的紧致性和适用性。

Conclusion: 新方法为鲁棒不变集及管控MPC中截断水平的选择提供了理论依据和实际工具，提升了算法效率和适用性。

Abstract: This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \( d_H(\mathcal{E}_N,\mathcal{E}_\infty) \le r_W\,γ^{N+1}/(1-γ), \) where $γ<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.

</details>


### [432] [Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control](https://arxiv.org/abs/2511.18486)
*Jasan Zughaibi,Denis von Arx,Maurus Derungs,Florian Heemeyer,Luca A. Antonelli,Quentin Boehler,Michael Muehlebach,Bradley J. Nelson*

Main category: cs.RO

TL;DR: 该论文提出通过系统级控制设计扩展电磁导航系统（eMNS）有效工作空间，显著降低外科手术工具所需电流，并验证了多项技术手段有效性。


<details>
  <summary>Details</summary>
Motivation: eMNS常受功率和热限制，导致磁操作手术工具的有效工作空间有限。提升工作空间对精确和复杂手术具有重要实际意义。

Method: 作者提出五种系统级方法：以运动为中心的力/力矩控制目标、能量最优电流分配、实时姿态估计、动态反馈以及高带宽eMNS组件。同时，将传统磁场对准策略替换为基于力/力矩的运动驱动控制，并扩展到多体控制，实现多倒立摆同时稳定。

Result: 通过上述优化，八线圈OctoMag系统下维持3D倒立摆时，所需电流从8-14A大幅降至0.1-0.2A。多体控制实验及对比分析也验证方法可显著扩展有效工作空间。Navion eMNS跨平台评测显示50cm距离下仍能稳定操控。

Conclusion: 反馈机制和系统级控制设计为eMNS扩展工作空间、提升效率和临床适用性提供了有效途径，显示该方法在实际手术磁操作中的广阔前景。

Abstract: Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.

</details>


### [433] [SafeFall: Learning Protective Control for Humanoid Robots](https://arxiv.org/abs/2511.18509)
*Ziyu Meng,Tengyu Liu,Le Ma,Yingying Wu,Ran Song,Wei Zhang,Siyuan Huang*

Main category: cs.RO

TL;DR: 本文提出SafeFall框架，通过预测和应对即将不可避免的跌倒，有效减少仿人机器人在跌倒时的硬件损伤。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人由于双足步态本质上易跌倒，而一次跌倒会对昂贵的传感器、执行器和结构部件造成严重损坏。这成为仿人机器人现实部署的重要障碍，因此亟需提高机器人在跌倒时的保护能力。

Method: SafeFall由两部分组成：一是轻量级的GRU（门控循环单元）跌倒预测器，持续监控机器人状态判断是否即将不可避免地跌倒；二是强化学习控制策略，基于专门设计的、能够识别机器人结构弱点的奖励函数，学会以最小化硬件损伤的方式摔倒，仅在跌倒预测为不可避免时激活。

Result: 在实际尺寸的Unitree G1仿人机器人上实验证明，SafeFall能显著降低撞击力、关节扭矩和易损件的碰撞概率，分别减少68.3%、78.4%和99.3%。

Conclusion: SafeFall为仿人机器人提供了跌倒时的安全防护，大幅降低损坏风险，使更激进、更复杂的实验和真实环境部署成为可能。

Abstract: Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\%, peak joint torques by 78.4\%, and eliminated 99.3\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.

</details>


### [434] [Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation](https://arxiv.org/abs/2511.18525)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Yonghan Lee,Jaehoon Choi,Jianyu An,Stephen Cheng,Dinesh Manocha*

Main category: cs.RO

TL;DR: Splatblox是一种为有密集植被和不规则障碍物的户外环境设计的自主导航系统，利用高斯点云方法将RGB图像和激光雷达融合，提升了机器人对复杂地形的适应和导航成功率。


<details>
  <summary>Details</summary>
Motivation: 在户外复杂环境（如植被密集区域）中，现有机器人导航方法难以有效区分可通行植被與刚性障碍，也难以实现大范围高效规划，因此需要新的感知和场景建模方法。

Method: 提出利用高斯点云技术，将分割后的RGB图像与LiDAR点云融合，动态生成包含结构和语义信息的欧几里得符号距离场（ESDF），该场可辨别不同类型障碍和地形，并支持实时更新和远距离规划。

Result: 在四足机器人和轮式机器人平台的户外测试中，Splatblox在植被丰富的环境下，相较现有方法，导航成功率提升50%以上，卡死次数减少40%，路径缩短5%，速度提升13%，能支持100米长距离自主任务。

Conclusion: Splatblox显著提升了机器人在复杂户外环境中的自主导航能力，兼具语义理解与全方位几何感知，验证了其通用性和效率，为户外机器人应用提供了新的解决方案。

Abstract: We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io

</details>


### [435] [Object-centric Task Representation and Transfer using Diffused Orientation Fields](https://arxiv.org/abs/2511.18563)
*Cem Bilaloglu,Tobias Löw,Sylvain Calinon*

Main category: cs.RO

TL;DR: 本文提出了一种利用Diffused Orientation Fields (DOF)的方法，通过在局部参考系中表达操作任务，实现了机器人在不同曲面物体间的任务迁移。DOF通过条件扩散过程从点云数据在线计算，并证明可高效应对复杂几何变化。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，曲面物体因缺乏全局统一参考系，难以进行任务迁移，例如“沿着曲面”这类方向会随位置和形状改变，传统方法难以推广到不同物体。亟需新方法提升机器人跨曲面迁移能力。

Method: 作者提出DOF（一种平滑的局部参考系表达方式），结合稀疏关键点匹配，将复杂的曲面物体任务迁移问题化简。同时，DOF通过满足特定偏微分方程的扩散过程，从原始点云数据实时自动生成。

Result: 通过在几何、拓扑及定位扰动下的实验，DOF表现出强鲁棒性，实现了如检查、切片、剥皮等需连续物理交互的任务在不同曲面物体上的顺利迁移。

Conclusion: DOF为机器人在多种曲面物体间的任务迁移提供了高效、易于实现的框架，显著提升了跨物体迁移的能力，有助于推动机器人在实际复杂环境中的灵活应用。

Abstract: Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as "toward" or "along" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics

</details>


### [436] [An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms](https://arxiv.org/abs/2511.18604)
*Hannah Lee,James D. Motes,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文通过对约束分类（保守型和激进型），分析两类主流多智能体路径规划（MAPF）算法（CBS和CBSw/P）的表现，帮助设计和选择更有效的多智能体与多机器人路径规划算法。


<details>
  <summary>Details</summary>
Motivation: 随着多机器人系统规模和复杂性的增加，如何高效地进行路径规划成为研究热点。本研究意在通过约束分类优化约束搜索算法，提升整体求解效率和解质量，指导未来算法设计。

Method: 作者在混合格点-路径网格表征下，对比保守型（动作约束）与激进型（优先级约束）两种约束处理方式，分别应用于标准CBS和CBSw/P算法，并在不同分辨率及Agent数量的场景中实验分析其表现。

Result: 实验显示，在Agent数量和分辨率增加时，激进型约束（如优先级约束）解决实例数量更多，但在两种约束都能解出的情况下，保守型约束（动作约束）带来更优解。作者还综合实验数据等，形成决策流程图辅助约束选择。

Conclusion: 不同约束类型适用于不同规划需求，需结合问题特性、解质量和拓扑特征综合选择；研究对多机器人路径及运动规划具有指导意义。所有研究数据和实验结果已开源。

Abstract: This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis

</details>


### [437] [How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints](https://arxiv.org/abs/2511.18606)
*Kensuke Nakamura,Arun L. Bishop,Steven Man,Aaron M. Johnson,Zachary Manchester,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出LatentCBF方法，实现了在高维观测下的平滑安全过滤，并显著提升了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有的Latent Safety Filters能在潜在空间执行安全控制，但存在切换策略导致表现受损的问题。此外，将Hamilton-Jacobi可达函数用于平滑优化的控制屏障函数（CBF）时，现有潜在空间学习方法得到的价值函数存在根本不兼容的问题。本文旨在解决这些兼容性与任务性能损失的难题。

Method: 作者分析了现有方法的问题，发现一是margin函数不平滑导致CBF不兼容，二是只用安全策略训练导致价值估计不准确。为此，提出LatentCBF：通过梯度惩罚约束margin函数平滑性，以及混合名义和安全策略样本来训练价值函数，以同时兼顾平滑与准确性。

Result: 在仿真和真实视觉操控实验中，LatentCBF展现出平滑的安全过滤效果，在维持安全的同时，任务完成率比现有的切换式方法提升了一倍。

Conclusion: LatentCBF有效解决了潜在空间安全控制中的可达性与CBF兼容问题，实现了平滑过滤并显著提升任务性能，为高维视觉场景中的安全控制提供了新的解决思路。

Abstract: Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement "least-restrictive" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a "margin function" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.

</details>


### [438] [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)
*Litian Gong,Fatemeh Bahrani,Yutai Zhou,Amin Banayeeanzade,Jiachen Li,Erdem Biyik*

Main category: cs.RO

TL;DR: AutoFocus-IL提出了一种高效的视觉模仿学习方法，通过自动化手段引导策略关注任务相关特征，提升了学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习容易被无关特征或虚假关联干扰，为引导模型正确关注有效视觉信息，现有方法依赖昂贵的人类注视数据或人工标注，这限制了实用性。

Method: AutoFocus-IL利用视觉-语言模型自动识别演示中的关键物体，生成时序显著性图，仅突出因果视觉信号并抑制干扰项。将其用于规范行为克隆策略，使模型注意力更集中于任务线索，无需人工监督。

Result: 在CARLA模拟器和真实机器人操作实验中，AutoFocus-IL不仅优于标准行为克隆方法，也优于依赖人类注视等特权监督信息的先进方法。

Conclusion: AutoFocus-IL能在无需昂贵人工监督的前提下，显著提升模仿学习的表现，具有较强的实用价值。

Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

</details>


### [439] [Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles](https://arxiv.org/abs/2511.18683)
*Yinan Dong,Ziyu Xu,Tsimafei Lazouski,Sangli Teng,Maani Ghaffari*

Main category: cs.RO

TL;DR: 本文提出了一种结合误差状态MPC与在线学习模块的高效控制器，用于在未知扰动下实现自主水面载具（ASV）精确轨迹跟踪，并在仿真和实船实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: ASV在实际应用中易受环境扰动（如风和浪）的影响，导致轨迹跟踪精度下降，现有方法很难在动态复杂环境中实现高精度、鲁棒且高效的控制。因此，亟需一种能适应未知扰动且计算高效的控制方法。

Method: 本文方法融合了定义在Lie群上的凸误差状态模型预测控制（MPC）和在线学习组件。MPC部分保证轨迹跟踪的优化寻优与稳定性，在线学习模块则实时估计和补偿环境未知扰动，实现自适应控制能力。

Result: 通过大量数值仿真、VRX虚拟仿真和实际水面实验，验证了该方法在多种扰动场景下的轨迹跟踪精度均优于现有方法。

Conclusion: 所提出的控制框架在保证计算效率的同时，在动态复杂环境下实现了精确、鲁棒的轨迹跟踪，为ASV在实际应用中的广泛部署提供了新思路。

Abstract: Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.

</details>


### [440] [Stable Multi-Drone GNSS Tracking System for Marine Robots](https://arxiv.org/abs/2511.18694)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Zhizun Wang,Junming Shi,Gregory Dudek*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的多无人机协同定位跟踪系统，实现了水面及近水面机器人在GNSS信号受限环境下的高精度定位。


<details>
  <summary>Details</summary>
Motivation: GNSS在水下或近水面环境中信号极不稳定甚至不可用，现有惯性、声呐、SLAM等定位手段存在误差累积、算力消耗大或依赖额外设备等问题，因此亟需新的实用性强的定位方法。

Method: 系统整合了高效的视觉检测、轻量级多目标跟踪、基于无人机GNSS信息的三角测量、融合置信度的扩展卡尔曼滤波（EKF），并创新性地提出了跨无人机追踪ID一致性算法，可在无人机群间实现目标信息的全局一致，多视角协同跟踪水面及近水面机器人。

Result: 在多种复杂场景下测试，系统展现了良好的扩展性与鲁棒性，能实时稳定输出高精度GNSS定位信息，支持多机器人同步跟踪。

Conclusion: 提出的方法有效克服了水域GNSS不可用的难题，为水面和近水面机器人提供了可靠的定位手段，具有良好的实际应用价值。

Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.

</details>


### [441] [CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection](https://arxiv.org/abs/2511.18702)
*Xueyan Oh,Leonard Loh,Shaohui Foong,Zhong Bao Andy Koh,Kow Leong Ng,Poh Kang Tan,Pei Lin Pearlin Toh,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出了一种无基础设施、易于部署的摄像机位姿估计算法，可自动化飞机外观目视检查流程，为机场快速周转需求提供解决方案。该方法利用深度卷积神经网络，并只需合成图像进行微调，实现高精度的相机定位。


<details>
  <summary>Details</summary>
Motivation: 当前飞机外观目视检查依赖人工，需在短时间内完成，要求较高且机场环境复杂，现有定位方法通常需额外硬件基础设施，不易部署；同时，机场安全规定限制了与飞机表面的接触及无人机的使用，亟需无需外部基础设施的自动化检测技术。

Method: 提出了一种基于平移-俯仰-变焦（pan-tilt-zoom）相机的位姿估计方法，通过利用深度卷积神经网络，并采用只含合成图像的域随机化数据进行微调。引入飞机几何信息优化损失函数，提出了从位姿初始化、扫描路径规划到精确位置标定的完整工作流程。

Result: 在真实飞机数据上进行了实验，所提出系统实现了小于0.24米和2度（均方根误差）的相机位姿估计精度，显示出在实际机场场景下的有效性和可用性。

Conclusion: 该方法无需额外基础设施，即可高效实现小组人员少、环境复杂条件下的飞机自动化外观检测，降低了人工依赖，为机场快速周转提供了可行的技术路径。

Abstract: General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.

</details>


### [442] [Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication](https://arxiv.org/abs/2511.18703)
*Ardalan Tajbakhsh,Augustinos Saravanos,James Zhu,Evangelos A. Theodorou,Lorenz T. Biegler,Aaron M. Johnson*

Main category: cs.RO

TL;DR: 提出了一种面向通信延迟的分布式协调优化算法 (DA-ADMM)，实现多机器人系统在实际通信延迟下的鲁棒、无碰撞运动规划。


<details>
  <summary>Details</summary>
Motivation: 多机器人运动规划在现实中难以避免通信延迟，而现有分布式优化方法（如共识型 ADMM）对延迟敏感，现有的参数调整策略又未能显式考虑延迟影响，导致系统鲁棒性和表现下降。

Method: 提出延迟感知型 ADMM（DA-ADMM），关键在于根据实时延迟统计自适应地调整惩罚参数，使算法在共识和对偶更新时，对滞后信息降低权重、优先考虑新信息。

Result: 在2D和3D环境中，用双积分、Dubins-car、无人机动力学进行模拟，DA-ADMM显著提升了鲁棒性、成功率和解质量，优于固定参数、残差平衡和固定约束等基线方法。

Conclusion: DA-ADMM在各种延迟条件下均展现出优越的多机器人协调运动规划性能，表明有效利用延迟信息比仅关注延迟长度或频率更为关键。该方法为多机器人系统在实际通信条件下的高效鲁棒规划提供了有力工具。

Abstract: This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.

</details>


### [443] [GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration](https://arxiv.org/abs/2511.18708)
*Yanbin Li,Canran Xiao,Shenghai Yuan,Peilai Yu,Ziruo Li,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于广义Voronoi图（GVD）的环境拓扑地图实时更新方法，提升机器人探测任务中的地图准确性和细节捕捉能力，并通过多种机制优化路径规划和探索效率。


<details>
  <summary>Details</summary>
Motivation: 当前拓扑地图在机器人探索任务中相比于度量地图更合适，但如何实时、准确、细致地更新环境拓扑地图仍存在挑战，尤其是在避免无效或错误节点、提升探索灵活性等方面。

Method: 主要方法包括：1）通过对新观测区域去噪，避免无效GVD节点干扰拓扑结构；2）设计多粒度分层GVD生成方法，实现全局与局部的采样控制，提升拓扑准确性和细节表达，同时通过覆盖图防止GVD重叠，提高利用率；3）引入有连通性约束的节点聚类和切换机制避免不可达和错误节点，并用缓存结构存储连通信息提升效率；4）提出基于形态膨胀的前沿提取方法确保探索的可达性；5）采用轻量级代价函数实时评估和切换探索视点，提高策略适应性。

Result: 方法在与SOTA进行的对比实验中验证了其在探索任务中的性能提升，表现为更好的地图准确性、路径有效性和探索效率。

Conclusion: 所提出的GVD-based拓扑地图实时更新与优化方法能够有效提升机器人探索任务中的地图表达及策略调整能力，对提高无人探索系统的自主性和灵活性具有应用前景。

Abstract: Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.

</details>


### [444] [Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models](https://arxiv.org/abs/2511.18709)
*Xueyan Oh,Jonathan Her,Zhixiang Ong,Brandon Koh,Yun Hann Tan,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出利用基础模型进行机器人UV消毒表面选择，无需复杂训练，减少人工操作，以及通过VLM协助的分割精细化提升分割精度，在实际环境下达到92%以上的正确率。


<details>
  <summary>Details</summary>
Motivation: 传统UV消毒需要大量人工干预和场景标注，而深度学习方法又依赖巨量数据与调优，难以大规模应用，而且普遍忽视了对部分表面消毒的场景理解问题，易造成误消毒甚至安全风险。

Method: 1. 引入基础模型简化用于机械臂UV消毒的表面选择流程，减少人工和培训成本。2. 结合视觉语言模型（VLM）辅助的分割精细化模块，用于检测并排除细小、非目标物体，从而降低误分割发生。

Result: 该方法在真实世界中通过机械臂和模拟UV消毒实验，目标与非目标表面的正确分割率超过92%，显著降低了误分割。

Conclusion: 该方法无需复杂模型训练即可高效实现UV消毒自动化分区，提升实用性和安全性，具备实际部署的潜力。

Abstract: Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.

</details>


### [445] [Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control](https://arxiv.org/abs/2511.18712)
*Tianyu Wang,Chunxiang Yan,Xuanhong Liao,Tao Zhang,Ping Wang,Cong Wen,Dingchuan Liu,Haowen Yu,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出一种针对6自由度轮式双足机器人头部主动稳定的新方法，通过地面力估计和从动控制提升在不平坦地形上的表现。


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人在实地勘探中的应用受到重视，但现有研究仅关注机器人移动平台的稳定性，忽略了头部在世界坐标系下的主动稳定，导致头部传感器精度降低或易损坏。解决这一问题有助于提升医学、勘探等应用场景中机器人任务能力。

Method: 开发了一种基于模型的地面力估计算法，结合从动控制(admittance control)方法，主动进行头部稳定调节，提高机器人对复杂地形的适应能力。通过仿真验证其地面力估计实时性和机器人越障鲁棒性。

Result: 仿真实验证明所提出算法能够实时估计地面力，提升机器人在不平路面的通过能力与头部稳定性，减少垂直振荡。

Conclusion: 该方法有效提升了轮式双足机器人在复杂环境下的任务表现和传感器保护能力，具有良好的推广和应用价值。

Abstract: Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.

</details>


### [446] [AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation](https://arxiv.org/abs/2511.18718)
*Omar Garib,Jayaprakash D. Kambhampaty,Olivia J. Pinon Fischer,Dimitri N. Mavris*

Main category: cs.RO

TL;DR: 该论文提出了AIRHILT，一个用于航空冲突检测的多模态飞行员与空管协同仿真环境，支持集成各类AI模型和可重复性研究。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在航空安全领域应用的增长，现阶段缺少一个既能支持人机融合、又易于集成和测试各类模型的开放仿真框架，阻碍了相关领域的算法开发和公平对比。

Method: 作者基于Godot引擎，开发AIRHILT仿真环境，可同步飞行员、空管无线电、摄像头视频、ADS-B监视数据，并支持JSON接口灵活接入ASR、视觉检测、决策、TTS等AI模型。内置方案验证包括Whisper、YOLO、GPT-OSS-20B等模型链路，对多个典型冲突场景进行实验。

Result: 在跑道重叠代表场景中，集成助手模型平均首次预警时间约为7.7秒，ASR延迟5.9秒，视觉检测延迟0.4秒，展现出协同多模态方案的可行性和较快响应。

Conclusion: AIRHILT为航空场景的多模态态势感知和冲突检测研究提供了标准化、可重复的实验平台，有助于推动自动化协同系统与人机智能体的深入发展。

Abstract: We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.

</details>


### [447] [SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map](https://arxiv.org/abs/2511.18756)
*Xueyu Du,Lilian Zhang,Fuan Duan,Xincan Luo,Maosong Wang,Wenqi Wu,JunMao*

Main category: cs.RO

TL;DR: 该论文提出了一种新型基于滤波的立体视觉惯性导航系统（VINS），在保持高效性的同时实现了长期高精度定位，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有滤波型VINS虽然高效，但长期高精度定位受限于建图能力，无法满足移动机器人长时间任务需求。

Method: 提出一种基于关键帧和2D关键点的隐式环境地图，实现高效回环检测；引入融合地标重投影和射线约束的混合残差滤波框架，通过统一的雅可比矩阵进行测量更新；针对恶劣环境，设计了摄像头-IMU外参的在线标定机制。

Result: 通过基准实验，所提SP-VINS系统在计算效率和长期定位精度上均优于现有最先进（SOTA）方法。

Conclusion: 该系统显著提升了滤波型VINS在长期、复杂环境下的可用性，兼顾效率与精度，对移动机器人有实际应用价值。

Abstract: Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.

</details>


### [448] [MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent](https://arxiv.org/abs/2511.18810)
*Yuxia Fu,Zhizhen Zhang,Yuqi Zhang,Zijian Wang,Zi Huang,Yadan Luo*

Main category: cs.RO

TL;DR: 本文提出MergeVLA，一种专为多任务融合设计的视觉-语言-动作模型架构，可有效支持模型在多技能场景下的能力融合，并在多个基准和真实机器人实验中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在单一任务或单一机器人平台能有效微调，但在尝试合并不同任务专家（即多技能融合）时，效果几乎为零，暴露出多任务/多技能融合的困难。本研究旨在揭示导致多技能模型难以融合的原因，并解决这一关键挑战。

Method: 通过实验分析，发现不可合并性的两大根源：（1）微调阶段，LoRA适配器参数在主干网络中向不同任务方向发散，超出现有融合算法的能力；（2）动作专家因自注意反馈机制产生层间依赖，阻止模块化重组。针对这些问题，提出MergeVLA：采用按任务掩码稀疏激活的LoRA适配器以减少参数冲突，并在动作专家模块用仅跨注意力的结构（去除自注意）来保持任务专属化和可组合性。此外，引入测试时自动任务路由机制，可在任务未知时选择合适专家与掩码，实现无监督任务推断。

Result: MergeVLA在LIBERO、LIBERO-Plus、RoboTwin和真实SO101机械臂多任务实验中，表现与各单独微调专家相当甚至更优，展现了在多任务、多形态、多环境下的鲁棒泛化能力。

Conclusion: MergeVLA通过结构设计兼顾了多任务可融合性和专家性能，推动视觉-语言-动作多任务学习研发进展，对提升机器人泛化和多技能一体化能力有现实意义。

Abstract: Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.

</details>


### [449] [AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion](https://arxiv.org/abs/2511.18857)
*Changsheng Luo,Yushi Wang,Wenhan Cai,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为AutoOdom的全新自回归本体测程系统，通过两阶段训练（大规模仿真+有限现实数据自回归增强），极大提升了腿式机器人在无GPS或视觉退化环境中的测程准确性，显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前腿式机器人在GPS不可用或视觉受限环境中导航依赖本体测程，然而现有方法或依赖精确建模、或结合学习与滤波但仍受制于分析组件，纯学习则面临仿真到现实能力较弱和对数据依赖过高等难题。

Method: 提出分两阶段训练的AutoOdom系统，第一阶段用大规模仿真数据学习腿式机器人复杂动力学和接触状态，第二阶段通过自回归机制强化模型，利用有限现实数据弥补仿真与现实差距。

Result: 在Booster T1类人机器人上的实验证明，AutoOdom在测程准确性、姿态估计等指标上大幅优于主流方法基础线（如Legolas），绝对轨迹误差降低57.2%，Umeyama对齐误差降低59.2%，相对位姿误差降低36.2%。

Conclusion: AutoOdom能有效克服现有本体测程方法的缺陷，尤其是在传感器噪声大及动态环境下表现出极强鲁棒性，对传感器选择与时序建模有新发现，为复杂场景下腿式机器人的自我定位提供了有力支撑。

Abstract: Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.

</details>


### [450] [Accelerating Reinforcement Learning via Error-Related Human Brain Signals](https://arxiv.org/abs/2511.18878)
*Suzie Kim,Hye-Bin Shin,Hyo-Jeong Jang*

Main category: cs.RO

TL;DR: 本论文研究了通过融合脑电（EEG）相关神经反馈信号，对复杂机器臂操作任务中的强化学习进行加速。实验发现适当权重的人类神经反馈可以提高学习效率，使得强化学习在高维操作任务中的表现超过传统稀疏奖励基线，并具有良好个体适应性。


<details>
  <summary>Details</summary>
Motivation: 以往EEG引导的强化学习主要集中在低维任务（如导航、运动），高维复杂操作任务的神经反馈应用仍缺乏研究。作者希望评估神经反馈在障碍丰富、需精确控制的操作任务中对策略学习的提升作用。

Method: 作者将通过离线训练EEG分类器得到的错误相关电位（ErrP）整合进奖励塑形机制中，并系统地评估不同人类反馈权重对学习效果的影响。在七自由度机械臂和包含障碍的点到点操作环境中开展实验，同时验证跨个体鲁棒性。

Result: 结果显示，适当权重的人类EEG反馈能够显著加速操作任务的强化学习，某些权重下成功率超过只用稀疏奖励的基线；采用最佳权重对所有被试测试时也有持续加速效果，并能很好地适应个体EEG可译性的差异。

Conclusion: 脑电神经反馈能为复杂操作动作的强化学习加速赋能，适合扩展至超越移动与低维场景，为人机协同操控能力提供了新方向。

Abstract: In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.

</details>


### [451] [An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization](https://arxiv.org/abs/2511.18910)
*Samuel Cerezo,Seong Hun Lee,Javier Civera*

Main category: cs.RO

TL;DR: 本文提出了一种无需非线性优化、可闭式求解的视觉-惯性系统初始化方法，具备更高精度和更低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-惯性初始化依赖于迭代求解器，计算资源需求高，易陷入局部最优且初始化延迟较大。因此，需要一种更高效、稳定、简便的初始化方法。

Method: 作者基于小角度旋转和恒定速度的近似，建立了全状态的解析初始化公式，同时提出结合可观测性分析的两阶段初始化策略，使方法既准确又低延迟。

Result: 该方法在EuRoC数据集上的实验显示，相比优化方法，初始化误差降低10-20%，初始化窗口缩短4倍，计算成本下降5倍。

Conclusion: 新方法在保持耦合性的同时显著提升了初始化的精度和效率，适合在实际视觉-惯性系统中应用。

Abstract: In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.

</details>


### [452] [Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.18950)
*Juntao Gao,Feiyang Ye,Jing Zhang,Wenjing Qian*

Main category: cs.RO

TL;DR: 提出了一种高效的视觉-语言-动作模型视觉信息压缩方法Compressor-VLA，在保证任务表现的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理视觉信息时计算量过大，标准的通用token剪枝方法难以兼顾全局语义和细粒度信息，限制了机器人实时应用。

Method: 设计了Compressor-VLA框架，包含语义任务压缩器（STC）提取任务相关全局语义，空间细化压缩器（SRC）保留细粒度空间信息，两模块均由自然语言指令动态调节，实现任务导向的信息压缩。

Result: 在LIBERO基准上，Compressor-VLA性能可与基线相当，但浮点运算量减少59%，视觉token数减少3倍以上，实际机器人测试验证了迁移和实用性。

Conclusion: Compressor-VLA能根据指令动态关注任务相关视觉区域，高效压缩视信息，兼顾精度和效率，适用于实际机器人部署。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

</details>


### [453] [End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera](https://arxiv.org/abs/2511.19011)
*Jiale Zhang,Yeqiang Qian,Tong Qin,Mingyang Jiang,Siyuan Chen,Ming Yang*

Main category: cs.RO

TL;DR: 本文提出了一种仅利用摄像头进行车辆编队跟车的端到端系统，有效提升了驾驶表现，且成本低廉，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的车辆编队技术对车道线和高精度传感器依赖度高，导致适用性受限且成本高。本文旨在通过单一摄像头实现通用且经济的跟车系统，解决交通拥堵、事故与碳排放问题。

Method: 提出一种端到端的深度学习方法，仅通过相机采集的数据完成车辆跟车任务。方法中引入语义掩码缓解多帧数据融合中的因果混淆问题，同时提出动态采样机制以精确跟踪前车轨迹。

Result: 在真实道路的闭环实验中，所提出系统能在多种场景下有效跟车，性能优于传统多阶段算法。

Conclusion: 该端到端摄像头跟车系统兼顾普适性与低成本特点，是实现实际车辆编队自主驾驶的有前景解决方案。

Abstract: The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.

</details>


### [454] [Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors](https://arxiv.org/abs/2511.19031)
*Haihang Wu,Yuchen Zhou*

Main category: cs.RO

TL;DR: 该论文提出第一个多智能体的单目稠密SLAM系统，通过融合各个智能体的局部地图，实现了高效、准确的3D重建和位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有单目SLAM系统虽然可以生成精细的3D地图，但计算代价高且大多仅支持单智能体。该论文旨在提升效率，并解决多智能体协作问题。

Method: 基于MASt3R-SLAM，扩展到多智能体场景。每个智能体利用3D重建先验独立进行局部SLAM，然后通过回环检测驱动的地图融合机制，将局部地图合成为全局一致的地图。

Result: 新方法在真实数据集下，与现有方法相比，计算效率有明显提升，地图精度保持相似水平。

Conclusion: 该工作首次实现了多智能体的单目稠密SLAM，证明了融合重建先验与多智能体协作的可行性和效率优势，为实际应用提供了新思路。

Abstract: Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.

</details>


### [455] [Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework](https://arxiv.org/abs/2511.19094)
*David Bricher,Andreas Mueller*

Main category: cs.RO

TL;DR: 本论文提出了一种基于深度学习的人机安全框架（HRSF），可根据人与机器人之间的距离动态调整机器人速度，同时兼顾生物力学安全限制，并在实验中显著提升了生产效率。


<details>
  <summary>Details</summary>
Motivation: 当前协作机器人的安全标准（如ISO/TS-15066）通常要求在近距离作业时降低机器人速度，这虽然保证了人员安全，但大大限制了协作效率。如何在确保安全的前提下提升协作速度，是工业机器人领域亟需解决的问题。

Method: 作者设计了一套基于深度学习的人机安全框架HRSF。该框架可动态感知人与机器人之间的距离，并据此调整机器人速度，同时确保所受力量和压力不超过生物力学极限。论文对四种人体识别相关深度学习方法（人体识别、人体分割、姿态估计、身体部位分割）进行了对比实验。

Result: 实验结果显示，所提HRSF可将机器人工作周期时间降低多达15%，相较于传统依赖安全范围的工业安全技术大幅提升了作业效率。

Conclusion: 本研究证明了基于深度学习的人机安全感知能够在保证安全的前提下实现更高效的机器人协作作业，为提升人机协作的实际应用价值提供了重要参考。

Abstract: Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.

</details>


### [456] [Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts](https://arxiv.org/abs/2511.19135)
*Pascal Goldschmid,Aamir Ahmad*

Main category: cs.RO

TL;DR: 本文提出了一种多旋翼无人机（UAV）在飞艇上自主对接并充电/卸载数据的方法，通过新颖的风突变应对和避障技术延长UAV作业时间，同时保证对接安全性。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机因电池续航受限导致作业时间短。利用飞艇作为中继平台可补给能量并卸载数据，但飞艇易受风干扰，对接难度大，现有方法缺乏对风影响的实时应对与精准、避障对接策略。

Method: 1) 设计时间卷积网络预测风对飞艇影响，辅助判断安全的对接窗口；2) 基于预测结果，通过模型预测控制器（MPC）结合创新的近距离避障算法，规划无人机避障对接轨迹。

Result: 模拟实验显示，该方法在多场景下显著优于飞艇恒速模型的基线方法，并成功在真实环境中完成了全球首个多旋翼无人机与飞艇的自主对接。

Conclusion: 本研究提供了面向未来持久无人机任务的一种切实可行的飞艇自主对接与补给方案，有效提升了作业效率和安全性。

Abstract: Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.

</details>


### [457] [Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap](https://arxiv.org/abs/2511.19201)
*Ann-Sophia Müller,Moonkwang Jeong,Jiyuan Tian,Meng Zhang,Tian Qiu*

Main category: cs.RO

TL;DR: 本文提出了一种利用永久磁铁阵列实现2D磁力陷阱，在开放空间内稳定控制医学毫机器人，并基于GPU加速的优化算法优化磁体角度，高效扩展到大规模阵列，实现复杂轨迹跟踪，对微创手术应用具潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管利用磁力操控医学毫机器人的技术有望用于微创手术，但目前要在较大距离上对小型机器人施加强磁场仍是难题。永久磁铁虽可产生较大磁力，但其反馈控制和稳定性受限，尤其3D的稳定磁陷阱理论上无法实现，因此亟需突破性设计来提升磁控效率与控制能力。

Method: 作者提出一种新的GPU加速优化算法，利用均方误差（MSE）与Adam优化器计算阵列中多个永久磁铁的最优角度，实现平面2D稳定磁力陷阱。通过数值仿真和实际双磁铁阵列实验，对方法进行了验证，并展示了对100个磁铁的优化效率和可扩展性。

Result: 通过数值仿真和实际实验，阵列永久磁铁能在20-120mm可调距离内于开放空间形成稳定2D磁力陷阱，成功控制毫机器人沿复杂轨迹运动，优化算法能在三秒内完成100个磁铁角度优化。

Conclusion: 本研究实现了永久磁体阵列的大尺度、实时优化，解决了传统永久磁控稳定性差和控制难的难题，为毫机器人在微创医疗等场景下的稳定、远距离磁力操控提供了新方案。

Abstract: Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.

</details>


### [458] [Reference-Free Sampling-Based Model Predictive Control](https://arxiv.org/abs/2511.19204)
*Fabian Schramm,Pierre Fabre,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的模型预测控制（MPC）方法，无需手工设计步态或预定义接触序列，即可在四足机器人和类人机器人上实现多样的自发运动模式，包括行走、跳跃和倒立等高难度动作。该方法高效、实时、无需GPU加速，并通过实物和仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人运动控制往往依赖于手工设计的步态模式和精心制定的接触序列，限制了运动多样性与自适应能力。如何在不引入先验动作库的情况下，通过优化高层目标自动发掘丰富运动行为，是该领域的重要挑战。

Method: 基于MPPI（模型预测路径积分）方法，作者设计了一种双空间样条参数化策略，用于在位置和速度控制点上生成运动轨迹。方法通过采样少量轨迹，实现自动化的接触生成与断开，提升了采样效率，使其仅依赖普通CPU即可实时运行，无需GPU加速。

Result: 在真实四足机器人Go2上，方法实现了包括小跑、疾驰、静态站立、跳跃、单手倒立等高度多样化且适应性强的动作。在模拟器中，还演示了更复杂的行为，如后空翻、类人机器人动态走位与倒立运动。上述全部无需参考跟踪或离线预训练。

Conclusion: 作者提出的方法极大简化了高性能机器人运动控制的设计与部署流程，提升了动作多样性和即时适应性，同时具备极高的计算效率，为机器人自发运动能力发展开辟了新的技术路线。

Abstract: We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.

</details>


### [459] [Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation](https://arxiv.org/abs/2511.19211)
*Prabhat Kumar,Chandra Prakash,Josh Pinskier,David Howard,Matthijs Langelaar*

Main category: cs.RO

TL;DR: 本文提出一种系统的拓扑优化方法，用于设计软体气动夹爪（SPG），并考虑了致动载荷与结构设计相关的特性。通过仿真和实际3D打印实验证明，优化后的设计性能优于传统设计。


<details>
  <summary>Details</summary>
Motivation: 当前软体气动夹爪的结构设计常常忽略致动载荷与设计之间的关联，导致夹爪性能有限。该文旨在提高夹爪的性能与适应性，在结构设计阶段引入设计相关载荷建模。

Method: 作者将致动载荷通过修正的Darcy定律建模，采用鲁棒的最小-最大拓扑优化框架，分别对设计蓝图和侵蚀状态下的单位进行体积与应变能约束优化，利用MMA算法求解。最后通过有限元分析及Ogden材料模型评估优化效果，并进行3D打印与实验验证。

Result: 优化得到的2D单元性能优于常规矩形单元。将其延拓为3D模块，组装为软体臂，经实际3D打印制备。实验证明，该SPG能抓取各种不同体积、重量、刚度与形状的物体，夹持性能优秀。

Conclusion: 本文所提出的拓扑优化框架可有效提升软体气动夹爪的性能，且有实际可行性和通用性。该方法为软体机械手设计提供了新途径。

Abstract: This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.

</details>


### [460] [SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control](https://arxiv.org/abs/2511.19236)
*Yuxuan Wang,Haobin Jiang,Shiqing Yao,Ziluo Ding,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了SENTINEL——一种将语言指令直接转换为类人机器人全身动作的端到端模型，解决了现有系统中理解和执行脱节的问题，并在仿真和现实环境中展现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的类人控制系统，依赖于遥操作或将语言理解与物理执行相分离的模块化管道，这导致要么完全由人驱动，要么指令和行为之间耦合不紧密。因此，作者希望构建一个能够真正以端到端方式理解并执行自然语言指令的机器人控制系统。

Method: 作者通过使用预训练的全身控制器在仿真环境中跟踪人类动作，并结合相应的文本标注，构建了大规模数据集。模型输入为语言指令及机器人本体感觉（proprioceptive）数据，直接输出底层动作，无需中间表示。核心包括flow matching生成初步动作片段，以及余差动作头进一步精炼以适应现实部署。还可通过将多模态输入转为文本，拓展输入形式。

Result: 该方法表现出强大的语义理解能力和稳定的执行力，在仿真和现实类人机器人上均取得了优异的控制效果。还支持多模态输入，表现出良好的泛化能力。

Conclusion: SENTINEL实现了语言和动作之间的高效端到端对齐，提升了类人机器人对自然语言的理解和响应能力，为未来灵活的多模态人机交互打下基础。

Abstract: Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.

</details>


### [461] [Rethinking Intermediate Representation for VLM-based Robot Manipulation](https://arxiv.org/abs/2511.19315)
*Weiliang Tang,Jialin Gao,Jia-Hui Pan,Gang Wang,Li Erran Li,Yunhui Liu,Mingyu Ding,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言模型中间表示方法SEAM，提高了机器人操控任务中对人类指令的理解与泛化能力，并在多项实验中获得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有将人类指令转为机器人可执行表示的方法，在可理解性和泛化性间难以两全。因此需要新的表达方式，提升既易于VLM理解，又具较强泛化能力。

Method: 受上下文无关文法启发，提出SEAM表示，将中间表示分解为词汇和语法两部分，实现语义丰富的操作词汇和对多样任务友好的语法结构。此外，设计了新的开放词汇分割范式结合检索增强的小样本学习，用于定位操控相关微小部件，并引入新的评估指标。

Result: SEAM在可理解性和泛化性新指标下优于主流方法，且在实际多任务实验中都表现出领先性能，并拥有最短推理时间。

Conclusion: SEAM能在保证VLM友好的同时兼具良好泛化能力，有效提升了机器人操控的指令转译表现，是视觉-语言操控领域的一项新SOTA。

Abstract: Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.

</details>


### [462] [Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism](https://arxiv.org/abs/2511.19377)
*Mamoon Aamir,Mariyam Sattar,Naveed Ur Rehman Junejo,Aqsa Zafar Abbasi*

Main category: cs.RO

TL;DR: 论文提出了一种新型可展开天线桁架机构TSDTM，用于空间天线系统，结构便于运载发射后在轨部署，并通过多种建模与仿真方法验证与优化。


<details>
  <summary>Details</summary>
Motivation: 为满足空间任务对大口径天线的需求，解决大型天线在小型火箭整流罩有限空间中的发射难题，因此开发能够减小发射体积并在轨高效展开的新型伸展结构。

Method: 采用从几何建模、螺旋理论和牛顿方法的运动学分析，特征值及仿真的动力学分析，并用SolidWorks软件验证。材料选择采用支持向量机优化，几何参数则结合机器学习进行选择和优化。

Result: 所提出的TSDTM具备较优的结构动力学性能，仿真模拟与理论分析结果吻合良好。优化后的结构自然频率预测值与仿真值偏差仅1.94%，表明机器学习优化方法效果显著。

Conclusion: TSDTM为空间天线设计提供了一种高效部署、结构性能优越的解决方案，验证了AI方法在结构优化设计中的应用潜力，为未来空间结构设计提供了新思路。

Abstract: Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.

</details>


### [463] [Mixture of Horizons in Action Chunking](https://arxiv.org/abs/2511.19433)
*Dong Jing,Gang Wang,Jiaqi Liu,Weiliang Tang,Zelong Sun,Yunchao Yao,Zhenyu Wei,Yunhui Liu,Zhiwu Lu,Mingyu Ding*

Main category: cs.RO

TL;DR: 本文提出了混合时间跨度（Mixture of Horizons, MoH）策略，有效兼顾短期精度与长期前瞻性，提升了视觉-语言-动作（VLA）模型在机器人操作任务上的表现与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在训练时选用的固定动作分段长度（horizon）导致性能存在明显权衡：较长的horizon提升全局前瞻性却损失局部精度，较短的horizon则相反。因此，单一固定horizon并不能带来最优表现。论文旨在解决这一trade-off问题。

Method: 作者提出Mixture of Horizons (MoH)方法，将动作分段为多个不同长度（horizons）的区间，通过共享的动作Transformer并行处理，再用轻量级线性门融合输出。该策略可直接插拔于全注意力动作模块，支持动态自适应推理。

Result: MoH在多种策略（流式策略π_0、π_{0.5}与单步回归策略π_{reg}}）上，在模拟和真实场景下都带来显著且一致的性能提升。尤其在混合任务设定下，π_{0.5}结合MoH在LIBERO数据集上仅用30k训练迭代即达到99%的平均成功率，打破现有SOTA。

Conclusion: MoH方法能够联合利用不同horizon的优势，在不显著增加计算开销的前提下，提升VLA模型在复杂任务下的表现和泛化能力，对VLA领域的机器人操作任务有广泛影响和应用潜力。

Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

</details>
