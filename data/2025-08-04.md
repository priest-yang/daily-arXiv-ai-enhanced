<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为QME（基于质量引导的专家混合评分融合框架）的全身多模态生物特征识别方法，通过引入可学习的评分融合策略，提升多生物模态识别准确率，并在多个数据集上取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前的全身生物特征识别依赖于分别处理不同模态（如人脸、步态、人体），最后通过简单的分数融合实现识别。然而，这些分数融合方法忽视了不同模态分数分布的差异，导致融合效果不佳。为此，亟需一种能根据模态分数质量智能调整融合策略的新方法。

Method: 作者提出了一种基于专家混合（MoE）的可学习分数融合框架QME，结合了模态特定的质量估计器（QE）和伪质量损失函数以及三元组损失，用于提升分数空间对齐与判别性，从而学习更优的多模态融合策略。

Result: 在多个全身多模态生物特征识别数据集上进行了广泛实验，QME在各项评价指标上均优于传统基线方法，达到了最新的性能水平。

Conclusion: QME框架有效解决了多模态融合过程中分数分布不一致和数据质量变化等关键难题，能在多模态、多模型识别场景下推广应用，显著提升识别性能。

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [2] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 本文提出了一套用于评估动作识别模型“运动迁移能力”的基准与框架，并在三个包含合成与真实动作场景的数据集上系统评测13种主流模型，发现模型在新颖上下文中识别高层次动作时性能大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别模型普遍在已知分布上具备较强泛化能力，但尚不清楚它们能否将高层运动概念迁移至不同甚至相似分布的新场景中。作者关注模型能否识别有变体的复杂动作，如将“punching”泛化到“punching person”等新情景。

Method: 作者构建了一个运动迁移能力评测框架，并设计了三个数据集：合成3D动作数据Syn-TA、以及由Kinetics400和Something-Something-v2两个真实视频集改造而来的TA版本。通过在这些基准上评测13种动作识别顶尖模型，系统地分析了它们在新情境中的表现与难点。此外，作者考察了动作粗细粒度解耦在时序挑战性大的数据中的作用。

Result: 主要发现包括：1）多模态模型对较细粒度的未知动作迁移困难更大；2）无偏差的Syn-TA数据集同样具高度挑战性，模型在受控条件下反而性能下滑更明显；3）大模型在空间主导任务上提升明显，但在需复杂时序推理时效果有限，对物体和背景线索的依赖会阻碍泛化能力。

Conclusion: 该研究提出了运动迁移能力的评测新基准，量化了现有模型迁移能力的短板与挑战，并为后续促进模型时序推理泛化能力、动作粒度理解等提供了实验平台和数据资源。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [3] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: 该论文发现当前主流的视觉-惯性里程计（VIO）和SLAM系统在头戴设备的一些极端或现实情境下表现有限，因此提出了公开的Monado SLAM数据集，旨在促进研究和提升实际应用表现。


<details>
  <summary>Details</summary>
Motivation: 现有VIO/SLAM方法虽然技术先进，但在现实头戴应用中，面对高强度运动、动态遮挡、长时间追踪、低纹理区域、不良光照和传感器饱和等情况时仍然捉襟见肘。此外，目前公开的数据集很少包含这些情况，导致系统无法有效泛化到实际应用中。

Method: 作者收集并整理了涵盖上述现实挑战情景的追踪序列，内容来自多个虚拟现实头戴设备，并将其整理成Monado SLAM数据集，作为公开资源发布。

Result: Monado SLAM数据集包含了各种此前数据集难以覆盖的复杂、现实问题场景，能够更好地反映头戴设备追踪中的实际挑战。该数据集已经通过CC BY 4.0协议公开发布，方便研究者使用。

Conclusion: 公开的Monado SLAM数据集将推动VIO/SLAM算法在头戴场景下的研究进步，有助于算法更加健壮和适配实际应用环境。

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [4] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的新模型，利用眼部周围区域（periocular region）的彩色图像进行性别分类，取得了高准确率，为安全和监控等领域的实际应用提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 现有的性别分类方法常受到化妆和伪装等因素影响，导致准确率降低。眼部周围区域较难被改变或伪装，含有丰富的性别信息，因此探索其在性别分类中的有效性成为研究动机。

Method: 作者提出了一种复杂的CNN模型，专门分析眼睑、眉毛及其之间的眼周区域彩色图像。模型在CVBL和(Female and Male)两个数据库上进行了训练和测试，同时与其他先进方法进行了多指标比较。

Result: 该模型在CVBL数据库上获得了99%的准确率，在(Female and Male)数据库上达到96%的准确率，且参数量仅为7,235,089。

Conclusion: 所提出的CNN模型在性别分类任务中表现优越，显著优于现有方法，具有良好的实际应用潜力，特别适用于安全与监控等对性别识别有需求的场景。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [5] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: 提出了一种新的视频生成模型评估指标WCS（World Consistency Score），该指标通过整合四个可解释子指标，综合评估生成视频的内部世界一致性，并与人类判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估方法更关注视觉质量或文本一致性，很少考虑长时间内物理和逻辑上的世界一致性。为此，作者提出WCS，希望更准确地反映生成视频模型在维护“世界”连贯性方面的能力。

Method: WCS包括四个子分数：物体永久性、关系稳定性、因果逻辑遵从和闪烁惩罚，分别以跟踪、动作识别、CLIP嵌入和光流等开源工具计算；再通过基于人类偏好的学习加权方式，融合为单一分数。方法还包括与现有指标的实验对比、灵敏度分析和在多个基准上的相关性测试。

Result: 实验表明，WCS与人类主观评判有较高相关性，相较于旧指标（如FVD、CLIPScore等），WCS能更全面、准确地评估生成视频的世界一致性。

Conclusion: WCS提供了一套全面且可解释的生成视频模型评估框架，弥补了现有方法在世界一致性评价方面的不足，有助于推动视频生成领域发展。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [6] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer是一种结合好奇心驱动机制的新型主动地理定位（AGL）方法，能提升在不确定环境下的目标定位和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AGL方法通常依赖基于距离的强化学习奖励，但在距离估算困难或遇到新环境和目标时，表现出泛化能力差、鲁棒性弱的问题。主要原因是探索策略较为单一、不够多样。

Method: 提出GeoExplorer，通过融合好奇心驱动的内在奖励，使得探索过程与具体目标无关，从而促进更有效、多样和与环境上下文相关的探索。与传统只依赖距离奖励的方法相比，GeoExplorer依靠自主探索环境模型提升定位能力。

Result: 在四个主流AGL基准上进行大量实验，结果显示GeoExplorer在多样环境下（尤其是定位新目标和未知环境时）具有更好的效果和更强的泛化能力。

Conclusion: 好奇心驱动的探索策略能有效提升AGL任务中的鲁棒性和泛化性，为未来相关方向提供了更具适应性的解决方案。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [7] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种新型的点云表示方法——概率点云（PPC），能够在每个点上附加概率属性来反映测量不确定性，从而提升3D目标检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR点云在远距离、低反射率物体等场景下容易受噪声干扰，精度下降，且现有3D处理流程无法保留测量噪声的不确定性，最终导致感知模型性能下降。

Method: 提出概率点云（PPC）表示方法，为每个点增加概率属性来描述原始LiDAR测量的不确定性；进一步基于PPC设计了推理方法，这些方法可以轻松集成到现有3D推理流程中，用于提升鲁棒性。

Result: 通过仿真和真实数据测试，PPC方法在各种具有挑战性的场景下，如小物体、远距离或低反射物体、强环境光条件下，均优于现有LiDAR及融合模型方法。

Conclusion: 概率点云作为3D场景表示能够有效提升3D目标检测在困难场景下的性能，为将来面向鲁棒性的3D感知任务提供了新方向。

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [8] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 本文提出了一种名为选择性模态转移（SMS）的方法，用以检测多模态模型对图像或文本信息的依赖，揭示当前主流视觉-语言模型更多依赖文本信号而忽视视觉信息。


<details>
  <summary>Details</summary>
Motivation: 越来越多基于视觉-语言模型（VLMs）的医学决策系统能够分析医学影像与文本报告，但存在可能偏向某一模态（常常是文本）的问题，这会影响模型解释性和医学诊断安全性，因此需要针对模型实际所依赖的模态进行系统性评估。

Method: 作者提出选择性模态转移（SMS）方法，通过在二分类任务中对不同标签的样本进行图像或文本替换，来量化模型对每种模态的依赖。实验选择了6个开源VLMs（包括4个通用模型和2个医学专用模型），基于MIMIC-CXR和FairVLMed两个医学影像数据集，通过原始和扰动后模型表现的对比，以及关注机制的定性分析，评估模型模态依赖性和校准性。

Result: 实验发现所有模型在很大程度上依赖于文本输入，甚至在包含互补视觉信息时也是如此。此外，通过关注机制分析进一步证实图像信息经常被文本细节掩盖。

Conclusion: 本研究强调了促进和评估多模态医学模型真正融合视觉与文本信息的重要性，警示不能依赖单一模态信号，需在模型设计和评估时充分保证跨模态信息的整合能力。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [9] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 本文提出了一种利用分级图谱（graded graph）及其层次结构（lineages）来系统构建和分析数学模型架构的新方法，适用于机器学习与计算科学。


<details>
  <summary>Details</summary>
Motivation: 当前在机器学习和计算科学中，如何构建多层次、结构化的模型架构是一个核心难题，尤其是在需要表达多尺度、递归或复杂层级关系时。传统图操作方法在处理大规模多层次结构时效率和表达能力有限，因此需要新的理论体系和工具。

Method: 作者定义了结构化的图谱'lineages'，通过分层递增方式增长，支持指数级增长的节点和边。各层之间以二分图连接，并借鉴多重网格方法中的矩阵约束。提出了'graded graphs'的范畴以及可高效计算的代数操作，如交叉积、笛卡尔积、不交并与函数型，形成分层图谱的低成本变体（二元骨架算子）。还开发了厚化、前沿提升等一元空间紧凑操作。

Result: 通过上述理论，建立了分级图与分层图谱的代数型理论体系。二元骨架算子保有类似但不完全相同的代数与范畴理论性质。作者展示了该方法在深度神经网络（如视觉/特征尺度空间）和多重网格数值计算中的应用实例。

Conclusion: 本方法为构建分层模型架构（hierarchitectures）和在其上实现局部采样、搜索及优化算法提供了统一的理论框架，并兼具高效性和表达能力。预期在机器学习和计算科学等领域具有良好适用性。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [10] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的真实人格识别方法，通过模拟个体化的内部认知，提升对真实人格特质的自动评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动人格识别方法主要依据外部观察，推断出观察者对目标个体的印象，这与目标的真实人格存在较大偏差，导致识别准确率有限。因此，研究者希望寻找更加贴近真实人格的评估路径。

Method: 该方法模拟个体的内部认知机制，从目标对象可获取的简短音视频行为中推理出其个性化认知过程。该认知通过网络权重形式表征，使网络能重现个体特定的面部反应，然后将这些认知构造成包含二维节点与边特征矩阵的新型图结构，并提出了二维图神经网络（2D-GNN）进行人格特质识别。各模块通过端到端策略联合训练。

Result: 实验表明，模拟个体化内部认知并结合2D-GNN的识别方法在自动真实人格评估任务中优于传统基于外部行为观察的方法，有效提升了识别准确率。

Conclusion: 面向真实人格识别问题，本文方法通过模拟内部认知机制和创新的图神经网络结构，实现了对个体的深层次理解，为自动人格识别提供了更准确可行的技术路线。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [11] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: 本文提出了一种高效的方法（SAM-PTx），通过引入冻结CLIP文本特征，提升了Segment Anything Model（SAM）在语义提示下的分割能力，进一步减小参数量，并在多个数据集上获得了优于传统空间提示的新结果。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM在基于提示的分割任务中表现出色，但目前对语义文本提示的潜力挖掘不足，主要还是依赖于点、框等空间性提示。本工作旨在探索利用文本语义提示提升分割模型泛化与效率的可行性。

Method: 作者提出SAM-PTx方法，用冻结CLIP提取的文本嵌入作为类别级语义信息，通过一种轻量级并行文本适配器(Parallel-Text)注入至SAM的图像编码器，仅修改transformer模块中的MLP-parallel分支，保持空间推理路径的稳定。整体结构大多保持冻结。

Result: 在COD10K、COCO及ADE20K的少量标注数据子集上进行实验和消融分析，结果显示引入文本提示后，分割性能优于仅用空间提示的基线方法。并首次在COD10K实现了基于文本提示的分割。

Conclusion: 将语义条件信息注入SAM架构，为模型高效、可扩展的适应性提供了新思路，并且几乎不增加太多计算复杂度。该方法在实践中表现出很好的应用潜力。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [12] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 本文提出利用对象在图像中的位置信息，可以提升小样本图像分类性能，尤其是在存在多重对象或复杂背景时，通过Segment Anything Model（SAM）和无监督前景提取等方法在多个基准上带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 小样本图像分类任务由于样本极少，对抗多物体和复杂背景时准确率下降显著。作者希望通过引入对象定位信息，提升分类表现。

Method: 作者研究了在分类过程中结合对象位置信息的方法，如仅需指定目标像素的Segment Anything Model（SAM），或完全无监督的前景对象提取，检测并分离目标后再进行分类。

Result: 在多个标准数据集上，通过引入对象位置信息的方法带来了显著性能提升。尤其利用SAM或无监督提取，可以无需精细标注也获得较大增益。

Conclusion: 在小样本图像分类任务中，只需简单的对象定位或前景分离处理，即可有效提升模型分类效果，为小样本学习提供了具有实用性的改进路径。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [13] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了一种高效的新型多尺度融合U型Mamba(MSF-UM)模型，用于深度图超分辨率，能够有效降低参数量并提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有卷积神经网络在处理深度图的长距离依赖及全局建模上存在不足，而Transformer虽然能建模全局依赖但计算复杂度高，难以应用于高分辨率深度图。需要一种高效、能兼顾局部与全局信息且计算负担较低的新方法。

Method: 提出MSF-UM模型，将Mamba的高效状态空间建模能力融入到多尺度U型结构中，通过彩色图像引导，并结合残差密集通道关注块与Mamba状态空间模块，实现局部特征提取和长距离依赖建模。采用多尺度跨模态融合来提取和利用彩色图像的高频纹理信息，辅助深度图的超分辨率重建。

Result: 实验表明，该方法在多个公开数据集上都优于现有主流方法，尤其是在大规模深度图超分任务中表现出色。MSF-UM模型在保持较低参数量的前提下实现了更高的重建精度。

Conclusion: MSF-UM模型有效提升了深度图超分辨率任务的性能，实现了高质量、低计算成本的重建，在实际应用中具有较强的泛化能力和应用前景。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [14] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: 本文提出了PointGauss，一种用于Gaussian Splatting三维表示的实时多目标分割新框架，并构建了新数据集DesktopObjects-360。方法在分割效率、精度和多视角一致性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前高斯球划分（Gaussian Splatting）三维表示的多目标分割方法在初始化耗时、分割多视角一致性和数据集规模等方面存在显著不足，影响实际应用效果。

Method: 核心方法包括：(1) 基于点云的高斯原语解码器，可在1分钟内生成三维实例掩码；(2) 基于GPU加速的2D掩码渲染系统，保证不同视角下分割结果一致。同时，提出了复杂多目标、全景360°覆盖和三维评估的新数据集DesktopObjects-360。

Result: 大量实验验证，该方法多视角分割mIoU提升1.89%至31.78%，并且保持了高效率；在新数据集上，展现了大规模、高复杂度场景下的优异表现。

Conclusion: PointGauss方法显著提升了高斯三维分割的效果和效率，为多目标场景三维感知提供了新思路。DesktopObjects-360数据集补齐了现有基准的不足，推动领域发展。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [15] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成式视觉-语言模型（VLMs）持续学习的新方法，通过引入多专家视觉投影器和专家推荐机制，有效结合视觉信息和语言指令，实现对持续新任务的适应，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 已有的持续学习方法在将新任务知识融入VLMs时，往往通过调整视觉投影器来连接视觉编码器和大型语言模型，但忽略了对语言指令的重视，导致模型更关注视觉输入，影响了指令依从性。这种问题在重复类型指令的学习任务中特别突出。

Method: 提出了一个基于指令上下文的多视觉投影器混合框架。每个投影器作为视觉到语言的专门转换专家，由专家推荐机制根据指令语境推荐合适的专家，从而应对新任务。专家剪枝策略进一步减少历史任务积累的多余专家对新任务的干扰。

Result: 实验在多个异构视觉-语言任务上进行，结果表明该方法能更好地生成符合指令要求的回答，持续学习能力明显优于现有方法。

Conclusion: 通过在视觉-语言模型中引入指令感知的多专家混合与推荐及剪枝策略，实现了更优的持续学习表现，有效融合了视觉和语言指令信息。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [16] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文综述了多模态指代分割任务，系统梳理了相关背景、方法、数据集及应用，并对主流场景和基准测试进行了性能比对。


<details>
  <summary>Details</summary>
Motivation: 多模态指代分割用于根据文本或音频表达在不同视觉场景中精准圈定目标物体。随着深度学习尤其是卷积神经网络、Transformer和大语言模型的发展，该领域感知能力显著提升，需求与关注度持续上升，迫切需要对现有研究和进展进行总结梳理。

Method: 论文首先介绍了多模态指代分割的基本概念、问题定义和常用数据集，然后提出统一的元架构，对图像、视频、三维场景三大类主流视觉场景下的代表方法与挑战性问题（如广义表达）进行详实综述，并探讨相关任务及实际应用。

Result: 系统总结了各类主流方法在标准基准上的表现，归纳了其优缺点，并提供了横向对比，为研究者提供全面参考。

Conclusion: 本综述全面梳理了多模态指代分割的最新进展，有助于推动该领域进一步发展，并为实际应用和未来研究提供指导。

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [17] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 本文提出了一个针对恶劣条件下语义对应任务的全新基准数据集，并系统评估了当前方法的鲁棒性。结果显示：现有方法在困难环境下性能显著下降，大模型提升鲁棒性但微调有副作用，不同模型融合可增强鲁棒性，通用数据增强对该任务作用有限。


<details>
  <summary>Details</summary>
Motivation: 尽管当前语义对应在理想环境下取得了很大进步，但其在现实中常见的复杂、不利条件下的鲁棒性研究不足，因此有必要为该研究领域建立新的测试基准并深入分析其表现。

Method: 作者构建了一个包含14种常见恶劣场景（比如几何畸变、模糊、数字伪影和遮挡）的新数据集，并用该数据集系统评测了各类语义对应算法，比较了大模型与主流方法、不同模型融合，以及各种数据增强策略的效果。

Result: 1. 所有方法在恶劣条件下性能明显下降；2. 大规模视觉模型增强了整体鲁棒性，但微调后相对鲁棒性反而下降；3. DINO模型优于Stable Diffusion，二者融合表现更好；4. 传统的数据增强对提升鲁棒性帮助有限。

Conclusion: 现有语义对应方法在恶劣条件下的鲁棒性不足，提升鲁棒性需要开发面向任务的专用策略，单纯依赖数据增强或者模型微调难以解决实际问题。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [18] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 本论文提出了一种结合空间自注意力与LSTM的新框架，利用联邦学习以保护隐私且提升驾驶员疲劳检测准确率，并在多样化真实环境数据下取得了89.9%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳驾驶是导致交通事故和死亡的重要原因，然而在实际环境下由于数据分散且人脸信息差异大，准确检测驾驶员疲劳具有挑战性，因此需要开发兼顾隐私保护且能够处理多样性数据的有效检测方法。

Method: 提出结合空间自注意力机制（SSA）与长短期记忆网络（LSTM）的新检测框架，用于提取关键人脸特征。采用联邦学习并引入梯度相似度对比（GSC）机制，挑选各参与方中最相关的模型再聚合，以提升全局模型性能和隐私保护。同时开发了自动化工具进行视频帧提取、人脸检测、数据增强等处理。

Result: 在联邦学习设置下，提出的方法达到了89.9%的疲劳检测准确率，在多种部署场景下都优于现有技术。

Conclusion: 本方法能够有效应对实际应用中数据多样性和分散性问题，提升了疲劳检测的准确性和鲁棒性，具备在智能交通系统中推广应用的潜力，有助于提升道路安全。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [19] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了TITAN-Guide，一种在无需反向传播的情况下高效优化扩散模型隐变量的新方法，有效解决了现有无训练指导方法在内存占用和控制效果上的不足，显著提升了文本到视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有有条件扩散模型在特定任务上需要大量有监督微调，而训练无关的指导方法虽然可以避免微调，但往往存在内存消耗大或控制效果不佳的问题，限制了其在计算密集型T2V模型上的应用。

Method: 提出TITAN-Guide方法，通过前向梯度下降等技术，在不依赖判别模型反向传播的前提下，对扩散模型隐变量进行高效优化，从而实现训练无关的高效指导。

Result: 实验验证了所提方法在隐变量优化时的内存管理效率优于现有方法，并大幅提升了T2V扩散模型在多个指导基准上的性能。

Conclusion: TITAN-Guide显著降低了现有无训练指导方法的内存需求，提高了扩散模型的控制力和应用普适性，推进了高效文本到视频生成研究。

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [20] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出了AniMer+，一种融合高容量ViT与MoE的新型家族感知网络架构，并结合大规模合成数据集，实现了哺乳动物和鸟类统一3D重建，在多项基准测试中优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前，生物研究需对大量不同物种进行定量姿态与形态分析，但受限于模型通用性以及多物种数据集匮乏，相关工作发展不足。提升网络统一建模能力和数据多样性是亟待解决的问题。

Method: 1）设计了高容量、分家族与共享特征混合的ViT+MoE架构，实现对哺乳类与鸟类统一建模。2）利用扩散模型生成了大规模、带有3D注释的合成数据集CtrlAni3D（哺乳动物）和CtrlAVES3D（鸟类），扩展了训练样本。3）在真实与合成数据混合的41.3k哺乳动物及12.4k鸟类样本上进行训练，并开展消融实验。

Result: AniMer+在多个基准测试（包括更具挑战性的Animal Kingdom数据集）上，均显著超越现有方法。消融实验验证了新型网络结构与合成数据集在实际应用中的效果提升。

Conclusion: AniMer+为多物种统一3D姿态与形态估计提供了高效方案，解决了网络容量与数据不足难题，推动了动物空间智能的研究与实际应用。CtrlAVES3D亦填补了鸟类3D数据集空白。

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [21] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多视角行人视频可控编辑框架，结合了视频修复与人体运动控制技术，用于提升自动驾驶场景下行人检测模型的鲁棒性。该方法能高质量地进行行人的插入、替换和移除，实现优良的视觉真实感和时空一致性，对数据增强和场景仿真有广泛应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统中的行人检测模型由于训练数据集中危险行人场景样本不足，导致其在复杂环境下鲁棒性不足。为克服此问题，需要一种能够灵活编辑和生成多样化行人场景的视频编辑框架，以提升模型的训练效果和泛化能力。

Method: 该方法首先在多摄像头视角中识别出感兴趣的行人区域，通过扩展检测框并对这些区域进行统一尺寸调整和拼接，保持跨视角的空间关系。随后，利用二值掩码标记可编辑区域，在此基础上通过姿态序列控制进行定向行人编辑，实现插入、替换和移除等功能。核心技术包括跨视角视频修复与姿态控制驱动行人动作变化。

Result: 实验表明，该框架能够以高质量实现行人插入、替换和移除，生成的视频在视觉真实感、时空连续性和多视角一致性方面表现突出。相关实验覆盖了数据增强和仿真等任务，充分验证了方法的有效性和实用性。

Conclusion: 提出的方法为多视角行人视频的可控编辑提供了鲁棒且多样的解决方案，极大提高了相关模型的数据增强和场景仿真的能力，有望广泛应用于自动驾驶领域的数据生成与复杂场景测试中。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [22] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种利用事件相机提升低光照图像增强的新方法，通过解耦可见度恢复和结构细化两个阶段，充分发挥多模态传感的互补优势，最终优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的低光照图像增强方法未能充分利用不同传感模态（帧图像和事件流）的特性，仅简单地将两者输入同一模型，导致性能受限。作者分析发现帧和事件流分别在可见度恢复和结构信息重建方面具有优势，因此希望设计更有针对性的增强流程。

Method: 方法分为两阶段：第一，设计基于傅里叶空间振幅-相位耦合的可见度恢复网络，提升整体视觉亮度和清晰度；第二，提出带动态对齐的融合策略，解决帧和事件流因时间分辨率不同导致的空间错位，细化结构信息。此外，引入空间-频率插值模拟多样退化，并结合对比损失提升判别能力。

Result: 实验证明，所提方法在低光照图像增强任务上超过了现有的主流模型，表现出更好的视觉效果和结构还原能力。

Conclusion: 通过解耦增强流程与创新的多模态融合策略，并辅以新颖的训练损失，本文方法充分发挥了事件相机的潜力，有效提升了低光照图像增强性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [23] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于通用视觉-语言模型的数学公式OCR方法（DocTron-Formula），并引入了大规模复杂公式数据集（CSFormula），在多种样式和复杂布局下取得了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数学公式OCR方法或通用视觉-语言模型，难以应对数学内容中结构多样性、复杂性及现实场景变动，难以满足科学文献智能分析的需求。

Method: 作者将一般视觉-语言模型用于数学公式OCR任务，无需特殊定制架构，通过简单监督微调实现对多领域复杂公式的识别。同时，构建并发布了包含行级、段落级和页面级多学科复杂公式的大规模数据集CSFormula。

Result: 在多种科学领域和复杂排版风格的数据上，经过微调后的模型取得了最优性能，准确性和鲁棒性优于现有专用模型。

Conclusion: 该方法无需专用架构，展现出优越的通用性和准确性，为复杂科学文档自动理解树立了新范式。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [24] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本条件生成模型的弱监督视频异常检测方法，通过合成视频增强训练数据，并且采用损失缩放策略优化训练，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实中异常事件稀少且注释成本高，导致视频异常检测的数据集难以扩展，限制了模型的性能和泛化能力。

Method: 作者提出GV-VAD框架，利用文本条件视频生成模型，生成语义可控且物理合理的异常合成视频来扩充训练数据，并引入合成样本损失缩放策略以调节合成数据对训练的影响。

Result: 在UCF-Crime数据集上，所提方法优于现有最新方法，在多项指标上取得更好表现。

Conclusion: 本文通过生成模型和数据增强有效缓解了异常数据稀缺问题，提升了视频异常检测的准确率和泛化能力，为实际应用提供了更优方案。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [25] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 本文提出了一种个性化引导方法，可在保持文本文本可编辑性的同时，更好地适应特定目标概念，提升个性化文本到图像扩散模型的生成表现。


<details>
  <summary>Details</summary>
Motivation: 针对个性化文本到图像扩散模型微调时，存在目标分布匹配（如主体忠实度）与原模型泛化能力（如文本可编辑性）之间的权衡难题，当前常用的采样引导方法，如无分类器引导（CFG）和自动引导（AG）均无法实现两者的良好平衡。

Method: 作者提出了个性化引导（personalization guidance）方法：利用一个在无文本提示下未学习到目标信息的弱模型，并在推理阶段通过预训练模型与微调后模型的权重插值，动态调控弱模型的去学习程度，从而更有效地在目标分布对齐与文本可编辑性之间取得平衡。

Result: 实验结果显示，该方法不仅提升了生成图像的文本对齐度和目标分布忠实度，而且可与多种微调策略无缝集成，无需额外计算开销。

Conclusion: 个性化引导方法能够显著缓解个性化微调过程中的权衡问题，提升文本到图像扩散模型的个性化表现和实用性。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [26] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 本文提出一种利用衍射光栅实现相机光谱敏感度标定的新方法，无需昂贵器材，准确且实用。


<details>
  <summary>Details</summary>
Motivation: 准确标定相机的光谱敏感度对于颜色校正、光照估计和材料分析等计算机视觉任务至关重要，现有方法往往需要特殊设备或已知反射率的参考物，使用复杂、不便于实际推广。

Method: 方法仅需一个市售的未标定衍射光栅薄片，通过拍摄直接照明和经光栅后的衍射图案，推导出一种封闭解法，可同时估算相机的光谱敏感度和光栅参数。

Result: 在合成数据和真实数据上的实验均显示，该方法优于传统依赖参考物的标定方法，证明了其实用性和有效性。

Conclusion: 所提出的方法无需昂贵或特殊设备、简易可行、精度更高，为相机光谱敏感度标定提供了一种有效替代方案。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [27] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 本文针对聚光太阳能电站（CSP）中无人机航拍图像难以使用通用数据集进行模型训练的问题，提出并发布了一个高质量的CSP虚拟合成数据集（AerialCSP），显著提升了CSP场景下的目标检测与分割效果。


<details>
  <summary>Details</summary>
Motivation: CSP电站的航拍图像具有高度反光和特定结构，现有通用数据集和预训练模型泛化能力差。人工采集和标注不仅耗时，还成本高，制约了工业快速部署。

Method: 作者通过仿真技术，生成拟真CSP无人机航拍数据AerialCSP，并包含详细目标检测与分割标签。同时，在该数据集上对多种视觉模型进行基准测评，探索预训练对实际故障检测能力提升。

Result: 实验证明，基于AerialCSP数据集的预训练，能够在实际CSP场景中尤其是对稀有和微小缺陷检测能力有显著提升，减少了对大量人工标注数据的依赖。

Conclusion: AerialCSP虚拟数据集为CSP行业智能视觉检测提供了标准化基线和便利的预训练方案，对工业应用和相关模型研究具有重要意义。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [28] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种基于协作智能体的多图像推理框架，通过PromptEngineer与VisionReasoner两智能体的分工协作，在多模态、多任务下无需训练即可实现高效推理，对多图像输入的分类、问答及生成等任务均表现出色，在多个数据集上取得了接近最优的结果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理任务特别是需要对多图像进行复杂推理的场景，存在数据和任务多样、模型推理能力有限等挑战，且对训练和适应性要求高。本文旨在提出一套通用、自动化且无需训练的框架，提升LVLM模型在多图像场景下的推理能力和适应广泛任务的泛化性能。

Method: 构建了由两大智能体（PromptEngineer 和 VisionReasoner）组成的协作推理体系：PromptEngineer根据任务生成有针对性的上下文提示，VisionReasoner（大型视觉-语言模型）负责最终的多图像推理。在18个涵盖分类、问答、生成等多类型任务的数据集上进行系统性评估，并分析不同设计选择（如模型类型、示例数量、输入长度）对结果的影响。

Result: 框架实现了无需训练即可对多张图片、多种任务形态有效推理。以Claude 3.7为代表的LVLM在TQA、DocVQA、MMCoQA等多项复杂任务上取得了接近最佳的成绩（如TQA 99.13%准确率、DocVQA 96.87%、MMCoQA 75.28 ROUGE-L）。

Conclusion: 通过智能体协作与精心设计的上下文提示，可以大幅提升LVLM在多图像推理任务中的表现。不仅验证了方法的通用性和高效性，还为多模态与多任务推理模型的未来发展提供了新方向。

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [29] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出了一种新颖的、可扩展的基于多模态嵌入的运动检索方法，并构建了新数据集WayMoCo，在极端或复杂人类行为场景的检索上大幅提升精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在涉及弱势道路使用者（如行人等）异常或复杂行为的安全关键场景下可靠运行，而这类稀有长尾行为场景在大规模驾驶数据中难以检索，因此亟需有效方法来发现和评估这些关键场景。

Method: 提出基于SMPL（Skinned Multi-Person Linear）的人体运动序列和对应视频帧，将其编码到与自然语言对齐的多模态共享嵌入空间，实现通过文本检索目标人类行为及其上下文；同时提出WayMoCo数据集，含自动标注的运动与场景描述，数据来自Waymo Open Dataset自动生成的伪真值SMPL序列及图像。

Result: 实验结果显示，所提出方法在WayMoCo数据集上的运动-场景检索任务中，其准确率比当前最优方法提升最多达27.5%。

Conclusion: 所提出的多模态运动检索框架显著提升了对极端复杂人类行为场景的检索能力，有助于针对性地评估和改进自动驾驶系统的安全和泛化性能。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [30] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: 本文提出了一种速度引导的可学习卡尔曼滤波器（SG-LKF），能够据自车速度动态调整跟踪模型的不确定性，显著提升多目标跟踪在动态高速场景下的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于检测的多目标跟踪方法普遍采用静态的坐标变换，忽视了由于自车速度带来的观测噪声变化和参考系的切换，导致在高速动态场景下跟踪表现不稳定且精度下降。

Method: 作者提出了速度引导的可学习卡尔曼滤波器（SG-LKF），其核心是MSNet网络，可以根据自车速度自适应地预测卡尔曼滤波器的关键参数。同时，为提升帧间关联和轨迹连贯性，引入了自监督轨迹一致性损失，并与语义和位置约束联合优化。

Result: 在KITTI 2D MOT上SG-LKF以79.59%的HOTA指标位列所有视觉方法第一，在KITTI 3D MOT上获得82.03%的HOTA，在nuScenes 3D MOT上比SimpleTrack高2.2%的AMOTA。

Conclusion: SG-LKF能根据自车速度动态调整跟踪不确定性建模，大幅提升多目标跟踪在高速动态场景下的精度和稳定性，在多个权威基准数据集上取得了领先性能。

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [31] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D高斯定位导航框架（IGL-Nav），显著提升了基于图像目标的视觉导航效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉导航方法对于图像作为目标定位时，无法充分建模3D环境与目标图像之间的几何关系，且基于3D高斯表达的直接优化在探索阶段计算代价过高，实际运行效率低。

Method: 作者提出IGL-Nav：在探索过程中利用单目相机逐步增量式更新3D高斯场景表示，先通过几何信息做粗定位（高效的离散空间匹配，相当于高效3D卷积），之后在接近目标时再通过可微渲染优化精确目标位姿。

Result: IGL-Nav在多种实验设置下大幅超越当前方法，同时可适应更自由的视角目标，并能在实际机器人平台上用手机捕获任意姿态的目标图像进行导航。

Conclusion: IGL-Nav框架在精度、效率和适应性方面大幅提升了基于图像目标的视觉导航，有良好的实际部署前景。

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


### [32] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: 提出了联合空间和时间的协同感知方法（CoST），提升多智能体感知任务的效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 单一智能体在感知中面临遮挡、感知范围有限等问题。现有协同感知方法多将多智能体融合和多时刻融合分开处理，效率、鲁棒性存在局限。

Method: CoST方法提出将不同智能体、不同时间的观测信息同时融合到统一的时空空间。具体利用Spatio-temporal Transformer进行特征融合和信息传递，从而实现高效协作感知。

Result: CoST在效率和感知准确率上均有提升；每个静态物体只需信息传输一次，减少带宽消耗，同时整体融合提升了特征表达强度，在复杂场景下表现更优。

Conclusion: CoST方法提升了协同感知的效果和效率，具备通用性，可直接提升现有多种方法的准确性且减少传输开销。

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [33] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 本文提出一种基于机器学习方法自动分类蜂蜜植物来源，采用高光谱成像数据集，取得了目前最优的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 准确判断蜂蜜的植物来源对蜂蜜品质鉴定和市场监管具有重要意义，传统方法复杂且低效，亟需高效、自动化的分类方法。

Method: 方法分为三步：（1）数据集准备，采用类别转换方法提高类别可分性；（2）特征提取，使用线性判别分析（LDA）减少维度并提取有效特征；（3）分类，利用支持向量机（SVM）和K近邻（KNN）对提取特征进行分类。

Result: 在标准的蜂蜜高光谱成像数据集上，模型达到了95.13%的图片级分类准确率和92.80%的实例级分类准确率，优于现有方法。

Conclusion: 所提出的方法能够高效、准确地自动判别蜂蜜的植物来源，在相关领域具有重要应用价值。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [34] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 本文提出了一种新方法SparseRecon，能够在少量视角下重建高质量的3D场景，解决了现有方法泛化性不足或重建细节有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么在未见视角下泛化性较差（泛化类方法），要么几何细节有限（过拟合类方法），导致从稀疏视角重建三维场景质量不高。

Method: 提出SparseRecon，结合基于体渲染的特征一致性损失与不确定性引导的深度约束。特征一致性损失强化不同视角间的约束，提升重建完整性与流畅性，不确定性引导的深度约束，在遮挡和特征不明显区域辅助细节恢复。

Result: 实验显示，SparseRecon方法在小重叠区域等稀疏视角场景下，相比最新方法，能够有效提升3D几何重建的质量。

Conclusion: SparseRecon在稀疏视角输入条件下显著改善了三维重建的效果，优于现有最先进方法，具有较强实际应用潜力。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [35] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文介绍了一种新的无训练、模型无关的度量方法——Representation Shift，可实现与FlashAttention兼容的token压缩，加速Transformer等模型训练且无需关注权重图。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性提升，Transformer模型和token数量持续增长，导致自注意力机制计算和GPU内存占用急剧上升。现有token压缩方法往往依赖attention map，无法与高效的FlashAttention等内存优化方法兼容。因此，亟需一种无需attention map也能进行有效token压缩的新方法。

Method: 作者提出Representation Shift指标，用以衡量每个token表示的变化程度。通过该指标，无需attention map即可筛选掉冗余或不重要的token，实现与FlashAttention的无缝结合，无需额外训练和模型修改。该方法也可泛化到CNN和状态空间模型。

Result: 实验证明，Representation Shift方法在与FlashAttention结合使用时，在视频-文本检索、视频问答等任务上，可实现高效token压缩，分别带来5.5%和4.4%的速度提升。

Conclusion: Representation Shift是一种简单、高效、通用的token压缩方法，可兼容现有高效注意力实现，提升大型模型在复杂任务下的计算效率，具有良好的实用性和推广前景。

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [36] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: 本文提出BiAnt方法，通过结合正向预测和逆向预测提升视频中长时动作预判的准确性，实验表明优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作预判方法多为单向（一般自过去到未来），难以捕捉场景中的语义上有区分的子动作，导致预判能力受限。解决这一问题对于自动驾驶、机器人等领域的风险提前预警非常重要。

Method: 提出BiAnt方法，首次将正向（从过去到未来）和逆向（从未来回到过去）预测结合起来，利用大语言模型增强对动作序列的全局建模能力。这样有助于捕捉语义层级复杂的子动作关系。

Result: 在Ego4D数据集上，BiAnt方法在edit distance指标上相较于基线方法有明显提升，展示了更优的动作预判能力。

Conclusion: 双向动作预测可以更好地刻画动作的时序和语义特征，对于提升长时动作预判具有实用价值。

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [37] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出了一种改进的焊接缺陷检测框架“Adapt-WeldNet”，提升了检测精度和可解释性，增强了在海洋及离岸等复杂环境下的使用可靠性。


<details>
  <summary>Details</summary>
Motivation: 油气行业中管道的安全依赖于高效的焊缝缺陷检测。传统NDT方法对微小或内部缺陷识别能力有限，现有神经网络模型又依赖随意选择的结构且可解释性差，存在部署风险。

Method: 系统性评估多种预训练网络架构、迁移学习策略及自适应优化器，通过实验优化获得最佳模型及超参数；提出Defect Detection Interpretability Analysis (DDIA)新框架，结合Grad-CAM、LIME等XAI技术以及ASNT NDE Level II专家的领域评估，采用Human-in-the-Loop并贯彻可信AI原则。

Result: Adapt-WeldNet在焊接缺陷识别准确率及鲁棒性方面优于现有方法，并通过DDIA显著提升了模型的可解释性和专家认可度。

Conclusion: 该方法兼顾性能和可解释性，提升了焊接缺陷检测系统在实际复杂环境中的可信度、安全性和可靠性，有助于油气行业等关键场景的运维保障。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [38] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 本文提出了一种结合状态空间模型（SSM）和Vision Transformer（ViT）的混合骨干网络MV_Hybrid，用于从常规组织病理切片图像中预测空间转录组信息，在生物标志物预测等多项任务中均优于传统ViT架构。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术能揭示肿瘤组织中的基因表达模式，但成本高、技术复杂，难以广泛应用。用常规病理图像替代此技术进行分子层面预测，将极大促进精准医疗，但现有依赖ViT的视觉基础模型表现不理想。作者认为，通过骨干网络创新，可以更好捕捉低频和细微形态特征来提升分子特征预测能力。

Method: 提出融合状态空间模型（SSM）和Vision Transformer（ViT）的MV_Hybrid骨干网络，并与五种不同架构（同样以DINOv2自监督在结直肠癌数据集预训练）做对比，采用生物标志物随机分割和留一研究验证(LOSO)的泛化能力测试，以及下游分类、检索、生存预测任务评估。

Result: MV_Hybrid在LOSO设置下预测相关性比最佳ViT高57%，性能下降比随机分割小43%，展现出更强的性能和泛化鲁棒性；在分类、检索和生存预测任务上表现同样优于或等同于ViT模型。

Conclusion: MV_Hybrid作为下一代病理视觉基础模型骨干，在空间转录组预测和多项下游任务上均展现优越表现，有望促进空间组学大规模实际应用。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [39] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 本文提出了Cued-Agent，一个多智能体协作系统，用于自动将唇读与手势结合的Cued Speech转写为文本，显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有Cued Speech自动识别方法在多模态融合上因数据量有限导致性能不佳。因此，急需能够在有限数据条件下高效融合手势和唇动信息的新方法。

Method: 作者设计了Cued-Agent多智能体系统，包含手势识别、唇读识别、手势提示融合和语音自修正四个智能体。各模块协同工作，实现高效、准确的手语-唇动识别至文本过程。

Result: 作者扩展了普通话Cued Speech数据集，进行大量实验，结果显示Cued-Agent在常规和听障用户场景下的识别精度均显著优于主流方法。

Conclusion: Cued-Agent多智能体系统有助于在数据有限情况下提升Cued Speech自动识别效果，为听障群体交流提供了更优质的技术支持。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [40] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的Prompt Tuning（PT）框架DAPT，针对视觉-语言模型中视觉和文本信息传递不对称问题，通过显式分离和对齐前景与背景信息，有效提升了模型的任务迁移能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统Prompt Tuning方法未能关注视觉模态通常包含比文本模态更多上下文信息，导致两者对齐时产生信息不对称，从而引起偏置注意力，仅专注于场景上下文，降低了对目标对象的精确捕捉能力。

Method: 提出DAPT框架，核心思路为“先解耦后对齐”。具体做法为：（1）利用粗细不同的视觉分割提示，将视觉信息解耦为前景与背景；（2）前景与原有文本标签对齐，背景与手工构造的背景类别对齐，实现模态对称增强。（3）设计前景-背景专属的视觉pull-push正则，强化模型对区域兴趣目标的非偏置注意。

Result: 在少样本学习、基本-新颖类别泛化、数据高效学习等任务及多项主流视觉-语言基准测试中，DAPT框架取得了优越的性能，验证了其架构无关性与实际有效性。

Conclusion: DAPT显著缓解了视觉-文本信息对齐中的不对称和注意力偏置问题，增强了模型的细粒度目标理解和泛化能力，进一步推动了高效Prompt Tuning方法在视觉-语言领域的应用前景。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [41] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 本文提出了一种基于空间-时间一致性的双分支检测框架，通过结合RGB外观特征与光流残差来有效检测扩散式视频生成模型生成的伪造视频。


<details>
  <summary>Details</summary>
Motivation: 扩散式视频生成技术快速进步，生成内容愈加逼真，现有检测方法难以捕捉微妙的时序不一致，尤其是高保真和运动连贯的AI视频，亟需更高效的检测方法。

Method: 设计了双分支网络架构：一支分析RGB帧以检测外观伪影，另一支处理光流残差揭示时序合成缺陷，通过融合两者实现对伪造视频的有效识别。

Result: 在文本生成视频和图像生成视频任务上，涵盖十种主流生成模型的多组实验表明，所提方法具有优秀的鲁棒性和泛化能力。

Conclusion: 该方法能够可靠检测各类扩散模型生成的伪造视频，对提升视频内容真实性判断具有重要意义。

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [42] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: 本文提出了iSafetyBench，一个专为工业环境下正常与危险场景视频理解设计的视频-语言基准，用于评估VLMs在工业多场景中的表现，并公开了数据集。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言模型（VLMs）在多种视频理解任务上已经取得突破，尤其在零样本设定下的泛化能力。然而，它们在高风险的工业领域中的能力，尤其是同时识别日常操作与安全关键异常的能力，尚未得到充分探讨。因此，需要专门的基准来评估其在工业环境下的表现。

Method: 作者构建了名为iSafetyBench的视频-语言基准，包含来自真实工业现场的1,100段视频，涵盖98种常规和67种危险操作类型。每个视频都配有开放词汇的多标签行动注释及多项选择题，支持单标签和多标签两种评测方式。随后，作者选取八种主流视频-语言模型，在零样本设定下进行评测。

Result: 尽管这些模型在现有视频基准上表现优异，但在iSafetyBench，特别是在检测危险行为与多标签场景下，表现不佳。实验结果暴露出在工业安全领域的显著性能差距。

Conclusion: iSafetyBench揭示了当前VLM在工业场景下的不足，强调了开发更强健且具备安全感知能力的多模态模型的迫切需求。该基准为推动相关研究提供了新的测试平台。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [43] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: 本文介绍了Sari Sandbox，这是一个高保真、逼真的3D零售商店模拟环境，用于在购物任务中对具身智能体与人类的表现进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专为零售应用设计的高质量模拟环境，而这类环境对于训练和评估具身智能体在真实世界购物任务中的能力非常重要。

Method: 提出了Sari Sandbox环境，包含250+可交互杂货商品和3种店面布局，支持API控制、人类的VR交互和基于视觉语言模型(VLM)的智能体。同步推出了SariBench数据集，提供不同难度任务的人类演示标注。

Result: Sari Sandbox使智能体能够在逼真的店面中导航、检查与操作商品，并可以系统性与人类实验基线做对比。文中给出了相关基准测试及性能分析。

Conclusion: Sari Sandbox为具身智能体在零售场景的训练与评估提供了有力工具，并对提升环境的真实性和可扩展性提出了建议，源码已开源以便社区协作。

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [44] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 本论文针对大气湍流导致的远距离动态场景视频的几何畸变和模糊问题，提出了一种新的动态强度量化指标，以及结合物理模型驱动的多阶段视频恢复框架，有效提升了在复杂和高湍流环境下的视频修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强烈湍流和复杂动态条件下，难以恢复清晰细节并解决混合失真，主要挑战包括边缘细节恢复不足和动态区域失真难以消除。研究的动机在于需要更加准确地刻画视频动态强度，并针对性地提升视频恢复能力。

Method: 提出了动态效率指数（DEI），结合湍流强度、光流及动态区域比例，用于量化视频动态强度并构建高动态湍流训练数据集。设计了三阶段物理模型驱动的方法（PMR）：第一阶段为去倾斜稳定几何结构，第二阶段为运动分割增强以优化动态区域，第三阶段去模糊还原画质。该方法采用轻量级网络和分阶段联合训练，兼顾效率与修复质量。

Result: 实验结果表明，所提方法能有效抑制运动拖尾伪影，恢复细致的边缘细节，在高湍流和复杂动态的真实世界场景下展现出优良的泛化和修复能力。

Conclusion: 该方法显著提升了大气湍流复杂动态场景下视频修复效果，效率高、细节恢复强，并将在社区公开代码和数据集，推动领域发展。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [45] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Sortblock的新方法，通过动态缓存和跳过冗余计算，实现对Diffusion Transformers (DiTs) 的无训练加速，能在保持生成质量的基础上显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: DiTs虽然生成能力强，但其顺序去噪设计导致推理延迟高，难以应用于实时场景。现有无训练加速方法只在固定点利用中间特征，忽视了语义关注的动态变化，难以兼顾效率和质量。

Method: Sortblock通过比较相邻时刻的特征相似性，动态缓存和跳过冗余block的计算，依据残差演化对需要重计算的比例自适应排序，并引入轻量线性预测机制，减少跳过block带来的误差积累，实现高效推理。

Result: 在多项任务和不同DiT架构下实验，Sortblock能实现超过2倍的推理加速，同时生成质量几乎没有损失，展现了算法的有效性和通用性。

Conclusion: Sortblock为Diffusion模型提供了一种高效、无需训练的加速方案，在提升速度的同时保障了输出质量，适用于多种生成任务及改进现有推理流程。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [46] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 本文提出了一种新型的深度压缩自编码器DC-AE 1.5，显著提升了高分辨率扩散模型的训练效率与生成质量。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型中，提高自编码器潜在通道数虽然能提升重构质量，但却导致扩散模型收敛变慢、生成质量下降，限制了模型进一步提升。论文为了解决这一瓶颈而提出创新方法。

Method: 作者提出了两个新方法：一是结构化潜在空间，通过训练引入通道结构，让前段通道表征物体结构，后段通道表征图像细节；二是增强扩散训练，在重要通道上加入额外扩散训练目标，加快收敛。

Result: 在ImageNet 512x512数据集上，DC-AE-1.5-f64c128比DC-AE-f32c32生成的图像质量更高，且训练速度提高4倍。

Conclusion: 通过结构化潜在空间和增强扩散训练，DC-AE 1.5显著提升了收敛效率和高质量图像生成能力，有望推动高分辨率潜在扩散模型的进一步发展。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [47] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 本文提出了一种改进视频外扩（outpainting）方法，通过引入分层判别器和专用损失函数，有效提升了生成内容的感知质量和整体连贯性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频外扩技术需要在保持原有内容一致性的同时，拓展画面边界。但现有方法仅关注背景生成，导致外扩部分质量不佳，于是作者尝试利用视频修复模型用于外扩，并寻求提升外扩结果的感知质量。

Method: 作者设计了一种分层判别器，同时考虑全局和局部的生成质量；并结合此判别器提出专用的损失函数，将判别器的全局与局部特征引入训练。通过这些改进对原有视频修复模型进行微调。

Result: 经过改进后的微调模型在生成的外扩边界区域，无论在定量（数值指标）还是定性（视觉效果）上，都优于已有外扩方法。

Conclusion: 通过分层判别器和创新损失函数，显著提升了视频外扩任务中的生成质量，为后续相关研究提供了有效方案。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [48] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Mamba的新型水下实例分割方法UIS-Mamba，并通过引入动态树扫描（DTS）和隐藏状态弱化（HSW）模块，实现了在水下复杂环境中的高效分割，在UIIS和USIS10K数据集上达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 针对水下图像存在的颜色失真、实例边界模糊以及复杂背景等难题，现有Mamba模型的固定块扫描无法保证分割连续性，隐藏状态易受背景干扰，亟需更适合水下实例分割的新方法。

Method: 作者提出了UIS-Mamba模型，在Mamba骨干网络基础上，创新性地设计了动态树扫描（DTS）模块，使扫描块能够动态偏移和缩放，维持实例内连续性，同时通过最小生成树实现动态局部感受野；还设计了隐藏状态弱化（HSW）模块，利用基于Ncut的机制抑制复杂背景对隐藏状态的干扰，提升实例对象聚焦能力。

Result: 在UIIS和USIS10K两个公开水下实例分割数据集上，UIS-Mamba取得了最新SOTA性能，并且模型参数量和计算复杂度均较低。

Conclusion: UIS-Mamba有效解决了水下实例分割中的特有挑战，证明Mamba结合创新模块能够兼顾分割精度与高效性，在水下图像任务中展现了极大的应用潜力。

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [49] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 本文提出了一种面向人-物交互（HOI）场景的创全推断新方法，通过引入物理先验与多区域修复技术，大幅提升了在动态遮挡环境下的目标外观补全能力。


<details>
  <summary>Details</summary>
Motivation: 传统使用扩散模型等方法的amodal completion受限于对HOI理解有限，无法在复杂动态遮挡下生成合理的目标补全结果。为提升机器对人-物交互和动态场景的感知水平，需要引入更具物理意义的先验知识和分区修复策略。

Method: 方法核心分为两步：1）基于人体拓扑结构和接触信息定义主、次两个感兴趣区域，推断被遮挡概率和补全区域；2）在扩散模型基础上，针对主次区域采用不同降噪修复策略，结合物理和结构先验，提高补全细节与真实性。

Result: 实验表明，该方法在典型HOI任务上远优于现有扩散模型类修复方法，补全结果的形状合理性和细节真实度显著提升。此外，即使在无真实接触标注（ground-truth contact annotation）条件下，方法依然表现出良好鲁棒性。

Conclusion: 该方法将视觉推断与物理知识有效结合，推动了机器对动态人-物交互场景的理解能力，并拓宽了其在3D重建、新视角合成等领域的应用空间。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [50] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种针对管状结构分割任务的测试时自适应方法（TopoTTA），显著提升了在未知领域中的分割表现。


<details>
  <summary>Details</summary>
Motivation: 尽管管状结构分割（TSS）在各领域如血流动力学、路径导航中很重要，但其对领域变换特别敏感，出现性能下降。本论文旨在缓解由于拓扑结构和局部特征变化导致的分割失效这一关键问题。

Method: 提出Topology-enhanced Test-Time Adaptation（TopoTTA）框架，包括两阶段：第一阶段利用新颖的拓扑元差卷积（TopoMDCs）自适应调整网络的拓扑表征，且不改变原有预训练参数；第二阶段通过生成拓扑难例（TopoHG）并在这些难例上用伪标签预测对齐，进一步提升拓扑连续性。

Result: 在四种场景、十个数据集上进行了广泛实验，TopoTTA有效应对了拓扑分布变化，实现了平均31.81%的clDice提升。

Conclusion: TopoTTA不仅有效提升了TSS在未知领域的性能，还能作为现有CNN分割模型的即插即用测试时自适应解决方案。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [51] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: 该论文提出了SDMatte，一种基于扩散模型的交互式抠图新方法，有效提升了边缘细节提取能力。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式抠图方法在提取物体主要区域效果良好，但难以捕捉边缘的细粒度细节。而扩散模型在复杂图像建模和细节合成方面表现优异，因此有望提升抠图质量。

Method: 1）将扩散模型的文本驱动交互能力转化为视觉提示驱动，实现交互式抠图；2）将视觉提示的坐标嵌入和目标物体的不透明度嵌入融合进U-Net，增强模型对空间位置和不透明度的感知；3）引入掩码自注意力机制，使模型关注由视觉提示指定的区域。

Result: 在多个数据集上进行了大量实验，SDMatte在抠图性能上优于现有方法，验证了其有效性。

Conclusion: SDMatte利用扩散模型强大的先验能力和创新结构设计，显著提升了交互式抠图的细节处理能力和整体表现，具有广泛的应用前景。

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [52] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了AutoDebias框架，能自动检测并缓解文本到图像生成模型中的多种偏见，无需事先知道具体偏见类型，并在保持图像质量的同时，大幅降低偏见输出。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型虽然生成高质量图片，但容易无意中反映社会偏见（如性别、种族刻板印象），而现有去偏见方法难以应对复杂或隐晦的偏见问题。

Method: AutoDebias利用视觉-语言模型来自动发现带有偏见的视觉模式，并通过生成包容性的替代提示语，构建公平性参考指南，结合CLIP引导的训练过程，引导模型生成更为公平的输出。该方法无需预先知晓偏见类型，可以有效应对多个同时存在或隐晦的偏见。

Result: 在包含25种以上偏见情境的基准测试中，AutoDebias能以91.6%的准确率检测有害模式，将偏见输出比例从90%降至可忽略水平，同时保持原模型的图像保真度和多样性。

Conclusion: AutoDebias无需事先定义偏见类型，能高效检测和减缓文本到图像模型中的复杂刻板印象和多重交互偏见，为生成公正图像提供了有效自动化工具。

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [53] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了EgoMask，这是第一个针对自我中心（egocentric）视频的像素级时空定位基准，旨在通过文本查询在视频中精确定位目标实体。实验发现，现有技术在该基准表现较差，但经过针对性的训练后，效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 以自我中心视角采集的视频在增强现实和机器人等应用场景中越来越重要，但时空定位研究主要集中在旁视（exocentric）视频，对egocentric视频的理解不足。两者在目标持续时间、轨迹密度、目标大小和位置偏移上存在显著差异，需新基准推动相关研究。

Method: 提出EgoMask，包括自动标注流程，跨短、中、长视频自动生成带参照表达和目标掩码的数据集。构建了大规模训练集EgoMask-Train，设计针对性实验，评估并微调现有时空定位模型。

Result: 主流时空定位模型在EgoMask上的表现明显下降；但经过EgoMask-Train数据集微调后，模型在自我中心和旁视数据上的表现均得以提升。

Conclusion: EgoMask及对应训练集有效推动了自我中心视频时空定位研究，为该方向提供了新基准和数据资源，有助于提升模型对egocentric视频的理解能力。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [54] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态、多任务框架CLIPTime，用于通过图像和文本输入预测真菌生长的阶段和时间戳，并在合成数据集上取得了有效结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（如CLIP）尽管在视觉与文本联合推理方面表现优异，但在刻画生物生长等过程的时间动态性方面有限。因此，研究者希望提升模型对时间进程的识别能力，以便更准确地用于生物生长监测等场景。

Method: 作者提出了CLIPTime框架，在保留CLIP模型联结视觉和文本嵌入能力的基础上，联合进行生长阶段分类和时间戳回归预测。训练时采用一个合成的真菌生长数据集，包含有阶段标签和时间戳，并设计了特定的时间精度和回归误差指标来评估模型的效果。

Result: 实验结果显示，CLIPTime能够有效建模生物生长过程的时间规律，预测结果具备可解释性和时间相关性。在对比实验中表现出对真实世界生物监测的潜力。

Conclusion: CLIPTime拓展了视觉-语言模型在生物科学领域的应用边界，实现了对生物生长时序动态的精准建模，对生物监测等实际场景具有广泛应用前景。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [55] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多光谱和高光谱图像融合方法PIF-Net，克服以往因数据不对齐引起的融合难题，显著提升了融合图像的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 多光谱与高光谱图像融合任务面临如何在保持丰富光谱信息的同时获取高空间分辨率的挑战，且通常由于数据不对齐和观测有限导致本质上是个病态问题。以往方法对该病态性缺乏有效应对，影响了最终融合效果。

Method: 作者提出PIF-Net融合框架，显式引入病态先验，并基于可逆Mamba架构设计特征转换和融合过程，确保信息一致性和过程可逆性。为兼顾光谱建模和计算效率，还提出了融合感知低秩自适应（Fusion-Aware Low-Rank Adaptation）模块，实现模型轻量化同时动态校准光谱与空间特征。

Result: 在多个基准数据集上的实验表明，PIF-Net在图像恢复性能上明显优于现有主流方法，并且保持了极高的模型效率。

Conclusion: PIF-Net为多/高光谱图像融合提供了高效且高质量的新方案，有效解决了观测病态及数据对齐问题，推动了该领域的技术进步。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [56] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: 本文提出了一种结合语义与时空引导的创新视频超分辨率方法SeTe-VSR，大幅提升了细节还原与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频超分辨率方法虽然效果不断提升，但在增强视频细节的同时，难以兼顾与原始低分辨率视频的高保真对齐，以及视频帧间的时序一致性。为解决这一难题，需要引入新的引导机制改善视频重建质量。

Method: 提出SeTe-VSR方法，将高层语义信息与空间、时间信息共同引入潜在扩散空间进行引导。通过这一策略，模型在还原细节的同时，更好地保证了帧间的时序一致性。

Result: 大量实验证明，SeTe-VSR在细节还原和感知质量上优于现有方法，尤其在处理复杂视频超分辨率任务时表现突出。

Conclusion: SeTe-VSR有效平衡了视频重建的细节恢复与时序一致性，在超分辨率任务中表现出色，有望在实际低分辨率视频增强场景中推广应用。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [57] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: HyPCV-Former是一种用于3D点云视频异常检测的新型超球空间时空Transformer方法，能更好地捕捉视频中的层次结构和时空连续性，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 以往的异常检测方法主要基于RGB或深度域下的欧式空间表示，难以有效表示事件的层次结构和时空连续性，影响检测效果。本文希望通过新的表示和建模方式解决这些问题。

Method: 方法首先用点云特征提取器获取每帧的空间特征，然后将其嵌入到更能表达层次关系的Lorentzian超球空间。为建模时间动态，引入了超球多头自注意力机制（HMHA），利用Lorentz内积和曲率感知Softmax，在非欧空间下捕捉时序依赖，并在完整Lorentzian空间中进行全部特征变换和异常评分，不再依赖切空间近似。

Result: 在多个异常检测数据集上进行了大量实验，HyPCV-Former在TIMo数据集上提升7%，在DAD数据集提升5.6%，优于现有方法。

Conclusion: HyPCV-Former通过全超球空间的表征和时序建模，显著提升了视频异常检测精度，为该方向提供了新思路。代码将在论文接收后开放。

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [58] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC提出了一种无需训练即可进行可控多图合成的方法，显著提升了多图像一致性与布局控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有可控图像合成主要关注单图参考，面对多图参考且需要空间布局一致时效果有限，因此需要一种能处理多参考图像并注意空间布局的方法。

Method: LAMIC在MMDiT扩展下，提出了两种注意力机制：Group Isolation Attention（GIA）用于提升实体解耦，Region-Modulated Attention（RMA）用于提高空间布局感知，且无需训练，只需在推理阶段应用。此外，作者提出三项新指标评测模型在布局与背景保持方面的表现。

Result: 在多项核心指标（如ID-S、BG-S、IN-R、AVG得分和复杂合成任务的DPG）上，LAMIC超越现有多参考合成方法，验证了其在身份保持、背景一致、布局控制和按需生成等方面的领先表现。

Conclusion: LAMIC为可控多图像合成提供了零训练、即插即用的解决思路，展现出强大的零样本泛化能力，并有望随基础模型的发展持续提升性能，开创了可控多图像合成的新范式。

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [59] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: 本文提出了SAMSA 2.0，一种结合空间与光谱信息的交互式高光谱医学影像分割框架，通过引入光谱角提示来提升分割的精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的分割模型如SAM主要基于RGB信息，难以充分利用高光谱影像中的丰富光谱特征，导致在医学影像复杂环境下分割效果有限。该研究旨在融合光谱信息以提高分割的准确性和鲁棒性，特别是在数据稀缺和噪声环境下。

Method: SAMSA 2.0在Segment Anything Model的基础上，采用光谱角提示，将光谱相似性与空间提示进行早期融合，无需对模型重新训练即可实现光谱信息的有效利用。通过交互式方式引导分割过程，提高模型对不同高光谱数据集的适应能力。

Result: SAMSA 2.0实现在无需重训练的情况下，Dice分数比仅用RGB的模型提升最高3.8%，比此前的光谱融合方法提升最高3.1%。同时，在少样本和零样本的任务中表现优异，具备较强泛化能力，尤其适用于临床中的低数据和高噪声场景。

Conclusion: SAMSA 2.0有效融合光谱与空间信息，提升了高光谱医学影像分割的精度与鲁棒性，尤其适用于数据有限和复杂环境下，具有较好的泛化能力，无需额外训练即可应用于多种高光谱数据集。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [60] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: 本文提出了LesiOnTime方法，通过结合纵向（多时点）乳腺增强MRI影像和临床BI-RADS评分，有效提升了小病灶的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割方法主要关注大病灶，忽视了纵向影像和临床信息，而这些正是临床医生在筛查和诊断早期乳腺癌时依赖的重要依据，尤其对于早期、微小病灶的发现。

Method: 设计了LesiOnTime，核心包括：（1）时序先验注意力模块（TPA），动态融合前后期扫描信息；（2）BI-RADS一致性正则项（BCR），通过对具有相似临床评估（BI-RADS评分）的影像进行潜在空间约束，将临床知识嵌入训练过程。

Result: 在高风险患者的纵向乳腺增强MRI自有数据集上，LesiOnTime方法在Dice分数上比最新单时点或多时点基线提升5%。消融实验也验证了TPA和BCR均带来了互补性的性能提升。

Conclusion: 融入纵向影像和BI-RADS等临床上下文信息，可进一步提升乳腺癌早筛中小病灶分割的可靠性和实用性。该方法对真实临床场景具有重要应用价值。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [61] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 该论文提出了一种无需预标注数据的遥感影像自动分割和标注方法，提高了标注效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 遥感影像的人工标注文本消耗大量时间与成本，且传统标注方法依赖已有标注数据，限制了新场景和区域的适用性。因此需要一种无需预标注也能自动发现和标注图像内容的方法。

Method: 提出一种无监督管道，结合卷积神经网络和图神经网络对Sentinel-2卫星影像进行分割，将图像划分为基于颜色和空间相似性同质区域。通过图神经网络整合邻域信息，实现鲁棒性的特征表达，用于后续的相似性比较和辅助标注。

Result: 该方法较传统基于预标注的工具，提升了图像特征表达能力与鲁棒性，实现更细粒度、更准确的区域自动标注，并减少了标注中的异常值。

Conclusion: 所提无监督分割与标注方法能有效提升遥感影像自动标注的效率和质量，为用户提供更高效、便捷的标注工具。

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [62] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 本文提出了一种高效的路径聚合网络EPANet，用于水下鱼类检测。该方法通过高效特征整合，提升了检测准确率和推理速度，同时保持了较低的模型复杂度。实验结果显示EPANet优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测存在目标分辨率低、背景干扰大和鱼体与背景视觉相似性高等难题。当前方法多侧重于局部特征强化或引入复杂注意力机制，但这通常导致模型复杂度提升、效率下降。因此，需要一种能够高效整合特征、提升检测性能又不增加模型复杂度的新方法。

Method: 作者提出EPANet，包括两个关键组成：高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样划分短路径瓶颈（MS-DDSP瓶颈）。EPA-FPN通过跨尺度长距离跳连提升语义-空间互补性，并用跨层融合路径提高特征整合效率；MS-DDSP瓶颈将传统瓶颈结构细粒化，并采用多样卷积操作，增强局部特征多样性和表示能力。

Result: 在基准水下鱼类检测数据集上，EPANet在检测准确率和推理速度上都超过了现有最先进方法，同时参数量与之持平或更低。

Conclusion: EPANet能够高效地融合多尺度特征，并提升水下鱼类检测任务中的性能和效率，对低分辨率小目标检测具有重要意义，兼顾了精度和模型轻量化。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [63] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于参考视频的调色框架，通过扩散模型生成查找表(LUT)，实现输入视频与参考场景在色彩、氛围等属性上的对齐，可高效完成无结构损失的视频调色，并能结合文本提示调整对比度、亮度等细节。实验证明该方法效果优越，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 视频调色对艺术氛围和情感传达起到重要作用，但操作复杂且依赖专业技能，目前多由专业调色师完成，普通用户难以实现高质量调色。因此，亟需智能化、易用的自动视频调色工具。

Method: 提出一种利用扩散模型自动生成调色查找表(LUT)的方法，实现输入视频与参考视频在高层次色彩属性如氛围和情感上的匹配。此外，通过管道加入文本提示，允许用户进一步微调低层次特征（如对比度、亮度）。

Result: 实验和用户调研表明，该方法能够有效实现高质量、高效率的视频调色，同时保持视频结构细节不丢失，用户可以便捷实现个性化调色需求。

Conclusion: 所提出的方法在色彩表达和艺术氛围传达方面表现突出，具备结构细节保持与高效推理优势，能广泛应用于各种视频调色场景。

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [64] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: 本文评估了当前主流视觉-语言模型（VLM）在医学影像中判别解剖结构相对位置的能力，发现整体表现不佳，并提出新的基准数据集MIRP以促进该领域研究。


<details>
  <summary>Details</summary>
Motivation: 在临床决策中，了解解剖结构或异常的相对位置至关重要。然而，尽管视觉-语言模型在医学领域应用前景广阔，其在图像中判别相对位置的能力却鲜有深入研究。本文旨在弥补这一研究空白。

Method: 评估了GPT-4o、Llama3.2、Pixtral和JanusPro等主流模型在相对位置判断上的表现，并借鉴计算机视觉中常用的视觉提示（如将标记、颜色标记放置在解剖结构上）来提升模型性能。同时比较医学影像与自然图像场景下的表现差异。

Result: 所有主流模型在医学影像中均未能准确判断结构间相对位置。虽然引入视觉提示能带来一定提升，但在医学影像上的表现仍显著低于自然图像，模型更多依赖于先验知识而非真实图像内容。

Conclusion: 现有VLM在医学图像的空间关系推理能力显著不足，难以直接应用于临床相对位置判断任务。本文发布了MIRP基准数据集，为后续相关研究提供标准评测平台。

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [65] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新型扩散模型去对抗净化方法，简称DBLP，能在提高鲁棒性的同时大幅提升速度，仅需约0.2秒推理，具有极强的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的对抗净化方法通常需要耗时的迭代去噪，导致实际部署受限。亟需一种既高效又有效的对抗净化新方法。

Method: 提出了Diffusion Bridge Distillation for Purification (DBLP) 框架。其核心创新是噪声桥蒸馏目标，通过在潜在一致性模型中对齐对抗噪声分布与干净数据分布。同时引入自适应语义增强，将多尺度金字塔边缘图作为条件输入，进一步提升恢复图像的语义一致性。

Result: 在多个数据集上，DBLP在鲁棒准确率、图像质量均达到SOTA，同时推理速度提升到约0.2秒，显著优于以往扩散去噪方法。

Conclusion: DBLP突破性地提升了对抗净化的效率和效果，为落地实时对抗净化任务提供了有力技术基础。

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [66] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune是一种无需训练、与模型无关的视觉-语言模型（VLM）Token剪枝方案，可以在极大减少Token数量和推理计算量的情况下，几乎不损失模型精度。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs需要处理大量视觉Token，导致计算量大，推理效率低。现有方法对Token进行剪枝或合并，但依赖特殊Token或需特定训练，迁移性和通用性差。

Method: 提出HiPrune框架，利用视觉编码器的层次化注意力结构，无需重新训练。具体选择三类信息量大的Token：（1）中层注意力集中的Anchor Token用于对象区域，（2）邻近空间连续性的Buffer Token，（3）深层捕获全局特征的Register Token。操作简单且可直接应用于所有基于ViT的VLM。

Result: 在LLaVA-1.5、LLaVA-NeXT和Qwen2.5-VL等多个主流VLM上实验，HiPrune仅用33.3%的Token可保留99.3%的任务准确率，用11.1%的Token仍保留99.5%的准确率。推理计算量和时延最高可降低9倍，表现出极强的模型与任务泛化能力。

Conclusion: HiPrune极大提升了VLM的推理效率，在无需训练和兼容多架构情况下实现了业内最佳的Token剪枝效果，并且在多模型和多任务上都具备良好泛化性。

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [67] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、用于提升开放词汇语义分割（OVSS）性能的新方法——FreeCP，并在8个基准上验证了其有效性。作为即插即用模块，FreeCP显著提升了其他OVSS方法的分割效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的OVSS提升方法依赖于对视觉-语言模型的微调，过程耗时且资源消耗巨大，因此研究者希望找到无需重新训练即可提升分割性能的方法。现有无需训练的方法在处理类别冗余和视觉-语言歧义问题上存在不足，故本文试图解决这些关键挑战。

Method: 本文提出FreeCP框架（training-free class purification）。该方法针对类别冗余和语义歧义，通过纯化语义类别、纠正类别激活错误，生成更优质的类别表示。纯化后的类别嵌入用于最终的分割预测。

Result: 在8个公开分割基准上实验，结果显示FreeCP与现有OVSS方法结合时，能大幅提高分割性能，验证了其通用性和实用性。

Conclusion: FreeCP作为一种无需训练的类别纯化框架，有效克服了类别冗余和语义歧义带来的问题，是提升开放词汇语义分割性能的有效、便捷方案。

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [68] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的生成方法PhysNAP，用于生成与部分点云对齐且物理合理的可动关节物体。通过约束和类别信息提升了生成物体的一致性和物理可行性。


<details>
  <summary>Details</summary>
Motivation: 关节型物体在实际环境中广泛存在且常与人交互。现有生成方法难以同时保证点云对齐和物理可行性，因此有必要提出新方法提升生成质量。

Method: PhysNAP基于扩散模型，以SDF（符号距离函数）表示部件形状。采用点云对齐作为引导损失，并施加非穿透和可运动性约束，指导模型生成更物理合理的结构。同时支持类别感知以进一步提升对齐效果。

Result: 在PartNet-Mobility数据集上，以生成能力和约束一致性进行评估。PhysNAP相比无引导扩散模型，在约束一致性上有明显提升，并能在生成能力与物理合理性之间取得平衡。

Conclusion: PhysNAP有效提升了基于点云的可动关节物体生成的物理可行性和一致性，为实际应用如机器人路径规划和虚拟内容生成提供了更可靠工具。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [69] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: 本论文提出了一种仅依赖图像级标注的领域专用弱监督目标检测算法，极大降低了人工标注成本，并能在有限标注时间情况下优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 当前目标检测先进方法高度依赖大量有边界框数据集的人工标注，这一过程不仅昂贵且需专业知识，因此亟需降低标注成本和门槛的解决方案。

Method: 作者提出一种基于图像级标注的弱监督目标检测算法。其核心是利用已训练的模型预测图像中病毒的存在与否，从中提取知识生成伪标签，并在缩小感受野的优化策略下，直接提取病毒颗粒，无需特定网络结构。最终，利用这些伪标签训练先进目标检测模型。

Result: 大量实验证明所提伪标签方法不仅获取简单快捷，同时性能优于现有弱标注方法，在标注时间有限时甚至优于人工真值。

Conclusion: 本文提出的弱监督目标检测方法有效解决了高昂标注成本及效率瓶颈，在特定领域具有显著应用前景和价值。

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [70] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉里程计（VO）方法，通过跨帧不确定性传播，提高了动态场景下VO的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 无监督VO可以避免对昂贵标签的需求，但在动态物体存在时，传统方法因静态场景假设失效而性能下降。因此，作者希望通过更有效的不确定性建模提升VO在动态环境下的可靠性。

Method: 作者提出了Combined Projected Uncertainty VO (CoProU-VO)方法，将目标帧的不确定性与参考帧投影后不确定性结合，采用概率建模方式进行跨帧不确定性传播。模型基于视觉Transformer，同时学习深度、不确定性与相机位姿。无需显式的运动分割，直接过滤动态物体和遮挡区域。

Result: 实验在KITTI和nuScenes数据集上进行，CoProU-VO相比现有的无监督单目两帧方法，在动态和高速公路等复杂场景下表现出明显提升。消融实验也验证了跨帧不确定性传播的有效性。

Conclusion: 跨帧不确定性传播方法有效提升了无监督VO在动态场景下的表现，无需昂贵标签或显式运动分割，为今后的视觉导航和机器人定位提供了更鲁棒的解决方案。

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [71] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的似然比估计方法，有效提高了自动驾驶场景中未知对象的检测能力，在五个数据集上取得了优秀结果。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等实际场景中，传统的语义分割模型容易将未知对象误判为已知类别，现有的像素级分布外检测方法也常常将稀有类与真正未知类混淆，效果有限。解决提高模型识别未知对象的准确性与可靠性成为亟需问题。

Method: 作者提出基于证据推理分类器的似然比检验方法，将不确定性建模引入像素级分布外检测任务。方法输出的是不确定性分布而不是点估计，可以更好地反映训练样本稀有性和合成离群样本带来的不确定性，并提升离群暴露数据的利用效果。

Result: 该方法在五个标准数据集上进行实证，取得了2.5%的平均最低假阳性率，同时保证了90.91%的高平均精度，且计算开销极低，优于当前主流方法。

Conclusion: 通过引入不确定性估计，提出的方法显著提升了语义分割模型对未知对象识别的能力，并在多个基准测试中表现出色，具有较强的实际应用价值。源码已开源。

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [72] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的夜间灯光遥感重建框架（EVAL），有效提升了历史人类活动时空分布的定量能力，将高质量NTL数据回扩26年（至1986），性能优于现有产品，对社会经济研究极具价值。


<details>
  <summary>Details</summary>
Motivation: 现有夜间灯光遥感数据（如NPP-VIIRS）起始于2012年，无法满足更长时间序列（1986年及以后）的人类活动研究。以往回扩方法存在低估灯光强度与细节丢失的问题，亟需改进。

Method: 提出分两阶段：构建阶段采用层次融合解码器（HFD）提升初步重建质量，优化阶段用双特征精炼器（DFR）融合高分辨率不透水地表数据，提升结构细节，生产EVAL产品。

Result: EVAL产品将夜间灯光数据起点提至1986年，量化评估显示其R²由0.68提升至0.80，RMSE由1.27降至0.99，时间一致性好，与社会经济指标相关性高，优于现有方案。

Conclusion: EVAL数据集极大扩展了夜间灯光时序数据范畴，解决了强度低估和结构细节丢失难题，为社会经济与人类活动长期动态分析提供了重要支撑，且已公开共享。

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [73] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: 本文提出了一种名为Wukong的新NSFW检测框架，能够在文本生成图像的早期阶段高效检测不适当内容，兼具准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像技术可能输出包含暴力等NSFW内容，现有的文字和图像过滤方法要么易受攻击、缺乏针对性，要么计算开销大、引入延迟，因此亟需更高效、准确的检测方法。

Method: 作者通过研究扩散模型的中间特征（尤其是U-Net的早期去噪步骤及其跨注意力层），提出了Wukong框架：利用这些中间特征及预训练跨注意力参数，在图像尚未完全生成时即检测潜在NSFW内容。同时，构建了新数据集对方法进行评测。

Result: Wukong在新构建的数据集和两个公开基准上显著优于文本过滤方法，与图像过滤方法精度相当，但效率更高。

Conclusion: Wukong为T2I扩散模型提供了一种高效、准确的NSFW内容检测途径，有望提升AIGC内容的安全性与生成效率。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [74] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: 该论文提出了GeoMoE框架，针对两视图几何中复杂不均匀运动场的建模问题，通过MoE（专家混合模型）方法实现运动子场的分解与专门建模，从而显著提升了运动估计的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有两视图运动场估计方法在处理极端视角、尺度变化和深度突变的真实场景时，缺乏针对异质运动模式的有效建模，导致结果偏离真实结构。作者观察到MoE方法可将运动子场分配给不同专家，采用分而治之策略提升建模效果。

Method: 论文提出Probabilistic Prior-Guided Decomposition方法，利用内点概率信息将运动场结构化分解为多个异质子场，削弱异常点带来的偏差。接着设计MoE-Enhanced Bi-Path Rectifier，分别沿空间和通道语义路径增强各子场，再路由到合适专家进行针对性建模，有效去耦各类运动干扰，实现细粒度校正。

Result: GeoMoE在相对位姿和单应性估计任务上超越现有SOTA方法，并展现出很强的泛化能力。源码及预训练模型已开源。

Conclusion: 通过专家混合模型和概率先验引导的分解策略，GeoMoE能够有效应对复杂、异质的运动场场景，提升运动估计的准确性和鲁棒性，是两视图几何领域的重要进展。

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [75] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的三维全身人体姿态先验模型DPoser-X，能够更好地处理复杂的人体关节动作和数据稀缺问题，并在多个姿态建模任务上超越了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前三维全身人体姿态先验模型面临人体结构复杂和高质量数据集稀缺的问题，亟需更通用、更健壮的姿态先验方法以提升姿态估计、生成及相关任务的表现。

Method: 提出DPoser扩散模型作为姿态先验，并扩展为DPoser-X以适应更具表现力的全身姿态建模。方法核心在于采用扩散采样框架统一处理多种姿态任务，并引入针对姿态数据特性的截断时间步调度方法和局部遮罩训练机制，以整合全身与局部数据集、提高骨骼各部位间依赖建模能力及防止过拟合。

Result: 在人体、手部、面部及全身多项姿态建模基准测试上，DPoser-X展现出极高的鲁棒性和通用性，且在所有评测中表现优异，超过现有同类模型。

Conclusion: DPoser-X树立了全新的人体全身姿态先验建模“标杆”，为后续全身姿态理解及各类下游任务提供了强有力的工具。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [76] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 本文揭示了在面对非受控环境下的面部检测系统中，通过对象生成攻击（Face Generation Attacks）和地标漂移攻击（Landmark Shift Attack），可以有效破坏人脸检测和关键点回归过程，并提出了相应的防御方案。


<details>
  <summary>Details</summary>
Motivation: 当前人脸识别系统常遇到非受控环境下的图像采集问题（如光照不均、姿态各异），需要精准的人脸检测与对齐模块。然而，其安全性尚未充分研究，尤其是在新的攻击模式下的脆弱性。

Method: 作者提出并实验了两类新型攻击：（1）Face Generation Attack：基于对象生成，欺骗检测模块；（2）Landmark Shift Attack：首次实现，通过后门攻击影响人脸关键点坐标回归。同时，作者还针对这些攻击提出了缓解措施。

Result: 实验证明上述攻击能够有效迷惑甚至破坏主流人脸检测与对齐模型的判别能力，影响检测结果的可靠性。所提出的防御方法能在一定程度上缓解相关威胁。

Conclusion: 面部检测系统面临新的生成攻击和回归后门攻击风险。理解和防御这些威胁对于增强人脸识别系统的安全性至关重要。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [77] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 本文提出利用生成模型合成医学影像属性标注数据，以提升可解释分类模型的训练效果。实验显示合成数据可显著提升模型属性及目标预测准确率，有助于可解释模型在医学影像诊断中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像分类模型对临床具有解释性的输出需求越来越高，但具备属性标注的大规模数据稀缺，限制了模型的实际应用。研究者希望通过合成数据缓解数据不足带来的困难。

Method: 作者使用扩散生成模型（Diffusion Model），通过属性条件约束，仅用20个标注样本训练生成标注影像。合成标注影像再用于可解释性模型训练。

Result: 经过合成影像增强训练后，属性预测准确率提高了13.4%，最终分类准确率也提升了1.8%。

Conclusion: 合成属性标注数据有效缓解了数据稀缺问题，提升了可解释医学影像模型的性能，对于推动临床可用的AI影像诊断系统具有重要意义。

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [78] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 本论文提出并实现了首个针对目标检测器补丁攻击的标准化防御基准，通过系统分析和数据集推动领域研究发展。


<details>
  <summary>Details</summary>
Motivation: 目前针对目标检测器的补丁攻击防御评价缺乏统一、系统的框架，导致方法评估结果不一致且片面，阻碍相关防御研究的有效对比与进步。作者希望通过建立综合性基准填补这一领域空白。

Method: 作者系统性地回顾并实测了11种代表性补丁攻击防御方法，构建了包含2种攻击目标、13种补丁攻击、11种目标检测器和4项多样评价指标的补丁防御基准，形成包含94种补丁、94000张图像的大规模对抗补丁数据集，基于此开展分析。

Result: 分析揭示了三点关键新发现：1）防御自然风格补丁的难点主要源自数据分布而非高频特性，新的多分布数据集可以提升现有防御15.09%的AP@0.5；2）被攻击物体的平均精度与防御性能高度相关，超过补丁检测准确率；3）自适应攻击能显著绕过现有防御，而模型复杂/随机或具备通用补丁属性的防御更为鲁棒。

Conclusion: 论文建立了系统且可扩展的补丁攻击防御基准，为合理评价与设计防御方法提供了实践指导和新视角，并开放代码及数据集以促进后续研究。

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [79] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 本文针对图像去雾问题，提出利用大规模预训练的深度特征，通过一个可插拔的RGB-D融合模块提升各种架构下的去雾能力，经实验证明其效果优越且适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有的去雾方法虽然性能不错，但架构专有性强，难以适应不同精度和效率需求的场景。作者希望寻找一种更具通用性和适应性的解决方案。

Method: 作者首先实证分析大规模预训练深度表征在不同雾度下的一致性，然后提出一个可插拔的RGB-D融合模块，可与多种去雾方法集成，利用深度信息提升去雾效果。

Result: 大量实验表明，该RGB-D融合模块提升了去雾的有效性，并且在多个基准数据集和不同网络架构下均表现出良好的广泛适用性和优异性能。

Conclusion: 通过引入预训练深度特征和模块化融合方式，可显著提升去雾效果，实现跨架构通用，适应多样化实际需求。

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [80] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 提出了一个新颖的检测方法D3，无需训练、基于二阶动态特征，能高效且鲁棒地分辨真人视频与AI生成视频，在多个公开数据集上超过现有检测手段。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频检测方法对合成视频的时序伪影挖掘不足，导致检测准确率有限。随着高保真AI视频（如Sora）激增，开发更精准鲁棒的检测方法势在必行。

Method: 首先建立了基于牛顿力学的二阶动力分析理论框架，并提出专门用于时序伪影检测的二阶中心差分特征。进一步提出了一种无需训练的新方法D3（Detection by Difference of Differences），专门利用真实与AI生成视频在二阶特征分布上的根本差异来进行检测。

Result: 方法在Gen-Video、VideoPhy、EvalCrafter、VidProM等4个数据集40个子集上的检测表现均优于现有方法，其中在Gen-Video集上平均准确率提升10.39%。此外在计算速度与抗后处理能力上都有优异表现。

Conclusion: 基于二阶动力学的新检测方法D3，在分辨AI合成与真实视频方面高效、鲁棒，并显著超越此前最佳方法，有望在应对AI合成内容泛滥问题中发挥重要作用。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [81] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文针对多模态大模型在多张图片输入场景下产生幻觉（hallucination）的问题进行系统性研究，并提出了专用基准MIHBench和新方法，实验验证该方法能有效减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 以往对多模态大模型幻觉现象的研究主要集中于单图像输入，尚未系统探索多图像场景下的问题。多图像输入在实际应用中非常常见，因此有必要专门研究其产生幻觉的特点和解决方案。

Method: 作者设计了首个多图像幻觉的系统性评测基准MIHBench，包括多图像下的物体存在、计数与身份一致性三大任务，并提出动态注意力平衡机制，在保持整体视觉关注比例的基础上调整图片间的注意力分布。

Result: 通过对若干主流多模态大模型的广泛实验，发现：输入图片数量越多幻觉发生概率越高；单图像幻觉倾向与多图像环境高度相关；同物体图片比例和负样本顺序影响身份一致性幻觉。提出的动态注意力机制有效减少了幻觉，并提升了语义整合和推理的稳定性。

Conclusion: 本文首次从系统角度研究多图像场景下的多模态大模型幻觉，提出专用基准与新机制，有效应对相关任务，在提升多图像语义理解能力方面具有实际意义。

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [82] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了YOLO-Count，一种可微的开放词汇目标计数模型，不仅提升了一般计数任务，还为文本生成图像（T2I）带来了精确的数量控制。通过新颖的“基数”图、表征对齐和强弱监督结合，YOLO-Count实现了SOTA的计数精度并优化了生成模型的数量控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有目标计数方法难以适应多类别、开放词汇的应用场景，且文本生成图像系统通常无法精确地控制物体数量。作者希望通过提出新方法，解决开放词汇计数与T2I数量控制的难题。

Method: 引入“基数”图作为回归目标，能够适应不同物体大小和分布的计数需求。结合表征对齐技术和强-弱监督混合训练方式，将开放词汇计数与T2I生成数量控制有机结合。整体架构端到端可微分，适合基于梯度的方法优化。

Result: YOLO-Count在多个实验任务中取得了最先进的目标计数准确率，并在文本生成图像的数量控制上表现出强健和高效的能力。

Conclusion: YOLO-Count极大推动了开放词汇目标计数和T2I系统中的数量控制研究，展现出良好的通用性及实际应用前景。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [83] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Dense Backbone的轻量级3D目标检测主干网络，能够在保持检测精度的同时大幅降低模型参数和计算延迟。


<details>
  <summary>Details</summary>
Motivation: 当前3D目标检测方法虽然精度高，但普遍依赖于VGG或ResNet等大型主干网络，导致模型复杂度高，难以应用于实际轻量化场景。而2D目标检测的轻量化主干网络已有大量研究，3D领域的类似研究仍显不足。

Method: 作者设计了一种基于稠密层的轻量级主干网络Dense Backbone，将其集成到主流3D检测网络（如PillarNet）中，并与原始骨干网络进行对比。此外，该网络具有即插即用能力，无需修改其余网络结构即可替换主干。

Result: 将Dense Backbone应用于PillarNet后，模型参数减少了29%，延迟降低了28%，在nuScenes测试集上的检测精度仅下降2%。

Conclusion: Dense Backbone能够显著减少3D目标检测模型的计算量与参数规模，同时几乎保持检测性能，且易于集成到现有3D检测框架中，对于推进3D目标检测轻量化具有重要意义。

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [84] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO通过结合自监督视觉模型与3D几何感知，提出一种高效轻量、几何一致性的特征学习框架，实现更精准的零部件区分，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自监督视觉基础模型虽然能捕捉语义对应关系，但对物体三维几何结构的感知不足。现有评估标准也难以全面反映几何特征学习质量。该论文旨在弥补这一空白，实现更强的几何感知。

Method: 提出一种基于最优传输的训练框架，可在遮挡和显式结构变化下超越关键点监督，训练几何一致且区分性强的特征；使用轻量化架构以实现高速度推理。

Result: GECO在PFPascal、APK 和 CUB 三大数据集上以显著优势刷新 SOTA，PCK提升分别为6%、6.2%、4.1%；运行速度比前作快98.2%，达到30fps。同时，作者发现PCK等指标未能全面评估几何质量，并提出新度量方式及洞见。

Conclusion: GECO框架有效补全了自监督视觉模型在几何感知上的短板，兼具高性能与实时性，并推动了几何感知特征学习标准的发展。

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [85] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 本论文提出通过分析头像的面部动态动作，实现对用户身份的生物识别验证，以应对虚拟头像冒充的安全威胁。实验结果表明，采用面部运动特征可以有效地进行身份验证。


<details>
  <summary>Details</summary>
Motivation: 真实感虚拟头像在各种虚拟交流环境中（如会议、社交游戏）被广泛应用，但这也带来了被冒充的重大风险，仅凭视觉和声音很难分辨真伪。作者想探索，当头像外观高度仿真时，是否能通过更难伪造的行为生物特征（如面部运动模式）对身份进行可靠验证。

Method: 作者建立了一个新数据集，包含用前沿的GAGAvatar模型生成的真实及冒充头像视频；并提出一种轻量、可解释且只依赖面部特征点的时空图卷积网络，结合时间注意力池化，专注于面部动态姿势建模。

Result: 实验证明，面部动态动作作为行为生物特征用于身份识别具有较高区分力，AUC值接近80%。

Conclusion: 面部动作能够作为有效的行为生物识别特征，用于识别虚拟头像背后的真实身份。该研究为虚拟头像沟通系统提供了可信的生物安全防护手段，并公开了数据集和基准系统，推动相关研究的发展。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [86] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: 本文提出了一种面向卫星影像的超分辨率方法SU-ESRGAN，通过整合语义分割损失和不确定性估计，提升影像的语义一致性与输出置信度，适用于无人机等场景。


<details>
  <summary>Details</summary>
Motivation: 现有GAN超分辨率方法虽然能生成高质量影像，但往往存在语义一致性差和像素级置信度缺失，这在需要高可信度的遥感应用（如灾害响应、城市规划、农业）中存在局限。

Method: 提出SU-ESRGAN方法，在经典ESRGAN超分模型基础上，融合DeepLabv3语义分割损失以保留类别细节，并加蒙特卡洛Dropout实现像素级不确定性估计，实现结果语义一致且可分析可信度。系统同时具备模块化设计，便于集成于无人机在板或后处理流程中。

Result: 在无人机航拍数据集上，SU-ESRGAN与原始ESRGAN在PSNR、SSIM、LPIPS等指标表现相当。模型经领域微调后，在对齐训练数据特性的Aerial Maritime Drone Dataset上适应性更强，反映领域知识的重要性。

Conclusion: SU-ESRGAN不仅提升了超分影像的语义一致性与不确定性分析能力，还可适应不同应用领域，便于在实际遥感与无人机影像处理流程中的应用，实现空间分辨率、可靠性、应用灵活性的平衡。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [87] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 本论文提出了一种针对医学图像的动态测试时自适应（TTA）框架，能够根据每个测试样本的特点自适应调整图像翻译过程，从而在处理分布外样本时提升模型的健壮性，同时不会影响分布内样本的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像到图像翻译方法在遇到分布外样本时性能下降，难以有效适应实际医疗场景中输入数据多变的情况。

Method: 本文提出了一种新颖的测试时自适应方法：引入重构模块评估样本的领域偏移，并通过动态适应模块，有选择性地对预训练模型的特征进行调整，仅在需要时对分布外样本调整，而保持分布内样本不变。

Result: 在低剂量CT去噪和T1到T2 MRI翻译两项任务上，本文方法实现了比无自适应和以往自适应方法更优越的性能，特别是在样本分布多变的情况下提升显著。

Conclusion: 动态、按样本调节的测试时自适应方法可以有效提升医学图像到图像翻译模型对分布外样本的鲁棒性，是提升模型实际应用能力的重要方向。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [88] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PILOT的新框架，通过创新性的方法提升了零样本异常检测（ZSAD）在不同领域中的泛化能力，在13个工业和医疗数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本异常检测方法由于训练数据来源有限，难以应对领域转移，导致对新分布的泛化能力不足。解决这一问题对实际应用至关重要。

Method: PILOT包括两大创新：（1）提出双分支提示学习机制，将可学习的提示与结构化语义属性动态结合，根据输入自适应选取最相关的异常线索；（2）在无标签情况下利用置信度较高的伪标签，进行测试时提示参数的自适应更新，实现无监督/弱监督下的测试时自适应。

Result: 在13个工业和医疗领域基准数据集上的实验显示，PILOT在异常检测和定位方面在领域转移条件下均达到了最优性能。

Conclusion: PILOT框架通过创新的提示学习和测试时自适应方法，有效提升了零样本异常检测在跨领域的适应能力，对实际应用具有较大推动意义。

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [89] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: 本文分析了激光点云数据集在公共安全语义分割中的表现，并指出当前在异构标注和小目标识别方面存在明显挑战。


<details>
  <summary>Details</summary>
Motivation: 目前应用于公共安全的激光点云数据集往往存在标注方式不一致、类别不统一的问题，这对基于语义分割的安全特征检测带来困难。因此需要系统性分析现有方法在异构数据下的表现及其瓶颈。

Method: 作者以NIST的Point Cloud City数据集（Enfield和Memphis集合）为基础，采用了一种分级标注方案，利用KPConv网络架构，并通过IoU指标评估大型与小型类别在点云分割中的表现。

Result: 发现几何尺度较大的对象（如楼梯、窗户）分割效果更佳，而许多尺寸较小、但安全相关性强的目标（如报警器、紧急出口等）识别率显著下降，主要受类别不平衡及小目标几何特征不明显影响。

Conclusion: 可靠的点云语义分割技术需依赖标准化的标注协议及更优的标注方法。面对数据异构和小目标难检测问题，未来应发展自动标注和多数据集联合学习等策略。

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文评估了当前先进大型语言模型（LLM）在解答物理问题（包括数学和描述性问题）上的能力，并尝试多种方法提升其表现，提出了多智能体协作方法显著提升了模型解答困难问题的能力，还构建了一个包含19609道物理题目的新评测基准PhysicsEval。


<details>
  <summary>Details</summary>
Motivation: 传统的物理问题求解是自然语言推理中的重要任务，评估LLM在此方向的能力对于推动AI物理推理和应用具有重要意义。但当前缺少大规模、高质量的物理推理评测集，也未系统分析如何提升LLM在物理题上的表现。

Method: 本文评测了多种主流LLM在解答不同类型物理问题上的表现，采用了多种推理增强技术，尤其提出通过多智能体协作：主模型生成解答，由多个小模型逐步验证/修正解答。对比分析了不同技术和框架下的模型性能。此外，整理了19609道覆盖广泛内容的物理题目及标准答案，构建了PhysicsEval评测集。

Result: 多智能体协作显著提高了LLM在最初表现差的物理问题上的解答正确率。论文还建立了全面的物理评测集，可用于今后模型评测和研究。

Conclusion: LLM在物理问题解答方面有很大潜力，通过推理技术和多智能体协作可进一步提升表现。新提出的PhysicsEval基准填补了评测空白，为未来AI物理推理研究提供了重要工具。

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [91] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: 本研究比较了ChatGPT不同版本与人类（包括英语母语和非母语者）在词汇多样性方面的差异，结果显示所有LLM生成文本与人类写作在每个词汇多样性维度上都有显著不同，且新版本的LLM与人类差异更大。


<details>
  <summary>Details</summary>
Motivation: 当前对于大模型（LLM）生成文本与人类写作相似度的判断众说纷纭，尤其在文本的真正“拟人性”上仍有争议。作者希望通过系统考察词汇多样性，更精确评估LLM文本的拟人化程度。

Method: 作者收集了四个ChatGPT版本（3.5、4、o4 mini、4.5）生成的文本以及来自240名英语母语、非母语者（覆盖四个学历层次）的写作文本。对每篇文本测量了六种词汇多样性维度（词量、丰富度、多样-重复、均衡、差异、分布），并用MANOVA、ANOVA和支持向量机等方法对组间差异进行分析。

Result: 所有LLM生成文本与人类文本在所有词汇多样性维度上存在显著统计差异，其中ChatGPT-o4 mini和4.5两个新模型与人类差异最大，且ChatGPT-4.5词汇多样性更高但生成词数更少。人类写作者各子组之间（按学历和语言身份划分）在词汇多样性上无显著差别。

Conclusion: LLM生成文本在词汇多样性上并不像人类写作，新版本甚至比旧版本更不拟人。该结果对语言教学及相关领域有重要启示。

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [92] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 本文指出在计算人文学科中，主流方法对语言文化的复杂性处理不足，因而产生了理论和解释上的混淆。作者呼吁将建模视为文化语境与数学表达间的翻译，并强调需要更系统的理论来规避翻译误差和提升透明度。提出“符号复杂性”概念，批判对复杂语义内容进行过度简化的做法，并给出改善建议。


<details>
  <summary>Details</summary>
Motivation: 计算人文学科快速发展，但理论基础薄弱，特别是在方法论与解释层面缺乏充分论证，易导致建模结果失真。作者希望通过明确建模过程的理论基础，提高学科的成熟度和研究成果的可信度。

Method: 作者采用理论分析的方法，将建模过程比作跨文化、语言与计算域的‘翻译’，系统梳理当前研究中的典型误差，并提出“符号复杂性”概念来区分不同文本在解释上的多样性和复杂性，对现有评估方式进行批评和反思。

Result: 作者揭示了当前主流建模与评估实践在理论和翻译层面的盲点：简单化处理具备高度解释弹性的（即符号复杂的）数据，并未能有效反映其多义性，导致理论解释和结果的透明度不足。

Conclusion: 文章认为，计算人文学科必须更加重视理论建构，将“符号复杂性”等观念纳入模型设计和评估流程，以避免认知和翻译误差，提高研究透明度，推动学科健康发展。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [93] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: 本文提出了一种新的大规模、人工验证的长文本事实性评估基准FACTORY，用来更准确地衡量语言模型在生成信息真实长文本方面的能力，并实证其比现有基准更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本事实性评估基准多缺乏人工核查，导致部分质量问题，无法充分检验现有语言模型的真实生成能力，故需要更高质量、更具挑战的评测任务。

Method: 作者采用“模型-人协作”方式构建FACTORY基准集，由模型初步生成后由人工核查，确保每个prompt都是要求事实性、可回答且无歧义的高质量题目。然后在FACTORY及其他数据集上，用人工评测方式系统评估6个SOTA大模型的表现。

Result: 实验证明FACTORY更具挑战性：SOTA模型在该集合上的生成信息约40%不属实（比其他数据集的10%高很多）；也分析了FACTORY在可靠性和长尾事实推理方面对模型的优势。

Conclusion: FACTORY有效填补了现有长文本事实性评估的不足，未来事实性评估更应注重高质量人工核查基准及考察模型面向复杂长尾事实的推理能力。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [94] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: 神经语义解析器通常表现优异，但在处理如英语动词短语省略（ellipsis）等强上下文敏感现象时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前神经语义解析器在多种语言现象下表现出高准确率（语义匹配分数超过90%），但尚不清楚它们在处理需大规模语义信息复制的上下文敏感问题（如动词短语省略）时的能力。作者希望评估这些解析器在此类特殊现象上的表现。

Method: 作者构建了一个包含120个带有完整语义还原的动词短语省略现象语料库，并以此作为挑战集，测试多种神经语义解析器的处理能力。

Result: 神经语义解析器在标准测试集上表现良好，但在动词短语省略实例上的表现失败，无法有效解析此类现象。

Conclusion: 尽管神经语义解析器在常规任务上非常强大，但在需要高度语境理解和信息复制的现象上仍存在明显不足。

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [95] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: 本文总结并比较了当前流行的大型语言模型（LLMs），为选型提供实用参考。


<details>
  <summary>Details</summary>
Motivation: 目前开源和微调的大型语言模型种类繁多，选型难度增加，研究者和企业在选择合适模型时面临诸多困扰，如许可、硬件等条件限制。

Method: 收集整理了各种基础模型和特定领域模型的信息，特别关注发布时间、许可协议及硬件需求，并将这些信息在GitLab平台公开且持续更新。

Result: 建立了一个覆盖主流LLMs、详细对比其关键特性（如许可、硬件、发布时间等）的公开列表，便于用户查阅和比较。

Conclusion: 该工作为LLM选型提供了系统和动态更新的参考，帮助研究者和企业便捷、合理地选择满足自身需求的大型语言模型。

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [96] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 本文梳理了表格输入在大语言模型领域的研究现状，提出了表格结构和任务的分类体系，并指出该领域存在的关键研究挑战。


<details>
  <summary>Details</summary>
Motivation: 表格结构复杂多样，目前缺乏通用表格理解方法，相关任务和模型多为专用型，导致表格理解任务难以统一推进。本文旨在厘清表格输入和理解任务的分类，提高研究的系统性并指出现有的主要问题。

Method: 提出表格输入表示和表格理解任务的分类法（taxonomy），系统性梳理现有研究，总结当前主流方法，并分析表格数据建模中的关键挑战。

Result: 揭示了三个主要领域问题：1) 现有任务偏重检索，缺乏深入推理能力；2) 处理复杂结构、大规模和多表环境时模型表现不佳；3) 模型在不同表格格式间的泛化能力有限。

Conclusion: 当前表格理解在模型推理、复杂场景适应和格式泛化等方面存在显著不足，需要开展更具挑战性和通用性的研究方向。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [97] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 论文探索将离散小波变换（DWT）应用于词和句子嵌入中，在降维的同时保持语义信息，提升下游NLP任务效果。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号、图像处理中表现优异，但在自然语言处理（NLP）领域的应用尚少。作者希望验证DWT能否有效压缩嵌入表示、保持语义信息，并提升NLP任务表现。

Method: 将离散小波变换（DWT）应用于多种词/句子嵌入（包括大语言模型产生的嵌入）上，通过多分辨率分析和特征压缩，并在语义相似性任务及其它下游任务上评估效果。

Result: DWT可有效将嵌入维数降低50-93%，语义相似性任务性能几乎无下降，同时对大部分下游任务的准确率有提升。

Conclusion: DWT是一种高效的嵌入压缩方法，能够在减少计算资源消耗的同时保持甚至提升NLP任务表现，为NLP应用带来新的提升途径。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [98] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究发现自ChatGPT发布后，LLM常见词汇在非正式科普播客中的使用显著增加，显示人工智能可能正在影响人类语言用词习惯。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）广泛应用，尤其是生成文本领域，其特殊词汇风格日益渗透进科学与教育的正式书面语。作者想探究这种AI用语风格是否也开始影响真实人类语言（口语），还是只是AI文本生成过程中的现象。

Method: 作者构建了一个包含2210万词的非正式口语数据集（主要来自科普与科技类播客），对ChatGPT发布前后口语中LLM典型词汇出现频率进行趋势分析，与无关的同义词组作对照。

Result: 研究发现，2022年后LLM常见词汇在口语播客中使用频率有明显增高，对比用法与基线同义词无显著变化，说明这是特定AI相关用语的增长。

Conclusion: 短时间内LLM风格用语渗透至人类口语，或许正预示着一场新的语言变化浪潮。这种变化是自然发生还是因AI暴露所驱仍需进一步考察，同时也提示人们关注AI潜在的社会与伦理影响。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [99] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 本文提出了一种注重病因的注意力引导框架，以提升大语言模型在复杂急腹症诊断中的准确性和推理能力，验证结果显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医学文本理解和生成上表现突出，但在复杂临床场景下诊断的可靠性仍有限。该研究旨在提升LLM在医学诊断中的准确性和临床推理能力，使其在真实复杂场景中更值得信赖。

Method: 1）构建基于权威指南的临床推理脚手架（CRS），针对三种急腹症（急性阑尾炎、急性胰腺炎、急性胆囊炎）；2）设计病因感知注意力头识别算法，找出模型中有助于病因推理的注意力头；3）提出推理引导的参数高效微调法，将病因推理线索融入输入，并利用推理引导损失函数引导注意力聚焦关键信息。

Result: 在Consistent Diagnosis Cohort实验中，框架提升了平均诊断准确率15.65%，推理聚焦得分提升31.6%。在Discrepant Diagnosis Cohort的外部验证中同样显著提升诊断准确性。注意力频次评估也表明该方法在复杂实际场景中更可靠。

Conclusion: 本文方法通过将模型的注意力与结构化临床推理对齐，有效提升了LLM在诊断复杂急腹症时的可解释性与可靠性，为构建实用且可信赖的AI诊断系统提供了新范式。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [100] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 本文系统性地评估了裁剪、量化和token丢弃等优化方法对支持长上下文的大语言模型（LLM）在记忆占用、延迟和吞吐量等系统表现上的影响，以及这些优化组合对生成文本质量的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现优异，但存在资源消耗大和上下文窗口有限问题。虽然现有优化方法能够缓解这些问题，但在长上下文以及实际系统评测中的效果尚未充分研究。

Method: 本文针对两种支持长上下文的LLM架构，分别对裁剪、量化和token丢弃等单独优化方法，以及它们的组合进行系统性基准测试，涉及内存、延迟和吞吐量等指标。进一步在更大的70B参数模型上分析单一优化方法的扩展性，并结合系统层和任务层的评测分析结果。

Result: 实验表明，优化方法简单组合在大模型中容易导致近似误差积累，反而影响推理质量。此外，仅仅依赖F1指标容易掩盖实际问答任务中精确率和召回率的权衡。

Conclusion: 本文为业界和研究人员提供了优化LLM时在效率、准确率和可扩展性之间如何平衡的系统性见解，特别是在不同任务和硬件配置下的实际影响。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [101] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: 本文提出了一种改进多模态句子嵌入的方法MCSEO，通过利用更精细的对象-短语对齐机制来提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型在训练时依赖图像-描述对，但这些对中常含噪声，比如多余或无关的信息，影响嵌入质量。作者希望通过解决该问题，提升多模态表示学习能力。

Method: 方法名为MCSEO，核心为结合传统的图像-描述对齐与更细粒度的对象-短语对齐。具体做法是利用现有的分割与目标检测模型，自动提取图像中的对象及与其对应的文本短语，并以此优化针对对象-短语配对的对比学习目标。

Result: 在多种主流骨干模型与语义文本相似性(STS)任务上，MCSEO方法均优于各类强基线表现。

Conclusion: 精确的对象-短语对齐对于提升多模态表示学习能力至关重要，MCSEO可有效增强多模态句子嵌入的表现。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [102] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新型的大模型智能体范式AdaPlan，通过结合全局规划与任务执行，提升智能体对复杂长周期任务的处理能力，并提出了PilotRL训练框架，实验结果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在智能体任务部署中面临诸多挑战，如ReAct范式只关注单步推理，缺乏长远规划能力；执行器与规划器协调不足；依赖有监督微调导致泛化性差。因此，亟需一种能实现更优规划与执行协调、并具有更强泛化能力的新方法。

Method: 提出AdaPlan智能体范式，突出全局规划下的显式引导与执行相结合。以此为基础，设计了PilotRL训练框架，分三步进行：(1)训练模型根据全局计划引导任务执行；(2)优化生成计划的质量；(3)联合优化模型的规划与执行协调能力，并采用渐进式强化学习进行训练。

Result: 实验显示，采用LLaMA3.1-8B-Instruct + PilotRL的方法在智能体任务上超越了闭源的GPT-4o 3.60%，在与GPT-4o-mini参数规模相当时性能提升高达55.78%。

Conclusion: AdaPlan范式和PilotRL框架能有效提升大语言模型智能体在复杂长周期任务中的表现，在规划-执行协调和泛化能力上有显著优势。

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [103] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，让小型语言模型（SLM）通过动态构建和优化自身的任务向量，实现类似大型模型的推理能力，显著提升知识密集任务中的表现。所提出模型Lucy（1.7B参数）在SimpleQA数据集上表现与更大模型媲美。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型因其参数量和能力有限，通常在需要丰富知识和复杂推理的任务上表现不佳。现有方法多将模型推理步骤作为静态或启发式过程，限制了SLM的潜在能力。因此，作者希望探索一种方法，使SLM能像大型模型一样，在任务完成过程中灵活地构建和调整自身的推理策略和任务表示。

Method: 作者创新性地将模型在<think>和</think>标签间的内部推理过程视为任务向量的动态构建和优化。具体地，生成这些内容被视为模型实时创造和细化任务向量，通过提出的RLVR方法（强化学习+向量优化）训练模型，并且集成MCP结构，实现了可以自主检索网络的web-search智能体模型Lucy。

Result: 实验表明，Lucy（1.7B参数）在SimpleQA基准任务上获得了78.3%的准确率，达到了与大型模型（如DeepSeek-V3）相当的水平。

Conclusion: 小型语言模型只要配备结构化、动态的自我推理能力，便能在知识密集型任务中表现得与大模型相当。该工作证明了动态构建任务向量和推理机制在提升SLMs能力方面的可行性和有效性。

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [104] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: 本文提出了EdgeInfinite-Instruct方法，通过分段监督微调和精细的后训练量化，使大语言模型在移动端和边缘设备上高效地处理长序列任务。该方法在保证推理速度和模型精度的同时，降低了内存与计算开销，实验证明其在真实移动应用中的表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在边缘设备上执行长序列任务存在计算复杂度高、KV缓存需求大等问题。现有优化方案在改善内存效率的同时，难以降低首标记延迟并可能影响模型性能。此外，新的建模结构往往需要整体重训练且缺乏工具支持。作者旨在提升现有大模型在移动端的实用性和任务性能。

Method: 作者针对EdgeInfinite模型，提出分段式的有监督微调（S-SFT）以优化其长序列任务能力，并引入细粒度的后训练量化（PTQ），使模型可以高效地部署到边缘NPU设备。此外，还采用固定形状的计算图，根据实际应用场景自定义输入tokens和缓存尺寸，进一步提升计算与内存效率。

Result: 在长上下文基准和真实移动端任务中，EdgeInfinite-Instruct展现出对特定领域指令任务和长序列任务更优的表现，同时维持了NPU加速设备的高效率和模型精度。

Conclusion: EdgeInfinite-Instruct有效弥补了原方法在指令跟随与移动端适配方面的不足，在计算与存储资源有限的边缘设备上，为大语言模型的落地提供了高效且可定制的部署方案。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [105] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文对In-Context Learning (ICL)中示例（demonstration）无效的原因进行了分析，并提出利用梯度流（gradient flow）优选示例的新方法GradS，在多项实验中取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法大多认为所用的示例都是有效的，但实际上并非所有示例都能提升模型表现。因此，需要明确无效示例的成因并提出更优的示例选择策略。

Method: 基于梯度流和线性自注意力模型分析示例无效性的原因。提出GradS方法，通过计算每个示例对于指定用户查询的梯度流大小来进行筛选，仅保留对模型仍有增益作用的有效示例。实验基于四种主流大模型和五个数据集进行。

Result: 分析发现，随模型层次加深，示例有效性分化愈发明显。GradS方法能有效甄别有效示例，平均相较最优基线提升6.8%。

Conclusion: 并非所有示例对ICL有效。通过梯度流筛选方法（GradS）能够显著提高ICL表现，为示例选择提供了理论依据与实证支持。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [106] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的语义感知高斯课程调度（SA-GCS）方法，将课程学习系统性地集成到无人机视觉语言导航的强化学习训练中，有效提升了训练效率和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉语言导航任务强化学习方法在训练数据利用率低、收敛速度慢、未充分考虑样本难度变化等方面存在缺陷，限制了性能提升。

Method: 作者提出了SA-GCS训练框架。该框架包含两部分：1）语义感知难度估算器（SA-DE）用于量化训练样本的复杂度；2）高斯课程调度器（GCS）则根据难度动态调整采样分布，实现由易到难的有序训练。

Result: SA-GCS在CityNav基准上，对比多种强基线方法，在所有评测指标上均有更好表现，并且训练收敛更快、更稳定，对不同规模模型都具备良好泛化能力。

Conclusion: SA-GCS训练框架显著提升了无人机视觉语言导航任务的效率和泛化能力，具有鲁棒性和可扩展性，相关实现已公开。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [107] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 本文探索了将离散小波变换（DWT）应用于词和句子向量嵌入，以进行信息压缩和降维，并结合离散余弦变换（DCT）提出了一种无参数的模型，实现了固定长度、高信息密度的句子表示，在多个NLP应用任务中取得了与原嵌入向量相当甚至更优的效果。


<details>
  <summary>Details</summary>
Motivation: 小波技术已在图像与信号处理领域取得显著进展，其理论与实践成果启发作者将之引入自然语言处理，以更有效地管理和提取嵌入向量中的关键信息，同时实现向量降维和压缩。

Method: 作者将离散小波变换（DWT）应用于词嵌入和句子嵌入，进行信息整合与降维，并进一步结合离散余弦变换（DCT），构建了一种无参数、可压缩信息且定长高密度的句子向量表示模型。通过内在和外在评估方法验证了这些变换的有效性。

Result: 实验结果显示，该方法在多个下游NLP模型与任务上，压缩后的定长向量表现与原始向量相当，部分任务甚至优于原始嵌入，验证了方法的有效性与实用性。

Conclusion: 将DWT和DCT结合运用到NLP中的嵌入向量压缩和信息整理，可在不损失主要语义信息的前提下，有效减少向量维度，并在实际任务中取得良好性能，适合用于后续NLP模型的输入表示优化。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [108] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新型的图神经网络架构——Retrieval-augmented Graph Agentic Network (ReaGAN)，通过引入自主智能代理和检索增强机制，有效提升了图神经网络在信息不均衡和全局语义建模方面的能力，显著提升了在小样本场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在传播机制上存在两大限制：1）无法有效处理节点信息量不平衡的问题；2）消息传递仅基于局部结构，相忽略全局语义联系，导致难以捕捉远距离相关信息。解决上述痛点是本文研究的核心动机。

Method: 作者提出了ReaGAN框架。该框架中，每个节点被赋予智能代理属性，根据自身记忆自主规划行为、传播信息，实现自适应的消息传递。同时，结合检索增强生成（RAG）机制，使节点可以检索语义相关内容，建立起全局语义关联。更重要的是，ReaGAN利用一个冻结的大型语言模型（LLM）作为骨干，无需微调即可运行。

Result: ReaGAN在小样本、上下文有限设定下表现出色，性能达到甚至超越现有方法，验证了节点自主规划和局部-全局检索在图学习中的潜力。

Conclusion: 通过引入智能代理和检索增强机制，ReaGAN突破了传统GNN在信息不平衡和全局建模的瓶颈，为图学习方法的发展带来了新思路，尤其适用于数据有限的场景。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [109] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 本文提出了一种高效的多轮对话质量评估器，将多个大模型评审的知识整合到单一模型，显著降低评估成本，并在多个基准表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前主流的“大模型评审对话质量”方法存在评估偏差，采用多个大模型评审虽能缓解但计算开销大。因此需要一种高效、可靠的对话质量自动评估方法。

Method: 作者设计了一种新评估器，将多个大语言模型作为评审的偏好知识蒸馏整合到单一模型中，实现既保留多评审意见多样性的优点，又大幅降低推理时的计算成本。

Result: 在7个对话质量评估基准（包括单项评分和成对比较）上进行的实验表明，该方法在多种场景下均优于现有主流方法，表现出更高的效率和稳健性。

Conclusion: 新方法兼顾了准确性、效率与多样性，为大模型生成内容的自动评估提供了更可行、便捷的解决方案。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [110] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: 本文介绍了GETALP团队参加SIGDial 2025自动会议纪要挑战赛（第三轮）的成果，重点在会议转录文本上的问答任务。提出结合检索增强生成（RAG）与抽象意义表示（AMR）的三种系统，并验证了AMR在部分问题上显著提升了回答效果，特别是在区分不同参会者身份的问题上。


<details>
  <summary>Details</summary>
Motivation: 自动生成会议纪要和基于会议转录进行问答是自然语言处理中的重要任务。现有方法在辨别参会者及处理复杂语义关系时表现有限，因此探索如何结合RAG和AMR提升问答质量具有重要意义。

Method: 作者提出了三种结合RAG和AMR的方法，将AMR用于提升信息抽取和回答的准确性。在RAG框架下，结合结构化的AMR表示辅助检索和生成过程，从而增强模型对语义和参与者身份的理解能力。

Result: 实验结果显示，结合AMR的系统在约35%的提问中产生高质量回答，尤其在涉及区分会议参与者（如“谁提出某意见”）的问题上取得明显提升。

Conclusion: 结合AMR的RAG系统能有效提升基于会议转录的自动问答能力，尤其适用于对参与者身份及语义细节有较高要求的应用场景。

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [111] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 本论文提出了半真检测任务，发布了包含1.5万条政治声明的新数据集PolitiFact-Hidden，并提出了TRACER方法，有效提升了现有事实核查系统对遗漏信息的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统主要判断声明是否与证据相符，忽视了声明中可能存在的对关键信息的有意或无意遗漏，这会导致半真（既真实又具有误导性）的错误判断。因此，亟需一种能够检测因缺失重要上下文而产生误导的声明的方法。

Method: 作者提出了一个新的任务——半真检测，并构建了包含1.5万条政治声明的新数据集PolitiFact-Hidden，每条声明都标注了与句子级证据的对齐和推断出的声明意图。为解决该问题，提出了TRACER框架，通过对证据的对齐、声明隐含意图的推断以及隐藏信息因果影响的评估，检测基于遗漏的信息误导。

Result: TRACER框架可以集成到现有事实核查流程中，并在多个强基线模型上均提升了性能，尤其在Half-True分类F1指标上提升最多达16个百分点，显著优于以往方法。

Conclusion: 对声明中的信息遗漏进行建模是提升事实核查系统可信度的关键。TRACER方法有效提升了半真检测能力，为构建更值得信赖的事实核查工具打下了基础。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [112] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: 本文提出Flat-LoRA及其高效版本EFlat-LoRA，通过在低秩子空间中寻找平坦极小值点，提升了LoRA的泛化能力，实验证明在多个任务中优于LoRA和全参数微调。


<details>
  <summary>Details</summary>
Motivation: 虽然SAM方法已知能提升CNN和Transformer的泛化能力，但其与LoRA的泛化能力与表现能力之间的相关性尚少人探讨。另外，缺乏在LoRA中实证探索平坦极小值点或开展理论分析的工具。本文旨在填补这一空白，研究sharpness与LoRA模型泛化性的关系。

Method: 作者提出Flat-LoRA和EFlat-LoRA，通过理论证明全参数空间的扰动可转化为低秩子空间，从而在低秩参数适配过程中有效地寻找到平坦极小值点，避免了多矩阵扰动带来的干扰。并在大语言模型和视觉语言模型上进行了广泛实验。

Result: EFlat-LoRA在GLUE（用RoBERTa-large）上比LoRA和全参数微调分别提升1.0%、0.5%；在视觉语言模型Qwen-VL-Chat上，SQA和VizWiz分别提升1.5%、1.0%。算法效率与LoRA相当，性能更优。

Conclusion: EFlat-LoRA不仅提升了LoRA的泛化能力和效果，而且实验证明LoRA的泛化与sharpness密切相关，弥补了前人工作忽略的理论和实证方面的不足。

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [113] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: 本文探讨了表情符号（emoji）与语音语调之间的联系，发现表情符号能够影响说话人的语调表达，并且听众可以仅通过语调猜测出相应的表情符号。语义差异越大的表情符号，其语调差异也更大。


<details>
  <summary>Details</summary>
Motivation: 口语中，韵律特征如音高、节奏、语调有助于表达情感和意图，而文本环境下这些特征缺失，表情符号成为情感与语用信息的替代品。作者希望理解表情符号如何影响语调表达，以及听众是否能仅通过语调还原出表情符号的意义，从而揭示表情符号在数字交流中的语用功能。

Method: 通过结构化但具有一定开放性的语音生成和感知任务，收集了实际语音数据，分析在加入表情符号时发音人语调的变化，并测试听众能否凭语调判断出原本的表情符号。

Result: 说话人在表达不同表情符号时，会根据其语义调整语音韵律特征，听众往往能仅凭韵律特征正确识别出所表达的表情符号。且表情符号间语义差异越大，其在语音中的韵律表现也越有区别。

Conclusion: 表情符号不仅是视觉表意工具，也能作为语调意图的载体，在数字媒介中有效补充口语沟通中的韵律特征，丰富文本交流的情感和语用表达。

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [114] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 本论文提出了一种名为PaPaformer的新型解码器型Transformer结构，通过将低维并行路径组合成更大模型，实现语言模型在数小时内训练完成，大幅缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前大型或小型语言模型的训练耗时数天且需大量算力，极大限制了模型开发和迭代效率，因此需要更高效的训练和架构方法。

Method: 提出PaPaformer架构，将低维的并行路径独立训练（可用不同数据），训练后再组合为完整模型；该结构可减少模型参数、提升效率，并支持根据不同任务定制部分路径。

Result: 实验表明，PaPaformer能在缩短训练时间及降低参数规模的同时，获得更高的性能表现。

Conclusion: PaPaformer为高效训练和灵活定制Transformer模型提供了新的思路，有助于降低算力需求、提升开发效率。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [115] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: 本文提出了SynAdapt框架，通过合成连续链式思考（CCoT）作为大模型对齐目标，并集成难度分类器，实现了更高效且准确的推理能力，尤其在处理复杂问题时表现优越。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思考（CoT）能提升大语言模型的推理表现，但其离散生成过程耗时较长；而连续CoT虽然高效，但现有方法存在无法直接对齐、训练目标不一致等问题，亟需设计一种高效且对齐好的连续推理方法。

Method: SynAdapt框架首先自动生成高质量的合成连续CoT作为训练与指导目标，显式引导模型学习高效推理能力。针对难题，仅依赖CCoT仍不足，故引入难度分类器，结合题目与初步CoT结果检测难度，针对难题自适应追加推理思考，提高准确率。

Result: 在多个数据集与不同难度水平的基准测试中，SynAdapt方法在准确率与推理效率之间达到最佳平衡，实验证明其优越性。

Conclusion: SynAdapt通过创新性的合成和自适应推理策略显著提升了大语言模型面对各类推理任务时的准确性与效率，为后续高效智能推理提供了新方向。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [116] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: 本文提出了一种新的针对大语言模型（LLMs）的置信度估计框架CRUX，通过综合上下文相关性和答案一致性，提高了置信度评估的准确性，并在多个数据集上取得了领先的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM置信度估计方法通常忽视了输出与上下文间的相关性，而上下文相关性对于评估LLM输出的质量，尤其是在需要背景知识的场景中，是十分关键的。因此，亟需一种能有效结合上下文真实性和输出一致性的新方法。

Method: 本文提出CRUX框架，首次将上下文真实性和一致性纳入LLM置信度估计。具体包括两个新指标：（1）上下文信息熵减少，通过对比有无上下文情况下的采样结果，衡量背景知识减少的不确定性；（2）统一一致性检查，衡量模型在有无上下文时生成答案的整体一致性。

Result: 在三个通用（CoQA、SQuAD、QuAC）和两个领域特定（BioASQ、EduQG）数据集上进行实验，CRUX在置信度评估中均取得了比以往方法更高的AUROC表现。

Conclusion: CRUX能有效结合上下文相关性和输出一致性，极大提升了对LLM输出置信度的评估能力，为LLM在安全关键场景下的可靠部署提供了更加可信的支持。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [117] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于图卷积网络（GCN）的孟加拉语主题建模方法GHTM，并在多个数据集上优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 针对孟加拉语在自然语言处理领域中主题建模研究不足、形态复杂、资源匮乏等问题，亟需开发效果更好且适用于孟加拉语的主题建模方法。

Method: 设计了一种基于图卷积网络的混合主题模型GHTM，将文档嵌入为图结构节点，利用GCN获得语义丰富的嵌入表示，再结合非负矩阵分解（NMF）提取主题。模型与多种传统及现代主题建模方法在三个孟加拉语数据集上进行比较。

Result: 实验表明，GHTM模型在主题连贯性和多样性评价指标上均优于LDA、LSA、NMF、BERTopic、Top2Vec等现有方法。此外，作者还发布了一个新的孟加拉语数据集NCTBText。

Conclusion: GHTM有效提升了孟加拉语主题建模的性能，丰富了相关语料资源，有助于相关领域进一步发展。

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [118] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 本报告测试了通过“打赏（tipping）”和“威胁（threatening）”提示来提升AI模型表现的常见观点。结果显示，这两种策略对模型整体表现基本无显著影响，但对单个问题有时会有变化，总体上简单的提示变体并不如想象中有效。


<details>
  <summary>Details</summary>
Motivation: 针对业界和学界流传的‘打赏/威胁’能提升AI表现的观点，通过实验验证这些策略是否在标准测试中有效，帮助相关方面理性看待提示工程（prompting）的实际作用。

Method: 作者使用GPQA和MMLU-Pro两个基准测试集，对比采用普通提示、打赏提示、威胁提示三种方式下模型的表现，分析不同提示对AI回答效果的影响程度。

Result: 1. ‘打赏’和‘威胁’提示对整体测试表现无统计学显著提升；2. 某些单个问题会因提示方式变体表现显著不同，但难以预测何时有积极/消极影响。

Conclusion: 简单的提示策略（如威胁或打赏）不能系统性提升模型的表现，特别是针对困难问题时作用有限。提示方法在具体问题上效果差异大，实际应用中需谨慎使用。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [119] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 现有AI生成文本检测器在现实环境中表现不佳，本文提出了新的检测难题和数据集，并对检测方法展开实验，对未来检测器的改进提出了方向。


<details>
  <summary>Details</summary>
Motivation: 现有AIG文本检测器在内部测试中表现优异，但在实际应用中效果差，尤其对少量或单个示例生成、领域定制模型生成文本的检测能力薄弱，需要通过更具挑战性的数据集和方法探究其鲁棒性。

Method: 作者提出一个新的数据集DACTYL，专注于单样本/少样本生成和领域专用继续预训练（CPT）LLM生成的文本，并对现有主流AIG文本检测器与DACTYL数据集进行评测。此外，作者用标准二元交叉熵（BCE）和最新的深度X-risk优化（DXO）两种方式训练了自有分类器，并分别评估了其效果及泛化能力。

Result: 现有许多检测器在DACTYL数据集上的表现大幅下降，对单样本/少样本及CPT生成文本的检测效果差异明显。BCE优化的分类器在DACTYL测试集表现略优，但DXO优化分类器在分布外（OOD）的文本上表现更好。在模拟学生论文检测场景下，DXO优化分类器在最低误报率下F1分数比BCE高出50.56个百分点，表现出更好的泛化能力。

Conclusion: 深度X-risk优化有望提升AIG文本检测器在真实环境中的鲁棒性和泛化性，当前检测方法对多样和领域适应生成文本存在显著不足，未来AIG检测器研究需高度关注模型对复杂生成条件的适应能力。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [120] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大语言模型（LLMs）用于医学推理领域，从训练和测试两个阶段梳理了提升医学推理能力的主要方法，并评估了其在不同数据模态和临床应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学领域的LLMs虽然表现优异，但在系统化、透明且可验证的推理能力上存在明显不足，而这些能力是临床实践的基础。因此，亟需开发和评估专为医学推理设计的LLMs。

Method: 提出并应用了一套推理增强技术的分类体系，将其分为训练阶段（如监督微调、强化学习）和测试阶段（如提示工程、多智能体系统）技术，并分析了这些策略在文本、图像、代码等数据模态以及诊断、教学、治疗规划等主要临床场景中的应用。同时，系统梳理了从准确率到推理质量和可视化可解释性等一系列评估基准的演进。

Result: 基于对2022-2025年间60篇代表性研究的系统分析，评估了不同推理增强技术在各类场景下的应用成效，并识别出它们在实际部署和推理可信度等方面的优势与不足。

Conclusion: 总结了当前医学推理LLMs面临的关键挑战，包括推理可信度与表面合理性之间的鸿沟，以及对原生多模态医学推理的迫切需求，未来需致力于实现高效、健壮且社会技术负责任的医学AI。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [121] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本研究针对以英语为主的LLM评测短板，专注波斯语和伊朗文化，构建了19个新数据集，评测了41个主流LLM，以填补文化与语言评测的空白。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型LLM主要以英语及欧美文化为训练和评测对象，导致在其他语言和文化，特别是非西方背景下的评测资源和表现都存在明显不足。作者希望缩小这种差距，帮助LLM更好地适配多元文化。

Method: 作者设计并发布了19个专门针对波斯语和伊朗文化（如伊朗法律、波斯语语法、惯用语、大学入学考试题等）的评测数据集，并用这些数据集对41个主流LLM进行系统测评与对比。

Result: 通过新数据集的评测，系统呈现了现有主流LLM在波斯语及伊朗文化相关任务上的表现，为相关模型和社区指出了当前的不足与发展方向。

Conclusion: 本文显著推进了LLM多语言、多文化评测资源的建设，填补了波斯语及伊朗文化语境下的评测空白，有助于推动更具包容性的LLM研究与应用。

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [122] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文提出利用顺序句子对分类器（SSPC）模型检测文档中单句级别的写作风格变换。在PAN 2025官方测评集上，模型均优于基线及高级对比方法。


<details>
  <summary>Details</summary>
Motivation: 在计算机作者分析领域，检测文档中写作风格的变化点，尤其是细粒度如单句级别的检测，是一个长期存在且极具挑战性的重要问题。本任务旨在推动该领域发展，结合更具主题多样性的控制数据集，推动算法细致区分风格切换能力的提升。

Method: 该方法将待分析的句子序列整体建模，首先通过预训练语言模型（PLM）获得单句表示，然后用双向LSTM（BiLSTM）对所有句子顺序建模，捕获上下文信息。接着，将相邻句子的BiLSTM输出向量拼接后，输入多层感知机(MLP)，决定两句之间是否出现风格切换。

Result: 在PAN 2025测试集上，SSPC模型分别在EASY、MEDIUM与HARD三种难度的数据集上取得了0.923、0.828、0.724的macro-F1分数。其性能不仅显著优于随机基线，还超越了claude-3.7-sonnet模型的零样本表现。

Conclusion: 所提出的轻量级顺序句子对分类器能够有效利用上下文信息，特别是在处理风格浅、句子短的困难样本中具备显著优势。该方法为超细粒度风格变换检测任务提供了有效、可扩展的解决方案。

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [123] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文提出TraceRetriever系统，通过模拟实际法律检索，在仅有部分案件信息时，提取具有修辞意义的片段进行法律判例检索。该系统融合多种模型手段，并利用分层BiLSTM-CRF对片段进行修辞标注，在实际数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 面对判例文档数量日益增长及检索系统依赖完整文档的问题，作者希望解决在只掌握案件部分信息情况下如何高效、准确地检索关联判例，帮助提升法律检索的实用性和可扩展性。

Method: 系统提取具有修辞意义的案件片段，并结合传统BM25算法、向量数据库及跨编码器模型，通过Reciprocal Rank Fusion进行结果融合和重排序。片段的修辞标签由在印度判决书上训练的分层BiLSTM-CRF模型生成。

Result: 在IL-PCR和COLIEE 2025数据集上评估，TraceRetriever在仅用部分案件信息的情况下，能够可靠且高效地完成判例检索任务，对比传统检索方法表现更优。

Conclusion: TraceRetriever突破了仅能用完整文档检索的限制，有效解决大规模法律文档场景下的信息获取瓶颈，为仅知部分案件信息时的法律检索提供了可靠且可扩展的方法基础。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [124] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在句子级别写作风格变化检测任务中的零样本性能，发现其明显优于现有基线，对写作风格极其敏感，对内容无关的纯风格信号有较大辨识能力。


<details>
  <summary>Details</summary>
Motivation: 写作风格分析尤其是在句子级别的风格变化检测，是作者分析中极具挑战性的任务。传统方法准确率有限，因此需要评估当前最强大的LLM在该任务中的能力，推动该领域基线提升。

Method: 作者选用四种LLM，在PAN 2024和2025多作者写作风格分析官方数据集上进行零样本评测，对比并分析其在检测细粒度风格变化上的表现。同时，探究模型对语义影响的反应，分析其对风格信号的敏感度。

Result: 实验发现，最新LLM在句子级别风格检测上表现显著优于PAN竞赛的基线方法，并且显示出对写作风格非常敏感，能够较好地区分内容无关的风格特征。

Conclusion: 当前主流LLM已经能够在细粒度风格检测任务中提供强有力的基线，且相比过去，更能捕捉到与内容无关的风格信号，在作者分析等相关应用上具有巨大潜力。

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [125] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文提出了一种名为NyayaRAG的检索增强生成（RAG）框架，通过引入案件事实、相关法律条文和以语义方式检索的先前判例，提高了印度司法判决预测的准确性及解释质量。


<details>
  <summary>Details</summary>
Motivation: 以往印度司法判决预测主要依赖案件内部内容（如事实、争议、推理），但忽略了普通法的核心要素——法律条文和判例的引用。作者为弥补这一不足，提出结合更多法律知识的方法。

Method: 提出并实现了NyayaRAG框架，将案件事实、法律条文及语义检索得来的相关先例作为模型输入，通过专门针对印度法律体系设计的流程进行判决预测和法律解释生成。使用词法、语义评估指标及大模型评测（如G-Eval）对不同输入配置下的系统表现进行评估。

Result: 实验结果表明，在输入案件事实的基础上，进一步引入结构化法律知识（法律条文和判例）能显著提升判决预测的准确率和法律解释的质量。

Conclusion: 引入结构化法律知识，是提升印度司法判决自动化预测与可解释性的重要路径。NyayaRAG为结合多源法律信息的自动判决系统提供了有效范例。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [126] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的知识图谱问答（KGQA）方法——DAMR，通过动态自适应的蒙特卡洛树搜索（MCTS）结合LLM和Transformer打分器，提升了问答的准确率和效率，在多个基准上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法要么过于依赖静态路径提取，适应性差，要么依赖大语言模型（LLM）进行动态推理，计算成本高且路径评估不准。因此，急需开发一种兼具效率与推理适应性的KGQA方案。

Method: 作者提出Dynamically Adaptive MCTS-based Reasoning (DAMR) 框架，结合MCTS树探索（由LLM引导，动态选择相关关系以减少搜索空间）与轻量级Transformer判别器（根据问题和关系序列编码，提升路径评估的上下文感知能力）。DAMR还引入动态伪路径优化机制，用于持续生成训练信号，不断适应推理路径分布。

Result: 在多个公开KGQA基准数据集上，DAMR方法实现了显著的性能提升，超越了当前最优的方法。

Conclusion: DAMR框架通过结合符号搜索和自适应路径打分，极大提升了KGQA的效率与问答准确率。其动态伪路径机制也为低监督推理提供了可行方向，有望推广至广泛KGQA应用场景。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [127] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 本论文探讨了大型语言模型（LLMs），如OpenAI GPT 4o，能否利用训练数据中的信息进行推理，尤其是“脱离上下文的溯因推理”。研究发现GPT 4o能够根据对话示例和行为描述，推断出特定虚构聊天机器人的名称及其行为特征。结论对LLMs的情境意识和AI安全有重要意义。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在海量语料上进行训练，但它们是否能合理利用训练数据中的信息进行推理，尤其是在缺乏直接上下文的情况下，仍存在疑问。作者希望评估LLMs在类似场景下的拟人化推断能力。

Method: 设计了针对“脱离上下文的溯因推理”的实验：向GPT 4o提供虚构聊天机器人的姓名与行为描述，未直接展示这些机器人聊天对话的示例，然后让模型根据观察到的聊天行为推断出聊天机器人的身份和特点。对有无行为描述的两组LLM分别进行实验，比较推断结果。

Result: GPT 4o能根据样本聊天回复正确推断出至少一个机器人的名字。此前若对GPT 4o训练过该机器人的行为描述，则该模型在后续迭代训练中表现出更具代表性的相关行为特征。

Conclusion: GPT 4o具有一定的“情境意识”，能从训练数据抽取并推断相关信息。这一能力对LLMs的解释性、可控性和AI安全带来启示和挑战。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [128] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 本论文使用基于GPT-4的大语言模型生成式代理，重现HEXACO人格量表实验，探索其在社会科学研究中的有效性。实验发现这些AI代理展现出部分与人类一致的人格结构，但不同模型间存在描述差异与偏见。


<details>
  <summary>Details</summary>
Motivation: 随着生成式大模型日益逼真的语言能力，许多社会科学实验希望以AI代理替代真人参与者，以提升效率和可控性。本研究旨在验证，基于大模型的AI代理在模拟复杂人格特质实验中，是否能够有效代表人类群体，为未来社会科学研究的设计提供参考。

Method: 研究团队基于GPT-4，设计并赋予310个人格代理各自的角色设定，让其完成HEXACO人格问卷。对AI代理的回答结果进行因子分析，并与2004年Ashton等人的原始HEXACO实验数据进行比对，评估其人格特质结构的可恢复性与一致性，同时比较不同大模型下的结果差异。

Result: AI代理结果展示出较为一致、可靠的人格因子结构，与HEXACO理论部分契合。GPT-4生成的人格纬度具有较好的一致性与可靠性，但在跨模型分析时发现，不同模型对人格特质的刻画存在差异和一定偏见。

Conclusion: 基于大模型的人格代理能够在一定程度上重现人类人格结构，可作为社会科学研究的一种工具，但存在模型偏见与代表性局限。研究为设计更具代表性的人格代理提供了指导，也指出未来在使用AI代理代替人类实验时需要注意的实践问题。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [129] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 本文提出了一种新型的agentic RAG（基于代理的检索增强生成）框架，让大语言模型能够自主分解医学影像问答任务，更高效地检索、整合临床证据，并生成更加准确的答案。实验显示，该方法尤其显著提升了中小参数规模模型的诊断准确率，降低了幻觉发生率，并改善了事实基础性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在影像问答任务中只进行单步检索，难以应对复杂的临床推理和多证据整合的问题。现有大语言模型虽然有潜力，但在诊断准确性、事实性以及对小/中等规模模型的支持等方面仍有限。

Method: 作者提出了agentic RAG框架：LLM能够自主将复杂问题分解，迭代性检索Radiopaedia数据库获取相关证据，并动态综合证据生成基于事实的答案。该方法在104道专家整理的影像问答题（RSNA-RadioQA、ExtendedQA数据集）上，对24个LLM（涵盖不同规模、结构和微调方式）进行实验比较。

Result: agentic RAG显著提升了平均诊断准确率（73%对比传统的64%-68%）；特别在中小规模模型（如Mistral Large和Qwen 2.5-7B）上提升最大，提升幅度达9%-16%；对于超大模型提升有限。该方法还减少了幻觉（9.4%），提升了临床相关证据检索率（46%），且与临床微调模型互补。

Conclusion: agentic RAG框架能有效提升影像问答领域中大语言模型的准确性与事实性，特别是对中小型模型帮助显著。未来应进一步研究其在真实临床场景中的应用价值。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [130] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文提出了一种基于GLiNER理念的新模型GLiDRE，在文档级关系抽取任务中，特别是在few-shot场景下表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档级关系抽取方法大多依赖庞大的模型，对跨句子实体关系建模复杂，而在零样本/少样本场景下表现和适应性不足。作者受到GLiNER在命名实体识别任务中优于大型模型的启发，想要开发更轻量、高效的关系抽取模型。

Method: 提出GLiDRE模型，借鉴GLiNER的紧凑高效思想，针对文档级关系抽取场景设计。并在Re-DocRED数据集上，分别在全数据、零样本和少样本多种设定下，与现有SOTA模型做全面比较。

Result: GLiDRE在Re-DocRED数据集的few-shot（少样本）场景下，实现了优于当前最先进模型的表现，同时也在其他场景中有竞争力。

Conclusion: GLiDRE证实了轻量级模型也能在文档级关系抽取中取得极佳效果，尤其在少样本场景下具有明显优势，为该任务的模型设计与应用提供了新方向。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [131] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 本文提出了一个多模态BERT（MMBERT）框架，有效提升了中文社交网络中的仇恨言论检测，特别是在应对文本掩蔽规避手法上。


<details>
  <summary>Details</summary>
Motivation: 中文社交网络上的仇恨言论检测困难，因目标往往利用各种隐藏技术避开传统文本检测方法。已有研究多聚焦于英文数据，并且较少关注多模态（文本、语音、视觉）融合。为此，需要新的多模态方法提升中文场景下的检测准确性和鲁棒性。

Method: 提出MMBERT模型，将文本、语音和视觉三种模态输入，通过Mixture-of-Experts（MoE）架构融合处理。为解决MoE与BERT集成时的不稳定性，引入三阶段渐进式训练，并设计了模态专家、共享自注意力机制及基于路由的专家分配策略。

Result: 在多个中文仇恨言论数据集上，实验结果显示MMBERT在检测准确率和鲁棒性方面大幅优于微调BERT、微调大语言模型（LLMs），以及利用上下文学习的LLM方法。

Conclusion: MMBERT框架显著提升了中文多模态仇恨言论检测能力，尤其对抗文本遮蔽等对抗性攻击显示了更强的鲁棒性，优于现有主流方法。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [132] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文提出了一个基于大语言模型（LLM）的代码生成系统，面向SemEval-2025 Table QA任务。在两个子任务下，通过零样本方法与高级代码生成，有效提升了表格问答的表现，在竞赛中取得了较好排名。


<details>
  <summary>Details</summary>
Motivation: 表格数据的自动化问答具有实际应用价值，但现有方法在应对多领域与零样本任务时存在局限，亟需更高效且通用的方案。作者希望通过LLM推动表格问答系统的自动化和实用性。

Method: 系统采用先进的开源大语言模型，以优化的prompt策略自动生成可执行的Pandas Python代码，直接从问题到代码实现表格问答任务。对不同LLM在代码生成上的有效性进行了实验比较。

Result: 实验显示，不同LLM在Python代码生成能力上表现不同，代码生成方法在表格问答任务中的表现优于其它方法。系统在30个超越基线的开源系统中，子任务I获得第8名，子任务II获得第6名。

Conclusion: 基于LLM的代码生成框架在表格问答任务中展现出强大潜力，特别是在零样本场景下。该方法为提升复杂数据理解与自动问答能力提供了新思路。

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [133] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出了MISGENDERED+，一个用于评估大语言模型（LLM）在包容性代词使用上的新基准，涵盖更多最新模型和多种测试情景。结果显示部分代词表现提升，但新型与逆推任务仍有不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感和要求公平性的场景中应用增多，其在性别中性和非传统代词（neopronouns）上的表现日益重要。现有基准（如MISGENDERED）仅适用于旧模型，也缺乏多样化评价，无法满足当前需求。

Method: 作者开发了MISGENDERED+这一新基准，扩展了原有测试内容，并选择了五个主流LLM（GPT-4o、Claude 4、DeepSeek-V3、Qwen Turbo、Qwen2.5），对其在零样本、少样本和身份推断任务中的代词准确性进行系统评测。

Result: 相较以往研究，新一代LLM在二元性别及中性代词准确率上有明显提升，但在新兴代词（neopronouns）和逆向身份推理任务中的表现仍不稳定。

Conclusion: 虽然LLM在包容性代词处理方面取得进步，但在身份敏感推理和新型代词领域仍有提升空间，作者也指出了具体改进方向和未来研究的可能路径。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [134] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新策略DAEDAL，解决扩散大语言模型(DLLMs)需要预设静态生成长度的难题，实现动态自适应文本长度生成，并提升了模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的DLLMs因需要静态预定义生成长度，导致复杂任务下输出长度不够会影响性能，而预留过多长度又会增加不必要的计算和性能下降，因此亟需一种动态调整生成长度的方法。

Method: 作者提出DAEDAL，无需重新训练模型。它分两个阶段：第一阶段，利用内部信号和序列完成度指标自适应扩展文本长度至合适范围；第二阶段，在去噪过程中识别内容不足区域并插入mask token，保证最终输出完整。

Result: 实验证明，DAEDAL在多个任务上性能可与精调后固定长度的DLLMs媲美，有时更好，并能提高有效token比率，从而提升计算效率。

Conclusion: DAEDAL突破了DLLMs静态长度限制，使其接近甚至超越自回归大模型的灵活性和效率，为DLLMs在实际生成任务中开拓了新的应用空间。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: 本文提出了XRoboToolkit，一个基于OpenXR标准的跨平台扩展现实机器人远程操作框架，实现了低延迟视觉反馈、多模态跟踪和逆向运动学优化。XRoboToolkit可用于多种机器人平台，并通过示范精细操作任务和VLA模型训练验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型的发展需要大量高质量机器人演示数据，然而现有远程操作数据采集方式存在扩展性差、部署复杂及数据质量不足等问题，亟需更优的解决方案。

Method: 提出基于OpenXR的XRoboToolkit框架，支持跨平台的扩展现实机器人远程操作。系统具备低延迟立体视觉反馈、基于优化的逆向运动学算法及多种跟踪方式（如头部、手柄、手和辅助跟踪器）。模块化架构可集成至不同机器人和仿真环境。

Result: 在精密操作任务中展示了框架的有效性，并通过训练VLA模型，模型表现出较强的自主任务能力，验证了数据采集的高质量。

Conclusion: XRoboToolkit在提升机器人远程操作数据采集的易用性、扩展性和数据质量方面效果显著，为VLA相关研究与应用提供了坚实的数据基础和技术支撑。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [136] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种新型的可重构人形机器人远程操作系统CHILD，实现了标准婴儿背带式便携设计，支持全身关节级控制和动态力反馈，并已通过多种实体机器人系统验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有远程操作系统多局限于部分身体或特定动作，人形机器人无法实现全身复杂操作，严重限制其在多样任务中的应用。作者希望通过创新设计打破这种局限，实现更直观、更全面的交互控制。

Method: 设计了体积小巧且可穿戴的CHILD系统，用户通过穿在身上的“婴儿背带式”设备可实时操控机器人四肢关节动作，系统支持直接关节映射、全身和动作协同控制，并通过自适应力反馈提升体验和安全性。

Result: 在多类型人形及双臂机器人系统上测试，能够成功实现行走-操作一体化（loco-manipulation）及全身控制，系统以硬件开源形式发布，方便扩展和复现。

Conclusion: CHILD系统实现了人形机器人全身、实时、细粒度的远程操作，提升了机器人互动性与可操作性，具备实际应用和进一步研究推广价值。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [137] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 本文提出一种基于拓扑学的软体连续体机器人形态描述方法，并实现了机器人形态的定量描述、分类与控制。


<details>
  <summary>Details</summary>
Motivation: 现有软体连续体机器人因形态多变，缺乏有效的定量描述与分类方法，限制了其医学等领域的精确应用。

Method: 结合伪刚体（PRB）模型与Morse理论，通过统计各方向投影的临界点，将机器人形态离散化，并进行分类。同时，利用该描述符，将目标形态控制转化为优化问题，计算产生所需拓扑特征的驱动参数。

Result: 实现了对软体连续体机器人多模态形态的有效描述、离散分类及目标形态控制。

Conclusion: 该框架为软体连续体机器人的形态定量描述、分类与控制提供了统一方法，预计将提升其在微创外科和血管介入等医学领域的精度与适应性。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [138] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: 本文提出了一个用于无人机在开放世界中目标物体导航的大规模基准——UAV-ON，并表明现有方法在这一设置下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用依赖连续语言指令的视觉-语言导航范式，限制了规模化和自主性。为推动无人机在复杂、无结构环境下利用高层语义目标进行自主导航，亟需新的基准和方法。

Method: UAV-ON基准覆盖14个高保真虚幻引擎环境，涵盖多种场景，定义1270个有实例级说明的目标物体，提供类别、物理特征和视觉描述等高层语义目标。通过实现基线方法（包括集成指令语义和自我中心观测的Aerial ObjectNav Agent），评估无人机的长跨度目标导航能力。

Result: 所有基线方法在该基准下均表现不佳，显示出航拍导航与高层语义目标理解的复合挑战。

Conclusion: UAV-ON基准填补了无人机自主导航领域在高层语义目标驱动、复杂环境中的研究空白，有望推动更可扩展的无人机自治导航技术发展。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [139] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种结合拓扑地图信息的扩散模型框架，用于多模态的轨迹预测，能生成符合道路几何的多样性轨迹。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和轨迹预测领域对准确、可靠且能够适应道路结构的未来运动预测有强烈需求。以往方法无法充分利用地图结构信息，或需要复杂的显式约束。作者希望发展一种直接结合拓扑地图信息、自然服从道路几何的预测方法。

Method: TopoDiffuser利用条件扩散模型，将拓扑地图的结构信息嵌入去噪过程，提升模型对道路几何的敏感性。通过多模态融合编码器，将激光雷达数据、历史轨迹及路线信息统一到鸟瞰视图（BEV）中，构建整体输入。

Result: 在KITTI数据集上，TopoDiffuser在精度与多样性方面超越了现有主流方法，并展现出很强的道路几何一致性。消融实验验证了各输入模态、去噪步骤以及采样数量的作用和贡献。

Conclusion: TopoDiffuser为多模态轨迹预测提供了一种高效、可推广的解决方案，有效融合道路拓扑信息，表现优越且便于扩展。相关实现代码已开源，有助于该领域今后的研究。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [140] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的3D物体数字化建模流水线Omni-Scan，利用双臂机器人和单固定相机，通过重新抓取和旋转目标物体，实现高质量、全视角的3D Gaussian Splat模型自动生成。尤其适合用于瑕疵检测、虚拟现实等场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体扫描方法对设备有较高要求（如多摄像头阵列、激光扫描仪、或受限工作空间的机器人相机），且难以实现全视角扫描或自动化，限制了其在工业等场景的应用。本工作旨在简化操作流程，提高3D建模质量与自动化程度，突破工作空间和视角受限的瓶颈。

Method: 设计了Omni-Scan机器人流水线：一只机械臂抓取并旋转物体，单一固定相机采集图像，随后由第二只机械臂重新抓取以补全被遮挡部分。结合DepthAnything、Segment Anything和RAFT光流模型有效分割目标并去除机械臂和背景。针对由机械臂遮挡的问题，优化3D Gaussian Splat训练流程以支持多次拼接数据集，最终生成全方位、高质量3D模型。

Result: 在12种工业和家用物体上进行了验证，Omni-Scan能够以平均83%的准确率识别视觉和几何缺陷，展示了实用性和鲁棒性。支持交互式3D模型展示。

Conclusion: Omni-Scan显著降低了高质量3D数字孪生建模的门槛，可自动化生成全视角、高精度的3D Gaussian Splat模型，具有很强的工业和商业落地潜力，尤其适用于零部件缺陷检测等任务。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [141] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的时间优化策略（TOP），在仿真和现实中实现了更稳健、精确、高效的仿人机器人站立操作控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证高维上半身操作的精确控制与整体的鲁棒性，特别是在上半身动作快速时，机器人的站立操作表现受到限制。

Method: 1）用变分自编码器（VAE）基于动作先验建模上半身运动，提升上下半身协同；2）将全身控制解耦为上半身PD控制器（保证精度）和下半身强化学习控制器（保证鲁棒性）；3）结合TOP方法调整上半身动作的时间轨迹，减少由于快速动作给平衡带来的负担。

Result: 在仿真和实物机器人实验中验证了该方法的有效性，并在站立操作任务中表现出较传统方法更好的稳定性和准确性。

Conclusion: TOP方法可以有效提升仿人机器人在执行快速度复杂操作时的稳定性与精确性，有助于推进仿人机器人在多样化操作任务中的应用。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [142] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的全身运动模仿框架，使人形机器人能够更精确地模仿人类动作，同时保持平衡并抵抗外部干扰。


<details>
  <summary>Details</summary>
Motivation: 人类和人形机器人在运动学及动力学方面差异较大，导致机器人难以准确模仿人类动作并保持稳定。因此，需要一个能提升模仿精度与鲁棒性的机器人运动模仿方法。

Method: 提出了一种基于接触感知的全身运动重定向方法，生成模仿人类动作的参考轨迹初值，并结合非线性质心模型预测控制器以确保动作精度和平衡性。同时通过全身控制器实现精准的力矩控制。该方法在仿真和真实机器人上进行了多种动作模仿实验。

Result: 实验表明，无论在仿真还是真实环境中，所提方法能高效、准确地模仿多种人类动作，并具备良好的适应性和稳定性。

Conclusion: 所提出的框架能有效提升人形机器人模仿人类动作的精度和适应性，验证了方法的有效性。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [143] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种用于多智能体交通仿真的概率框架Neural Interactive Agents（NIVA），基于分层贝叶斯模型，实现了可控、闭环的真实场景仿真，实验证明性能优良并具备更好的行为控制能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车的快速部署，亟需更真实、可扩展的多智能体交通仿真器以高效评测算法。已有方法难以兼顾多样性、交互性与真实闭环仿真需求。

Method: 提出了基于分层贝叶斯模型的NIVA，通过自回归地从有限的高斯混合潜变量采样，实现了观测条件下的闭环多体仿真，并将轨迹预测和闭环仿真方法统一于贝叶斯推断框架。

Result: 在Waymo Open Motion数据集上，NIVA实现了与现有方法相当的仿真表现，同时还能实现对于智能体意图和驾驶风格的细致可控。

Conclusion: NIVA框架为多智能体交通仿真提供了统一、可控且精度可靠的解决方案，有助于推进自动驾驶仿真评测和智能体行为建模的发展。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [144] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 该论文提出了一种名为 SubCDM 的方法，通过仅利用机器人群体的部分子集即可实现资源高效的集体决策，同时保持与全体参与决策相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人集群的集体决策通常要求所有机器人全部参与，导致资源消耗巨大，限制了机器人在其他任务中的分配与应用。因此，研究如何减少参与决策的机器人数量，提高资源利用率成为亟需解决的问题。

Method: 提出了一种动态、去中心化的子集决策方法（SubCDM），其特征在于采用局部信息，动态确定和调整参与决策的机器人子集规模，以适应不同难度的共识任务。整个子集的构建过程无需中心化控制，机器人自主协作完成。

Result: 在100个机器人仿真实验中，SubCDM 方法能在仅使用部分机器人参与决策的情况下，获得与全员参与时近似的决策准确率，同时显著降低了集体决策所需的机器人数量。

Conclusion: 该方法可在保持高决策准确率的同时，极大提升资源使用效率，为群体机器人系统的集体决策场景提供了一种更优的解决方案。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [145] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出了一种基于模仿学习的方法（HannesImitationPolicy），用于控制假肢手在无结构环境中的抓取任务，并展示了其优异的实验表现。


<details>
  <summary>Details</summary>
Motivation: 现有假肢手的自主控制多依赖于摄像头以及其他传感器，虽然可以减轻用户的认知负担，但其对抓取和操作等复杂任务的数据采集方式繁琐。机器人领域中的模仿学习在提升复杂操作任务学习方面迅速发展，但在假肢手控制中的应用尚未深入探索。本研究旨在弥补这一空白，提升假肢恢复灵巧性并扩展其实际应用场景。

Method: 作者提出一种基于模仿学习的假肢手控制方法HannesImitationPolicy，并构建HannesImitationDataset数据集，包含桌面、货架和人与假肢交互等多场景的抓取演示。利用这些数据训练单一扩散策略模型，通过预测手腕方向和手部闭合程度实现对象抓取。最后，将模型部署到Hannes假肢手上。

Result: 实验结果显示，该方法能适应多种物体和不同场景下的抓取任务，并且在无结构场景中表现优于基于分割的视觉伺服控制器。

Conclusion: 基于模仿学习的控制策略可显著提升假肢手在多样且无结构场景下的抓取能力，有助于推广假肢手的实用性和灵活应用。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [146] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: 该论文提出了一种新的基于Transformer的神经网络（OmniUnet），用于机器人在非结构化环境下的多模态场景分割，结合RGB、深度和热成像信息，提高了对复杂地形的识别能力，并成功应用于类火星地形的实际场景中。


<details>
  <summary>Details</summary>
Motivation: 在非结构化自然环境（如火星表面）下，机器人导航需要依靠多种类型的环境信息来做出准确判断。但如何有效利用和融合不同传感器（RGB、深度、热成像）获得的异构信息，一直是个技术挑战。

Method: 开发了OmniUnet，这是一种基于Transformer的神经网络架构，可处理RGB、深度和热成像（RGB-D-T）多模态数据，进行语义分割。研究者自制3D打印多模态传感器模组并安装在火星车实验平台上，在西班牙Bardenas半沙漠收集并标注数据，采用监督学习训练网络，并在硬件资源受限的设备（Jetson Orin Nano）上进行推理测试。

Result: 网络在复杂非结构化地形的分割任务上表现优异，像素级分割准确率达到80.37%。在Jetson Orin Nano上平均预测时间673毫秒，证明了其实时在机器人的适用性。

Conclusion: OmniUnet能有效融合多模态感知信息，大幅提升了机器人在类火星复杂环境下的场景理解能力，适合嵌入式部署。网络模型和标注数据集已开源，有助于推动行星机器人多模态地形感知研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [147] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 本文提出了一种用于人-机器人协作搬运物体的控制方案，采用搭载MIGHTY吸盘的四足机器人，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 人-机器人协作搬运任务需要安全高效，传统方法在可控性与人类操作负担之间存在权衡，且需防止物体脱落，因此亟需新的控制策略。

Method: 基于顺应控制（admittance control），设计了带有可变阻尼项的控制方案，提升人类的可控性并降低操作负担。同时，加入基于障碍人工势场的附加控制信号，以防止搬运过程中物体从吸盘脱落。

Result: 在Unitree Go1四足机器人与MIGHTY吸盘的实验平台上，展示了控制方案的可行性和性能，证明了其被动性和良好的协作效果。

Conclusion: 本文的控制方法在提升人-机器人协作效率和安全性方面效果突出，为未来协作搬运系统提供了新的思路和实现方式。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [148] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout是一款用于科研和工业的开源移动机器人，v1.1版本增加了更简化、更廉价且更强大的机载计算硬件，并实现了ROS2接口和Gazebo仿真。


<details>
  <summary>Details</summary>
Motivation: 推动开源硬件移动机器人平台的可获取性和实用性，降低成本、提升性能，便于科研及工业界使用和开发。

Method: 对OpenScout机器人硬件系统进行迭代升级，设计并集成新型计算硬件，扩展软件以支持ROS2接口，并开发Gazebo仿真环境，采用案例研究方法对更改的动机和效果进行分析。

Result: v1.1版本实现了硬件简化、成本下降和性能提升，成功集成了ROS2接口及仿真支持。

Conclusion: OpenScout v1.1有效提升了开源硬件移动机器人的性能、可用性和扩展性，为相关领域的研究和应用提供了更好平台。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [149] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 该论文提出了一种基于数据驱动方法的踝关节外骨骼协助系统，并首次在中风后步态障碍人群中实现了自适应辅助和实时控制原型。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动外骨骼控制多应用于健康年轻成人，但由于中风步态数据稀缺与异质性大，使此类方法难以用于中风后步态障碍患者。解决这一挑战有助于推动外骨骼在实际中风康复中的适用性与有效性。

Method: 作者收集了4名中风后步态障碍患者的跑台步行数据，采用多任务时序卷积网络（TCN）进行训练。模型还在6名健康人数据上进行预训练，仅用三个IMU信号作为输入。最后，将该数据驱动踝力矩估算方案集成到一套可穿戴外骨骼原型中，在一名中风患者身上进行了实时控制验证。

Result: 模型力矩估算的R^2达到0.74±0.13，初步实现了对中风后步态障碍患者步行中踝关节力矩的准确估算。原型系统在一名患者上展示了其实时感知、估算与执行的可行性。

Conclusion: 论文展示了将数据驱动方法应用于中风后步态障碍患者的可行性，为实用自适应外骨骼康复助力控制奠定基础，并验证了早期实时控制原型的可用性。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [150] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: 本文提出了LightDP框架，通过网络压缩和采样步骤精简，实现扩散策略在移动设备上的实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在机器人模仿学习任务取得进展，但在计算资源有限的移动平台上因计算效率低和内存占用大难以应用。

Method: 1) 分析现有扩散策略架构，确定主要瓶颈在去噪网络。2) 提出统一剪枝-再训练流程，提升剪枝后模型恢复能力。3) 结合剪枝与一致性蒸馏，减少采样步骤同时保证动作预测准确率。

Result: 在PushT、Robomimic、CALVIN和LIBERO等标准数据集上，LightDP实现了在移动设备上的实时动作预测且性能有竞争力。实际实验表明其表现与最先进扩散策略相当。

Conclusion: LightDP实现了扩散策略在资源受限环境下的实用部署，为移动端机器人应用带来突破。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [151] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 本文提出了一种利用视频生成促进机器人策略学习的新框架，通过生成机器人执行任务的视频辅助策略提取，实现更少演示数据下的高效和鲁棒学习，并在模型泛化性上超过了传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人视觉-动作策略受限于泛化能力弱和需要大量人工演示数据，这极大限制了其实用性，亟需兼顾泛化性和数据高效性的解决方案。

Method: 提出Video Policy模块化框架，将视频生成与动作生成结合，端到端训练。通过学习机器人任务执行的视频，帮助策略从极少量演示中学习，提升泛化与采样效率。利用大规模视频生成模型，无需过多动作标注数据。

Result: 该方法在仿真与现实环境中，均能对未见过的物体、背景和任务强泛化，大幅提升了鲁棒性和样本效率。相比传统行为克隆方法，在多个评测维度上达到了更优表现。视频数据的引入尤其有助于泛化新任务。

Conclusion: 利用视频生成提升机器人策略学习，可极大降低对演示数据的需求，并增强对新场景和任务的泛化能力，是实现数据高效型机器人学习的重要方向。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>
