<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 105]
- [cs.CL](#cs.CL) [Total: 30]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的类脑（神经形态）事件驱动型眼动追踪模型，大幅降低了算力和能耗，同时保持高精度，适合可穿戴设备的实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统帧式眼动追踪方法难以满足可穿戴系统对低延迟和毫瓦级功耗的需求，并且易受运动模糊、高计算成本和时间分辨率有限等问题的影响。尤其是在AR、VR等新兴技术中，精确的用户视线追踪对于沉浸感和交互设计至关重要。

Method: 作者提出将表现优异的事件型眼动追踪模型的循环和注意力模块替换为轻量化的LIF（Leaky Integrate-and-Fire）神经元，并运用深度可分离卷积来进一步简化模型复杂度。由此将原模型改造为类脑的脉冲神经网络（SNN），提升其效率。

Result: 所提出的模型在精度上取得了3.7-4.1像素的平均误差，接近专用系统Retina（3.24像素）的水平。同时，相比最接近的ANN变体，模型尺寸降低了20倍，理论计算量降低了850倍。预计以3.9-4.9毫瓦功耗、3毫秒延迟、1kHz速率运行。

Conclusion: 高性能的事件型眼动追踪模型能够以SNN的方式重构，显著提升算力和能耗效率，并保持适合实时可穿戴部署的准确性。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ABBSPO的新方法，用于水平框（HBox）弱监督的定向目标检测，通过自适应框缩放和对称先验角损失，显著提升了现有方法的精度。


<details>
  <summary>Details</summary>
Motivation: 在弱监督情境下，完全监督的定向目标检测成本高昂且效率低下。现有的HBox监督方式存在标注框与预测旋转框尺寸不准确的问题和无效的角度学习，导致检测精度受限。因此需要新的方法提升精度和鲁棒性。

Method: 提出ABBSPO框架，包含：（1）自适应边界框缩放（ABBS）：针对每个预测的旋转框，动态调整GT HBox的尺寸，以获得更精确的尺度估计；（2）对称先验角损失（SPA loss）：利用空中目标的对称性，引入自监督，缓解三种增强视图（原图、旋转、翻转）预测均失败时梯度消失的问题。

Result: 大量实验表明，ABBSPO在弱监督定向目标检测任务中达到了最新的最优性能，明显优于现有方法。

Conclusion: ABBSPO通过引入自适应标注框缩放和对称性引导的角度损失，显著改善了弱监督水平框定向目标检测的准确度，为低成本、高效率的目标检测提供了更好的方案。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

</details>


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型与自回归方法的新生成范式，实现了有竞争力甚至领先于现有最佳方法的视觉生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散去噪模型在生成任务中表现良好，但在离散领域未能超越传统自回归方法。为充分利用两者优点，作者希望探索协同而非替代的方式提升生成效果。

Method: 提出了"Show, Suggest and Tell"（SST）框架，将扩散模型作为自回归生成的建议模块，利用扩散模型的双向修正和自回归方法的强语言结构，对生成过程进行辅助和优化。

Result: SST在COCO数据集上达到了125.1的CIDEr-D分数（无强化学习），超过了目前的自回归和扩散SOTA方法（分别多出1.5和2.5分），且通过大量实验验证了建议模块的有效影响。

Conclusion: 结合扩散和自回归的建议式生成模型在质量和性能上取得了突破，建议和生成内容相关性强，为未来该方向研究提供新突破。

Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.

</details>


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成式扩散建模框架MetaVoxel，可以同时建模医学影像和临床元数据，支持多任务统一建模和灵活推断，且不需针对每个任务单独训练模型。实验证明其性能与常规专用模型相当。


<details>
  <summary>Details</summary>
Motivation: 目前大多数深度学习方法只针对特定任务和特定输入训练模型，难以灵活处理多种任务、不同输入组合，限制了医学AI的普适性和临床应用范围。

Method: 提出MetaVoxel框架，通过单一扩散过程联合学习医学影像和临床元数据的联合分布，实现多任务（如影像生成、年龄估计、性别预测）的统一建模和灵活推断。利用逾1万例MRI及相关元数据对其进行训练和验证。

Result: MetaVoxel模型在影像生成、年龄估计、性别预测三项任务上都达到了与各自常用专用基线模型相当的性能，并展示了可使用不同输入组合进行灵活零样本推断的能力。

Conclusion: 联合多模态扩散模型MetaVoxel为医学AI模型的统一化和临床广泛适用性提供了新方向，具备灵活推断和多任务处理能力，显示了良好的应用前景。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

</details>


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 本文提出一种名为独立密度估计（IDE）的方法，提高大规模视觉-语言模型的组合泛化能力，通过学习词语与图像特征的独立对应关系，实现更强的组合泛化，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模视觉-语言模型在图像描述和生成等任务中取得了巨大成功，但仍然难以实现类似人类的组合泛化能力——即能对未见过的词语组合做出合理理解和生成。本文旨在解决这一核心挑战。

Method: 作者提出独立密度估计（IDE）方法，学习句中每个词与对应图像特征的独立关系。基于IDE理念，构建了两种模型：一是用完全解耦视觉表征为输入，二是利用变量自编码器（VAE）从原始图像中获得部分解耦特征。此外，提出基于熵的组合推断方法，将句中每个词的独立预测组合起来提升整体泛化能力。

Result: 在多个数据集上的实验表明，作者提出的模型在组合泛化（即对未见过的词与特征新组合的理解）上表现优于现有视觉-语言模型。

Conclusion: 独立密度估计方法有效提升了视觉-语言模型的组合泛化能力，能够更好地处理新颖的词语与图像特征组合，为视觉-语言理解和生成任务带来改进。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

</details>


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: 本文提出TraceFlow框架，提升动态镜面场景中高保真渲染效果，采用创新的2D高斯溅射表示与混合渲染流程，实现更准确、真实的反射建模。实验表明其镜面反射细节和真实性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在动态镜面场景下，现有方法在反射方向估算及物理准确性方面存在不足，导致渲染出的镜面反射模糊或不真实。作者旨在改善这一问题，提升渲染真实性与细节表现。

Method: 1. 提出残差材质增强2D高斯溅射表示，统一建模动态几何和材质属性，实现精确反射光线计算。2. 引入动态环境高斯及混合渲染管线，将渲染分解为漫反射和镜面反射两部分，通过栅格化和光线追踪物理真实地合成镜面反射。3. 设计粗到细的训练策略，提升优化稳定性及分解的物理意义。

Result: 在多个动态场景数据集上进行大量实验，TraceFlow在量化指标和视觉效果上均优于以往方法，能生成更锐利、更真实的镜面反射，尤其在复杂环境下表现突出。

Conclusion: TraceFlow为动态镜面场景的高保真渲染提供了新方法，实现了更准确的反射方向估算及更真实的镜面反射合成，推动了该领域的技术进步。

Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.

</details>


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 本文提出了层级实例追踪的新任务，并构建了首个支持该任务的数据集，对现有模型进行了评估，发现任务具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的实例追踪任务往往只关注对象本身，忽略了对象内部的层级关系（如部件和整体之间的关系）。因此，作者提出一个新的任务，旨在推动对更复杂的层级结构追踪能力的研究。

Method: 1. 提出了层级实例追踪任务，需要同时追踪对象及其部件，并保持它们的层级关联关系。
2. 构建了第一个支持该任务的基准数据集，包括552个视频、2765个唯一实体，涉及40个（对象及部件）类别。
3. 设计并评估了七种模型变体，以适配和验证该任务。

Result: 模型评测结果表明，该数据集和任务具有很高的挑战性，现有方法在层级追踪能力方面有待提升。

Conclusion: 该研究为层级实例追踪任务奠定了基础，数据集公开推动该领域进一步发展，并促进更强大模型的研究。

Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/

</details>


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 该论文提出了一种结合小波和持久性同调的信号处理方法，提高乳腺癌筛查模型的泛化能力，显著提升外部测试数据上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌筛查虽然可以降低死亡率，但当前的自动化判读模型在不同类型扫描仪、成像方式及患者群体上表现不佳，导致假阴性和假阳性率高。因此，需要设计一种能提升模型泛化能力、适应外部数据集的改进方法。

Method: 作者利用拓扑数据分析（TDA），通过小波分解和持久性同调将乳腺影像结构信息转化为空间-多尺度的特征图，这些特征可抵抗小强度扰动。随后将这些特征图与原始影像作为通道输入，构成两阶段检测流程。该方法在美国CBIS-DDSM影像队列上训练和验证，并于葡萄牙INbreast和中国CMMD两个独立数字乳腺摄影数据集上评估性能。

Result: 在葡萄牙INbreast测试集上，将ConvNeXt Tiny模型与小波-持久性特征通道结合后，患者级AUC从0.55提升至0.75，在训练预算有限情况下效果显著。

Conclusion: 以小波-持久性同调为代表的拓扑特征能提升模型跨设备、跨人群的泛化表现，对乳腺癌筛查自动化判读的实际部署和临床应用具有重要意义。

Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.

</details>


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 本文介绍了一种神经网络中间特征压缩标准与模型，FCTM，支持边缘与云之间高效特征传输，平均可减小85.14%的比特率，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在机器视觉领域应用广泛，但直接在边缘设备部署计算量大，资源受限。传统全部本地或云推理方法存在延迟、带宽和隐私权衡。将推理分割至边与云可兼顾效率，但传输中间特征带来带宽新难题，因此亟需高效压缩这些特征的标准方案。

Method: 提出和实现了MPEG主导的Feature Coding for Machines (FCM)中间特征压缩标准，设计测试模型Feature Coding Test Model (FCTM)，构建专门的比特流语法和编解码流程，对多项视觉任务的中间特征进行高效编码并传输。

Result: FCTM在多个视觉任务上实现了平均85.14%的比特率压缩，同时准确率无显著降低，证明了其高效与实用性。

Conclusion: FCM标准及其测试模型FCTM为带宽有限、隐私敏感的场景下，智能特征的高效、互操作部署提供了可扩展路径，有望推动端云协同AI应用发展。

Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.

</details>


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动驾驶推理方法，用于提升自动驾驶模型在复杂场景下的推理和决策能力。新方法通过潜在空间（而非自然语言）进行连贯推理，以提升效率和效果。实验表明，其在推理速度和驾驶表现上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶模型多通过自然语言描述推理过程（CoT），但文本并非推理的最高效表达方式，存在效率和效果的局限。因此，改进推理过程的表达形式，有助于提升自动驾驶系统在关键场景下的性能和安全性。

Method: 提出Latent-CoT-Drive（LCDrive）模型，将推理过程和决策统一在动作对齐的潜在空间中，并通过动作建议token和基于世界模型的token（表达动作带来的潜在未来结果）交替推理。模型以场景真实后验轨迹为监督，冷启动Latent CoT推理能力，并通过闭环强化学习微调优化。

Result: 在大规模端到端自动驾驶基准测试中，LCDrive实现了更快的推理速度、更优的轨迹质量，并且比非推理和文本推理方法有更大的强化学习收益。

Conclusion: LCDrive证明了用潜在空间表达的连贯推理在自动驾驶推理和决策中更高效、更有效，具有优于自然语言推理的性能，为自动驾驶共融推理与决策开辟了新方向。

Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

</details>


### [11] [Emerging Standards for Machine-to-Machine Video Coding](https://arxiv.org/abs/2512.10230)
*Md Eimran Hossain Eimon,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 本文分析了机器对机器视觉通信中的编码方法，重点对比了用于机器任务的视频和特征编码效果。


<details>
  <summary>Details</summary>
Motivation: 传统的视频通信编码为人类视觉优化，导致带宽消耗大、安全性低，不适合日益增长的机器间视觉通信需求。作者希望通过更适合机器处理的编码方式，提升效率与安全性。

Method: 论文介绍了两种新范式：VCM（视频为机器编码）与FCM（特征编码）。其中，FCM通过压缩神经网络的中间特征代替像素数据，用于任务感知的通信。并对多种主流编码（HEVC、VVC、AVC）在FCM管道下的性能进行了对比实验。

Result: 实验表明：FCM可在大幅降低比特率的情况下，保持与边缘推理相近的准确率。HEVC与VVC机器任务表现几乎一致，只提升了1.39%比特率，而AVC劣于VVC需提升32.28%。跟踪任务中HEVC甚至优于VVC，且传统已部署硬件同样支持。

Conclusion: 特征编码（FCM）可在大幅节省带宽、增强隐私的同时，不牺牲算法准确率，且现有主流视频编码硬件大多可无缝支撑，无需额外部署，有望推动机器视觉通信高效落地。

Abstract: Machines are increasingly becoming the primary consumers of visual data, yet most deployments of machine-to-machine systems still rely on remote inference where pixel-based video is streamed using codecs optimized for human perception. Consequently, this paradigm is bandwidth intensive, scales poorly, and exposes raw images to third parties. Recent efforts in the Moving Picture Experts Group (MPEG) redesigned the pipeline for machine-to-machine communication: Video Coding for Machines (VCM) is designed to apply task-aware coding tools in the pixel domain, and Feature Coding for Machines (FCM) is designed to compress intermediate neural features to reduce bitrate, preserve privacy, and support compute offload. Experiments show that FCM is capable of maintaining accuracy close to edge inference while significantly reducing bitrate. Additional analysis of H.26X codecs used as inner codecs in FCM reveals that H.265/High Efficiency Video Coding (HEVC) and H.266/Versatile Video Coding (VVC) achieve almost identical machine task performance, with an average BD-Rate increase of 1.39% when VVC is replaced with HEVC. In contrast, H.264/Advanced Video Coding (AVC) yields an average BD-Rate increase of 32.28% compared to VVC. However, for the tracking task, the impact of codec choice is minimal, with HEVC outperforming VVC and achieving BD Rate of -1.81% and 8.79% for AVC, indicating that existing hardware for already deployed codecs can support machine-to-machine communication without degrading performance.

</details>


### [12] [Multi-dimensional Preference Alignment by Conditioning Reward Itself](https://arxiv.org/abs/2512.10237)
*Jiho Jang,Jinyoung Kim,Kyungjune Baek,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种改进的从人类反馈强化学习方法MCDPO，有效解决了现有方法在多评价维度上奖励冲突的问题，提升了扩散模型的多轴对齐和控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型多通过人类反馈强化学习（RLHF）进行对齐，但标准DPO方法盘聚多个评价维度（如美学与语义）为单一标量奖励，导致不同维度间冲突，使模型在某一维度上不得不丢弃本来良好的特性。需新方法解决奖励冲突，实现多维度独立优化。

Method: 提出Multi Reward Conditional DPO（MCDPO），将奖励拆分为多个独立维度，引入解耦版Bradley-Terry目标函数，通过在训练中显式注入偏好结果向量，指导模型按各维度独立优化。设计维度奖励dropout确保优化均衡。

Result: 在Stable Diffusion 1.5和SDXL等多个基准上进行实验证明，MCDPO在性能上优于现有方法。

Conclusion: MCDPO能在推理阶段实现动态多维度控制，通过Classifier Free Guidance放大某些奖励维度，无需额外训练或外部奖励模型，这大大提升了扩散模型的鲁棒性和灵活性。

Abstract: Reinforcement Learning from Human Feedback has emerged as a standard for aligning diffusion models. However, we identify a fundamental limitation in the standard DPO formulation because it relies on the Bradley-Terry model to aggregate diverse evaluation axes like aesthetic quality and semantic alignment into a single scalar reward. This aggregation creates a reward conflict where the model is forced to unlearn desirable features of a specific dimension if they appear in a globally non-preferred sample. To address this issue, we propose Multi Reward Conditional DPO (MCDPO). This method resolves reward conflicts by introducing a disentangled Bradley-Terry objective. MCDPO explicitly injects a preference outcome vector as a condition during training, which allows the model to learn the correct optimization direction for each reward axis independently within a single network. We further introduce dimensional reward dropout to ensure balanced optimization across dimensions. Extensive experiments on Stable Diffusion 1.5 and SDXL demonstrate that MCDPO achieves superior performance on benchmarks. Notably, our conditional framework enables dynamic and multiple-axis control at inference time using Classifier Free Guidance to amplify specific reward dimensions without additional training or external reward models.

</details>


### [13] [Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective](https://arxiv.org/abs/2512.10244)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.CV

TL;DR: 论文提出了一种面向半监督小样本学习（SSFSL）的简单而有效的方法SWIFT，通过改进开源视觉-语言模型(VLM)微调过程中的伪标签置信度及温度参数，显著提升了未标注样本的利用率，实现比现有方法高约5个百分点的精度。


<details>
  <summary>Details</summary>
Motivation: 虽然现实应用（如自动标注）需要用很少的标注样本和大量未标注样本进行学习，但现有SSFSL方法很少利用开源VLM及其预训练数据资源，而这在少样本学习(FSL)中已被证明有效。因此，作者希望解决这一领域对VLM利用的不足。

Method: 作者分析了直接采用现有SSL方法微调VLM时性能不佳的原因，发现VLM输出的softmax概率分布过“平”，导致伪标签置信度低，信息利用率差。为了解决这个问题，作者采用了两项简单却有效的技术：分类器初始化和温度调节。此外，作者提出分阶段微调结合温度调节（SWIFT），能更好地结合有限标注数据、大量未标注数据和VLM预训练集中的可噪声相关数据。

Result: 在五个SSFSL基准上，SWIFT方法将开源VLM的微调效果提升了大约5个准确率点，优于当前流行的FSL和SSL方法，并且在部分场景下能接近有监督学习（即标注所有数据）的表现。

Conclusion: SWIFT证明了通过简单的温度调优和分类器初始化，可以大幅提升VLM在半监督小样本场景下的表现，为自动标注等现实任务提供了强有力的技术支持，也表明了进一步开发开源VLM及其预训练数据的潜力。

Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!

</details>


### [14] [RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection](https://arxiv.org/abs/2512.10248)
*Zhuo Wang,Xiliang Liu,Ligang Sun*

Main category: cs.CV

TL;DR: 论文指出，现有AIGC视频检测工作忽略了生成模型数字水印对检测器的影响，因此提出了RobustSora基准和数据集，系统评估水印相关问题，并揭示多数检测器对水印有依赖。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术（AIGC）日益普及，影响信息真实性。许多先进生成模型在生成视频时嵌入数字水印，而检测器有可能在不自知间依赖这些水印进行检测，导致评测失真。因此，亟需专门考虑水印影响的检测基准。

Method: 提出RobustSora基准，包含6,500个视频，分为真实干净（A-C）、真实带伪水印（A-S）、生成带水印（G-W）、生成去水印（G-DeW）四类。设计两个任务：Task-I评估去除水印后AI视频检测效果，Task-II评估带伪水印的真实视频误报率。使用10种模型（专用AIGC检测器、Transformer架构和MLLM）系统实验。

Result: 在水印干预下，检测器性能普遍下降2-8个百分点。Transformer模型对水印有6-8pp依赖，MLLM模型依赖差异较大（2-8pp）。说明检测器确实部分依靠水印特征。

Conclusion: AIGC检测器对内容水印有显著依赖，评测体系应引入水印维度。RobustSora基准为研发更健壮的检测方法提供关键工具，未来需考虑水印感知的训练机制。

Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.

</details>


### [15] [THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose](https://arxiv.org/abs/2512.10251)
*Eunho Lee,Chaehyeon Song,Seunghoon Jeong,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的6D位姿估计算法THE-Pose，通过拓扑先验和混合图融合，有效提升了类别级物体位姿估计的鲁棒性和准确度，实验验证性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D图卷积（3D-GC）的方法主要关注于局部几何和深度信息，对于复杂物体和视觉歧义表现不佳，缺乏对全局上下文和拓扑的建模，难以应对类别内变化和遮挡。

Method: 提出THE-Pose框架：从图像域中提取一致且不变的拓扑特征，通过混合图融合（Hybrid Graph Fusion, HGF）模块，将这些拓扑特征与点云特征自适应融合，实现2D全局语义与3D几何结构的统一。

Result: 在REAL275数据集上，THE-Pose相比3D-GC基线（HS-Pose）提升了35.8%，并在所有主要指标上超过了先前的SOTA方法7.2%。

Conclusion: THE-Pose通过引入拓扑先验和混合特征融合，有效提升了类别级6D位姿估计的泛化能力和鲁棒性，特别适合处理复杂及遮挡严重的物体。

Abstract: Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose

</details>


### [16] [GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule](https://arxiv.org/abs/2512.10252)
*Rui Wang,Yimu Sun,Jingxing Guo,Huisi Wu,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出了一种新型心脏超声视频分割方法GDKVM，在两个主流数据集上实现了比现有方法更高的分割精度和鲁棒性，并具备实时性能。


<details>
  <summary>Details</summary>
Motivation: 心脏超声图像在定量分析心脏功能、辅助临床诊断和治疗中至关重要，但图像噪声、伪影以及心脏变形和运动等因素给分割算法带来挑战。现有卷积神经网络、Transformer和时空记忆网络方法在捕捉远距离时空依赖与高效细粒度特征表征之间存在权衡难题。

Method: GDKVM模型采用了线性键值关联（LKVA）来有效建模帧间相关性，引入门控Delta规则（GDR）以高效存储中间记忆状态，并通过关健像素特征融合（KPFF）模块整合多尺度本地和全局特征，增强对模糊边界和噪声干扰的鲁棒性。

Result: 在CAMUS和EchoNet-Dynamic两个主流心脏超声视频数据集上的实验结果显示，GDKVM在分割精度和鲁棒性上均优于多种最新先进方法，并且实现了实时分割性能。

Conclusion: GDKVM兼顾了长程时空依赖建模能力、细粒度表征与计算效率，对心脏超声分割任务有明显优势，为相关临床应用提供了新的技术手段。代码已开源，有利于后续研究与应用。

Abstract: Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.

</details>


### [17] [VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models](https://arxiv.org/abs/2512.10262)
*Yuetong Su,Baoguo Wei,Xinyu Wang,Xu Li,Lixin Li*

Main category: cs.CV

TL;DR: LLM-NCD是一种结合视觉和文本特征的新颖类别发现方法，大幅提升了未知类别的识别准确率，尤其在数据长尾分布下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有新颖类别发现（NCD）方法主要依赖图像的视觉特征，导致区分性不足，且在长尾分布的数据集表现不佳，亟须突破这一瓶颈。

Method: 提出LLM-NCD多模态框架，通过融合视觉与文本语义特征，并引入原型引导的聚类。其创新点在于联合优化已知类别的图像和文本特征以建模聚类中心和语义原型，并设计了双阶段发现机制，利用语义亲和阈值和自适应聚类动态区分已知和新颖样本。

Result: 在CIFAR-100数据集上，相比现有方法，新方法对未知类别的识别准确率提升高达25.3%。

Conclusion: LLM-NCD不仅显著提升了新颖类别发现性能，还首次在NCD领域展现了对长尾分布的独特鲁棒性。

Abstract: Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.

</details>


### [18] [Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction](https://arxiv.org/abs/2512.10267)
*Chen Ziwen,Hao Tan,Peng Wang,Zexiang Xu,Li Fuxin*

Main category: cs.CV

TL;DR: 本文提出Long-LRM++模型，结合半显式场景表达与轻量级解码器，在保持高质量渲染的同时，实现了实时性能，解决了以往隐式表示方法渲染缓慢的问题。


<details>
  <summary>Details</summary>
Motivation: 当前GS方法能较快重建场景，但直接预测海量高斯参数易产生模糊，尤其对细节处理不佳；相反，隐式方法能实现更高保真度，但依赖深度解码过程，无法实时渲染。如何兼具高质量与实时性能成为亟需解决的问题。

Method: 提出Long-LRM++模型，采用半显式场景表达与轻量级解码器：场景信息部分以显式高斯方式存储，减轻一次性预测误差影响；解码网络深度大幅减小，省略复杂、串行的“解压”过程，从而提升渲染速度。

Result: Long-LRM++在DL3DV数据集上渲染质量可与隐式方法LaCT媲美，且在A100 GPU上达到14FPS；在950×540分辨率下支持多达64视图。还在ScanNetv2上实现了优于直接高斯深度渲染的生成式深度预测。消融实验验证了各模块的有效性。

Conclusion: Long-LRM++实现了高效、实时的场景级渲染，兼具隐式方法高质量与GS类方法的速度优势，为实时三维重建与渲染提供了新方案。

Abstract: Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\times540$ resolution, achieving 360° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential "decompression" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.

</details>


### [19] [Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation](https://arxiv.org/abs/2512.10275)
*Hongsin Lee,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出了一种新的对抗蒸馏方法SAAD，通过根据对抗可迁移性自适应地调整训练样本权重，显著提升了紧凑学生模型的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗蒸馏方法认为更强的教师模型能够带来更强的鲁棒性，但实际效果出现饱和，不能持续提升学生模型鲁棒性，因此需要重新分析影响鲁棒性迁移的核心因素。

Method: 作者通过实验分析，发现学生生成对抗样本在教师模型上的可迁移性是影响鲁棒性迁移的关键。基于此，提出了Sample-wise Adaptive Adversarial Distillation（SAAD）方法：对每个样本，根据其对抗可迁移性对训练损失自适应加权，以促进更有效的鲁棒性迁移。该方法无需增加计算成本。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet等数据集上，SAAD方法相较于已有对抗蒸馏方法在AutoAttack评估下的鲁棒性表现均有显著提升。

Conclusion: 对抗可迁移性是影响对抗鲁棒性迁移的关键，提出的SAAD方法通过自适应加权显著提升了学生模型的鲁棒性，推动了高效鲁棒模型的实用落地。

Abstract: Adversarial distillation in the standard min-max adversarial training framework aims to transfer adversarial robustness from a large, robust teacher network to a compact student. However, existing work often neglects to incorporate state-of-the-art robust teachers. Through extensive analysis, we find that stronger teachers do not necessarily yield more robust students-a phenomenon known as robust saturation. While typically attributed to capacity gaps, we show that such explanations are incomplete. Instead, we identify adversarial transferability-the fraction of student-crafted adversarial examples that remain effective against the teacher-as a key factor in successful robustness transfer. Based on this insight, we propose Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by their measured transferability without incurring additional computational cost. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SAAD consistently improves AutoAttack robustness over prior methods. Our code is available at https://github.com/HongsinLee/saad.

</details>


### [20] [MotionEdit: Benchmarking and Learning Motion-Centric Image Editing](https://arxiv.org/abs/2512.10284)
*Yixin Wan,Lei Ke,Wenhao Yu,Kai-Wei Chang,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出了MotionEdit数据集和MotioNFT方法，专注于运动相关的图像编辑，即在保留主体身份和结构的前提下实现合理的运动变换。该研究还发布了相关基准评测和改进方法，有效提升了当前图像编辑模型在运动编辑任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大多数图像编辑数据和方法侧重于静态外观变换，而忽略了现实应用（如视频生成、动画等）中更需求的动作编辑，这类编辑不仅技术挑战大，现有方法和数据也难以支持高质量运动变化。

Method: 1) 构建MotionEdit高质量运动变换图像对数据集；2) 提出MotionEdit-Bench，系统衡量模型在运动编辑任务上的表现；3) 提出MotionNFT后训练方法，通过运动流对齐奖励，引导扩散模型更精准地实现动作变换。

Result: 在FLUX.1 Kontext和Qwen-Image-Edit等主流模型上，大量实验显示MotionNFT方法能大幅提升运动编辑的质量和动作的保真度，同时保持了对常规编辑任务的能力。

Conclusion: 运动编辑是一个对现有扩散模型仍然极具挑战性的任务。MotionNFT后训练框架为该难题提供了有效解决方案，兼具提升与兼容性，对实际运动相关图像生成应用具有重要意义。

Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.

</details>


### [21] [ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions](https://arxiv.org/abs/2512.10286)
*Xiaoxue Wu,Xinyuan Chen,Yaohui Wang,Yu Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ShotDirector的高效多镜头视频生成框架，实现了更具电影感且可控的镜头转场。通过参数级摄像机控制和层次化剪辑模式提示，有效提升了镜头间的叙事连贯性和编辑设计。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法主要关注视觉一致性，忽略了转场的设计和电影语言如何影响叙事连贯性，导致生成的视频仅仅是简单的顺序切换，缺乏专业的剪辑模式和艺术表达。

Method: ShotDirector框架结合了参数级摄像机控制（通过6-DoF摄像机位姿和内参设置）和层次化剪辑意识提示（shot-aware mask及编辑模式提示），以同时实现对低层摄像参数和高层语义指导的精细控制。为了支持训练和评估，作者构建了ShotWeaver40K数据集，并设计了一套可控多镜头视频生成的评价指标。

Result: 通过大量实验验证，该框架能够实现具有电影风格和可控性的镜头转场，输出视频在连贯性、专业编辑风格等方面都优于现有方法。

Conclusion: ShotDirector框架结合了低层次摄像条件和高层次编辑模式的控制，初步实现了更高水准的多镜头视频生成，在电影感的表达和剪辑连贯性方面显著提升，为后续智能视频内容生成提供了新的方法和思路。

Abstract: Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [22] [Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings](https://arxiv.org/abs/2512.10293)
*Karthikeya KV,Narendra Bandaru*

Main category: cs.CV

TL;DR: Disentangled360是一种创新的三维感知方法，通过独特的方向解耦体渲染，实现了单张图像的360°视角合成，广泛适用于医学影像和自然场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建技术往往对于各向异性光照简化过度，或缺乏在不同场景间的泛化能力，特别是在医学和真实世界应用中，难以兼顾效果与实用性。该工作旨在突破这些局限，实现高质量、具备方向感的多场景三维重建与渲染。

Method: 提出了一种基于Gaussian Splatting的框架，通过区分各向同性和各向异性成分，采用双分支条件网络：一支用于基于CT强度的散射建模，另一支针对RGB真实场景利用标准化相机嵌入。引入混合的姿态无关锚定技术，实现自适应深度采样和材料过渡，从而在场景蒸馏中保持结构真实感。整个流程支持从术前X射线到消费级360°渲染的一体化推理。

Result: 在Mip-NeRF 360、RealEstate10K和DeepDRR等数据集上取得了优异的SSIM和LPIPS评价指标。运行时测试显示其可用于交互式应用，效率和视觉质量兼具。

Conclusion: Disentangled360实现了无需场景特定微调或复杂光子仿真的高效、真实三维重建，支持医学混合现实监督、机器人感知和沉浸式内容创作等多元场景。

Abstract: We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.

</details>


### [23] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一种可扩展的多视角Transformer，用于高效、精确的4D（时间+空间）重建。相比以往方法，其可同时处理多种传感器数据，并显著提升精度和计算速度。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建方法普遍只能处理2视图场景流或稀疏的3D点跟踪，功能有限，且对数据源类型支持狭窄，难以适应实际多样化需求。

Method: Any4D 提出了模块化的4D场景表示方法，将每视角的4D预测编码为本地（深度图、相机内参）和全局（相机外参、场景流）两类因子，并基于多视角Transformer结构，实现对多帧、多模态（如RGB-D、IMU、雷达等）信息的融合与高效重建。

Result: Any4D在多种重建设置下均实现了2-3倍误差降低和15倍计算加速，显著优于现有方法。

Conclusion: Any4D不仅准确且高效，还能灵活适应多样输入数据，推动了4D重建在实际应用中的发展。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [24] [Efficient-VLN: A Training-Efficient Vision-Language Navigation Model](https://arxiv.org/abs/2512.10310)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了高效的多模态大模型VLN（Efficient-VLN），通过改进记忆机制和策略，极大降低了训练开销，并在主要数据集上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言导航（VLN）任务表现突出，但其长序列处理和探索效率权衡导致训练成本极高，亟需优化。

Method: 方法上，作者设计了两种高效记忆机制：渐进记忆（给最近观测分配更多tokens）与可学习递归记忆（通过可学习token的KV缓存存储历史）。同时，提出动态混合策略以平衡探索与效率。

Result: Efficient-VLN在R2R-CE和RxR-CE数据集上分别获得64.2%和67.0%的成功率，达到当前最好水平，同时显著减少为282 GPU小时的训练资源消耗。

Conclusion: Efficient-VLN大幅减少了训练开销，实现了高效与高性能的统一，为VLN等相关领域高效MMLM训练提供了新思路。

Abstract: Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

</details>


### [25] [DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation](https://arxiv.org/abs/2512.10314)
*Anh M. Vu,Khang P. Le,Trang T. K. Vo,Ha Thach,Huy Hung Nguyen,David Yang,Han H. Huynh,Quynh Nguyen,Tuan M. Pham,Tuan-Anh Le,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出了一种结合视觉和语言原型的弱监督语义分割框架，显著提升了病理图像的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督病理切片分割方法受制于类间同质性、类内异质性及CAM区域收缩效果，限制了其准确性。作者旨在通过改进表示和监督机制，以更好地发现分割区域并提升分割精度。

Method: 采用视觉-语言对齐的原型驱动框架，引入了CoOp式可学习提示调优生成文本原型，并与可学习的图像原型结合，构建了双模态原型库。同时，通过多尺度金字塔模块缓解ViT特征的过平滑问题，提升空间定位准确性。

Result: 在BCSS-WSSS基准上，本方法在分割任务上超越了最新的同类方法。分析显示，文本描述多样性、上下文长度以及文本与图像原型的互补性均显著提升了分割性能。

Conclusion: 联合利用文本语义和视觉原型学习可有效提升数字病理领域的弱监督语义分割表现。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.

</details>


### [26] [ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.10316)
*Khang Le,Ha Thach,Anh M. Vu,Trang T. K. Vo,Han H. Huynh,David Yang,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种用于病理影像弱监督语义分割的新型原型学习框架，集成了视觉-语言模型和现代分割网络的优势，实现了无像素级标注下的高质量分割。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督病理影像分割方法多依赖分类主干网络，导致模型仅聚焦于最具判别性的区域，难以完整捕获组织结构的空间范围。视觉-语言模型和分割模型具备互补长处，但其融合与高效利用在弱监督条件下存在难题。

Method: 框架结合了CONCH的结构建模能力、SegFormer的多尺度空间保真性和文本引导的语义对齐，通过文本引导的原型初始化结合病理描述提升伪掩码完整性，并用结构蒸馏机制将SegFormer的空间知识迁移至原型学习，保持空间及组织边界信息。

Result: 该方法无需像素级标注即可生成高质量伪掩码，有效提升组织区域的定位完整性与多组织类型间的语义一致性。在BCSS-WSSS数据集上的实验表明，该方法优于现有弱监督分割方法，并具备高效的计算性能。

Conclusion: 通过结合视觉-语言语义和分割模型的空间结构优势，提出的原型学习框架实现在弱监督条件下的高质量病理影像分割，为相关应用提供了有效的解决方案。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.

</details>


### [27] [Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset](https://arxiv.org/abs/2512.10321)
*Hyunsoo Lee,Daeum Jeon,Hyeokjae Oh*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D人体姿态估计生成方法Point2Pose，并构建了丰富的大型室内多模态数据集MVPose3D，实验结果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 3D人体姿态估计因人体结构复杂、自遮挡关节和大规模真实数据需求等问题具有挑战性。

Method: 提出Point2Pose框架，利用时空点云编码器和姿态特征编码器提取关节点特征，并结合基于注意力的生成回归器；提供包含IMU、密集多视角点云和RGB图像的大型室内数据集MVPose3D。

Result: 实验表明，所提方法在多个数据集上表现优于基线模型，具有更优的性能。

Conclusion: Point2Pose框架能够有效提升3D人体姿态估计的精度，大型多模态数据集的引入有助于领域发展。

Abstract: We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.

</details>


### [28] [EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)
*Chao Gong,Depeng Wang,Zhipeng Wei,Ya Guo,Huijia Zhu,Jingjing Chen*

Main category: cs.CV

TL;DR: 提出了EchoingPixels框架，通过跨模态语义筛选机制（CS2）和同步增强旋转位置编码（Sync-RoPE），实现了对音视频大模型输入token的高效联合压缩，在保持性能的同时大幅提升推理速度和节省内存。


<details>
  <summary>Details</summary>
Motivation: 现有音视频LLM因需要处理大量音频和视频token而计算代价极高。视频单模态token压缩方法难以兼顾音视频间的协作信息和动态信息密度，不适用于联合音视频流。如何高效减少音视频整体token数量成为瓶颈。

Method: 提出EchoingPixels，一种启发于现实音画共存与交互的新型跨模态token压缩框架。设计了CS2模块，实现音视频的早期交互，并以单池自适应压缩token，而不是针对单一模态分别压缩。此外，设计Sync-RoPE机制，增强少量保留token的时序表达能力。

Result: 在多个实验中，EchoingPixels能将token数缩减至5-20%，且在保持与强基线相当性能的基础上，带来2-3倍的速度与内存提升。

Conclusion: EchoingPixels通过跨模态互动与联合token压缩，有效突破音视频LLM推理效率瓶颈，为多模态大模型的实用化提供了新方向。

Abstract: Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.

</details>


### [29] [StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology](https://arxiv.org/abs/2512.10326)
*Jiawen Li,Jiali Hu,Xitong Ling,Yongqiang Lv,Yuxuan Chen,Yizhi Wang,Tian Guan,Yifei Liu,Yonghong He*

Main category: cs.CV

TL;DR: 作者提出了StainNet，一个针对特殊染色病理图像的基础模型，并基于ViT架构与自蒸馏自监督学习进行训练，显著提升了在特殊染色场景下的表现，并公开提供模型权重。


<details>
  <summary>Details</summary>
Motivation: 现有的病理基础模型主要在H&E染色图像上预训练，但临床中常用的特殊染色（如免疫组化）图像应用较少，导致基础模型泛化能力有限，实际应用受限。因此，亟需一个专为特殊染色图像设计的基础模型，提升其在多样病理染色图像中的适用性。

Method: 提出了一种基于ViT（视觉Transformer）架构的特殊染色病理图像基础模型StainNet。模型采用自蒸馏的自监督学习方法，在HISTAI数据库中超过140万张、来自20231张WSI切片的特殊染色图像patch进行训练。通过组织内部的肝脏恶性肿瘤分类和两个公开ROI数据集进行评估，还进行少样本学习和检索实验，并与近期大型PFMs进行对比。

Result: StainNet在肝脏恶性肿瘤分类和两个公开ROI水平数据集上展示出强大的识别能力，在few-shot学习和检索任务中表现优异，在对比实验中优于部分大型现有模型。

Conclusion: StainNet可作为特殊染色病理图像分析的高效基础模型，提升了在实际多样染色场景下的泛化能力和应用价值。模型权重已对外公开，有望助力更多下游病理图像任务。

Abstract: Foundation models trained with self-supervised learning (SSL) on large-scale histological images have significantly accelerated the development of computational pathology. These models can serve as backbones for region-of-interest (ROI) image analysis or patch-level feature extractors in whole-slide images (WSIs) based on multiple instance learning (MIL). Existing pathology foundation models (PFMs) are typically pre-trained on Hematoxylin-Eosin (H&E) stained pathology images. However, images with special stains, such as immunohistochemistry, are also frequently used in clinical practice. PFMs pre-trained mainly on H\&E-stained images may be limited in clinical applications involving special stains. To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a self-distillation SSL approach and is trained on over 1.4 million patch images cropping from 20,231 publicly available special staining WSIs in the HISTAI database. To evaluate StainNet, we conduct experiments on an in-house slide-level liver malignancy classification task and two public ROI-level datasets to demonstrate its strong ability. We also perform few-ratio learning and retrieval evaluations, and compare StainNet with recently larger PFMs to further highlight its strengths. We have released the StainNet model weights at: https://huggingface.co/JWonderLand/StainNet.

</details>


### [30] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法ISMVC，针对多视图数据存在缺失和不平衡情况下的聚类问题，在多个基准数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视图数据在现实中常因视角缺失和数据分布不均给聚类带来极大挑战。传统填补法易引入噪音和偏差，无填补法在严重缺失时表现不佳，因此亟需更有效且稳健的处理方法。

Method: 作者提出了基于信息性的选择性填补多视图聚类方法ISMVC。具体做法是评估每个缺失位置的信息量，只有在有充分依据时才进行填补。还将选择性填补与带有高斯混合先验的变分自编码器结合，进行分布级的填补，并建模填补的不确定性以增强鲁棒性。

Result: 在多个更贴合实际且视图缺失不均的基准数据集上，ISMVC方法在聚类效果上优于现有的基于填补和无填补的方案。

Conclusion: ISMVC是一种轻量、数据驱动、模型无关的插件模块，可有效提升多视图聚类的稳健性和准确性，对现有IMC模型有较好的兼容性和实际应用价值。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [31] [A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images](https://arxiv.org/abs/2512.10334)
*Yi Liu,Yichi Zhang*

Main category: cs.CV

TL;DR: 本论文提出一种基于Pix2Pix架构的条件生成框架，用于从二值掩码生成显微镜下的真实丝状结构图像，并引入结构损失提升合成图像的结构相似性。实验结果显示，有效提升了模型在缺乏标注数据情况下的分割性能，优于仅用真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 丝状结构如微管、肌动蛋白丝在生命系统中具有重要作用，其自动分割对于生物图像定量分析至关重要。但由于这些结构分布密集、形态复杂，精细像素级标注极为耗时，目前缺乏大规模高质量标注数据，限制了深度学习方法的进一步提升。作者旨在通过合成数据解决数据匮乏问题。

Method: 作者提出采用基于Pix2Pix的条件生成式对抗网络，由二值掩码生成逼真的丝状结构显微图像。为提高合成图像和真实图像间的结构相似性，还引入了专门针对丝状结构的结构损失函数。

Result: 实验结果表明，采用该合成数据辅助训练的分割模型，相较于仅用真实数据训练，在分割性能上表现更佳，验证了合成数据和结构损失的有效性。

Conclusion: 本工作为丝状结构分割领域提供了高效的合成数据生成方法，可显著缓解标注不足问题，为相关生物图像分析任务提供了有力工具。

Abstract: Thin and elongated filamentous structures, such as microtubules and actin filaments, often play important roles in biological systems. Segmenting these filaments in biological images is a fundamental step for quantitative analysis. Recent advances in deep learning have significantly improved the performance of filament segmentation. However, there is a big challenge in acquiring high quality pixel-level annotated dataset for filamentous structures, as the dense distribution and geometric properties of filaments making manual annotation extremely laborious and time-consuming. To address the data shortage problem, we propose a conditional generative framework based on the Pix2Pix architecture to generate realistic filaments in microscopy images from binary masks. We also propose a filament-aware structural loss to improve the structure similarity when generating synthetic images. Our experiments have demonstrated the effectiveness of our approach and outperformed existing model trained without synthetic data.

</details>


### [32] [Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution](https://arxiv.org/abs/2512.10340)
*Yi-Cheng Liao,Shyang-En Weng,Yu-Syuan Xu,Chi-Wei Hsiao,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种面向真实世界图像超分辨率的新方法HD-CLIP，通过解耦图像的语义和劣化信息，有效引导扩散模型恢复高质量图像，并提升真实感和细节还原能力。


<details>
  <summary>Details</summary>
Motivation: 真实世界的低质图像退化复杂多样，传统方法往往假设已知具体退化程度，且依赖于CLIP文本编码器，无法表达数值化的退化等级，导致泛化能力有限。

Method: 提出HD-CLIP方法，将低质图像分解为语义嵌入和可序数的劣化嵌入，实现对未见退化级别的插值。将其以免分类器引导（CFG）和免分类器投影引导（CFPG）方式集成到扩散模型中。HD-CLIP作为即插即用模块，无需训练即可集成到不同的超分辨率框架。

Result: HD-CLIP在多个真实世界数据集上显著提升了细节保真度及感知真实感，有效抑制了伪影和虚假内容的产生。

Conclusion: HD-CLIP突破了现有方法在真实世界超分领域对劣化建模的局限，为无训练即插即用的图像恢复提供了高效方案，具备广泛应用潜力。

Abstract: Real-World Image Super-Resolution (Real-ISR) aims to recover high-quality images from low-quality inputs degraded by unknown and complex real-world factors. Real-world scenarios involve diverse and coupled degradations, making it necessary to provide diffusion models with richer and more informative guidance. However, existing methods often assume known degradation severity and rely on CLIP text encoders that cannot capture numerical severity, limiting their generalization ability. To address this, we propose \textbf{HD-CLIP} (\textbf{H}ierarchical \textbf{D}egradation CLIP), which decomposes a low-quality image into a semantic embedding and an ordinal degradation embedding that captures ordered relationships and allows interpolation across unseen levels. Furthermore, we integrated it into diffusion models via classifier-free guidance (CFG) and proposed classifier-free projection guidance (CFPG). HD-CLIP leverages semantic cues to guide generative restoration while using degradation cues to suppress undesired hallucinations and artifacts. As a \textbf{plug-and-play module}, HD-CLIP can be seamlessly integrated into various super-resolution frameworks without training, significantly improving detail fidelity and perceptual realism across diverse real-world datasets.

</details>


### [33] [CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates](https://arxiv.org/abs/2512.10342)
*Shresth Grover,Priyank Pathak,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 本文提出了CoSPlan基准，用于评估视觉—语言模型（VLMs）在需要纠错的视觉序列规划任务中的表现，并提出了一种提升方法SGI。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉—语言模型在复杂推理方面表现优异，但在多步动作执行（视觉序列规划）方面研究较少，尤其是在实际任务中经常包含非最优（错误）步骤，需要模型能够识别并纠正。

Method: 作者提出了CoSPlan基准，涵盖迷宫导航、方块重排列、图像重建和物体再组织四大视觉顺序任务，评测VLMs的错误检测和步骤补全能力。同时，提出了一种新颖的、无需训练的方法——Scene Graph Incremental updates（SGI），通过引入中间推理步骤，增强模型的顺序推理能力。

Result: 即使采用最新推理技术（如Chain-of-Thought及Scene Graphs），当前主流VLMs如Intern-VLM和Qwen2在CoSPlan表现不佳，不能有效利用上下文信息完成目标。引入SGI方法后，模型平均性能提升5.2%。

Conclusion: SGI方法提升了VLMs在纠错型序列规划任务中的可靠性，并可泛化到传统规划任务如Plan-Bench和VQA。

Abstract: Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.

</details>


### [34] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 本文是首次系统性研究了强化学习（RL）在文本到3D自回归生成中的应用，提出了一套从奖励设计、RL算法到基准和分层优化的完整方案，并发布了新模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 虽然RL已在文本生成和2D图像生成中广泛应用且表现优异，但3D生成因空间复杂度高、几何及纹理需保持一致而远未被充分研究。现有方法无法很好捕捉3D生成中的隐式推理能力，对奖励设计和算法极度敏感。该工作希望填补该空白，系统性探索RL驱动的3D生成机制。

Method: 1）系统评估奖励设计与模型选择，强调与人类偏好的对齐，并利用多模态通用模型增强3D属性表征；2）深入研究GRPO及其变体在token级优化中的有效性，分析训练规模与迭代影响；3）提出新基准MME-3DR以有效测度模型推理能力；4）提出分层全局-局部优化的新范式Hi-GRPO，通过分层奖励提升生成质量。最终，研发并发布了AR3D-R1模型。

Result: 通过实验，作者发现多模态模型可为3D奖励设计提供强信号，人类偏好对齐至关重要；GRPO变体在token级表现突出，分层奖励显著增强了细粒度纹理与全局几何一致性；新基准更好地揭示了模型的3D推理差异。AR3D-R1在业内首次实现了端到端RL增强的文本到3D全过程生成。

Conclusion: 本研究证明了RL在文本到3D自回归生成中的巨大潜力，提出了分层奖励、先进算法和新基准的综合框架，有效提升了3D生成的推理与表现能力。为今后RL驱动的3D生成研究提供了实践范例和理论指导。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [35] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: 该论文提出了OmniZoo，一个大规模动物动作数据集，并基于此开发了可适应任意骨骼拓扑与文本条件的通用化动作生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成方法大多依赖于固定骨骼模板，难以泛化到多样或变异的骨骼结构。同时，缺乏大规模异构动物动作数据和统一的生成框架限制了领域发展。

Method: 作者构建了OmniZoo数据集，涵盖140种动物、32,979个动作序列，并提供多模注释。在此基础上，提出了一种自回归的动作生成框架，引入“拓扑感知的骨骼嵌入模块”，融合骨骼结构特征与文本语义，实现面向任意骨骼拓扑的动作生成。

Result: 方法能够根据文本提示和任意目标骨架，生成时间连贯、物理合理且语义匹配的动作。同时支持跨物种的动作风格迁移。

Conclusion: 本文为动作生成领域带来了一套覆盖广泛、结构灵活的解决方案，有助于推进多样生物体动作合成和基于文本的动画生成。

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [36] [Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation](https://arxiv.org/abs/2512.10353)
*Yiheng Lyu,Lian Xu,Mohammed Bennamoun,Farid Boussaid,Coen Arrow,Girish Dwivedi*

Main category: cs.CV

TL;DR: TranSamba 是一种针对医学影像体数据弱监督分割的新模型，结合了 Transformer 和 Mamba 架构，能更好地捕捉三维上下文，取得了新的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像弱监督分割方法多依赖于2D编码器，忽略了体数据的三维结构，导致上下文信息利用不足。

Method: 提出TranSamba，将Vision Transformer主干与跨切片的Mamba块结合。Mamba块基于状态空间模型，实现高效的相邻切片信息交换，加强Transformer内部的自注意力机制，用于更精准的物体定位。该模型可线性扩展至更深的体数据，且维持常量内存消耗。

Result: 在三个公开数据集上的实验显示TranSamba全面优于现有方法，涵盖多种医学影像模态和病理类型，达到了最新的SOTA水平。

Conclusion: TranSamba 通过混合Transformer与Mamba高效桥接了3D信息，为体数据弱监督分割提供了更强的建模能力和更优的表现，是当前该领域新一代领先方案。

Abstract: Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.

</details>


### [37] [mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar](https://arxiv.org/abs/2512.10357)
*Tarik Reza Toha,Shao-Jung,Lu,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 本文提出了一种名为mmCounter的新方法，利用毫米波雷达通过提取超低频信号（如呼吸和微小体动），实现对密集、静止人群的准确计数，显著提升了在静止环境下的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波雷达在人群密集且人员静止时，因分辨率和对运动依赖性，无法准确检测和计数个体。为解决密集静止人群计数的问题，作者提出了新的技术方案。

Method: mmCounter方法通过专门提取<1 Hz的超低频信号（主要为呼吸及细微身体移动），并结合多阶段信号处理流程，有效将这些特征信号与背景噪声和周边静态物体区分开，进而实现个体分辨和数目映射。

Result: 在多种环境下实验，mmCounter在已知环境中的平均F1分数为87%，平均绝对误差为0.6；在未见过的新环境下，平均F1分数为60%，平均绝对误差为1.1。方法可在3平方米空间内计数多达7名人员。

Conclusion: mmCounter突破了传统毫米波雷达对密集静态人群的检测局限，实现了基于低频体动信号的高精度人数计数，对室内密集区域管理和智能感知具有重要应用前景。

Abstract: mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.

</details>


### [38] [Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task](https://arxiv.org/abs/2512.10359)
*Sunqi Fan,Jiashuo Cui,Meng-Hao Guo,Shuojin Yang*

Main category: cs.CV

TL;DR: 提出了一个可扩展的视频工具包和STAR时空推理框架，有效提升了大模型在视频问答任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视频问答任务中难以同时建模空间关系和理解时序动态，导致难以应对复杂、需要推理的视频问答场景。

Method: 作者为多模态语言模型设计了一个视频工具包，丰富工具数量与多样性，并提出了STAR时空推理框架，智能调度时序和空间工具顺序，逐步定位视频关键区域，以提升模型的时空推理能力。

Result: 在VideoMME和LongVideoBench两个基准上，通过集成STAR框架和轻量级工具将GPT-4o的性能分别提升了8.2%和4.6%。

Conclusion: 视频工具包和STAR框架显著提升了大模型在视频分析和推理任务中的能力，为构建智能化、自动化的视频分析助手迈出了重要一步。

Abstract: Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.

</details>


### [39] [Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models](https://arxiv.org/abs/2512.10362)
*Woojun Jung,Jaehoon Go,Mingyu Jeon,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新方法Visual Funnel，通过更好地结合局部细节和全局上下文，显著提升了多模态大模型在精细视觉任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在图像理解中常常忽略细粒度视觉细节，主要因现有方法在提取关键区域时导致对全局与局部信息结构的分离，造成“情境盲区”。为解决这一结构性盲点，作者重新思考了输入多样性对模型感知能力的影响。

Method: 提出无需额外训练的两步法Visual Funnel。第一步是Contextual Anchoring，单次前向识别感兴趣区域；第二步是根据注意力熵动态确定裁剪尺寸，构建保留层次语境的Entropy-Scaled Portfolio，实现由局部到全局的信息嵌套。

Result: 通过大量实验验证，Visual Funnel大幅超越了传统的单裁剪和非结构化多裁剪方法，突出展示了以层级结构组织输入对提升模型感知的关键作用。同时，实验也表明，简单增加无结构裁剪对性能提升有限甚至有害。

Conclusion: Visual Funnel有效解决了多模态大模型中的情境盲区问题，强调了输入结构多样性和层级组织对精细感知任务的决定性作用。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.

</details>


### [40] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的零样本长视频时刻检索方法P2S，无需专门训练即可高效、准确地在小时级长视频中根据自然语言查询找到对应时间片段，且显著优于现有有监督方法。


<details>
  <summary>Details</summary>
Motivation: 长视频时刻检索（LVMR）任务中，处理长视频的高计算复杂度和零样本泛化性的欠缺严重限制了传统方法的实用性。现有零样本方法要么候选爆炸式增长，要么精炼阶段依赖计算资源密集的多模态大模型，亟需更高效、实用的检索框架。

Method: 本文提出无训练的Point-to-Span（P2S）框架，包括自适应时间段生成器解决候选爆炸问题，以及查询分解机制，在无需高成本多模态大模型的前提下提升精细检索能力，革新了长视频分析中的“搜索-精炼”范式。

Result: P2S在无需训练的前提下，实现了在小时级长视频中的高效零样本时刻定位，在多个数据集（如MAD）上表现远超目前有监督SOTA（例：R5@0.1提高3.7%）。

Conclusion: P2S首次实现了真正意义上的零样本长视频时刻检索，在效率和准确度上都有突破，解决了业界长期存在的候选爆炸与高消耗精炼两大难题，展示了大规模、实际应用的潜力。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


### [41] [Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views](https://arxiv.org/abs/2512.10369)
*Zhankuo Xu,Chaoran Feng,Yingtao Li,Jianbin Zhao,Jiashu Yang,Wangbo Yu,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: 本文提出了CoherentGS框架，可在输入图片稀疏且存在运动模糊的情况下，实现高质量3D重建，突破了3D Gaussian Splatting对高质量密集输入的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting方法对高质量、密集输入图片依赖性大，但现实中经常遇到稀疏、模糊的输入，导致重建失败。解决这一实际问题对于扩大3DGS的应用范围意义重大。

Method: 提出一种双先验策略：结合一个专门的去模糊网络（提供细节与光度引导）与扩散模型（提供场景几何先验），并配合基于一致性的摄像机探索模块引导生成过程，以及深度正则损失确保几何合理性。

Result: 在合成和真实场景的实验中，CoherentGS在仅有3/6/9幅输入时，均能显著优于现有方法，表现了新的state-of-the-art水平。

Conclusion: CoherentGS有效打破了稀疏、模糊输入导致的3D重建瓶颈，为实际中低质量输入的三维重建提供了一条高效的技术路径。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.

</details>


### [42] [RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds](https://arxiv.org/abs/2512.10376)
*Jingyun Fu,Zhiyu Xiang,Na Zhao*

Main category: cs.CV

TL;DR: 本文首次提出将4D毫米波雷达与激光雷达（LiDAR）融合用于场景流估计，并构建了相应数据集和联合学习框架RaLiFlow，显著提升了动态场景下的流估计性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态（如图像与LiDAR）融合在场景流估计上已有进展，但4D毫米波雷达与LiDAR的融合尚未被探索，而雷达具有低成本、强鲁棒性和可感知速度等独特优势，可弥补LiDAR的不足，但其输入较为稀疏、噪声大，且无相关公开数据集，亟需相关方法和资源。

Method: 1）基于现有自动驾驶数据集构建了首个Radar-LiDAR场景流数据集；2）提出雷达去噪和场景流标签生成策略，提升边界外雷达点标签可靠性；3）设计RaLiFlow模型，通过动态感知的双向跨模态融合（DBCF）模块和新损失函数，实现雷达与LiDAR特征高效互补整合，并提升训练和动态目标一致性。

Result: 在新构建的数据集上，RaLiFlow方法在场景流估计任务中，相较于现有基于单一模态（LiDAR或雷达）的方法取得了显著精度提升，尤其在动态前景区域表现优异。

Conclusion: 4D毫米波雷达与LiDAR的联合使用在场景流估计上具有高度互补性。所提出的数据集和RaLiFlow方法为多传感器融合提供了有效范例和工具，为后续相关研究奠定基础。

Abstract: Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.

</details>


### [43] [Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching](https://arxiv.org/abs/2512.10379)
*Alberto Rota,Elena De Momi*

Main category: cs.CV

TL;DR: 本论文提出了一种用于内镜图像间特征匹配的深度学习自监督方法，有效提升了匹配精度并降低了误差，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 在微创手术中，精确的图像空间理解对于3D重建、相机跟踪及场景解释至关重要，但内镜图像因视角、组织反射和解剖变形导致常规视觉方法效果不佳。现有深度学习特征未针对外科图像微细匹配任务进行优化，亟需改进。

Method: 论文提出端到端的深度学习管道，结合新颖的视图合成方式生成像素级真实匹配点，并在对比学习框架下进行三元组挖掘，通过自监督机制训练模型。在DINOv2骨干网络基础上，加入Transformer层，优化生成适用于余弦相似度匹配的特征嵌入。

Result: 在SCARED手术内镜数据集上实验表明，该方法在精度和极线误差方面均优于现有技术，实现了更高水平的特征匹配。

Conclusion: 本文方法为内镜手术场景下高层视觉应用（如3D重建、相机位姿估计等）提供了更精确的基础工具，对外科图像计算机视觉研究具有重要贡献。

Abstract: Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.

</details>


### [44] [Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies](https://arxiv.org/abs/2512.10384)
*Cong Pang,Hongtao Yu,Zixuan Chen,Lewei Lu,Xin Lou*

Main category: cs.CV

TL;DR: 提出了FROW基准，用于细粒度识别能力的衡量，并通过优化数据和训练流程，显著提升了LVLMs的识别表现。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）的评测多聚焦于推理任务，忽视了细粒度识别能力，而细粒度识别对于实际应用至关重要。因此，亟需针对这方面能力进行系统性评测和提升。

Method: 1. 构建了全新的细粒度识别评测基准FROW，利用GPT-4o进行详细评测。
2. 数据层面：引入马赛克数据（融合多条简短回答）和开放世界数据（用GPT-4o生成真实问答），丰富训练和评测数据。
3. 训练层面：将细粒度数据引入预训练过程，优化LVLMs的识别能力。

Result: 1. 马赛克数据使类别识别精度提升1%；
2. 开放世界数据让FROW基准的识别精度提升10%-20%，内容精度提升6%-12%；
3. 细粒度数据预训练可提升类别识别精度最高10%。

Conclusion: 针对LVLMs在实际应用中对细粒度识别能力的需求，FROW基准及优化策略显著提升了模型性能，为评测和改进LVLMs提供了新的方向和工具。

Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.

</details>


### [45] [Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method](https://arxiv.org/abs/2512.10386)
*Ge Zhang,Chunyang Wang,Bo Xiao,Xuelian Liu,Bin Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种自适应双重加权引力模型的点云去噪方法，实现了高效且高精度的点云去噪，兼顾了实时性和边界保护，在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在自动驾驶和三维重建等领域至关重要，但由于激光雷达采集过程受多种干扰，易生成大量噪声点，严重影响后续物体识别与检测。同时，以往去噪方法往往在精度和效率间难以兼顾，难以处理多种复杂噪声场景。

Method: 本方法首先采用八叉树对点云空间分区，实现并行加速。其次在每个子节点内结合自适应体素统计与k近邻密度估算，快速剔除明显孤立及低密度噪声点。最后设计了结合密度权重和自适应距离权重的引力打分函数，精细辨别噪声点与目标点。

Result: 在Stanford 3D Scanning Repository、CADC数据集及实验室自采点云上测试，方法在不同噪声条件下均显著提升了F1、PSNR及Chamfer距离等评价指标，同时降低了单帧处理时长，性能优于现有主流方法。

Conclusion: 该方法能够在多噪声场景下高效、准确去除点云噪声，并兼顾了对象边界和细节的保护，具有很好的实时性、鲁棒性和实际应用价值。

Abstract: High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.

</details>


### [46] [MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos](https://arxiv.org/abs/2512.10408)
*Qiyue Sun,Tailin Chen,Yinghui Zhang,Yuchen Zhang,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文针对短视频平台上的多模态仇恨言论，提出了首个弱监督下的多模态仇恨定位框架MultiHateLoc，在仅有视频级标签的情况下，实现了细粒度的仇恨片段定位，且效果领先。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视频级的仇恨识别，缺乏对仇恨言论具体发生时间点的定位研究，且在数据标注粗略、模态间信息动态交互需求下，传统静态融合方法表现不佳。

Method: MultiHateLoc框架包括：1) 可感知模态信息的时序编码器建模多模态顺序特征；2) 动态跨模态融合机制+对比对齐策略，提升模态间信息一致性与表达能力；3) 基于模态的多实例学习（MIL）损失，实现在弱监督下的视频片段级仇恨检测。

Result: 在HateMM与MultiHateClip两个数据集上，MultiHateLoc在仇恨言论定位任务上达到了目前最优的性能，能够做出细粒度、可解释的帧级检测。

Conclusion: MultiHateLoc以创新的多模态时序建模与动态融合策略，有效缓解了弱监督仇恨定位的难题，推动了多模态仇恨检测的实际应用发展。

Abstract: The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.

</details>


### [47] [Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction](https://arxiv.org/abs/2512.10416)
*Wenfei Guan,Jilin Mei,Tong Shen,Xumin Wu,Shuo Wang,Cheng Min,Yu Hu*

Main category: cs.CV

TL;DR: 本文提出了一种更适于非城市野外环境的道路矢量提取方法和数据集，通过MaGRoad新方法和WildRoad数据集显著提升了野外道路提取的精度与适应性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法虽在城市道路提取中取得进展，但在野外道路环境中因缺乏大规模数据集及主流方法结构性弱点而表现不佳，具体表现为拓扑结构错误和对遮挡、模糊交叉点敏感。

Method: 本文一是发布了WildRoad——一个专为野外路网构建的全球性矢量化道路数据集，通过互动式标注工具高效构建。二是提出MaGRoad方法，采用路径中心范式，通过多尺度视觉证据聚合，增强候选路径的连通性推断，提升方法健壮性。

Result: 在WildRoad基准上，MaGRoad方法取得了当前最优的性能，同时在城市数据集上表现出良好的泛化能力，且推理速度提升约2.5倍。

Conclusion: WildRoad数据集和基于路径的MaGRoad方法为野外道路映射提供了更强的技术基础，推动该领域发展并提升实际应用价值。

Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.

</details>


### [48] [TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning](https://arxiv.org/abs/2512.10419)
*Phu Pham,Damon Conover,Aniket Bera*

Main category: cs.CV

TL;DR: 提出了TransLocNet，一个通过跨模态注意力机制融合地面LiDAR点云和空中影像的定位框架，实现了高精度空地协同定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 空地定位难以实现，根本原因在于地面LiDAR与航拍影像之间存在很大视角和模态差异，传统方法难以有效融合两种信息。

Method: 1. 将地面LiDAR点云投影为鸟瞰图，与空中影像特征通过双向注意力机制对齐。2. 使用似然解码器输出空间位置与方向的概率分布。3. 设计对比学习模块，使两种模态在同一嵌入空间对齐，提升跨模态一致性。

Result: 在CARLA和KITTI数据集实验中，TransLocNet较现有最佳方法将定位误差降低最多63%，并实现亚米级、亚角度精度。

Conclusion: TransLocNet能够在合成和真实场景下实现鲁棒且可泛化的空地协同定位，显著提升定位性能。

Abstract: Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.

</details>


### [49] [Neural Collapse in Test-Time Adaptation](https://arxiv.org/abs/2512.10421)
*Xiao Chen,Zhongjing Du,Jiazhen Huang,Xu Jiang,Li Lu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了NCTTA方法，针对TTA场景下由于样本特征与分类器权重错位导致性能下降的问题，通过几何属性与置信度混合优化，显著提升了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应（TTA）方法提升了模型在分布外数据下的表现，但缺乏理论解释其失败的原因。因此，作者希望探究深度网络几何属性对TTA的启示，寻找性能下降的根本原因。

Method: 作者将Neural Collapse（NC）扩展到单样本层面，提出了Sample-wise Alignment Collapse（NC3+）现象，发现每个样本特征与对应的分类器权重高度对齐。进一步，作者分析分布偏移下样本对齐被破坏是性能下降的根因。并提出NCTTA方法，通过融合几何距离和预测置信度，缓解伪标签失效带来的负面影响，实现特征与分类器权重的重新对齐。

Result: 大量实验验证了NCTTA在多项任务上均能提升对分布漂移的鲁棒性。例如，在ImageNet-C上，NCTTA相比主流方法Tent有14.52%的提升。

Conclusion: NCTTA为测试时自适应提供了新的理论与方法路径，通过对齐几何特征和分类器权重，有效解决了标签失真带来的挑战，从而提升了模型在域偏移下的整体表现。

Abstract: Test-Time Adaptation (TTA) enhances model robustness to out-of-distribution (OOD) data by updating the model online during inference, yet existing methods lack theoretical insights into the fundamental causes of performance degradation under domain shifts. Recently, Neural Collapse (NC) has been proposed as an emergent geometric property of deep neural networks (DNNs), providing valuable insights for TTA. In this work, we extend NC to the sample-wise level and discover a novel phenomenon termed Sample-wise Alignment Collapse (NC3+), demonstrating that a sample's feature embedding, obtained by a trained model, aligns closely with the corresponding classifier weight. Building on NC3+, we identify that the performance degradation stems from sample-wise misalignment in adaptation which exacerbates under larger distribution shifts. This indicates the necessity of realigning the feature embeddings with their corresponding classifier weights. However, the misalignment makes pseudo-labels unreliable under domain shifts. To address this challenge, we propose NCTTA, a novel feature-classifier alignment method with hybrid targets to mitigate the impact of unreliable pseudo-labels, which blends geometric proximity with predictive confidence. Extensive experiments demonstrate the effectiveness of NCTTA in enhancing robustness to domain shifts. For example, NCTTA outperforms Tent by 14.52% on ImageNet-C.

</details>


### [50] [An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time](https://arxiv.org/abs/2512.10437)
*Stylianos Kandylakis,Christos Orfanopoulos,Georgios Siolas,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的算法框架，用于在移动设备上实时识别、分类与评估人体物理治疗动作。核心方法为姿态估计与序列比对，实验结果表明其能有效用于远程物理治疗监督和移动健康应用。


<details>
  <summary>Details</summary>
Motivation: 远程物理治疗和移动健康需求日益增长，如何实时、准确地评估患者的动作执行情况是亟需解决的问题。传统方法难以在移动设备端高效实现，因此需要一种可扩展且实时的解决方案。

Method: 将物理治疗动作分解为一系列静态姿势，通过摄像头输入与姿态识别神经网络估算人体关键点，进而计算角度特征，并通过轻量级监督学习模型在逐帧层面进行分类。整套动作以及偏差检测采用改进版Levenshtein距离的动态规划方案进行序列匹配和定位。整个系统在客户端本地实时运行。

Result: 实验评估表明，该方法可高效、准确地识别和评估身体锻炼动作，并能检测与标准模式的偏差，满足实时应用需求。

Conclusion: 该系统不仅提高了物理治疗动作的自动化分析效率，还具备良好的可扩展性和实际应用价值，特别适合远程医疗和移动健康应用场景。

Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.

</details>


### [51] [Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment](https://arxiv.org/abs/2512.10450)
*Han Li,Shaohui Li,Wenrui Dai,Chenglin Li,Xinlong Pan,Haipeng Wang,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的统一变换（unified-transform）视频压缩框架，利用双域逐步时域对齐和质量条件混合专家（QCMoE），显著提升了压缩性能并解决了误差传播问题，同时保证了质量一致和流畅的视频压缩。


<details>
  <summary>Details</summary>
Motivation: 现有学习型视频压缩框架在运动估计与补偿时在不准确的时域对齐和误差传播之间存在权衡：分别变换的方式容易误差传播，统一变换虽然可消除传播但运动估计效果不佳。本研究旨在突破这种局限，提升时域对齐的精度，并彻底消除误差传播。

Method: 提出了一种“双域逐步时域对齐”方法，首先在像素域通过光流进行粗对齐，然后在潜变量域通过引入多参考帧的流引导可变形变换器（FGDT）进行精细对齐，实现复杂运动的长期优化。另外，设计了QCMoE模块，可针对不同像素和目标质量动态调整量化步长，实现连续、细致的码率控制。

Result: 实验表明，该方法在压缩率-失真（R-D）性能上达到甚至超越了最新方法，同时完全解决了误差传播的问题。

Conclusion: 所提方法可实现高效、稳定、质量一致且无误差传播的学习型视频压缩，对于流式视频传输有很大应用价值。

Abstract: Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.

</details>


### [52] [Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network](https://arxiv.org/abs/2512.10498)
*Khurram Ashfaq,Muhammad Tariq Mahmood*

Main category: cs.CV

TL;DR: 本文提出了一种结合传统与深度学习的Shape-from-Focus（SFF）深度估计新方法，采用手工设计的方向性扩张Laplacian卷积与轻量GRU网络多步细化深度，最终通过学习的上采样模块重建高分辨深度图，在多数据集上优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的SFF方法通常分为两步：用重特征编码器提取聚焦体，再用简单聚合方式估深度，易引入噪声和伪影，影响深度图质量。因此亟需提高聚焦体的鲁棒性及深度重建的精细度。

Method: 该方法先用手工方向性扩张Laplacian核在多尺度上获取聚焦体，捕捉长距离与方向性聚焦变化；再采用轻量多尺度GRU模块对深度进行逐步细化，配合学习型凸优化上采样模块还原高分辨深度图，实现更低计算量下的高精度恢复。

Result: 在合成与真实数据集上的实验显示，本方法准确性与泛化能力均显著优于最先进深度学习和传统方法，能在复杂多样的焦点条件下重建更精细的深度图。

Conclusion: 综合实验证明，融合手工特征与深度神经网络，能有效提高SFF的深度估计精度和细节保留能力，是实现高效高质量三维重建的有效途径。

Abstract: Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

</details>


### [53] [3D Blood Pulsation Maps](https://arxiv.org/abs/2512.10517)
*Maurice Rohr,Tobias Reinhardt,Tizian Dege,Justus Thies,Christoph Hoog Antink*

Main category: cs.CV

TL;DR: 提出了Pulse3DFace，这是首个用于估计三维面部脉搏波动地图的数据集，为远程脉搏检测和相关研究提供重要资源。


<details>
  <summary>Details</summary>
Motivation: 当前远程脉搏估计（基于视频的光电容积描记）技术受到光照干扰等限制，且缺乏三维面部脉搏真实标注数据用于验证和提升算法性能，因此急需高质量三维脉搏数据集推动基础及应用研究。

Method: 构建了包含15位受试者在23个视角下的原始RGB视频（30Hz），同步采集脉搏参考信号，并用单目SfM获得3D面部模型。生成与FLAME模型兼容的三维脉搏波动地图，数据内容包括信噪比、脉搏幅值、相位等多维信息。对数据集的光照条件、地图一致性及生理特征表征能力进行了定量评估。

Result: Pulse3DFace 数据集能够有效反映面部和颈部皮肤区的生理信号特征，三维波动地图具备较好一致性，在多视角多照明条件下维持较高信噪比。为新型抗光照干扰算法和三维远程脉搏估计算法研发提供了评测标准和研究基础。

Conclusion: Pulse3DFace首创性地为面部三维脉搏分布研究提供了高质量标准数据，为后续生理信号感知、深度学习、以及合成医学视频数据生成开拓了空间，并助力相关算法的开发和优化。

Abstract: We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.

</details>


### [54] [Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA](https://arxiv.org/abs/2512.10521)
*Pasquale De Marinis,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 提出了一种名为TaP的新方法，通过低秩适应 (LoRA) 让few-shot语义分割（FSS）编码器能够针对新类别高效快速适应，显著提升了分割表现，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有FSS方法主要关注解码器改进，而忽视了编码器对新类别特征抽取能力的不足，这成为提升FSS性能的关键瓶颈。作者希望提升编码器对未见类别的泛化和适应能力。

Method: 采用低秩适应（LoRA）技术，在支持集上微调编码器，增强模型对新类别的快速适应能力，且计算开销小。该方法与模型无关，可无缝集成到现有FSS流程中。

Result: 在COCO $20^i$、Pascal $5^i$以及DeepGlobe、ISIC、Chest X-ray等跨域数据集上，TaP在不同模型和shot设置下都显著提升了分割性能；多类别复杂场景下收益尤为明显。参数敏感性分析显示，即使采用较低秩设置，也能获得优秀表现。

Conclusion: TaP有效缓解了FSS编码器对新类别泛化不足的问题，实现了更强健、高效且易推广的分割系统。其实现简便、开源，具有实际应用潜力。

Abstract: Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.

</details>


### [55] [Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding](https://arxiv.org/abs/2512.10548)
*Yuchen Feng,Zhenyu Zhang,Naibin Gu,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CV

TL;DR: 针对多模态大模型视觉感知有限的问题，提出以仿人“眨眼”式关注机制提升视觉感知能力的Blink框架。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在视觉-语言任务上虽有进展，但视觉感知仍有限。相比之下，人类能够高效动态地关注场景中的显著区域，从而提升感知效果。因此，探究和效仿人类这种受“眨眼”启发的关注方式，可能有助于提升MLLMs的视觉感知。

Method: 作者首先分析了MLLMs是否有类似人类“眨眼”式动态关注区域变化的行为。基于发现，在不同层关注区域不同且增加显著token的计算量可提升性能，提出了Blink动态视觉token分辨框架。Blink包含两个核心模块：突显性引导扫描与动态token分辨。它通过注意力图估算每层token的重要性，对关键token通过TokenSR模块提高分辨率，层间则动态丢弃已不重要的token，实现自适应的关注和计算分配。

Result: 实验证明Blink能有效提升模型视觉感知能力和多模态理解性能。

Conclusion: Blink借鉴人类视觉关注机制，动态增强对重要区域的处理，有效提升了MLLMs的视觉感知能力，且验证了方法的适用性和效率。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential "blink-like" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.

</details>


### [56] [Grounding Everything in Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2512.10554)
*Xiangxuan Ren,Zhongdao Wang,Liping Hou,Pin Tang,Guoqing Wang,Chao Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GETok的空间表示方法，通过引入可学习的空间格点和偏移token，有效提升了多模态大语言模型（MLLMs）在二维空间物体定位与推理的能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs基于自回归Transformer结构，需要对图像进行token化，但这样会限制其在二维图像空间中对物体精准定位能力。作者希望解决如何让序列语言token更好地支持二维空间物体定位的问题。

Method: 作者提出GETok空间表示方法，给模型引入了可学习的格点token用于将图像划分为空间锚点，再结合偏移token实现定位预测的精细迭代优化。通过在token中内嵌空间关系，无需改变自回归结构即可提升空间推理能力。

Result: 通过大量实验，GETok在多种指代任务上，无论是有监督微调还是强化学习设置下，都优于其他先进方法。

Conclusion: GETok方法有效提升了MLLMs在原生二维空间中的物体定位和推理能力，为多模态模型空间认知提供了新的方向。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.

</details>


### [57] [Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks](https://arxiv.org/abs/2512.10562)
*Meher Md Saad*

Main category: cs.CV

TL;DR: 提出了一种基于原型网络和骨架编码器的少样本手语识别方法，大幅提升了在样本稀缺与长尾分布下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手语识别数据稀缺且类别分布不均，导致传统分类方法容易过拟合常见类别，难以识别稀有手语。

Method: 将少样本原型网络（Prototypical Network）框架应用于骨架编码器，结合时空图卷积网络（ST-GCN）和多尺度时间聚合模块（MSTA），使用度量学习和原型分类进行训练和推理。

Result: 在WLASL数据集上，方法的Top-1准确率为43.75%，Top-5达到77.10%，较同骨干结构的标准分类方法提升逾13%。对未见的SignASL数据集零样本识别准确率近30%。

Conclusion: 所提出方法在样本稀缺下显著优于传统分类模型，并具备良好的跨数据集泛化能力，为大规模手语识别提供了可扩展途径。

Abstract: Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.

</details>


### [58] [Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner](https://arxiv.org/abs/2512.10571)
*Haojie Zheng,Shuchen Weng,Jingqi Liu,Siqi Yang,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: 提出了AVI-Edit框架，实现了更精细、同步的音频驱动视频实例级编辑，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法普遍忽视了音频与视频的同步问题，并缺乏对单个实例进行精细时空控制的能力，严重制约了精确内容创作与应用落地。

Method: 1）提出粒度感知的蒙版优化器，将用户提供的粗蒙版迭代细化成实例级别精确区域。2）设计自反馈音频代理，实现对高质量音频指导的精细时间控制。3）构建大规模实例级标注音视对应数据集，为相关任务提供有力支持。

Result: 广泛实验表明，AVI-Edit在视觉质量、条件符合度与音视频同步能力上均优于现有最新方法。

Conclusion: AVI-Edit显著提升了视频编辑时音视同步与实例级精细操控能力，为音频驱动视频编辑任务树立了新标准。

Abstract: Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.

</details>


### [59] [Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration](https://arxiv.org/abs/2512.10581)
*Wenlong Jiao,Heyang Lee,Ping Wang,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种全新而简化的全能型图像复原网络SymUNet，通过对称U-Net结构实现多种退化类型的统一处理，不依赖复杂架构和提示策略，并在多个基准测试上达到了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有全能图像复原方法依赖日益复杂的网络架构及提示机制，导致训练和推理成本高，模型难以推广。作者希望通过简化结构，提高效率，同时保持甚至超越现有方法的性能。

Method: 提出对称U-Net（SymUNet），通过对称设计对编码器和解码器特征进行对齐，以及高效的跨尺度信息传递和简单的跳跃融合方式，强化退化信息的表达。进一步提出SE-SymUNet，利用冻结CLIP语义特征通过交叉注意力机制，增强退化先验。

Result: SymUNet和SE-SymUNet在多个公开基准数据集上表现优异，超越了现有SOTA方法，同时显著降低了计算成本。

Conclusion: 对称U-Net及其语义增强变种为全能型图像复原提供了更简单、更高效且更强大的基础。未来相关工作可在此基础上进一步探索和拓展。

Abstract: All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.

</details>


### [60] [Salient Object Detection in Complex Weather Conditions via Noise Indicators](https://arxiv.org/abs/2512.10592)
*Quan Chen,Xiaokai Yang,Tingyu Wang,Rongfeng Lu,Xichun Sheng,Yaoqi Sun,Chenggang Yan*

Main category: cs.CV

TL;DR: 本文提出了一种针对复杂天气条件的显著性目标检测（SOD）新框架，通过引入噪声指示向量和噪声指示融合模块（NIFM），提升了恶劣天气下的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态SOD方法多假设低噪声视觉条件，忽略了真实场景中因天气噪声导致的分割精度下降问题。为提高在复杂天气下的泛化和鲁棒性，亟需设计适配多种天气噪声的SOD方法。

Method: 提出包括特定编码器和可更换解码器的SOD网络结构，利用one-hot向量作为噪声指示器代表不同天气类型；设计了噪声指示融合模块（NIFM），结合语义特征与噪声指示器，在编码器阶段逐步嵌入天气感知先验，同时保证编码器与主流SOD解码器的兼容性。

Result: 在WXSOD数据集上，使用不同量级训练集（100%, 50%, 30%）和多组编码器、解码器验证了方法有效性。结果表明，特定编码器（NIFM增强）在复杂天气条件下显著提升了目标分割准确率，优于传统编码器。

Conclusion: 提出的SOD框架能有效提升复杂天气场景下的目标检测性能，具有良好的泛化性和兼容主流解码器的优势，对实际应用具有现实意义。

Abstract: Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.

</details>


### [61] [Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.10596)
*J. Xiao,Y. Guo,X. Zi,K. Thiyagarajan,C. Moreira,M. Prasad*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新型遥感图像语义检索方法，通过丰富的文本描述实现高效检索，并引入了新的RSRT数据集作为评测基准。


<details>
  <summary>Details</summary>
Motivation: 遥感图像检索长期受到“语义鸿沟”影响，现有视觉-语言模型虽有潜力但需要高昂的特定领域训练，且缺乏零样本检索下对文本生成效用的评价基准。

Method: 作者提出RSRT数据集，每张图片配有多样的结构化描述。基于该数据集，提出训练无关、完全基于文本的检索方法TRSLLaVA，将跨模态检索转化为文本匹配问题，通过丰富文本描述与VLM生成字幕在统一文本嵌入空间检索，无需模型训练或微调。

Result: 在RSITMD和RSICD两个实验基准上，TRSLLaVA方法显示出与当前有监督方法接近的效果。在RSITMD上，平均召回率达42.62%，远超零样本CLIP基线（23.86%），同时超过若干顶级有监督模型。

Conclusion: 高质量、结构化文本语义表示为遥感图像检索提供了一种高效且低成本的新范式，无需任何训练即可实现有竞争力的性能。

Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.

</details>


### [62] [Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos](https://arxiv.org/abs/2512.10607)
*Bishoy Galoaa,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 该论文提出了一个名为TCAM（Track and Caption Any Motion）的运动中心视频理解新框架，能够自动发现并描述视频中的运动模式，同时实现空间定位和自然语言描述，且无需用户查询。


<details>
  <summary>Details</summary>
Motivation: 在遮挡、伪装或剧烈运动等场景下，传统基于静态外观的视频理解方法难以充分捕捉有用信息，动作为识别与描述视频内容提供了更有区分度的语义信号，因此需要一种以运动为核心的视频自动理解方法。

Method: TCAM系统自动分析视频，通过运动场注意力机制，识别并跟踪多个运动轨迹，将每段轨迹与对应的自然语言描述进行空间绑定。采用多头交叉注意（multi-head cross-attention），结合全局视频－文本对齐与细粒度空间对应的统一训练，利用对比视觉－语言表示增强运动模式的语义识别能力。

Result: 在MeViS基准测试中，TCAM实现了58.4%的视频到文本检索准确率、64.9的空间定位评分（JF），平均每个视频能发现4.8个相关表达，精确率达到84.7%，表现出较强的跨任务泛化能力。

Conclusion: TCAM展现了以运动为核心的视频理解新范式，无需用户查询能自主发现与描述多种运动表达，显著提升了在复杂场景中的泛化能力。

Abstract: We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.

</details>


### [63] [Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation](https://arxiv.org/abs/2512.10608)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour*

Main category: cs.CV

TL;DR: 该论文提出了一种结合可解释性深度学习和高精度视网膜血管分割的自动眼疾诊断方法，提高诊断精度与临床适用性。


<details>
  <summary>Details</summary>
Motivation: 近年来威胁视力的眼部疾病发病率大幅上升，亟需大规模、准确的筛查方案。现有CNN方法缺乏可解释性，影响其在临床中的信任与应用。

Method: 采用深度特征提取与可解释的图像处理模块相结合的流水线，将高保真视网膜血管分割作为辅助任务，引导疾病分类模型关注临床相关形态学特征。

Result: 模型通过结合血管分割辅助信息，有效减少了假阳性，提高了自动眼疾诊断的准确性，并增强了模型的可解释性。

Conclusion: 该方法有望缩小AI模型输出与专家医学验证之间的差距，为眼科实际临床部署提供更具可行性的工具。

Abstract: In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the "black-box" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.

</details>


### [64] [Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces](https://arxiv.org/abs/2512.10617)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出Lang2Motion框架，实现基于语言引导的点轨迹生成，能够对任意物体生成明确轨迹，并在多项任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多聚焦于人体动作或视频合成，缺乏针对任意物体轨迹生成的通用方法。希望开发出能结合自然语言描述和运动信息，生成精细轨迹的模型。

Method: 提出基于Transformer的自编码器模型，通过点追踪从真实世界视频中提取运动信息，将文本描述与轨迹可视化分别输入CLIP的冻结编码器，实现双重监督学习与交叉模态对齐。模型在CLIP对齐的嵌入空间中建模轨迹特征，支持风格迁移、语义插值和潜空间编辑。

Result: 在文本检索轨迹任务上Recall@1达到34.2%，比主流视频方法高12.5个百分点；相比视频生成基线，运动精度提升33-52%（12.4 ADE对比18.3-25.3）；即便仅以物体轨迹训练，也能在人体动作识别上取得88.3%的Top-1准确率，显示领域泛化能力。

Conclusion: Lang2Motion能高效利用自然语言和视觉信号进行点轨迹生成，结果优于现有视频方法，具备跨域泛化、自主编辑和风格迁移等优势。

Abstract: We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.

</details>


### [65] [DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM](https://arxiv.org/abs/2512.10619)
*Qintong Zhang,Junyuan Zhang,Zhifei Ren,Linke Ouyang,Zichen Wen,Junbo Niu,Yuan Qu,Bin Wang,Ka-Ho Chow,Conghui He,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出DOCR-Inspector框架，通过细粒度误差检测和分析，对文档解析结果进行全面可靠的质量评估，并发布了相关数据集与基准，模型实现上优于商业和开源同类产品。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析方法依赖标准基准测试来选择模型，但这些基准可能存在数据集偏差，且评价指标过于笼统，难以真实反映模型在实际场景下的表现和错误类型。因此，需要一种细致、全面的评测方法。

Method: 本文提出DOCR-Inspector，将文档解析质量评估形式化为28类细粒度错误检测与分析，结合VLM（视觉语言模型）进行自动化评价。为支撑模型开发，作者构建了DOCRcase-200K训练集，并提出Chain-of-Checklist推理范式，分层次完成评估。验证环节引入了手工注释的DOCRcaseBench基准测试。

Result: 实验证明，DOCR-Inspector-7B在DOCRcaseBench基准中超越了包括Gemini 2.5 Pro等商业模型和主流开源模型。同时，该模型生成的质量评估对解析结果修正具有指导意义。

Conclusion: DOCR-Inspector作为评估工具兼顾实际价值与推动领域发展，对文档解析系统的大规模改进具有促进作用。相关模型和代码已开源发布，便于后续研究与应用。

Abstract: Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.

</details>


### [66] [K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices](https://arxiv.org/abs/2512.10628)
*Bishoy Galoaa,Pau Closas,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: K-Track提出了一种结合深度学习关键帧与卡尔曼滤波的点追踪加速框架，在大幅降低推理成本的同时保留大部分精度，实现实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习跟踪器在准确性上取得了突破，但高昂的计算资源消耗严重限制了在边缘设备上的实际部署。当前主流方法依赖于逐帧的GPU推理，在算力、功耗和连通性受限的设备上难以运行。

Method: 作者提出K-Track框架，将稀疏的深度网络关键帧预测与轻量级卡尔曼滤波相结合，对中间帧进行高效估算，并采用贝叶斯不确定性传播算法确保时序连贯性。这种方法可以适配不同的追踪器，并大幅减少每帧推理的计算需求。

Result: K-Track在多个主流点追踪任务和硬件平台（如Jetson Nano、RTX Titan）上进行了测试，结果显示该方法在保持原有追踪器85%以上精度的前提下，推理速度提升5-10倍，能够实现实时性能。

Conclusion: K-Track为高性能点追踪算法在算力受限设备上的实际部署提供了有效解决方案，显著降低了资源需求并缩小了学术应用与工程实践的差距。

Abstract: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.

</details>


### [67] [TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection](https://arxiv.org/abs/2512.10652)
*Jian-Yu Jiang-Lin,Kang-Yang Huang,Ling Zou,Ling Lo,Sheng-Ping Yang,Yu-Wen Tseng,Kun-Hsiang Lin,Chia-Ling Chen,Yu-Ting Ta,Yan-Tsung Wang,Po-Ching Chen,Hongxia Xie,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 本文提出了一个新的用于可解释DeepFake检测的基准TriDF，系统评估技术在识别伪造内容的准确性、证据感知和解释可靠性三方面的能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型的发展使得伪造现实人物变得容易，带来了安全、交流和公众信任等方面的严重风险。现有检测系统不仅需要判断内容真伪，还要能够提供透明可靠的解释，因此需要新的评测标准推动该领域发展。

Method: 作者构建了TriDF基准，涵盖16个图像、视频和音频的DeepFake类型，综合评估感知（细粒度伪造痕迹识别）、检测（分类性能）以及幻觉（解释的可靠性）。并在多模态大语言模型上进行实验分析三者之间的关系和影响。

Result: 实验表明，准确的伪造痕迹感知对检测结果至关重要，而模型产生的解释幻觉现象会严重干扰决策，三者紧密相关。

Conclusion: TriDF为可解释DeepFake检测提供了统一的评测框架，有助于推动构建可信赖的多模态伪造检测系统，应对现实中的合成媒体威胁。

Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.

</details>


### [68] [NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation](https://arxiv.org/abs/2512.10660)
*Hanfeng Wu,Marlon Steiner,Michael Schmidt,Alvaro Marcos-Ramiro,Christoph Stiller*

Main category: cs.CV

TL;DR: 本论文提出了NaviHydra，一种兼具可控性与导航指导能力的端到端自动驾驶模型，通过融入高层导航指令，实现了高效、安全的轨迹生成，并在NAVSIM基准上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中，传统基于规则的方法难以应对动态环境，而端到端方法又难以遵循明确的导航指令，实现可控导航和安全驾驶仍有挑战。

Method: 提出了NaviHydra模型，通过从已有的基于规则的仿真器蒸馏知识，结合高层导航指令作为控制信号，生成与意图一致的轨迹。同时采用俯视图（BEV）轨迹采集增强特征提取，并设计了新的导航遵循度评价指标，用于评估模型对指定路线的服从性。

Result: NaviHydra在多种导航命令下的可控性测试中，都优于现有基线模型，并在NAVSIM基准上取得了最优结果。

Conclusion: NaviHydra模型有效地提升了自动驾驶系统对高层导航命令的响应能力和安全性，为可控的端到端自动驾驶提供了新的解决方案。

Abstract: The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.

</details>


### [69] [XDen-1K: A Density Field Dataset of Real-World Objects](https://arxiv.org/abs/2512.10668)
*Jingxuan Zhang,Tianqi Yu,Yatu Zhang,Jinze Wu,Kaixin Yao,Jingyang Liu,Yuyao Zhang,Jiayuan Gu,Jingyi Yu*

Main category: cs.CV

TL;DR: 该论文提出了XDen-1K，这是首个专注于现实物理属性（尤其是体密度）的大规模多模态数据集，并展示其在重心估算和机器人操作等任务中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽然能很好地捕捉物体的表面几何和外观，但通常忽视了内部物理属性（如体密度），而这些属性对机器人操作和物理仿真至关重要。造成这一不足的主要原因是缺乏现实世界的大规模相关数据。

Method: 作者构建了XDen-1K数据集，包含1000个实际对象、148个类别，提供高分辨率3D几何模型、部件级注释及真实双平面X射线扫描图。提出新的优化框架，通过稀疏X射线视图恢复每个物体的高保真体密度场。同时，将X射线图像作为条件信号用于分割网络，实现体积分割，并在下游机器人任务上进行实验。

Result: 实验证明，该数据集能有效提升重心估算准确率和机器人操作成功率。

Conclusion: XDen-1K为物理属性感知提供了基础资源和挑战性新基准，有望推动具备物理推理能力的视觉感知和机器人智能体的研究进展。

Abstract: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.

</details>


### [70] [Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching](https://arxiv.org/abs/2512.10674)
*Javier Villena Toro,Mehdi Tarkian*

Main category: cs.CV

TL;DR: 本文提出了Geo6DPose，一种无需训练、可在本地设备上实时运行的6D姿态估计算法，在保持较高准确率的同时，大幅降低对计算资源和网络连通性的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有零样本6D物体姿态估计方法依赖于大型云端模型，带来高延迟、能耗大以及数据安全等问题，难以满足机器人本地快速推理的实际需求。作者因此希望开发无需云端、无需训练且更适合本地硬件环境的解决方案。

Method: Geo6DPose综合利用基础模型的视觉特征和几何过滤策略。对模板和场景分别提取视觉描述符，并通过相似度计算建立互相关系，结合三维几何投影实现关键点配对。最终姿态通过RANSAC算法恢复，并用兼顾重投影一致性和空间支持度的度量进行排序以增强在噪声、遮挡等情况的鲁棒性。

Result: Geo6DPose在单张常规GPU上可实现亚秒级推理速度（1.08 FPS），且平均召回率达到53.7，与现有大模型云端方案相当。无需训练、微调或联网，可兼容最新视觉主干网络。

Conclusion: Geo6DPose为6D姿态估计领域提供了一条全本地、轻量级、无训练的实现路径，有效解决了实时机器人感知场景中对推理速度、鲁棒性和部署灵活性的三重需求。

Abstract: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.

</details>


### [71] [Optimal transport unlocks end-to-end learning for single-molecule localization](https://arxiv.org/abs/2512.10683)
*Romain Seailles,Jean-Baptiste Masson,Jean Ponce,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出了一种新的单分子定位显微（SMLM）深度学习方法，摒弃传统的非极大值抑制（NMS），采用端到端的最优传输损失函数，并结合显微成像物理知识，显著提升了高密度荧光分子样本的定位精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的SMLM方法为避免荧光分子信号重叠，需要低密度激发，导致成像时间较长，限制了活细胞等快速生物过程的观测。虽然深度学习能处理较高密度数据，但依赖NMS，存在不可微和漏检等问题。

Method: 作者将SMLM中的检测任务表述为集合匹配问题，利用最优传输理论设计了全新的损失函数，摆脱NMS的依赖，实现了可微的端到端训练。同时提出了迭代神经网络结构，将显微镜光学系统知识融入神经网络模型中。

Result: 在合成及真实生物数据集的实验表明，新提出的损失函数与神经网络结构在中高密度发射体条件下均超越现有方法。

Conclusion: 该方法突破了SMLM在密集荧光分子环境下的检测瓶颈，缩短了成像时间，提高了成像分辨率与精确度，为活细胞快速成像等应用带来新可能。

Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.

</details>


### [72] [Sharp Monocular View Synthesis in Less Than a Second](https://arxiv.org/abs/2512.10685)
*Lars Mescheder,Wei Dong,Shiwei Li,Xuyang Bai,Marcel Santos,Peiyun Hu,Bruno Lecouat,Mingmin Zhen,Amaël Delaunoy,Tian Fang,Yanghai Tsin,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: SHARP是一种从单张图片生成高质量虚拟视角照片的新方法，通过单次神经网络推理快速生成3D高斯表示，并能实时渲染照片级真实感多视角图像，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有从单张图片生成真实感新视角的方法常存在渲染慢、泛化能力弱、缺乏绝对尺度等问题。本文希望提升速度、质量和泛化能力，并支持度量相机运动。

Method: 该方法（SHARP）使用神经网络对输入图片直接回归生成场景的3D高斯参数，通过单次前向推理完成，极大地加快了速度。得到的3D高斯可用于高分辨率、实时渲染和多样视角生成，具有绝对尺度信息。

Result: SHARP方法可在标准GPU上1秒内完成单次推理，支持零样本泛化，在多数据集上LPIPS和DISTS指标显著优于现有最佳方法，同时渲染速度提升三个数量级。

Conclusion: SHARP实现了从单张图片高效生成精准、多视角真实感新图像，综合速度、泛化性及画质刷新了同类任务新水平。

Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp

</details>


### [73] [CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images](https://arxiv.org/abs/2512.10715)
*Matias Cosarinsky,Nicolas Gaggion,Rodrigo Echeveste,Enzo Ferrante*

Main category: cs.CV

TL;DR: 本文针对医学影像分割中的解剖学标志点（landmark-based）分割，提出了两种互补的不确定性估计方法，并发布了大规模带不确定性估计的新数据集CheXmask-U，为实际临床安全应用提供有力保障。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割系统的实际部署对于安全性有极高要求，必须能识别出低置信度或不可靠的预测结果以实现人类监管。以往大多聚焦于像素级分割的不确定性，忽略了具有拓扑结构优势的landmark-based分割。而相关的不确定性分析尚不充分，亟需研究。

Method: 受混合神经网络结构启发，本文采用标准卷积编码器结合基于图的生成式解码器，并利用其变分潜在空间，提出了两种互补的不确定性度量：(i) 潜在不确定性（由学习到的分布参数直接获得）；(ii) 预测不确定性（由潜在空间多次采样生成随机预测结果后统计）。两者可反映全局及局部的分割可靠性。

Result: 通过控制噪声干扰实验，作者发现上述不确定性指标能随着扰动强度升高而同步提高，对全局与局部降解都敏感。进一步，这些信号能有效识别出不可靠预测，与人工标签对比结果良好，并能支持分布外样本检测。在CheXmask数据集上验证有效。最终，作者发布了标注节点级不确定性的大规模胸部X线标志点数据集CheXmask-U。

Conclusion: 不确定性估计能够提升landmark-based解剖结构分割方法的稳健性和部署安全性，有助于临床实际应用和后续研究推进。CheXmask-U数据集的发布为进一步研究提供了重要资源。

Abstract: Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.

</details>


### [74] [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719)
*Peizheng Li,Zhenghao Zhang,David Holtz,Hang Yu,Yutong Yang,Yuzhi Lai,Rui Song,Andreas Geiger,Andreas Zell*

Main category: cs.CV

TL;DR: 论文提出了SpaceDrive框架，通过将空间信息以位置编码（PEs）而非传统数字Token输入，提升VLM自主驾驶的空间理解和规划能力，并在主流数据集上获得业界领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）虽然具备强大的视觉和推理能力，但在理解3D空间关系方面存在显著不足，而这对自动驾驶等物理交互系统至关重要。

Method: 提出SpaceDrive框架，将3D空间信息通过统一的位置编码（PEs）方式嵌入，覆盖来自多视角深度估计、历史自车状态和文本提示的坐标数据。这些3D PEs既增强2D视觉Token，也作为通用坐标输入输出，实现空间语义联合推理和轨迹直接回归。

Result: SpaceDrive在nuScenes数据集上获得了最优的开放环路自动驾驶表现，并在Bench2Drive闭环基准获得了78.02的次高分，优于现有的VLM自动驾驶方法。

Conclusion: 通过空间感知位置编码，有效提升了VLM在3D物理任务中的空间理解与动作规划能力，推动了端到端视觉语言自动驾驶的性能极限。

Abstract: End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.

</details>


### [75] [Video Depth Propagation](https://arxiv.org/abs/2512.10725)
*Luigi Piccinelli,Thiemo Wandel,Christos Sakaridis,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了一种高效且鲁棒的在线视频深度估计方法VeloDepth，通过传播历史帧深度与特征，提升了推断速度与时序一致性，并在多个基准数据集上实现了领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频深度估计方法常因逐帧单目估计导致时间不连续和误差积累，或需高昂的算力支持复杂的时序建模，难以满足实时应用需求。为此，作者希望设计一种既高效又能确保时序一致性的深度估计方案。

Method: 提出了一种基于深度特征传播的管道VeloDepth，核心为新颖的传播模块：利用光流引导的特征变换进行深度预测传播，并通过残差学习优化误差，有效融合历史信息，实现端到端时序一致性约束。

Result: 在多个主流基准上进行zero-shot测试，VeloDepth在推断速度、时序一致性上超过主流方法，精度也十分有竞争力。

Conclusion: VeloDepth为多样化视觉感知任务提供了可行、实时、高效的深度估计解决方案，并且开源了代码与模型，促进后续研究与应用。

Abstract: Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth

</details>


### [76] [IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation](https://arxiv.org/abs/2512.10730)
*Yuan-Ming Li,Qize Yang,Nan Lei,Shenghao Fu,Ling-An Zeng,Jian-Fang Hu,Xihan Wei,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新范式，通过将动作生成、评估与优化多轮交互结合，实现更高效的动作理解与生成。作者开发了IRG-MotionLLM模型，并在多个基准上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的动作大模型通常将动作理解和生成任务分开处理，缺乏二者之间的交互和知识流通，无法充分发挥这两个任务的协同作用。作者希望探索如何促动理解与生成间的反馈，提升整体性能。

Method: 文章提出了动作用交错推理(Interleaved Reasoning for Motion Generation, IRMoGen)的范式，通过多轮动作生成、评估和优化提升模型性能。具体实现为IRG-MotionLLM模型，采用三阶段训练、自动生成标注数据，并实现动作生成、评估和优化的无缝集成。

Result: 实验证明：(1) 评估与优化显著提升文本-动作对齐；(2) 交错的生成、评测和优化带来稳定性能提升；(3) IRG-MotionLLM在各项标准任务上超越基线模型，表现优异。跨评测标准下其有效性也得到了验证。

Conclusion: 通过在动作生成、评估和优化间交错推理，IRG-MotionLLM实现了动作大模型理解与生成的协同提升，可为后续相关研究提供新的方向和方法。

Abstract: Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.

</details>


### [77] [LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation](https://arxiv.org/abs/2512.10750)
*Tianyu Zhou,Junyi Tang,Zehui Li,Dahong Qian,Suncheng Xiang*

Main category: cs.CV

TL;DR: 本论文提出了一套基于多模态大模型（MLLMs）的息肉诊断自动报告生成方法LDP，通过构建专家标注的多模态内镜数据集MMEndo，并进行参数高效微调与偏好优化，实现了诊断报告质量的显著提升且训练成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有结肠镜息肉自动化报告系统由于高质量多模态医疗数据稀缺，导致报告存在不一致和虚构信息，难以满足临床需求，因此亟需更高效、更准确的自动诊断报告方法。

Method: 1. 构建了MMEndo多模态内镜数据集，包含专家标注的图像文本对；2. 采用Qwen2-VL-7B为基础，利用参数高效微调（LoRA）和直接偏好优化（DPO）对模型进一步调整，使其更符合临床标准。

Result: 在自动及临床专家评估中，该方法均优于现有基线方法，获得了7.2/10的医生评分。此外，相比完全微调方法，训练算力成本降低了833倍，且在IU-XRay数据集上验证了鲁棒性。

Conclusion: LDP方法为基层医疗提供了一种高可扩展性和临床可用的息肉诊断自动化报告解决方案，显著提升了自动化诊断的准确性和实用性。

Abstract: Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.

</details>


### [78] [Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2512.10765)
*Rene Lisasi,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个自动化的冠状动脉血流动力学数据生成与AI学习管道，通过深度学习直接从CCTA图像高效预测血压分布，大幅替代传统CFD计算，提高了冠心病辅助诊断的效率与可用性。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法诊断冠心病虽能提供有价值的血流动力学指标，但计算量大、耗时且难以大规模临床应用，导致难以获取充足的标注数据训练AI模型，限制了无创生理评估方法的推广。

Method: 作者开发了端到端自动化管道，包括冠状动脉几何结构从CCTA的自动提取、自动化仿真数据生成，以及扩散回归模型的冠状动脉血压预测，实现无需CFD即可通过CCTA特征直接推断血压分布。

Result: 在冠状动脉血流动力学模拟数据集上，该方法取得了R2为64.42%、RMSE为0.0974、标准化RMSE为0.154的效果，全面优于多种基线方法，达到了当前最优水平。

Conclusion: 该研究创新性地提供了一个高效、自动化、可扩展的冠状动脉血压非侵入性预测框架，有助于推动基于生理信号的冠心病快速辅助诊断在临床的广泛应用。

Abstract: Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.

</details>


### [79] [What matters for Representation Alignment: Global Information or Spatial Structure?](https://arxiv.org/abs/2512.10794)
*Jaskirat Singh,Xingjian Leng,Zongze Wu,Liang Zheng,Richard Zhang,Eli Shechtman,Saining Xie*

Main category: cs.CV

TL;DR: 本文探讨了生成模型训练中的表示对齐（REPA）机制，发现空间结构信息比全局语义更关键，并提出了一种简易改进方法iREPA以提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 目前常认为，预训练视觉编码器的全局语义能力越强，对生成模型的表示对齐训练帮助越大。然而，实际哪些表征特性最有助于生成性能尚存疑问，推动作者对目标表示中“全局语义”与“空间结构”的作用进行系统研究。

Method: 大规模实证分析了27种不同视觉编码器及模型规模，比较了全局语义与空间结构对生成表现的影响。随后，提出用卷积层替换标准REPA中的MLP层，并在外部表征上加入空间归一化，突出空间信息的迁移，形成轻量方法iREPA。

Result: 实验表明，全局语义表现（如ImageNet-1K准确率）并非主导生成表现，目标表示的空间结构更为重要。提出的iREPA方法能够在各种模型和训练变体下显著提升REPA的收敛速度。

Conclusion: 研究颠覆了以往关于表征对齐优化机制的认知，并提出了简洁实用的优化方法。iREPA有助于更高效地训练生成模型，为今后表征对齐机制的理论与方法研究提供了新视角。

Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa

</details>


### [80] [Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading](https://arxiv.org/abs/2512.10808)
*Masum Shah Junayed,John Derek Van Vessem,Qian Wan,Gahie Nam,Sheida Nabavi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图拉普拉斯注意力变换器（GLAT）和迭代细化模块（IRM）的方法，用于提高前列腺癌全切片图像的分级准确性和空间一致性，并在多个公开和私有数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 全切片图像体积庞大且组织结构异质性强，传统分级方法常依赖随机或静态切块选择，易引入冗余、无信息区域，影响分类性能。因此，亟需提高关键区域识别与空间信息利用能力的新方法。

Method: 方法包括两个核心部分：1）IRM利用预训练ResNet50和零梯度基础模型，迭代地细化Patch选择，仅保留与诊断最相关的区域。2）GLAT将Patch构建为图节点，通过图拉普拉斯约束增强空间一致性，并用可学习的过滤机制优化特征表达。另有凸聚合机制动态调整Patch权重，形成健壮的切片级表征。

Result: 在五个公开及一个私有数据集上，实验表明该方法在前列腺癌分级任务上的准确性和空间一致性显著优于现有先进方法，并具备良好的计算效率。

Conclusion: 所提GLAT-IRM方法在提升特征学习、空间一致性和关键区域识别能力方面效果突出，为数字病理图像分级提供了新的高效解决方案。

Abstract: Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.

</details>


### [81] [Self-Ensemble Post Learning for Noisy Domain Generalization](https://arxiv.org/abs/2512.10818)
*Wang Lu,Jindong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SEPL（Self-Ensemble Post Learning）的新方法，应对多领域泛化（Domain Generalization）下的标签噪声问题，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉和机器学习方法在处理数据分布转移和标签噪声时，鲁棒性表现不足。标签噪声会加剧模型对非本质特征的误用，降低泛化能力，因此亟需提升现有泛化方法在有噪声环境下的表现。

Method: SEPL方法包括：1）特征探测训练，通过利用模型中的中间特征层，训练多个子分类器以多样化模型特征；2）预测集成推理，将不同子分类器的预测通过类众包推理整合。针对噪声标签，还使用半监督算法进行训练，使判别更加稳定和可靠。

Result: 大量实验表明，SEPL方法能显著提升现有多领域泛化方法在噪声环境下的鲁棒性和表现能力，并具备很强的实际应用潜力。

Conclusion: SEPL不仅提升了算法在标签噪声下的鲁棒性，还为实际场景提供了高灵活性、高性能的候选解决方案，对计算机视觉领域具有重要意义。

Abstract: While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.

</details>


### [82] [PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning](https://arxiv.org/abs/2512.10840)
*Jianqi Chen,Biao Zhang,Xiangjun Tang,Peter Wonka*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的6D姿态估计算法PoseGAM，无需传统的特征匹配，能更好地处理未见过的对象，实验结果表明其在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 6D姿态估计在处理未见过的新对象时一直是难点，现有方法过于依赖显式的特征对应，易受干扰且泛化性不足。

Method: 提出了PoseGAM，一种基于多视角和几何感知的6D姿态估计方法。该方法利用多张模板图片和目标图片，结合显式点云几何以及通过神经网络学习到的几何特征，直接预测目标的6D姿态，无需人工特征匹配。此外，还构建了包含19万多对象、复杂环境下的大规模合成数据集来提升算法鲁棒性与泛化能力。

Result: 在多个公开基准数据集上，PoseGAM取得了平均5.1%的性能提升，部分数据集上甚至提升达17.6%，远超现有同类方法，证明了其对未知对象的强大泛化能力。

Conclusion: PoseGAM方法在提升6D姿态估计的泛化性和准确度方面具有明显优势，有助于推动相关领域的发展。

Abstract: 6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .

</details>


### [83] [SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation](https://arxiv.org/abs/2512.10860)
*Kehong Gong,Zhengyu Wen,Mingxi Xu,Weixia He,Qi Wang,Ning Zhang,Zhengyu Li,Chenbin Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: SWiT-4D 是一种能够将单目视频转换为高质量可动画4D网格资产的新方法，即使在极少4D监督数据下也表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前单目视频转4D网格动画资产非常困难，主要原因是缺乏大规模真实4D网格数据集，导致数据驱动方法难以推广。同时，图像到3D生成领域已有成熟的先验模型和数据集，可以加以利用。

Method: SWiT-4D利用滑动窗口Transformer，参数无损地对视频序列进行时空建模，可与任何基于DiT的图像到3D生成器无缝结合，在保持单帧生成流程的同时，实现任意长度视频的4D网格重建。此外，针对静态相机的单目视频，提出了基于优化的轨迹模块用于全局平移恢复。

Result: SWiT-4D通过极少量（如一个小于10秒短视频）微调即可获得高保真几何与稳定的时序一致性。实验证实SWiT-4D在各类数据集（包括域内与复杂跨域场景）上在时间平滑性上显著优于现有方法。

Conclusion: SWiT-4D极大提升了单目视频到4D网格生成的易用性与效果，对高质量4D动态资产生成具有实际部署价值，并有效降低了4D标注数据的需求。

Abstract: Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/

</details>


### [84] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一个全面的、由人工标注的视频空间智能基准MMSI-Video-Bench，用于评估多模态大模型（MLLMs）在基于视频的空间理解能力，并对现有25个主流模型进行了评测，结果显示人机差距巨大。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能系统评估MLLMs在物理场景中空间理解进展的基准，阻碍模型向通用物理环境助手发展。

Method: 构建了MMSI-Video-Bench基准，涵盖感知、规划、预测和跨视频推理四个层级，共1,106个问题，覆盖1,278个视频片段，内容由3D视觉专家精心设计并注明推理理由，还细分为三大子任务面向具体领域能力评测。

Result: 对25个主流MLLMs测试后发现，许多模型表现接近随机，最好的推理模型仍落后人类近60%。即使经过空间微调的模型，在该基准下仍无法很好泛化。进一步细致误差分析揭示几何推理、运动识别、长时预测和多视频关联等方面存在系统性失败。常见帧采样、3D空间线索和思维链提示对表现提升有限。

Conclusion: MMSI-Video-Bench为基于视频的空间智能研究提供了权威测试平台，有助于推进MLLMs在复杂物理环境中的发展。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [85] [From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867)
*Zongzhao Li,Xiangzhe Kong,Jiahui Su,Zongyang Ma,Mingze Li,Songyou Li,Yuelin Zhang,Yu Rong,Tingyang Xu,Deli Zhao,Wenbing Huang*

Main category: cs.CV

TL;DR: 本文提出了微观空间智能（MiSI）的概念，并构建了大规模基准MiSI-Bench，用于评估视觉-语言模型（VLMs）在理解和推理微观空间关系方面的能力。结果显示，当前VLMs在此任务上显著落后于人类，但经过微调的模型在部分子任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 科学发现高度依赖于对微观实体空间关系的感知和理解，目前尚无系统性评测VLMs在该领域能力的基准。本文提出新基准，以推动科学领域AGI发展。

Method: 作者构建了MiSI-Bench基准，包含逾16.3万个问答对和58.7万张分子图片，涵盖九项任务，全面考察VLMs的空间推理、多关系识别等能力，并对现有主流VLMs及微调模型进行系统评测。

Result: 主流VLMs在整体任务上表现明显不如人类，但对7B微调模型，在空间变换子任务上表现出色，甚至超过人类；但在与科学知识紧密相关的任务（如氢键识别）上表现较差。

Conclusion: 现有VLMs在微观空间智能任务上整体能力有限，尤其缺乏科学领域知识，集成显性专业知识是实现科学AGI的关键。

Abstract: This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.

</details>


### [86] [MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos](https://arxiv.org/abs/2512.10881)
*Kehong Gong,Zhengyu Wen,Weixia He,Mingxi Xu,Qi Wang,Ning Zhang,Zhengyu Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的类别无关动作捕捉（CAMoCap）方法MoCapAnything，能够实现任意带骨骼的3D模型动画重建，无需针对特定物种或模板定制。


<details>
  <summary>Details</summary>
Motivation: 现有动作捕捉方法多数仅适用于特定人类或某种生物，缺乏对不同种类模型（特别是非人类、多样形态资产）统一、高质量动作捕捉的能力，极大限制了内容创作的灵活性和可扩展性。作者希望打破这种局限，实现真正通用化、类别无关的动作捕捉。

Method: MoCapAnything通过三个可学习模块和轻量IK阶段实现：1）Reference Prompt Encoder从资产骨骼、网格及渲染图像中提取每个关节的查询特征；2）Video Feature Extractor从视频提取稠密视觉特征并重建4D变形网格，用于桥接视频到关节空间的鸿沟；3）Unified Motion Decoder融合上述线索生成时间连贯的3D关节轨迹；最后通过约束感知的逆向运动学（IK）恢复资产特定的骨骼旋转动画。此外，作者新建了Truebones Zoo数据集为训练和验证提供支持。

Result: 在多个标准测试集和实际‘野外’视频上，MoCapAnything能输出高质量动画，并且可以将动作跨不同形体/物种资产进行有效迁移，兼容多种异构绑定（rig）。

Conclusion: MoCapAnything实现了基于提示的3D动作捕捉新范式，无需针对模板定制，极大扩展了动作捕捉的适用范围和自动化程度，为未来任意资产的动画重建和内容生成提供了有力工具。

Abstract: Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/

</details>


### [87] [PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction](https://arxiv.org/abs/2512.10888)
*Brandon Smock,Valerie Faucon-Morin,Max Sokolov,Libin Liang,Tayyibah Khanam,Maury Courtland*

Main category: cs.CV

TL;DR: 本文提出了一个新大规模数据集PubTables-v2，用于推动视觉文档理解中的表格提取（TE）任务，特别是多页表格结构识别，并基于该数据集评估了领域专用视觉-语言模型和提出了一种新模型POTATR。


<details>
  <summary>Details</summary>
Motivation: 表格提取是文档理解的重要难题，但大规模标注数据的缺失，限制了模型尤其是深度学习和视觉-语言模型的发展与评测。

Method: 作者构建了一个名为PubTables-v2的大规模数据集，涵盖多种复杂表格提取任务，特别包括首个多页表格结构识别基准；并基于此，评测了视觉-语言模型的表现，同时开发了一种新的图结构模型POTATR用于综合页面级的表格提取。

Result: 使用PubTables-v2，作者展示了现有视觉-语言模型在多个TE任务上的进展，并进一步验证了POTATR模型在全面页面级表格提取上的有效性。

Conclusion: PubTables-v2数据集为表格提取任务提供了强大支持，推动了多页和复杂文档场景下的研究进展，为后续方法开发和评测提供了坚实基础。数据、代码与模型将公开发布，促进该领域发展。

Abstract: Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.

</details>


### [88] [DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance](https://arxiv.org/abs/2512.10894)
*Peiying Zhang,Nanxuan Zhao,Matthew Fisher,Yiran Xu,Jing Liao,Difan Liu*

Main category: cs.CV

TL;DR: 本文提出了DuetSVG，一种联合生成图像和SVG的多模态模型，有效提升了SVG生成的视觉一致性和几何连贯性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的SVG生成方法只依赖文本，缺乏视觉信号，导致复杂语义理解和高质量SVG生成困难。

Method: DuetSVG模型联合生成图像token和对应SVG token，并在训练时结合图像和SVG数据；推理阶段引入新颖的测试时调节策略，利用模型的视觉预测结果引导SVG解码。

Result: 大量实验表明，DuetSVG在视觉一致性、语义对齐和语法规范性等方面均优于现有SVG生成方法。

Conclusion: DuetSVG能生成视觉优良、语义契合、结构合理的SVG，广泛适用于多种生成任务，显著推动SVG生成领域发展。

Abstract: Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.

</details>


### [89] [FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos](https://arxiv.org/abs/2512.10927)
*Yulu Gan,Ligeng Zhu,Dandan Shan,Baifeng Shi,Hongxu Yin,Boris Ivanovic,Song Han,Trevor Darrell,Jitendra Malik,Marco Pavone,Boyi Li*

Main category: cs.CV

TL;DR: 本文提出一个完全自动的数据筛选管道 FoundationMotion，用于大规模、细粒度运动数据集的构建，实现了对运动理解任务模型的显著提升。


<details>
  <summary>Details</summary>
Motivation: 运动理解对于物理推理和未来状态预测至关重要，但受限于高质量、细粒度运动数据集匮乏，而现有数据集人工标注成本高，不利于扩展。

Method: 构建 FoundationMotion 自动化管道，先通过视频检测与追踪获取物体轨迹，然后结合大语言模型（LLM）生成详细运动描述和多样的问题-答案对，形成丰富的运动数据集。利用这些数据集对 NVILA-Video-15B、Qwen2.5-7B 等开源模型进行微调。

Result: 微调后的模型在运动理解和空间推理能力上有大幅提升，超过了 Gemini-2.5 Flash 等闭源模型和 Qwen2.5-VL-72B 等大型开源模型，在多个运动理解基准测试上表现优异。

Conclusion: FoundationMotion 解决了细粒度运动数据集扩展难题，为运动理解和空间推理模型的性能提升提供了可扩展的数据支撑。

Abstract: Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.

</details>


### [90] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: 本文提出了BabyVLM-V2，这是一种受婴幼儿发展启发的视觉-语言基础模型预训练方法，在样本效率和认知能力评测等方面有显著提升，部分任务超越了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿的学习轨迹高效且自然，启发了对视觉基础模型进行更具发展性和样本效率的预训练，旨在提升AI学习能力并评估其认知发展水平。

Method: 作者构建了一个以婴儿为中心的长时序、多模态预训练数据集，包括视频-语音、图片-语音及多轮对话，并将NIH Baby Toolbox的视觉相关测评转化为十项多模态任务组成的DevCV Toolbox，对模型的空间推理、记忆和词汇理解能力进行评估。

Result: 实验显示，从零开始预训练的小型模型在DevCV Toolbox多项任务中表现突出，部分超过了GPT-4o，验证了模型的发展合理性和高效率。

Conclusion: BabyVLM-V2为婴幼儿启发的视觉基础模型预训练提供了统一且系统的方案，有望推动该领域对认知发展合理性的研究和AI模型训练效率的提升。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [91] [GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting](https://arxiv.org/abs/2512.10939)
*Madhav Agarwal,Mingtian Zhang,Laura Sevilla-Lara,Steven McDonagh*

Main category: cs.CV

TL;DR: 本文提出了一种结合高斯泼溅（Gaussian Splatting）和3D可变形模型的新方法，可根据音频实时生成表情稳定、个性化的说话人头像视频。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法（如扩散模型和高斯泼溅）可生成高保真或实时的说话人头像，但时域稳定性差或易出现伪影，无法满足真实场景的需要。作者旨在提升人物头像的表情连贯性和个性化表现。

Method: 该方法将高斯泼溅与3D可变形模型相结合，通过transformer直接根据音频预测驱动模型参数，从而提升输出的时序一致性。输入为单目视频和独立的音频语音信号，实时生成说话人头部动画。

Result: 实验显示该方法在定量和定性评估中都取得了具有竞争力的表现，能够实时生成具有较高时序一致性和稳定性的说话人头像视频。

Conclusion: 基于3D可变形模型和音频驱动transformer的高斯泼溅方法，有效提升了说话人头像的实时性、稳定性和个性化，适用于真实交互场景。

Abstract: Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.

</details>


### [92] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView提出了统一的4D一致性扩散模型框架，实现了对空间、时间和视角的灵活表达，在多项基准任务与特定模型相比结果优异，能够泛化并提升多视角合成与相机控制视频生成。


<details>
  <summary>Details</summary>
Motivation: 以往方法仅针对4D一致性任务的某一子集（如新视角合成、带相机控制的文本到视频、图像到视频等），数据分散、模型适用范围有限，缺乏可统一处理多种任务的框架。作者希望解决各任务碎片化训练、欠泛化的问题。

Method: OmniView将空间、时间、视角分别建模，允许多种组合输入。模型利用一致性扩散机制，支持静态/动态/多视角输入生成新视角，还可时间前后预测、基于文本或图片与相机轨迹控制生成视频，形成统一的4D一致性解决方案。

Result: OmniView在多项基准测试上优于/媲美现有专用模型。特别是在多视角NVS LLFF数据集图像质量提升33%、动态NVS数据集提升60%、RE-10K静态相机提升20%、文本视频生成中相机轨迹误差降低4倍，展现其强泛化能力。

Conclusion: OmniView首次实现了通用性强的4D一致性扩散模型，能够高效处理各类相机控制条件下的视频生成任务，为统一、通用4D生成模型研究提供了有力方案。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [93] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Mull-Tokens的多模态推理方法，使模型能在空间推理任务中自由地以图像或文本方式进行中间思考，无需复杂的跨模态手工处理。实验显示Mull-Tokens显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型难以大规模扩展，依赖于昂贵的图像生成、专业工具或手工推理数据，无法灵活自由地在图像和文本之间切换。本文旨在通过更简洁有效的方法让模型以更通用的方式进行推理。

Method: 提出Mull-Tokens，即模态无关的潜变量token，用于承载文本或图像的中间信息。训练方法包括：先通过交错的文本与图像推理轨迹进行有监督预训练，再通过仅利用最终答案的无监督微调。

Result: 在4个具有挑战性的空间推理任务（如解谜、视角转换）中，Mull-Tokens相较文本推理或交错模态推理基线平均提升3%，在某些推理分割实验上最高提升16%。

Conclusion: Mull-Tokens为多模态抽象推理提供了一种简单且有效的解决方案，能够促进文本与视觉推理结合相关研究的发展。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [94] [VL-JEPA: Joint Embedding Predictive Architecture for Vision-language](https://arxiv.org/abs/2512.10942)
*Delong Chen,Mustafa Shukor,Theo Moutakanni,Willy Chung,Jade Yu,Tejaswi Kasarla,Allen Bolourchi,Yann LeCun,Pascale Fung*

Main category: cs.CV

TL;DR: VL-JEPA是一种新的视觉-语言模型，通过预测连续嵌入而非传统的token生成，参数更少、性能更好，支持高效推理及多任务。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉-语言模型通常采用自回归token生成方式，参数量大、效率较低。作者旨在设计一种在参数数量和推理效率上更优，同时不损失性能的方法。

Method: VL-JEPA在联合嵌入预测架构（JEPA）下工作，直接预测目标文本的连续嵌入。训练采用与标准token空间VLM同样的视觉编码器和数据，推理时只在需要时调用文本解码器将嵌入还原为文本，并支持选择性解码，从而减少解码操作。

Result: 实验证明，VL-JEPA比传统token空间VLM参数少50%，在八个视频分类和八个检索任务上性能优于CLIP和SigLIP2等，在GQA、TallyQA等四个VQA任务上表现与主流大模型相当。解码操作次数大幅下降，效率提升2.85倍。

Conclusion: VL-JEPA能以更少的参数和更高的推理效率实现优秀表现，且具备多任务能力，如开放词汇分类、文本到视频检索等，在多个数据集上超越现有模型。

Abstract: We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.

</details>


### [95] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 本文提出了AlcheMinT框架，实现了视频中多主体的精细化时序控制，首次解决了现有个性化视频生成无法控制主体出现/消失时间的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的主体驱动视频生成方法无法对主体的出现和消失进行精细时序控制，这对于合成、故事版和可控动画等应用非常关键。因此，需要一种能够自由控制主体时序的生成方法。

Method: AlcheMinT引入了显式时间戳条件，在扩散模型预训练的视频生成模型的时序位置编码基础上，提出新型位置编码，能准确表示主体的时间区间。同时通过结合主体描述文本token，实现视觉识别与文本描述的紧密绑定，减少歧义；方法还采用token级拼接，避免了增加复杂的交叉注意力模块，基本不增加模型参数量。

Result: 作者建立了包含主体相似性、视频质量和时序精确性的基准，并实验证明AlcheMinT生成的视频视觉质量与SOTA方法相当，但首次实现了多主体的精准时序控制。

Conclusion: AlcheMinT不仅保证了高质量的个性化视频生成，同时带来了对多主体在时序上的精确可控性，在视频合成等实际应用中具有重要价值。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [96] [MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation](https://arxiv.org/abs/2512.10945)
*Henghui Ding,Chang Liu,Shuting He,Kaining Ying,Xudong Jiang,Chen Change Loy,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为MeViS的大规模多模态数据集，专注于基于运动表达的目标视频分割与跟踪，并评测了现有方法及提出的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的指代视频分割数据集往往着重于显著目标且语言表达偏重静态属性，忽视了运动在视频及语言中的作用，因此作者希望推进基于运动表达的像素级视频理解。

Method: 作者构建了MeViS数据集，涵盖了2,006段视频中的8,171个对象和33,072个人工标注的运动表达（文本和语音），并在四个相关任务上基准测试了15种现有方法，还提出了LMPM++方法提升性能。

Result: 基准测试表明，现有方法在运动表达引导的视频理解任务中存在明显不足，作者提出的LMPM++在多个任务上取得了新的最佳结果。

Conclusion: MeViS数据集和LMPM++方法为基于运动表达的视频理解提供了新的平台和强有力的工具，有助于推动该领域的发展。

Abstract: This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/

</details>


### [97] [Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving](https://arxiv.org/abs/2512.10947)
*Jiawei Yang,Ziyu Chen,Yurong You,Yan Wang,Yiming Li,Yuxiao Chen,Boyi Li,Boris Ivanovic,Marco Pavone,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的场景编码器Flex，可显著加速多摄像头数据的处理效率，提升自动驾驶系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶依赖多视角、高维数据输入，处理效率低，且通常依赖强3D先验（如BEV等），限制了模型的灵活性与扩展性。作者旨在去除这些限制，提升系统效率和效果。

Method: 提出Flex编码器，使用少量可学习的scene tokens，直接从所有摄像头和时间步的图像数据联合压缩、编码场景信息，无需任何3D结构或显式先验，仅依靠数据驱动学习，且极大压缩下游模型的视觉输入。

Result: 在包含2万小时驾驶的大型数据集上，Flex实现了2.2倍的推理效率提升，同时显著优于当前最优方法的自动驾驶表现；此外，编码器还能自行分解场景而无需人工监督。

Conclusion: 无需引入3D先验的整体编码策略，可以更高效、可扩展地处理自动驾驶场景，为未来端到端驾驶系统提供了新的方向。

Abstract: We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.

</details>


### [98] [ClusIR: Towards Cluster-Guided All-in-One Image Restoration](https://arxiv.org/abs/2512.10948)
*Shengkai Hu,Jiaqi Ma,Jun Wan,Wenwen Min,Yongcheng Jing,Lefei Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出了一种新颖的图像统一修复方法ClusIR，通过可学习聚类显式建模退化类型，提升了对复杂退化的自适应修复能力，取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有All-in-One图像修复方法难以明确建模退化类型，面对复杂或混合退化时恢复表现有限，缺乏对退化语义的精细感知与自适应调节能力。

Method: 提出ClusIR框架，包含两个核心模块：概率聚类引导路由机制（PCGRM）和退化感知频域调制模块（DAFMM）。PCGRM将退化识别与专家激活分离，通过可学习聚类提升退化语义感知和路由稳健性；DAFMM利用聚类先验进行自适应频域分解和调制，针对结构与纹理分别精细修复。整体以聚类为核心连接空间和频域的修复策略。

Result: 在多个公开基准和多类型退化场景下，ClusIR展现出优异和竞争力的图像修复效果。

Conclusion: ClusIR有效增强了统一图像修复的退化语义理解和自适应能力，实现了跨多种退化类型的高质量图像恢复。

Abstract: All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.

</details>


### [99] [E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training](https://arxiv.org/abs/2512.10950)
*Qitao Zhao,Hao Tan,Qianqian Wang,Sai Bi,Kai Zhang,Kalyan Sunkavalli,Shubham Tulsiani,Hanwen Jiang*

Main category: cs.CV

TL;DR: E-RayZer是一种全新的自监督3D视觉大模型，能直接从无标签多视图图像中学习真正3D感知的表征，其性能在多个任务上明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自监督预训练已广泛应用于语言、2D图像和视频领域，但在通过多视图图像直接学习3D感知表征方面尚未深入研究。现有方法（如RayZer）主要通过隐空间合成间接推断3D信息，存在几何绑定不真实、易陷入捷径问题等局限。为实现从根本上几何驱动的3D视觉预训练，提出了本工作的必要性。

Method: E-RayZer直接在三维空间内进行自监督显式几何重建，采用独特的细粒度学习课程，实现从易到难的无监督数据组织和训练，同时融合异质数据源。这样克服了捷径问题，并能在完全无人标注下自适应提高模型能力与泛化性。

Result: 相比RayZer，E-RayZer在姿态估计等任务上表现显著更好。在一些三维重建任务上甚至可以匹敌或超越全监督方法（如VGGT）。其预训练表征在3D下游任务迁移中，也优于领先的视觉预训练方法（如DINOv3、CroCo v2、VideoMAE V2和RayZer）。

Conclusion: E-RayZer确立了三维感知视觉自监督预训练的新范式，能够以无监督方式获得高质量3D表征，为后续三维视觉任务提供卓越的基础能力。

Abstract: Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.

</details>


### [100] [Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration](https://arxiv.org/abs/2512.10954)
*Sicheng Mo,Thao Nguyen,Richard Zhang,Nick Kolkin,Siddharth Srinivasan Iyer,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型推理方法，让图像样本在生成时相互协作而非独立生成，通过“组扩散”(Group Diffusion)机制共享跨图片的注意力，大幅提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在推理/图像生成阶段通常独立对每张图片进行处理，没有利用不同样本间的潜在关联信息。作者希望挖掘协同生成潜力，提升生成质量。

Method: 提出组扩散(Group Diffusion)方法，将注意力机制从传统的“图片内部patch”为单位，拓展到“跨图片”共享。即在推理阶段，多个图片共同去噪、注意力可相互访问与学习内外图片间相关性。还引入定性指标来度量这种跨样本注意力。

Result: 发现随着协同生成的图片组规模变大，跨样本注意力越强，生成质量越高。实验证明，该方法在ImageNet 256x256数据集上，相比基线最多可提升32.2%的FID分数。还发现新引入的指标与FID紧密相关。

Conclusion: 作者首次揭示了跨样本协同推理是提升生成模型性能的有效新机制，为扩散模型推理场景带来新思路，并大幅提升了图像生成质量。

Abstract: In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.

</details>


### [101] [Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization](https://arxiv.org/abs/2512.10955)
*Tsai-Shien Chen,Aliaksandr Siarohin,Guocheng Gordon Qian,Kuan-Chieh Jackson Wang,Egor Nemchinov,Moayed Haji-Ali,Riza Alp Guler,Willi Menapace,Ivan Skorokhodov,Anil Kag,Jun-Yan Zhu,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 本文提出Omni-Attribute方法，首次实现了开放词汇的图像属性编码，能实现高保真且属性特异的表示，从而提升了视觉概念个性化的效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视觉概念方法依赖于整体嵌入，难以分离图像中的单一属性，导致信息泄露和失真。对此，需发展能独立编码特定属性的技术。

Method: 提出Omni-Attribute，创新性设计数据和模型：（1）构建带正负属性标注的语义相关图像对，明确指导编码器保留/抑制特定属性；（2）采用生成+对比双目标训练，同时确保重建质量和属性解耦。

Result: 所提属性嵌入在属性检索、个性化及组合生成任务上表现优异，多项基准测试达SOTA水平。

Conclusion: Omni-Attribute促进了开放词汇图像属性编码，为精确视觉个性化和组合生成提供了新的有效工具。

Abstract: Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.

</details>


### [102] [Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision](https://arxiv.org/abs/2512.10956)
*Wentao Zhou,Xuweiyi Chen,Vignesh Rajagopal,Jeffrey Chen,Rohan Chandra,Zezhou Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种结合立体视觉和中层视觉模块的机器人导航基础模型（StereoWalker），通过引入深度估计和像素级追踪等中层视觉先验，有效提升了机器人导航的性能，显著减少了对大规模训练数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端机器人导航基础模型（NFM）大多依赖单目视觉，假设导航所需的视觉能力能隐式涌现，但这需要大量像素到动作的监督数据，且在动态和复杂环境中效果受限，尤其是单目存在深度尺度模糊，影响空间推理。因此，探索如何有效利用更多视觉信息和显式视觉先验成为研究动力。

Method: 作者提出StereoWalker，将立体视觉输入和中层视觉模块（如深度估计、像素级追踪）引入NFM架构。同时，作者构建了大规模的立体导航数据集，利用互联网立体视频自动标注动作以支持模型训练。

Result: 实验表明：引入中层视觉先验使StereoWalker在只用1.5%训练数据时即可达到现有最优水平，且用全部数据时性能超越了SOTA；立体视觉相比单目视觉显著提升了导航表现。

Conclusion: 完全依赖单目和隐式视觉涌现的导航范式效率低下，显式引入立体视觉与中层视觉模块不仅提升导航表现，还大幅降低了数据需求。研究证实，将结构化视觉先验纳入端到端导航基础模型具有很大价值。

Abstract: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.
  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.

</details>


### [103] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SceneMaker的解耦式3D场景生成框架，通过分离去遮挡和3D生成过程，并采用新颖的姿态估计方法，有效提升了严重遮挡和开放集场景下的几何质量和姿态准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集场景下，难以同时处理高质量3D几何和精准姿态估计，主要因为缺乏足够的去遮挡和姿态估计先验。

Method: 首先将去遮挡与3D对象生成过程解耦，通过利用图像数据集和自建的去遮挡数据集丰富开放集遮挡样式；其次，提出融合全局与局部机制的统一姿态估计模型，结合自注意力和交叉注意力机制提升准确率；另外，建立了一个开放集3D场景数据集增强模型泛化性能。

Result: 大量实验证明，本文提出的解耦框架在室内和开放集场景中均优于现有方法，在几何质量和姿态准确率方面表现突出。

Conclusion: SceneMaker通过模块化的解耦设计和增强的姿态估计，大幅提升了复杂场景下的3D生成效果，为3D视觉系统在实际应用中的泛化与鲁棒性提供了有力支持。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


### [104] [WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World](https://arxiv.org/abs/2512.10958)
*Ao Liang,Lingdong Kong,Tianyi Yan,Hongsi Liu,Wesley Yang,Ziqi Huang,Wei Yin,Jialong Zuo,Yixuan Hu,Dekai Zhu,Dongyue Lu,Youquan Liu,Guangfeng Jiang,Linfeng Li,Xiangtai Li,Long Zhuo,Lai Xing Ng,Benoit R. Cottereau,Changxin Gao,Liang Pan,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了WorldLens，一个用于评估生成式世界模型（如自动驾驶等体感AI场景中用于合成4D仿真环境）的新基准，全面衡量模型在视觉真实性、几何一致性、物理可信性和功能可靠性等方面的表现。还发布了人类注释的大规模数据集（WorldLens-26K）及自动评分模型（WorldLens-Agent），以统一行业标准。


<details>
  <summary>Details</summary>
Motivation: 生成式世界模型在体感AI中的应用（如自动驾驶）发展迅速，但目前缺乏统一、全面且客观的评价标准，难以准确判断世界模型在保持几何结构、符合物理规律和支撑高质量控制等方面的能力。现有模型在不同评价维度间存在显著权衡，比如视觉真实与物理合理性难以兼顾。因此需要权威评价体系促进行业进步。

Method: 1) 构建包含生成、重建、行动跟踪、下游任务和人类偏好5大维度的基准WorldLens，分别评测视觉、几何、物理与功能。2) 人工标注2.6万段视频，形成WorldLens-26K数据集，配有人评分与理由。3) 训练解释性评分模型WorldLens-Agent，利用人工数据实现自动化、可解释的世界模型测评能力。三者共同搭建标准化测评生态。

Result: 实验表明，现有的世界模型在不同评价维度上的表现存在明显短板。例如，渲染效果好的模型容易违反物理规律，而几何结构稳定的模型则在行为一致性方面表现欠佳。WorldLens有效展现了这些性能分布，为行业发现问题和优化模型提供支持。

Conclusion: WorldLens基准、WorldLens-26K数据集和WorldLens-Agent评分体系共同为生成式世界模型提供了统一、全面、可扩展的评价标准，不仅关注“看起来是否像真的”，更强调“行为是否像真的”，有助于推动整个领域迈向更高水平的真实感和可靠性。

Abstract: Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.

</details>


### [105] [StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space](https://arxiv.org/abs/2512.10959)
*Tjark Behrens,Anton Obukhov,Bingxin Ke,Fabio Tosi,Matteo Poggi,Konrad Schindler*

Main category: cs.CV

TL;DR: 本文提出了StereoSpace，一种基于扩散模型的单目转立体合成方法，无需明确深度或变形建模，仅通过视角条件控制建模几何关系，生成高质量立体图像结果。


<details>
  <summary>Details</summary>
Motivation: 现有单目转立体方法通常依赖深度估计或图像变形，容易引入噪声、形变或视差错误。作者希望无需任何深度信息，仅通过条件控制，实现端到端、更鲁棒的立体图像生成。

Method: StereoSpace采用扩散模型，使用正则化空间和视角条件引导生成器推断左右目之间的对应关系，并自动填补视角遮挡（disocclusion）区域。测试阶段完全不使用真实或代理几何数据。作者还提出新的评价协议和关注下游相关性的新指标（iSQoE、MEt3R）。

Result: 实验表明StereoSpace在立体锐度、视差清晰度和多层/非朗伯特场景鲁棒性上明显优于warp & inpaint、latent-warping等传统类别方法，并且具备较高的下游应用性能指标。

Conclusion: 以视点条件控制的扩散模型，能够在没有深度估计的前提下，实现可扩展、高质量的立体影像生成。StereoSpace为单目转立体任务提供了新的无深度解决思路。

Abstract: We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [106] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文分析了当前基于token补全的大型语言模型（LLM）推理能力，认为它们生成的推理并非真正的溯因推理，而是对训练数据中人类推理结构的拟合。虽然表现出类似推理的输出，但缺乏真实性验证与理解。文章呼吁对LLM产出要有批判性评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM广泛被认为能进行类人推理，特别是溯因推理（基于最佳解释的推断），但实际上它们主要依赖模式生成与概率分布，是否真正具备溯因能力值得质疑，因此作者希望澄清LLM推理的本质。

Method: 文章通过理论分析及具体生成案例，比较LLM输出与真正人类推理（尤其是溯因推理）的异同，检查模型推断的真实性、语义基础和验证能力，并逐条回应常见反对意见。

Result: 结果显示，LLM虽然能模拟出看似合理的解释和推理，但其本质是基于统计相关性和训练语料库的人为推理样本，对事实真理、理解和验证没有把握，无法真正进行溯因推理。

Conclusion: LLM能辅助人类想法生成与推理模拟，但对其输出必须进行批判性解读，不能指望其提供真正的解释或真理。分析指出LLM推理的局限，并给出总体评价。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [107] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 本文探讨了小型语言模型（SLM）在自动生成高质量问题方面的潜力，并提出了一种新颖的“生成-验证”管道，用于提升问题生成的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数学术研究均依赖于大型语言模型（LLM）进行学习分析中的问题生成，但LLM资源消耗大，实际应用受限。作者希望探索资源成本较低的SLM是否也能胜任高质量问题自动生成任务，尤其是在教育等需要大规模应用场景下。

Method: 作者设计了一个“生成-验证”问题生成管道：首先利用SLM大规模自动生成候选问题，然后通过新颖的概率推理机制对这些问题进行筛选和优化。接着，通过两轮评估（专家人工评判+LLM辅助评判）来验证所生成问题的质量。

Result: 人工专家和LLM普遍认为，SLM生成的问题答案明确，且大致符合学习目标，整体质量较高，表现接近大型模型生成的水平。

Conclusion: 结果表明，小型语言模型通过合理设计的管道同样能够胜任高质量自动问题生成任务，为教育等领域的普及应用提供了低成本替代方案。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [108] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 当前大模型在垂直领域长文本生成面临“低幻觉、强逻辑、个性化表达”三难困境。针对这一问题，本文提出了DeepNews框架，通过拟合专家写作流程大幅提升事实准确性和逻辑性，并在真实媒体场景中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 长文本生成应用（如财经报道）要求输出低幻觉、高逻辑和个性表达，而现有大模型因“统计平滑陷阱”难以三者兼得。

Method: 提出DeepNews框架，包括三部分：1) 基于信息觅食理论的多粒度检索，强制高信息输入比例降低幻觉；2) 基于领域专家知识（叙事元、原子块）的结构化规划，提升逻辑框架；3) 对抗性约束提示，打破文本概率平滑性。实验通过上下文长度与内容真实性的关系验证方法有效性。

Result: 实验显示，金融领域中，当检索上下文少于15000字符时，内容真实性显著下降；若输入高冗余超过30000字，可将无幻觉率稳定在85%以上。实际盲测中，DeepNews系统在头部媒体投稿通过率达25%，优于SOTA大模型的0%。

Conclusion: DeepNews框架通过模拟专家认知过程和对抗性策略，有效突破了大模型生成财经长文时的三难困境，在实际应用中显著提升了内容质量和可用性。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [109] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段提示框架，通过从短评论中推断用户画像，并将其用于自动个性化回复生成，以提升食品外卖平台上自动回复的相关性和多样性。


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限的场景（如食品外卖平台）下，自动回复往往缺乏个性和相关性，导致用户互动效果下降。提升自动回复的个性化水平，是提升用户体验和平台服务质量的关键。

Method: 作者设计了一个两阶段提示（prompting）框架：首先直接从短评论文本中推断用户的显性（如偏好）和隐性（如风格、人口属性）画像，然后将推断的用户画像信息加入到回复生成的prompt中，结合调整解码温度生成多样且贴合的回复，无需对大语言模型进行微调。

Result: 在韩国某食品外卖App的真实世界数据集上评估，结果表明该方法能够显著提升自动回复的个性化、相关性、多样性和语义一致性。

Conclusion: 通过画像增强的提示方法，可在无需微调模型的情况下，实现更相关且个性化的自动回复生成，适用于用户信息有限的实际平台。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [110] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在微调适应新任务过程中可能出现的安全性下降，并提出将安全保护视为持续学习（CL）问题。通过系统比较不同CL方法，发现DER在保持任务效果的同时最有效地减缓了安全退化。结论适用于多个任务和模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用越来越广泛，如何在模型微调定制化的同时不丧失原有安全保障成为关键难题，尤其是在用户上传数据进行模型定制时更为突出。

Method: 作者将大语言模型微调中的安全保持问题建模为持续学习问题，借鉴并适配了文献中的正则化法、记忆法以及模型合并法等多种CL策略，分别在用户数据为正常和投毒两种情况下进行系统评估，并对比标准微调方式。

Result: 所有CL方法在安全性（攻击成功率）上均优于标准微调，其中DER方法表现尤为突出，在保证任务性能的同时最大程度保留原有安全性。结果在三个下游任务和三类模型上均得以验证。

Conclusion: 持续学习方法，尤其是DER，是在微调大语言模型过程中保护初始安全性的有效通用解决方案，适合实际服务场景应用。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [111] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 提出了AutoMedic框架，用于自动化和多维度评估医学领域大语言模型（LLMs）在多轮临床对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域LLMs应用亟需更安全、可靠的评估；现有静态问答基准难以覆盖真实动态多轮对话和多维度表现评估。

Method: 设计了AutoMedic，多智能体仿真框架，将静态QA数据转换为虚拟病人，实现LLM之间真实的多轮临床对话，并用CARE评估指标体系从多角度打分。

Result: 采用AutoMedic对多种LLM临床对话体进行评估，医疗专家验证了系统的有效性，结果显示该框架和指标能全面评价LLMs的会话能力。

Conclusion: AutoMedic为医学对话LLMs提供了标准化、自动化、多维度的评测工具，对后续开发和安全应用有指导意义。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [112] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文聚焦于将英文视觉-语言模型（VLM）适配到多语言环境，比较了多种适配方法，发现数据翻译质量是制约多语言VLM表现的关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型在英文领域取得重大进展，但面向多语言（尤其为非英文用户）仍受限。为提高技术普惠性，亟需提升VLM的多语言适配能力。

Method: 提出并比较三种不同的适配策略：基于翻译的数据管道、LoRA微调、以及分离视觉与语言适应的两阶段微调。通过标准多模态基准数据（目标语言翻译版）和母语专家手动评估来进行测试。

Result: 发现数据集翻译质量是实现多语言VLM的主要障碍，翻译数据的质量问题极大影响微调和评测的效果。

Conclusion: 未来多语言VLM的发展应更关注原生语言数据集的采集与翻译质量的提升。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [113] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了Confucius Code Agent (CCA)，一种开源的工业级AI编程智能体，并展示其在实际软件工程任务中的强大表现。该方案兼具可扩展性、可解释性与高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开源编程智能体难以应对大规模工业任务，而闭源智能体虽性能强但不具可扩展性和可控性。因此，亟需透明且高效的AI编程智能体，满足工业级需求与持续迭代。

Method: CCA基于Confucius SDK开发，SDK设计强调智能体体验（AX）、用户体验（UX）和开发者体验（DX）。通过统一编排器、分层工作记忆和持久化笔记系统保证长上下文推理与跨会话持续学习，并引入模块化工具系统支持复杂工具链协作。元智能体负责智能体配置的自动合成、测试与改进，实现快速迭代和新任务适配。

Result: CCA在SWE-Bench-Pro等实际软件工程任务上效果突出，以Resolve@1 54.3%的得分创下业界最佳成绩，显著优于现有开源方案。

Conclusion: Confucius SDK与CCA为AI编程智能体提供了透明、可扩展、可复现的基础设施，有效衔接了学术研究与工业落地的桥梁，推动AI在软件工程领域的实用化进程。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [114] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 本文提出了一种用于Transformer大语言模型（LLM）的滑动窗口注意力适配（SWAA）方法，有效解决因训练和推理过程中注意力模式不一致导致的长上下文性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的大语言模型在长文本推理中，自注意力机制的计算量随输入长度成二次增长，导致计算和性能代价高昂。滑动窗口注意力（SWA）能将复杂度降为线性，但直接在推理阶段替换原有全注意力（FA）会严重损害性能，需探索无须再训练的高效适配方法。

Method: 作者提出SWAA方法，包括：1）仅在prefill阶段应用SWA；2）保留sink token；3）交错FA与SWA层；4）引入chain-of-thought（CoT）推理方式；5）在数据上微调。并通过实验对不同方法和组合进行了系统对比和分析。

Result: 实验表明，单一适配方法无法恢复原有长上下文性能，但多种方法协同组合可有效解决性能退化。同时，对各种配置的性能与效率进行了分析并给出实际推荐。

Conclusion: 本文证实，在无需完全重新预训练模型的前提下，通过合理设计和结合多种SWAA适配方法，FA预训练的大语言模型能够高效且有效地适应SWA，提升长上下文推理性能。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [115] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的RAG框架（CoopRAG），通过让检索器和LLM互相协作，并在检索器不同层之间合作，有效提升问答任务中信息检索和推理的准确性，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG能减少大语言模型的幻觉和事实性错误，但现有的RAG方法在简单和多跳问答中依然存在不准确检索和幻觉的问题。本文旨在设计更有效的RAG架构来克服这些问题。

Method: CoopRAG框架包括以下步骤：(1) 将原问题分解为子问题和推理链，对不确定部分做掩码；(2) 基于增强后的问题检索相关文档；(3) 利用检索器早晚层之间的对比机制重新排序文档；(4) 由LLM补全推理链中的掩码部分，最终生成答案。整个过程中，检索器和LLM之间、以及检索器自身不同层之间实现协同合作。

Result: 实验结果表明，CoopRAG在三个多跳问答数据集和一个简单问答数据集上的信息检索和问答准确性均优于现有最佳方法。

Conclusion: CoopRAG通过多层次、多主体的协作机制，有效提升了RAG问答系统的检索和推理表现，为高准确性知识问答提供了新思路。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [116] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: T-pro 2.0 是一个专为俄语设计的高效推理开源大模型，配套公开了权重、数据集和评测工具，为俄语AI研究和应用提供了完整平台。


<details>
  <summary>Details</summary>
Motivation: 现有的俄语大模型在推理和推理效率上表现有限，缺乏专为俄语优化的开源资源，制约了俄语AI应用的发展。

Method: T-pro 2.0 使用了针对西里尔字母优化的分词器，并结合改进版的EAGLE speculative-decoding 推理流程，以支持直接回答与推理过程生成，显著降低推理延迟。同时发布助力研究的训练权重、指令数据集与推理评价基准。

Result: 该模型实现了高效的俄语推理能力，通过公开的web演示展示了推理与非推理模式下的速度提升，显示其高效推理机制对不同领域的适用性。

Conclusion: T-pro 2.0 作为开源的俄语大模型平台，降低了俄语AI研发门槛，为实际高效应用和学术研究均提供了重要工具和参考标准。

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [117] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: 本文提出了一种新方法（SRAP），能检测并还原伪装抄袭文献中的“扭曲短语”，提升科学文献可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前科学文献正受自动改写工具带来的抄袭威胁，这些工具通过生成统计上不常见但语法正确的“扭曲短语”掩盖抄袭，传统检测方法对新型伪装效果差且无法溯源，亟需有效检测及还原原文方法。

Method: 提出SRAP框架，分两步：（1）利用SciBERT与token级拟似困惑度进行统计异常检测；（2）通过密集向量检索与句级对齐（FAISS+SBERT），实现源文术语的语义还原。

Result: SRAP在对抗性科学文本数据集上，恢复原文术语的准确率达到23.67%，远超零样本基线的0.00%。并证明只有静态决策边界才能在术语多变的科学文本中保持检测鲁棒性。

Conclusion: SRAP不仅有效检测伪装抄袭，还能溯源原始文献，有助于提升科学文献的取证和可信度。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [118] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 本文将知识图谱（KGs）整合进大型语言模型（LLMs），提升其在知识密集型任务中的表现，并增强其事实一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在自然语言处理任务中表现出色，但因缺乏结构化知识，时常出现事实不一致的问题。

Method: 作者将KG-BERT等知识图谱相关技术与现有LLMs结合，使模型能够利用结构化知识进行推理和知识支持。

Result: 实验证明，融合KG后的模型在问答、实体链接等知识密集型任务上取得了显著提升。

Conclusion: 该方法提升了LLMs的事实可靠性，有助于实现更加可依赖、上下文感知的下一代语言模型。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [119] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 本文提出了一种心理感知型对话代理，结合多种人工智能技术，以同步提升学生学业表现与情绪健康。在大学生实验中，系统能有效提升动机、降低压力，并取得中等程度的学业提升。


<details>
  <summary>Details</summary>
Motivation: 现有教育类对话机器人要么只关注学业辅导、要么只关注情感支持，缺乏能够同时整合学习与情绪管理的系统。因此论文旨在开发一个能实时分析学生认知及情感状态，为学生提供更加个性化、以学生为中心的教育干预工具。

Method: 本研究构建了融合大型语言模型（LLMs）、知识图谱增强的KG-BERT，以及带注意力机制的双向长短期记忆网络（LSTM）的系统，实时分类学生认知与情感状态。系统多模态输入，涵盖文本语义、语音韵律特征和行为时序信息，用于推断学生的参与度、压力和概念掌握情况。

Result: 大学生初步试验表明，该系统在提升学生学习动机、降低压力及学业成绩等方面优于传统基线方法，取得了积极成果。

Conclusion: 多模态融合与时序建模手段能更好地实现语义推理和情境识别，有望推动适应性、以学生为主的教育干预和支持。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [120] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 本研究测试了大语言模型（LLM）是否能区分语法结构相关的对比，并由此推断模型是否具有内部结构性语言知识。通过让模型对主语-助动词倒装和寄生空缺进行可接受性判断，发现GPT-4、LLaMA-3等模型能可靠区分语法正确和错误的句子——表明其不仅仅关注线性顺序，而对结构具有敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统生成语法学认为，语法结构的内部层次性通过语法性对比得以体现。然而，目前大语言模型仅基于表层文本训练，是否能体现类似的结构性知识仍是一个未解问题。本研究希望通过实证分析，检验LLM是否表现出结构性敏感。

Method: 作者设计了主语-助动词倒装和寄生空缺两类经典语法结构测试，采用提示词让LLM做句子可接受性判断，并对比语法正确与错误句型的区分能力，使用的模型包括GPT-4、LLaMA-3等。

Result: LLM能够稳定地区分两类结构中的语法正确与错误句型，并展现对句法结构而不仅仅是线性顺序的敏感。

Conclusion: LLM在只接受表层训练的情况下依然展示出对句法结构的泛化能力，说明经过预测训练后模型能够学到句法结构的功能相关性，无需明确的符号编码。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [121] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 本文关注大型语言模型在中低资源语言上的表现不足，提出通过优化语言分布和数据再加权方法提升多语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要受高资源语言（如英语）主导，导致模型在中低资源语言上的性能不佳，需要通过合理的数据分布优化改进多语言表现。

Method: 作者提出通过训练域重加权算法DoGE的多语言扩展XDoGE，调节训练语料中不同语言的分布权重。具体步骤包括：1）利用小模型和XDoGE对语言分布权重进行优化，2）依照优化结果重新采样数据，使用完整大模型从头训练或持续预训练。

Result: 以六种语言（高/中/低资源）为对象，实验证明合理增大低资源语言数据重复率、降低高资源语言采样，有助于提升整体多语言性能。通过IberoBench量化评测，显著提升了小语种效果。

Conclusion: 提出的XDoGE语言权重优化方法有效缓解了高资源语言主导问题，并在多种伊比利亚语言及英语上的性能获得提升，还发布了新模型IberianLLM-7B-Instruct为相关研究提供资源。

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [122] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文探讨了当下流行的大语言模型在因果推理任务中的局限性，发现仅依赖ICL难以实现稳健的因果推理，特定架构经过微调后性能更优。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在诸多任务取得进展，但其在因果推理上的表现尚不明朗。因果推理需要多步推断和严格的逻辑控制，仅靠输入中表层词汇关系容易误导模型判断，因此作者希望明确不同模型架构在此类任务中的表现差异。

Method: 作者比较了encod er、encoder-decoder和decoder-only三种大模型架构，分别在零样本和少样本ICL设定下，以及自然语言和非自然语言数据上，对这些模型进行微调，并测试其在因果推理任务（多跳、结合性强的推理）上的表现。

Result: 实验发现，单靠ICL难以实现鲁棒的因果推理，模型常关注无关特征。decoder-only模型对分布变化敏感，表现不稳定。相较之下，经过微调的encoder和encoder-decoder架构在各测试场景下展现出更强泛化能力，尤其在非自然语言环境下表现更优。只有在模型规模极大时，decoder-only架构才与前两者相当甚至超越。

Conclusion: 对于成本敏感、推理链较短并要求鲁棒性的因果推理任务，经过定向微调的encoder或encoder-decoder架构更为适用，优于常用的decoder-only架构。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [123] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文提出了RoleRMBench，这是首个针对角色扮演对话中奖励建模的系统性基准，发现当前通用奖励模型与人类判断存在较大差距，并提出新的RoleRM模型有效提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模方法在主观性强且开放性的领域（如角色扮演）存在表现劣化，难以捕捉细致且依赖人设的人类偏好，本研究旨在解决这一痛点。

Method: 1. 构建了RoleRMBench基准，涵盖叙事管理、角色一致性、参与度等七个细分能力；2. 提出了RoleRM奖励模型，通过连续隐式偏好（CIP）训练，用多种结构化策略将主观性评价转化为持续一致的成对监督信号。

Result: 实验显示RoleRM较主流的开源或闭源奖励模型平均提升24%以上，特别在叙事连贯性和风格贴合度上表现突出。

Conclusion: 连续偏好表达和标注一致性对提升主观性对话系统的奖励建模至关重要，RoleRMBench及RoleRM为人性化对话系统的主观对齐奠定了基础。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [124] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-Omni是一个集成语音、视觉和文本的农业多模态大模型框架，提出了大规模跨语言数据集、统一模型结构和三模态评测基准，并显著提升农业领域的多语言理解表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽然进展迅速，但农业领域因缺乏多语言语音数据、统一架构和完善评测，应用受限。论文旨在解决这些瓶颈，推动多模态智能农业发展。

Method: 1) 构建了可扩展的数据合成与采集流程，将文本和图像转化为训练用语音数据，包含六种语言的大规模合成与真实语音；2) 采用三阶段范式训练多模态农业模型，包括文本知识注入、多模态对齐及基于GRPO的强化学习，实现多语言多模态统一推理；3) 构建首个农业三模态评测基准AgriBench-Omni-2K，覆盖多任务和多语言，配备标准协议与复现工具。

Result: AgriGPT-Omni在多语言和多模态推理能力、真实语音理解等方面，显著优于通用基线方法。

Conclusion: 该研究为农业多模态人工智能开发奠定基础，推动了复现性研究、包容性农业智能和可持续AI在低资源区域的应用。所有模型、数据和工具将开放共享以促进社区进步。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [125] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 本文以塞尔维亚语为例，分析了低资源语言在大语言模型中面临的结构性、历史性和社会技术性问题，并提出了以CARE原则为基础的数据关怀（Data Care）框架，旨在从源头改进语料设计和治理，促进文化包容和可持续的语言技术发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型主要基于英语等主流语言训练，导致对低资源语言的支持存在严重偏见。本文旨在剖析这些偏见背后的原因，并为如何更公平、细致地发展低资源语言的AI技术提供理论和方法支持。

Method: 通过对10位学者与实践者（语言学家、数字人文学者和AI开发者）的半结构化访谈，梳理了影响塞尔维亚语语言技术开发的历史性损失、工程优先路径以及数据和语料的局限性。

Result: 发现包括文本遗产毁损、过度依赖英语模型、浅层转写、数据偏见和忽视文化语境的语料整理等多重问题阻碍了低资源语言的公平发展。

Conclusion: 提出以CARE（集体利益、控制权、责任、伦理）为指导的数据关怀（Data Care）框架，将偏见缓解整合进数据收集、注释与治理全过程，主张该框架可为构建具包容性和文化根基的低资源语言技术提供可复制模式。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [126] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出了一个全面的数据偏见检测与缓解流程，并在性别、宗教和年龄等敏感属性上验证了其有效性，但发现数据去偏未必能稳定改善模型偏见。


<details>
  <summary>Details</summary>
Motivation: 文本数据存在多元化的偏见（如有害语言、人口分布失衡），而欧盟AI法等法规要求识别并缓解针对受保护群体的数据偏见，以防止模型输出不公，然而目前缺乏切实可行的操作指南。

Method: 设计了包含四个环节的数据偏见检测与缓解流程，针对代表性偏见和显性刻板印象两个类型，步骤包括：1）利用LLM生成的高质量群体标签词表；2）用Demographic Representation Score量化代表性偏见；3）通过社会语言学过滤检测与缓解刻板印象；4）用语法和语境感知的反事实数据增强补偿代表性偏见。实验上分别评估流程中各环节对性别、宗教和年龄的去偏效果，并对基于去偏数据微调的大模型进行偏见基准测试。

Result: 流程各环节能有效减少文本数据中的代表性偏见和显性刻板印象，但基于去偏数据微调的LLM在偏见基准测试中未必表现更优，揭示了当前评价方法的局限。

Conclusion: 数据层面的去偏措施虽然在数据本身上有效，但不一定稳定转化为模型层面的公平性提升，需进一步研究更有针对性的数据操作及更科学的偏见评价体系。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [127] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出一种新型的结果-过程联合验证器（OPV），有效提升大语言模型复杂推理链条中中间步骤和最终结果的自动化验证能力，并大幅降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 尽管现有结果型验证器（OV）和过程型验证器（PV）对大模型推理有提升，但分别难以细致检查中间推理错误或高质量标注成本过高。因此需要更高效可靠的新方法兼顾准确性和成本。

Method: 提出OPV，整合了结果和过程联合验证优势。采用专家标注驱动的主动学习框架，每轮挑选最不确定案例进行人类标注，通过拒绝微调（RFT）和RLVR逐步提升验证器性能，极大缓解人工负担。

Result: OPV在自建验证基准上取得F1 83.1，明显优于Qwen3-Max-Preview等更大模型。还能高效检测合成数据中的假阳性，与专家评估高度一致，并推动下游策略模型显著提升准确率（最高从55.2%提升到73.3%）。

Conclusion: OPV能够以更低成本提供高效精准的推理验证支撑，广泛适用于大模型推理链数据验证场景，未来可进一步缩小人工标注需求。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [128] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: 论文提出TRIDENT系统，针对加勒比地区英语方言，提升应急语音识别和调度效率，弥补现有系统在此类口音下的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有应急语音识别系统难以处理加勒比地区等非标准英语口音，导致服务效率下降、部分人群获取帮助受限，因而亟需针对方言场景的系统解决方案。

Method: TRIDENT为三层调度支持架构，集成加勒比口音定制的ASR、大语言模型下的本地实体抽取，以及生物声学应激识别，为调度员提供文字转写置信度、临床结构化实体识别和声音应激指标三类信号，并以低ASR置信度和高声学应激为危急优先信号，通过多层信息提升紧急呼叫的准确分流。

Result: 本文主要为系统架构和理论分析，未包含实际加勒比紧急呼叫的实证验证，实地效果验证为未来工作方向。

Conclusion: TRIDENT提供了一个可扩展、对方言鲁棒的应急AI调度框架，使加勒比地区用户更公平地接受既有国家分诊协议，有助于提升灾害场景下的应急响应普适性与可靠性。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [129] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新型验证器OPV（Outcome-based Process Verifier），能够高效且准确地验证大型语言模型（LLM）在复杂推理任务下的中间推理步骤，并显著提升自动化审核与标注质量。实验结果显示OPV表现超越现有大型开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的验证器（OVs）难以识别长链推理中的中间错误，而基于过程的验证器（PVs）由于高质量人工标注稀缺，检测复杂推理链中错误的能力有限。因此迫切需要一种能够高效、低成本、准确验证LLM复杂推理过程的新方法。

Method: 作者提出OPV，通过对长推理链的结果进行摘要后验证其推理过程，结合专家迭代主动学习、拒绝微调（RFT）以及RLVR大幅降低标注成本，并以此不断优化验证器性能。每轮迭代中，挑选当前模型最不确定的案例，通过专家标注持续提升验证器能力。

Result: OPV在Authors自建基准OPV-Bench上F1得分达83.1，显著优于Qwen3-Max-Preview（76.3）；在合成数据上有效识别假阳性，与专家评估高度一致；与决策模型协同，可将DeepSeek-R1-Distill-Qwen-32B在AIME2025准确率从55.2%提升至73.3%。

Conclusion: OPV能在降低标注成本的同时大幅提升大模型自动审核准确率，推进大规模高质量推理标注与评测，部分突破了复杂推理自动验证的难题，对提升大模型自动推理可靠性具有重要意义。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [130] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 本文探讨了通过放大预训练模型的规模（scaling），以更高效地适应中低资源目标语言，并且分析了其与传统持续预训练方法的优劣对比。结果表明扩大的模型在数据效率和保持原有能力方面具备优势。还初步探讨了多语言模块化模型合并的可行性和挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多语种大模型在中、低资源语言上性能仍不及专用单语模型，尤其是在模型规模较小时。如何高效将已训练的高资源语言模型适配到低资源目标语言成为挑战。本研究旨在寻找更高效的数据利用和模型迁移方法，解决多语种一致表现和灾难性遗忘问题。

Method: 作者采用大约FLOP匹配的方法，对多种规模的英语基线模型进行缩放实验。对比扩大的英语模型在适应目标语言时，与标准持续预训练方法（直接以新语言数据继续预训练）的表现差异。同时分析了模型能力保持及多语言模型分模块合并（merge）的效果。

Result: 研究发现，经过足够目标语言数据训练后，放大后的基础模型在目标语言上的表现能匹配甚至超过用大量数据持续预训练的小模型。大模型能更好保留英语能力，减少灾难性遗忘。多语种模块化模型合并虽然仍不及联合训练，但大模型合并效果要优于小模型。不同合并方法表现差距大，说明优化空间巨大。

Conclusion: 将基线模型放大是一种高效的数据迁移与模型适配方案，尤其适用于需要平衡多语言、数据量有限的场景。模块化多语言模型合并尚处于初步阶段，未来可通过优化合并方法进一步提升效果。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [131] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型（LLM）在印度重要医疗领域对印度语言罗马化输入的处理表现。实证显示，罗马化输入使得LLM的性能比本地脚本表现下降5-12分，产生重大安全隐患。


<details>
  <summary>Details</summary>
Motivation: 在印度，许多临床场景下用户习惯使用罗马字母而非本地脚本交流。然而，大多数已有研究未充分关注这一实际输入形式对LLM性能的影响，尤其在医疗等高风险应用中。

Method: 作者构建了一个涵盖5种印度语言和尼泊尔语的真实用户健康查询数据集，对比评估了主流LLM对原生脚本和罗马化输入的表现（以F1分为指标），并分析了背后原因。

Result: 结果显示，LLM对罗马化输入的性能（F1分）普遍低于原生脚本5-12分。在印度最大的合作医疗机构，该差距预计将导致近200万个错误分诊。分析发现，这一表现差距并非因临床推理失误，而是罗马化输入带来的正字法噪声影响LLM输出。

Conclusion: LLM在处理罗马化印度语言的表现存在重大安全盲区：即使模型能理解这些输入语义，实际决策输出依然脆弱，亟需改进用于罗马化输入的算法与实际部署策略。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [132] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 本文提出了FACTS Leaderboard，一个全面评估语言模型事实性的新基准套件，包括多项评分子榜，通过统一指标衡量模型生成内容的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成文本时，事实准确性是关键问题，但缺乏统一、全面而细致的评测基准。本工作旨在填补这一空白，对不同场景下模型的事实能力做出系统性评估。

Method: FACTS Leaderboard由四个子榜单组成：（1）多模态事实榜，评估图像问答事实性；（2）参数知识榜，评估模型基于参数回答事实问题的能力；（3）检索事实榜，模型需利用检索API回答信息型查询；（4）内容依据榜，评估长文本回答是否基于给定文档，并提升了判别模型的可靠性。每项评分均依赖自动化判分系统，最终得分为四项平均。

Result: 该套件成功实现了对语言模型不同事实生成情境的多维覆盖，并建立了在线Leaderboard，方便外部参与及持续维护。

Conclusion: FACTS Leaderboard为评估语言模型事实性提供了统一、健全且可扩展的方案，可推动语言模型在事实性方向的提升和标准化评测。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [133] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一种结合传统transformer分类器与大型语言模型（LLM）的文本分类融合方法，能提升多类别、多标签任务的准确率并兼顾成本与效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的流行，研究如何利用它们与传统模型（如RoBERTa）互补提升文本分类效果，同时平衡算力和推断成本，是业界痛点。现有方法往往要么利用强化的transformer，要么独用LLM，但二者强项各异。

Method: LabelFusion创新性地将传统transformer（如RoBERTa）的embedding与多个LLM（如GPT、Gemini、DeepSeek）通过结构化prompt工程得到的每类置信度分数拼接为联合特征，再经一个多层感知器（FusionMLP）学习最终预测。全流程可用通用接口训练，也支持灵活定制。

Result: LabelFusion在主流基准如AG News上达92.4%准确率，在Reuters 21578十分类上达92.3%。泛化性好，且支持不同模型组合下精度、延迟与推断成本的灵活权衡。

Conclusion: 通过融合transformer和LLM的推理优势，LabelFusion能在多领域实现高准确率且高效的文本分类，是兼顾实用性和前沿性的解决方案。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [134] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 本研究利用计算文本分析方法探索《霍比特人》对话的情感基调，发现其整体基调积极平和且主导感随情节推进提升。


<details>
  <summary>Details</summary>
Motivation: 文学作品的情感节奏塑造读者体验，但其细微情绪结构难以量化。作者希望借助计算工具揭示《霍比特人》情感节奏的细致变化，从而为数字人文与文学批评提供方法论借鉴。

Method: 首先利用正则表达式从文本中提取出所有对话内容，然后进行数据预处理，借助NRC-VAD情感词典对每个对话语句在愉悦度（valence）、激活度（arousal）、主导性（dominance）三个维度进行量化评分，通过可视化方法（如情感轨迹图、词云）展现情绪变化。

Result: 分析结果显示，对话整体以高愉悦度和低激活度为主，呈现积极和冷静的情感色彩，主导性感随故事推进逐渐增强。情节中的危险与激动时刻常被幽默、友情和释然所调和，展现出情绪张弛有度的节奏。

Conclusion: 通过结合计算分析与文学诠释，本研究揭示了《霍比特人》对话中的微观情感结构，说明数字人文方法可以帮助发现文学作品中节奏感与情绪调节的细腻机制。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [135] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 本文评估了多模态大型语言模型（mLLMs）在分析视频中情绪唤起方面的有效性。结果显示在理想条件下表现良好，但在现实政治语境中表现有限。


<details>
  <summary>Details</summary>
Motivation: 越来越多研究利用多模态方法（如视听材料）分析情绪在政治传播中的作用，同时生成式多模态AI快速发展，但该领域还缺乏对其情绪分析有效性的系统评估。

Method: 作者采用两个人工标注的视频数据集，评估当前多模态大型语言模型在视频中情绪唤起分析任务中的表现，比较其准确性及人口统计偏见。

Result: 在理想测试环境下，mLLMs对情绪唤起的评分高度可靠，并几乎无明显的人口统计偏见；但在实际议会辩论视频中，模型性能明显下降，可能影响后续统计推断。

Conclusion: 当前多模态AI在情绪分析方面理想条件下有潜力，但现实复杂场景下效果有限。建议持续严格评估生成式AI方法并提出可复用的分析框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [136] [Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.10071)
*Junjie Bai,Yu-Wei Chao,Qizhi Chen,Jinwei Gu,Moo Jin Kim,Zhaoshuo Li,Xuan Li,Tsung-Yi Lin,Ming-Yu Liu,Nic Ma,Kaichun Mo,Delin Qu,Shangkun Sun,Hongchi Xia,Fangyin Wei,Xiaohui Zeng*

Main category: cs.RO

TL;DR: 本文介绍了第二名团队在2025 BEHAVIOR Challenge中的解决方案，该挑战关注现实中的长期任务。作者通过系统性实验展示了预训练和后训练对性能的提升，并总结了对业界有价值的经验。


<details>
  <summary>Details</summary>
Motivation: 推动机器人在现实家庭环境中完成长时序任务的研究进展，弥合现有研究与实际人类需求之间的差距。

Method: 在基于π_{0.5}模型的基础上，系统化分析和实验不同训练技巧及数据规模，通过消融实验检验预训练和后训练对模型性能的影响。

Result: 所提方案在挑战赛中获得了非常接近第一名的第二名，并显著超越其他参赛队伍。预训练和后训练阶段的合理扩展能显著提升机器人任务完成能力。

Conclusion: 总结了有效训练技巧和数据选择对复杂体态AI任务的重要意义，为将来类似场景中的模型适应性提供了有操作性的建议和经验。

Abstract: The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.

</details>


### [137] [Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation](https://arxiv.org/abs/2512.10099)
*Steven Caro,Stephen L. Smith*

Main category: cs.RO

TL;DR: 本文提出了一种层次化强化学习-扩散模型策略（HeRD），用于解决复杂环境下的非抓取操作问题（如推物体），在二维模拟环境中取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作（如推物体）在复杂、杂乱环境中的控制难点在于接触动力学复杂和需要长时规划，这一直是机器人控制中的挑战。当前方法在路径效率、任务成功率和泛化能力上仍有提升空间。

Method: 文章提出HeRD架构，将推物体任务分为两个层次：高层使用强化学习智能体选择中间目标点，低层用条件扩散模型生成到达这些中间目标的优化轨迹。这样结合了RL在长期奖励优化方面的能力和扩散模型在复杂轨迹生成上的能力。

Result: 在二维模拟环境下，HeRD在任务成功率、路径效率和跨环境泛化能力等指标上，相比现有最先进方法取得了更好表现。

Conclusion: 采用层次化控制和生成式低层轨迹规划，是实现可扩展、目标导向型非抓取操作的有前景方向。

Abstract: Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.

</details>


### [138] [Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks](https://arxiv.org/abs/2512.10116)
*Andrew Razjigaev,Hans Lohr,Alejandro Vargas-Uscategui,Peter King,Tirthankar Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的基于任务空间分解、阻尼最小二乘法和Halley法的功能冗余逆解算法，用于优化六轴机器人在冷喷涂非平面表面上的运动规划，实现更快且鲁棒的在线运动规划，减少关节运动，提高操作空间，并通过工业ABB机械臂实际验证。


<details>
  <summary>Details</summary>
Motivation: 六轴机器人在焊接、增材制造等制造任务中常因为工具轴对称存在功能冗余（实为五轴任务），如果有效利用该冗余，可以显著优化工具路径规划，但目前主流却仍依赖计算量大的离线规划方法，逆解算法虽快却未被充分利用。

Method: 提出一种融合任务空间分解、阻尼最小二乘法与Halley法的功能冗余逆解算法，用以快速、鲁棒地求解运动学逆解，且关节运动最小。同时，将该方法应用于冷喷涂复杂非平面表面路径优化。

Result: 实验结果显示，该算法可快速求取减少关节运动的最优运动轨迹，扩大了机器人复杂路径操作的可行工作空间。算法在ABB工业机器人和冷喷涂枪实际案例中实现了验证。

Conclusion: 本文方法能有效解决功能冗余下复杂轨迹的运动学逆解问题，兼具速度、鲁棒性和运动优化，适于实际工业应用，可提升复杂制造工艺的自动化水平。

Abstract: Industrial automation with six-axis robotic arms is critical for many manufacturing tasks, including welding and additive manufacturing applications; however, many of these operations are functionally redundant due to the symmetrical tool axis, which effectively makes the operation a five-axis task. Exploiting this redundancy is crucial for achieving the desired workspace and dexterity required for the feasibility and optimisation of toolpath planning. Inverse kinematics algorithms can solve this in a fast, reactive framework, but these techniques are underutilised over the more computationally expensive offline planning methods. We propose a novel algorithm to solve functionally redundant inverse kinematics for robotic manipulation utilising a task space decomposition approach, the damped least-squares method and Halley's method to achieve fast and robust solutions with reduced joint motion. We evaluate our methodology in the case of toolpath optimisation in a cold spray coating application on a non-planar surface. The functionally redundant inverse kinematics algorithm can quickly solve motion plans that minimise joint motion, expanding the feasible operating space of the complex toolpath. We validate our approach on an industrial ABB manipulator and cold-spray gun executing the computed toolpath.

</details>


### [139] [Inertial Magnetic SLAM Systems Using Low-Cost Sensors](https://arxiv.org/abs/2512.10128)
*Chuan Huang,Gustaf Hendeby,Isaac Skog*

Main category: cs.RO

TL;DR: 本文提出了两种使用惯性测量单元、磁力计阵列和气压计的非视觉IM-SLAM系统，并通过实验验证了紧耦合系统通常能获得更低的定位误差，显示了低成本传感器实现三维磁场SLAM的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有磁场SLAM方法往往依赖于视觉里程计或轮速计等低漂移里程计数据，但这些数据在低能见度或极端环境下难以获取，限制了系统的适用范围。本文希望开发基于非视觉、低成本传感器的SLAM方案，提高其在特殊环境下的鲁棒性。

Method: 作者提出了松耦合与紧耦合两种惯性-磁场融合SLAM系统，均采用IMU、磁力计阵列和气压计。两者均通过状态空间建模，区别在于磁场局部与全局模型的耦合方式。松耦合系统将二者分别建模；紧耦合系统在统一状态空间内综合二者信息。

Result: 实验显示，在多数测试场景下，紧耦合IM-SLAM系统显著优于松耦合系统，定位误差一般在每行进一百米误差为数米的量级。

Conclusion: 低成本非视觉传感器可实现高性能的三维磁场SLAM，可为极端环境（如矿井、火灾救援）中的定位与地图构建提供有效技术方案。

Abstract: Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning. Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly. Moreover, they have bounded error within mapped regions. However, state-of-the-art methods typically require low-drift odometry data provided by visual odometry or a wheel encoder, etc. This is because these systems need to minimize/reduce positioning errors while exploring, which happens when they are in unmapped regions. To address these limitations, this work proposes a loosely coupled and a tightly coupled inertial magnetic SLAM (IM-SLAM) system. The proposed systems use commonly available low-cost sensors: an inertial measurement unit (IMU), a magnetometer array, and a barometer. The use of non-visual data provides a significant advantage over visual-based systems, making it robust to low-visibility conditions. Both systems employ state-space representations, and magnetic field models on different scales. The difference lies in how they use a local and global magnetic field model. The loosely coupled system uses these models separately in two state-space models, while the tightly coupled system integrates them into one state-space model. Experiment results show that the tightly coupled IM-SLAM system achieves lower positioning errors than the loosely coupled system in most scenarios, with typical errors on the order of meters per 100 meters traveled. These results demonstrate the feasiblity of developing a full 3D IM-SLAM systems using low-cost sensors and the potential of applying these systems in emergency response scenarios such as mine/fire rescue.

</details>


### [140] [Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine](https://arxiv.org/abs/2512.10235)
*Hui Li,Akhlak Uz Zaman,Fujian Yan,Hongsheng He*

Main category: cs.RO

TL;DR: 本文提出了一种结合上下文奖励机制的强化学习框架，用于任务导向型抓取，实现了高效、精确的抓取任务分解与学习。


<details>
  <summary>Details</summary>
Motivation: 抓取任务复杂且多变，现有强化学习方法在学习速度和数据效率上仍有提升空间。本文希望通过对任务进行分解、引入上下文信息和奖励机制，提升学习效率和成功率。

Method: 利用Contextual Reward Machine将复杂抓取任务分解为多个子任务，每个阶段定义特定的奖励函数、动作空间和状态抽象，辅助学习。在阶段切换时通过奖励或惩罚引导合理的任务序列，并结合PPO算法进行训练和优化。

Result: 在1000次模拟抓取任务中达到95%的成功率，学习速度和成功率均超越最优方法，并在真实机器人上进行迁移测试，成功率为83.3%。

Conclusion: 该方法显著提升了任务导向抓取的准确率、数据利用率和学习效率，展现了在模拟和现实场景中的应用潜力。

Abstract: This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

</details>


### [141] [Lies We Can Trust: Quantifying Action Uncertainty with Inaccurate Stochastic Dynamics through Conformalized Nonholonomic Lie Groups](https://arxiv.org/abs/2512.10294)
*Luís Marques,Maani Ghaffari,Dmitry Berenson*

Main category: cs.RO

TL;DR: 该论文提出了CLAPS方法，可在不依赖对系统动力学和误差分布强假设的前提下，为机器人的动作预测结果提供概率性覆盖保证；通过引入李群理论，适用于非欧几里得空间（如SE(2)）上的机器人系统，并提升了预测集的效率和不确定性刻画能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人动作结果的不确定性量化依赖于对误差分布、模型等的强假设，且未能充分考虑系统的对称结构与配置空间特性，而这对实现安全控制至关重要。现有无分布假设的共形预测理论主要服务于欧氏空间，不适用于本质配置为李群的机器人。

Method: 本方法基于共形预测理论，结合李群上的系统配置和对称性，提出了一种新的非一致性评分方法。该方法扩展了理论上的概率覆盖保证至SE(2)等非欧几里得空间，能量化和表征现实机器人动作的不确定性。

Result: 在JetBot仿真平台和真实MBot机器人上的实验表明，CLAPS方法相比传统欧氏空间共形方法，生成的预测集体积更小，能够更有效地代表实际不确定性。

Conclusion: CLAPS方法能够在更宽松假设下，为非欧配置空间的机器人提供高效、概率保证的动作预测集，对机器人安全控制具有理论与实际意义。

Abstract: We propose Conformal Lie-group Action Prediction Sets (CLAPS), a symmetry-aware conformal prediction-based algorithm that constructs, for a given action, a set guaranteed to contain the resulting system configuration at a user-defined probability. Our assurance holds under both aleatoric and epistemic uncertainty, non-asymptotically, and does not require strong assumptions about the true system dynamics, the uncertainty sources, or the quality of the approximate dynamics model. Typically, uncertainty quantification is tackled by making strong assumptions about the error distribution or magnitude, or by relying on uncalibrated uncertainty estimates - i.e., with no link to frequentist probabilities - which are insufficient for safe control. Recently, conformal prediction has emerged as a statistical framework capable of providing distribution-free probabilistic guarantees on test-time prediction accuracy. While current conformal methods treat robots as Euclidean points, many systems have non-Euclidean configurations, e.g., some mobile robots have SE(2). In this work, we rigorously analyze configuration errors using Lie groups, extending previous Euclidean Space theoretical guarantees to SE(2). Our experiments on a simulated JetBot, and on a real MBot, suggest that by considering the configuration space's structure, our symmetry-informed nonconformity score leads to more volume-efficient prediction regions which represent the underlying uncertainty better than existing approaches.

</details>


### [142] [Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot](https://arxiv.org/abs/2512.10319)
*Muhammad Usama,Muhammad Ibrahim Khan,Ahmad Hasan,Muhammad Shaaf Nadeem,Khawaja Fahad Iqbal,Jawad Aslam,Mian Ashfaq Ali,Asad Nisar Awan*

Main category: cs.RO

TL;DR: 本文提出了一种自主激光除草机器人，通过低能激光精准去除农田杂草，兼具速度、准确性与农田适应能力，有助于提升精准农业的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 传统农田除草多依赖机械手段或化学除草剂，前者效率低，后者破坏土壤生态，因此亟需更高效、环保的替代方案。激光除草在精准农业中具有更可持续性。

Method: 设计了一款具六轮和双四连杆悬挂系统的自主移动机器人，采用3D线性驱动机构精确引导激光扫除检测到的杂草，并在田野环境中进行实地测试。

Result: 机器人能有效越障（最高15cm），在最佳运行速度下，杂草检测率为86.2%，每米作业时间87秒。激光定位平均误差仅1.54毫米，命中率高达97%。

Conclusion: 所提机器人在速度、准确性和适应性方面表现优异，展示了提升精准农业实践的巨大潜力，有助于实现可持续、环保的农田管理。

Abstract: Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.

</details>


### [143] [Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing](https://arxiv.org/abs/2512.10349)
*Quan Yuan,Zhenting Du,Daqian Cao,Weibang Bai*

Main category: cs.RO

TL;DR: 本文提出一种新型腱驱动欠驱动机械手指，通过创新的同步腱布线方式，实现了用单个执行器驱动所有指关节，提升了紧凑性、刚度及自适应能力，并在五指机器人手上验证了良好的抓取性能和可预测的结构刚度。


<details>
  <summary>Details</summary>
Motivation: 现有腱驱动欠驱动手指在减少驱动器数量和简化结构方面有优势，但难以兼顾高负载能力和自适应顺应性，尤其在结构紧凑性要求较高时问题突出，因此需要新的设计实现两者平衡。

Method: 提出了一种各关节角速度固定比例耦合的同步腱布线，使得一根腱和单个执行器即可驱动全部关节。推导了包含腱弹性的运动学和静态模型，并制作了单指原型进行静载实验，测量指尖负载下的挠度和刚度。将该手指集成到五指机械手，测试多场景物体操作能力。

Result: 实验表明单指模型在静载下的挠度预测误差为1.0 mm，占总指长0.322%，在3kg负载下测得刚度为1.2x10^3 N/m。组装成五指机械手后，在各种实际抓取中表现出可靠的刚度和抓握性能，且仍保持极简的执行器数量。

Conclusion: 所提基于同步腱布线方式的欠驱动机械手指结构在保证紧凑和轻量化的前提下，实现了高顺应性与高刚度的兼顾，为多指机器人手设计提供了切实可行的方案。

Abstract: Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.

</details>


### [144] [CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation](https://arxiv.org/abs/2512.10360)
*Liuyi Wang,Zongtao He,Jinlong Li,Xiaoyan Qi,Mengxian Hu,Chenpeng Yao,Chengju Liu,Qijun Chen*

Main category: cs.RO

TL;DR: 提出CLASH框架，结合小模型规划与大模型推理，提升视觉-语言导航任务（VLN-CE）的性能，并在仿真及真实环境中取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 大模型虽然推理能力强，但在视觉-语言导航任务中往往不如专用小模型。目标是融合两者优势，提升导航性能和泛化性。

Method: 提出CLASH框架，由反应式小模型规划器（RSMP）和反思式大模型推理器（RLMR）组成。RSMP采用因果学习双分支架构，大模型通过链式推理与全景视觉提示辅助空间理解。引入不确定性感知融合机制自适应整合两者决策。拓展仿真和现实环境下的障碍避让与路径规划能力。

Result: CLASH在VLN-CE榜单测试未知集上取得第一，SR与SPL指标大幅领先此前SOTA方法。真实环境实验验证其鲁棒性和有效性。

Conclusion: CLASH有效结合大、小模型能力，在视觉-语言导航任务中实现了优异、领先的效果，并兼顾仿真与实际应用。

Abstract: Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.

</details>


### [145] [RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI](https://arxiv.org/abs/2512.10394)
*Weifan Guan,Huasen Xi,Chenxiao Zhang,Aosheng Li,Qinghao Hu,Jian Cheng*

Main category: cs.RO

TL;DR: RoboNeuron是一种新型通用部署框架，将大语言模型（LLM）、视觉-语言-动作（VLA）模型与ROS深度结合，通过MCP协议实现高适应性与模块解耦。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能系统在跨场景适应性差、模块间耦合紧密、推理加速分散等方面存在工程障碍，急需新的架构提升灵活性和开发效率。

Method: 提出RoboNeuron框架，首次将LLM和VLA模型与ROS实时执行系统无缝集成，利用Model Context Protocol（MCP）语义桥接，实现LLM动态调度机器人工具。框架通过ROS通信接口实现传感、推理、控制高度解耦，并引入自动化工具将ROS消息翻译成MCP函数，简化开发流程。

Result: RoboNeuron提升了跨场景的适应性和组件灵活性，大幅简化开发流程，并实现横向性能基准测试的平台化支持。

Conclusion: RoboNeuron为现实大规模具身智能应用提供了可扩展、模块化与高适应性的系统基础，有力推动了具身AI系统在复杂真实场景中的工程落地。

Abstract: Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.

</details>


### [146] [Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots](https://arxiv.org/abs/2512.10477)
*Timur Ishuov,Michele Folgheraiter,Madi Nurmanov,Goncalo Gordo,Richárd Farkas,József Dombi*

Main category: cs.RO

TL;DR: 本论文提出了一种旨在提升机器人从零开始训练效率和安全性的算法，结合了多种创新方法，包括“Swaddling”正则化、受控噪声注入及时间优势更新机制。


<details>
  <summary>Details</summary>
Motivation: 当前机器人从零开始（零经验）学习时，训练步骤冗长且存在安全隐患，而人类在成长中也并非快速学习，真正学习过程需要时间和合理的限制。因此，作者希望引入更现实且安全高效的训练方法。

Method: 提出“Swaddling”正则化以在早期训练中限制动作幅度，减少不稳定发展；引入Symphony算法（Transition-policy Deterministic Actor and Critic），在动作中仅加入有限参数噪声，实现弱噪声下自适应探索，并在必要时提升动作幅度；提出Fading Replay Buffer，将记忆库以新旧记忆融合并调节采样概率，引入Temporal Advantage方法，使Actor和Critic网络能够同步更新，提高训练效率和简化实现。

Result: 实验表明，与传统随机算法相比，Symphony算法能够在动作安全性提升的同时，实现更高效的采样利用率与更安全的训练过程。弱噪声正则化降低了对机器人本体和环境的损害，训练过程更加平滑和可控。

Conclusion: 论文展示的新型训练方法有效提升了机器人训练的样本效率和安全性，为从零开始训练人形机器人提供了一种实际可行且更为安全高效的解决方案。

Abstract: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.

</details>


### [147] [Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF](https://arxiv.org/abs/2512.10480)
*Jiaqiang Zhang,Xianjia Yu,Sier Ha,Paola Torrico Moron,Sahar Salimpour,Farhad Kerama,Haizhou Zhang,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种融合GNSS、UWB和IMU的行人定位系统，并对三种概率后端（ESKF、滑动窗口因子图、粒子滤波）进行了统一评估。系统通过PDR为运动主骨架，结合户外GNSS和室内UWB，采用地图约束增强过渡鲁棒性。ESKF法表现最优。


<details>
  <summary>Details</summary>
Motivation: 在户外-室内环境中实现连续、准确的行人定位很难，因为GNSS、UWB、PDR各有弱点（如信号遮挡、多路径、累积漂移），单独使用皆存在漏洞。因此，迫切需要一种能无缝集成多源传感器并有效融合信号的新型定位框架。

Method: 设计了一套基于IMU的PDR为主体，分别融合户外GNSS与室内UWB的定位系统。在GNSS变差或信号遮挡区，通过轻量级地图约束（使用OpenStreetMap楼宇轮廓，非特定建筑内部不可通行，仅指定建筑可导航）提升定位鲁棒性。后端采用ESKF、滑动窗口因子图与粒子滤波三种方式进行对比实验。系统实时运行于可穿戴平台，基于ROS 2实现。

Result: 在三种场景（室内、室外、无缝切换）下分别测试。结果显示，误差状态扩展卡尔曼滤波器（ESKF）表现最为一致和优秀。

Conclusion: 融合GNSS、UWB、IMU及地图约束可实现可靠的行人无缝定位。在所选后端实现中，ESKF表现最佳。该方案提升了户外-室内切换的定位连续性与鲁棒性，适合实时、可穿戴应用。

Abstract: Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.

</details>


### [148] [Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks](https://arxiv.org/abs/2512.10481)
*Gaozhao Wang,Xing Liu,Zhenduo Ye,Zhengxiong Liu,Panfeng Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种新的物理驱动接触认知方法“Contact SLAM”，依靠触觉和场景先验知识来实现机器人在视觉遮挡下的高效操作，并设计了主动探索策略来提升环境状态估计能力，验证了该方法在多个复杂操作任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，尤其是涉及大量接触的任务时，视觉信息经常被遮挡，无法获取实时环境状态，导致操作变得困难。为提升机器人在“盲操作”条件下的感知与执行能力，需要研发无需视觉反馈即可准确估计环境状态的方法。

Method: 作者提出了“Contact SLAM”方法，通过融合触觉传感器数据与环境先验知识，估计操作场景的状态。同时，设计了一种主动探索策略，允许机器人逐步降低操作的不确定性，提高探索效率。

Result: 实验在多个接触丰富操作任务（如插座装配、推方块）中开展，结果表明该方法能够准确、高效地感知环境并顺利完成操作任务，验证了方案的有效性。

Conclusion: 本文证明了依靠触觉和先验知识进行盲操作的可行性及优势，为复杂环境下机器人的自主操作提供了新的解决方案。

Abstract: Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.

</details>


### [149] [Neural Ranging Inertial Odometry](https://arxiv.org/abs/2512.10531)
*Si Wang,Bingqi Shen,Fei Wang,Yanjun Cao,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合UWB（超宽带）与惯性测程的新型神经网络融合定位框架IR-ULSG，显著提升了复杂环境下的定位精度和鲁棒性，适用于多样化的现实应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统UWB定位受限于多径干扰、传感器布置等因素，特别是在长隧道等典型应用中，定位精度大大下降。作者希望通过新的方法提升UWB在严苛环境下的定位性能，扩展其应用领域。

Method: 提出了一套神经融合框架，将图注意力UWB网络与循环神经惯性网络相结合。该方法无需标定即可自适应不同数量的锚点，学习场景相关的测距模式。同时融合最小二乘法和标称帧，以提升系统整体性能和可扩展性。

Result: 在室内、室外和隧道等不同场景的公开和自采集数据集上进行了大量实验。结果显示，所提方法在复杂挑战条件下表现优异，支持凸包外定位，甚至在只有一个锚点时依然具备较好定位精度和鲁棒性。

Conclusion: 所提出的IR-ULSG方法显著提升了UWB+惯性定位系统在恶劣、复杂环境下的性能，不仅免去了繁琐的系统标定，同时提高了系统的适应性和推广价值。

Abstract: Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.

</details>


### [150] [Mr. Virgil: Learning Multi-robot Visual-range Relative Localization](https://arxiv.org/abs/2512.10540)
*Si Wang,Zhehan Li,Jiadong Lu,Rong Xiong,Yanjun Cao,Yue Wang*

Main category: cs.RO

TL;DR: 本论文提出了一种结合UWB和视觉信息的多机器人相对定位系统Mr. Virgil，利用图神经网络实现数据关联，并通过可微分位姿图优化提升定位精度，能有效应对匹配错误等挑战，在多种场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 多机器人相对定位在实际场景中应用广泛，但现有UWB-视觉融合方法在机器人和视觉检测之间的匹配存在较大挑战，过度自信但错误的匹配会导致不可逆的定位错误，现有方法过度依赖带身份编码的硬件或精细调参，实际落地难度大。

Method: 提出一个端到端学习框架Mr. Virgil，包括：1）利用图神经网络在UWB测距和视觉检测间进行鲁棒数据关联，提供初步匹配、位置预测及不确定性估计；2）设计可微分的位姿图优化后端，进一步提升位姿估计准确性；3）实现分布式系统以适用于实际多机器人平台。

Result: 在多机器人数量、仿真与实测、遮挡与非遮挡等多种条件下进行实验，Mr. Virgil在匹配稳定性和定位精度上明显优于传统方法，验证了其有效性和广泛适应性。

Conclusion: 本文提出的Mr. Virgil框架能够有效消除多机器人UWB-视觉融合定位中的关联难题和错误匹配冲击，具备鲁棒、高精度和可落地性，能为多机器人体系的协同定位提供切实可行的解决方案。

Abstract: Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.

</details>


### [151] [Motion Planning for Safe Landing of a Human-Piloted Parafoil](https://arxiv.org/abs/2512.10595)
*Maximillian Fainkich,Kiril Solovey,Anna Clarke*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样运动规划（Stable Sparse RRT, SST）的降落伞飞行轨迹优化方法，在安全性和控制成本上显著优于人类飞行员，并可为未来初学者训练模拟器提供指导。


<details>
  <summary>Details</summary>
Motivation: 大部分跳伞事故源于降落伞操控阶段的人为判断失误。现有模拟器匮乏且不方便，且缺乏适用于人类训练的轨迹规划方法，因此亟需更好的飞行训练和指导工具。

Method: 采用改进的采样运动规划器SST，结合约束条件，将最小化横滚角（控制努力）作为安全代理，生成安全着陆轨迹。随后，将算法结果与真实人类飞行轨迹数据对比评估。

Result: 与人类飞行员的实际轨迹相比，计算机生成方案在成本（控制努力）上提升20%-80%；算法轨迹更平顺，能精准到达合适高度并保持安全性。

Conclusion: 计算机生成的着陆指导比传统经验规则更有效，具备显著提升学员训练效率和飞行安全性的潜力，建议将其整合进未来降落伞飞行模拟器中。

Abstract: Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\%-80\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.

</details>


### [152] [LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator](https://arxiv.org/abs/2512.10605)
*Lihuang Chen,Xiangyu Luo,Jun Meng*

Main category: cs.RO

TL;DR: LEO-RobotAgent提出了一种通用的、基于大语言模型的机器人智能体框架，能够在多场景、多类型机器人间执行复杂任务，并提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有利用大模型进行机器人任务规划的研究大多聚焦于单一任务和单一机器人类型，结构复杂且泛化能力差，亟需一种具有通用性、简洁性和高效性的新方法。

Method: 设计了LEO-RobotAgent框架，具备模块化、易注册的工具集支持，简化结构，使大模型能在该框架下独立思考、规划和行动。同时集成了人机协作机制，实现人类与算法间的伙伴式协作。

Result: 实验表明，该框架能方便适配主流机器人平台（如无人机、机械臂和轮式机器人），并高效完成多种不同复杂度的任务。

Conclusion: LEO-RobotAgent框架兼具通用性、鲁棒性与高效性，大幅提升了机器人任务规划和人机交互能力，为复杂任务执行和多平台适配提供了有效方案。

Abstract: We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.

</details>


### [153] [Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675)
*Gemini Robotics Team,Coline Devin,Yilun Du,Debidatta Dwibedi,Ruiqi Gao,Abhishek Jindal,Thomas Kipf,Sean Kirmani,Fangchen Liu,Anirudha Majumdar,Andrew Marmon,Carolina Parada,Yulia Rubanova,Dhruv Shah,Vikas Sindhwani,Jie Tan,Fei Xia,Ted Xiao,Sherry Yang,Wenhao Yu,Allan Zhou*

Main category: cs.RO

TL;DR: 该论文提出了一种基于前沿视频生成模型的机器人策略仿真与评估系统，通过在多视角、多类型泛化场景下生成高保真视觉数据，实现对策略在正常和分布外环境中的综合评估，包括性能、泛化和安全性测试。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在机器人领域的应用主要局限于与训练数据相关的场景，对真正分布外、复杂环境下的策略评估支持有限，缺乏通用、可扩展的高保真仿真能力，制约了策略泛化和安全性检测的发展。

Method: 作者基于Veo前沿视频基础模型，构建了一个支持机器人动作条件输入和多视角一致性的生成评估系统，并集成了生成式图像编辑与多视角补全技术以合成多种泛化维度的现实场景变化。该系统可编辑场景，包括引入新的交互物体、视觉背景及干扰对象，并对这些变化下的策略行为进行仿真预测。

Result: 系统能准确模拟各种编辑场景，反映不同策略在正常及分布外条件下的相对性能差异、各泛化轴对策略影响及物理/语义安全性问题。通过实际部署，累计对八个策略检查点和五项双臂机器人任务完成1600余次真实评估，验证了系统的有效性。

Conclusion: 前沿视频模型不仅可用于高保真仿真常规场景，还能支持对策略泛化能力和安全性多维度评估，为机器人策略部署、优化和监测提供了通用、可靠的生成式评测工具。

Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.

</details>


### [154] [How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning](https://arxiv.org/abs/2512.10698)
*Jianbo Wang,Galina Sidorenko,Johan Thunberg*

Main category: cs.RO

TL;DR: 本文研究通过深度强化学习（DRL）提升自动驾驶车辆紧急制动场景下的安全性和整体伤害减轻效果。提出一种结合车辆间通讯和之前优化减速度分析法的DRL混合方法，提升了多车跟车情形下的碰撞避免和集体伤害缩减能力。


<details>
  <summary>Details</summary>
Motivation: 传统“最坏情况”控制方法牺牲了灵活性和整体性能，难以在实际多车紧急制动中最优权衡车辆安全与整体伤害最小化。因此，本文尝试探索如何利用现代智能算法，在保证安全的前提下提升自动驾驶车辆集体处理突发情况的水平。

Method: 提出了一种混合方法，将深度强化学习（DRL）与已有的基于车辆最优恒定减速度分析法相结合，并利用车辆间通讯信息指导三车跟车紧急制动情景下的决策，实现整体伤害减轻或碰撞回避。

Result: 实验结果显示，混合方法在可靠性、集体伤害降低以及碰撞避免性能上均优于单独的DRL算法和传统的分析方法。

Conclusion: 将深度强化学习与传统分析最优方法相结合并应用于自动驾驶车辆紧急制动，不仅提高了整个系统的可靠性，还显著提升了多车场景下的整体安全性和伤害减轻效果。

Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.

</details>


### [155] [On the Stabilization of Rigid Formations on Regular Curves](https://arxiv.org/abs/2512.10700)
*Mohamed Elobaid,Shinkyu Park,Eric Feron*

Main category: cs.RO

TL;DR: 本文提出了一种方法，使多智能体队形能够在一般平面曲线上稳定地形成等边多边形结构，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，如机器人集群、无人机编队等领域，经常需要多个智能体按照特定形状（如正多边形）在复杂空间路径上运动和集结。现有方法对一般平面曲线上的队形稳定性支持有限，且难以保证收敛至目标队形并避免碰撞。因此，亟需开发可用于一般曲线轨迹上的多智能体编队稳定方法。

Method: 1）利用随机多起点的类Newton算法，在给定曲线上寻找到内接、中心在目标点的正多边形；2）基于此，设计连续反馈控制律，确保智能体能首先完成曲线巡检，然后收敛至目标多边形顶点，并保证智能体间不会发生碰撞。3）通过不同类型曲线与刚性多边形队形的数值仿真，验证所提算法的有效性。

Result: 数值仿真结果表明，提出的方法可有效地使多智能体在各种复杂平面曲线上，实现等边多边形队形的自组织与稳定，并保证避障需求。

Conclusion: 本文提出的多智能体刚性队形稳定方法适用于广泛的曲线类型，具有理论收敛性和实际可实施性。相关算法代码已公开，便于复现与拓展。

Abstract: This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026

</details>


### [156] [AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators](https://arxiv.org/abs/2512.10773)
*Samaksh Ujjawal,Shivansh Pratap Singh,Naveen Sudheer Nair,Rishabh Dev Yadav,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: 此论文提出一种基于条件扩散（diffusion）模型的新方法，以更准确建模空中操作机器人的剩余动力学，实现更高的控制精度。


<details>
  <summary>Details</summary>
Motivation: 空中操作机器人由于结构和运动变化，动力学中的惯性耦合力和气动力随配置迅速且显著变化，导致传统动力学建模难以精确描述这些非线性和非平稳特性，影响其可靠控制。

Method: 作者提出了一种“基于工况条件（regime-conditioned）的扩散建模框架”，使用条件扩散过程配合轻量级时序编码器来建模动力学剩余力的全分布。时序编码器对近期运动与配置进行编码，作为模型的条件，使模型能在突变或新负载的情况下也能一致地预测残差。

Result: 该方法并与自适应控制器结合，在现实环境下进行测试，通过补偿动力学不确定性，显著提升了轨迹跟踪的精度。

Conclusion: 所提出的框架能有效应对传统方法动力学建模的不足，提升空中操作机器人在复杂环境下的控制表现，对实际应用具有重要意义。

Abstract: Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.

</details>


### [157] [Iterative Compositional Data Generation for Robot Control](https://arxiv.org/abs/2512.10891)
*Anh-Quan Pham,Marcel Hussing,Shubhankar P. Patankar,Dani S. Bassett,Jorge Mendez-Mendez,Eric Eaton*

Main category: cs.RO

TL;DR: 该论文提出了一种语义可组合扩散变换器模型，能以更高效的方式生成用于机器人操作的新型组合任务数据，并通过自我改进机制显著提升未见任务的解决能力。


<details>
  <summary>Details</summary>
Motivation: 多物体、多机器人和多环境下的任务组合爆炸性增长，获取所有任务演示数据成本极高。现有生成模型难以泛化到组合任务，且忽略了机器人的组合结构。为解决这一难题，亟需可泛化且利用组合结构的方法。

Method: 提出“语义可组合扩散变换器”，把环境状态的转换分解为与机器人、物体、障碍物和目标相关的不同部分，并通过注意力机制建模其交互；模型仅需在少量任务上训练即可，之后能零样本生成未见组合任务高质量转换序列。此外，引入自我改进机制：用离线强化学习验证合成数据的有效性，并循环纳入训练，不断提升模型能力。

Result: 实验表明，所提方法在未知组合任务上的零样本生成与策略学习能力，大幅超过单一结构或人工编码的组合基线，几乎能解决所有保留的组合任务。

Conclusion: 这种面向机器人领域组合结构的生成式模型及自我提升流程，有效提升了组合泛化能力，为机器人多任务学习和自动生成操作数据提供了新路径。

Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.

</details>


### [158] [Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit](https://arxiv.org/abs/2512.10934)
*Zamirddine Mari,Jérôme Pasquet,Julien Seinturier*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机导航方法，实现无人机在未知三维管道环境中的自主导航，无需预先知道管道几何信息，仅依赖LiDAR和有限的视觉信息，显著优于传统的Pure Pursuit算法。


<details>
  <summary>Details</summary>
Motivation: 受限于管道几何、壁面接近、视觉受限等因素，无人机在狭小管道中自主导航具有极大的挑战。现有方法通常需要管道的精确几何信息，限制了其应用范围。作者希望开发一种能在未知、复杂三维管道中仅靠局部感知完成导航的通用方法。

Method: 采用强化学习（PPO）模型，通过渐进式课程学习使无人机适应不同弯曲程度的管道，逐步增加难度，加快收敛。导航过程中仅利用LiDAR及偶尔可见的管道中心视觉信息，结合了转弯协商机制（包括可见性、方向记忆和LiDAR对称性线索），稳健应对部分可观测问题。与以中心线为基础的确定性Pure Pursuit算法对比。

Result: 实验表明，PPO策略在鲁棒性和泛化能力方面均优于deterministic控制器（Pure Pursuit），即使在没有完整几何信息的前提下，依然能稳定且高效地通过曲折管道。并在高保真3D仿真环境中验证了方法可迁移性和实际动力学表现。

Conclusion: 提出的方法为未知管道环境下的无人机自主导航提供了完整框架，对工业、地下和医疗等领域的狭窄弱感知通道自主通过难题具有重要应用前景和参考价值。

Abstract: Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.
  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.

</details>


### [159] [ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning](https://arxiv.org/abs/2512.10946)
*Wendi Chen,Han Xue,Yi Wang,Fangyuan Zhou,Jun Lv,Yang Jin,Shirun Tang,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 本论文提出了一种名为ImplicitRDP的端到端视觉-力觉扩散策略网络，通过结构化的慢-快学习机制和虚拟目标正则化，有效融合视觉和力觉信息，从而极大提升了机器人在复杂接触操作任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 人类级别的复杂接触操作通常依赖于视觉和力觉的协同作用。视觉可提供丰富的空间信息但响应较慢，力觉则捕捉快速本地接触动态。如何高效融合这两种物理频率和信息内容差异极大的模态，对实现高性能机器人操作具有重大挑战。

Method: 本文提出了ImplicitRDP，一种统一的端到端视觉-力觉扩散策略网络，集成视觉规划和力觉反馈控制。核心方法包括：(1) 结构化的慢-快学习（Structural Slow-Fast Learning），用因果注意机制异步处理视觉与力觉信号，实现在力觉频率下的闭环调节，同时保证动作的时序连贯；(2) 虚拟目标表征正则化（Virtual-target-based Representation Regularization），把力觉反馈映射到动作空间，提供基于物理更强的辅助学习信号。

Result: 在多项接触丰富的机器人操作实验中，ImplicitRDP显著优于仅视觉或分层基线方法，取得更高的反应性和成功率，同时简化了训练流程。

Conclusion: 本文方法有效融合视觉与力觉信息，克服了不同模态间频率和信息量差异导致的融合难题，为机器人接触操作提供了新的高效端到端解决方案。

Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.

</details>
