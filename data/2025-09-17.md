<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 108]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 50]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction](https://arxiv.org/abs/2509.12242)
*Mustafa Khanbhai,Giulia Di Nardo,Jun Ma,Vivienne Freitas,Caterina Masino,Ali Dolatabadi,Zhaoxun "Lorenz" Liu,Wey Leong,Wagner H. Souza,Amin Madani*

Main category: cs.CV

TL;DR: 本文提出了一种基于人机协作新型机器学习方法（U-Mamba），用于提升3D解剖结构分割和重建算法在不同医学影像数据集中的泛化能力，取得了优越的性能并对临床多方面产生积极影响。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像分割模型在面对不同患者来源和应用场景时普遍泛化能力不足，影响临床前规划和个性化医疗需求，亟需提升分割算法的泛化性和适用性。

Method: 研究从120份乳腺MRI影像（2018-2023年）出发，分为影像匿名与手工标注、解剖结构配准及分割、ITK-SNAP 3D可视化三阶段，通过结合人工修正的U-Mamba模型推进分割过程，并用Dice系数评估自动分割效果，辅以临床访谈评价方法实际意义。

Result: U-Mamba模型在T1加权影像上，整器官、纤维腺体、肿瘤Dice均值分别为0.97、0.96、0.82，能准确产出高质量3D重建，并且被医生及患者认同可提升规划、术中导航、沟通与教育。

Conclusion: 人机协作的机器学习新方法成功提升了3D解剖结构分割的泛化性及可视化表现，对临床规划、患者教育和医生决策支持都带来显著提升，有助于推动医学多应用场景下的智能与个性化诊疗。

Abstract: Effective preoperative planning requires accurate algorithms for segmenting
anatomical structures across diverse datasets, but traditional models struggle
with generalization. This study presents a novel machine learning methodology
to improve algorithm generalization for 3D anatomical reconstruction beyond
breast cancer applications. We processed 120 retrospective breast MRIs (January
2018-June 2023) through three phases: anonymization and manual segmentation of
T1-weighted and dynamic contrast-enhanced sequences; co-registration and
segmentation of whole breast, fibroglandular tissue, and tumors; and 3D
visualization using ITK-SNAP. A human-in-the-loop approach refined
segmentations using U-Mamba, designed to generalize across imaging scenarios.
Dice similarity coefficient assessed overlap between automated segmentation and
ground truth. Clinical relevance was evaluated through clinician and patient
interviews. U-Mamba showed strong performance with DSC values of 0.97
($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and
0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate
3D reconstructions enabling visualization of complex anatomical features.
Clinician interviews indicated improved planning, intraoperative navigation,
and decision support. Integration of 3D visualization enhanced patient
education, communication, and understanding. This human-in-the-loop machine
learning approach successfully generalizes algorithms for 3D reconstruction and
anatomical segmentation across patient datasets, offering enhanced
visualization for clinicians, improved preoperative planning, and more
effective patient education, facilitating shared decision-making and empowering
informed patient choices across medical applications.

</details>


### [2] [RU-Net for Automatic Characterization of TRISO Fuel Cross Sections](https://arxiv.org/abs/2509.12244)
*Lu Cai,Fei Xu,Min Xian,Yalei Tang,Shoukun Sun,John Stempien*

Main category: cs.CV

TL;DR: 本论文提出利用卷积神经网络（CNN）自动分割TRISO颗粒燃料显微图像，以加快分析流程并提高分割客观性。RU-Net（本文开发）在分割表现上优于其他主流CNN结构。


<details>
  <summary>Details</summary>
Motivation: TRISO燃料粒子在辐照过程中会发生核壳膨胀和缓冲层致密化，影响燃料性能。每个燃料块包含数千个粒子，人工分析显微镜图像十分繁琐且主观性强，因此需要一种自动化、高效且客观的方法来处理大量显微图像数据。

Method: 作者建立了包含2000多张TRISO横截面显微图像及对应标注数据集，分别采用自研的RU-Net和现有的U-Net、ResNet、Attention U-Net卷积神经网络，对不同的TRISO层进行自动分割，并评估分割模型性能。

Result: 实验结果表明，RU-Net在交并比（IoU）指标上表现最好，优于U-Net、ResNet和Attention U-Net等已有结构，能够更加准确地分割TRISO的不同层。

Conclusion: 通过应用CNN模型，可以大幅提升TRISO粒子横截面分析的速度和分割的客观性，显著减少手工劳动量，对核燃料材料的自动分析具有实际应用前景。

Abstract: During irradiation, phenomena such as kernel swelling and buffer
densification may impact the performance of tristructural isotropic (TRISO)
particle fuel. Post-irradiation microscopy is often used to identify these
irradiation-induced morphologic changes. However, each fuel compact generally
contains thousands of TRISO particles. Manually performing the work to get
statistical information on these phenomena is cumbersome and subjective. To
reduce the subjectivity inherent in that process and to accelerate data
analysis, we used convolutional neural networks (CNNs) to automatically segment
cross-sectional images of microscopic TRISO layers. CNNs are a class of
machine-learning algorithms specifically designed for processing structured
grid data. They have gained popularity in recent years due to their remarkable
performance in various computer vision tasks, including image classification,
object detection, and image segmentation. In this research, we generated a
large irradiated TRISO layer dataset with more than 2,000 microscopic images of
cross-sectional TRISO particles and the corresponding annotated images. Based
on these annotated images, we used different CNNs to automatically segment
different TRISO layers. These CNNs include RU-Net (developed in this study), as
well as three existing architectures: U-Net, Residual Network (ResNet), and
Attention U-Net. The preliminary results show that the model based on RU-Net
performs best in terms of Intersection over Union (IoU). Using CNN models, we
can expedite the analysis of TRISO particle cross sections, significantly
reducing the manual labor involved and improving the objectivity of the
segmentation results.

</details>


### [3] [Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture](https://arxiv.org/abs/2509.12247)
*Abigail R. Cohen,Yuming Sun,Zhihao Qin,Harsh S. Muriki,Zihao Xiao,Yeonju Lee,Matthew Housley,Andrew F. Sharkey,Rhuanito S. Ferrarezi,Jing Li,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种高效、分层的多光谱影像处理管道，用于作物养分异常检测及状态估计，兼顾了精度和能耗，对农业智能诊断具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 高效的养分管理对作物生长与资源可持续利用至关重要。然而，现有方法分析过程耗时，难以实现实时优化；影像分析虽然快速但计算资源消耗大，限制了其在资源受限场景下的应用。因此需要一个兼顾实时性、能效和精度的诊断与估计方法。

Method: 作者开展了不同肥料强度（100%, 50%, 25%）的养分消耗实验，结合多光谱成像，提出了分层管道：首先用自编码器检测异常以早期预警；再用两种不同复杂度的模块估计鲜重、干重和组织养分——一种基于植被指数与随机森林，一种基于整图的ViT深度神经网络。系统对比了不同方法的能效与准确性。

Result: 异常检测早期识别效果显著（在移栽后9天达到73%的T3异常样本检测率），且能耗远低于因浪费氮肥所需能量。在状态估计方面，ViT在磷、钙估计上优于随机森林，但消耗更高能源（R2：0.61对0.58；0.48对0.35）。整体展示了效率与精度间的权衡。

Conclusion: 提出的模块化分析管道能够在能效和准确率之间灵活权衡，适用于边缘设备，具备实际农田快速诊断和养分管理优化潜力，推动农业可持续发展。

Abstract: Efficient nutrient management is critical for crop growth and sustainable
resource consumption (e.g., nitrogen, energy). Current approaches require
lengthy analyses, preventing real-time optimization; similarly, imaging
facilitates rapid phenotyping but can be computationally intensive, preventing
deployment under resource constraints. This study proposes a flexible, tiered
pipeline for anomaly detection and status estimation (fresh weight, dry mass,
and tissue nutrients), including a comprehensive energy analysis of approaches
that span the efficiency-accuracy spectrum. Using a nutrient depletion
experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer
strength) and multispectral imaging (MSI), we developed a hierarchical pipeline
using an autoencoder (AE) for early warning. Further, we compared two status
estimation modules of different complexity for more detailed analysis:
vegetation index (VI) features with machine learning (Random Forest, RF) and
raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated
high-efficiency anomaly detection (73% net detection of T3 samples 9 days after
transplanting) at substantially lower energy than embodied energy in wasted
nitrogen. The state estimation modules show trade-offs, with ViT outperforming
RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at
higher energy cost. With our modular pipeline, this work opens opportunities
for edge diagnostics and practical opportunities for agricultural
sustainability.

</details>


### [4] [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
*Yuriel Ryan,Rui Yang Tan,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 本文提出PixelHumor数据集，通过2800个带注释的多格漫画，评测多模态大模型在理解幽默和叙事连贯性上的能力。实验表明当前领先模型的表现远不及人类。


<details>
  <summary>Details</summary>
Motivation: 理解幽默是社会智能的核心方面，但现有大型多模态模型在此仍面临巨大挑战。缺乏系统评测模型多模态幽默理解能力的基准数据集。

Method: 作者构建了一个名为PixelHumor的多模态基准数据集，包含2800个已注释的多格漫画，用于测试模型在幽默理解和叙事顺序识别方面的能力。

Result: 在该数据集上测试的最先进多模态大模型在面板排序任务上仅能取得61%的准确率，显著低于人类表现，显示模型在视觉和文本线索的整合及连贯叙事理解上存在明显不足。

Conclusion: PixelHumor为多模态上下文和叙事推理提供了严格的评测框架，有助于推动更具自然和社会意识交互能力的大型多模态模型的发展。

Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a
significant challenge for Large Multimodal Models (LMMs). We introduce
PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed
to evaluate LMMs' ability to interpret multimodal humor and recognize narrative
sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for
instance, top models achieve only 61% accuracy in panel sequencing, far below
human performance. This underscores critical limitations in current models'
integration of visual and textual cues for coherent narrative and humor
understanding. By providing a rigorous framework for evaluating multimodal
contextual and narrative reasoning, PixelHumor aims to drive the development of
LMMs that better engage in natural, socially aware interactions.

</details>


### [5] [OnlineHOI: Towards Online Human-Object Interaction Generation and Perception](https://arxiv.org/abs/2509.12250)
*Yihong Ji,Yunze Liu,Yiyao Zhuo,Weijiang Yu,Fei Ma,Joshua Huang,Fei Yu*

Main category: cs.CV

TL;DR: 该论文关注人体-物体交互（HOI）的感知与生成在线任务，提出基于Mamba框架和记忆机制的OnlineHOI网络，并在多个数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前HOI方法多在离线场景下建模，能获取整个交互序列的信息，然而实际任务通常是在线（只能用历史和当前的信息），现有离线方法在在线场景下表现不佳。

Method: 提出两个新任务：在线HOI生成与感知，并构建了OnlineHOI框架。该架构基于Mamba网络，利用其对流数据的建模能力，结合记忆机制高效集成历史信息，以适应在线HOI任务。

Result: 在Core4D和OAKINK2在线生成任务及HOI4D在线感知任务上，OnlineHOI都取得了最优性能（state-of-the-art）。

Conclusion: OnlineHOI框架有效解决了在线HOI建模需求，克服了离线方法的局限性，为在线交互感知与生成提供了高效方案，推动了领域进步。

Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial
for fields such as robotics, AR/VR, and human behavior understanding. However,
current approaches model this task in an offline setting, where information at
each time step can be drawn from the entire interaction sequence. In contrast,
in real-world scenarios, the information available at each time step comes only
from the current moment and historical data, i.e., an online setting. We find
that offline methods perform poorly in an online context. Based on this
observation, we propose two new tasks: Online HOI Generation and Perception. To
address this task, we introduce the OnlineHOI framework, a network architecture
based on the Mamba framework that employs a memory mechanism. By leveraging
Mamba's powerful modeling capabilities for streaming data and the Memory
mechanism's efficient integration of historical information, we achieve
state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as
well as the online HOI4D perception task.

</details>


### [6] [EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces](https://arxiv.org/abs/2509.12258)
*Li Kun,Milena Radenkovic*

Main category: cs.CV

TL;DR: 深度学习带来了深远影响，其中Deepfake技术在社会生活中应用广泛，但也带来了隐私泄露、名誉受损和国家安全等重大风险。


<details>
  <summary>Details</summary>
Motivation: 分析深度学习，尤其是Deepfake技术在现实社会中的普及和应用，从而评估其对隐私、安全、政治等方面的负面影响。

Method: 对当前深度学习和Deepfake技术的应用现状进行探讨，列举其在社会各领域中的影响和风险，包括对个人隐私、名人声誉以及国家安全的威胁。

Result: Deepfake技术能够生成几乎无法辨别的伪造图像和视频，威胁到人脸识别系统的有效性，并通过软件滥用带来了广泛的社会危害，例如在选举中通过伪造信息影响公众认知、损害政治和经济结构。

Conclusion: 尽管深度学习推动了技术前沿，但Deepfake等技术的滥用会对社会造成严峻冲击，需要警惕并采取措施应对相关风险。

Abstract: Currently, deep learning has been utilised to tackle several difficulties in
our everyday lives. It not only exhibits progress in computer vision but also
constitutes the foundation for several revolutionary technologies. Nonetheless,
similar to all phenomena, the use of deep learning in diverse domains has
produced a multifaceted interaction of advantages and disadvantages for human
society. Deepfake technology has advanced, significantly impacting social life.
However, developments in this technology can affect privacy, the reputations of
prominent personalities, and national security via software development. It can
produce indistinguishable counterfeit photographs and films, potentially
impairing the functionality of facial recognition systems, so presenting a
significant risk.
  The improper application of deepfake technology produces several detrimental
effects on society. Face-swapping programs mislead users by altering persons'
appearances or expressions to fulfil particular aims or to appropriate personal
information. Deepfake technology permeates daily life through such techniques.
Certain individuals endeavour to sabotage election campaigns or subvert
prominent political figures by creating deceptive pictures to influence public
perception, causing significant harm to a nation's political and economic
structure.

</details>


### [7] [A Modern Look at Simplicity Bias in Image Classification Tasks](https://arxiv.org/abs/2509.12265)
*Xiaoguang Chang,Teng Wang,Changyin Sun*

Main category: cs.CV

TL;DR: 该论文研究了神经网络中的简约性偏置（Simplicity Bias, SB），主要在CLIP模型及其在图像分类任务中的表现。作者提出了新的衡量SB的方法，并证实这种衡量方法比以往更有效。此外，SB对不同任务表现有不同影响，如更高的SB有助于OOD泛化，但对对抗鲁棒性提升有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明简约性偏置对神经网络的泛化有积极影响，但过强可能对复杂任务有害，不同任务对SB的需求也不同。目前主要关注点在小模型或合成任务上，缺乏对大规模模型特别是CLIP这类通用视觉模型中SB影响的系统研究。

Method: 作者首先理论分析了现有小模型复杂性度量的局限，提出了一种频率感知的、新的SB量化方法，并对CLIP模型应用了两种最新SB调节技术以验证其有效性。随后，研究SB对CLIP在多种图像分类任务下（包括zero-shot和微调环境）的表现影响。

Result: 新提出的SB衡量方法比传统方法更精细和一致。实验表明，不同SB水平的模型在多种分类任务下表现各异。例如，更高的SB有助于提升对分布外数据的泛化（OOD generalization），但对提升对抗鲁棒性效果一般。

Conclusion: 模型的归纳偏置（inductive bias）应与具体任务需求相匹配。新方法能更好刻画大模型SB差异，实验结果则证明在实际应用中合理匹配SB对模型性能提升至关重要。

Abstract: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to
represent simple functions, is a key factor in their generalization
capabilities. Recent studies show that an excessive SB may harm performance on
complex tasks, and the need for this bias varies across tasks. Many of these
studies focus on simple models or synthetic tasks. It remains challenging to
measure the SB in large models and little is known about the relevance of the
SB to various image classification tasks.
  In this paper, we investigate the relationship between the SB in CLIP models
and their performance across image classification tasks. First, we
theoretically analyze the potential limitation of existing measures of
complexity that have been used to characterize small models. To address this,
we propose a frequency-aware measure capturing finer-grained SB differences. We
validate this measure on CLIP models subjected to two recent SB-modulation
methods, demonstrating that it is more informative and consistent than previous
measures. Second, we examine the relation between the SB of those models and
their performance across a range of image classification tasks, including
zero-shot and fine-tuning settings. These experiments reveal a range of
behaviors. For example, a stronger SB correlates with a better performance on
OOD generalization than on adversarial robustness. These results highlight the
benefits of aligning a model's inductive biases with the characteristics of the
target task.

</details>


### [8] [GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions](https://arxiv.org/abs/2509.12277)
*Mehdi Yousefzadeh,Parsa Esfahanian,Sara Rashidifar,Hossein Salahshoor Gavalan,Negar Sadat Rafiee Tabatabaee,Saeid Gorgin,Dara Rahmati,Maryam Daneshpazhooh*

Main category: cs.CV

TL;DR: 本文提出了一种名为GraphDerm的新型AI方法，将皮肤镜图像、毫米级标尺校准和患者元数据有效融合，在ISIC-2019皮肤病变数据集上进行多类别分类。与传统只用图像的AI模型相比，该方法显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的皮肤镜AI模型通常忽视了患者基本信息（如年龄、性别、病变部位）以及影响几何分析的实际物理尺度信息，导致分类性能受限。为增加诊断精度，作者尝试将图像、标尺校准信息和元数据在图结构中统一建模。

Method: 作者整理了ISIC 2018/2019皮肤镜数据，合成带标尺的图像与精确标注，采用U-Net（SE-ResNet-18）分割病灶和标尺，使用1D-CNN回归像素与毫米的换算比例，提取真实几何描述符。结点特征用EfficientNet-B3提取，边则用元数据和几何相似性加权编码，构建光谱GNN进行半监督分类；仅含图像的ANN作对比基线。

Result: 标尺和病灶分割的Dice系数分别为0.904和0.908，尺度回归误差MAE为1.5像素。用全连接和稀疏阈值图推断，AUC达0.9812和0.9788，明显优于仅用图像基线AUC 0.9440，各类别AUC普遍为0.97-0.99。

Conclusion: 将标尺校准、病灶几何和患者元数据以图结构联合输入AI，能大幅提升皮肤镜病变分类效果。稀疏图结构仍可保持高精度，利于高效部署。基于尺度和图的AI方法为皮肤镜辅助诊断提供了有前景的方向，未来可优化边的语义学习并在更大尺度验证。

Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often
ignores patient metadata (age, sex, site) and the physical scale needed for
geometric analysis. We present GraphDerm, a population-graph framework that
fuses imaging, millimeter-scale calibration, and metadata for multiclass
dermoscopic classification, to the best of our knowledge the first ISIC-scale
application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,
synthesize ruler-embedded images with exact masks, and train U-Nets
(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are
regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.
From lesion masks we compute real-scale descriptors (area, perimeter, radius of
gyration). Node features use EfficientNet-B3; edges encode metadata/geometry
similarity (fully weighted or thresholded). A spectral GNN performs
semi-supervised node classification; an image-only ANN is the baseline.
Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale
regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a
thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440
for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99
range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in
a population graph yields substantial gains over image-only pipelines on
ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient
deployment. Scale-aware, graph-based AI is a promising direction for
dermoscopic decision support; future work will refine learned edge semantics
and evaluate on broader curated benchmarks.

</details>


### [9] [PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models](https://arxiv.org/abs/2509.12278)
*Wanru Zhuang,Wenbo Li,Zhibin Lan,Xu Han,Peng Li,Jinsong Su*

Main category: cs.CV

TL;DR: 本文提出了新的TIMT任务扩展——位置感知的图像文本机器翻译（PATIMT），并构建了全新的PATIMT基准数据集及优化方案，实现了更精细、布局保持的机器翻译。


<details>
  <summary>Details</summary>
Motivation: 现有的TIMT主要关注将图片中所有文本翻译成另一种语言，但忽略了文本在图像中的位置信息（如边界框）和实际复杂场景的多样性，导致无法支持真实应用中的精细化和布局保持的翻译需求。

Method: 作者提出了PATIMT，在TIMT基础上支持区域特定翻译和全图文本带定位翻译，并构建了包含10种真实场景的PATIMTBench基准，提出自适应图像OCR优化管道针对不同场景选择最合适的OCR工具并提升识别效果，人工标注1200个高质量测试样本，并对大型视觉-语言模型进行了微调。

Result: 经微调的紧凑型大型视觉语言模型在PATIMT及其两个子任务上取得了最新最优表现，训练数据展现出良好的可扩展性和泛化能力。

Conclusion: 提出的PATIMT及其数据集和方法为图像文本机器翻译任务带来了更加精细、位置感知和可实际应用的解决方案，推动了该领域的研究发展。

Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within
an image into another language. Current TIMT studies primarily focus on
providing translations for all the text within an image, while neglecting to
provide bounding boxes and covering limited scenarios. In this work, we extend
traditional TIMT into position-aware TIMT (PATIMT), aiming to support
fine-grained and layoutpreserving translation, which holds great practical
value but remains largely unexplored. This task comprises two key sub-tasks:
regionspecific translation and full-image translation with grounding. To
support existing models on PATIMT and conduct fair evaluation, we construct the
PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world
scenarios. Specifically, we introduce an Adaptive Image OCR Refinement
Pipeline, which adaptively selects appropriate OCR tools based on scenario and
refines the results of text-rich images. To ensure evaluation reliability, we
further construct a test set, which contains 1,200 high-quality instances
manually annotated and reviewed by human experts. After fine-tuning on our
data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art
performance on both sub-tasks. Experimental results also highlight the
scalability and generalizability of our training data

</details>


### [10] [Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance](https://arxiv.org/abs/2509.12279)
*He Gao,Baoxiang Huang,Milena Radenkovic,Borui Li,Ge Chen*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态领域自适应框架SimMemDA，用于提升SAR图像下船舶尾流检测的准确性和鲁棒性，通过风格迁移、特征相似性过滤和记忆引导等方法缓解光学与SAR图像的域差异。


<details>
  <summary>Details</summary>
Motivation: SAR图像具有全天候、宽视场的观测优势，但图像表达抽象且噪声多，难以准确标注。直接用光学图像训练的模型应用于SAR会因域差而性能下降，需要有效的跨模态领域自适应方法以提升检测效果。

Method: 方法包括：先用WakeGAN对光学图像做风格迁移，生成接近SAR风格的伪影像；其次，通过实例级特征相似性过滤，优先选用分布类似目标域的源样本，减少负迁移；同时，构建特征-置信度记忆库，结合K近邻置信度加权策略，动态校正目标域伪标签；最后，通过区域混合训练，将源域标注和目标域校正伪标签结合，提升泛化能力。

Result: 实验结果表明，SimMemDA方法在跨模态船舶尾流检测任务中能显著提升准确性与鲁棒性，优于现有方法。

Conclusion: SimMemDA框架有效缓解了SAR与光学图像的域差，提升了跨模态船舶尾流检测的准确性和泛化能力，验证了思路的可行性和有效性。

Abstract: Synthetic Aperture Radar (SAR), with its all- weather and wide-area
observation capabilities, serves as a crucial tool for wake detection. However,
due to its complex imaging mechanism, wake features in SAR images often appear
abstract and noisy, posing challenges for accurate annotation. In contrast,
optical images provide more distinct visual cues, but models trained on optical
data suffer from performance degradation when applied to SAR images due to
domain shift. To address this cross-modal domain adaptation challenge, we
propose a Similarity-Guided and Memory-Guided Domain Adap- tation (termed
SimMemDA) framework for unsupervised domain adaptive ship wake detection via
instance-level feature similarity filtering and feature memory guidance.
Specifically, to alleviate the visual discrepancy between optical and SAR
images, we first utilize WakeGAN to perform style transfer on optical images,
generating pseudo-images close to the SAR style. Then, instance-level feature
similarity filtering mechanism is designed to identify and prioritize source
samples with target-like dis- tributions, minimizing negative transfer.
Meanwhile, a Feature- Confidence Memory Bank combined with a K-nearest neighbor
confidence-weighted fusion strategy is introduced to dynamically calibrate
pseudo-labels in the target domain, improving the reliability and stability of
pseudo-labels. Finally, the framework further enhances generalization through
region-mixed training, strategically combining source annotations with
calibrated tar- get pseudo-labels. Experimental results demonstrate that the
proposed SimMemDA method can improve the accuracy and robustness of cross-modal
ship wake detection tasks, validating the effectiveness and feasibility of the
proposed method.

</details>


### [11] [Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning](https://arxiv.org/abs/2509.12329)
*Shengjie Kris Liu,Siqin Wang,Lu Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于数据驱动、物理指导的深度学习方法，实现对美国本土2公里分辨率每小时近地面气温的高精度重建和预测。


<details>
  <summary>Details</summary>
Motivation: 目前气象站和卫星数据各有局限，前者覆盖面有限，后者因云层等因素导致数据间断，缺乏无缝的时空分辨率气温数据。因此，需要创新方法弥补这两类数据的不足，实现高分辨率、无缝的气温监测。

Method: 方法采用名为Amplifier Air-Transformer的深度学习框架，第一步利用内嵌年温周期的神经网络重建云层遮挡下的GOES-16地表温度数据，通过叠加ERA5温度信息和卷积层处理区分空间与时间变化。第二步，借助另一神经网络，将重建后的地表温度与地表物理特性结合，转化为近地面气温。同时，通过深度集成学习估算预测不确定度，提高结果可靠性。

Result: 方法基于全国77.7亿地表温度像素和1.55亿气象站气温观测，在2018-2024年间验证，地面站点小时气温重建精度达到1.93°C。

Conclusion: 该方法显著提升了高时空分辨率气温数据的重建精度和可靠性，可推广至其他卫星平台，实现更广域、无缝的气温监测，为相关研究和应用提供有力支撑。

Abstract: Near-surface air temperature is a key physical property of the Earth's
surface. Although weather stations offer continuous monitoring and satellites
provide broad spatial coverage, no single data source offers seamless data in a
spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep
learning approach to generate hourly air temperature data at 2 km resolution
over the contiguous United States. The approach, called Amplifier
Air-Transformer, first reconstructs GOES-16 surface temperature data obscured
by clouds. It does so through a neural network encoded with the annual
temperature cycle, incorporating a linear term to amplify ERA5 temperature
values at finer scales and convolutional layers to capture spatiotemporal
variations. Then, another neural network transforms the reconstructed surface
temperature into air temperature by leveraging its latent relationship with key
Earth surface properties. The approach is further enhanced with predictive
uncertainty estimation through deep ensemble learning to improve reliability.
The proposed approach is built and tested on 77.7 billion surface temperature
pixels and 155 million air temperature records from weather stations across the
contiguous United States (2018-2024), achieving hourly air temperature mapping
accuracy of 1.93 C in station-based validation. The proposed approach
streamlines surface temperature reconstruction and air temperature prediction,
and it can be extended to other satellite sources for seamless air temperature
monitoring at high spatiotemporal resolution. The generated data of this study
can be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project
webpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.

</details>


### [12] [DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification](https://arxiv.org/abs/2509.12353)
*Anthony Miyaguchi,Chandrasekaran Maruthaiyannan,Charles R. Clark*

Main category: cs.CV

TL;DR: 本文分析了用于动物重新识别的深度学习模型，发现后置的度量学习效果取决于基础特征提取器的质量和专用性。专用模型（MegaDescriptor）配合三元组学习提升显著，而通用模型提升有限。强调针对特定任务进行领域预训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 动物重新识别任务中，通用和专用模型的基础嵌入质量对后续度量学习的影响尚不明确，本文希望探究不同基础模型在该任务上的表现和局限。

Method: 分别采用通用的DINOv2和领域专用的MegaDescriptor作为主干网络，通过K近邻分类器做个体判别，并引入三元组学习投影头对特征空间进行优化，分析不同主干在后置度量学习中的表现。

Result: 专用主干（MegaDescriptor）配合三元组学习，BAKS和BAUS指标提升0.13；通用主干（DINOv2）提升仅0.03，并出现验证损失停滞。定性可视化进一步证明通用主干难以适应细粒度任务。

Conclusion: 后置优化通用特征在数据有限的专业任务中效果有限，领域专用预训练不可或缺。领域特定主干加后续度量学习才更适用于此类难任务。

Abstract: This paper details the DS@GT team's entry for the AnimalCLEF 2025
re-identification challenge. Our key finding is that the effectiveness of
post-hoc metric learning is highly contingent on the initial quality and
domain-specificity of the backbone embeddings. We compare a general-purpose
model (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A
K-Nearest Neighbor classifier with robust thresholding then identifies known
individuals or flags new ones. While a triplet-learning projection head
improved the performance of the specialized MegaDescriptor model by 0.13
points, it yielded minimal gains (0.03) for the general-purpose DINOv2 on
averaged BAKS and BAUS. We demonstrate that the general-purpose manifold is
more difficult to reshape for fine-grained tasks, as evidenced by stagnant
validation loss and qualitative visualizations. This work highlights the
critical limitations of refining general-purpose features for specialized,
limited-data re-ID tasks and underscores the importance of domain-specific
pre-training. The implementation for this work is publicly available at
github.com/dsgt-arc/animalclef-2025.

</details>


### [13] [GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images](https://arxiv.org/abs/2509.12380)
*Florian Zager,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 该论文提出GhostNetV3-Small，针对低分辨率任务优化，显著提升精度，且模型结构优化优于知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络计算开销大，不适合资源受限的边缘设备。需要模型压缩与优化以便在如CIFAR-10等低分辨率环境高效推理。

Method: 从GhostNetV3出发，提出GhostNetV3-Small以更好适配低分辨率输入，并系统评价传统知识蒸馏、助教模式及教师集合等多种蒸馏方法。

Result: GhostNetV3-Small在CIFAR-10上显著优于原始GhostNetV3，精度达到93.94%。所有尝试的知识蒸馏方式均未超过基线精度。

Conclusion: 在小规模图像分类任务中，架构适配比知识蒸馏更有效，未来需进一步研究低分辨率领域高效模型设计和蒸馏技术。

Abstract: Deep neural networks have achieved remarkable success across a range of
tasks, however their computational demands often make them unsuitable for
deployment on resource-constrained edge devices. This paper explores strategies
for compressing and adapting models to enable efficient inference in such
environments. We focus on GhostNetV3, a state-of-the-art architecture for
mobile applications, and propose GhostNetV3-Small, a modified variant designed
to perform better on low-resolution inputs such as those in the CIFAR-10
dataset. In addition to architectural adaptation, we provide a comparative
evaluation of knowledge distillation techniques, including traditional
knowledge distillation, teacher assistants, and teacher ensembles. Experimental
results show that GhostNetV3-Small significantly outperforms the original
GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to
expectations, all examined distillation strategies led to reduced accuracy
compared to baseline training. These findings indicate that architectural
adaptation can be more impactful than distillation in small-scale image
classification tasks, highlighting the need for further research on effective
model design and advanced distillation techniques for low-resolution domains.

</details>


### [14] [From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization](https://arxiv.org/abs/2509.12400)
*Rongkun Zhu,Kangning Cui,Wei Tang,Rui-Feng Wang,Sarra Alqahtani,David Lutz,Fan Yang,Paul Fine,Jordan Karubian,Robert Plemmons,Jean-Michel Morel,Victor Pauca,Miles Silman*

Main category: cs.CV

TL;DR: 本文比较了无人机正射影像与原始影像在热带森林棕榈树检测与冠心定位中的表现，发现原始影像更适合实际部署，而正射影像对跨域泛化有利。引入冠心标注能进一步提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 生态监测和森林管理需要精确的单棵树定位。现有正射影像方法存在缝合伪影及预处理繁琐的问题，限制了现场应用。因此，探索是否能用原始无人机影像提升检测与定位性能，具有重要实践意义。

Method: 该研究基于最先进的目标检测及关键点模型，对比分析了无人机正射影像与原始图像在棕榈树检测和冠心定位任务中的表现，并考察了域内与跨域的检测迁移效果，以及引入冠心标注对定位精度的影响。

Result: 实验显示原始影像在部署相关场景下检测与定位精度优于正射影像，但正射影像在需要跨域泛化时表现更佳。训练时加入冠心标注，可以进一步提升树冠定位精度。

Conclusion: 原始无人机影像适用于实际场景下的树木检测和定位，正射影像适合需要泛化能力的任务。应用冠心标注可提供更加精准的树木位置，为生态监测和多样性保护提供更有力的数据支持。

Abstract: Accurate mapping of individual trees is essential for ecological monitoring
and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs)
is widely used, but stitching artifacts and heavy preprocessing limit its
suitability for field deployment. This study explores the use of raw UAV
imagery for palm detection and crown-center localization in tropical forests.
Two research questions are addressed: (1) how detection performance varies
across orthomosaic and raw imagery, including within-domain and cross-domain
transfer, and (2) to what extent crown-center annotations improve localization
accuracy beyond bounding-box centroids. Using state-of-the-art detectors and
keypoint models, we show that raw imagery yields superior performance in
deployment-relevant scenarios, while orthomosaics retain value for robust
cross-domain generalization. Incorporating crown-center annotations in training
further improves localization and provides precise tree positions for
downstream ecological analyses. These findings offer practical guidance for
UAV-based biodiversity and conservation monitoring.

</details>


### [15] [DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction](https://arxiv.org/abs/2509.12430)
*Mayank Patel,Rahul Jain,Asim Unmesh,Karthik Ramani*

Main category: cs.CV

TL;DR: 本文针对机械装配体（如齿轮）运动的推断问题，提出了新数据集MechBench和神经网络方法DYNAMO，实现了仅依据静态几何预测复杂耦合运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理日常铰接物体（如门、笔记本）运动推断时，多依赖简化运动结构或人工关节标注，而对复杂机械装配体中的齿轮类耦合运动难以处理。机械运动的几何耦合特性和缺乏相关数据集，成为3D感知和自动设计领域的一大挑战。

Method: （1）构建了MechBench数据集，涵盖693个多样化的合成齿轮装配体，并且为每个部件提供了真实的运动轨迹标注；（2）提出了依赖感知神经网络DYNAMO，直接从分割的CAD点云输入，预测每个部件的SE(3)运动轨迹，实现对复杂耦合关系的建模。

Result: DYNAMO在各种齿轮结构上均优于现有强基线，能准确且时序一致地预测分部件运动。

Conclusion: MechBench与DYNAMO共同构建了系统化的新范式，使基于数据驱动的方法可直接从几何推断机械装配体中复杂的耦合运动，为CAD设计与3D感知领域带来重要进展。

Abstract: Understanding the motion of articulated mechanical assemblies from static
geometry remains a core challenge in 3D perception and design automation. Prior
work on everyday articulated objects such as doors and laptops typically
assumes simplified kinematic structures or relies on joint annotations.
However, in mechanical assemblies like gears, motion arises from geometric
coupling, through meshing teeth or aligned axes, making it difficult for
existing methods to reason about relational motion from geometry alone. To
address this gap, we introduce MechBench, a benchmark dataset of 693 diverse
synthetic gear assemblies with part-wise ground-truth motion trajectories.
MechBench provides a structured setting to study coupled motion, where part
dynamics are induced by contact and transmission rather than predefined joints.
Building on this, we propose DYNAMO, a dependency-aware neural model that
predicts per-part SE(3) motion trajectories directly from segmented CAD point
clouds. Experiments show that DYNAMO outperforms strong baselines, achieving
accurate and temporally consistent predictions across varied gear
configurations. Together, MechBench and DYNAMO establish a novel systematic
framework for data-driven learning of coupled mechanical motion in CAD
assemblies.

</details>


### [16] [Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions](https://arxiv.org/abs/2509.12442)
*Rui-Feng Wang,Mingrui Xu,Matthew C Bauer,Iago Beffart Schardong,Xiaowen Ma,Kangning Cui*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级实时检测器Cott-ADNet，用于棉花铃与花朵识别，性能优异，适用于田间自动化采收和高通量表型分析。


<details>
  <summary>Details</summary>
Motivation: 棉花采收主要依赖人工，存在效率低、劳动强度大、易错过最佳采收期等问题。为实现自动化采收、产量估算和育种研究，亟需高效、准确地识别棉花铃及其成熟度。

Method: 作者基于YOLOv11n提出了Cott-ADNet，采用改进卷积结构提升空间表达和鲁棒性，引入NeLU增强全局注意力机制和膨胀感受野SPPF模块，提升低对比度特征捕捉与多尺度建模能力。同时，构建并公开了标注数据集和外部验证集。

Result: Cott-ADNet在4,966张训练图片和1,216张验证图片测试下，达到Precision 91.5%、Recall 89.8%、mAP50 93.3%、mAP 71.3%、F1-Score 90.6%，仅需7.5 GFLOPs，能稳定适应多尺度和旋转变化。

Conclusion: Cott-ADNet以较低计算资源实现了高精度、高效率的棉花铃与花识别，有望广泛应用于田间实际自动采收和棉花表型研究等场景。

Abstract: Cotton is one of the most important natural fiber crops worldwide, yet
harvesting remains limited by labor-intensive manual picking, low efficiency,
and yield losses from missing the optimal harvest window. Accurate recognition
of cotton bolls and their maturity is therefore essential for automation, yield
estimation, and breeding research. We propose Cott-ADNet, a lightweight
real-time detector tailored to cotton boll and flower recognition under complex
field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial
representation and robustness through improved convolutional designs, while
introducing two new modules: a NeLU-enhanced Global Attention Mechanism to
better capture weak and low-contrast features, and a Dilated Receptive Field
SPPF to expand receptive fields for more effective multi-scale context modeling
at low computational cost. We curate a labeled dataset of 4,966 images, and
release an external validation set of 1,216 field images to support future
research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%
Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,
maintaining stable performance under multi-scale and rotational variations.
These results demonstrate Cott-ADNet as an accurate and efficient solution for
in-field deployment, and thus provide a reliable basis for automated cotton
harvesting and high-throughput phenotypic analysis. Code and dataset is
available at https://github.com/SweefongWong/Cott-ADNet.

</details>


### [17] [Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications](https://arxiv.org/abs/2509.12452)
*Zhenxin Zhang,Zhihua Xu,Yuwei Cao,Ningli Xu,Shuye Wang,Shen'ao Cui,Zhen Li,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文综述了深度学习在点云处理中的应用，分析了当前方法在现实应用中的局限与挑战，特别关注数据体量、场景内容、点密度和数据类型等实际问题，并对未来发展提出建议。


<details>
  <summary>Details</summary>
Motivation: 当前点云处理主要被深度学习方法主导，但现有综述多侧重于网络结构本身，忽略了实际应用过程中面临的复杂问题，如大规模数据、场景多样性和数据多模态。因此需要对深度学习方法及其在不同应用中遇到的挑战进行全面回顾与分析。

Method: 通过系统性地回顾和梳理近年来深度学习在点云场景补全、配准、语义分割和建模等关键任务的最新方法和公开数据集，并分析这些方法在城市与环境实际应用中的表现与挑战。

Result: 总结了深度学习驱动的点云处理方法在多种重要实际任务中的应用、优势与不足，识别了当前方法在实际落地过程中存在的若干鸿沟和待突破的问题。

Conclusion: 作者认为，深度学习在点云处理领域有很大发展空间，但仍需进一步解决实际应用中的数据量、场景复杂性、多样性等实际难题，强调未来研究应更加关注算法的实用性和可扩展性。

Abstract: Point cloud processing as a fundamental task in the field of geomatics and
computer vision, has been supporting tasks and applications at different scales
from air to ground, including mapping, environmental monitoring, urban/tree
structure modeling, automated driving, robotics, disaster responses etc. Due to
the rapid development of deep learning, point cloud processing algorithms have
nowadays been almost explicitly dominated by learning-based approaches, most of
which are yet transitioned into real-world practices. Existing surveys
primarily focus on the ever-updating network architecture to accommodate
unordered point clouds, largely ignoring their practical values in typical
point cloud processing applications, in which extra-large volume of data,
diverse scene contents, varying point density, data modality need to be
considered. In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling. By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.

</details>


### [18] [Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis](https://arxiv.org/abs/2509.12453)
*Yiran Song,Yikai Zhang,Silvia Orengo-Nania,Nian Wang,Fenglong Ma,Rui Zhang,Yifan Peng,Mingquan Lin*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段解耦框架（TSDF），用于可变长度的青光眼预测，突破了以往方法输入长度固定与数据集规模有限的局限。


<details>
  <summary>Details</summary>
Motivation: 现有青光眼预后方法受限于固定长度历史序列数据，且大多为端到端模型，难以有效利用有限且异质的数据集。此问题阻碍了模型泛化及在实际场景的应用。

Method: 该方法分为两步：第一步使用自监督学习特征表示模块，融合多个不同来源青光眼数据集，忽略它们的标签差异，以提升特征表达；第二步引入基于注意力机制的时序聚合模块，可处理不同长度序列，实现数据的灵活高效利用。

Result: 在OHTS和GRAPE两个规模和临床背景差异很大的公开基准数据集上，实验结果显示该方法在有效性和鲁棒性上均具有显著提升，同时保持了模型紧凑参数量。

Conclusion: TSDF框架能够更好处理实际临床中青光眼数据的多样性与时序变动性，相比传统方法表现更优，具有实际应用潜力。

Abstract: Glaucoma is one of the leading causes of irreversible blindness worldwide.
Glaucoma prognosis is essential for identifying at-risk patients and enabling
timely intervention to prevent blindness. Many existing approaches rely on
historical sequential data but are constrained by fixed-length inputs, limiting
their flexibility. Additionally, traditional glaucoma prognosis methods often
employ end-to-end models, which struggle with the limited size of glaucoma
datasets. To address these challenges, we propose a Two-Stage Decoupling
Framework (TSDF) for variable-length glaucoma prognosis. In the first stage, we
employ a feature representation module that leverages self-supervised learning
to aggregate multiple glaucoma datasets for training, disregarding differences
in their supervisory information. This approach enables datasets of varying
sizes to learn better feature representations. In the second stage, we
introduce a temporal aggregation module that incorporates an attention-based
mechanism to process sequential inputs of varying lengths, ensuring flexible
and efficient utilization of all available data. This design significantly
enhances model performance while maintaining a compact parameter size.
Extensive experiments on two benchmark glaucoma datasets:the Ocular
Hypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal
Progression Ensemble (GRAPE),which differ significantly in scale and clinical
settings,demonstrate the effectiveness and robustness of our approach.

</details>


### [19] [Image Tokenizer Needs Post-Training](https://arxiv.org/abs/2509.12474)
*Kai Qiu,Xiang Li,Hao Chen,Jason Kuen,Xiaohao Xu,Jiuxiang Gu,Yinyi Luo,Bhiksha Raj,Zhe Lin,Marios Savvides*

Main category: cs.CV

TL;DR: 该论文针对当前图像生成模型中影像分词器(tokenizer)面对的重建与生成分布不一致问题，提出了一种包含主训练与后训练的新型分词器训练方案，大幅提升生成质量和收敛速度，并设计了新指标pFID与gFID进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分词器只注重生成前的重建任务，忽视了生成推理过程中的生成误差，导致重建分布与生成分布存在较大差距，影响图像生成质量。解决分布不一致问题，是提升生成模型表现的关键。

Method: 提出两阶段的分词器训练框架：主训练阶段引入潜在干扰策略，模拟生成采样过程中的噪声（如生成推理中出现的异常token），用于提升分词器鲁棒性；后训练阶段则针对训练好的生成模型进一步优化分词器解码器，缩小生成token与重建token的分布差异。同时，提出了新评价指标pFID和gFID用于量化分词器与生成质量的关系。

Result: 通过在约4亿参数量级的生成器上实验证明，采用主训练的新型分词器使gFID降至1.60，加上后训练进一步降至1.36。该后训练策略还在多种现有离散和连续型分词器、以及自回归和扩散式生成模型上进行了广泛验证，显示出优异的泛化性和有效性。

Conclusion: 该方法能显著提升图像生成模型的分词器质量与最终生成效果，提出的训练框架和评测指标为生成质量优化提供了实际有效的技术路线，并对后续相关研究具有参考和推动意义。

Abstract: Recent image generative models typically capture the image distribution in a
pre-constructed latent space, relying on a frozen image tokenizer. However,
there exists a significant discrepancy between the reconstruction and
generation distribution, where current tokenizers only prioritize the
reconstruction task that happens before generative training without considering
the generation errors during sampling. In this paper, we comprehensively
analyze the reason for this discrepancy in a discrete latent space, and, from
which, we propose a novel tokenizer training scheme including both
main-training and post-training, focusing on improving latent space
construction and decoding respectively. During the main training, a latent
perturbation strategy is proposed to simulate sampling noises, \ie, the
unexpected tokens generated in generative inference. Specifically, we propose a
plug-and-play tokenizer training scheme, which significantly enhances the
robustness of tokenizer, thus boosting the generation quality and convergence
speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully
correlates the tokenizer performance to generation quality. During
post-training, we further optimize the tokenizer decoder regarding a
well-trained generative model to mitigate the distribution difference between
generated and reconstructed tokens. With a $\sim$400M generator, a discrete
tokenizer trained with our proposed main training achieves a notable 1.60 gFID
and further obtains 1.36 gFID with the additional post-training. Further
experiments are conducted to broadly validate the effectiveness of our
post-training strategy on off-the-shelf discrete and continuous tokenizers,
coupled with autoregressive and diffusion-based generators.

</details>


### [20] [Towards Foundational Models for Single-Chip Radar](https://arxiv.org/abs/2509.12482)
*Tianshu Huang,Akarsh Prabhakara,Chuhan Chen,Jay Karhade,Deva Ramanan,Matthew O'Toole,Anthony Rowe*

Main category: cs.CV

TL;DR: 本文提出并训练了目前最大规模的原始毫米波雷达数据集（100万样本，29小时），并基于此提出了通用雷达变换器（GRT），可实现接近高分辨率传感器的3D占据和语义分割。实验表明，GRT具有良好的泛化能力，并通过分析得知仅原始数据远优于常用损耗表示。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达是经济、耐用且环境鲁棒的传感器，但多用于单芯片的低成本版本，其角分辨率差，严重限制感知能力。目前缺乏针对毫米波雷达的大规模数据集和基础模型，导致大多为任务定制、小样本学习，泛化能力弱。

Method: 作者收集了目前最大规模的毫米波雷达原始数据集，同时设计并训练了Generalizable Radar Transformer基础模型，能够从单芯片雷达的原始数据中预测3D占据和进行语义分割。这一模型各项任务和环境均可迁移，并对不同输入表示和设计选择进行了消融实验。

Result: GRT在多环境和任务下均展现良好泛化能力，通过微调即可适配不同任务。用原始数据显著优于常见的有损表达，相当于将训练集扩大10倍。同时数据增大对性能提升符合对数规律（每扩充10倍数据提升约20%）。

Conclusion: 基于原始数据和大规模数据集训练的GRT模型，为毫米波雷达感知提供了强有力的通用基础模型。结果显示，若能获得1亿样本（约3000小时），有望充分挖掘GRT及毫米波雷达的潜力。

Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust
to occlusions and work regardless of environmental conditions, such as weather
and darkness. However, this comes at the cost of poor angular resolution,
especially for inexpensive single-chip radars, which are typically used in
automotive and indoor sensing applications. Although many have proposed
learning-based methods to mitigate this weakness, no standardized foundational
models or large datasets for the mmWave radar have emerged, and practitioners
have largely trained task-specific models from scratch using relatively small
datasets.
  In this paper, we collect (to our knowledge) the largest available raw radar
dataset with 1M samples (29 hours) and train a foundational model for 4D
single-chip radar, which can predict 3D occupancy and semantic segmentation
with quality that is typically only possible with much higher resolution
sensors. We demonstrate that our Generalizable Radar Transformer (GRT)
generalizes across diverse settings, can be fine-tuned for different tasks, and
shows logarithmic data scaling of 20\% per $10\times$ data. We also run
extensive ablations on common design decisions, and find that using raw radar
data significantly outperforms widely-used lossy representations, equivalent to
a $10\times$ increase in training data. Finally, we roughly estimate that
$\approx$100M samples (3000 hours) of data are required to fully exploit the
potential of GRT.

</details>


### [21] [Evaluating Robustness of Vision-Language Models Under Noisy Conditions](https://arxiv.org/abs/2509.12492)
*Purushoth,Alireza*

Main category: cs.CV

TL;DR: 本文系统评估了多种主流视觉-语言模型（VLMs）在各种噪声干扰下的性能，如光照变化、运动模糊和压缩失真，发现模型尺寸、数据集描述性和噪声类型共同影响模型的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在多模态任务中表现优异，但它们在有噪声条件下的稳健性尚不清楚，因此需要一个系统方法来标准化评估其鲁棒性。

Method: 作者设计了一个评估框架，对主要的VLMs模型在不同类型的受控扰动（如光照变化、运动模糊、JPEG压缩等）下进行测试，使用了词法指标（BLEU、METEOR、ROUGE、CIDEr）和基于句子嵌入的神经语义相似度来量化它们的表现。

Result: 实验证明，真实标签描述的细致程度对模型结果影响大；大模型（如LLaVA）在语义理解上表现更好，但未必在所有情形下都胜过小模型；某些噪声（例如JPEG压缩、运动模糊）会极大削弱所有模型的表现。

Conclusion: 不同模型在噪声鲁棒性方面存在权衡：模型体积、数据集特征和噪声类型都会影响鲁棒性表现。文章提出了标准化基准，为今后的稳健多模态学习研究提供了参考。

Abstract: Vision-Language Models (VLMs) have attained exceptional success across
multimodal tasks such as image captioning and visual question answering.
However, their robustness under noisy conditions remains unfamiliar. In this
study, we present a comprehensive evaluation framework to evaluate the
performance of several state-of-the-art VLMs under controlled perturbations,
including lighting variation, motion blur, and compression artifacts. We used
both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based
similarity measures using sentence embeddings to quantify semantic alignment.
Our experiments span diverse datasets, revealing key insights: (1)
descriptiveness of ground-truth captions significantly influences model
performance; (2) larger models like LLaVA excel in semantic understanding but
do not universally outperform smaller models; and (3) certain noise types, such
as JPEG compression and motion blur, dramatically degrade performance across
models. Our findings highlight the nuanced trade-offs between model size,
dataset characteristics, and noise resilience, offering a standardized
benchmark for future robust multimodal learning.

</details>


### [22] [Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.12496)
*Ali Torabi,Sanjog Gaihre,MD Mahbubur Rahman,Yaqoob Majeed*

Main category: cs.CV

TL;DR: IG-CAM是一个用于弱监督语义分割的新方法，通过引入实例引导、影响函数和多尺度边界增强，有效提升了定位精度和分割质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 弱监督语义分割只使用图像级标签训练分割模型，节省了像素级标注成本，但现有方法难以实现完整目标覆盖和精确边界分割。为解决这些问题，需要更有效的特征引导和边界增强机制。

Method: 提出IG-CAM方法，核心包括三点创新：（1）实例引导精炼，通过实例级线索和分割真值提升CAM的覆盖完整性和准确性；（2）影响函数的整合，关联训练样本与预测结果，增强特征鲁棒性；（3）多尺度边界增强，通过渐进式策略获得更锋利的分割边界。

Result: IG-CAM在PASCAL VOC 2012数据集上，后处理前mIoU为82.3%，应用CRF后达到86.6%，大幅度优于其它WSSS方法。对600张多样化图片的质性和消融实验验证了其鲁棒性与泛化能力。

Conclusion: IG-CAM为无像素级标注场景下的弱监督语义分割提供了高性能和高精度的实用方案，设立了新的基准，展现出在定位精度、目标覆盖完整性及边界分割等方面的显著优越性。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of
training segmentation models using only image-level annotations, eliminating
the need for expensive pixel-level labeling. While existing methods struggle
with precise object boundary localization and often focus only on the most
discriminative regions, we propose IG-CAM (Instance-Guided Class Activation
Mapping), a novel approach that leverages instance-level cues and influence
functions to generate high-quality, boundary-aware localization maps. Our
method introduces three key innovations: (1) Instance-Guided Refinement that
uses ground truth segmentation masks to guide CAM generation, ensuring complete
object coverage rather than just discriminative parts; (2) Influence Function
Integration that captures the relationship between training samples and model
predictions, leading to more robust feature representations; and (3)
Multi-Scale Boundary Enhancement that employs progressive refinement strategies
to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art
performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before
post-processing, which further improves to 86.6% after applying Conditional
Random Field (CRF) refinement, significantly outperforming previous WSSS
methods. Our approach demonstrates superior localization accuracy, with
complete object coverage and precise boundary delineation, while maintaining
computational efficiency. Extensive ablation studies validate the contribution
of each component, and qualitative comparisons across 600 diverse images
showcase the method's robustness and generalization capability. The results
establish IG-CAM as a new benchmark for weakly supervised semantic
segmentation, offering a practical solution for scenarios where pixel-level
annotations are unavailable or prohibitively expensive.

</details>


### [23] [Artist-Created Mesh Generation from Raw Observation](https://arxiv.org/abs/2509.12501)
*Yao He,Youngjoong Kwon,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的方法，可以将噪声大或缺失的点云数据直接转化为高质量的艺术风格网格模型。该方法无需复杂的多阶段流程，也不要求输入必须干净完整。核心创新在于将3D点云重建转化为2D修复任务，便于应用生成式模型。实验结果表明该方法效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有的生成艺术风格网格的方法对于输入点云的质量要求高，通常只适用于理想环境下的干净、完整数据。面对真实传感器（如LiDAR, RGB-D相机）采集到的噪声或不完整数据时，现有方法效果不佳，且往往流程复杂。为提升实际应用能力，亟需简化流程并增强鲁棒性的方法。

Method: 提出一种端到端的方法，直接从原始点云到艺术风格网格，将3D点云精修问题转化为2D修复问题，从而能够利用强大的2D生成模型。流程包括输入点云的2D重构、基于生成模型的修复以及重建高质量网格输出。

Result: 在ShapeNet数据集上进行了初步实验，展示了本方法能够输出更加干净、完整、符合艺术风格的网格模型，优于传统的多阶段管线。

Conclusion: 提出的方法有效简化了点云到艺术风格网格的生产流程，提高了对于噪声和缺失输入的鲁棒性，为商业图形生产提供了更高效、更实用的解决方案。

Abstract: We present an end-to-end framework for generating artist-style meshes from
noisy or incomplete point clouds, such as those captured by real-world sensors
like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for
commercial graphics pipelines due to their compatibility with animation and
texturing tools and their efficiency in rendering. However, existing approaches
often assume clean, complete inputs or rely on complex multi-stage pipelines,
limiting their applicability in real-world scenarios. To address this, we
propose an end-to-end method that refines the input point cloud and directly
produces high-quality, artist-style meshes. At the core of our approach is a
novel reformulation of 3D point cloud refinement as a 2D inpainting task,
enabling the use of powerful generative models. Preliminary results on the
ShapeNet dataset demonstrate the promise of our framework in producing clean,
complete meshes.

</details>


### [24] [Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery](https://arxiv.org/abs/2509.12511)
*Benjamin Vail,Rahul Harsha Cheppally,Ajay Sharda,Sidharth Rai*

Main category: cs.CV

TL;DR: 本论文提出了一种基于RGB-D图像的几何感知计算机视觉流程，能够自动、准确、高效地测量植物茎杆直径，为现代作物表型分析提供可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 作物表型性状如机械稳定性、生物量和抗病性等的改良离不开高效、准确的高通量表型分析。目前茎杆直径的传统测量方法存在劳动密集、误差大且难以大规模应用等问题，亟需自动化、高效的新方法。

Method: 本方法将深度学习的实例分割、三维点云重建与基于主成分分析（PCA）的轴对齐切片技术结合，处理RGB-D图像，通过纠正植物弯曲、遮挡及噪声的影响，实现了鲁棒的茎杆直径估算。

Result: 结果表明该方法能够可靠地估算茎杆直径，并有效提升高通量表型的数据获取效率与准确性。

Conclusion: 本方法为作物育种和农业研究中茎杆直径等结构性状的大规模、高通量分析提供了可行且实用的技术手段。

Abstract: Accurate, high-throughput phenotyping is a critical component of modern crop
breeding programs, especially for improving traits such as mechanical
stability, biomass production, and disease resistance. Stalk diameter is a key
structural trait, but traditional measurement methods are labor-intensive,
error-prone, and unsuitable for scalable phenotyping. In this paper, we present
a geometry-aware computer vision pipeline for estimating stalk diameter from
RGB-D imagery. Our method integrates deep learning-based instance segmentation,
3D point cloud reconstruction, and axis-aligned slicing via Principal Component
Analysis (PCA) to perform robust diameter estimation. By mitigating the effects
of curvature, occlusion, and image noise, this approach offers a scalable and
reliable solution to support high-throughput phenotyping in breeding and
agronomic research.

</details>


### [25] [Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew](https://arxiv.org/abs/2509.12544)
*Can Peng,Yuyuan Liu,Yingyu Yang,Pramit Saha,Qianye Yang,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出了一种用于多标签联邦学习的新方法，通过对特征进行解耦和聚类，提升分布异质和标签分布不均场景下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多标签数据在联邦学习中的表现受本地数据多样性和全局分布差异影响严重，现有研究多聚焦于单标签分类，而现实应用如医学影像常涉及多标签任务，急需更有效的解决方案。

Method: 作者受到Neural Collapse理论启发，提出通过特征对齐及聚类学习提升多标签联邦学习性能。具体方法包括设计特征解耦（disentanglement）模块提取语义特征，并用共享的NC结构约束各客户端特征分布，同时引入正则化损失促进潜空间聚类紧致性。

Result: 在四个公开多标签数据集和八种不同设置下，所提方法系统性优于现有方法，显著提高了多标签联邦学习的性能。

Conclusion: 该方法有效缓解了多标签联邦学习中因本地数据异质性和标签分布失衡引发的性能下降问题，为实际多标签联邦学习提供了创新且可推广的技术路线。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, the performance of
deep learning often deteriorates in FL due to decentralized and heterogeneous
data. This challenge is further amplified in multi-label scenarios, where data
exhibit complex characteristics such as label co-occurrence, inter-label
dependency, and discrepancies between local and global label relationships.
While most existing FL research primarily focuses on single-label
classification, many real-world applications, particularly in domains such as
medical imaging, often involve multi-label settings. In this paper, we address
this important yet underexplored scenario in FL, where clients hold multi-label
data with skewed label distributions. Neural Collapse (NC) describes a
geometric structure in the latent feature space where features of each class
collapse to their class mean with vanishing intra-class variance, and the class
means form a maximally separated configuration. Motivated by this theory, we
propose a method to align feature distributions across clients and to learn
high-quality, well-clustered representations. To make the NC-structure
applicable to multi-label settings, where image-level features may contain
multiple semantic concepts, we introduce a feature disentanglement module that
extracts semantically specific features. The clustering of these disentangled
class-wise features is guided by a predefined shared NC structure, which
mitigates potential conflicts between client models due to diverse local data
distributions. In addition, we design regularisation losses to encourage
compact clustering in the latent feature space. Experiments conducted on four
benchmark datasets across eight diverse settings demonstrate that our approach
outperforms existing methods, validating its effectiveness in this challenging
FL scenario.

</details>


### [26] [Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection](https://arxiv.org/abs/2509.12546)
*Yingxin Lai,Zitong Yu,Jun Wang,Linlin Shen,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多智能体模拟的人脸伪造数据生成框架Agent4FaceForgery，旨在提升伪造检测在真实场景中的效果，通过模拟真实社交环境下的文本-图像互动，生成更具生态有效性的数据，实验显示其可显著提升主流检测器性能。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造检测在实验室基准和真实世界表现之间存在显著差距，主要原因是训练数据缺乏生态有效性。当前数据无法很好地涵盖真实社交环境中的多样化伪造意图与复杂的文本-图像互动，导致检测方法泛化能力不足。

Method: 作者提出了Agent4FaceForgery框架，使用具备个人档案和记忆模块的多大模型驱动智能体(Large Language Model-powered agents)，在模拟的社交环境中互动，生成融合复杂文本-图像关联的伪造样本，并采用自适应拒绝采样机制确保数据质量和多样性。样本的标签涵盖更细腻的文本-图像一致性，不再局限于简单的真假分类。

Result: 通过大量实验，作者发现由Agent4FaceForgery框架生产的数据，能显著提升多种主流人脸伪造检测器的性能，涵盖不同架构，说明该方法生成的数据具有较高的适用性和生态有效性。

Conclusion: Agent4FaceForgery通过多智能体仿真，生成高多样性、高质量的人脸伪造数据，有效弥补现有训练集的生态失效问题，并显著提升现实场景下的伪造检测性能，为人脸伪造检测任务提供了有价值的新型数据生成和建模手段。

Abstract: Face forgery detection faces a critical challenge: a persistent gap between
offline benchmarks and real-world efficacy,which we attribute to the ecological
invalidity of training data.This work introduces Agent4FaceForgery to address
two fundamental problems: (1) how to capture the diverse intents and iterative
processes of human forgery creation, and (2) how to model the complex, often
adversarial, text-image interactions that accompany forgeries in social media.
To solve this,we propose a multi-agent framework where LLM-poweredagents,
equipped with profile and memory modules, simulate the forgery creation
process. Crucially, these agents interact in a simulated social environment to
generate samples labeled for nuanced text-image consistency, moving beyond
simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism
ensures data quality and diversity. Extensive experiments validate that the
data generated by our simulationdriven approach brings significant performance
gains to detectors of multiple architectures, fully demonstrating the
effectiveness and value of our framework.

</details>


### [27] [Explicit Multimodal Graph Modeling for Human-Object Interaction Detection](https://arxiv.org/abs/2509.12554)
*Wenxuan Ji,Haichao Shi,Xiao-Yu zhang*

Main category: cs.CV

TL;DR: 提出了多模态图神经网络方法（MGNM），结合图神经网络结构和多层视觉语言特征，显著提升人-物体交互（HOI）检测性能。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然是当前HOI检测主流方法，但其未能显式建模HOI中的关系结构，影响了交互识别效果。相比之下，图神经网络天然擅长建模关系结构，更适合此任务。因此，作者希望通过引入GNN克服Transformer的不足，提高HOI检测准确性。

Method: 设计了一个多模态图网络（MGNM）框架，通过四阶段图结构显式建模HOI任务，并引入多层级视觉与语言特征的交互机制，增强人-物对间的信息传播能力。

Result: 在HICO-DET和V-COCO两个权威数据集上达到新的SOTA（最佳）水平，并且结合更高级的目标检测器后表现进一步提升，同时能较好兼顾常见类别和稀有类别。

Conclusion: MGNM通过引入GNN关系建模与多层特征交互机制，有效提升了HOI检测任务的性能，优于当前主流基于Transformer的方法。

Abstract: Transformer-based methods have recently become the prevailing approach for
Human-Object Interaction (HOI) detection. However, the Transformer architecture
does not explicitly model the relational structures inherent in HOI detection,
which impedes the recognition of interactions. In contrast, Graph Neural
Networks (GNNs) are inherently better suited for this task, as they explicitly
model the relationships between human-object pairs. Therefore, in this paper,
we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork
\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to
enhance HOI detection. Specifically, we design a multimodal graph network
framework that explicitly models the HOI task in a four-stage graph structure.
Furthermore, we introduce a multi-level feature interaction mechanism within
our graph network. This mechanism leverages multi-level vision and language
features to enhance information propagation across human-object pairs.
Consequently, our proposed MGNM achieves state-of-the-art performance on two
widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a
more advanced object detector, our method demonstrates a significant
performance gain and maintains an effective balance between rare and non-rare
classes.

</details>


### [28] [VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf](https://arxiv.org/abs/2509.12556)
*Kunliang Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的光照估计算法VQT-Light，结合VQVAE与ViT架构，实现了更快、更精细的光照纹理预测，且超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 光照估计一直是计算机视觉与图形学中的重要且具有挑战性的问题。现有方法要么无法恢复光照贴图的精细纹理，要么在运行速度或纹理保真度上表现有限。因此，迫切需要一个同时兼顾高保真度和高速度的光照估计框架。

Method: VQT-Light包含两个模块：1）基于VQVAE提取离散特征，避免“posterior collapse”问题；2）利用ViT（视觉Transformer）而非传统CNN，增强对整图上下文和依赖关系的建模，从而优化视野外的光照预测。光照估计被表述为多分类任务以提升整体性能。

Result: VQT-Light能以40FPS的速度推理，同时在纹理丰富性和保真度上优于其它方法。实验（定性和定量）验证了其超越现有SOTA的表现。

Conclusion: VQT-Light高效地实现了更丰富且高保真的光照图预测，兼具实时速度，其方法强于目前主流方案，展现了实际应用潜力。

Abstract: Accurate lighting estimation is a significant yet challenging task in
computer vision and graphics. However, existing methods either struggle to
restore detailed textures of illumination map, or face challenges in run-ning
speed and texture fidelity. To tackle this problem, we propose a novel
framework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes
two modules: feature extraction and lighting estima-tion. First, we take
advantages of VQVAE to extract discrete features of illumination map rather
than con-tinuous features to avoid "posterior collapse". Second, we capture
global context and dependencies of in-put image through ViT rather than CNNs to
improve the prediction of illumination outside the field of view. Combining the
above two modules, we formulate the lighting estimation as a multiclass
classification task, which plays a key role in our pipeline. As a result, our
model predicts light map with richer texture and better fidelity while keeping
lightweight and fast. VQT-Light achieves an inference speed of 40FPS and
im-proves multiple evaluation metrics. Qualitative and quantitative experiments
demonstrate that the proposed method realizes superior results compared to
existing state-of-the-art methods.

</details>


### [29] [Adaptive Sampling Scheduler](https://arxiv.org/abs/2509.12569)
*Qi Wang,Shuliang Zhu,Jinjia Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种适用于不同一致性蒸馏框架的自适应采样调度器，大幅提升了扩散模型采样效率和灵活性。核心在于动态选择采样步数，通过多种创新算法，有效提升了生成质量与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有一致性蒸馏方法虽然加速效果显著，但对采样步长的选择过于依赖特定策略，导致灵活性不足，难以适应不同实际场景，限制了扩散模型的潜在性能。

Method: 作者设计了一种自适应采样调度器，包括：(1)根据步长重要性动态选择采样步数，适应不同一致性蒸馏方法；(2)沿溶液轨迹优化交替采样，结合前向去噪和反向加噪探索解空间；(3)利用平滑裁剪和色彩均衡方法，保证高引导尺度下生成效果的稳定性和高质量。

Result: 大量实验表明，该调度器适配多种一致性蒸馏框架，在复杂任务和高引导尺度下大幅提升了生成性能，展现出极强的灵活性和适应性。

Conclusion: 自适应采样调度器不仅显著提升扩散模型生成效率与质量，还扩展了一致性蒸馏方法在实际复杂生成场景中的应用空间。

Abstract: Consistent distillation methods have evolved into effective techniques that
significantly accelerate the sampling process of diffusion models. Although
existing methods have achieved remarkable results, the selection of target
timesteps during distillation mainly relies on deterministic or stochastic
strategies, which often require sampling schedulers to be designed specifically
for different distillation processes. Moreover, this pattern severely limits
flexibility, thereby restricting the full sampling potential of diffusion
models in practical applications. To overcome these limitations, this paper
proposes an adaptive sampling scheduler that is applicable to various
consistency distillation frameworks. The scheduler introduces three innovative
strategies: (i) dynamic target timestep selection, which adapts to different
consistency distillation frameworks by selecting timesteps based on their
computed importance; (ii) Optimized alternating sampling along the solution
trajectory by guiding forward denoising and backward noise addition based on
the proposed time step importance, enabling more effective exploration of the
solution space to enhance generation performance; and (iii) Utilization of
smoothing clipping and color balancing techniques to achieve stable and
high-quality generation results at high guidance scales, thereby expanding the
applicability of consistency distillation models in complex generation
scenarios. We validated the effectiveness and flexibility of the adaptive
sampling scheduler across various consistency distillation methods through
comprehensive experimental evaluations. Experimental results consistently
demonstrated significant improvements in generative performance, highlighting
the strong adaptability achieved by our method.

</details>


### [30] [DisorientLiDAR: Physical Attacks on LiDAR-based Localization](https://arxiv.org/abs/2509.12595)
*Yizhen Lao,Yu Zhang,Ziting Wang,Chengbo Wang,Yifei Xue,Wanpeng Shao*

Main category: cs.CV

TL;DR: 本文提出了一种针对基于LiDAR定位系统的新型对抗攻击框架DisorientLiDAR，通过识别并移除关键点，显著降低自动驾驶车辆的定位准确性，在KITTI数据集及实际车辆平台中均验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在自动驾驶中存在对抗攻击风险，但针对自动驾驶定位（特别是基于LiDAR的）攻击研究较少。传统对抗攻击多集中于3D感知任务而非定位。为弥补这一空白，本文提出专门面向基于LiDAR定位的攻击方法。

Method: 作者通过逆向工程分析定位模型（如特征提取网络），识别出影响定位结果的关键点，并有针对性地移除这些关键点。该方法在三种主流点云配准模型（HRegNet、D3Feat、GeoTransformer）及Autoware自动驾驶平台中进行实验，并在物理世界中用近红外吸收材料遮挡关键区域验证攻击效果。

Result: 实验结果表明，移除包含Top-K关键点的区域可显著降低点云配准和定位的准确度。在实际平台测试中，仅遮挡少量关键区域即产生明显的定位漂移，物理世界实验同样重现了数据集中的攻击效果。

Conclusion: DisorientLiDAR框架有效揭示了当前LiDAR定位系统在现实物理世界下也可能遭受对抗攻击的脆弱性，强调了加强此类系统安全性的紧迫性。

Abstract: Deep learning models have been shown to be susceptible to adversarial attacks
with visually imperceptible perturbations. Even this poses a serious security
challenge for the localization of self-driving cars, there has been very little
exploration of attack on it, as most of adversarial attacks have been applied
to 3D perception. In this work, we propose a novel adversarial attack framework
called DisorientLiDAR targeting LiDAR-based localization. By
reverse-engineering localization models (e.g., feature extraction networks),
adversaries can identify critical keypoints and strategically remove them,
thereby disrupting LiDAR-based localization. Our proposal is first evaluated on
three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and
GeoTransformer) using the KITTI dataset. Experimental results demonstrate that
removing regions containing Top-K keypoints significantly degrades their
registration accuracy. We further validate the attack's impact on the Autoware
autonomous driving platform, where hiding merely a few critical regions induces
noticeable localization drift. Finally, we extended our attacks to the physical
world by hiding critical regions with near-infrared absorptive materials,
thereby successfully replicate the attack effects observed in KITTI data. This
step has been closer toward the realistic physical-world attack that
demonstrate the veracity and generality of our proposal.

</details>


### [31] [Exploring Spectral Characteristics for Single Image Reflection Removal](https://arxiv.org/abs/2509.12627)
*Pengbo Guo,Chengxu Liu,Guoshuai Zhao,Xingsong Hou,Jialie Shen,Xueming Qian*

Main category: cs.CV

TL;DR: 本文提出了一种基于光谱学习的新方法，通过重建反射图像的光谱信息，提升图像反射去除效果。


<details>
  <summary>Details</summary>
Motivation: 以往的反射去除方法主要在图像域操作，忽视了反射光的光谱属性差异，导致难以有效区分反射和真实内容。作者希望通过利用反射和透射成分在光谱上的差异，提高反射去除的准确性。

Method: 提出了光谱码本（Spectral Codebook）来重建反射图像的光谱信息，通过感知不同光源在光谱上的波长差异，有效区分反射。在此基础上，设计了两个光谱先验细化模块：一种在空间维度重新分布像素，另一种在波长维度自适应增强光谱差异。此外，引入了Spectrum-Aware Transformer，联合恢复光谱域和像素域的真实内容。

Result: 在三个主流反射数据集上的实验表明，该方法在反射去除任务上优于现有最先进的模型，具有更强的泛化能力。

Conclusion: 基于光谱重构和光谱先验强化的反射去除方法能够更有效地区分和去除反射，提升图像还原质量，证明了结合光谱特征的重要价值。

Abstract: Eliminating reflections caused by incident light interacting with reflective
medium remains an ill-posed problem in the image restoration area. The primary
challenge arises from the overlapping of reflection and transmission components
in the captured images, which complicates the task of accurately distinguishing
and recovering the clean background. Existing approaches typically address
reflection removal solely in the image domain, ignoring the spectral property
variations of reflected light, which hinders their ability to effectively
discern reflections. In this paper, we start with a new perspective on spectral
learning, and propose the Spectral Codebook to reconstruct the optical spectrum
of the reflection image. The reflections can be effectively distinguished by
perceiving the wavelength differences between different light sources in the
spectrum. To leverage the reconstructed spectrum, we design two spectral prior
refinement modules to re-distribute pixels in the spatial dimension and
adaptively enhance the spectral differences along the wavelength dimension.
Furthermore, we present the Spectrum-Aware Transformer to jointly recover the
transmitted content in spectral and pixel domains. Experimental results on
three different reflection benchmarks demonstrate the superiority and
generalization ability of our method compared to state-of-the-art models.

</details>


### [32] [Maps for Autonomous Driving: Full-process Survey and Frontiers](https://arxiv.org/abs/2509.12632)
*Pengxin Chen,Zhipeng Luo,Xiaoqi Jiang,Zhangcai Yin,Jonathan Li*

Main category: cs.CV

TL;DR: 本文综述了自动驾驶地图从高精度地图（HD）、轻量级地图（Lite）到隐式地图（Implicit）的发展历程，并探讨了前沿研究及集成方式。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对高效、精准的地图有极高要求。随着技术进步，传统地图逐渐难以满足实时性、灵活性和泛化能力需求，因此有必要系统梳理和总结新型地图的演变、挑战及其解决方案。

Method: 作者将自动驾驶地图的发展分为三个阶段：高精度地图、轻量级地图和隐式地图。对每一阶段的地图生产流程进行了全面回顾，总结了面临的技术挑战，归纳了学界提出的解决办法。同时，分析了最新的地图表示研究进展，并探讨其与端到端自动驾驶结合的可能性。

Result: 系统地归纳了三种类型地图的生产方法、技术难点及已有解决方案。总结了学界在新型地图表示上的最新研究，以及多种创新方法集成到端到端自动驾驶系统的方案。

Conclusion: 本文全面回顾与分析了自动驾驶地图的演变历程及其面临的难题，指出隐式地图与端到端框架结合是未来趋势，为自动驾驶地图的研究和应用提供了参考框架和方向。

Abstract: Maps have always been an essential component of autonomous driving. With the
advancement of autonomous driving technology, both the representation and
production process of maps have evolved substantially. The article categorizes
the evolution of maps into three stages: High-Definition (HD) maps, Lightweight
(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.
Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.

</details>


### [33] [CIARD: Cyclic Iterative Adversarial Robustness Distillation](https://arxiv.org/abs/2509.12633)
*Liming Lu,Shuchao Pang,Xu Zheng,Xiang Gu,Anan Du,Yunhuai Liu,Yongbin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗鲁棒性蒸馏（ARD）方法，即Cyclic Iterative ARD（CIARD），有效提升学生模型的对抗鲁棒性并减少在干净样本上的性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有ARD方法虽然提升了学生模型的对抗鲁棒性，但常导致其在干净样本上的性能下降。主要成因在于（1）双教师模型（分别专注于干净和对抗鲁棒性）间优化目标不一致，阻碍知识有效传递；（2）训练过程中的迭代生成对抗样本，使鲁棒教师模型性能恶化。

Method: 本文提出CIARD方法，包含两个创新：一是采用多教师框架并引入对比推挤损失对齐，化解双教师优化目标冲突；二是持续对抗再训练，动态提升教师模型鲁棒性，缓解对抗训练带来的性能退化。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet数据集上的大量实验表明，CIARD方法对比现有方法在多种攻击场景下平均提升3.53的对抗防御率，在干净样本准确率方面提升5.87。

Conclusion: CIARD方法在学生模型的鲁棒性和泛化能力上实现了显著提升，为小型模型在资源受限场景下的安全应用提供了新的技术基线。

Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance
and robustness from teacher model to lightweight student model, enabling
resilient performance on resource-constrained scenarios. Though existing ARD
approaches enhance student model's robustness, the inevitable by-product leads
to the degraded performance on clean examples. We summarize the causes of this
problem inherent in existing methods with dual-teacher framework as: 1. The
divergent optimization objectives of dual-teacher models, i.e., the clean and
robust teachers, impede effective knowledge transfer to the student model, and
2. The iteratively generated adversarial examples during training lead to
performance deterioration of the robust teacher model. To address these
challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key
innovations: a. A multi-teacher framework with contrastive push-loss alignment
to resolve conflicts in dual-teacher optimization objectives, and b. Continuous
adversarial retraining to maintain dynamic teacher robustness against
performance degradation from the varying adversarial examples. Extensive
experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD
achieves remarkable performance with an average 3.53 improvement in adversarial
defense rates across various attack scenarios and a 5.87 increase in clean
sample accuracy, establishing a new benchmark for balancing model robustness
and generalization. Our code is available at https://github.com/eminentgu/CIARD

</details>


### [34] [Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations](https://arxiv.org/abs/2509.12653)
*Jinjie Shen,Yaxiong Wang,Lechao Cheng,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出了首个语义对齐的多模态内容操控检测与定位方法，并构建了与实际语义一致性欺骗更接近的新数据集，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 目前多模态数据操控检测的基准数据存在不真实的跨模态失配，现实中的攻击常常保持语义一致，现有数据集和方法无法有效反映和检测这类真实操控。

Method: 作者构建了全新的SAMM数据集，先用最新的图像操控技术修改图像，再自动生成语义一致的文本配对。随后提出RamDG框架，利用外部知识库检索语境信息，结合输入进行伪造区域定位和深度操控检测。

Result: 大量实验表明，RamDG在新的SAMM数据集上检测准确率比现有最佳方法提升2.06%。

Conclusion: 所提数据集和方法提升了多模态真实语义一致性操控检测的能力，为媒体取证领域带来更有现实意义的评测工具和技术。

Abstract: The detection and grounding of manipulated content in multimodal data has
emerged as a critical challenge in media forensics. While existing benchmarks
demonstrate technical progress, they suffer from misalignment artifacts that
poorly reflect real-world manipulation patterns: practical attacks typically
maintain semantic consistency across modalities, whereas current datasets
artificially disrupt cross-modal alignment, creating easily detectable
anomalies. To bridge this gap, we pioneer the detection of
semantically-coordinated manipulations where visual edits are systematically
paired with semantically consistent textual descriptions. Our approach begins
with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)
dataset, generated through a two-stage pipeline: 1) applying state-of-the-art
image manipulations, followed by 2) generation of contextually-plausible
textual narratives that reinforce the visual deception. Building on this
foundation, we propose a Retrieval-Augmented Manipulation Detection and
Grounding (RamDG) framework. RamDG commences by harnessing external knowledge
repositories to retrieve contextual evidence, which serves as the auxiliary
texts and encoded together with the inputs through our image forgery grounding
and deep manipulation detection modules to trace all manipulations. Extensive
experiments demonstrate our framework significantly outperforms existing
methods, achieving 2.06\% higher detection accuracy on SAMM compared to
state-of-the-art approaches. The dataset and code are publicly available at
https://github.com/shen8424/SAMM-RamDG-CAP.

</details>


### [35] [MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization](https://arxiv.org/abs/2509.12673)
*YiTong Liu,TianZhu Liu,YanFeng GU*

Main category: cs.CV

TL;DR: 该论文提出了一种用于跨视角地理定位的新方法MFAF，通过新颖的频率注意力机制显著提升了在无人机和导航任务中的定位性能。


<details>
  <summary>Details</summary>
Motivation: 以往方法在提取图像特征时忽视了空间和语义信息，难以应对不同视角下显著的外观差异，导致特征表达区分度不足。为了解决这个问题，作者希望设计一种能更好捕获不同视角下结构和细节特征的方法。

Method: 提出了基于EVA02的多尺度频率注意力融合（MFAF）方法。该方法包含多频分支块（MFB）和频率感知空间注意力（FSA）模块。MFB能够多尺度捕捉低频结构特征和高频边缘细节，提升特征的一致性与鲁棒性。FSA模块自适应关注在关键频率区域，减弱背景噪声和视角变化的干扰。

Result: 在三个主流基准数据集University-1652、SUES-200和Dense-UAV上进行了大量对比实验。结果显示MFAF方法在无人机定位和导航等任务中表现出竞争力。

Conclusion: MFAF方法通过引入多频率分支与频率感知空间注意力，有效提升了跨视角图像地理定位的准确性和鲁棒性，在相关领域具有很好的应用前景。

Abstract: Cross-view geo-localization aims to determine the geographical location of a
query image by matching it against a gallery of images. This task is
challenging due to the significant appearance variations of objects observed
from variable views, along with the difficulty in extracting discriminative
features. Existing approaches often rely on extracting features through feature
map segmentation while neglecting spatial and semantic information. To address
these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion
(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block
(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block
effectively captures both low-frequency structural features and high-frequency
edge details across multiple scales, improving the consistency and robustness
of feature representations across various viewpoints. Meanwhile, the FSA module
adaptively focuses on the key regions of frequency features, significantly
mitigating the interference caused by background noise and viewpoint
variability. Extensive experiments on widely recognized benchmarks, including
University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method
achieves competitive performance in both drone localization and drone
navigation tasks.

</details>


### [36] [A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks](https://arxiv.org/abs/2509.12682)
*Gordon Hung,Ivan Felipe Rodriguez*

Main category: cs.CV

TL;DR: 本文评估了YOLO系列最新版本在水下图像上的表现，发现YOLOv10在速度和精度间取得最佳平衡，并提供了开放的基准数据和代码。


<details>
  <summary>Details</summary>
Motivation: 随着自主水下机器人（AUV）越来越依赖计算机视觉执行栖息地映射、生态监测和基础设施检查等任务，提升在水下复杂环境下的物体检测能力变得尤为重要。传统YOLO模型多在陆地数据集上评测，缺乏水域环境下系统比较。

Method: 作者整理了两组开放水下数据集（珊瑚疾病、鱼类种类），设置不同的训练规模，对YOLOv8-s、v9-s、v10-s、v11-s四个版本进行统一参数训练，并评估其精度、召回率、mAP等，同时用Grad-CAM分析特征利用和定位效果。

Result: 实验显示，无论在珊瑚或鱼类数据集上，YOLOv10相较前代在速度和精度上达到最优平衡，整体检测精度自YOLOv9后趋于饱和，最新版本主要优化推理效率。

Conclusion: 轻量的YOLOv10非常适合嵌入式AUV部署，本文提供了首个水下YOLO系统对比结果，公开基准和代码，有助于推动水下视觉领域发展。

Abstract: Autonomous underwater vehicles (AUVs) increasingly rely on on-board
computer-vision systems for tasks such as habitat mapping, ecological
monitoring, and infrastructure inspection. However, underwater imagery is
hindered by light attenuation, turbidity, and severe class imbalance, while the
computational resources available on AUVs are limited. One-stage detectors from
the YOLO family are attractive because they fuse localization and
classification in a single, low-latency network; however, their terrestrial
benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how
successive YOLO releases perform in the marine domain. We curate two openly
available datasets that span contrasting operating conditions: a Coral Disease
set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20
classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %,
100 % of the images) while keeping balanced validation and test partitions
fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical
hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate
precision, recall, mAP50, mAP50-95, per-image inference time, and
frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature
utilization and localization faithfulness. Across both datasets, accuracy
saturates after YOLOv9, suggesting architectural innovations primarily target
efficiency rather than accuracy. Inference speed, however, improves markedly.
Our results (i) provide the first controlled comparison of recent YOLO variants
on underwater imagery, (ii) show that lightweight YOLOv10 offers the best
speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an
open, reproducible benchmark and codebase to accelerate future marine-vision
research.

</details>


### [37] [StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo](https://arxiv.org/abs/2509.12683)
*Xianda Guo,Chenming Zhang,Ruilin Wang,Youmin Zhang,Wenzhao Zheng,Matteo Poggi,Hao Zhao,Qin Zou,Long Chen*

Main category: cs.CV

TL;DR: StereoCarla 是一个基于 CARLA 模拟器、高保真的合成双目视觉数据集，涵盖丰富的相机配置和环境条件，可显著提升自动驾驶深度感知模型的泛化能力。其上训练的模型在多个基准测试中优于以往 11 个数据集，适用于多数据集联合训练。


<details>
  <summary>Details</summary>
Motivation: 目前学习型双目匹配算法在自动驾驶和机器人领域取得很大进步，但现有训练数据集多样性有限，导致模型泛化能力受限。为解决这一问题，作者希望通过构建更丰富、真实可控的大规模合成数据集，提升算法在不同场景下的性能。

Method: 作者基于 CARLA 仿真平台，生成了包含多样镜头基线、视角、传感器布局，以及变化光照、天气和道路几何的高质量双目数据集 StereoCarla，并在此基础上进行了跨域实验，将其与其它 11 个已有数据集的训练表现进行对比。

Result: 用 StereoCarla 训练的模型在 KITTI2012、KITTI2015、Middlebury、ETH3D 四项基准上表现出更强泛化性，超越了以往主流数据集训练的模型；StereoCarla 融入多数据集联合训练时还能进一步提升性能，显示其扩展性与兼容性。

Conclusion: StereoCarla 为双目视觉深度感知算法发展提供了一个更贴近实际、易于扩展和控制的高质量测试与训练数据基准，有助于推动自动驾驶车辆深度感知系统的鲁棒性提升与真实部署。

Abstract: Stereo matching plays a crucial role in enabling depth perception for
autonomous driving and robotics. While recent years have witnessed remarkable
progress in stereo matching algorithms, largely driven by learning-based
methods and synthetic datasets, the generalization performance of these models
remains constrained by the limited diversity of existing training data. To
address these challenges, we present StereoCarla, a high-fidelity synthetic
stereo dataset specifically designed for autonomous driving scenarios. Built on
the CARLA simulator, StereoCarla incorporates a wide range of camera
configurations, including diverse baselines, viewpoints, and sensor placements
as well as varied environmental conditions such as lighting changes, weather
effects, and road geometries. We conduct comprehensive cross-domain experiments
across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury,
ETH3D) and demonstrate that models trained on StereoCarla outperform those
trained on 11 existing stereo datasets in terms of generalization accuracy
across multiple benchmarks. Furthermore, when integrated into multi-dataset
training, StereoCarla contributes substantial improvements to generalization
accuracy, highlighting its compatibility and scalability. This dataset provides
a valuable benchmark for developing and evaluating stereo algorithms under
realistic, diverse, and controllable settings, facilitating more robust depth
perception systems for autonomous vehicles. Code can be available at
https://github.com/XiandaGuo/OpenStereo, and data can be available at
https://xiandaguo.net/StereoCarla.

</details>


### [38] [SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes](https://arxiv.org/abs/2509.12701)
*Wenzhuo Jin,Qianfeng Yang,Xianhao Wu,Hongming Chen,Pengpeng Li,Xiang Chen*

Main category: cs.CV

TL;DR: 论文提出了一个用于火灾早期监控场景下图像去烟的真实世界数据集SmokeBench，并基于该数据集评测了多种去烟算法。


<details>
  <summary>Details</summary>
Motivation: 火灾初期监控画面受烟雾严重影响，降低了紧急应对效率；当前缺乏真实且配对的去烟数据集，限制了算法的发展和评估。

Method: 构建并公开了SmokeBench数据集，包含多种场景及烟雾浓度下，精确对齐的含烟与无烟图像对，并以该数据集全面评测了现有多种去烟方法。

Result: SmokeBench为多类去烟算法提供了真实、严格的测试基础，支持了有监督学习和准确的算法评估。

Conclusion: SmokeBench数据集将推动真实火灾场景下的图像去烟算法研究，助力相关算法的实用化和鲁棒性提升。

Abstract: Early-stage fire scenes (0-15 minutes after ignition) represent a crucial
temporal window for emergency interventions. During this stage, the smoke
produced by combustion significantly reduces the visibility of surveillance
systems, severely impairing situational awareness and hindering effective
emergency response and rescue operations. Consequently, there is an urgent need
to remove smoke from images to obtain clear scene information. However, the
development of smoke removal algorithms remains limited due to the lack of
large-scale, real-world datasets comprising paired smoke-free and
smoke-degraded images. To address these limitations, we present a real-world
surveillance image desmoking benchmark dataset named SmokeBench, which contains
image pairs captured under diverse scenes setup and smoke concentration. The
curated dataset provides precisely aligned degraded and clean images, enabling
supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of desmoking methods on our dataset. Our
dataset provides a valuable foundation for advancing robust and practical image
desmoking in real-world fire scenes. This dataset has been released to the
public and can be downloaded from https://github.com/ncfjd/SmokeBench.

</details>


### [39] [RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation](https://arxiv.org/abs/2509.12710)
*Siju Ma,Changsiyu Gong,Xiaofeng Fan,Yong Ma,Chengjie Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种结合语言与图像信息的红外与可见光图像融合新方法，通过文本指导提升融合效果，并显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本驱动的红外-可见光图像融合方法，缺乏针对文本对融合效果贡献的监督与评估机制。作者发现图像指代分割（RIS）与文本驱动融合有共同目标，因此尝试将二者结合，通过更明确的目标实现更有效的融合。

Method: 提出了RIS-FUSION级联框架，将图像融合与指代分割通过联合优化统一起来。关键创新点在于LangGatedFusion模块，把语言特征有效地注入融合主干网络，实现语义上的精确对齐。此外，作者还构建了MM-RIS大规模多模态指代分割基准，包括大量带标注的红外—可见光图像对、分割掩码和文本指代。

Result: 在MM-RIS基准上，RIS-FUSION取得了超过11%的mIoU提升，显著领先于现有方法，实验充分验证了方法的有效性。

Conclusion: 通过联合图像融合与指代分割任务，并利用文本引导融合过程，RIS-FUSION实现了更优的性能，为多模态智能感知领域提供了新思路。

Abstract: Text-driven infrared and visible image fusion has gained attention for
enabling natural language to guide the fusion process. However, existing
methods lack a goal-aligned task to supervise and evaluate how effectively the
input text contributes to the fusion outcome. We observe that referring image
segmentation (RIS) and text-driven fusion share a common objective:
highlighting the object referred to by the text. Motivated by this, we propose
RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint
optimization. At its core is the LangGatedFusion module, which injects textual
features into the fusion backbone to enhance semantic alignment. To support
multimodal referring image segmentation task, we introduce MM-RIS, a
large-scale benchmark with 12.5k training and 3.5k testing triplets, each
consisting of an infrared-visible image pair, a segmentation mask, and a
referring expression. Extensive experiments show that RIS-FUSION achieves
state-of-the-art performance, outperforming existing methods by over 11% in
mIoU. Code and dataset will be released at
https://github.com/SijuMa2003/RIS-FUSION.

</details>


### [40] [Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2509.12711)
*Haozhe Zhang,Chenchen Jing,Mingyu Liu,Qingsheng Wang,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的Debiased Feature Augmentation (DeFA)方法，通过引入去偏置的特征增强技术，提升了组合零样本学习在未知属性-物体组合识别上的表现。


<details>
  <summary>Details</summary>
Motivation: 组合零样本学习(CZSL)由于属性和物体特征纠缠以及真实数据中长尾分布问题，导致泛化能力有限。受到神经科学中想象与感知过程相似性的启发，作者试图通过模拟新组合特征来提升模型泛化能力。

Method: 作者设计了一个DeFA方法，结合了解耦重构的特征增强框架和去偏置策略。DeFA能够利用已知属性和物体的先验知识，合成高质量的组合特征，辅助模型学习更具泛化能力的组合表征。

Result: 在三个主流数据集上的大量实验表明，DeFA在传统闭世界及开放世界设置下均达到了最新的性能水平，显著优于现有方法。

Conclusion: 作者证明了DeFA方法在组合零样本学习任务中的有效性和优越性，为处理属性-物体组合泛化提供了一条可行的新思路。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen
attribute-object compositions by learning prior knowledge of seen primitives,
\textit{i.e.}, attributes and objects. Learning generalizable compositional
representations in CZSL remains challenging due to the entangled nature of
attributes and objects as well as the prevalence of long-tailed distributions
in real-world data. Inspired by neuroscientific findings that imagination and
perception share similar neural processes, we propose a novel approach called
Debiased Feature Augmentation (DeFA) to address these challenges. The proposed
DeFA integrates a disentangle-and-reconstruct framework for feature
augmentation with a debiasing strategy. DeFA explicitly leverages the prior
knowledge of seen attributes and objects by synthesizing high-fidelity
composition features to support compositional generalization. Extensive
experiments on three widely used datasets demonstrate that DeFA achieves
state-of-the-art performance in both \textit{closed-world} and
\textit{open-world} settings.

</details>


### [41] [AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models](https://arxiv.org/abs/2509.12715)
*Heng Zhang,Haichuan Hu,Yaomin Shen,Weihao Yu,Yilei Yuan,Haochen You,Guo Cheng,Zijian Zhang,Lubin Gan,Huihui Wei,Hao Zhang,Jin Huang*

Main category: cs.CV

TL;DR: 本文提出了AsyMoE，一种为视觉-语言大模型设计的新型专家混合架构，能更好处理视觉与语言信息不对称，提高多模态任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的专家混合（MoE）方法在视觉-语言模型中遇到困难，原因在于视觉和语言处理方式的差异——视觉信息是空间完整的，而语言需要序列上下文。深入分析发现语言专家在深层对上下文的依赖减少，更多根据参数化知识推理，限制了模型跨模态能力。

Method: 作者提出AsyMoE架构，引入三组专家：针对单一模态处理的intra-modality专家、进行层级式跨模态交互的双曲型inter-modality专家、保障语言上下文且抑制参数偏见的evidence-priority语言专家，专门应对视觉-语言处理的不对称性。

Result: 实验表明，AsyMoE在准确率上比传统MoE和模态特定MoE分别提升26.58%和15.45%，同时激活参数量比稠密模型减少25.45%。

Conclusion: AsyMoE更高效地利用视觉和语言信息，在多模态任务上显著优于现有MoE方法，为LVLM发展提供新思路。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on multimodal tasks through scaled architectures and extensive training.
However, existing Mixture of Experts (MoE) approaches face challenges due to
the asymmetry between visual and linguistic processing. Visual information is
spatially complete, while language requires maintaining sequential context. As
a result, MoE models struggle to balance modality-specific features and
cross-modal interactions. Through systematic analysis, we observe that language
experts in deeper layers progressively lose contextual grounding and rely more
on parametric knowledge rather than utilizing the provided visual and
linguistic information. To address this, we propose AsyMoE, a novel
architecture that models this asymmetry using three specialized expert groups.
We design intra-modality experts for modality-specific processing, hyperbolic
inter-modality experts for hierarchical cross-modal interactions, and
evidence-priority language experts to suppress parametric biases and maintain
contextual grounding. Extensive experiments demonstrate that AsyMoE achieves
26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific
MoE respectively, with 25.45% fewer activated parameters than dense models.

</details>


### [42] [EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer](https://arxiv.org/abs/2509.12718)
*Pukun Zhao,Longxiang Wang,Miaowei Wang,Chen Chen,Fanqing Zhou,Haojian Huang*

Main category: cs.CV

TL;DR: 本文提出了两个动态空间基准，用于系统评估模型在局部可观测环境中的空间理解和自适应规划能力，揭示了主流模型在动态空间推理和长期记忆方面的局限。


<details>
  <summary>Details</summary>
Motivation: 当前空间推理基准多关注静态或全局可观测环境，无法反映部分可观测和动态变化下长时序推理和记忆利用的挑战。为推动空间推理和记忆机制相关方法的发展，作者设计了新的动态空间基准。

Method: 作者设计了两个动态空间基准：局部可观测迷宫导航和配对消除（match-2 elimination），每个决策都带来环境结构的变化，需要模型持续更新认知和策略。还提出了一种基于主观经验的记忆机制，用于跨任务经验迁移与验证。

Result: 实验结果显示，提出的基准充分暴露了主流模型在动态空间推理和长期记忆能力上的不足。

Conclusion: 新基准与记忆机制为空间推理和长期记忆模型的未来研究提供了系统性平台，有助于发现并应对现有方法的短板，推动该领域方法学进步。

Abstract: Most existing spatial reasoning benchmarks focus on static or globally
observable environments, failing to capture the challenges of long-horizon
reasoning and memory utilization under partial observability and dynamic
changes. We introduce two dynamic spatial benchmarks, locally observable maze
navigation and match-2 elimination that systematically evaluate models'
abilities in spatial understanding and adaptive planning when local perception,
environment feedback, and global objectives are tightly coupled. Each action
triggers structural changes in the environment, requiring continuous update of
cognition and strategy. We further propose a subjective experience-based memory
mechanism for cross-task experience transfer and validation. Experiments show
that our benchmarks reveal key limitations of mainstream models in dynamic
spatial reasoning and long-term memory, providing a comprehensive platform for
future methodological advances. Our code and data are available at
https://anonymous.4open.science/r/EvoEmpirBench-143C/.

</details>


### [43] [SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation](https://arxiv.org/abs/2509.12721)
*Jingdong Zhang,Weikai Chen,Yuan Liu,Jionghao Wang,Zhengming Yu,Zhuowen Shen,Bo Yang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 论文提出了一种新型单视图3D生成方法SPGen，通过球面投影（Spherical Projection）将几何信息转化为结构化的多层2D图像表示，并利用2D扩散模型生成3D对象，解决了多视角不一致性及复杂内部结构重建难题。实验显示，该方法在几何质量和计算效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单视图3D生成方法依赖多视图扩散先验，常出现视角不一致、难以还原复杂内部结构或特殊拓扑（如环形等）的问题。为解决这些瓶颈，作者动机是在单视图下提升3D重建的真实性、一致性和结构表达能力。

Method: SPGen方法将物体三维表面投影到一个包围球上，通过一对一函数变换展开为多层2D球面投影图，再在纯图像域内进行扩散模型生成。这种结构化表示支持嵌套内部结构表达，模型直接利用成熟的2D扩散技术，且资源消耗少，易于微调。

Result: 实验结果显示，SPGen在重建几何质量上大幅优于现有主流基线模型，尤其在表达复杂内部结构、拓扑以及不同视角一致性方面效果突出。同时，该方法在计算资源和效率上表现良好。

Conclusion: SPGen为单视图3D重建提供了一种创新、高效和有效的新范式，通过引入结构化球面投影和2D扩散模型，解决了多视角不一致和复杂结构表达难题，有潜力推广到更广泛的3D重建应用场景中。

Abstract: Existing single-view 3D generative models typically adopt multiview diffusion
priors to reconstruct object surfaces, yet they remain prone to inter-view
inconsistencies and are unable to faithfully represent complex internal
structure or nontrivial topologies. In particular, we encode geometry
information by projecting it onto a bounding sphere and unwrapping it into a
compact and structural multi-layer 2D Spherical Projection (SP) representation.
Operating solely in the image domain, SPGen offers three key advantages
simultaneously: (1) Consistency. The injective SP mapping encodes surface
geometry with a single viewpoint which naturally eliminates view inconsistency
and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal
structures and support direct lifting to watertight or open 3D surfaces; (3)
Efficiency. The image-domain formulation allows the direct inheritance of
powerful 2D diffusion priors and enables efficient finetuning with limited
computational resources. Extensive experiments demonstrate that SPGen
significantly outperforms existing baselines in geometric quality and
computational efficiency.

</details>


### [44] [Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models](https://arxiv.org/abs/2509.12724)
*Yunhan Zhao,Xiang Zheng,Xingjun Ma*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型（VLMs）的新型越狱攻击方法Defense2Attack，显著提高了越狱的有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在许多任务上表现卓越，但容易受到越狱攻击，现有攻击手法尚存在效率和有效性不足的问题。因此，作者旨在探索并提升VLMs越狱攻击的效果。

Method: 作者发现将弱防御机制融入攻击流程能有效提升越狱表现。基于此，提出Defense2Attack方法，包括三部分：1）嵌入肯定与鼓励语义的视觉扰动优化器；2）基于防御风格改写输入的文本优化器；3）通过强化微调生成红队后缀优化越狱攻击。

Result: 在四个VLM和四个安全基准上进行实证评估，Defense2Attack在越狱有效性和尝试次数上均优于当前最优方法，首次尝试即能显著突破防护。

Conclusion: Defense2Attack为绕过视觉语言模型的安全防护提供了新视角，也推动了模型安全领域对于越狱攻击机制的进一步理解。

Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been
shown to be vulnerable to jailbreak attacks. While recent jailbreaks have
achieved notable progress, their effectiveness and efficiency can still be
improved. In this work, we reveal an interesting phenomenon: incorporating weak
defense into the attack pipeline can significantly enhance both the
effectiveness and the efficiency of jailbreaks on VLMs. Building on this
insight, we propose Defense2Attack, a novel jailbreak method that bypasses the
safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak
prompt design. Specifically, Defense2Attack consists of three key components:
(1) a visual optimizer that embeds universal adversarial perturbations with
affirmative and encouraging semantics; (2) a textual optimizer that refines the
input using a defense-styled prompt; and (3) a red-team suffix generator that
enhances the jailbreak through reinforcement fine-tuning. We empirically
evaluate our method on four VLMs and four safety benchmarks. The results
demonstrate that Defense2Attack achieves superior jailbreak performance in a
single attempt, outperforming state-of-the-art attack methods that often
require multiple tries. Our work offers a new perspective on jailbreaking VLMs.

</details>


### [45] [Effective Gaussian Management for High-fidelity Object Reconstruction](https://arxiv.org/abs/2509.12742)
*Jiateng Liu,Hao Gao,Jiu-Cheng Xie,Chi-Man Pun,Jian Xiong,Haolun Li,Feng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的高斯管理方法，用于高保真目标重建，在提升重建质量的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯投影方法在属性分配上较为粗放，导致重建过程存在梯度冲突，影响精度和参数效率，因此需要一种更智能的高斯管理机制。

Method: 提出新的致密化策略，动态激活球谐函数(SHs)或法线，并由表面重建模块监督以缓解多重监督带来的梯度冲突。采用轻量级高斯表示法：根据梯度自适应地调整SH阶数，并通过任务解耦修剪策略移除对重建影响最小的高斯点，综合优化表示能力与参数数量。

Result: 实验证明，该方法在重建质量和效率上均优于现有方法，以更少的参数实现更好表现。

Conclusion: 所提出的高斯管理方法通用性强，可集成至其他框架中，有效提升性能并减小模型规模，为高效三维重建问题提供了新方向。

Abstract: This paper proposes an effective Gaussian management approach for
high-fidelity object reconstruction. Departing from recent Gaussian Splatting
(GS) methods that employ indiscriminate attribute assignment, our approach
introduces a novel densification strategy that dynamically activates spherical
harmonics (SHs) or normals under the supervision of a surface reconstruction
module, which effectively mitigates the gradient conflicts caused by dual
supervision and achieves superior reconstruction results. To further improve
representation efficiency, we develop a lightweight Gaussian representation
that adaptively adjusts the SH orders of each Gaussian based on gradient
magnitudes and performs task-decoupled pruning to remove Gaussian with minimal
impact on a reconstruction task without sacrificing others, which balances the
representational capacity with parameter quantity. Notably, our management
approach is model-agnostic and can be seamlessly integrated into other
frameworks, enhancing performance while reducing model size. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art approaches in both reconstruction quality and efficiency,
achieving superior performance with significantly fewer parameters.

</details>


### [46] [Modelling and analysis of the 8 filters from the "master key filters hypothesis" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory](https://arxiv.org/abs/2509.12746)
*Tony Lindeberg,Zahra Babaiee,Peyman M. Kiasari*

Main category: cs.CV

TL;DR: 本文分析了基于ConvNeXt架构的depthwise-separable深度网络中学习到的感受野（receptive fields），并通过聚类方法提取出8个“主关键滤波器”。结果发现这些滤波器可被离散高斯核的尺度空间模型很好地拟合和近似。


<details>
  <summary>Details</summary>
Motivation: 目前对深度网络中卷积核实际学习到的空间特性理解有限，尤其是在depthwise-separable结构下。作者希望揭示这些滤波器的属性，并探索是否能用理想化的数学模型逼近，从而提升对网络结构的认知和可解释性。

Method: 作者先度量学习到的滤波器空间扩展性（用加权均值和方差），用聚类方法提8个“主关键滤波器”。接着，尝试用离散高斯核加上差分算子两种方式（各方向尺度参数相同或不同）建立理想化模型，并通过方差匹配或$l_1$/$l_2$最小化对模型拟合。最后通过实验分析这些模型替换真实滤波器时的效果。

Result: 理想化的尺度空间模型与实际学习到的滤波器形状和分布特性高度吻合。在深度网络中用这些模型滤波器替换训练得到的滤波器，表现出良好的可预测性和适用性。

Conclusion: depthwise-separable深度网络学习得到的滤波器在空间分布上可以用离散尺度空间滤波器（如高斯核及其差分形式）良好建模，为相关网络的可解释性和模型简化提供了理论和实验依据。

Abstract: This paper presents the results of analysing and modelling a set of 8
``master key filters'', which have been extracted by applying a clustering
approach to the receptive fields learned in depthwise-separable deep networks
based on the ConvNeXt architecture.
  For this purpose, we first compute spatial spread measures in terms of
weighted mean values and weighted variances of the absolute values of the
learned filters, which support the working hypotheses that: (i) the learned
filters can be modelled by separable filtering operations over the spatial
domain, and that (ii) the spatial offsets of the those learned filters that are
non-centered are rather close to half a grid unit. Then, we model the clustered
``master key filters'' in terms of difference operators applied to a spatial
smoothing operation in terms of the discrete analogue of the Gaussian kernel,
and demonstrate that the resulting idealized models of the receptive fields
show good qualitative similarity to the learned filters.
  This modelling is performed in two different ways: (i) using possibly
different values of the scale parameters in the coordinate directions for each
filter, and (ii) using the same value of the scale parameter in both coordinate
directions. Then, we perform the actual model fitting by either (i) requiring
spatial spread measures in terms of spatial variances of the absolute values of
the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or
$l_2$-norms between the idealized receptive field models and the learned
filters.
  Complementary experimental results then demonstrate the idealized models of
receptive fields have good predictive properties for replacing the learned
filters by idealized filters in depthwise-separable deep networks, thus showing
that the learned filters in depthwise-separable deep networks can be well
approximated by discrete scale-space filters.

</details>


### [47] [What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment](https://arxiv.org/abs/2509.12750)
*Rishab Parthasarathy,Jasmine Collins,Cory Stephenson*

Main category: cs.CV

TL;DR: 论文对比了人类与多模态大模型（LLMs）在自动评估文本生成图像质量时所依赖的图像属性，并揭示了二者在判断标准和侧重点上的差异。


<details>
  <summary>Details</summary>
Motivation: 当前自动化评估生成式文本到图像模型的质量是一大挑战。虽有多模态LLMs参与评测，但其如何利用人类关心的图像属性（如风格、构图等）作出总体评判尚不清楚。本研究旨在深入理解LLMs与人类在图像评判时关注属性的异同。

Method: 研究者首先通过合成生成图像对，采集了人类关于图像偏好的数据集，并分析各图像质量属性间的人类判断相关性。之后，他们对LLMs进行了相同分析，并通过精控的合成数据集逐项考查各属性，比较LLMs与人类在单一属性判断上的表现。

Result: 人类评判各图像质量属性（如美学、无伪影、解剖准确性、构图、内容符合度与风格）时，这些属性间高度关联；而LLMs对这些属性间的相关性把控较弱。此外，对某些复杂属性（如解剖准确性），LLMs评判能力远逊于人类。

Conclusion: 人类和多模态LLMs在感知和评价图像之时，所依据的属性及联系存在显著差异。这些发现为未来文本到图像模型的自动化评估方法设计提供了参考，也提示需改进LLMs在复杂视觉理解方面的能力。

Abstract: Automated evaluation of generative text-to-image models remains a challenging
problem. Recent works have proposed using multimodal LLMs to judge the quality
of images, but these works offer little insight into how multimodal LLMs make
use of concepts relevant to humans, such as image style or composition, to
generate their overall assessment. In this work, we study what attributes of an
image--specifically aesthetics, lack of artifacts, anatomical accuracy,
compositional correctness, object adherence, and style--are important for both
LLMs and humans to make judgments on image quality. We first curate a dataset
of human preferences using synthetically generated image pairs. We use
inter-task correlation between each pair of image quality attributes to
understand which attributes are related in making human judgments. Repeating
the same analysis with LLMs, we find that the relationships between image
quality attributes are much weaker. Finally, we study individual image quality
attributes by generating synthetic datasets with a high degree of control for
each axis. Humans are able to easily judge the quality of an image with respect
to all of the specific image quality attributes (e.g. high vs. low aesthetic
image), however we find that some attributes, such as anatomical accuracy, are
much more difficult for multimodal LLMs to learn to judge. Taken together,
these findings reveal interesting differences between how humans and multimodal
LLMs perceive images.

</details>


### [48] [Recurrent Cross-View Object Geo-Localization](https://arxiv.org/abs/2509.12757)
*Xiaohan Zhang,Si-Yuan Cao,Xiaokai Bai,Yiming Li,Zhangkai Shen,Zhe Wu,Xiaoxi Hu,Hui-liang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨视域目标地理定位方法ReCOT，将任务从一次性检测转化为递归定位，大幅提升了准确率并减少了模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有的跨视域目标地理定位方法对特征噪声较为敏感、缺乏错误修正机制，导致定位精度受限，因此需要更鲁棒且高效的解决方案。

Method: 作者提出了ReCOT模型，将CVOGL建模为递归过程，引入可学习token用于整合查询图片和提示embedding，并通过多次关注参考特征不断优化预测位置。同时，加入了基于SAM的知识蒸馏策略（利用Segment Anything Model的分割先验，增强语义引导）和参考特征增强模块（采用层次化注意力，突出与目标相关区域）。

Result: ReCOT在主流CVOGL基准上取得了最新最优性能（SOTA），相较于此前最佳方法参数量减少了60%。

Conclusion: ReCOT突破了以往跨视域目标地理定位易受噪声干扰的问题，实现了更精确、高效的目标定位，对大规模实际应用具有重要意义。

Abstract: Cross-view object geo-localization (CVOGL) aims to determine the location of
a specific object in high-resolution satellite imagery given a query image with
a point prompt. Existing approaches treat CVOGL as a one-shot detection task,
directly regressing object locations from cross-view information aggregation,
but they are vulnerable to feature noise and lack mechanisms for error
correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object
geo-localization Transformer, which reformulates CVOGL as a recurrent
localization task. ReCOT introduces a set of learnable tokens that encode
task-specific intent from the query image and prompt embeddings, and
iteratively attend to the reference features to refine the predicted location.
To enhance this recurrent process, we incorporate two complementary modules:
(1) a SAM-based knowledge distillation strategy that transfers segmentation
priors from the Segment Anything Model (SAM) to provide clearer semantic
guidance without additional inference cost, and (2) a Reference Feature
Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize
object-relevant regions in the reference features. Extensive experiments on
standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art
(SOTA) performance while reducing parameters by 60% compared to previous SOTA
approaches.

</details>


### [49] [A-TDOM: Active TDOM via On-the-Fly 3DGS](https://arxiv.org/abs/2509.12759)
*Yiwei Xu,Xiang Wang,Yifei Yu,Wentian Gan,Luca Morelli,Giulio Perda,Xiongwu Xiao,Zongqian Zhan,Xin Wang,Fabio Remondino*

Main category: cs.CV

TL;DR: 该论文提出了一种基于实时3D高斯球优化的正射影像图（TDOM）生成方法A-TDOM，相比传统复杂离线流程，可实现接近实时的TDOM渲染，并在新图像采集后秒级优化，保持合理质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统TDOM生成需要复杂的离线光测处理流程，导致生成延迟，不利于实时应用；并且结果易受相机姿态、DSM精度及遮挡等问题影响，降低TDOM质量。

Method: 该方法结合On-the-Fly三维重建（SfM），每获取一张新影像就计算其姿态和稀疏点云，然后通过3D高斯球（3DGS）对新增区域优化集成，并结合正交投影渲染，实现每步更新后即时生成TDOM。

Result: 在多个数据集上初步实验，A-TDOM能以秒级优化时间，主动地接近实时渲染TDOM，同时保持较好的渲染质量和几何精度。

Conclusion: A-TDOM方法有效突破了TDOM传统生成的时效与质量瓶颈，适合对实时性有更高要求的城市管理、测绘等场景应用。

Abstract: True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in
various fields such as urban management, city planning, land surveying, etc.
However, traditional TDOM generation methods generally rely on a complex
offline photogrammetric pipeline, resulting in delays that hinder real-time
applications. Moreover, the quality of TDOM may degrade due to various
challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and
scene occlusions. To address these challenges, this work introduces A-TDOM, a
near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As
each image is acquired, its pose and sparse point cloud are computed via
On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously
unseen or coarsely reconstructed regions. By integrating with orthogonal
splatting, A-TDOM can render just after each update of a new 3DGS field.
Initial experiments on multiple benchmarks show that the proposed A-TDOM is
capable of actively rendering TDOM in near real-time, with 3DGS optimization
for each new image in seconds while maintaining acceptable rendering quality
and TDOM geometric accuracy.

</details>


### [50] [DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation](https://arxiv.org/abs/2509.12763)
*Yican Zhao,Ce Wang,You Hao,Lei Li,Tianli Liao*

Main category: cs.CV

TL;DR: DyGLNet是一种创新性的医学图像分割模型，能够高效融合全局与局部特征，并通过动态上采样实现更精确的分割，特别适用于边界及小目标。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临多尺度病变、模糊边界和高计算需求的挑战，现有方法往往难以兼顾分割精度与效率。

Method: 提出DyGLNet，设计了SHDCBlock模块（结合单头自注意力和多尺度空洞卷积）联合提取全局与局部特征，并引入学习型动态自适应上采样模块DyFusionUp以更好地重建特征图。此外，模型采用轻量化架构以降低计算消耗。

Result: 在七个公开数据集上实验，DyGLNet相较于现有方法有更优性能，特别是在边界准确性及小目标分割方面表现突出，并且具备更低的计算复杂度。

Conclusion: DyGLNet为临床医学图像分析提供了一种高效、可靠的分割解决方案，兼顾分割精度和推理效率。

Abstract: Medical image segmentation grapples with challenges including multi-scale
lesion variability, ill-defined tissue boundaries, and computationally
intensive processing demands. This paper proposes the DyGLNet, which achieves
efficient and accurate segmentation by fusing global and local features with a
dynamic upsampling mechanism. The model innovatively designs a hybrid feature
extraction module (SHDCBlock), combining single-head self-attention and
multi-scale dilated convolutions to model local details and global context
collaboratively. We further introduce a dynamic adaptive upsampling module
(DyFusionUp) to realize high-fidelity reconstruction of feature maps based on
learnable offsets. Then, a lightweight design is adopted to reduce
computational overhead. Experiments on seven public datasets demonstrate that
DyGLNet outperforms existing methods, particularly excelling in boundary
accuracy and small-object segmentation. Meanwhile, it exhibits lower
computation complexity, enabling an efficient and reliable solution for
clinical medical image analysis. The code will be made available soon.

</details>


### [51] [BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers](https://arxiv.org/abs/2509.12768)
*Mohammed Al-Habib,Zuping Zhang,Abdulrahman Noman*

Main category: cs.CV

TL;DR: 本文提出了一种适用于小样本学习的Vision Transformer（ViT）方法BATR-FST，通过双阶段的token自适应精炼，有效提升了ViT在小样本场景下的表现。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在小样本学习任务中表现受限，主要原因在于token级交互优化不足、训练数据有限和归纳偏置不强，现有方法的token匹配与相似性度量方式限制了上下文和局部信息的有效利用。

Method: 该方法分为两个阶段：预训练阶段利用Masked Image Modeling（MIM）增强ViT的patch-level特征泛化能力；元微调阶段引入Bi-Level Adaptive Token Refinement模块，包括token聚类、基于不确定性加权特征、双层注意力机制（平衡聚类内外关系），还结合了图token传播和类别分离惩罚，全面精炼token以提升类别区分度。

Result: 在三个主流小样本数据集上，BATR-FST在1-shot与5-shot任务中都获得了优于现有方法的性能表现。

Conclusion: BATR-FST增强了ViTs在小样本学习中的特征表达能力和归纳偏置，有效促进了Transformer结构在小样本分类上的应用。

Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision
applications. However, their performance in few-shot learning is limited by
challenges in refining token-level interactions, struggling with limited
training data, and developing a strong inductive bias. Existing methods often
depend on inflexible token matching or basic similarity measures, which limit
the effective incorporation of global context and localized feature refinement.
To address these challenges, we propose Bi-Level Adaptive Token Refinement for
Few-Shot Transformers (BATR-FST), a two-stage approach that progressively
improves token representations and maintains a robust inductive bias for
few-shot classification. During the pre-training phase, Masked Image Modeling
(MIM) provides Vision Transformers (ViTs) with transferable patch-level
representations by recreating masked image regions, providing a robust basis
for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates
a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to
capture localized interactions, Uncertainty-Aware Token Weighting to prioritize
dependable features, and a Bi-Level Attention mechanism to balance
intra-cluster and inter-cluster relationships, thereby facilitating thorough
token refinement. Furthermore, Graph Token Propagation ensures semantic
consistency between support and query instances, while a Class Separation
Penalty preserves different class borders, enhancing discriminative capability.
Extensive experiments on three benchmark few-shot datasets demonstrate that
BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and
improves the few-shot classification via transformers.

</details>


### [52] [CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT](https://arxiv.org/abs/2509.12777)
*Zhifang Gong,Shuo Gao,Ben Zhao,Yingjing Xu,Yijun Yang,Shenghong Ju,Guangquan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于多期增强CT的胰腺肿瘤亚型自动识别方法，通过引入Mamba模型对空间和时间信息进行联合建模，显著提升了亚型诊断性能。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤影像表现高度异质且变化大，现有方法未能有效挖掘多期增强CT的时空上下文信息，影响了诊断准确性，因此亟需更智能的自动化方法提升亚型识别效果。

Method: 首次提出利用Mamba模型，将多期增强CT数据进行空间和时间建模。方法包括双层次的增强感知Mamba模块，结合空间和时间采样序列，探索病灶内外期变化；引入相似度引导模块，突出具有显著时间变化的肿瘤区域；设计空间互补集成及多尺度融合模块，实现不同尺度的特征聚合。

Result: 在270例真实病例数据集上，模型区分胰腺导管腺癌（PDAC）与胰腺神经内分泌肿瘤（PNETs）取得了97.4%的准确率和98.6%的AUC。

Conclusion: 本文方法充分挖掘多期增强CT时空信息，能够更高效、精准地进行胰腺肿瘤亚型识别，显示出临床应用潜力。

Abstract: Contrast-enhanced computed tomography (CECT) is the primary imaging technique
that provides valuable spatial-temporal information about lesions, enabling the
accurate diagnosis and subclassification of pancreatic tumors. However, the
high heterogeneity and variability of pancreatic tumors still pose substantial
challenges for precise subtyping diagnosis. Previous methods fail to
effectively explore the contextual information across multiple CECT phases
commonly used in radiologists' diagnostic workflows, thereby limiting their
performance. In this paper, we introduce, for the first time, an automatic way
to combine the multi-phase CECT data to discriminate between pancreatic tumor
subtypes, among which the key is using Mamba with promising learnability and
simplicity to encourage both temporal and spatial modeling from multi-phase
CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware
Mamba module incorporating two novel spatial and temporal sampling sequences to
explore intra and inter-phase contrast variations of lesions. A
similarity-guided refinement module is also imposed into the temporal scanning
modeling to emphasize the learning on local tumor regions with more obvious
temporal variations. Moreover, we design the space complementary integrator and
multi-granularity fusion module to encode and aggregate the semantics across
different scales, achieving more efficient learning for subtyping pancreatic
tumors. The experimental results on an in-house dataset of 270 clinical cases
achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between
pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors
(PNETs), demonstrating its potential as a more accurate and efficient tool.

</details>


### [53] [Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection](https://arxiv.org/abs/2509.12784)
*Zhehao Li,Yucheng Qian,Chong Wang,Yinghao Lu,Zhihao Yang,Jiafei Wu*

Main category: cs.CV

TL;DR: 本文提出了一种用于人-物体交互（HOI）检测的新模型，通过融合情境化表示、工具辅助和可学习Prompt，提高对复杂交互的识别效果，并在主流数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统的HOI检测方法由于上下文建模不完整，对于复杂或依赖辅助实体（如工具）的交互识别效果有限。作者旨在解决这一不足，提高模型对多元和复杂场景的理解能力。

Method: 提出Contextualized Representation Learning Network，将可学习的contextual prompt（包含实例类别信息）与视觉特征融合，并采用注意力机制实现语言与图像内容的多层级对齐。同时，通过显式建模<人，工具，物体>三元组中的工具功能属性（affordance），识别如“filling”等工具依赖型的交互关系。

Result: 该方法在HICO-Det和V-COCO两个主流HOI检测数据集的大多数场景下均取得了领先的性能，优于过去最新的两阶段方法。

Conclusion: 通过引入工具辅助和情境化学习，模型能够更好理解复杂交互关系，有助于推动HOI检测技术向更丰富场景和精准识别发展。

Abstract: Human-Object Interaction (HOI) detection aims to simultaneously localize
human-object pairs and recognize their interactions. While recent two-stage
approaches have made significant progress, they still face challenges due to
incomplete context modeling. In this work, we introduce a Contextualized
Representation Learning Network that integrates both affordance-guided
reasoning and contextual prompts with visual cues to better capture complex
interactions. We enhance the conventional HOI detection framework by expanding
it beyond simple human-object pairs to include multivariate relationships
involving auxiliary entities like tools. Specifically, we explicitly model the
functional role (affordance) of these auxiliary objects through triplet
structures <human, tool, object>. This enables our model to identify
tool-dependent interactions such as 'filling'. Furthermore, the learnable
prompt is enriched with instance categories and subsequently integrated with
contextual visual features using an attention mechanism. This process aligns
language with image content at both global and regional levels. These
contextualized representations equip the model with enriched relational cues
for more reliable reasoning over complex, context-dependent interactions. Our
proposed method demonstrates superior performance on both the HICO-Det and
V-COCO datasets in most scenarios. Codes will be released upon acceptance.

</details>


### [54] [Double Helix Diffusion for Cross-Domain Anomaly Image Generation](https://arxiv.org/abs/2509.12787)
*Linchun Wu,Qin Zou,Xianbiao Qi,Bo Du,Zhongyuan Wang,Qingquan Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成方法DH-Diff，能高质量生成异常图像及其像素级标注，有效缓解异常样本稀缺问题，并极大提升异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前制造业视觉异常检测因真实异常样本稀缺而受限，现有生成方法存在生成图像结构不合理和特征纠缠问题，亟需更真实、标注一致的异常数据合成技术。

Method: 提出了Double Helix Diffusion（DH-Diff）框架，通过受双螺旋结构启发的特有架构，循环进行特征分离、连接与融合，结合了域解耦注意力机制减少图像与注释的特征纠缠，以及语义分数对齐确保结构一致性，同时支持文本提示与可选图形引导。

Result: 大量实验表明，DH-Diff在生成数据的多样性、真实性等方面大幅优于现有方法，并进一步显著提升了下游异常检测任务的性能。

Conclusion: DH-Diff能为制造业提供高质量的异常图像及标注合成能力，为异常检测任务带来实质性进步，是现有方法的重要突破。

Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the
scarcity of real anomaly samples for training robust detectors. Synthetic data
generation presents a viable strategy for data augmentation; however, current
methods remain constrained by two principal limitations: 1) the generation of
anomalies that are structurally inconsistent with the normal background, and 2)
the presence of undesirable feature entanglement between synthesized images and
their corresponding annotation masks, which undermines the perceptual realism
of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel
cross-domain generative framework designed to simultaneously synthesize
high-fidelity anomaly images and their pixel-level annotation masks, explicitly
addressing these challenges. DH-Diff employs a unique architecture inspired by
a double helix, cycling through distinct modules for feature separation,
connection, and merging. Specifically, a domain-decoupled attention mechanism
mitigates feature entanglement by enhancing image and annotation features
independently, and meanwhile a semantic score map alignment module ensures
structural authenticity by coherently integrating anomaly foregrounds. DH-Diff
offers flexible control via text prompts and optional graphical guidance.
Extensive experiments demonstrate that DH-Diff significantly outperforms
state-of-the-art methods in diversity and authenticity, leading to significant
improvements in downstream anomaly detection performance.

</details>


### [55] [Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation](https://arxiv.org/abs/2509.12791)
*Julien Walther,Rémi Giraud,Michaël Clément*

Main category: cs.CV

TL;DR: 本文提出了一种名为SPAM（SuperPixel Anything Model）的新型超像素分割方法，结合传统与深度学习优势，实现高精度且规则的超像素分割。


<details>
  <summary>Details</summary>
Motivation: 传统超像素方法依赖低级特征，深度学习方法虽然提升了分割精度，但牺牲了超像素规则性，导致分割结果难以解释。需要一种能够兼顾准确性和规则性的超像素方法。

Method: 作者提出SPAM框架，首先训练一个模型提取有利于超像素生成的图像特征。推理阶段，利用大规模预训练模型实现与物体掩码对齐的无类别分割。SPAM还能结合任意高级分割先验，解决不确定区域，并支持对特定目标的交互式处理。

Result: 实验表明，SPAM在定性与定量评价中均优于当前主流超像素分割方法，在分割任务上的表现突出。

Conclusion: SPAM实现了高精度且规则的超像素分割，适用范围广，是分割相关任务的有力工具。

Abstract: Superpixels are widely used in computer vision to simplify image
representation and reduce computational complexity. While traditional methods
rely on low-level features, deep learning-based approaches leverage high-level
features but also tend to sacrifice regularity of superpixels to capture
complex objects, leading to accurate but less interpretable segmentations. In
this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework
for segmenting images into accurate yet regular superpixels. We train a model
to extract image features for superpixel generation, and at inference, we
leverage a large-scale pretrained model for semantic-agnostic segmentation to
ensure that superpixels align with object masks. SPAM can handle any prior
high-level segmentation, resolving uncertainty regions, and is able to
interactively focus on specific objects. Comprehensive experiments demonstrate
that SPAM qualitatively and quantitatively outperforms state-of-the-art methods
on segmentation tasks, making it a valuable and robust tool for various
applications. Code and pre-trained models are available here:
https://github.com/waldo-j/spam.

</details>


### [56] [Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation](https://arxiv.org/abs/2509.12815)
*Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D Studio是一个AI驱动的端到端3D资产生成平台，能够从图片或文本快速生成高质量、适用于游戏的3D模型，并大幅简化生产流程。


<details>
  <summary>Details</summary>
Motivation: 传统高质量3D资产制作流程繁琐且依赖专业技能，既耗时又难以普及，限制了游戏开发的效率和创意发挥。

Method: Hunyuan3D Studio将多种先进神经网络模块（如零件级3D生成、多边形生成、语义UV等）集成到统一、易用的系统中，实现从单一概念图像或文本到优化几何与PBR材质齐全的高质量3D模型的自动化生成。

Result: 平台生成的3D资产不仅美观，而且满足主流游戏引擎的技术要求，显著提高了迭代速度，降低了3D内容生产的门槛。

Conclusion: Hunyuan3D Studio极大推动了AI辅助的游戏开发流程，助力创意实现，提升互动媒体生产力。

Abstract: The creation of high-quality 3D assets, a cornerstone of modern game
development, has long been characterized by labor-intensive and specialized
workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered
content creation platform designed to revolutionize the game production
pipeline by automating and streamlining the generation of game-ready 3D assets.
At its core, Hunyuan3D Studio integrates a suite of advanced neural modules
(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into
a cohesive and user-friendly system. This unified framework allows for the
rapid transformation of a single concept image or textual description into a
fully-realized, production-quality 3D model complete with optimized geometry
and high-fidelity PBR textures. We demonstrate that assets generated by
Hunyuan3D Studio are not only visually compelling but also adhere to the
stringent technical requirements of contemporary game engines, significantly
reducing iteration time and lowering the barrier to entry for 3D content
creation. By providing a seamless bridge from creative intent to technical
asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted
workflows in game development and interactive media.

</details>


### [57] [A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control](https://arxiv.org/abs/2509.13089)
*Jonas Werheid,Shengjie He,Aymen Gannouni,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.CV

TL;DR: 本论文提出了一种基于CAD数据和目标检测算法的组装质量控制方法，利用合成数据提升检测精度，显著降低人工成本，实现对制造业装配流程的有效监控。


<details>
  <summary>Details</summary>
Motivation: 组装过程的质量控制对制造业至关重要，但中小企业在人工采集、标注图像及算法训练方面资源有限，现有计算机视觉方法成本较高，亟需一种低成本、高效的数据获取与控制方案。

Method: 作者开发了一条可集成、数据高效的视觉组装控制流程，核心方法是利用CAD数据模拟场景生成合成图像，并结合目标检测算法进行分析，大幅减少真实图像采集和人工标注工作。

Result: 在行星齿轮系统组件的识别任务中，合成数据训练下的检测模型在仿真数据上取得99.5%的mAP@0.5:0.95，在实际拍摄测试数据上迁移后也达到93%的精度，显示出合成数据的应用价值。

Conclusion: 该研究验证了合成数据生成与自动检测流程的效果，适用于资源有限的中小企业，能够帮助其高效、经济地实现视觉组装质量控制。

Abstract: Quality control of assembly processes is essential in manufacturing to ensure
not only the quality of individual components but also their proper integration
into the final product. To assist in this matter, automated assembly control
using computer vision methods has been widely implemented. However, the costs
associated with image acquisition, annotation, and training of computer vision
algorithms pose challenges for integration, especially for small- and
medium-sized enterprises (SMEs), which often lack the resources for extensive
training, data collection, and manual image annotation. Synthetic data offers
the potential to reduce manual data collection and labeling. Nevertheless, its
practical application in the context of assembly quality remains limited. In
this work, we present a novel approach for easily integrable and data-efficient
visual assembly control. Our approach leverages simulated scene generation
based on computer-aided design (CAD) data and object detection algorithms. The
results demonstrate a time-saving pipeline for generating image data in
manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)
up to 99,5% for correctly identifying instances of synthetic planetary gear
system components within our simulated training data, and up to 93% when
transferred to real-world camera-captured testing data. This research
highlights the effectiveness of synthetic data generation within an adaptable
pipeline and underscores its potential to support SMEs in implementing
resource-efficient visual assembly control solutions.

</details>


### [58] [SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention](https://arxiv.org/abs/2509.12817)
*Yuan Cao,Dong Wang*

Main category: cs.CV

TL;DR: 本文提出SAGA方法，通过引入自适应门控机制优化线性注意力，提高Transformer在高分辨率视觉任务中的效率和表现。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然擅长捕捉长距离依赖，但其注意力机制的二次复杂度导致高分辨率图像处理时计算和显存消耗极大。线性注意力可降低复杂度，但存在特征冗余、信息压缩过度和性能下降的问题。因此，需解决线性注意力低秩特征所带来的表达能力不足。

Method: 提出SAGA方法，在KV特征聚合过程中引入输入自适应的可学习门控，对信息选择性调制，提升语义多样性，并减缓低秩约束。同时，利用高效的Hadamard积分解方法进行门控计算，做到无额外显存开销。

Result: SAGA在1280x1280分辨率下，相对PVT-T实现了1.76倍的吞吐提升和2.69倍的最大GPU显存降低。在ImageNet上，top-1准确率最多提升了4.4%。

Conclusion: SAGA能够在保证全局感受野和极高计算效率的同时，弥补线性注意力的表达能力短板，是一种高效且表现优秀的视觉Transformer改进方案。

Abstract: While Transformer architecture excel at modeling long-range dependencies
contributing to its widespread adoption in vision tasks the quadratic
complexity of softmax-based attention mechanisms imposes a major bottleneck,
particularly when processing high-resolution images. Linear attention presents
a promising alternative by reformulating the attention computation from $(QK)V$
to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to
$\mathcal{O}(N)$ while preserving the global receptive field. However, most
existing methods compress historical key-value (KV) information uniformly,
which can lead to feature redundancy and the loss of directional alignment with
the query (Q). This uniform compression results in low-rank $KV$ feature maps,
contributing to a performance gap compared to softmax attention. To mitigate
this limitation, we propose \textbf{S}elective \textbf{A}daptive
\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which
introduces input-adaptive learnable gates to selectively modulate information
aggregation into the $KV$ feature map. These gates enhance semantic diversity
and alleviate the low-rank constraint inherent in conventional linear
attention. Additionally, we propose an efficient Hadamard-product decomposition
method for gate computation, which introduces no additional memory overhead.
Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in
throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at
a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up
to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency
and model effectiveness.

</details>


### [59] [Data Scaling Laws for Radiology Foundation Models](https://arxiv.org/abs/2509.12818)
*Maximilian Ilse,Harshita Sharma,Anton Schwaighofer,Sam Bond-Taylor,Fernando Pérez-García,Olesya Melnichenko,Anne-Marie G. Sykes,Kelly K. Horst,Ashish Khandelwal,Maxwell Reynolds,Maria T. Wetscherek,Noel C. F. Codella,Javier Alvarez-Valle,Korfiatis Panagiotis,Valentina Salvatelli*

Main category: cs.CV

TL;DR: 本论文系统研究了两种主流视觉编码器在医疗影像领域中，利用大规模院内胸片数据持续预训练的效能提升，并指出不同编码器在不同任务上的优劣。


<details>
  <summary>Details</summary>
Motivation: 尽管通用视觉编码器如CLIP和DINOv2在自然图像任务中表现卓越，但医疗领域因数据集较小等因素，相关基础模型的能力提升和影响机制尚不明确，因此亟需系统探索数据规模和预训练范式对性能的影响。

Method: 作者以单机构高达350万张胸部X光片为基线数据，对代表CLIP和DINOv2范式的MedImageInsight（MI2）和RAD-DINO两种视觉编码器进行持续预训练，并在分类、分割及报告生成等多任务场景下，采用固定的计算资源和评测协议进行系统化评估。

Result: 实验发现，MI2在病变相关任务上更具扩展性，而RAD-DINO在管路检测任务上表现更优。将报告与结构化标签融合进行UniCL联合训练进一步提升模型性能。此外，仅需3万张院内样本即可在部分任务上超越开放权重的基金会模型。

Conclusion: 通过持续预训练充分利用院内数据，可显著提升医疗影像基础模型的性能，尤其在特定中心场景下价值突出，为医疗机构制定自有影像AI策略提供有力支撑。

Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale
data, exhibit strong transfer performance across tasks and datasets. However,
medical imaging foundation models remain constrained by smaller datasets,
limiting our understanding of how data scale and pretraining paradigms affect
performance in this setting. In this work, we systematically study continual
pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO
representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M
chest x-rays from a single institution, holding compute and evaluation
protocols constant. We evaluate on classification (radiology findings, lines
and tubes), segmentation (lines and tubes), and radiology report generation.
While prior work has primarily focused on tasks related to radiology findings,
we include lines and tubes tasks to counterbalance this bias and evaluate a
model's ability to extract features that preserve continuity along elongated
structures. Our experiments show that MI2 scales more effectively for
finding-related tasks, while RAD-DINO is stronger on tube-related tasks.
Surprisingly, continually pretraining MI2 with both reports and structured
labels using UniCL improves performance, underscoring the value of structured
supervision at scale. We further show that for some tasks, as few as 30k
in-domain samples are sufficient to surpass open-weights foundation models.
These results highlight the utility of center-specific continual pretraining,
enabling medical institutions to derive significant performance gains by
utilizing in-domain data.

</details>


### [60] [Exploring Metric Fusion for Evaluation of NeRFs](https://arxiv.org/abs/2509.12836)
*Shreyas Shivakumara,Gabriel Eilertsen,Karljohan Lundin Palmerius*

Main category: cs.CV

TL;DR: 本文针对NeRF生成视图的评估难题，提出了融合两种主观质量评价指标（DISTS和VMAF）的方案，并探讨了不同归一化和融合策略。实验显示该融合指标与人工主观评分相关性更高，具备更强鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: NeRF生成的新视角质量评价困难，因为其常出现与传统图像不同的人为伪影，且单一评价指标无法在所有数据集表现良好。因此，作者希望通过融合不同感知方式的评价指标，提升与主观质量的相关性。

Method: 作者选用DISTS和VMAF两种在结构与纹理、视频质量评价中表现优异的指标，尝试两种归一化策略和两种融合策略，并在Synthetic与Outdoor两个公开数据集上进行三种不同配置的实验，评测融合指标与人工主观评分的相关性。

Result: 实验结果表明，所提出的融合评价指标在相关系数上优于单项指标，且在不同数据集和配置下表现出较好的鲁棒性和泛化能力。

Conclusion: 融合不同类型的感知质量评价指标，可以更有效地测量NeRF生成图像的主观质量，提高评价准确性和通用性。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in
synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,
remains a challenge due to the unique artifacts they exhibit, and no individual
metric performs well across all datasets. We hypothesize that combining two
successful metrics, Deep Image Structure and Texture Similarity (DISTS) and
Video Multi-Method Assessment Fusion (VMAF), based on different perceptual
methods, can overcome the limitations of individual metrics and achieve
improved correlation with subjective quality scores. We experiment with two
normalization strategies for the individual metrics and two fusion strategies
to evaluate their impact on the resulting correlation with the subjective
scores. The proposed pipeline is tested on two distinct datasets, Synthetic and
Outdoor, and its performance is evaluated across three different
configurations. We present a detailed analysis comparing the correlation
coefficients of fusion methods and individual scores with subjective scores to
demonstrate the robustness and generalizability of the fusion metrics.

</details>


### [61] [Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses](https://arxiv.org/abs/2509.12866)
*Martin Thißen,Thi Ngoc Diep Tran,Barbara Esteve Ratsch,Ben Joel Schönbein,Ute Trapp,Beate Egner,Romana Piat,Elke Hergenröther*

Main category: cs.CV

TL;DR: 本文探索了利用大语言模型(LLM)生成犬类肌肉骨骼疾病视觉数据，从而缓解真实病例数据稀缺的问题，并证实合成数据训练出的模型在真实数据测试集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 由于某些任务的数据采集困难（如罕见疾病的病例少或获取成本高），需要寻找新的方式来生产训练用的数据。本研究关注如何为犬类肌肉骨骼疾病的AI可视化诊断生成足够的数据。

Method: 作者将犬只体图上的异常标注映射至文本领域，通过LLM（结合引导解码、思维链推理、少量示例提示）生成1000份膝盖脱位诊断的合成视觉文档和1000份其他诊断的合成文档。最后用这些数据构建二分类数据集，并在真实记录上测试。

Result: 合成文档能准确反映异常发生的位置与严重程度，且与犬只性别无关。基于合成数据训练的模型在70条真实数据上，F1-score达88%。

Conclusion: LLM生成的合成数据能有效缓解医疗罕见或高成本病例下的数据匮乏问题。这一方法对医疗领域以及更广泛的应用场景具有推广潜力。

Abstract: It is well-established that more data generally improves AI model
performance. However, data collection can be challenging for certain tasks due
to the rarity of occurrences or high costs. These challenges are evident in our
use case, where we apply AI models to a novel approach for visually documenting
the musculoskeletal condition of dogs. Here, abnormalities are marked as
colored strokes on a body map of a dog. Since these strokes correspond to
distinct muscles or joints, they can be mapped to the textual domain in which
large language models (LLMs) operate. LLMs have demonstrated impressive
capabilities across a wide range of tasks, including medical applications,
offering promising potential for generating synthetic training data. In this
work, we investigate whether LLMs can effectively generate synthetic visual
training data for canine musculoskeletal diagnoses. For this, we developed a
mapping that segments visual documentations into over 200 labeled regions
representing muscles or joints. Using techniques like guided decoding,
chain-of-thought reasoning, and few-shot prompting, we generated 1,000
synthetic visual documentations for patellar luxation (kneecap dislocation)
diagnosis, the diagnosis for which we have the most real-world data. Our
analysis shows that the generated documentations are sensitive to location and
severity of the diagnosis while remaining independent of the dog's sex. We
further generated 1,000 visual documentations for various other diagnoses to
create a binary classification dataset. A model trained solely on this
synthetic data achieved an F1 score of 88% on 70 real-world documentations.
These results demonstrate the potential of LLM-generated synthetic data, which
is particularly valuable for addressing data scarcity in rare diseases. While
our methodology is tailored to the medical domain, the insights and techniques
can be adapted to other fields.

</details>


### [62] [Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment](https://arxiv.org/abs/2509.12871)
*Avinaash Manoharan,Xiangyu Yin,Domenik Helm,Chih-Hong Cheng*

Main category: cs.CV

TL;DR: 提出了一种无需标签、用于部署阶段持续监控和评估目标检测模型的新指标Cumulative Consensus Score（CCS），能够替代人工标注进行模型表现度量。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，目标检测模型部署后，通常没有地面真实标注，导致难以持续评估模型性能。传统指标依赖标注，无法适用于真实无标注场景，因此需要开发无标签、能可靠反映模型表现的评估方法。

Method: CCS通过对每张图片进行测试时的数据增强，收集增强后图像的预测框，并利用IoU（交并比）测量不同视图间的预测结果一致性。将最大IoU归一化后，平均得到整个增强对的一致性分数，用作为没有标签情况下模型预测可靠性的代理评价。该方法可应用于各种类型（单阶段、两阶段）检测器，且能够定位个案级表现不佳的场景。

Result: 在Open Images和KITTI两个数据集上的对照实验显示，CCS与F1分数、概率检测质量和最优纠正代价等传统指标有超过90%的结果一致性。这表明CCS能够有效反映模型的实际表现，且具有通用性。

Conclusion: CCS为无标注环境下目标检测模型的DevOps式持续监控和性能比较提供了坚实基础，适用于实际生产部署和模型运维场景。

Abstract: Evaluating object detection models in deployment is challenging because
ground-truth annotations are rarely available. We introduce the Cumulative
Consensus Score (CCS), a label-free metric that enables continuous monitoring
and comparison of detectors in real-world settings. CCS applies test-time data
augmentation to each image, collects predicted bounding boxes across augmented
views, and computes overlaps using Intersection over Union. Maximum overlaps
are normalized and averaged across augmentation pairs, yielding a measure of
spatial consistency that serves as a proxy for reliability without annotations.
In controlled experiments on Open Images and KITTI, CCS achieved over 90%
congruence with F1-score, Probabilistic Detection Quality, and Optimal
Correction Cost. The method is model-agnostic, working across single-stage and
two-stage detectors, and operates at the case level to highlight
under-performing scenarios. Altogether, CCS provides a robust foundation for
DevOps-style monitoring of object detectors.

</details>


### [63] [Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation](https://arxiv.org/abs/2509.12878)
*Qianguang Zhao,Dongli Wang,Yan Zhou,Jianxun Li,Richard Irampa*

Main category: cs.CV

TL;DR: 提出了一种用于小样本3D点云语义分割的新方法PENet，通过结合扩展原型能力与对齐机制，提升了新类别的分割效果，在S3DIS和ScanNet数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有原型方法在小样本3D点云分割中受制于原型表达力不足（无法覆盖类别变异性）和支持集与查询集间的原型失配，导致分割性能有限。本文希望缓解上述两种挑战。

Method: 提出Prototype Expansion Network（PENet）框架，通过扩展原型能力实现更强表征。具体方法为：利用预训练扩散模型获取生成性特征，与传统的监督学习特征形成双流学习架构，然后通过Prototype Assimilation Module（PAM）对两个原型进行push-pull对齐，最后采用原型校准机制（PCM）抑制语义漂移。

Result: 在S3DIS和ScanNet两大3D点云语义分割数据集上，PENet在多种小样本实验设置下均显著优于现有最优方法。

Conclusion: 通过引入生成式扩展特征和对齐机制，PENet有效提升了小样本3D点云分割的原型表达力与泛化能力，为该领域带来新的技术进步。

Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel
categories using a minimal number of annotated support samples. While existing
prototype-based methods have shown promise, they are constrained by two
critical challenges: (1) Intra-class Diversity, where a prototype's limited
representational capacity fails to cover a class's full variations, and (2)
Inter-set Inconsistency, where prototypes derived from the support set are
misaligned with the query feature space. Motivated by the powerful generative
capability of diffusion model, we re-purpose its pre-trained conditional
encoder to provide a novel source of generalizable features for expanding the
prototype's representational range. Under this setup, we introduce the
Prototype Expansion Network (PENet), a framework that constructs big-capacity
prototypes from two complementary feature sources. PENet employs a dual-stream
learner architecture: it retains a conventional fully supervised Intrinsic
Learner (IL) to distill representative features, while introducing a novel
Diffusion Learner (DL) to provide rich generalizable features. The resulting
dual prototypes are then processed by a Prototype Assimilation Module (PAM),
which adopts a novel push-pull cross-guidance attention block to iteratively
align the prototypes with the query space. Furthermore, a Prototype Calibration
Mechanism (PCM) regularizes the final big capacity prototype to prevent
semantic drift. Extensive experiments on the S3DIS and ScanNet datasets
demonstrate that PENet significantly outperforms state-of-the-art methods
across various few-shot settings.

</details>


### [64] [Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder](https://arxiv.org/abs/2509.12883)
*Qifei Jia,Yu Liu,Yajie Chai,Xintong Yao,Qiming Lu,Yasen Zhang,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CV

TL;DR: Lego-Edit是一种新型的基于指令的图像编辑方法，结合多模态大模型与模块化工具，有效泛化到各种真实世界的编辑指令，并表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在面对真实世界多样化的用户指令时，泛化能力不足，难以应用于实际场景。

Method: 提出Lego-Edit，利用多模态大模型（MLLM）整合多种模型级编辑工具，通过（1）构建多样化编辑工具箱（包含多种高效训练模型和图像编辑功能）；（2）采用三阶段渐进强化学习，利用无标注、开放领域的指令反馈训练MLLM增强归纳推理能力。

Result: Lego-Edit在GEdit-Bench和ImgBench数据集上取得了当前最优性能，能够对开放领域指令表现出强大的推理与泛化能力，并能无须额外微调直接应用新增编辑工具。

Conclusion: Lego-Edit显著提升了基于指令的图像编辑泛化性和实用性，是面向复杂、开放指令环境下图像编辑的重要进展。

Abstract: Instruction-based image editing has garnered significant attention due to its
direct interaction with users. However, real-world user instructions are
immensely diverse, and existing methods often fail to generalize effectively to
instructions outside their training domain, limiting their practical
application. To address this, we propose Lego-Edit, which leverages the
generalization capability of Multi-modal Large Language Model (MLLM) to
organize a suite of model-level editing tools to tackle this challenge.
Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising
diverse models efficiently trained on limited data and several image
manipulation functions, enabling fine-grained composition of editing actions by
the MLLM; and (2) a three-stage progressive reinforcement learning approach
that uses feedback on unannotated, open-domain instructions to train the MLLM,
equipping it with generalized reasoning capabilities for handling real-world
instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art
performance on GEdit-Bench and ImgBench. It exhibits robust reasoning
capabilities for open-domain instructions and can utilize newly introduced
editing tools without additional fine-tuning.
  Code is available: https://github.com/xiaomi-research/lego-edit.

</details>


### [65] [Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing](https://arxiv.org/abs/2509.12888)
*Weiming Chen,Zhihan Zhu,Yijia Wang,Zhihai He*

Main category: cs.CV

TL;DR: 本文针对Rectified flow（RF）模型在真实应用中的两大挑战——低反演精度和多模态注意力纠缠，提出了高阶反演方法和解耦多模态注意力机制，有效提升了图像还原和文本引导编辑任务的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然RF模型生成性能优越，但其在实际应用中面临反演精度低（影响还原一致性）及多模态（图文）注意力机制无法精确控制（影响语义编辑精度）的问题，需要解决这些短板来提升落地效果。

Method: 针对反演困难，提出了基于微分方程Runge-Kutta算法的高阶反演方法，提高了RF模型的逆向还原能力；针对注意力控制问题，设计了解耦扩散Transformer注意力（DDTA）机制，将文本和图像注意力分离，提升了语义精度。

Result: 通过大量图像重建和文本驱动编辑的实验，验证了所提方法在保真度和可编辑性方面均达到当前最优水平。

Conclusion: 新方法有效提升了RF模型的实用性和灵活性，在生成和编辑任务中表现优异，为后续多模态生成模型提供了可行的优化方向。

Abstract: Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.

</details>


### [66] [MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization](https://arxiv.org/abs/2509.12893)
*Yiyi Zhang,Yuchen Yuan,Ying Zheng,Jialun Pei,Jinpeng Li,Zheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的MEJO框架，通过解决多任务学习中的优化冲突，有效提升了手术三元组（器械、动作、目标及其组合）识别的性能，在公开手术场景数据集上取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 手术三元组识别对于复杂手术场景理解至关重要，但当前方法面临两大挑战：1）多任务表示纠缠导致的任务间优化冲突；2）类别不均衡引发的任务内优化矛盾。

Method: MEJO框架主要包括：一是引入S$^2$D学习方案，将特征表示解耦为任务共享与任务特定两部分，并结合MLLM促成的专业提示池动态增强视觉语义；二是采用不同任务的时空提示，进一步缓解任务间模糊；三是提出CGL策略，针对类别分布长尾，通过平衡头尾类损失梯度，优化训练过程。

Result: 在CholecT45和CholecT50两个权威手术视频数据集上进行了大量实验，结果显示该方法在三元组识别准确率等指标上均优于现有主流方法。

Conclusion: MEJO框架不仅解决了多任务手术三元组识别中的关键优化冲突问题，还在实际数据中验证了其实用性和优越性，对智能手术场景分析具有重要意义。

Abstract: Surgical triplet recognition, which involves identifying instrument, verb,
target, and their combinations, is a complex surgical scene understanding
challenge plagued by long-tailed data distribution. The mainstream multi-task
learning paradigm benefiting from cross-task collaborative promotion has shown
promising performance in identifying triples, but two key challenges remain: 1)
inter-task optimization conflicts caused by entangling task-generic and
task-specific representations; 2) intra-task optimization conflicts due to
class-imbalanced training data. To overcome these difficulties, we propose the
MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and
intra-task optimization for surgical triplet recognition. For inter-task
optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning
scheme that decomposes representations into task-shared and task-specific
components. To enhance task-shared representations, we construct a Multimodal
Large Language Model (MLLM) powered probabilistic prompt pool to dynamically
augment visual features with expert-level semantic cues. Additionally,
comprehensive task-specific cues are modeled via distinct task prompts covering
the temporal-spatial dimensions, effectively mitigating inter-task ambiguities.
To tackle intra-task optimization conflicts, we develop a Coordinated Gradient
Learning (CGL) strategy, which dissects and rebalances the positive-negative
gradients originating from head and tail classes for more coordinated learning
behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets
demonstrate the superiority of our proposed framework, validating its
effectiveness in handling optimization conflicts.

</details>


### [67] [DialNav: Multi-turn Dialog Navigation with a Remote Guide](https://arxiv.org/abs/2509.12894)
*Leekyeung Han,Hyunji Min,Gyeom Hwangbo,Jonghyun Choi,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 本文提出了DialNav任务，促进导航代理与远程引导通过多轮对话协作完成导航，并构建了相关数据集和评测基准，以推动具身对话领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有具身对话任务中，定位和多轮沟通能力的整体评估不足，且缺乏Guide推断Navigator位置的场景。为实现更真实、复杂的人机协作，需要设计兼具导航与对话的新任务与数据集。

Method: 设计DialNav任务，要求Navigator和Guide多轮对话协作定位导航；采集RAIN数据集，包含真实环境中的人类对话及导航轨迹；建立评测基准，分析不同模型在对话和导航上的表现。

Result: 通过评测不同的Navigator和Guide模型，揭示了当前具身对话和协作导航面临的关键挑战，初步展示了任务难度和未来潜力。

Conclusion: DialNav和RAIN数据集为具身多轮对话协作导航提供了新基准，并促进该领域相关的算法和模型研究，所发布的资源将推动后续工作的发展。

Abstract: We introduce DialNav, a novel collaborative embodied dialog task, where a
navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn
dialog to reach a goal location. Unlike prior work, DialNav aims for holistic
evaluation and requires the Guide to infer the Navigator's location, making
communication essential for task success. To support this task, we collect and
release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog
paired with navigation trajectories in photorealistic environments. We design a
comprehensive benchmark to evaluate both navigation and dialog, and conduct
extensive experiments analyzing the impact of different Navigator and Guide
models. We highlight key challenges and publicly release the dataset, code, and
evaluation framework to foster future research in embodied dialog.

</details>


### [68] [Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models](https://arxiv.org/abs/2509.12897)
*Jianfei Zhao,Feng Zhang,Xin Sun,Lingxing Kong,Zhixing Tan,Chong Feng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Cross-Layer Vision Smoothing (CLVS)的新方法，用于提升大规模视觉语言模型(LVLMs)的视觉理解能力，通过跨层的视觉记忆平滑注意力分布，实现对关键目标更加持续与充分的关注。实验表明此方法在多个模型和基准任务上表现优异，尤其是在关系和属性理解任务上。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs能够准确定位图片中的关键目标，但模型对这些目标的注意往往不够持久。作者认为，持续且平滑地关注关键目标，有助于进一步提升模型的视觉理解能力。

Method: CLVS方法核心为引入“视觉记忆”，在模型各层间平滑注意力分布。具体是：第一层用无位置偏置的视觉注意力初始化视觉记忆，其后每一层的视觉注意力都结合上层的视觉记忆，并不断更新，从而在信息流动过程中，维持关键目标上的持续关注。对于视觉理解主要集中于模型前中层的特性，CLVS还设计了用不确定性来判定何时停止平滑过程，优化模型计算与效果。

Result: 在三个LVLMs的四个基准任务上进行了实验，验证了方法的有效性和通用性。CLVS在多项视觉理解任务上达到了当前最优表现，特别是在关系理解和属性理解方面提升明显。

Conclusion: CLVS方法通过跨层持续关注关键目标，提升了LVLMs的视觉理解能力，在多个任务上实现了突破，有潜力成为推动视觉语言融合模型发展的有效工具。

Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in
images, yet their attention to these objects tends to be very brief. Motivated
by the hypothesis that sustained focus on key objects can improve LVLMs' visual
capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of
CLVS is to incorporate a vision memory that smooths the attention distribution
across layers. Specifically, we initialize this vision memory with
position-unbiased visual attention in the first layer. In subsequent layers,
the model's visual attention jointly considers the vision memory from previous
layers, while the memory is updated iteratively, thereby maintaining smooth
attention on key objects. Given that visual understanding primarily occurs in
the early and middle layers of the model, we use uncertainty as an indicator of
completed visual understanding and terminate the smoothing process accordingly.
Experiments on four benchmarks across three LVLMs confirm the effectiveness and
generalizability of our method. CLVS achieves state-of-the-art performance on a
variety of visual understanding tasks, with particularly significant
improvements in relation and attribute understanding.

</details>


### [69] [MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion](https://arxiv.org/abs/2509.12901)
*Guihui Li,Bowei Dong,Kaizhi Dong,Jiayi Li,Haiyong Zheng*

Main category: cs.CV

TL;DR: MSGFusion提出了一种用多模态场景图指导红外与可见光图像融合的深度学习框架，既提升了细节保留，也显著增强了高层语义和任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前红外与可见光图像融合虽依赖深度学习提升特征提取与融合，但主要基于低层视觉信息，对高层语义捕捉能力不足。尝试用文本作语义指导，但方式多为非结构化，缺乏对实体、属性、关系的明确建模与空间定位，限制了细粒度融合效果。

Method: MSGFusion框架将源自文本与视觉的结构化场景图深度融合，显式表示实体、属性和空间关系，通过一系列场景图表示、分层聚合和图驱动融合模块，协同优化高层语义与低层细节，提升融合效果。

Result: MSGFusion在多个公开数据集上进行大量实验，融合细节与结构清晰度显著优于现有方法，对下游如低照度目标检测、语义分割和医学图像融合等任务展现更高的语义一致性和泛化能力。

Conclusion: MSGFusion实现了红外与可见光图像融合在细节、结构、和高层语义上的全面提升，为多任务融合应用提供了更优的基础与指导。

Abstract: Infrared and visible image fusion has garnered considerable attention owing
to the strong complementarity of these two modalities in complex, harsh
environments. While deep learning-based fusion methods have made remarkable
advances in feature extraction, alignment, fusion, and reconstruction, they
still depend largely on low-level visual cues, such as texture and contrast,
and struggle to capture the high-level semantic information embedded in images.
Recent attempts to incorporate text as a source of semantic guidance have
relied on unstructured descriptions that neither explicitly model entities,
attributes, and relationships nor provide spatial localization, thereby
limiting fine-grained fusion performance. To overcome these challenges, we
introduce MSGFusion, a multimodal scene graph-guided fusion framework for
infrared and visible imagery. By deeply coupling structured scene graphs
derived from text and vision, MSGFusion explicitly represents entities,
attributes, and spatial relations, and then synchronously refines high-level
semantics and low-level details through successive modules for scene graph
representation, hierarchical aggregation, and graph-driven fusion. Extensive
experiments on multiple public benchmarks show that MSGFusion significantly
outperforms state-of-the-art approaches, particularly in detail preservation
and structural clarity, and delivers superior semantic consistency and
generalizability in downstream tasks such as low-light object detection,
semantic segmentation, and medical image fusion.

</details>


### [70] [AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring](https://arxiv.org/abs/2509.12905)
*Branko Mitic,Philipp Seeböck,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成式异常检测方法，用于医学影像中异常的自动检测与分割，并在胸部CT和脑部MRI上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的异常检测与分割对疾病早期发现、病变严重程度评估和自动化筛查等任务非常重要。然而，现有生成式异常检测方法在处理肺部等组织的细粒度正常变异时表现有限，容易误检或漏检。作者提出新方法以应对此挑战。

Method: 该方法包括两步：首先通过图像到图像的转换实现无异常影像重建；其次对原始与重建后的影像采取基于patch的相似度打分，进行精确的异常定位。方法在胸部CT感染性病变检测和脑部MRI缺血性脑卒中病灶分割任务上进行了验证。

Result: 与其他主流重建类异常检测方法相比，本文方法在胸部CT和脑部MRI任务中的像素级异常分割表现更优，DICE分数分别提升了1.9%和4.4%。

Conclusion: 提出的方法有效提升了医学影像中细粒度异常分割的效果，且具有良好的通用性，可广泛应用于不同类型的医学影像异常检测任务。

Abstract: Early detection of newly emerging diseases, lesion severity assessment,
differentiation of medical conditions and automated screening are examples for
the wide applicability and importance of anomaly detection (AD) and
unsupervised segmentation in medicine. Normal fine-grained tissue variability
such as present in pulmonary anatomy is a major challenge for existing
generative AD methods. Here, we propose a novel generative AD approach
addressing this issue. It consists of an image-to-image translation for
anomaly-free reconstruction and a subsequent patch similarity scoring between
observed and generated image-pairs for precise anomaly localization. We
validate the new method on chest computed tomography (CT) scans for the
detection and segmentation of infectious disease lesions. To assess
generalizability, we evaluate the method on an ischemic stroke lesion
segmentation task in T1-weighted brain MRI. Results show improved pixel-level
anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score
improvements of +1.9% and +4.4%, respectively, compared to other
state-of-the-art reconstruction-based methods.

</details>


### [71] [T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking](https://arxiv.org/abs/2509.12913)
*Hojat Ardi,Amir Jahanshahi,Ali Diba*

Main category: cs.CV

TL;DR: 提出了T-SiamTPN，一种显式时序建模的空中目标跟踪方法，相比同类方法在精度和成功率方面有明显提升，并可实时运行于嵌入式设备。


<details>
  <summary>Details</summary>
Motivation: 现有的空中目标跟踪方法主要依赖空间信息，忽视了时序依赖，导致在长时跟踪和遮挡时表现不佳，同时传统Siamese跟踪器在应对复杂非线性外观变化时存在不足。

Method: 提出了T-SiamTPN，在原有SiamTPN架构基础上引入了时序特征融合和注意力机制，增强了时序一致性和特征表达能力，并保证了计算效率。

Result: 在嵌入式设备Jetson Nano上可实时运行（7.1 FPS），相较基线提升成功率13.7%、精度14.7%，达到主流先进方法的性能。

Conclusion: 显式时序建模显著提升了Siamese跟踪架构的鲁棒性和有效性，T-SiamTPN为空中目标跟踪任务提供了高效实用的解决方案。

Abstract: Aerial object tracking remains a challenging task due to scale variations,
dynamic backgrounds, clutter, and frequent occlusions. While most existing
trackers emphasize spatial cues, they often overlook temporal dependencies,
resulting in limited robustness in long-term tracking and under occlusion.
Furthermore, correlation-based Siamese trackers are inherently constrained by
the linear nature of correlation operations, making them ineffective against
complex, non-linear appearance changes. To address these limitations, we
introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends
the SiamTPN architecture with explicit temporal modeling. Our approach
incorporates temporal feature fusion and attention-based interactions,
strengthening temporal consistency and enabling richer feature representations.
These enhancements yield significant improvements over the baseline and achieve
performance competitive with state-of-the-art trackers. Crucially, despite the
added temporal modules, T-SiamTPN preserves computational efficiency. Deployed
on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1
FPS, demonstrating its suitability for real-world embedded applications without
notable runtime overhead. Experimental results highlight substantial gains:
compared to the baseline, T-SiamTPN improves success rate by 13.7% and
precision by 14.7%. These findings underscore the importance of temporal
modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and
efficient solution for aerial object tracking. Code is available at:
https://github.com/to/be/released

</details>


### [72] [A Novel Compression Framework for YOLOv8: Achiev-ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation](https://arxiv.org/abs/2509.12918)
*Melika Sabaghian,Mohammad Ali Keyvanrad,Seyyedeh Mahila Moghadami*

Main category: cs.CV

TL;DR: 本文提出了一种高效的三阶段YOLOv8模型压缩方案，使其在端侧资源受限设备上实现高效空中目标检测，同时保持检测精度。主要措施包括稀疏感知训练、结构化通道剪枝及通道级知识蒸馏。结果在VisDrone数据集上显著降低了模型体积和计算量，并提升了推理速度，且精度损失极小，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（如YOLOv8）在空中目标检测应用中，对于边缘设备部署仍受到模型体积大、运算消耗高的限制。现有压缩方法往往对性能和速度间权衡处理不足。作者希望在极大地减少模型参数和计算量的情况下，最大程度保留检测效果，并能在实际终端设备实现实时性。

Method: 采用三阶段压缩流程：其一，通过稀疏感知训练引入动态稀疏性，平衡参数减少与准确率；其二，基于BN缩放因子进行结构化通道剪枝，去除冗余通道，缩小模型体积；其三，应用通道级知识蒸馏（CWD），通过可调温度和损失权重，有针对性地迁移原模型知识，提升小目标和中等目标检测能力。最终辅以TensorRT轻量化优化进一步提升推理速度。

Result: 以YOLOv8m为例，参数由25.85M降至6.85M，FLOPs由49.6G降至13.3G，MACs由101G降至34.5G，AP50仅下降2.7%，最终压缩模型AP50达47.9，推理速度从26 FPS提升至45 FPS。经TensorRT优化后AP50略降至47.6，但速度提升至68 FPS，系列实验体现出各阶段方法的有效性和协同增益。

Conclusion: 提出的三阶段压缩管线显著提升了YOLOv8模型在资源受限场景下的实用性，能以最小准确率损失大幅降低模型复杂度与资源需求，并且大幅提升推理速度，具备优异的实际部署价值。

Abstract: Efficient deployment of deep learning models for aerial object detection on
resource-constrained devices requires significant compression without
com-promising performance. In this study, we propose a novel three-stage
compression pipeline for the YOLOv8 object detection model, integrating
sparsity-aware training, structured channel pruning, and Channel-Wise Knowledge
Distillation (CWD). First, sparsity-aware training introduces dynamic sparsity
during model optimization, effectively balancing parameter reduction and
detection accuracy. Second, we apply structured channel pruning by leveraging
batch normalization scaling factors to eliminate redundant channels,
significantly reducing model size and computational complexity. Finally, to
mitigate the accuracy drop caused by pruning, we employ CWD to transfer
knowledge from the original model, using an adjustable temperature and loss
weighting scheme tailored for small and medium object detection. Extensive
experiments on the VisDrone dataset demonstrate the effectiveness of our
approach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model
parameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to
13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The
resulting compressed model achieves 47.9 AP50 and boosts inference speed from
26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge
devices. We further apply TensorRT as a lightweight optimization step. While
this introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly
improves inference speed from 45 to 68 FPS, demonstrating the practicality of
our approach for high-throughput, re-source-constrained scenarios.

</details>


### [73] [MATTER: Multiscale Attention for Registration Error Regression](https://arxiv.org/abs/2509.12924)
*Shipeng Liu,Ziliang Xiong,Khac-Hoang Ngo,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 本论文提出了一种用于点云配准（PCR）质量验证的新方法，将验证任务由分类转为回归，更精细地量化配准误差，同时结合多尺度特征提取和注意力机制，显著提升了误差估算的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 点云配准广泛用于SLAM、目标跟踪等领域，验证配准结果的质量至关重要。但现有方法多为粗粒度的分类，难以满足高精度验证需求。

Method: 作者提出采用回归方法来细致量化点云配准误差，并利用多尺度特征提取和注意力机制聚合上下文信息，以提升对不同密度点云的适应性和误差估算能力。

Result: 实验表明，该方法在多种公开数据集上，对配准误差的估算准确率和鲁棒性均优于基于分类的主流方法。

Conclusion: 将PCR质量验证由分类转为回归，加之多尺度与注意力机制的特征处理方案，可更准确、稳健地量化配准误差，并在后续的地图构建等任务中进一步提升实际性能。

Abstract: Point cloud registration (PCR) is crucial for many downstream tasks, such as
simultaneous localization and mapping (SLAM) and object tracking. This makes
detecting and quantifying registration misalignment, i.e.,~{\it PCR quality
validation}, an important task. All existing methods treat validation as a
classification task, aiming to assign the PCR quality to a few classes. In this
work, we instead use regression for PCR validation, allowing for a more
fine-grained quantification of the registration quality. We also extend
previously used misalignment-related features by using multiscale extraction
and attention-based aggregation. This leads to accurate and robust registration
error estimation on diverse datasets, especially for point clouds with
heterogeneous spatial densities. Furthermore, when used to guide a mapping
downstream task, our method significantly improves the mapping quality for a
given amount of re-registered frames, compared to the state-of-the-art
classification-based method.

</details>


### [74] [4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar](https://arxiv.org/abs/2509.12931)
*Xiao Tang,Guirong Zhuo,Cong Wang,Boyuan Zheng,Minqing Huang,Lianqing Zheng,Long Chen,Shouyi Lu*

Main category: cs.CV

TL;DR: 本文提出了一种结合4D雷达信息的自监督3D重建框架（4DRadar-GS），解决动态驾驶场景下现有方法重建动态物体不准确的问题，显著提升了3D重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督3D重建方法在动态场景下重建动态物体时表现不佳，主要原因是运动估计不准和时序一致性较弱，导致重建结果缺失或失真。因此，急需一种能准确处理动态场景并提升重建质量的方法。

Method: 提出了4DRadar-GS框架，包括：（1）利用4D雷达速度和空间信息的高斯点初始化方案，实现对动态物体的分割及深度尺度恢复；（2）设计了Velocity-guided PointTrack（VGPT）模型，在重建过程中引入场景流监督，以追踪细粒度动态轨迹，确保时间上一致的重建。

Result: 在OmniHD-Scenes数据集上，4DRadar-GS在动态驾驶场景3D重建任务上取得了当前最优的性能。

Conclusion: 4DRadar-GS有效提升了自监督3D重建在动态驾驶场景下的准确性和完整性，尤其是在没有标注框的情况下，验证了其先进性和实用价值。

Abstract: 3D reconstruction and novel view synthesis are critical for validating
autonomous driving systems and training advanced perception models. Recent
self-supervised methods have gained significant attention due to their
cost-effectiveness and enhanced generalization in scenarios where annotated
bounding boxes are unavailable. However, existing approaches, which often rely
on frequency-domain decoupling or optical flow, struggle to accurately
reconstruct dynamic objects due to imprecise motion estimation and weak
temporal consistency, resulting in incomplete or distorted representations of
dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a
4D Radar-augmented self-supervised 3D reconstruction framework tailored for
dynamic driving scenes. Specifically, we first present a 4D Radar-assisted
Gaussian initialization scheme that leverages 4D Radar's velocity and spatial
information to segment dynamic objects and recover monocular depth scale,
generating accurate Gaussian point representations. In addition, we propose a
Velocity-guided PointTrack (VGPT) model, which is jointly trained with the
reconstruction pipeline under scene flow supervision, to track fine-grained
dynamic trajectories and construct temporally consistent representations.
Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art
performance in dynamic driving scene 3D reconstruction.

</details>


### [75] [Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings](https://arxiv.org/abs/2509.12938)
*Abdalla Arafa,Didier Stricker*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过将预分解的对象级高斯和多视角CLIP特征聚合结合，实现了3D场景中开词汇语义理解，提升了3DGS在AR/VR和机器人领域的应用能力。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）能进行高质量实时渲染，但其模糊性阻碍了对3D场景的精确理解和广泛应用。现有大多采用2D模型蒸馏的方式学习语义，但由于alpha blending会把语义平均，对象层级的3D理解难以实现。因此亟需解决3D语义理解的局限。

Method: 摒弃了对渲染可微分的依赖。方法核心：先将高斯点预分解至对象级，然后利用多视角CLIP特征聚合，把每个对象表示为一个聚合的嵌入集合（bag of embeddings）。这样，通过对对象级的embedding与文本查询对比，可实现准确的开放词汇对象检索，也能将对象ID传播至像素或高斯点，实现2D分割和3D提取。

Result: 实验表明，该方法可以有效实现3D开放词汇对象提取，在2D开放词汇分割任务上，也能维持与现有最佳方法相当的表现，无明显性能损失。

Conclusion: 该方法克服了3D高斯模糊带来的场景理解障碍，实现了对象级的开放词汇检索与多任务适应，为3DGS技术在增强现实、虚拟现实和机器人领域的应用打开了新局面。

Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian
Splatting (3DGS), enabling real-time photorealistic rendering. However, the
inherent fuzziness of Gaussian Splatting presents challenges for 3D scene
understanding, restricting its broader applications in AR/VR and robotics.
While recent works attempt to learn semantics via 2D foundation model
distillation, they inherit fundamental limitations: alpha blending averages
semantics across objects, making 3D-level understanding impossible. We propose
a paradigm-shifting alternative that bypasses differentiable rendering for
semantics entirely. Our key insight is to leverage predecomposed object-level
Gaussians and represent each object through multiview CLIP feature aggregation,
creating comprehensive "bags of embeddings" that holistically describe objects.
This allows: (1) accurate open-vocabulary object retrieval by comparing text
queries to object-level (not Gaussian-level) embeddings, and (2) seamless task
adaptation: propagating object IDs to pixels for 2D segmentation or to
Gaussians for 3D extraction. Experiments demonstrate that our method
effectively overcomes the challenges of 3D open-vocabulary object extraction
while remaining comparable to state-of-the-art performance in 2D
open-vocabulary segmentation, ensuring minimal compromise.

</details>


### [76] [Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain](https://arxiv.org/abs/2509.12959)
*Yuqi Xie,Shuhan Ye,Chong Wang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian*

Main category: cs.CV

TL;DR: 本文提出一种名为TMKT的时序混合知识迁移方法，结合事件相机与脉冲神经网络（SNN），显著提升跨模态视觉处理效率。


<details>
  <summary>Details</summary>
Motivation: 事件相机的数据稀疏且有限，直接训练表现受限，现有用RGB数据迁移知识的方法又忽略了与DVS间的分布差异，导致迁移效果不佳。

Method: 提出Time-step Mixup知识迁移（TMKT）方法，在SNN训练的不同时间步融合RGB和DVS输入，并引入模态感知的辅助学习目标，实现更平滑有效的跨模态标签融合与判别能力提升。

Result: 通过大量实验，在多个数据集上验证了TMKT方法在脉冲神经网络视觉分类任务中的有效性和优越性。

Conclusion: TMKT方法能有效缓解训练中的模态转移难题，促进RGB到DVS知识迁移，在能效与性能之间实现更优权衡，未来具有广泛的应用前景。

Abstract: The integration of event cameras and spiking neural networks holds great
promise for energy-efficient visual processing. However, the limited
availability of event data and the sparse nature of DVS outputs pose challenges
for effective training. Although some prior work has attempted to transfer
semantic knowledge from RGB datasets to DVS, they often overlook the
significant distribution gap between the two modalities. In this paper, we
propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing
strategy that exploits the asynchronous nature of SNNs by interpolating RGB and
DVS inputs at various time-steps. To enable label mixing in cross-modal
scenarios, we further introduce modality-aware auxiliary learning objectives.
These objectives support the time-step mixup process and enhance the model's
ability to discriminate effectively across different modalities. Our approach
enables smoother knowledge transfer, alleviates modality shift during training,
and achieves superior performance in spiking image classification tasks.
Extensive experiments demonstrate the effectiveness of our method across
multiple datasets. The code will be released after the double-blind review
process.

</details>


### [77] [MMMS: Multi-Modal Multi-Surface Interactive Segmentation](https://arxiv.org/abs/2509.12963)
*Robin Schön,Julian Lorenz,Katja Ludwig,Daniel Kienzle,Rainer Lienhart*

Main category: cs.CV

TL;DR: 本文提出了一种基于用户点击的交互式多表面分割方法，支持多模态输入，并引入了新的评价指标，用于处理复杂图像中多个表面的贴合及交互问题。该方法在多个数据集上展现了有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割方法难以应对同一图像中多个复杂或纠缠的目标表面，且如何结合多模态信息和用户交互以提升分割质量仍有技术难题。

Method: 提出了一种基于深度网络的多模态交互式分割架构，输入包括RGB图像、非RGB模态、初始分割掩码及点击编码。模型亮点包括黑盒式RGB骨干和特定的信息融合机制，以便在特征提取和多模态融合后集成用户交互信息。

Result: 在DeLiVER和MFNet数据集上，通过引入多模态输入，平均每个表面的点击数分别减少1.28和1.19，显示分割精度和效率提升。RGB-only基线在单掩膜场景下依然表现出色，有时优于其他方法。

Conclusion: 提出的方法有效利用多模态数据与用户交互，实现了复杂多表面场景下的高质量分割，并提出了适用该场景的全新评价方法。RGB-only方法亦具备竞争力。

Abstract: In this paper, we present a method to interactively create segmentation masks
on the basis of user clicks. We pay particular attention to the segmentation of
multiple surfaces that are simultaneously present in the same image. Since
these surfaces may be heavily entangled and adjacent, we also present a novel
extended evaluation metric that accounts for the challenges of this scenario.
Additionally, the presented method is able to use multi-modal inputs to
facilitate the segmentation task. At the center of this method is a network
architecture which takes as input an RGB image, a number of non-RGB modalities,
an erroneous mask, and encoded clicks. Based on this input, the network
predicts an improved segmentation mask. We design our architecture such that it
adheres to two conditions: (1) The RGB backbone is only available as a
black-box. (2) To reduce the response time, we want our model to integrate the
interaction-specific information after the image feature extraction and the
multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface
interactive segmentation (MMMS). We are able to show the effectiveness of our
multi-modal fusion strategy. Using additional modalities, our system reduces
the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to
1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline
achieves competitive, and in some cases even superior performance when tested
in a classical, single-mask interactive segmentation scenario.

</details>


### [78] [ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)](https://arxiv.org/abs/2509.12965)
*Silvia Zottin,Axel De Nardin,Giuseppe Branca,Claudio Piciarelli,Gian Luca Foresti*

Main category: cs.CV

TL;DR: 本文提出了一项针对古代手写文档文本行分割的FEST竞赛，要求参赛者在U-DIADS-TL数据集上，仅用每份手稿3张标注图片进行训练，推动少样本学习方法的发展。


<details>
  <summary>Details</summary>
Motivation: 古代手写文档存在书写不规则、墨迹褪色、版式复杂等难题，且缺乏大规模标注数据，传统全监督方法难以应用，因此亟需少样本学习方法以减轻人工标注负担。

Method: 本研究通过举办FEST竞赛，设定参赛者在多样化、退化程度高和格式不标准的U-DIADS-TL数据集上，仅能使用极少量（每个手稿3张）标注图像进行文本行分割系统训练，强调模型的少样本适应能力和泛化能力。

Result: 竞赛的数据集高度反映真实历史文档情况，促进了能够在极少标注条件下实现鲁棒分割的系统研发，推动了该领域方法的进步和创新。

Conclusion: FEST竞赛验证和推动了少样本学习在历史手写文档文本行分割中的应用，为人文学者提供了更易用、需人工标注更少的自动化文档分析工具，有助于这些工具在历史研究中更广泛应用。

Abstract: Text line segmentation is a critical step in handwritten document image
analysis. Segmenting text lines in historical handwritten documents, however,
presents unique challenges due to irregular handwriting, faded ink, and complex
layouts with overlapping lines and non-linear text flow. Furthermore, the
scarcity of large annotated datasets renders fully supervised learning
approaches impractical for such materials. To address these challenges, we
introduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents
(FEST) Competition. Participants are tasked with developing systems capable of
segmenting text lines in U-DIADS-TL dataset, using only three annotated images
per manuscript for training. The competition dataset features a diverse
collection of ancient manuscripts exhibiting a wide range of layouts,
degradation levels, and non-standard formatting, closely reflecting real-world
conditions. By emphasizing few-shot learning, FEST competition aims to promote
the development of robust and adaptable methods that can be employed by
humanities scholars with minimal manual annotation effort, thus fostering
broader adoption of automated document analysis tools in historical research.

</details>


### [79] [SHREC 2025: Protein surface shape retrieval including electrostatic potential](https://arxiv.org/abs/2509.12976)
*Taher Yacoub,Camille Depenveiller,Atsushi Tatsuma,Tin Barisin,Eugen Rusakov,Udo Gobel,Yuxu Peng,Shiqiang Deng,Yuki Kagaya,Joon Hong Park,Daisuke Kihara,Marco Guerra,Giorgio Palmieri,Andrea Ranieri,Ulderico Fugacci,Silvia Biasotti,Ruiwen He,Halim Benhabiles,Adnane Cabani,Karim Hammoudi,Haotian Li,Hao Huang,Chunyan Li,Alireza Tehrani,Fanwang Meng,Farnaz Heidar-Zadeh,Tuan-Anh Yang,Matthieu Montes*

Main category: cs.CV

TL;DR: 本文介绍了SHREC 2025蛋白质表面形状检索赛道的结果，9支队伍提出了15种方法，在含有11,555个蛋白质表面的数据集上进行了评测，重点考察了电势等分子表面描述子在检索性能提升中的作用。


<details>
  <summary>Details</summary>
Motivation: 蛋白质表面形状对于生物分子间作用至关重要，高效准确地检索相似形状有助于药物设计和生物功能预测。然而，传统检索多数仅考虑形状本身，未充分利用分子表面上的电势等信息。

Method: 组织比赛让9个团队提出15种检索方法，在包含电势的蛋白质表面大数据集上，采用Accuracy、F1、Precision、Recall等多指标系统评测检索性能，比较是否融合了电势信息方法的效果差异。

Result: 融合电势等表面分子描述子的检索方法在整体及少样本类别上均取得了最佳表现，显著优于仅用形状信息的方法。

Conclusion: 结合电势等多模态分子表面描述子能显著提升蛋白质表面形状检索的效果，尤其在类别样本稀少时作用更突出，未来应充分利用更多表面属性提升相关应用。

Abstract: This SHREC 2025 track dedicated to protein surface shape retrieval involved 9
participating teams. We evaluated the performance in retrieval of 15 proposed
methods on a large dataset of 11,555 protein surfaces with calculated
electrostatic potential (a key molecular surface descriptor). The performance
in retrieval of the proposed methods was evaluated through different metrics
(Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best
retrieval performance was achieved by the proposed methods that used the
electrostatic potential complementary to molecular surface shape. This
observation was also valid for classes with limited data which highlights the
importance of taking into account additional molecular surface descriptors.

</details>


### [80] [Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER](https://arxiv.org/abs/2509.12980)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Steven H. Frankel*

Main category: cs.CV

TL;DR: 论文提出了 WINNER（神经表示加噪权重初始化）方法，通过在 SIREN 网络权重初始化中引入与目标信号光谱分布相关的自适应高斯噪声，显著提升了 SIREN 在处理不同频率信号时的表现，解决了“频谱瓶颈”问题。该方法无需增加额外参数，在音频、图像和三维形状拟合任务中均取得了优越的效果。


<details>
  <summary>Details</summary>
Motivation: SIREN（正弦表达网络）能够高效地拟合高频信号，但在初始化不当或目标信号频谱与网络不匹配时，存在“频谱瓶颈”现象，模型输出几乎为零且无法学习。在实际任务中，这极大制约了 SIREN 在各种信号（如音频、图像、3D 形状）处理中的能力。因此作者希望解决这一根本性缺陷。

Method: 作者提出 WINNER 方法，即在 SIREN 的均匀初始化权重基础上，按目标信号的光谱质心自适应地叠加高斯噪声（噪声尺度随目标频谱分布调整）。新方法无需引入其他可训练参数，类似随机傅立叶嵌入（RFE）策略，但更简洁高效。

Result: WINNER 在音频拟合中取得了最优效果，并在图像、三维形状拟合等任务上，相较于基础 SIREN 都有显著提升。

Conclusion: WINNER 有效地缓解了 SIREN 频谱偏置问题，极大拓展了该模型在各种不同信号拟合任务上的适用性，并为更广泛的自适应、目标感知的深度网络初始化方法开辟了新方向。

Abstract: We identify and address a fundamental limitation of sinusoidal representation
networks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann
et al. (2020), when not initialized appropriately, can struggle at fitting
signals that fall outside their frequency support. In extreme cases, when the
network's frequency support misaligns with the target spectrum, a 'spectral
bottleneck' phenomenon is observed, where the model yields to a near-zero
output and fails to recover even the frequency components that are within its
representational capacity. To overcome this, we propose WINNER - Weight
Initialization with Noise for Neural Representations. WINNER perturbs uniformly
initialized weights of base SIREN with Gaussian noise - whose noise scales are
adaptively determined by the spectral centroid of the target signal. Similar to
random Fourier embeddings, this mitigates 'spectral bias' but without
introducing additional trainable parameters. Our method achieves
state-of-the-art audio fitting and significant gains in image and 3D shape
fitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new
avenues in adaptive, target-aware initialization strategies for optimizing deep
neural network training. For code and data visit
cfdlabtechnion.github.io/siren_square/.

</details>


### [81] [PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era](https://arxiv.org/abs/2509.12989)
*Xu Zheng,Chenfei Liao,Ziqiao Weng,Kaiyu Lei,Zihao Dongfang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Lu Qi,Li Chen,Danda Pani Paudel,Kailun Yang,Linfeng Zhang,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 综述了全景视觉（全方位视觉）在机器人、工业检测和环境监测等领域的重要性和发展趋势，探讨了最新的突破和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着工业需求增长和学术关注提升，全景视觉被认为能显著提升场景感知的全面性和决策可靠性，但相关基础研究相对滞后，亟需系统梳理和推进。

Method: 本文综述了全景生成、感知、理解及相关数据集的最新进展，提出了PANORAMA全景系统架构，包括四个关键子系统，并结合学术和工业观点分析趋势与挑战。

Result: 总结了全景视觉领域在生成、感知、理解等方面的突破，提出理想系统架构，分析了和具身AI结合的趋势与影响，指出了未来的研究方向和待解决的问题。

Conclusion: 全景视觉正迅速发展并在具身AI时代具有广阔前景，亟需进一步突破和融合，构建通用、鲁棒的全方位AI系统。

Abstract: Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.

</details>


### [82] [Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection](https://arxiv.org/abs/2509.12990)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Sicong Li,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dual-Stage Reweighted Mixture-of-Experts (DR-MoE)的新方法，以从第一视角视频中识别用户动作错误，尤其对罕见和细微错误表现优异。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，用户动作错误往往发生频率低且不易察觉，因此需要一种能够有效识别稀有错误实例的方法。传统算法在处理类别不平衡和难识别样本时效果有限。

Method: 方法包括两个阶段：第一阶段用冻结ViViT和LoRA微调ViViT特征提取，并通过特征级专家模块融合；第二阶段训练三个分类器，分别采用重加权交叉熵、AUC损失和结合Label-aware及Sharpness-aware最小化的损失，最后通过分类级专家模块进行结果融合。

Result: 实验结果显示，所提方法在识别稀有与模糊错误实例方面性能显著优于已有方法。

Conclusion: DR-MoE框架有效增强了第一视角视频中稀有动作错误识别能力，对于实际应用中动作监测及纠错有重要意义。

Abstract: In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.

</details>


### [83] [Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection](https://arxiv.org/abs/2509.12995)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Jinhua Zeng,Bin Li*

Main category: cs.CV

TL;DR: 专用的AI生成图片检测器在现实场景下表现糟糕，而直接在最新视觉基础模型（VFM）上训练一个简单的线性分类器能大幅提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有专用检测器虽然在标准数据集上表现优异，但在真实环境下检出率低，因此需要更有效的方法来检测AI生成图片。

Method: 作者用相同训练数据，在现代VFM上训练简单的线性分类器，并比较其与专用检测器的表现。通过分析VLMs（视觉语言模型）对合成图片和伪造概念的对齐能力，并测试模型对训练集外的新数据的泛化性。

Result: 基于VFM的简单线性分类器比专用检测器在真实场景基准上准确率高出20%以上。VFM能够更好地识别AI生成图片，但这种能力依赖于训练数据的时间范围，当遇到全新的数据集时，准确率会显著下降。

Conclusion: 1）最新VFM在AI图像检测方面远强于定制检测器；2）想要真正评估模型泛化能力，测试数据必须完全独立于模型训练和预训练过程。

Abstract: While specialized detectors for AI-generated images excel on curated
benchmarks, they fail catastrophically in real-world scenarios, as evidenced by
their critically high false-negative rates on `in-the-wild' benchmarks. Instead
of crafting another specialized `knife' for this problem, we bring a `gun' to
the fight: a simple linear classifier on a modern Vision Foundation Model
(VFM). Trained on identical data, this baseline decisively `outguns' bespoke
detectors, boosting in-the-wild accuracy by a striking margin of over 20\%.
  Our analysis pinpoints the source of the VFM's `firepower': First, by probing
text-image similarities, we find that recent VLMs (e.g., Perception Encoder,
Meta CLIP2) have learned to align synthetic images with forgery-related
concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate
that this is due to data exposure, as both this alignment and overall accuracy
plummet on a novel dataset scraped after the VFM's pre-training cut-off date,
ensuring it was unseen during pre-training. Our findings yield two critical
conclusions: 1) For the real-world `gunfight' of AI-generated image detection,
the raw `firepower' of an updated VFM is far more effective than the
`craftsmanship' of a static detector. 2) True generalization evaluation
requires test data to be independent of the model's entire training history,
including pre-training.

</details>


### [84] [Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire](https://arxiv.org/abs/2509.12997)
*Anton Eldeborg Lundin,Rasmus Winzell,Hanna Hamrell,David Gustafsson,Hannes Ovrén*

Main category: cs.CV

TL;DR: 本文提出了一种基于脉冲神经网络和神经形态相机（事件相机）的无人机自动检测系统，系统实现为端到端的神经形态实现，具有极高能效和便携性。


<details>
  <summary>Details</summary>
Motivation: 小型无人机对军队和民用基础设施日益构成威胁，因此需要早期且自动化的检测手段。传统检测方案能耗较高，不便于在缺乏电力设施的区域部署。

Method: 作者开发了一个利用脉冲神经网络和神经形态（事件）相机的无人机检测系统，并将检测模型部署在神经形态芯片上，实现了完全的神经形态系统。通过多点部署可形成虚拟警戒线，用于检测无人机进入受限区域。模型训练过程中还探讨了合成数据的应用。

Result: 该神经形态检测系统在能耗方面比基于edge GPU的参考方案高出若干数量级，使得系统用电池即可运行超过一年。实验表明，模型主要依赖无人机的形状特征而非螺旋桨的时序信息进行检测。系统体积小、能效高，适合在电力基础设施不足或对抗性区域布置。

Conclusion: 所提出的神经形态无人机检测系统兼具高能效与便携性，适合在缺乏电力资源的复杂环境下长时间运行，为军用和民用安全保护提供了优良的自动化检测解决方案。

Abstract: Small drones are an increasing threat to both military personnel and civilian
infrastructure, making early and automated detection crucial. In this work we
develop a system that uses spiking neural networks and neuromorphic cameras
(event cameras) to detect drones. The detection model is deployed on a
neuromorphic chip making this a fully neuromorphic system. Multiple detection
units can be deployed to create a virtual tripwire which detects when and where
drones enter a restricted zone. We show that our neuromorphic solution is
several orders of magnitude more energy efficient than a reference solution
deployed on an edge GPU, allowing the system to run for over a year on battery
power. We investigate how synthetically generated data can be used for
training, and show that our model most likely relies on the shape of the drone
rather than the temporal characteristics of its propellers. The small size and
low power consumption allows easy deployment in contested areas or locations
that lack power infrastructure.

</details>


### [85] [Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image](https://arxiv.org/abs/2509.13013)
*Gaofeng Liu,Hengsen Li,Ruoyu Gao,Xuetong Li,Zhiyuan Ma,Tao Fang*

Main category: cs.CV

TL;DR: 提出了一种高效、可文本控制的两阶段3D虚拟人生成方法Dream3DAvatar，实现了从单张图片生成可动画化的高保真3D虚拟人，并在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单张图像重建3D虚拟人方法，由于单目输入信息不足，在遮挡区域的几何和纹理可控性上存在挑战，生成质量难以保障。

Method: 方法分为两阶段：第一阶段提出带有Pose-Adapter和ID-Adapter-G的多视角轻量生成模型，通过融合SMPL-X和骨骼信息增强几何一致性，并采用BLIP2生成高质量文本描述提升遮挡区域的文本可控性；第二阶段设计带多视图特征融合的Transformer，将多视图图像重建为高保真3D Gaussian Splat（3DGS）模型，并通过ID-Adapter-R提升面部细节还原。

Result: 实验结果表明，该方法可自动生成高真实感、可直接用于动画的3D虚拟人，并在多项评测指标上超过现有同类方法，无需后处理。

Conclusion: Dream3DAvatar有效提升了单图3D虚拟人重建的保真度与可控性，在几何、姿态和细节还原方面表现出色，对虚拟人生成领域具有实际应用前景。

Abstract: With the rapid advancement of 3D representation techniques and generative
models, substantial progress has been made in reconstructing full-body 3D
avatars from a single image. However, this task remains fundamentally
ill-posedness due to the limited information available from monocular input,
making it difficult to control the geometry and texture of occluded regions
during generation. To address these challenges, we redesign the reconstruction
pipeline and propose Dream3DAvatar, an efficient and text-controllable
two-stage framework for 3D avatar generation. In the first stage, we develop a
lightweight, adapter-enhanced multi-view generation model. Specifically, we
introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information
into SDXL, enforcing geometric and pose consistency across views. To preserve
facial identity, we incorporate ID-Adapter-G, which injects high-resolution
facial features into the generation process. Additionally, we leverage BLIP2 to
generate high-quality textual descriptions of the multi-view images, enhancing
text-driven controllability in occluded regions. In the second stage, we design
a feedforward Transformer model equipped with a multi-view feature fusion
module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)
from the generated images. Furthermore, we introduce ID-Adapter-R, which
utilizes a gating mechanism to effectively fuse facial features into the
reconstruction process, improving high-frequency detail recovery. Extensive
experiments demonstrate that our method can generate realistic, animation-ready
3D avatars without any post-processing and consistently outperforms existing
baselines across multiple evaluation metrics.

</details>


### [86] [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.13031)
*Yan Chen,Long Li,Teng Xi,Long Zeng,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段强化学习框架，用于同时提升视觉-语言模型（VLMs）的感知和推理能力，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习已被证明能有效提升大语言模型的推理能力，但直接移植至视觉-语言模型效果并不理想，因为VLMs的任务更复杂，需要先准确感知视觉信息。为了解决VLMs推理前需高质量视觉理解的难题，作者提出专为VLMs设计的新方法。

Method: 作者提出两阶段强化学习框架：首先通过数据级采样分别提升模型的感知或推理能力；训练时，第一阶段聚焦于粗细粒度的视觉理解提升感知能力，第二阶段则专注于增强推理能力。整个流程旨在针对VLMs感知与推理的特点逐步优化。

Result: 该方法训练得到了PeBR-R1模型，在七个基准数据集上的实验结果表明其显著提升了模型的视觉理解与推理性能，且在多项视觉推理任务上优于现有方法。

Conclusion: 两阶段强化学习设计能有效联合提升VLMs的视觉感知和推理表现，为VLMs的进一步发展提供了新思路和技术路径。

Abstract: Reinforcement learning (RL) has proven highly effective in eliciting the
reasoning capabilities of large language models (LLMs). Inspired by this
success, recent studies have explored applying similar techniques to
vision-language models (VLMs), aiming to enhance their reasoning performance.
However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as
the tasks faced by VLMs are inherently more complex. Specifically, VLMs must
first accurately perceive and understand visual inputs before reasoning can be
effectively performed. To address this challenge, we propose a two-stage
reinforcement learning framework designed to jointly enhance both the
perceptual and reasoning capabilities of VLMs. To mitigate the vanishing
advantage issue commonly observed in RL training, we first perform
dataset-level sampling to selectively strengthen specific capabilities using
distinct data sources. During training, the first stage focuses on improving
the model's visual perception through coarse- and fine-grained visual
understanding, while the second stage targets the enhancement of reasoning
abilities. After the proposed two-stage reinforcement learning process, we
obtain PeBR-R1, a vision-language model with significantly enhanced perceptual
and reasoning capabilities. Experimental results on seven benchmark datasets
demonstrate the effectiveness of our approach and validate the superior
performance of PeBR-R1 across diverse visual reasoning tasks.

</details>


### [87] [HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models](https://arxiv.org/abs/2509.13067)
*Xu Li,Yuxuan Liang,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HERO的高效高分辨率视觉-语言模型视觉token早期丢弃框架，在保持模型精度的同时大幅减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视觉-语言模型（HR-LVLMs）通常将图片裁剪成多个小块并分别编码，从而提升了对图像细节的理解能力，但也导致视觉token数量激增，带来高昂的计算和内存成本。如何兼顾高分辨率图像理解和计算效率，是一个亟需解决的问题。

Method: 作者系统性分析了HR-LVLMs中视觉token的利用方式，发现不同tile的重要性有较大差别，并揭示了CLIP编码器CLS token的关注阶段变化。基于这些观察，提出HERO框架：通过内容自适应分配token预算和功能感知的token选择机制，智能地仅保留重要且互补的视觉tokens，从而有效减少token数量，在推理阶段无须重新训练。

Result: HERO在多个数据集测试和不同规模模型中，均取得更优的效率与准确率权衡，显著降低了推理的资源消耗。

Conclusion: 本研究不仅提供了HR-LVLMs视觉token利用机制的实证洞察，还提出了切实可行的高效推理方案，有助于推动大模型实际部署应用。

Abstract: By cropping high-resolution images into local tiles and encoding them
independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have
demonstrated remarkable fine-grained visual understanding capabilities.
However, this divide-and-conquer paradigm significantly increases the number of
visual tokens, resulting in substantial computational and memory overhead. To
better understand and address this challenge, we empirically investigate visual
token utilization in HR-LVLMs and uncover three key findings: (1) the local
tiles have varying importance, jointly determined by visual saliency and task
relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage
attention pattern across layers, with each stage attending to different types
of visual tokens; (3) the visual tokens emphasized at different stages encode
information at varying levels of granularity, playing complementary roles
within LVLMs. Building on these insights, we propose HERO, a High-resolution
visual token early dropping framework that integrates content-adaptive token
budget allocation with function-aware token selection. By accurately estimating
tile-level importance and selectively retaining visual tokens with
complementary roles, HERO achieves superior efficiency-accuracy trade-offs
across diverse benchmarks and model scales, all in a training-free manner. This
study provides both empirical insights and practical solutions toward efficient
inference in HR-LVLMs.

</details>


### [88] [TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation](https://arxiv.org/abs/2509.13070)
*Qianqi Lu,Yuxiang Xie,Jing Zhang,Shiwei Zou,Yan Chen,Xidao Luan*

Main category: cs.CV

TL;DR: 本论文关注于基于文本描述的图像分割（RIS）任务，提出了一种三阶段图像-文本特征对齐网络（TFANet），有效提升了多模态对齐与语义保留，在复杂场景分割中取得更佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有的RIS方法在多模态对齐和语言语义保留上表现不佳，尤其在存在多个外观相似物体且目标由唯一描述时，常出现目标定位错误或分割不完整。本论文旨在解决复杂场景下的多模态对齐、长距离依赖建模和语义保留难题。

Method: 提出了三阶段的对齐网络TFANet，包括三大模块：1）多尺度线性交叉注意力模块（MLAM），实现不同尺度下图文语义的双向交流和对齐；2）跨模态特征扫描模块（CFSM），利用选择性扫描增强长距离依赖建模和多模态融合；3）词级语言特征引导语义深化模块（WFDM），针对前面阶段的语义衰减进行补偿和语义深化。

Result: 通过在复杂场景下的实验，TFANet在多模态对齐和目标分割准确性方面较现有方法有明显提升，特别是在针对多个相似物体、唯一目标描述等高难度情境下表现突出。

Conclusion: TFANet多阶段分层对齐结构和创新模块有效解决了RIS任务中的多模态错配和语义丢失，在复杂场景中优于现有主流方法，具有较高的应用价值和理论意义。

Abstract: Referring Image Segmentation (RIS) is a task that segments image regions
based on language expressions, requiring fine-grained alignment between two
modalities. However, existing methods often struggle with multimodal
misalignment and language semantic loss, especially in complex scenes
containing multiple visually similar objects, where uniquely described targets
are frequently mislocalized or incompletely segmented. To tackle these
challenges, this paper proposes TFANet, a Three-stage Image-Text Feature
Alignment Network that systematically enhances multimodal alignment through a
hierarchical framework comprising three stages: Knowledge Plus Stage (KPS),
Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the
first stage, we design the Multiscale Linear Cross-Attention Module (MLAM),
which facilitates bidirectional semantic exchange between visual features and
textual representations across multiple scales. This establishes rich and
efficient alignment between image regions and different granularities of
linguistic descriptions. Subsequently, the KFS further strengthens feature
alignment through the Cross-modal Feature Scanning Module (CFSM), which applies
multimodal selective scanning to capture long-range dependencies and construct
a unified multimodal representation. This is essential for modeling long-range
cross-modal dependencies and enhancing alignment accuracy in complex scenes.
Finally, in the KIS, we propose the Word-level Linguistic Feature-guided
Semantic Deepening Module (WFDM) to compensate for semantic degradation
introduced in earlier stages.

</details>


### [89] [Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement](https://arxiv.org/abs/2509.13083)
*Yan Xingyang,Huang Xiaohong,Zhang Zhao,You Tian,Xu Ziheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度增强网络LLFDisc，结合了自注意力和门控机制，并引入了基于KL散度的分布感知损失，用于更有效地保留和增强图像的傅里叶域信息，实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的傅里叶频域信息拟合方法多采用像素级损失函数，导致过度关注局部信息而可能损失全局信息，因此需要一种更能保持全局结构的损失设计和网络结构。

Method: 提出LLFDisc网络：采用U型架构，整合了跨注意力机制和门控机制，使模型能够在增强过程中更好地利用频域信息。引入了新的分布感知损失函数，利用KL散度对傅里叶域信息进行直接拟合，并在VGG感知损失中融合KL散度增强结构保真能力。

Result: 在多个公开基准数据集上的广泛实验表明，LLFDisc在定性和定量评价中均达到当前最优的效果。

Conclusion: 所提方法在保留和增强图像傅里叶域信息、结构保真度以及整体视觉效果上优于现有方法，具有良好应用前景。

Abstract: In the Fourier domain, luminance information is primarily encoded in the
amplitude spectrum, while spatial structures are captured in the phase
components. The traditional Fourier Frequency information fitting employs
pixel-wise loss functions, which tend to focus excessively on local information
and may lead to global information loss. In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement. We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective. This
enables the model to align Fourier-domain information more robustly than with
conventional MSE-based losses. Furthermore, we enhance the perceptual loss
based on VGG by embedding KL-Divergence on extracted deep features, enabling
better structural fidelity. Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations. Our code will be released at:
https://github.com/YanXY000/LLFDisc

</details>


### [90] [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](https://arxiv.org/abs/2509.13084)
*Yunyao Lu,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双网络半监督3D医学图像分割框架，通过结合跨伪标签和熵过滤的监督方法，有效降低了伪标签噪音，并通过动态加权策略和对比学习机制增强了特征空间的监督与不确定性处理，达到优于现有方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的监督式医学图像分割需要大量标注数据，但在实际中往往难以获取。半监督方法利用未标注数据，但现有方法存在伪标签噪声大和特征空间监督不足的问题。

Method: 提出了基于双网络架构的半监督分割方法。包括跨一致性增强模块（结合跨伪标签和熵过滤减少噪声），动态加权策略（基于不确定性感知调整伪标签贡献），以及自监督式对比学习（区分可信与不可信预测，减少不确定性）。

Result: 在Left Atrial、NIH Pancreas和BraTS-2019这三个3D分割数据集上进行了大量实验，方法在不同设定下均优于SOTA（如在Left Atrial上仅用10%标注数据达到89.95%的Dice分数），并通过消融实验验证了各模块有效性。

Conclusion: 所提方法在半监督3D医学图像分割任务中能够更有效利用未标注数据，显著提升分割性能并减少对标注数据的依赖，具有良好的实际应用前景。

Abstract: Despite the remarkable performance of supervised medical image segmentation
models, relying on a large amount of labeled data is impractical in real-world
situations. Semi-supervised learning approaches aim to alleviate this challenge
using unlabeled data through pseudo-label generation. Yet, existing
semi-supervised segmentation methods still suffer from noisy pseudo-labels and
insufficient supervision within the feature space. To solve these challenges,
this paper proposes a novel semi-supervised 3D medical image segmentation
framework based on a dual-network architecture. Specifically, we investigate a
Cross Consistency Enhancement module using both cross pseudo and
entropy-filtered supervision to reduce the noisy pseudo-labels, while we design
a dynamic weighting strategy to adjust the contributions of pseudo-labels using
an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In
addition, we use a self-supervised contrastive learning mechanism to align
uncertain voxel features with reliable class prototypes by effectively
differentiating between trustworthy and uncertain predictions, thus reducing
prediction uncertainty. Extensive experiments are conducted on three 3D
segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed
approach consistently exhibits superior performance across various settings
(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to
the state-of-the-art methods. Furthermore, the usefulness of the proposed
modules is further validated via ablation experiments.

</details>


### [91] [Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2509.13107)
*Kohou Wang,Huan Hu,Xiang Liu,Zezhou Chen,Ping Chen,Zhaoxiang Liu,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出了一种用于检测深度伪造人脸的集成深度学习架构（HDFF），通过集成多种预训练模型实现了高效伪造检测，在比赛中获得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的发展对数字安全和内容真实性构成了严峻挑战，现有检测方法难以广泛适应多种复杂伪造手段，因此需要更加鲁棒和泛化能力强的检测模型。

Method: 作者提出了层次化深度融合框架（HDFF），集成了Swin-MLP、CoAtNet、EfficientNetV2和DaViT四个经过预训练的子模型，并在MultiFFDI数据集上分阶段微调。通过特征级联，将各子模型的特征整合，再训练最终分类层实现判别。

Result: 该框架在竞赛私人排行榜上取得了0.96852的高分，最终位列184支队伍中的第20名。

Conclusion: HDFF能够有效融合多模型特征，提升复杂图像分类和深度伪造检测的性能，验证了分层融合策略的有效性。

Abstract: The proliferation of sophisticated deepfake technology poses significant
challenges to digital security and authenticity. Detecting these forgeries,
especially across a wide spectrum of manipulation techniques, requires robust
and generalized models. This paper introduces the Hierarchical Deep Fusion
Framework (HDFF), an ensemble-based deep learning architecture designed for
high-performance facial forgery detection. Our framework integrates four
diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,
which are meticulously fine-tuned through a multi-stage process on the
MultiFFDI dataset. By concatenating the feature representations from these
specialized models and training a final classifier layer, HDFF effectively
leverages their collective strengths. This approach achieved a final score of
0.96852 on the competition's private leaderboard, securing the 20th position
out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex
image classification tasks.

</details>


### [92] [Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving](https://arxiv.org/abs/2509.13116)
*Ruibo Li,Hanyu Shi,Zhe Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: 本文提出了一种针对激光雷达点云的弱监督与自监督无类别运动预测方法，可用极少量或无需人工标注实现高效运动理解，性能优于现有自监督方法，部分情况下接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等动态环境下，理解物体运动至关重要，但全监督方法的数据标注代价高昂，因此亟需通过弱监督或自监督方式，在少量甚至无需标注的情况下，提升运动预测能力。

Method: 作者提出用前景/背景或非地面/地面掩码取代传统运动标注，采用极少量（如1%、0.1%、0.01%）掩码实现弱监督，同时也有纯自监督方法；并设计了鲁棒一致性Chamfer距离损失以提升自监督训练效果，抑制异常点影响。

Result: 实验显示，提出的弱监督与自监督模型在运动预测任务上优于现有自监督方法，弱监督模型在某些场景下性能甚至可与全监督模型媲美。

Conclusion: 该研究提出的方法在运动预测任务中实现了性能与标注成本的良好平衡，为降低自动驾驶等应用中的数据标注负担提供了有效途径。

Abstract: Understanding motion in dynamic environments is critical for autonomous
driving, thereby motivating research on class-agnostic motion prediction. In
this work, we investigate weakly and self-supervised class-agnostic motion
prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile
foregrounds and static backgrounds, allowing motion understanding to be
associated with scene parsing. Based on this observation, we propose a novel
weakly supervised paradigm that replaces motion annotations with fully or
partially annotated (1%, 0.1%) foreground/background masks for supervision. To
this end, we develop a weakly supervised approach utilizing
foreground/background cues to guide the self-supervised learning of motion
prediction models. Since foreground motion generally occurs in non-ground
regions, non-ground/ground masks can serve as an alternative to
foreground/background masks, further reducing annotation effort. Leveraging
non-ground/ground cues, we propose two additional approaches: a weakly
supervised method requiring fewer (0.01%) foreground/background annotations,
and a self-supervised method without annotations. Furthermore, we design a
Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame
information and robust penalty functions to suppress outliers in
self-supervised learning. Experiments show that our weakly and self-supervised
models outperform existing self-supervised counterparts, and our weakly
supervised models even rival some supervised ones. This demonstrates that our
approaches effectively balance annotation effort and performance.

</details>


### [93] [Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline](https://arxiv.org/abs/2509.13133)
*Zhihao Zhang,Chunyu Lin,Lang Nie,Jiyuan Wang,Yao Zhao*

Main category: cs.CV

TL;DR: 本文针对自动泊车系统中的车位检测问题，提出了大规模多样化的数据集CRPS-D，并首次应用半监督学习方法（SS-PSD）提升检测性能。SS-PSD能充分利用无标签数据，在多个数据集上均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有车位检测数据集规模小，缺乏复杂环境噪声，且人工标注成本高、易出错，阻碍实际应用效果和算法发展。

Method: 1）构建CRPS-D大规模车位检测数据集，涵盖多种光照、天气和复杂车位形态；2）提出半监督学习基线SS-PSD，基于教师-学生模型，结合置信引导的掩码一致性和自适应特征扰动，利用无标签数据协同训练。

Result: 1）CRPS-D数据集相比已有数据集规模更大、场景更多样、车位更密集；2）SS-PSD在新数据集和公开数据集上性能均优于最新方法，且利用更多无标签数据时，精度提升更显著。

Conclusion: CRPS-D为复杂实际场景下的车位检测提供了强有力数据支持，提出的SS-PSD方法有效提升了检测性能，推动了自动泊车系统的进一步发展。数据与代码已开源。

Abstract: As automatic parking systems evolve, the accurate detection of parking slots
has become increasingly critical. This study focuses on parking slot detection
using surround-view cameras, which offer a comprehensive bird's-eye view of the
parking environment. However, the current datasets are limited in scale, and
the scenes they contain are seldom disrupted by real-world noise (e.g., light,
occlusion, etc.). Moreover, manual data annotation is prone to errors and
omissions due to the complexity of real-world conditions, significantly
increasing the cost of annotating large-scale datasets. To address these
issues, we first construct a large-scale parking slot detection dataset (named
CRPS-D), which includes various lighting distributions, diverse weather
conditions, and challenging parking slot variants. Compared with existing
datasets, the proposed dataset boasts the largest data scale and consists of a
higher density of parking slots, particularly featuring more slanted parking
slots. Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data. To our knowledge, this is the first semi-supervised approach in
parking slot detection, which is built on the teacher-student model with
confidence-guided mask consistency and adaptive feature perturbation.
Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset. Particularly, the more unlabeled data there is, the more significant
the gains brought by our semi-supervised scheme. The relevant source codes and
the dataset have been made publicly available at
https://github.com/zzh362/CRPS-D.

</details>


### [94] [MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation](https://arxiv.org/abs/2509.13149)
*Minqing Huang,Shouyi Lu,Boyuan Zheng,Ziyao Li,Xiao Tang,Guirong Zhuo*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的多阶段蒸馏框架MSDNet，用于实现4D雷达点云超分辨率重建，既提升准确性又兼顾推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达点云超分方法存在训练代价高、依赖复杂采样方法、推理延迟大和泛化性差等问题，难以在准确率与效率之间取得平衡。

Method: 作者提出MSDNet，包括两个阶段：第一阶段进行重建引导的特征蒸馏，通过特征重构对学生网络特征对齐密化；第二阶段采用扩散引导特征蒸馏，将第一阶段输出视为有噪音版教师表示，用轻量级扩散网络进一步精细去噪。同时引入噪音适配器，根据扩散步长自适应对齐噪音水平。

Result: 在VoD和自有数据集上的大量实验表明，MSDNet实现了高保真重建和低延迟推理，同时下游任务上性能也有一致提升。

Conclusion: MSDNet能有效实现4D雷达点云超分辨率，提高重建质量与效率，方法具备实用性且代码将在发表时公开。

Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point
clouds into dense and geometrically consistent representations, is a
foundational problem in autonomous perception. However, existing methods often
suffer from high training cost or rely on complex diffusion-based sampling,
resulting in high inference latency and poor generalization, making it
difficult to balance accuracy and efficiency. To address these limitations, we
propose MSDNet, a multi-stage distillation framework that efficiently transfers
dense LiDAR priors to 4D radar features to achieve both high reconstruction
quality and computational efficiency. The first stage performs
reconstruction-guided feature distillation, aligning and densifying the
student's features through feature reconstruction. In the second stage, we
propose diffusion-guided feature distillation, which treats the stage-one
distilled features as a noisy version of the teacher's representations and
refines them via a lightweight diffusion network. Furthermore, we introduce a
noise adapter that adaptively aligns the noise level of the feature with a
predefined diffusion timestep, enabling a more precise denoising. Extensive
experiments on the VoD and in-house datasets demonstrate that MSDNet achieves
both high-fidelity reconstruction and low-latency inference in the task of 4D
radar point cloud super-resolution, and consistently improves performance on
downstream tasks. The code will be publicly available upon publication.

</details>


### [95] [TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images](https://arxiv.org/abs/2509.13151)
*Rohan Kumar,Jyothi Swaroopa Jinka,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 本论文提出了TexTAR，一种面向文本属性识别的多任务、上下文感知Transformer模型，在多语种、多领域下能够高效准确识别文本的粗体、斜体、下划线、删除线等视觉属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在嘈杂、多语言环境下识别文本属性时，面临效率低和适应性差的问题，而文本属性对理解文本语义、结构和视觉呈现具有重要意义。

Method: 提出了TexTAR模型，结合多任务Transformer结构和2D RoPE式上下文编码机制；同时设计了新的数据选择流程提升上下文感知能力；并构建了一个覆盖多语种和多领域（如法律、公告、教材）的MMDAT数据集，包含丰富的文本属性标注。

Result: 实验结果显示，TexTAR在准确率和泛化能力上均优于现有方法，特别是在属性识别任务中取得了SOTA性能。

Conclusion: 上下文感知机制对于提升文本属性识别效果十分关键，TexTAR方法具备高效和适应复杂实际场景的能力，可作为文档分析等任务的有力支持。

Abstract: Recognizing textual attributes such as bold, italic, underline and strikeout
is essential for understanding text semantics, structure, and visual
presentation. These attributes highlight key information, making them crucial
for document analysis. Existing methods struggle with computational efficiency
or adaptability in noisy, multilingual settings. To address this, we introduce
TexTAR, a multi-task, context-aware Transformer for Textual Attribute
Recognition (TAR). Our novel data selection pipeline enhances context
awareness, and our architecture employs a 2D RoPE (Rotary Positional
Embedding)-style mechanism to incorporate input context for more accurate
attribute predictions. We also introduce MMTAD, a diverse, multilingual,
multi-domain dataset annotated with text attributes across real-world documents
such as legal records, notices, and textbooks. Extensive evaluations show
TexTAR outperforms existing methods, demonstrating that contextual awareness
contributes to state-of-the-art TAR performance.

</details>


### [96] [Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)](https://arxiv.org/abs/2509.13161)
*Zhihao He,Tianyao He,Tieyuan Chen,Yun Xu,Huabin Liu,Chaofan Gan,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 本文提出了一种多视频协作框架以改进视频语言模型的推理能力，通过结构化视频表征和图融合技术有效利用相关视频信息。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型无法有效处理单个视频因时空不完整带来的幻觉和不准确问题，而直接用多个视频信息输入大语言模型又会引入冗余甚至负面影响，因而亟需高效、灵活的多视频协作方案。

Method: 作者首先提出视频结构化模块（Video Structuring Module），把视频知识表示为时空图；随后设计图融合模块（Graph Fusion Module）将多个相关视频的结构化知识整合入增强的图节点token中；最后构建多视频结构化提示，将图、视觉及文本token一同作为输入送入大语言模型。

Result: 大量实验验证了该框架能显著提升视频语言模型的推理能力，具备良好应用前景。

Conclusion: 多视频协作框架通过结构化方法有效缓解了单视频时空不完整问题，是提升视频语言推理能力的有效途径。

Abstract: Despite the prosperity of the video language model, the current pursuit of
comprehensive video reasoning is thwarted by the inherent spatio-temporal
incompleteness within individual videos, resulting in hallucinations and
inaccuracies. A promising solution is to augment the reasoning performance with
multiple related videos. However, video tokens are numerous and contain
redundant information, so directly feeding the relevant video data into a large
language model to enhance responses could be counterproductive. To address this
challenge, we propose a multi-video collaborative framework for video language
models. For efficient and flexible video representation, we establish a Video
Structuring Module to represent the video's knowledge as a spatio-temporal
graph. Based on the structured video representation, we design the Graph Fusion
Module to fuse the structured knowledge and valuable information from related
videos into the augmented graph node tokens. Finally, we construct an elaborate
multi-video structured prompt to integrate the graph, visual, and textual
tokens as the input to the large language model. Extensive experiments
substantiate the effectiveness of our framework, showcasing its potential as a
promising avenue for advancing video language models.

</details>


### [97] [WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory](https://arxiv.org/abs/2509.13172)
*Ruifei Ding,Zhe Chen,Wen Fan,Chen Long,Huijuan Xiao,Yelu Zeng,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 本论文介绍了WHU-STree，一个跨城市、丰富标注、多模态的城市街道树木数据集，用于支持自动化、多任务的街道树木普查与管理，并通过相关实验验证其跨域应用价值。


<details>
  <summary>Details</summary>
Motivation: 城市街道树木对生态和社会环境至关重要，但现有基于MMS的自动化树木普查数据集存在场景小、注释有限、模态单一等问题，难以满足大规模、全面分析的需求。因此，亟需更大规模、跨城市、多模态并且详细标注的数据集来推动相关算法与应用的发展。

Method: 提出并采集了WHU-STree数据集，涵盖了两座城市的同步点云与高分辨率影像，包含21007棵树木、50个物种和2个形态参数。该数据集支持十余种街道树木普查相关任务，并以树种分类和单株分割两项关键任务为例，进行了基准实验和多模态数据融合分析。

Result: 实验表明，多模态数据融合在街道树木相关任务上具有显著的提升潜力。分析还强调了数据集在跨域泛化能力方面的重要作用，并指出了现有方法在真实部署中面临的主要挑战。

Conclusion: WHU-STree为城市街道树木普查和算法研究提供了坚实的数据基础。多模态融合及跨域普适性是算法实际落地的关键所在，未来可在多模态协同、多任务学习、空间模式识别和多模态大模型等方向进一步深挖。本数据集已向公众开放。

Abstract: Street trees are vital to urban livability, providing ecological and social
benefits. Establishing a detailed, accurate, and dynamically updated street
tree inventory has become essential for optimizing these multifunctional assets
within space-constrained urban environments. Given that traditional field
surveys are time-consuming and labor-intensive, automated surveys utilizing
Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing
MMS-acquired tree datasets are limited by small-scale scene, limited
annotation, or single modality, restricting their utility for comprehensive
analysis. To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset. Collected across
two distinct cities, WHU-STree integrates synchronized point clouds and
high-resolution images, encompassing 21,007 annotated tree instances across 50
species and 2 morphological parameters. Leveraging the unique characteristics,
WHU-STree concurrently supports over 10 tasks related to street tree inventory.
We benchmark representative baselines for two key tasks--tree species
classification and individual tree segmentation. Extensive experiments and
in-depth analysis demonstrate the significant potential of multi-modal data
fusion and underscore cross-domain applicability as a critical prerequisite for
practical algorithm deployment. In particular, we identify key challenges and
outline potential future works for fully exploiting WHU-STree, encompassing
multi-modal fusion, multi-task collaboration, cross-domain generalization,
spatial pattern learning, and Multi-modal Large Language Model for street tree
asset management. The WHU-STree dataset is accessible at:
https://github.com/WHU-USI3DV/WHU-STree.

</details>


### [98] [More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era](https://arxiv.org/abs/2509.13175)
*Yingtai Li,Haoran Lai,Xiaoqian Zhou,Shuai Ming,Wenxin Ma,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出利用大型语言模型（LLM）自动从放射学报告中提取诊断标签，构建大规模医学影像-语言对比学习的“银标”数据集，从而推动视觉-语言模型的对齐和性能提升。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型需要大量标注数据，但人工标注成本高昂且难以规模化。作者希望利用LLM自动生成高质量标签，降低数据集构建成本，提升医学多模态AI的发展效率和门槛。

Method: 作者采用LLM自动从大量放射学报告中提取诊断标签，构建约5万对CT图像-报告的“银标”数据集；然后，用该数据集对3D ResNet-18模型进行标准CLIP方法的对比学习预训练，最后在多个医学影像数据集上评估性能。

Result: LLM自动标签提取的平均AUC超过96%，新构建的数据集训练出的视觉编码器效果与基于专业BERT模型标注的数据集持平。在多个医学任务上（如CT-RATE和RAD-ChestCT数据集的零样本诊断、跨模态检索等）取得最新最优成绩。

Conclusion: LLM可以低成本高精度地自动生成医学影像-语言数据标签，使大规模医学多模态模型的有监督预训练变得可行和普惠，推动医学多模态AI系统性能和规模的提升。

Abstract: The emergence of Large Language Models (LLMs) presents unprecedented
opportunities to revolutionize medical contrastive vision-language
pre-training. In this paper, we show how LLMs can facilitate large-scale
supervised pre-training, thereby advancing vision-language alignment. We begin
by demonstrate that modern LLMs can automatically extract diagnostic labels
from radiology reports with remarkable precision (>96\% AUC in our experiments)
without complex prompt engineering, enabling the creation of large-scale
"silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report
pairs). Further, we find that vision encoder trained on this "silver-standard"
dataset achieves performance comparable to those trained on labels extracted by
specialized BERT-based models, thereby democratizing the access to large-scale
supervised pre-training. Building on this foundation, we proceed to reveal that
supervised pre-training fundamentally improves contrastive vision-language
alignment. Our approach achieves state-of-the-art performance using only a 3D
ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot
diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements
in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for
report-image). These results demonstrate the potential of utilizing LLMs to
facilitate {\bf more performant and scalable} medical AI systems. Our code is
avaiable at https://github.com/SadVoxel/More-performant-and-scalable.

</details>


### [99] [Road Obstacle Video Segmentation](https://arxiv.org/abs/2509.13181)
*Shyam Nandan Rai,Shyamgopal Karthik,Mariana-Iuliana Georgescu,Barbara Caputo,Carlo Masone,Zeynep Akata*

Main category: cs.CV

TL;DR: 该论文提出了基于视频的道路障碍物分割新方法，并构建了新的评测基准，实验结果取得了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶中道路障碍物分割大多基于单帧图像，忽略了序列帧间的时序关联，导致预测不一致。本文认为分割任务本质上应考虑时序特征。

Method: 作者整理并适配了4个针对视频的道路障碍物分割评测基准，对11种主流图像和视频分割方法进行了综合评测。同时，提出了2种基于视觉基础模型的强基线方法。

Result: 在长视频序列的分割任务上，作者的方法在新基准下取得了当下最优效果。

Conclusion: 强调分割任务的时序特性，基于视频的方法显著优于单帧方法，为未来研究提供了可靠的方向与评测资源。

Abstract: With the growing deployment of autonomous driving agents, the detection and
segmentation of road obstacles have become critical to ensure safe autonomous
navigation. However, existing road-obstacle segmentation methods are applied on
individual frames, overlooking the temporal nature of the problem, leading to
inconsistent prediction maps between consecutive frames. In this work, we
demonstrate that the road-obstacle segmentation task is inherently temporal,
since the segmentation maps for consecutive frames are strongly correlated. To
address this, we curate and adapt four evaluation benchmarks for road-obstacle
video segmentation and evaluate 11 state-of-the-art image- and video-based
segmentation methods on these benchmarks. Moreover, we introduce two strong
baseline methods based on vision foundation models. Our approach establishes a
new state-of-the-art in road-obstacle video segmentation for long-range video
sequences, providing valuable insights and direction for future research.

</details>


### [100] [Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance](https://arxiv.org/abs/2509.13210)
*Ligang Chang,Shengkai Xu,Liangchang Shen,Binhan Xu,Junqiao Wang,Tianyu Shi,Yanhui Du*

Main category: cs.CV

TL;DR: 本文提出了一种名为Vi-SAFE的空间-时序框架，通过优化YOLOv8与时序段网络（TSN）结合，有效提升了公共视频监控中的暴力行为检测性能。实验结果显示该方法在准确率和效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 公共场所暴力行为检测对公共安全至关重要，但实际应用中存在如目标尺度小、场景复杂及需实时分析等挑战。作者旨在提升实际监控条件下暴力检测的准确性和效率。

Method: 方法上，作者提出Vi-SAFE框架：以优化的YOLOv8（采用轻量化GhostNetV3骨干、EMA注意力机制、剪枝减少计算量）提取行人区域，再由时序段网络（TSN）对行为进行二分类。两者分别在行人数据集与暴力数据集上训练。

Result: 在RWF-2000数据集上，Vi-SAFE的准确率达到0.88，明显高于仅用TSN（0.77），且优于当前已发表的同类方法，无论在准确性还是计算效率方面都有优势。

Conclusion: Vi-SAFE能为实际公共安全监控提供高效准确的暴力行为检测，具有良好的现实应用前景。

Abstract: Violence detection in public surveillance is critical for public safety. This
study addresses challenges such as small-scale targets, complex environments,
and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as
a lightweight backbone, an exponential moving average (EMA) attention
mechanism, and pruning to reduce computational cost while maintaining accuracy.
YOLOv8 and TSN are trained separately on pedestrian and violence datasets,
where YOLOv8 extracts human regions and TSN performs binary classification of
violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE
achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming
existing methods in both accuracy and efficiency, demonstrating its
effectiveness for public safety surveillance. Code is available at
https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.

</details>


### [101] [End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection](https://arxiv.org/abs/2509.13214)
*Fei Wang,Xuecheng Wu,Zheng Zhang,Danlei Huang,Yuheng Huang,BoWang*

Main category: cs.CV

TL;DR: 提出了一种新的检测方法（End4），有效检测基于扩散模型修补生成的图片，在不同遮挡模式和干扰下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成与修补图片方面进步显著，但其潜在滥用引发安全担忧，现有检测手段对基于扩散模型修补的图片识别效果有限。

Method: 提出End4检测方法，核心包括端到端去噪重建模型，提高重建与检测过程潜在空间对齐度，重建利于检测的特征；设计多尺度金字塔融合模块（SPFM），借助注意力机制细化局部特征，增强可区分性。同时，构建包含五种不同遮挡区域的新基准数据集，用于全面评测。

Result: End4在新遮挡模式（未见过的mask）和多种扰动下表现出很强的泛化能力和稳健性，检测效果显著优于现有方法。

Conclusion: End4能有效检测由扩散模型修补生成的图片，对未来生成内容溯源和内容安全具有重要意义。

Abstract: The powerful generative capabilities of diffusion models have significantly
advanced the field of image synthesis, enhancing both full image generation and
inpainting-based image editing. Despite their remarkable advancements,
diffusion models also raise concerns about potential misuse for malicious
purposes. However, existing approaches struggle to identify images generated by
diffusion-based inpainting models, even when similar inpainted images are
included in their training data. To address this challenge, we propose a novel
detection method based on End-to-end denoising diffusion (End4). Specifically,
End4 designs a denoising reconstruction model to improve the alignment degree
between the latent spaces of the reconstruction and detection processes, thus
reconstructing features that are more conducive to detection. Meanwhile, it
leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local
image features under the guidance of attention pyramid layers at different
scales, enhancing feature discriminability. Additionally, to evaluate detection
performance on inpainted images, we establish a comprehensive benchmark
comprising images generated from five distinct masked regions. Extensive
experiments demonstrate that our End4 effectively generalizes to unseen masking
patterns and remains robust under various perturbations. Our code and dataset
will be released soon.

</details>


### [102] [Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation](https://arxiv.org/abs/2509.13229)
*Hugo Carlesso,Josiane Mothe,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的课程式多任务自监督学习（CMTSSL）框架，旨在为高光谱遥感数据的轻量级模型学习提供更高效的解决方案。验证表明，该方法支持极轻量级模型仍可获得优异泛化能力和下游分割表现。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像数据维度高，且卫星端传输速率慢，急需高效紧凑的模型在卫星上进行数据预处理、降冗余，提高下游任务处理效率且降低带宽消耗。现有方法无法兼顾高效率与特征泛化，为此作者提出改进。

Method: 作者提出课程多任务自监督学习（CMTSSL）框架，将掩码图像建模与空间、光谱拼图任务解耦融合，并采用课程学习策略递进提升自监督难度，有效提升模型对光谱连续性、空间结构和语义特征的联合建模能力。算法架构专为轻量化设计，兼顾计算效率和表征学习。

Result: 在四个公开高光谱数据集上实证，CMTSSL训练的模型在下游分割任务上，仍能显著优于传统SOTA，并且模型参数量可达到比部分先进模型轻16000倍。结果展示了方法在轻量级、高效表示学习方面的显著优势。

Conclusion: CMTSSL框架能够推动高光谱遥感的轻量化、高性能模型，实现实际卫星端应用，促进高效特征学习与泛化能力，具有较强工程价值和应用前景。

Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across
hundreds of contiguous bands per pixel, being indispensable for remote sensing
applications such as land-cover classification, change detection, and
environmental monitoring. Due to the high dimensionality of HSI data and the
slow rate of data transfer in satellite-based systems, compact and efficient
models are required to support onboard processing and minimize the transmission
of redundant or low-value data, e.g. cloud-covered areas. To this end, we
introduce a novel curriculum multi-task self-supervised learning (CMTSSL)
framework designed for lightweight architectures for HSI analysis. CMTSSL
integrates masked image modeling with decoupled spatial and spectral jigsaw
puzzle solving, guided by a curriculum learning strategy that progressively
increases data complexity during self-supervision. This enables the encoder to
jointly capture fine-grained spectral continuity, spatial structure, and global
semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously
addresses spatial and spectral reasoning within a unified and computationally
efficient design, being particularly suitable for training lightweight models
for onboard satellite deployment. We validate our approach on four public
benchmark datasets, demonstrating consistent gains in downstream segmentation
tasks, using architectures that are over 16,000x lighter than some
state-of-the-art models. These results highlight the potential of CMTSSL in
generalizable representation learning with lightweight architectures for
real-world HSI applications. Our code is publicly available at
https://github.com/hugocarlesso/CMTSSL.

</details>


### [103] [Intelligent Vacuum Thermoforming Process](https://arxiv.org/abs/2509.13250)
*Andi Kuswoyo,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉的质量控制系统，通过有限数据优化真空热成型工艺参数，从而提升成品质量。


<details>
  <summary>Details</summary>
Motivation: 真空热成型过程中，材料性质和工具配置的变化导致成品质量难以保持一致，因此需要更智能的质量控制手段。

Method: 研究构建了一个基于真空成型样品视觉数据的数据集，并通过图像增强提升模型训练效果；采用K近邻算法通过低质量与高质量样品的映射，识别和调整工艺参数。

Result: 模型在加热功率、加热时间和真空时间调整方面表现出较好效果，能有效减少缺陷并提升生产效率。

Conclusion: 基于视觉的质量控制系统及KNN方法可以以较少的数据需求实现真空热成型工艺参数优化，提高产品质量和生产效率。

Abstract: Ensuring consistent quality in vacuum thermoforming presents challenges due
to variations in material properties and tooling configurations. This research
introduces a vision-based quality control system to predict and optimise
process parameters, thereby enhancing part quality with minimal data
requirements. A comprehensive dataset was developed using visual data from
vacuum-formed samples subjected to various process parameters, supplemented by
image augmentation techniques to improve model training. A k-Nearest Neighbour
algorithm was subsequently employed to identify adjustments needed in process
parameters by mapping low-quality parts to their high-quality counterparts. The
model exhibited strong performance in adjusting heating power, heating time,
and vacuum time to reduce defects and improve production efficiency.

</details>


### [104] [ResidualViT for Efficient Temporally Dense Video Encoding](https://arxiv.org/abs/2509.13255)
*Mattia Soldan,Fabian Caba Heilbron,Bernard Ghanem,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 本文提出一种高效的视频理解方法，能在保持准确性的同时大幅降低高时间分辨率任务的计算成本和推理时间。


<details>
  <summary>Details</summary>
Motivation: 高时间分辨率的视频任务需要对大量帧进行密集推理，导致计算量巨大。现有方法难以兼顾效率与准确性，因此亟需更高效的计算方案。

Method: 文章提出了一种名为ResidualViT的视觉Transformer架构，其创新点包括：(1)引入可学习残差连接以保证帧间时序一致性；(2)设计token reduction模块，通过丢弃冗余信息并复用预训练模型权重来加速处理；此外，还提出了轻量级蒸馏策略来高效逼近原有特征。

Result: 在四个任务、五个数据集上，零样本和完全监督两种设置下，ResidualViT方法在最大60%计算成本降低及2.5倍推理加速的同时，几乎无损准确率。

Conclusion: ResidualViT大幅提升了视频密集识别任务的效率，证明了新方法能在大幅节省资源的前提下保持高性能，具有广泛应用潜力。

Abstract: Several video understanding tasks, such as natural language temporal video
grounding, temporal activity localization, and audio description generation,
require "temporally dense" reasoning over frames sampled at high temporal
resolution. However, computing frame-level features for these tasks is
computationally expensive given the temporal resolution requirements. In this
paper, we make three contributions to reduce the cost of computing features for
temporally dense tasks. First, we introduce a vision transformer (ViT)
architecture, dubbed ResidualViT, that leverages the large temporal redundancy
in videos to efficiently compute temporally dense frame-level features. Our
architecture incorporates (i) learnable residual connections that ensure
temporal consistency across consecutive frames and (ii) a token reduction
module that enhances processing speed by selectively discarding temporally
redundant information while reusing weights of a pretrained foundation model.
Second, we propose a lightweight distillation strategy to approximate the
frame-level features of the original foundation model. Finally, we evaluate our
approach across four tasks and five datasets, in both zero-shot and fully
supervised settings, demonstrating significant reductions in computational cost
(up to 60%) and improvements in inference speed (up to 2.5x faster), all while
closely approximating the accuracy of the original foundation model.

</details>


### [105] [RadGame: An AI-Powered Platform for Radiology Education](https://arxiv.org/abs/2509.13270)
*Mohammed Baharoon,Siavash Raissi,John S. Jun,Thibault Heintz,Mahmoud Alabbad,Ali Alburkani,Sung Eun Kim,Kent Kleinschmidt,Abdulrahman O. Alhumaydhi,Mohannad Mohammed G. Alghamdi,Jeremy Francis Palacio,Mohammed Bukhaytan,Noah Michael Prudlo,Rithvik Akula,Brady Chrisler,Benjamin Galligos,Mohammed O. Almutairi,Mazeen Mohammed Alanazi,Nasser M. Alrashdi,Joel Jihwan Hwang,Sri Sai Dinesh Jaliparthi,Luke David Nelson,Nathaniel Nguyen,Sathvik Suryadevara,Steven Kim,Mohammed F. Mohammed,Yevgeniy R. Semenov,Kun-Hsing Yu,Abdulrhman Aljouie,Hassan AlOmaish,Adam Rodman,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 本文介绍了一个名为RadGame的AI驱动放射学教育游戏平台，针对定位异常和报告撰写两项核心技能，通过自动化AI反馈和游戏化机制显著提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前放射学教育依赖被动案例学习或有监督临床实践，缺乏即时且可扩展的反馈机制，难以大规模提升学习效果。

Method: RadGame平台包含两个模块：1）Localize：用户在医学影像上标记异常，系统与专家注释对比、用视觉语言模型生成反馈；2）Report：用户根据影像编写报告，系统用自动化指标评估并与专家报告比对，生成结构化反馈和评分。平台基于公共数据集。

Result: 与传统被动学习相比，RadGame用户在定位准确率上提升68%（传统仅17%），报告撰写准确率提升31%（传统仅4%），同样案例下具有明显优势。

Conclusion: RadGame展示了AI驱动的游戏化平台在放射学教育中的潜力，可大规模提供反馈丰富的训练，重新定义了医疗AI在教育领域的应用。

Abstract: We introduce RadGame, an AI-powered gamified platform for radiology education
that targets two core skills: localizing findings and generating reports.
Traditional radiology training is based on passive exposure to cases or active
practice with real-time input from supervising radiologists, limiting
opportunities for immediate and scalable feedback. RadGame addresses this gap
by combining gamification with large-scale public datasets and automated,
AI-driven feedback that provides clear, structured guidance to human learners.
In RadGame Localize, players draw bounding boxes around abnormalities, which
are automatically compared to radiologist-drawn annotations from public
datasets, and visual explanations are generated by vision-language models for
user missed findings. In RadGame Report, players compose findings given a chest
X-ray, patient age and indication, and receive structured AI feedback based on
radiology report generation metrics, highlighting errors and omissions compared
to a radiologist's written ground truth report from public datasets, producing
a final performance and style score. In a prospective evaluation, participants
using RadGame achieved a 68% improvement in localization accuracy compared to
17% with traditional passive methods and a 31% improvement in report-writing
accuracy compared to 4% with traditional methods after seeing the same cases.
RadGame highlights the potential of AI-driven gamification to deliver scalable,
feedback-rich radiology training and reimagines the application of medical AI
resources in education.

</details>


### [106] [Image Realness Assessment and Localization with Multimodal Features](https://arxiv.org/abs/2509.13289)
*Lovish Kaushik,Agnij Biswas,Somdyuti Paul*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉-语言模型通过文本描述来评估AI生成图像的真实性，并定位图像中视觉不一致区域的新方法。该框架能更准确评估整体和局部的图像真实性，有助于提升生成模型的真实感。


<details>
  <summary>Details</summary>
Motivation: 目前AI生成图像在实际应用时对真实性有较高需求，且需要明确识别图像中不自然或失真区域。然而，缺乏有效的自动评估方法，现有人类标注成本高、效率低。因此，开发一种高效、客观的真实性量化方法尤为重要。

Method: 作者提出了一种多模态框架：首先使用大规模数据集训练的视觉-语言模型，对AI生成图片的视觉不一致性进行文本描述；然后由模型自动判断整体和局部的图像真实性和一致性，生成密集的真实性映射图。该框架用模型输出替代了人工标注。

Result: 实验结果表明，该方法在客观预测真实性方面优于已有方法，且可生成高分辨率的真实性地图，有效区分图像中真实与不真实的空间区域。

Conclusion: 该研究为AI生成图像的真实性评估和局部失真检测提供了高效可靠的工具，对提升生成图片的实际可用性及后续生成模型训练具有重要促进作用。

Abstract: A reliable method of quantifying the perceptual realness of AI-generated
images and identifying visually inconsistent regions is crucial for practical
use of AI-generated images and for improving photorealism of generative AI via
realness feedback during training. This paper introduces a framework that
accomplishes both overall objective realness assessment and local inconsistency
identification of AI-generated images using textual descriptions of visual
inconsistencies generated by vision-language models trained on large datasets
that serve as reliable substitutes for human annotations. Our results
demonstrate that the proposed multimodal approach improves objective realness
prediction performance and produces dense realness maps that effectively
distinguish between realistic and unrealistic spatial regions.

</details>


### [107] [StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance](https://arxiv.org/abs/2509.13301)
*Zefan Qu,Zhenwei Wang,Haoyuan Wang,Ke Xu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出了一种名为StyleSculptor的新方法，实现了无需训练情况下，基于内容图片和一种或多种风格图片的风格可控3D资产生成，在纹理和几何风格控制方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在游戏、虚拟现实等实际应用中，生成与现有资产风格一致的3D资产需求很大，而当前3D对象生成方法难以实现细致的风格可控和灵活迁移，尤其难以在无训练的情境下完成。

Method: 作者提出StyleSculptor方法，通过Style Disentangled Attention（SD-Attn）模块实现内容图与风格图之间的动态交互，融合特征并引入基于特征方差的风格-内容通道选择策略来减少语义内容泄漏。此外，Style Guided Control（SGC）机制实现了纹理、几何及二者专属风格迁移和可调强度控制。

Result: StyleSculptor能够无训练下高效生成符合用户需求的风格化3D资产，并在纹理、几何或二者的控制上细粒度可调。实验表明该方法在高保真3D生成任务中优于现有基线方法。

Conclusion: 无监督、零样本条件下，StyleSculptor显著提升了风格可控3D资产生成的质量与灵活性，为相关实际应用（如游戏、虚拟现实）提供了更实用和高效的技术手段。

Abstract: Creating 3D assets that follow the texture and geometry style of existing
ones is often desirable or even inevitable in practical applications like video
gaming and virtual reality. While impressive progress has been made in
generating 3D objects from text or images, creating style-controllable 3D
assets remains a complex and challenging problem. In this work, we propose
StyleSculptor, a novel training-free approach for generating style-guided 3D
assets from a content image and one or more style images. Unlike previous
works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,
enabling fine-grained 3D style control that captures the texture, geometry, or
both styles of user-provided style images. At the core of StyleSculptor is a
novel Style Disentangled Attention (SD-Attn) module, which establishes a
dynamic interaction between the input content image and style image for
style-guided 3D asset generation via a cross-3D attention mechanism, enabling
stable feature fusion and effective style-guided generation. To alleviate
semantic content leakage, we also introduce a style-disentangled feature
selection strategy within the SD-Attn module, which leverages the variance of
3D feature patches to disentangle style- and content-significant channels,
allowing selective feature injection within the attention framework. With
SD-Attn, the network can dynamically compute texture-, geometry-, or
both-guided features to steer the 3D generation process. Built upon this, we
further propose the Style Guided Control (SGC) mechanism, which enables
exclusive geometry- or texture-only stylization, as well as adjustable style
intensity control. Extensive experiments demonstrate that StyleSculptor
outperforms existing baseline methods in producing high-fidelity 3D assets.

</details>


### [108] [3D Aware Region Prompted Vision Language Model](https://arxiv.org/abs/2509.13317)
*An-Chieh Cheng,Yang Fu,Yukang Chen,Zhijian Liu,Xiaolong Li,Subhashree Radhakrishnan,Song Han,Yao Lu,Jan Kautz,Pavlo Molchanov,Hongxu Yin,Xiaolong Wang,Sifei Liu*

Main category: cs.CV

TL;DR: 本文提出SR-3D模型，可统一处理2D图片与多视角3D数据，支持灵活的区域标注，实现更优越的场景理解和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多局限于单一2D图片或受限的3D标注，难以灵活地将2D与3D数据联系起来，限制了对复杂场景空间关系的理解。

Method: 作者提出SR-3D，通过在2D特征中引入3D位置嵌入，实现共享的视觉token空间。模型支持在任意帧上用框、分割掩码或3D空间直接标注区域，无需多帧繁琐标注，增强了模型对跨帧空间关系的表达力。

Result: SR-3D在通用2D视觉语言任务和专业3D空间基准上均取得SOTA表现，能融合2D和3D特征实现精确空间推理。在未含3D输入或人工标注的真实视频中，也能准确推断空间关系和度量。

Conclusion: SR-3D有效统一了2D与3D表示空间，提升了场景理解能力，并具备迁移到真实环境视频的潜力，对视觉-语言和空间感知任务有广泛应用价值。

Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that
connects single-view 2D images and multi-view 3D data through a shared visual
token space. SR-3D supports flexible region prompting, allowing users to
annotate regions with bounding boxes, segmentation masks on any frame, or
directly in 3D, without the need for exhaustive multi-frame labeling. We
achieve this by enriching 2D visual features with 3D positional embeddings,
which allows the 3D model to draw upon strong 2D priors for more accurate
spatial reasoning across frames, even when objects of interest do not co-occur
within the same view. Extensive experiments on both general 2D vision language
and specialized 3D spatial benchmarks demonstrate that SR-3D achieves
state-of-the-art performance, underscoring its effectiveness for unifying 2D
and 3D representation space on scene understanding. Moreover, we observe
applicability to in-the-wild videos without sensory 3D inputs or ground-truth
3D annotations, where SR-3D accurately infers spatial relationships and metric
measurements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [109] [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)
*Nikolay Banar,Ehsan Lotfi,Jens Van Nooten,Cristina Arhiliuc,Marija Kliocaite,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文针对荷兰语在多语言资源中的稀缺问题，推出了新评测基准、训练数据集和高效嵌入模型，全面促进荷兰语嵌入研究发展。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言嵌入资源大多对荷兰语支持有限，缺乏高质量的评测基准与模型，制约了其进一步发展。

Method: 1）提出MTEB-NL评测基准，集合现有和新创Dutch数据集，涵盖多种任务；2）整理并合成新的训练数据集，利用大模型生成新型合成数据以拓展任务类型；3）发布E5-NL系列紧凑高效的嵌入模型。

Result: 所提出的E5-NL模型在多项任务中展现较强表现，所构建的评测基准及数据集丰富了荷兰语相关资源。

Conclusion: 通过提供新的基准、数据和模型，为荷兰语嵌入研究提供了重要资源，推动了其在多语言NLP中的应用与发展。

Abstract: Recently, embedding resources, including models, benchmarks, and datasets,
have been widely released to support a variety of languages. However, the Dutch
language remains underrepresented, typically comprising only a small fraction
of the published multilingual resources. To address this gap and encourage the
further development of Dutch embeddings, we introduce new resources for their
evaluation and generation. First, we introduce the Massive Text Embedding
Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and
newly created ones, covering a wide range of tasks. Second, we provide a
training dataset compiled from available Dutch retrieval datasets, complemented
with synthetic data generated by large language models to expand task coverage
beyond retrieval. Finally, we release a series of E5-NL models compact yet
efficient embedding models that demonstrate strong performance across multiple
tasks. We make our resources publicly available through the Hugging Face Hub
and the MTEB package.

</details>


### [110] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
*Matteo Marcuzzo,Alessandro Zangari,Andrea Albarelli,Jose Camacho-Collados,Mohammad Taher Pilehvar*

Main category: cs.CL

TL;DR: 论文提出了MORABLES基准，通过历史文学中的寓言和短篇小说评估大模型的道德推理能力，发现主流大模型更多依赖表面特征而非真正理解道德推理，且在对抗测试中表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在标准阅读理解任务上表现突出，研究者希望进一步评价它们在复杂推理和抽象理解，尤其是道德推理方面的能力。标准的理解测试难以覆盖这一能力，因此需要更具深度的评测基准。

Method: 作者构建了MORABLES基准，基于历史寓言和短篇故事，设计多项选择题，问题和干扰选项专门针对道德推理而非表层文本理解。此外还构建了对抗样本，用以检测模型脆弱性和非预期的 shortcut 影响。

Result: 大型模型在该基准上的表现优于小模型，但仍容易受到对抗样本影响，解题时依赖表层特征而非真实道德推理，并在不少案例中出现自我矛盾（约20%情况下会否定自己先前的答案）。提升推理模块并未显著提升模型表现，规模而非推理能力主导了结果。

Conclusion: 当前LLM在复杂道德推理任务上远未达标，它们受到模型规模驱动，而缺乏真正的道德理解和抽象推理能力，需要发展新的方法以突破瓶颈。

Abstract: As LLMs excel on standard reading comprehension benchmarks, attention is
shifting toward evaluating their capacity for complex abstract reasoning and
inference. Literature-based benchmarks, with their rich narrative and moral
depth, provide a compelling framework for evaluating such deeper comprehension
skills. Here, we present MORABLES, a human-verified benchmark built from fables
and short stories drawn from historical literature. The main task is structured
as multiple-choice questions targeting moral inference, with carefully crafted
distractors that challenge models to go beyond shallow, extractive question
answering. To further stress-test model robustness, we introduce adversarial
variants designed to surface LLM vulnerabilities and shortcuts due to issues
such as data contamination. Our findings show that, while larger models
outperform smaller ones, they remain susceptible to adversarial manipulation
and often rely on superficial patterns rather than true moral reasoning. This
brittleness results in significant self-contradiction, with the best models
refuting their own answers in roughly 20% of cases depending on the framing of
the moral choice. Interestingly, reasoning-enhanced models fail to bridge this
gap, suggesting that scale - not reasoning ability - is the primary driver of
performance.

</details>


### [111] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
*Anu Pradhan,Alexandra Ortan,Apurv Verma,Madhavan Seshadri*

Main category: cs.CL

TL;DR: 本研究提出并实证了在法律领域中利用大模型作为生成式推荐系统评估者的可行策略，并对如何科学选择评估指标和比较系统进行了系统分析。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的崛起，推荐系统的评估难度增加，传统指标难以衡量法律等专业领域中推荐质量的细微差异。如何有效、可靠地用大模型自身替代人工评判成为急需解决的问题。

Method: 作者提出在法律推荐系统评估场景中，让大语言模型（LLM）作为评判者，并围绕模型与人工结果一致性的度量，以及系统间的科学统计比较方法展开实证研究。对比了多种一致性和统计指标。

Result: 实验发现，传统的Krippendorff's alpha指标在这类AI评估中不适用，而Gwet's AC2及秩相关系数更能正确反映LLM和人工评审的一致性。对于系统比较，Wilcoxon符号秩检验配合Benjamini-Hochberg修正具备更高统计可靠性。

Conclusion: 以大语言模型为核心的自动化评估框架可大幅提升法律推荐系统的评审效率和科学性，为高精度场景提供了兼具成本与可扩展性的解决思路。

Abstract: The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.

</details>


### [112] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
*Mitchell Plyler,Yilun Zhang,Alexander Tuzhilin,Saoud Khalifah,Sen Tian*

Main category: cs.CL

TL;DR: 本文提出了一种新颖、高效的LLM生成文本检测模型SENTRA，在跨领域检测中效果显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）能力提升及其广泛应用，相关文本被滥用的风险增加，亟需一种通用且有效的方法来检测未声明的LLM生成文本。

Method: 提出了SElected-Next-Token tRAnsformer（SENTRA），这是一种基于Transformer的编码器模型。它利用选取的下一词概率序列，并在大量无标签数据上进行对比学习预训练，然后在标注数据上进行监督微调。

Result: 在三个主流公开数据集、24个文本领域上的实验显示，SENTRA作为一种通用文本分类器，在跨领域检测任务中明显优于现有流行的基线模型。

Conclusion: SENTRA是一个适用性强、性能优异的LLM文本检测工具，特别在跨领域应用中具有显著优势，为LLM生成内容的安全检测提供了有效的新方法。

Abstract: LLMs are becoming increasingly capable and widespread. Consequently, the
potential and reality of their misuse is also growing. In this work, we address
the problem of detecting LLM-generated text that is not explicitly declared as
such. We present a novel, general-purpose, and supervised LLM text detector,
SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder
leveraging selected-next-token-probability sequences and utilizing contrastive
pre-training on large amounts of unlabeled data. Our experiments on three
popular public datasets across 24 domains of text demonstrate SENTRA is a
general-purpose classifier that significantly outperforms popular baselines in
the out-of-domain setting.

</details>


### [113] [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)
*Wen-wai Yim,Asma Ben Abacha,Zixuan Yu,Robert Doerning,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 本文提出了MORQA（Medical Open-Response QA）多语种基准，用于评估医学领域NLG系统的自动评价指标，结果显示基于LLM（如GPT-4, Gemini）的评价方式显著优于传统指标，更贴近专家判断。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域NLG系统对准确性和专业性要求极高，但传统自动评价指标在开放式医学问答任务中效果有限，难以区分高质量输出，因此亟需更有效的评估方法和标准数据集。

Method: 作者构建了一个包含英文和中文、由医学专家撰写2-4个标准答案并附有人类专家评分的多模态医学QA基准（MORQA），覆盖三类医学视觉和文本QA数据集。对比了传统自动评价指标（如BLEU、ROUGE、BERTScore）和基于LLM的自动评估方法（如GPT-4、Gemini），并分析不同方法与专家判断的一致性及其原因。

Result: 实验发现，LLM为基础的评价方法在与专家评分一致性上远超传统指标，LLM在把握语义细节和应对多样正确答案方面展现出更高的鲁棒性。

Conclusion: LLM基础的自动评价方法在医学NLG系统评测中表现更优，强调了开发与人类判断更一致的自动评价手段的重要性。该基准及数据将公开，促进未来相关研究。

Abstract: Evaluating natural language generation (NLG) systems in the medical domain
presents unique challenges due to the critical demands for accuracy, relevance,
and domain-specific expertise. Traditional automatic evaluation metrics, such
as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between
high-quality outputs, especially given the open-ended nature of medical
question answering (QA) tasks where multiple valid responses may exist. In this
work, we introduce MORQA (Medical Open-Response QA), a new multilingual
benchmark designed to assess the effectiveness of NLG evaluation metrics across
three medical visual and text-based QA datasets in English and Chinese. Unlike
prior resources, our datasets feature 2-4+ gold-standard answers authored by
medical professionals, along with expert human ratings for three English and
Chinese subsets. We benchmark both traditional metrics and large language model
(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based
approaches significantly outperform traditional metrics in correlating with
expert judgments. We further analyze factors driving this improvement,
including LLMs' sensitivity to semantic nuances and robustness to variability
among reference answers. Our results provide the first comprehensive,
multilingual qualitative study of NLG evaluation in the medical domain,
highlighting the need for human-aligned evaluation methods. All datasets and
annotations will be publicly released to support future research.

</details>


### [114] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
*Jiayi He,Yangmin Huang,Qianyun Du,Xiangying Zhou,Zhiyang He,Jiaxue Hu,Xiaodong Tao,Lixian Lai*

Main category: cs.CL

TL;DR: 本文提出了MedFact，这是一个面向中文医学事实核查的高难度新基准数据集，旨在更好评价大语言模型（LLM）在医疗场景的事实可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医学事实核查基准数据集领域窄、无法反映真实医疗信息的复杂性。因此亟需更全面、复杂的检测平台辅助评估和改进医疗大模型的表现。

Method: 作者构建了MedFact数据集，共含2116个由医学专家标注的案例，涵盖13个医学专科、8类细粒度错误、4种文体及不同难度级别。数据集采用AI与人工结合的混合构建流程，通过多轮专家反馈迭代，AI多准则筛选保证数据质量和难度。同时选取20个主流LLM，在事实分类和错误定位方面全面评测其性能，并与专家表现对比。

Result: 实验发现，LLM大多能够判断文本中是否存在错误，但在定位具体错误上远不及专业人士。此外发现模型普遍存在"过度批评"问题，即倾向于将正确信息误判为错误，且该现象在采用高阶推理技巧（如多代理、推理时间扩展）时更加明显。

Conclusion: MedFact揭示了当前LLM在医疗应用中面临的重大挑战，尤其是事实定位和误判问题。该数据集为后续开发更高事实可靠性与医学素养的大模型提供了重要基准和资源。

Abstract: The increasing deployment of Large Language Models (LLMs) in healthcare
necessitates a rigorous evaluation of their factual reliability. However,
existing benchmarks are often limited by narrow domains of data, failing to
capture the complexity of real-world medical information. To address this
critical gap, we introduce MedFact, a new and challenging benchmark for Chinese
medical fact-checking. MedFact comprises 2,116 expert-annotated instances
curated from diverse real-world texts, spanning 13 medical specialties, 8
fine-grained error types, 4 writing styles, and multiple difficulty levels. Its
construction employs a hybrid AI-human framework where iterative expert
feedback refines an AI-driven, multi-criteria filtering process, ensuring both
high data quality and difficulty. We conduct a comprehensive evaluation of 20
leading LLMs, benchmarking their performance on veracity classification and
error localization against a human expert baseline. Our results reveal that
while models can often determine if a text contains an error, precisely
localizing it remains a substantial challenge, with even top-performing models
falling short of human performance. Furthermore, our analysis uncovers a
frequent ``over-criticism'' phenomenon, a tendency for models to misidentify
correct information as erroneous, which is exacerbated by advanced reasoning
techniques such as multi-agent collaboration and inference-time scaling. By
highlighting these critical challenges for deploying LLMs in medical
applications, MedFact provides a robust resource to drive the development of
more factually reliable and medically aware models.

</details>


### [115] [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)
*Wonbin Kweon,SeongKu Kang,Runchu Tian,Pengcheng Jiang,Jiawei Han,Hwanjo Yu*

Main category: cs.CL

TL;DR: 提出了TopicK方法，根据主题覆盖选择in-context learning的示例，提升泛化效果，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有的in-context learning示例检索方法常基于嵌入相似度或生成概率，导致选出的示例存在无关或冗余问题，难以覆盖测试输入所需的细粒度知识。

Method: 提出TopicK检索框架：分析测试输入所需的主题，并评估大模型在这些主题上的知识掌握情况，然后优先选择能够补充未覆盖主题、且大模型相关知识较弱的示例，实现对测试输入知识需求的全面覆盖。

Result: 在多个数据集、不同类型的大模型（包括开源和闭源）上进行了大量实验，实验结果验证了所提方法的有效性。

Conclusion: TopicK 能系统补充主题级知识，提升in-context learning效果，为示例选择提供新的思路和实用工具。

Abstract: The effectiveness of in-context learning relies heavily on selecting
demonstrations that provide all the necessary information for a given test
input. To achieve this, it is crucial to identify and cover fine-grained
knowledge requirements. However, prior methods often retrieve demonstrations
based solely on embedding similarity or generation probability, resulting in
irrelevant or redundant examples. In this paper, we propose TopicK, a topic
coverage-based retrieval framework that selects demonstrations to
comprehensively cover topic-level knowledge relevant to both the test input and
the model. Specifically, TopicK estimates the topics required by the input and
assesses the model's knowledge on those topics. TopicK then iteratively selects
demonstrations that introduce previously uncovered required topics, in which
the model exhibits low topical knowledge. We validate the effectiveness of
TopicK through extensive experiments across various datasets and both open- and
closed-source LLMs. Our source code is available at
https://github.com/WonbinKweon/TopicK_EMNLP2025.

</details>


### [116] [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)
*Suvojit Acharjee,Utathya Aich,Asfak Ali*

Main category: cs.CL

TL;DR: 本论文评估了多种主流大语言模型在处理精细语言现象（如时态、否定、语态和情态）上的能力，针对英语和孟加拉语，提出了新的评测指南和数据集，并发现Compound-Beta模型表现最为稳健，与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自然语言生成和理解方面取得很大进展，但在时态、否定、语态等细致的语言现象上仍有明显不足。尤其在联合国可持续发展目标4（教育领域）相关的教育应用中，完善这些问题对于提升教学质量至关重要。

Method: 作者构建了LUCID数据集，包含英语和孟加拉语中专门设计的句子对，聚焦考察时态、否定、语态等要素对语言理解的影响，并提出了新的评估准则(Route for Evaluation of Cognitive Inference in Systematic Environments)。对包括MISTRAL-SABA-24B、LLaMA-4-Scout-17B等5种主流模型进行评测，使用皮尔逊相关系数、斯皮尔曼相关和MAE，以及新的人类容忍度度量HCE准确率综合评估模型表现。

Result: Compound-Beta模型在所有评估指标中表现最为均衡，尤其在英语和多语言混合数据上展现了最高的人类一致性（Pearson相关最高，MAE最低），能够稳健地模拟人类对精细语言现象的容忍和理解能力。

Conclusion: 尽管主流大语言模型在细粒度语言理解上仍有进步空间，Compound-Beta模型表现出令人瞩目的跨语言和跨情境稳定性。因此，在对语言准确性要求严格的教育型人工智能应用中，该类模型更具实际应用前景。

Abstract: Despite advances in natural language generation and understanding, LM still
struggle with fine grained linguistic phenomena such as tense, negation, voice,
and modality which are the elements central to effective human communication.
In the context of the United Nations SDG 4, where linguistic clarity is
critical, the deployment of LMs in educational technologies demands careful
scrutiny. As LMs are increasingly powering applications like tutoring systems,
automated grading, and translation, their alignment with human linguistic
interpretation becomes essential for effective learning. In this study, we
conduct a evaluation of SOTA language models across these challenging contexts
in both English and Bengali. To ensure a structured assessment, we introduce a
new Route for Evaluation of Cognitive Inference in Systematic Environments
guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence
pairs in English and Bengali, specifically challenges these models on critical
aspects of language comprehension, including negation, tense, voice variations.
We assess the performance of SOTA models including MISTRAL-SABA-24B,
LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard
metrics like Pearson correlation, Spearman correlation, and Mean Absolute
Error, as well as novel, linguistically inspired metric the HCE accuracy. The
HCE accuracy measures how often model predictions fall within one standard
deviation of the mean human rating, thus capturing human like tolerance for
variability in language interpretation. Our findings highlight Compound-Beta as
the most balanced model, consistently achieving high correlations and low MAEs
across diverse language conditions. It records the highest Pearson correlation
in English and demonstrates robust performance on mixed-language data,
indicating a strong alignment with human judgments in cross lingual scenarios.

</details>


### [117] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
*Sumanta Bhattacharyya,Sara Riaz,Pedram Rooshenas*

Main category: cs.CL

TL;DR: 提出了R2tA方法，通过生成和精炼模型推理步骤，提升在数据稀缺任务上的小型特定推理模型训练效果，并以数据库EERD评估为案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 在人工标注或高质量标签短缺的情况下，任务特定的小型推理模型很难训练，而LLMs的推理链可被用作合成监督信号。作者希望利用大型模型生成并精炼的推理作为监督，弥补监督信号不足，实现低成本高效适配。

Method: 提出Reason-Refine-then-Align（R2tA）：1）利用开源基座模型在任务输入上生成初步推理及答案；2）对推理链进行精炼，修正幻觉与不一致问题，形成高质量数据集；3）采用两阶段对齐，包括有监督微调（SFT）和直接偏好优化（DPO），先校准推理链，再基于对齐后的推理生成最终输出。案例为数据库EERD设计评估。

Result: 作者自建了含11类错误、共600个EERD变式的数据集，并将R2tA应用到该复杂结构任务。实验显示，该方法在数据稀缺领域可低成本、大规模适配LLM，提升推理和输出表现。

Conclusion: R2tA为任务特定推理模型提供了一套实用、可扩展、低成本的适配路径，能在训练数据有限场景下提升模型性能，具有面向教育及其它领域的应用潜力。

Abstract: Training a task-specific small reasoning model is challenging when direct
human supervision or high-quality labels are scarce. However, LLMs with
reasoning capabilities produce abundant intermediate reasoning traces that can
be systematically refined to create effective supervision signals. We propose
Reason-Refine-then-Align (R2tA), which turns refined model rationales into
supervision for training task-specific reasoning models. Our method generates
initial reasoning and responses from an open-source base model on task-specific
inputs, then refines these traces, fixing hallucinations and inconsistencies,
to form a high-fidelity dataset. We perform a two-stage alignment, supervised
fine-tuning (SFT), followed by direct preference optimization (DPO) to
calibrate the model's intermediate reasoning with human-validated conceptual
preferences and then condition the final output on that aligned reasoning. As a
case study, we apply R2tA to evaluate extended entity relationship diagrams
(EERDs) in database system design, a structurally complex task where
prompt-only methods miss or hallucinate errors. We curated a dataset of 600
EERD variants (train/test split of 450/150, respectively) with induced mistakes
spanning 11 categories. Empirical evaluation suggests R2tA provides a
practical, cost-effective path to scalable LLM adaptation in data-scarce
domains, enabling reproducible AI tools for education and beyond.

</details>


### [118] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
*Keyu An,Yanni Chen,Chong Deng,Changfeng Gao,Zhifu Gao,Bo Gong,Xiangang Li,Yabin Li,Xiang Lv,Yunjie Ji,Yiheng Jiang,Bin Ma,Haoneng Luo,Chongjia Ni,Zexu Pan,Yiping Peng,Zhendong Peng,Peiyao Wang,Hao Wang,Wen Wang,Wupeng Wang,Biao Tian,Zhentao Tan,Nan Yang,Bin Yuan,Jieping Ye,Jixing Yu,Qinglin Zhang,Kun Zou,Han Zhao,Shengkui Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: FunAudio-ASR 是一种结合大数据、大模型、LLM与强化学习的ASR系统，针对实际应用场景优化，兼具强性能和部署能力。


<details>
  <summary>Details</summary>
Motivation: 近年来ASR领域通过数据扩展、模型增大和与大语言模型（LLMs）的深度融合取得巨大进步，但LLM可能产生幻觉，影响实际应用用户体验。为此，作者希望设计兼具高准确率与行业可用性的ASR系统。

Method: 提出FunAudio-ASR，将大规模数据、强大模型、LLMs集成和强化学习协同应用，并在流式处理、抗噪、语码转换、热词定制等真实部署能力上进行专项优化。

Result: 在开源基准测试上达到强性能，在真实行业数据上超越现有LLM-ASR系统，实现SOTA和优良的实际部署效果。

Conclusion: FunAudio-ASR不只在公开测试集上效果优秀，在实际行业评测中也表现出色，表明其部署优化确保了高效能与实际可用性。

Abstract: In recent years, automatic speech recognition (ASR) has witnessed
transformative advancements driven by three complementary paradigms: data
scaling, model size scaling, and deep integration with large language models
(LLMs). However, LLMs are prone to hallucination, which can significantly
degrade user experience in real-world ASR applications. In this paper, we
present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically
combines massive data, large model capacity, LLM integration, and reinforcement
learning to achieve state-of-the-art performance across diverse and complex
speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized
for practical deployment, with enhancements in streaming capability, noise
robustness, code-switching, hotword customization, and satisfying other
real-world application requirements. Experimental results show that while most
LLM-based ASR systems achieve strong performance on open-source benchmarks,
they often underperform on real industry evaluation sets. Thanks to
production-oriented optimizations, FunAudio-ASR achieves SOTA performance on
real application datasets, demonstrating its effectiveness and robustness in
practical settings.

</details>


### [119] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
*Chiara Bonfanti,Michele Colombino,Giulia Coucourde,Faeze Memari,Stefano Pinardi,Rosa Meo*

Main category: cs.CL

TL;DR: 本文比较了三种基于Transformer的神经网络管道，用于构建班巴拉语（Bambara）机器翻译系统，并在多个数据集上进行了测试和评估。


<details>
  <summary>Details</summary>
Motivation: 班巴拉语作为非洲的一种低资源语言，缺乏高质量的机器翻译系统。作者旨在探索不同的神经网络训练管道，提升法语到班巴拉语的机器翻译性能。

Method: 本文设计了三种翻译模型训练管道：1）基于Transformer的直接翻译模型；2）对LLaMA3（3B-8B）Instructor模型的微调，采用解码器结构；3）基于学生-教师的双神经网络蒸馏，将班巴拉语集成到预训练的LaBSE模型中，并结合BERT扩展生成翻译。三种方法分别在医学和多领域数据集上进行评估。

Result: 最简单的Transformer模型在Bayelemagaba数据集上表现最好（BLEU 10%，chrF 21%），在新建的Yiri数据集上也达到较高准确率（BLEU 33.81%，chrF 41%）。基于Instructor的方法在单一数据集上优于混合数据集，表明更擅长捕捉特定数据集模式。

Conclusion: 对于低资源语言班巴拉语，简单的Transformer模型反而取得了最佳效果。更复杂模型和蒸馏方法虽有优势，但主要体现在特定条件和数据集下。

Abstract: This work compares three pipelines for training transformer-based neural
networks to produce machine translators for Bambara, a Mand\`e language spoken
in Africa by about 14,188,850 people. The first pipeline trains a simple
transformer to translate sentences from French into Bambara. The second
fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures
for French-to-Bambara translation. Models from the first two pipelines were
trained with different hyperparameter combinations to improve BLEU and chrF
scores, evaluated on both test sentences and official Bambara benchmarks. The
third pipeline uses language distillation with a student-teacher dual neural
network to integrate Bambara into a pre-trained LaBSE model, which provides
language-agnostic embeddings. A BERT extension is then applied to LaBSE to
generate translations. All pipelines were tested on Dokotoro (medical) and
Bayelemagaba (mixed domains). Results show that the first pipeline, although
simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on
Bayelemagaba), consistent with low-resource translation results. On the Yiri
dataset, created for this work, it achieves 33.81% BLEU and 41% chrF.
Instructor-based models perform better on single datasets than on aggregated
collections, suggesting they capture dataset-specific patterns more
effectively.

</details>


### [120] [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)
*Vijay Govindarajan,Pratik Patel,Sahil Tripathi,Md Azizul Hoque,Gautam Siddharth Kashyap*

Main category: cs.CL

TL;DR: 该论文提出了一种零样本自动音频描述(AAC)系统，利用预训练模型和新的解码方法，在不需要大量训练数据的情况下，显著提升了音频内容描述质量。


<details>
  <summary>Details</summary>
Motivation: 由于音频描述的公开数据集远少于图像描述，导致AAC任务训练受限，因此需要方法在数据稀缺时仍能获得高性能。

Method: 利用预训练的音频CLIP模型提取音频特征，并生成结构化提示，结合大语言模型(LLM)生成描述。与传统贪婪解码不同，文中方法通过CLIP模型细化token选择，增强文本与音频内容一致性。

Result: 在WavCaps模型上用MAGIC search提升了NLG平均分数35%（由4.7提升至7.3）。性能高度依赖于音频-文本匹配模型和关键字选择，单关键词提示效果最佳，若无关键词列表则性能下降50%。

Conclusion: 提出的方法在无需大量训练的条件下，通过结合预训练模型和有效的关键词设计，大幅提升了自动音频描述的表现。

Abstract: Automated Audio Captioning (AAC) generates captions for audio clips but faces
challenges due to limited datasets compared to image captioning. To overcome
this, we propose the zero-shot AAC system that leverages pre-trained models,
eliminating the need for extensive training. Our approach uses a pre-trained
audio CLIP model to extract auditory features and generate a structured prompt,
which guides a Large Language Model (LLM) in caption generation. Unlike
traditional greedy decoding, our method refines token selection through the
audio CLIP model, ensuring alignment with the audio content. Experimental
results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using
MAGIC search with the WavCaps model. The performance is heavily influenced by
the audio-text matching model and keyword selection, with optimal results
achieved using a single keyword prompt, and a 50% performance drop when no
keyword list is used.

</details>


### [121] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文主要关注自动定理证明（ATP）领域中大语言模型在推理效率上的优化，并提出了能大幅减少计算成本的新方法，在保证性能的同时降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在自动定理证明任务上通过多次采样和链式推理策略提升了性能，但这些方法在推理时引入了巨大的计算开销，尤其是在token消耗和采样次数上。同时，已有的分析往往忽视了不同策略间实际的采样成本差异。本文旨在系统性地对比各种推理策略的效率，并寻找减低计算资源消耗的方法。

Method: 作者提出了EconRL统一流程，包括两种互补的方法：一是动态的链式思维（CoT）切换机制，用来避免不必要的token消耗；二是带可训练前缀的多样化并行采样强化学习（RL），可在受限采样次数下提升推理通过率。

Result: 在miniF2F和ProofNet数据集上的实验表明，EconProver在保证与主流方法相当推理性能的前提下，将计算成本降低到原来的12%。

Conclusion: 本文为低成本、高效率的自动定理证明模型部署提供了实践性指导，实现了性能与资源消耗的良好平衡。

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [122] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
*Yu,Wang,Sheng Shen,Rémi Munos,Hongyuan Zhan,Yuandong Tian*

Main category: cs.CL

TL;DR: 本文提出了一种新的位置编码方法TAPA，并证明它在长上下文建模上优于RoPE。


<details>
  <summary>Details</summary>
Motivation: RoPE存在距离相关性偏差，限制了其对长上下文的建模能力，现有RoPE扩展方法需要额外的训练后调整，使用不便。

Method: 作者提出了Token-Aware Phase Attention (TAPA)，将可学习的相位函数引入注意力机制中，并能够通过直接且轻量的微调适应更长的上下文。

Result: TAPA在长上下文任务中表现优异，可以保留长距离Token间的关系，对未见过的长度具有较强的泛化能力，并在长上下文数据上取得显著低于RoPE类方法的perplexity。

Conclusion: TAPA方法相比RoPE更好地解决了长上下文建模问题，提升了Transformer在扩展长度时的性能和适应性。

Abstract: We prove under practical assumptions that Rotary Positional Embedding (RoPE)
introduces an intrinsic distance-dependent bias in attention scores that limits
RoPE's ability to model long-context. RoPE extension methods may alleviate this
issue, but they typically require post-hoc adjustments after pretraining, such
as rescaling or hyperparameters retuning. This paper introduces Token-Aware
Phase Attention (TAPA), a new positional encoding method that incorporates a
learnable phase function into the attention mechanism. TAPA preserves token
interactions over long range, extends to longer contexts with direct and light
fine-tuning, extrapolates to unseen lengths, and attains significantly lower
perplexity on long-context than RoPE families.

</details>


### [123] [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)
*Li Fu,Yu Xin,Sunlu Zeng,Lu Fan,Youzheng Wu,Xiaodong He*

Main category: cs.CL

TL;DR: 本文提出了一种发音感知上下文（PAC）框架，针对LLM自动语音识别中的发音建模和同音词区分问题，实现了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的自动语音识别（ASR）系统在处理生僻词和同音词时存在误识别的挑战，尤其在发音建模和同音词区分方面有待提升。

Method: 提出了两阶段的学习范式。首先引入了发音引导的上下文学习方法，通过交错的字母-音素建模与引入仅含字母的干扰项，让模型充分利用音素信息提升识别。其次，设计了基于发音区分的强化学习方法，采用扰动标签采样，进一步强化了模型同音词区分能力。

Result: 在Librispeech（英语）和AISHELL-1（中文）语音识别任务上，PAC模型分别比预训练LLM-ASR模型降低了30.2%和53.8%的词错误率（WER），对长尾词汇也实现了31.8%和60.5%的WER下降幅度。

Conclusion: PAC框架在提升发音建模和同音词区分能力上效果显著，特别对生僻词识别和长尾分布场景有强大适应性，为LLM驱动的ASR进一步发展提供了有效方案。

Abstract: This paper presents a Pronunciation-Aware Contextualized (PAC) framework to
address two key challenges in Large Language Model (LLM)-based Automatic Speech
Recognition (ASR) systems: effective pronunciation modeling and robust
homophone discrimination. Both are essential for raw or long-tail word
recognition. The proposed approach adopts a two-stage learning paradigm. First,
we introduce a pronunciation-guided context learning method. It employs an
interleaved grapheme-phoneme context modeling strategy that incorporates
grapheme-only distractors, encouraging the model to leverage phonemic cues for
accurate recognition. Then, we propose a pronunciation-discriminative
reinforcement learning method with perturbed label sampling to further enhance
the model\'s ability to distinguish contextualized homophones. Experimental
results on the public English Librispeech and Mandarin AISHELL-1 datasets
indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and
53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and
60.5% relative reductions in biased WER for long-tail words compared to strong
baselines, respectively.

</details>


### [124] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 本文提出了一种无需访问大模型内部、可用于检测大语言模型（LLMs）在输出中存在意识形态倾向的方法。该方法通过分析围绕特定主题的输出分布转变，检测潜在的意识形态引导，适用于黑盒审计。实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于大众产品，其输出可能影响个人信念乃至社会舆论。如果有人能有意引导模型输出特定政治或宗教立场，这将带来对公共话语的巨大影响。因此，亟需发展检测类似引导行为的方法。

Method: 作者基于此前提出的统计方法，调整用于意识形态偏见审计。该方法无需了解模型内部结构，通过分析与特定话题相关的多样化输入下输出的分布变化，以判断模型是否被潜在引导，具备模型无关性，特别适合封闭黑盒模型的审计。

Result: 通过大量实验证明，所提方法能有效识别大语言模型在特定主题下输出的意识形态倾向转变，展示了其实用性和可靠性。

Conclusion: 该方法为独立后验审计大语言模型在意识形态偏见和引导方面提供了有力工具，尤其适用于无法访问模型内部的商业黑盒系统，有助于提升大模型在公共领域的透明度和可信度。

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [125] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
*Yougen Zhou,Qin Chen,Ningning Zhou,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CL

TL;DR: 本文针对大语言模型（LLM）在情感支持对话（ESC）中的策略倾向偏好问题，揭示其根本原因，并提出基于强化学习与双重奖励的偏好缓解方法，在多个数据集和模型上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在情感支持对话策略规划的准确性不高，且存在过度偏向特定策略的倾向，造成对话效果有限。虽然此前有通过微调策略规划器缓解偏好问题的方法，但缺少对偏好根源的深入研究。

Method: 作者首先揭示了偏好产生的根本原因，即LLM在策略规划中的知识边界。然后提出利用融合准确性与基于熵的置信度的双重奖励函数，通过强化学习优化模型在不同知识区间内的策略规划。实验采用ESCov和ExTES数据集，并在多种LLM架构上进行检验。

Result: 在两个真实情感支持对话数据集和多种大语言模型主干上，所提出的方法均显著优于基线方法，有效提升了策略多样性和对话效果。

Conclusion: 论文证实了通过识别知识边界并设计双重奖励函数的强化学习方法能够有效缓解LLM在ESC任务中的偏向问题，提升了模型策略规划的准确性和多样性。

Abstract: Emotional support conversation (ESC) aims to alleviate distress through
empathetic dialogue, yet large language models (LLMs) face persistent
challenges in delivering effective ESC due to low accuracy in strategy
planning. Moreover, there is a considerable preference bias towards specific
strategies. Prior methods using fine-tuned strategy planners have shown
potential in reducing such bias, while the underlying causes of the preference
bias in LLMs have not well been studied. To address these issues, we first
reveal the fundamental causes of the bias by identifying the knowledge
boundaries of LLMs in strategy planning. Then, we propose an approach to
mitigate the bias by reinforcement learning with a dual reward function, which
optimizes strategy planning via both accuracy and entropy-based confidence for
each region according to the knowledge boundaries. Experiments on the ESCov and
ExTES datasets with multiple LLM backbones show that our approach outperforms
the baselines, confirming the effectiveness of our approach.

</details>


### [126] [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
*Zequn Xie,Chuxin Wang,Sihang Cai,Yeqiang Wang,Shulei Wang,Tao Jin*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注的新型文本-人像检索系统，通过生成和交互式模块提升检索准确率和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的人员检索（TBPS）依赖大量高质量自然语言描述作为标签，但人工标注成本极高，严重制约了系统的扩展性和实际应用。

Method: 提出了两个互补模块：1）多轮文本生成（MTG），利用多模态大模型（MLLM）进行模拟对话，自动生成细粒度且多样化的伪标签，实现无监督文本标注；2）多轮文本交互（MTI），在推理阶段动态优化用户检索描述，通过对话式推理应对用户描述模糊、不完整或有歧义的问题。

Result: 实验表明，该方法在无需人工标注的情况下，检索准确率、鲁棒性与易用性均达到或者超过当前主流方法。

Conclusion: 该框架显著降低了人工标注成本，实现了可扩展且实用的文本-人像检索，为TBPS系统的实际部署提供了新思路。

Abstract: Text-based person search (TBPS) enables the retrieval of person images from
large-scale databases using natural language descriptions, offering critical
value in surveillance applications. However, a major challenge lies in the
labor-intensive process of obtaining high-quality textual annotations, which
limits scalability and practical deployment. To address this, we introduce two
complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text
Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues
with MLLMs, producing fine-grained and diverse visual descriptions without
manual supervision. MTI refines user queries at inference time through dynamic,
dialogue-based reasoning, enabling the system to interpret and resolve vague,
incomplete, or ambiguous descriptions - characteristics often seen in
real-world search scenarios. Together, MTG and MTI form a unified and
annotation-free framework that significantly improves retrieval accuracy,
robustness, and usability. Extensive evaluations demonstrate that our method
achieves competitive or superior results while eliminating the need for manual
captions, paving the way for scalable and practical deployment of TBPS systems.

</details>


### [127] [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
*Shaz Furniturewala,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出了一种基于可解释性方法的新策略，通过识别和抑制有毒内容分类器（如BERT和RoBERTa）中易受攻击的关键组件，从而提升应对LLM生成文本的对抗攻击性能，并揭示模型在不同群体间的公平性和鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成内容大量涌现，传统基于人工文本训练的内容审核系统面临误判，尤其是对抗性攻击难以识别，现有防护多为被动反应，缺乏针对模型内部脆弱点的主动防御。

Method: 本文使用机制可解释性技术，分析BERT和RoBERTa具毒性分类器，通过对多种数据集和少数群体的文本实施对抗攻击，识别模型内部易受攻击的回路（circuits），并通过压制这些回路来提升分类器的健壮性。

Result: 实验发现，模型内部存在对性能至关重要或易受攻击的不同注意力头（heads），抑制脆弱头能显著提升模型对抗攻击下的性能，同时发现针对不同群体，模型的脆弱头也存在不同分布。

Conclusion: 识别并抑制有毒内容分类器中的脆弱组件可提升其鲁棒性，且对不同群体需关注各自的脆弱点，为更公平和可靠的内容审核模型提供了新的方向。

Abstract: The volume of machine-generated content online has grown dramatically due to
the widespread use of Large Language Models (LLMs), leading to new challenges
for content moderation systems. Conventional content moderation classifiers,
which are usually trained on text produced by humans, suffer from
misclassifications due to LLM-generated text deviating from their training data
and adversarial attacks that aim to avoid detection. Present-day defence
tactics are reactive rather than proactive, since they rely on adversarial
training or external detection models to identify attacks. In this work, we aim
to identify the vulnerable components of toxicity classifiers that contribute
to misclassification, proposing a novel strategy based on mechanistic
interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa
classifiers, testing on diverse datasets spanning a variety of minority groups.
We use adversarial attacking techniques to identify vulnerable circuits.
Finally, we suppress these vulnerable circuits, improving performance against
adversarial attacks. We also provide demographic-level insights into these
vulnerable circuits, exposing fairness and robustness gaps in model training.
We find that models have distinct heads that are either crucial for performance
or vulnerable to attack and suppressing the vulnerable heads improves
performance on adversarial input. We also find that different heads are
responsible for vulnerability across different demographic groups, which can
inform more inclusive development of toxicity detection models.

</details>


### [128] [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)
*Hiroyuki Deguchi,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了一种基于案例的决策理论解码方法（CBDT），能够提升文本生成的质量，尤其在跨领域场景下优于传统的MBR和MAP解码。


<details>
  <summary>Details</summary>
Motivation: MBR解码需要从生成模型中抽样文本，但对域外知识获取有限，难以在新领域正确生成。为了解决这一问题，作者希望通过引入外部域数据的示例来提升生成效果。

Method: 作者提出了CBDT解码，该方法通过领域数据示例来估计期望效用，可结合MBR解码共同提升生成效果。具体实现中，CBDT利用案例样本来辅助解码决策，不再完全依赖模型内部分布抽样。

Result: CBDT解码不仅优于MAP解码方法，还在7个领域的De-En和Ja↔En翻译任务、以及MSCOCO与nocaps图像描述任务中，联合MBR与CBDT解码显著优于单独使用MBR。

Conclusion: CBDT解码能有效提升跨领域文本生成质量，结合现有的MBR解码有更优表现，尤其在领域迁移和复杂多样化场景中极具应用前景。

Abstract: Minimum Bayes risk (MBR) decoding is a decision rule of text generation,
which selects the hypothesis that maximizes the expected utility and robustly
generates higher-quality texts than maximum a posteriori (MAP) decoding.
However, it depends on sample texts drawn from the text generation model; thus,
it is difficult to find a hypothesis that correctly captures the knowledge or
information of out-of-domain. To tackle this issue, we propose case-based
decision-theoretic (CBDT) decoding, another method to estimate the expected
utility using examples of domain data. CBDT decoding not only generates
higher-quality texts than MAP decoding, but also the combination of MBR and
CBDT decoding outperformed MBR decoding in seven domain De--En and
Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO
and nocaps datasets.

</details>


### [129] [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)
*Biswadip Mandal,Anant Khandelwal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文提出了HistoryBank数据库，实现了跨10种语言、涵盖1000万以上历史事件的多语言历史时序推理资源，并构建了全面的多语言时序问答基准，用以评估主流语言模型的历史事件时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的时序推理能力评估数据集存在规模有限、缺乏多语言支持、过于聚焦当代事件等问题，影响了多语种历史事件推理能力的研究。

Method: 作者从Wikipedia的时间轴页面和信息框提取历史事件，构建了覆盖10种语言、包含千万级历史事件的数据库（HistoryBank）。此外，设计了六类时序推理问答任务，组成多语言历史时序推理基准，并对多种主流大模型进行评测。

Result: GPT4o在各语言及各类型的回答中效果最佳，Gemma-2表现优于其它小型语言模型。历史事件时序问答基准充分反映了各模型的多语种时序推理能力。

Conclusion: HistoryBank为历史事件的多语言时序推理研究提供了系统性资源和评测基准，有助于推动相关NLP任务和模型的发展，数据和代码将在论文接收后开放。

Abstract: Temporal reasoning about historical events is a critical skill for NLP tasks
like event extraction, historical entity linking, temporal question answering,
timeline summarization, temporal event clustering and temporal natural language
inference. Yet efforts on benchmarking temporal reasoning capabilities of large
language models (LLMs) are rather limited. Existing temporal reasoning datasets
are limited in scale, lack multilingual coverage and focus more on contemporary
events. To address these limitations, we present HistoryBank, a multilingual
database of 10M+ historical events extracted from Wikipedia timeline pages and
article infoboxes. Our database provides unprecedented coverage in both
historical depth and linguistic breadth with 10 languages. Additionally, we
construct a comprehensive question answering benchmark for temporal reasoning
across all languages. This benchmark covers a diverse set of 6 temporal QA
reasoning tasks, and we evaluate a suite of popular language models
(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their
performance on these tasks. As expected GPT4o performs best across all answer
types and languages; Gemma-2 outperforms the other small language models. Our
work aims to provide a comprehensive resource for advancing multilingual and
temporally-aware natural language understanding of historical events. To
facilitate further research, we will make our code and datasets publicly
available upon acceptance of this paper.

</details>


### [130] [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)
*Omri Suissa,Muhiim Ali,Shengmai Chen,Yinuo Cai,Shekhar Pradhan*

Main category: cs.CL

TL;DR: 本文提出了一种新方法CLEAR GLASS，让视觉语言模型(VLM)更好地具备抽象概念识别能力。通过分组对比损失和自创的数据集MAGIC，提升模型对高层次语义的感知能力，并在实验中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 人类能够基于图像抽象出高层次概念，而不仅仅是识别物体及其关系。目前主流VLM模型对这种抽象能力有限。因此作者希望提升模型的概念抽象识别能力。

Method: 提出MAGIC数据集和分组对比损失（包括外部和内部损失），让模型在训练时将分组中的图像和文本表征共同的高层语义特征，而无需直接暴露高层次概念标签。新损失机制促使模型隐式学习并靠近高层概念的语义空间。

Result: 通过新的训练方法和损失设计，训练出的CLEAR GLASS模型在抽象概念识别任务上超过了当前SOTA模型。

Conclusion: 该方法有效提升了模型对高层次、抽象语义的理解能力，验证了通过分组对比损失和MAGIC数据集能提升抽象概念建模的能力，对后续VLM研究有借鉴意义。

Abstract: Humans can recognize an image as an instance of a general concept, beyond
simply identifying its objects and their relationships. In this paper, we
investigate 1. The extent to which VLMs have this concept abstraction capacity,
and 2. Strategies for encoding the sort of higher-concept information in images
that would enable the resulting VLM model (CLEAR GLASS model) to have this
capability to a greater degree. To this end, we introduce a grouped
image-caption dataset (MAGIC), which consists of several groups of image
captions and for each group a set of associated images and higher-level
conceptual labels. We use a novel contrastive loss technique to induce the
model to encode in the representation of each image (caption) in a group the
information that is common to all members of the image-caption group. Our main
contribution is a grouped contrastive loss function based on text-image
contrastive groups (outer contrastive loss) as well as an inner loss which
measures the distances between image-caption instances in the group. Our
training methodology results in the CLEAR GLASS model having the concept
abstraction capacity as an emergent capacity because the model is not exposed
to the higher-level concepts associated with each group. Instead, the training
forces the model to create for each image-caption group a semantic
representation that brings it closer to the semantic representation of the
higher-level concepts in the latent semantic space. Our experiments show that
this training methodology results in a model which shows improvement in
abstract concept recognition compared to SOTA models.

</details>


### [131] [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)
*Binquan Ji,Jiaqi Wang,Ruiting Li,Xingchen Han,Yiyang Qi,Shichao Wang,Yifei Lu,Yuantao Han,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出了一种用于大型语言模型生成长文本的新框架，先检索知识、再聚类结构，避免内容不连贯与虚假，提升可靠性与结构性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型“先规划后检索证据”的顶层方法，在生成长文本时容易出现结构碎片化与事实错误，与底层知识脱节。亟需新方法确保内容紧扣知识来源、提升准确性。

Method: 创新提出“先检索后聚类”的知识优先、结构聚类框架。流程为：先从知识库全面、迭代式检索相关信息，再用无监督聚类算法将信息归为不同‘知识簇’，据此生成大纲与最终文档。避免模型主观臆断与内容漂移。

Result: 在14B和32B参数量模型实验中，该方法效果达到或超越最新一流基线，并在需高忠实度和结构连贯度的知识受限场景表现出独特优势。

Conclusion: 该框架有效提升长文本生成的可靠性与结构性，有助于大模型应用于关键、知识密集型领域，推动生成式AI任务发展。

Abstract: Large Language Models (LLMs) have shown remarkable prowess in text
generation, yet producing long-form, factual documents grounded in extensive
external knowledge bases remains a significant challenge. Existing "top-down"
methods, which first generate a hypothesis or outline and then retrieve
evidence, often suffer from a disconnect between the model's plan and the
available knowledge, leading to content fragmentation and factual inaccuracies.
To address these limitations, we propose a novel "bottom-up," data-driven
framework that inverts the conventional generation pipeline. Our approach is
predicated on a "Retrieval-First for Knowledge, Clustering for Structure"
strategy, which first establishes the "knowledge boundaries" of the source
corpus before any generative planning occurs. Specifically, we perform
exhaustive iterative retrieval from the knowledge base and then employ an
unsupervised clustering algorithm to organize the retrieved documents into
distinct "knowledge clusters." These clusters form an objective, data-driven
foundation that directly guides the subsequent generation of a hierarchical
outline and the final document content. This bottom-up process ensures that the
generated text is strictly constrained by and fully traceable to the source
material, proactively adapting to the finite scope of the knowledge base and
fundamentally mitigating the risk of hallucination. Experimental results on
both 14B and 32B parameter models demonstrate that our method achieves
performance comparable to or exceeding state-of-the-art baselines, and is
expected to demonstrate unique advantages in knowledge-constrained scenarios
that demand high fidelity and structural coherence. Our work presents an
effective paradigm for generating reliable, structured, long-form documents,
paving the way for more robust LLM applications in high-stakes,
knowledge-intensive domains.

</details>


### [132] [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)
*Kurt Micallef,Nizar Habash,Claudia Borg*

Main category: cs.CL

TL;DR: 本文探讨阿拉伯语资源是否能够通过跨语种增强技术促进马耳他语NLP发展。作者尝试多种文本对齐方法，并提出新的转写系统，实验结果表明阿拉伯语数据增强显著提升了马耳他语NLP任务表现。


<details>
  <summary>Details</summary>
Motivation: 马耳他语属于闪米特语系，却长期受意大利语、英语等影响，采用拉丁字母拼写，因此其与阿拉伯语之间存在形态与书写鸿沟。受限于资源稀缺，马耳他语NLP发展受阻，因此作者希望借助富资源的阿拉伯语，探索跨语种数据增强，提升马耳他语的自然语言处理能力。

Method: 作者采用了多种策略将阿拉伯语文本与马耳他语对齐，包括研究不同的转写方案与机器翻译方法，并创新性地提出了更符合马耳他语拼写的转写系统。随后，将这些增强数据应用于单语及多语NLP模型，评估其效果。

Result: 实验结果表明，采用阿拉伯语数据增强后，无论是在单语还是多语模型中，马耳他语NLP任务表现都有显著提升。提出的新转写系统在马耳他语正字法表达上更为精准，也有助于跨语种增强。

Conclusion: 阿拉伯语资源通过合理的对齐与转写方法，能够显著促进马耳他语NLP的发展。本文提出的新转写系统和跨语种方法为资源稀缺语言的NLP研究提供了新的方向和思路。

Abstract: Maltese is a unique Semitic language that has evolved under extensive
influence from Romance and Germanic languages, particularly Italian and
English. Despite its Semitic roots, its orthography is based on the Latin
script, creating a gap between it and its closest linguistic relatives in
Arabic. In this paper, we explore whether Arabic-language resources can support
Maltese natural language processing (NLP) through cross-lingual augmentation
techniques. We investigate multiple strategies for aligning Arabic textual data
with Maltese, including various transliteration schemes and machine translation
(MT) approaches. As part of this, we also introduce novel transliteration
systems that better represent Maltese orthography. We evaluate the impact of
these augmentations on monolingual and mutlilingual models and demonstrate that
Arabic-based augmentation can significantly benefit Maltese NLP tasks.

</details>


### [133] [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)
*Fuyu Xing,Zimu Wang,Wei Wang,Haiyang Zhang*

Main category: cs.CL

TL;DR: 本论文系统性评估了代表性的多模态大模型（LVLMs）在多媒体事件抽取（M2E2）任务上的表现，揭示了其在不同子任务和设定下的优劣与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体内容的激增，亟需开发高效的多媒体事件抽取系统。尽管多模态大模型展现了强大的跨模态能力，其在M2E2任务上的潜力尚未被详细研究，因此需要系统性地评估其效果与局限。

Method: 作者选择了DeepSeek-VL2和Qwen-VL系列等典型LVLMs，在M2E2数据集上，分别在文本、图像及跨媒体子任务下，采用少样本提示与微调两种方式进行系统性评估，并进行细致的错误分析。

Result: 主要发现包括：1）少样本提示下LVLMs在视觉任务表现突出，但文本任务表现较差；2）利用LoRA方法微调模型可大幅提升效果；3）多模态信息融合时，LVLMs表现出很强的协同能力，跨模态效果最佳。此外，论文揭示了语义精确性、定位和跨模态对齐等依然是LVLMs在M2E2任务中的主要挑战。

Conclusion: LVLMs在多媒体事件抽取任务中展现了巨大潜力，尤其在跨模态场景下表现优异，但依然面临若干难以突破的问题，未来应针对语义、定位和跨模态对齐等关键瓶颈进一步研究。

Abstract: The proliferation of multimedia content necessitates the development of
effective Multimedia Event Extraction (M2E2) systems. Though Large
Vision-Language Models (LVLMs) have shown strong cross-modal capabilities,
their utility in the M2E2 task remains underexplored. In this paper, we present
the first systematic evaluation of representative LVLMs, including DeepSeek-VL2
and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,
image-only, and cross-media subtasks, assessed under both few-shot prompting
and fine-tuning settings. Our key findings highlight the following valuable
insights: (1) Few-shot LVLMs perform notably better on visual tasks but
struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA
substantially enhances model performance; and (3) LVLMs exhibit strong synergy
when combining modalities, achieving superior performance in cross-modal
settings. We further provide a detailed error analysis to reveal persistent
challenges in areas such as semantic precision, localization, and cross-modal
grounding, which remain critical obstacles for advancing M2E2 capabilities.

</details>


### [134] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种全新的题目难度估计方法，仅依赖于大模型自身的隐藏向量，无需生成答案或重复采样，实验显示可大幅提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的问题难度估计方法需多次采样或额外模型，导致计算资源消耗大、泛化性有限，亟需更高效且通用的替代方案。

Method: 作者将大模型的生成过程建模为马尔可夫链，并定义值函数来估算任意隐藏状态下的期望输出质量，最终实现仅通过初始隐藏状态估算问题难度，无需生成回答。

Result: 在文本和多模态任务上，该方法在估计准确性和效率上持续优于主流基线方法。同时，难度估计结果进一步用于指导自适应推理，实现更高推理效率。

Conclusion: 只利用LLM的隐藏向量即可高效、准确估计问题难度，为自适应推理等下游应用带来显著性能提升，具有良好的应用前景。

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [135] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
*Shiyu Li,Yang Tang,Ruijie Liu,Shi-Zhe Chen,Xi Chen*

Main category: cs.CL

TL;DR: 提出了一种新型的1.4B参数大模型Conan-embedding-v2，用于文本嵌入任务，并通过从零开始预训练和一系列新方法实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型通过LoRA微调做文本嵌入受限于数据和训练方式的差异，且很难兼顾多语言和更优表现，需要新的模型和训练范式以缩小这些差距。

Method: 1. 加入新闻数据和多语言对进行预训练，弥补数据差距；2. 构建跨语言检索数据集提升多语言嵌入能力；3. 引入soft-masking机制，在token因果mask和句子级双向mask间平滑过渡，提升模型泛化；4. 动态困难负例挖掘，让模型见识更多难样本，提高判别力。

Result: 虽然参数量仅为1.4B，Conan-embedding-v2在MTEB和中文MTEB上都取得了最新SOTA成绩。

Conclusion: 通过创新的数据构建、训练策略和结构设计，Conan-embedding-v2有效提升了文本嵌入性能，是高效、有效的新一代大模型文本向量方案。

Abstract: Large language models (LLMs) have recently demonstrated excellent performance
in text embedding tasks. Previous work usually use LoRA to fine-tune existing
LLMs, which are limited by the data and training gap between LLMs and embedding
models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM
trained from scratch and fine-tuned as a text embedder. First, we add news data
and multilingual pairs for LLM pretraining to bridge the data gap. Based on
this, we propose a cross-lingual retrieval dataset that enables the LLM to
better integrate embeddings across different languages. Second, whereas LLMs
use a causal mask with token-level loss, embedding models use a bidirectional
mask with sentence-level loss. This training gap makes full fine-tuning less
effective than LoRA. We introduce a soft-masking mechanism to gradually
transition between these two types of masks, enabling the model to learn more
comprehensive representations. Based on this, we propose a dynamic hard
negative mining method that exposes the model to more difficult negative
examples throughout the training process. Being intuitive and effective, with
only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA
performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese
MTEB (May 19, 2025).

</details>


### [136] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
*Caiqi Zhang,Chang Shu,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出了一套无监督、基于图的方法来提升大语言模型在推理任务中的置信度估计能力，并在多项实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法多针对事实类问答任务设计，难以推广到复杂推理任务，因此需要开发更适用于推理场景的新方法。

Method: 作者提出将推理路径以有向图建模，通过分析图的中心性、路径收敛性和路径权重等性质，实现对推理置信度的量化。该方法无需额外训练，直接基于模型输出推理过程。

Result: 在两个大语言模型和三个推理数据集上的实验显示，该方法显著提升了置信度估计准确性，并在两个下游任务中也带来性能提升。

Conclusion: 基于图的置信度估计方法有助于提升大模型在推理任务中的可靠性，展现出广阔应用前景。

Abstract: Confidence estimation is essential for the reliable deployment of large
language models (LLMs). Existing methods are primarily designed for factual QA
tasks and often fail to generalize to reasoning tasks. To address this gap, we
propose a set of training-free, graph-based confidence estimation methods
tailored to reasoning tasks. Our approach models reasoning paths as directed
graphs and estimates confidence by exploiting graph properties such as
centrality, path convergence, and path weighting. Experiments with two LLMs on
three reasoning datasets demonstrate improved confidence estimation and
enhanced performance on two downstream tasks.

</details>


### [137] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
*Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本论文提出了一种自动化生成完整科研流程的新方法，并在自然语言处理领域进行了案例验证，显著提升了研究复现性和流程自动化水平。


<details>
  <summary>Details</summary>
Motivation: 提升科研流程的可复现性和加速“AI for Science”的研究模式，克服以往方法只能提取零散流程信息、难以自动化生成完整科研工作流的缺陷。

Method: 提出端到端的流程生成框架：1）用PU Learning结合SciBERT识别描述流程的段落；2）用Flan-T5生成流程短语；3）用ChatGPT进行阶段分类；4）通过文档索引绘制可视化流程图。基于NLP学科论文数据集验证方法有效性。

Result: 段落提取F1分数达到0.9772，流程短语生成的ROUGE-1/2/L分别为0.4543/0.2877/0.4427，分类精度0.958。能自动生成完整、结构化的科研流程图谱，并可追溯文献来源。

Conclusion: 提出的方法能够自动化、高质量地生成科研工作流，为流程分析和科学范式转变的实证研究提供了强大工具。可扩展至其它领域，有助于提高科研透明度和效率。

Abstract: The automated generation of research workflows is essential for improving the
reproducibility of research and accelerating the paradigm of "AI for Science".
However, existing methods typically extract merely fragmented procedural
components and thus fail to capture complete research workflows. To address
this gap, we propose an end-to-end framework that generates comprehensive,
structured research workflows by mining full-text academic papers. As a case
study in the Natural Language Processing (NLP) domain, our paragraph-centric
approach first employs Positive-Unlabeled (PU) Learning with SciBERT to
identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.
Subsequently, we utilize Flan-T5 with prompt learning to generate workflow
phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of
0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically
categorized into data preparation, data processing, and data analysis stages
using ChatGPT with few-shot learning, achieving a classification precision of
0.958. By mapping categorized phrases to their document locations in the
documents, we finally generate readable visual flowcharts of the entire
research workflows. This approach facilitates the analysis of workflows derived
from an NLP corpus and reveals key methodological shifts over the past two
decades, including the increasing emphasis on data analysis and the transition
from feature engineering to ablation studies. Our work offers a validated
technical framework for automated workflow generation, along with a novel,
process-oriented perspective for the empirical investigation of evolving
scientific paradigms. Source code and data are available at:
https://github.com/ZH-heng/research_workflow.

</details>


### [138] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
*Yuval Weiss,David Demitri Africa,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 本文首次系统性地研究了ReLoRA方法在小型语言模型(SLMs)预训练中的表现，发现ReLoRA整体效果不如标准训练，并随着模型变大表现更差，暗示低秩更新策略在SLMs上难以迁移。


<details>
  <summary>Details</summary>
Motivation: LoRA等高效参数微调方法极大简化了LLM微调，但将其扩展到预训练领域，尤其是计算资源友好的小模型(SLMs)尚未得到充分探索，因此需要评估ReLoRA在SLMs中的可行性与表现。

Method: 在11M-66M参数规模的小型语言模型上，运用ReLoRA进行系统性预训练实验，并与标准训练进行性能与学习动态对比，辅以消融实验和分析模型秩缺陷情况。

Result: 实验表明：无论在损失、Paloma困惑度还是BLiMP基准上，ReLoRA在SLMs中的表现均弱于标准训练，且模型越大差距越明显；进一步分析表明，ReLoRA强化了小模型中本已存在的秩缺陷。

Conclusion: 当前低秩更新策略如ReLoRA难以有效迁移到小型语言模型的预训练，针对低算力场景需要新的高效预训练方法。

Abstract: Parameter-efficient methods such as LoRA have revolutionised the fine-tuning
of LLMs. Still, their extension to pretraining via ReLoRA is less well
understood, especially for small language models (SLMs), which offer lower
computational and environmental costs. This work is the first systematic study
of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and
learning dynamics. Through ablation experiments, we find that ReLoRA generally
performs worse than standard training on loss, Paloma perplexity and BLiMP,
with the gap widening for the larger models. Further analysis of the learning
dynamics of the models indicates that ReLoRA reinforces the rank deficiencies
found in smaller models. These results indicate that low-rank update strategies
may not transfer easily to SLM pretraining, highlighting the need for more
research in the low-compute regime.

</details>


### [139] [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)
*Chenye Zou,Xingyue Wen,Tianyi Hu,Qian Janice Wang,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本论文提出了中英文葡萄酒评论的跨文化适应任务，并收集了首个相关的专业双语评论数据集。研究发现现有模型难以准确传达文化细节，凸显了机器翻译和大语言模型在文化内容处理上的局限。


<details>
  <summary>Details</summary>
Motivation: 传统翻译模型大多聚焦于字面意义，忽略了文化差异，而葡萄酒评论极具文化色彩，因此有必要研究如何使翻译结果更契合目标文化。

Method: 作者收集并整理了包含8000条中文评论和16000条英语评论的葡萄酒专业评论平行语料库，并以神经机器翻译模型和现有的先进大语言模型作为基线，通过自动指标和人工评估（引入文化贴近性、中立性、真实性三个新维度）衡量翻译质量。

Result: 实验结果显示，无论是基线还是先进的大语言模型，在翻译涉及文化内容的葡萄酒描述时，均表现出较大困难，未能很好把握目标文化的细腻差异。

Conclusion: 论文凸显了现有翻译技术在处理文化相关任务时的显著挑战，未来需开发更能理解和适应文化差异的模型，以提升跨文化交流效果。

Abstract: Recent advances in large language models (LLMs) have opened the door to
culture-aware language tasks. We introduce the novel problem of adapting wine
reviews across Chinese and English, which goes beyond literal translation by
incorporating regional taste preferences and culture-specific flavor
descriptors. In a case study on cross-cultural wine review adaptation, we
compile the first parallel corpus of professional reviews, containing 8k
Chinese and 16k Anglophone reviews. We benchmark both
neural-machine-translation baselines and state-of-the-art LLMs with automatic
metrics and human evaluation. For the latter, we propose three culture-oriented
criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness
-- to assess how naturally a translated review resonates with target-culture
readers. Our analysis shows that current models struggle to capture cultural
nuances, especially in translating wine descriptions across different cultures.
This highlights the challenges and limitations of translation models in
handling cultural content.

</details>


### [140] [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)
*Jian Gao,Fufangchen Zhao,Yiyang Zhang,Danfeng Yan*

Main category: cs.CL

TL;DR: 本文提出了一种名为SitLLM的多模态轻量级坐姿监测框架，通过结合柔性压力传感和大语言模型，提升了坐姿识别的精细度和个性化健康反馈能力。


<details>
  <summary>Details</summary>
Motivation: 现有的坐姿监测系统对不良坐姿长期导致的肌肉骨骼疾病关注不足，且多采用视觉、IMU或压力方式，但识别能力粗糙且反馈不具语义丰富性，无法满足个性化健康需求。

Method: SitLLM包含三大模块：（1）高斯鲁棒传感嵌入模块，将压力分布图拆分为局部空间块并加入噪声以增强特征提取的鲁棒性；（2）基于提示的跨模态对齐模块，通过多头注意力机制将传感器嵌入映射到LLM语义空间；（3）多上下文提示模块，融合特征、结构、统计和语义多层信息，提升指令理解与响应能力。

Result: SitLLM实现了对坐姿的精细识别与个性化健康建议生成，突破了传统方法粗粒度识别和反馈表达不足的瓶颈。

Conclusion: 集成柔性压力感知与大语言模型，SitLLM不仅提高了坐姿识别的准确性和细粒度，还能输出更具个性化和健康导向的语义反馈，对预防和干预不良坐姿相关健康问题具有实用意义。

Abstract: Poor sitting posture is a critical yet often overlooked factor contributing
to long-term musculoskeletal disorders and physiological dysfunctions. Existing
sitting posture monitoring systems, although leveraging visual, IMU, or
pressure-based modalities, often suffer from coarse-grained recognition and
lack the semantic expressiveness necessary for personalized feedback. In this
paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that
integrates flexible pressure sensing with large language models (LLMs) to
enable fine-grained posture understanding and personalized health-oriented
response generation. SitLLM comprises three key components: (1) a
\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps
into spatial patches and injects local noise perturbations for robust feature
extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that
reprograms sensor embeddings into the LLM's semantic space via multi-head
cross-attention using the pre-trained vocabulary embeddings; and (3) a
\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,
statistical-level, and semantic-level contextual information to guide
instruction comprehension.

</details>


### [141] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
*Nolan Platt,Pragyansmita Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种使用大型语言模型（LLM）作为一次性教师，进而训练便宜小模型的方法，在海事智能领域实现了大幅成本降低，同时保持了较高的准确率。


<details>
  <summary>Details</summary>
Motivation: LLM在特定领域的应用受限于高昂成本和领域数据稀缺，尤其是在如海事智能等专业领域，存在大模型推理费用高、人工标注难以获得等问题。

Method: 通过GPT-4o和o3-mini等多模型生成，对32亿条AIS船只追踪记录合成为2万余个问答对，避免过拟合并确保推理准确，最终对Qwen2.5-7B模型进行微调。

Result: 微调后的Qwen2.5-7B模型在海事任务上准确率达到75%，推理成本相比直接使用大型LLM减少261倍。

Conclusion: 在专业领域，经过合成数据微调的小模型可达到与大模型相近的准确率但成本远低，框架易复现，适用于人工标注难的领域，并能立即用于海事安全和交通管理等应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
many domains, yet their appli- cation to specialized fields remains constrained
by the scarcity and complexity of domain-specific training data. We present a
novel approach that achieves a 261x cost reduction for maritime intelligence by
using LLMs as one-time teachers rather than using them directly for inference.
Our method transforms 3.2 billion Automatic Identification System (AIS) vessel
tracking records into 21,543 synthetic question and answer pairs through
multi-model generation (GPT-4o and o3-mini), preventing over- fitting and
ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves
75% accuracy on maritime tasks, while being substantially cheaper than using a
larger model for inference. We show that smaller, cheaper models - when fine
tuned properly - can provide similar accuracy compared to larger models that
are prohibitively expensive. Our work contributes to the growing field of
synthetic dataset generation for specialized AI applications and presents a
highly reproducible framework for domains where manual annotation is
infeasible. Beyond expand- ing research in the growing field of specialized
small language models, our approach has immediate applications in maritime
safety, security operations, and vessel traffic management systems in various
industries.

</details>


### [142] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
*Francesco Pappone,Ruggero Marino Lazzaroni,Federico Califano,Niccolò Gentile,Roberto Marras*

Main category: cs.CL

TL;DR: 本文提出利用小型高效的编码器-Only Transformer作为语义奖励模型，通过cosine相似度为解释生成任务提供丰富语义奖励信号，促进模型输出更符合专家推理的高质量解释，显著提升模型解释的准确性与清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成高质量、教学合理的文本任务中难以通过现有指标（如ROUGE、人工评价）准确衡量输出质量，人工评价成本高，关键词匹配缺乏语义深度。因此亟需更高效且能捕捉深层语义的自动化奖励评价方法。

Method: 在Group Relative Policy Optimisation(GRPO)强化学习框架下，采用小型编码器-Only Transformer，通过计算生成解释与真实参考解释的向量余弦相似度来提供连续语义奖励信号，引导训练模型产生在结构和概念上与专家推理一致的解释。同时结合常规领域自适应继续预训练(CPT)和有监督微调(SFT)训练医学考试解释生成模型。

Result: 使用所提出的语义奖励模型结合GRPO强化学习后，模型在医学考试解释生成任务上，其解释的忠实性和清晰度较强监督微调（SFT）基线显著提升。

Conclusion: 轻量级编码器语义奖励模型能为复杂文本生成提供细致奖励信号，有效提升LLM在高质量解释生成的表现，证明该方法对深层语义任务具有广泛应用潜力。

Abstract: While Large Language Models (LLMs) excel at generating human-like text,
aligning their outputs with complex, qualitative goals like pedagogical
soundness remains a significant challenge. Standard reinforcement learning
techniques often rely on slow and expensive LLM-as-a-judge evaluations or on
brittle, keyword-based metrics like ROUGE, which fail to capture the semantic
essence of a high-quality explanation. In this work, we introduce a novel
approach to reward shaping within the Group Relative Policy Optimisation (GRPO)
framework. Our central contribution is the use of a small, efficient
encoder-only transformer as a semantic reward model. This model provides a
dense, semantically rich reward signal based on the cosine similarity between a
generated explanation and a ground-truth reference, guiding the policy towards
explanations that are not just factually correct but also structurally and
conceptually aligned with expert reasoning. We apply this method to the task of
training a model for the Italian medical-school entrance examinations,
following standard domain-adaptive continued pre-training (CPT) and supervised
fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic
reward significantly improves explanation faithfulness and clarity over a
strong SFT baseline, showcasing the power of using lightweight encoder models
for nuanced reward shaping in complex generation tasks

</details>


### [143] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
*Sijia Cui,Shuai Xu,Aiyao He,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: 本文提出了PLAP框架，结合LLM与参数化技能，实现AI智能体在复杂长期任务中的有效规划与执行。实验证明，在MicroRTS环境下，PLAP驱动的智能体超越大部分基线模型，并首次建立了长时序策略规划领域的LLM排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI智能体在应对长期、复杂和对抗性环境时，面临动作生成不可靠或高层任务落地依赖专家经验的问题，需有更有效的落地机制。

Method: 提出PLAP框架，包括包含特定环境技能的技能库、基于LLM的技能规划器和技能执行器，将高层语言规划与低层动作执行结合。在MicroRTS游戏中实施并评测，使用多种LLM（如GPT-4o、Qwen2-72B）进行对比分析。

Result: PLAP可支持零样本（zero-shot）或少样本（few-shot）设置下，GPT-4o驱动的PLAP优于80%基线，Qwen2-72B驱动的PLAP超过顶尖脚本智能体CoacAI。并建立了LLM在长时序策略规划方面的排行榜。

Conclusion: PLAP框架有效促进了LLM智能体在长时域复杂环境中的任务执行表现，未来有望进一步拓展至更多实际应用场景。

Abstract: Recent advancements in Large Language Models(LLMs) have led to the
development of LLM-based AI agents. A key challenge is the creation of agents
that can effectively ground themselves in complex, adversarial long-horizon
environments. Existing methods mainly focus on (1) using LLMs as policies to
interact with the environment through generating low-level feasible actions,
and (2) utilizing LLMs to generate high-level tasks or language guides to
stimulate action generation. However, the former struggles to generate reliable
actions, while the latter relies heavily on expert experience to translate
high-level tasks into specific action sequences. To address these challenges,
we introduce the Plan with Language, Act with Parameter (PLAP) planning
framework that facilitates the grounding of LLM-based agents in long-horizon
environments. The PLAP method comprises three key components: (1) a skill
library containing environment-specific parameterized skills, (2) a skill
planner powered by LLMs, and (3) a skill executor converting the parameterized
skills into executable action sequences. We implement PLAP in MicroRTS, a
long-horizon real-time strategy game that provides an unfamiliar and
challenging environment for LLMs. The experimental results demonstrate the
effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting
outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully
crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.
Additionally, we design comprehensive evaluation metrics and test 6
closed-source and 2 open-source LLMs within the PLAP framework, ultimately
releasing an LLM leaderboard ranking long-horizon skill planning ability. Our
code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [144] [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)
*Jinxin Li,Gang Tu,ShengYu Cheng,Junjie Hu,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: 本文提出了一种基于隐藏信号分析（HSAD）的LLM幻觉检测新框架，利用隐藏层动态和频域分析，有效提升了检测准确性，较现有方法提升超10个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有大模型幻觉检测方法受限于外部知识覆盖或静态隐藏态分析，导致效果与鲁棒性有限。为解决此问题，研究者尝试捕捉生成过程中的推理动态，以提升幻觉检测效能。

Method: 提出HSAD方法，在自回归生成中采样各层激活，采用快速傅里叶变换（FFT）获得频域特征，提取最强非直流分量作为频谱特征，并结合推理过程，寻找最佳检测时机。

Result: 在包括TruthfulQA等多个基准测试上，HSAD检测方法比现有最佳方法提升超10个百分点。

Conclusion: 通过将推理过程建模与频域分析相结合，HSAD为大模型幻觉检测开辟了更稳健的新范式。

Abstract: Hallucination remains a critical barrier for deploying large language models
(LLMs) in reliability-sensitive applications. Existing detection methods
largely fall into two categories: factuality checking, which is fundamentally
constrained by external knowledge coverage, and static hidden-state analysis,
that fails to capture deviations in reasoning dynamics. As a result, their
effectiveness and robustness remain limited. We propose HSAD (Hidden Signal
Analysis-based Detection), a novel hallucination detection framework that
models the temporal dynamics of hidden representations during autoregressive
generation. HSAD constructs hidden-layer signals by sampling activations across
layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain
representations, and extracts the strongest non-DC frequency component as
spectral features. Furthermore, by leveraging the autoregressive nature of
LLMs, HSAD identifies optimal observation points for effective and reliable
detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over
10 percentage points improvement compared to prior state-of-the-art methods. By
integrating reasoning-process modeling with frequency-domain analysis, HSAD
establishes a new paradigm for robust hallucination detection in LLMs.

</details>


### [145] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
*Yongjian Tang,Doruk Tuncel,Christian Koerner,Thomas Runkler*

Main category: cs.CL

TL;DR: 本论文研究在大语言模型（LLMs）中，过多的提示样例（over-prompting）反而会导致性能下降，并提出结合选择方法优化提示的策略。


<details>
  <summary>Details</summary>
Motivation: 以往经验认为，为LLM提供更多相关few-shot（少量样例）能提升性能，但现实发现样例数过多可能出现负作用，特别是在需求分析等真实场景。

Method: 作者设计了一个提示框架，结合三种few-shot样例选择方法：随机采样、语义嵌入与TF-IDF向量，在包括GPT-4o等多种LLM上，通过逐步增加样例数量，在真实的软件需求分类数据集上测试其表现，并寻找最佳样例数量。

Result: 实验发现，过度加入领域相关样例会在某些模型中降低效果；采用TF-IDF选择+分层抽样，可以以更少样例数获得更好分类结果。最终性能在功能性和非功能性需求分类上，比当前最佳方法高1%。

Conclusion: 合理选择和控制few-shot样例数量及其选取方法，是提升LLM实际任务表现的关键。过多样例可能导致过度提示，影响结果，需优化策略以避免该问题。

Abstract: Over-prompting, a phenomenon where excessive examples in prompts lead to
diminished performance in Large Language Models (LLMs), challenges the
conventional wisdom about in-context few-shot learning. To investigate this
few-shot dilemma, we outline a prompting framework that leverages three
standard few-shot selection methods - random sampling, semantic embedding, and
TF-IDF vectors - and evaluate these methods across multiple LLMs, including
GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.
Our experimental results reveal that incorporating excessive domain-specific
examples into prompts can paradoxically degrade performance in certain LLMs,
which contradicts the prior empirical conclusion that more relevant few-shot
examples universally benefit LLMs. Given the trend of LLM-assisted software
engineering and requirement analysis, we experiment with two real-world
software requirement classification datasets. By gradually increasing the
number of TF-IDF-selected and stratified few-shot examples, we identify their
optimal quantity for each LLM. This combined approach achieves superior
performance with fewer examples, avoiding the over-prompting problem, thus
surpassing the state-of-the-art by 1% in classifying functional and
non-functional requirements.

</details>


### [146] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: LLM在真实对话场景下推断人格特质的能力有限，目前与心理学真实测评的对齐程度较低。


<details>
  <summary>Details</summary>
Motivation: LLM正被广泛应用于情感支持、咨询等需理解人类个性的场景，但其在实际对话中对人格特质的理解能力尚未被系统评估。此前多基于社交媒体及离散标签，缺乏与连续、真实人格评估数据的对比。

Method: 研究者构建了一个新数据集，包含半结构化访谈转录和连续“大五人格”分数。采用GPT-4.1 Mini的zero-shot和思维链提示、针对RoBERTa和Meta-LLaMA的LoRA微调，以及基于BERT和OpenAI静态嵌入的回归三类范式系统评测。

Result: 所有模型预测与人格真值的皮尔逊相关系数均低于0.26，思维链提示相对zero-shot提升极为有限，表明当前LLM在人格特质推断方面尚能力不足。

Conclusion: 当前LLM与心理学真实人格维度对齐性较差，凸显了LLM在处理复杂人类属性时的挑战。未来研究需关注特征化提示、上下文建模及具备对齐意识的微调。

Abstract: Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.

</details>


### [147] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
*Ali Salamatian,Amirhossein Abaskohi,Wan-Cyuan Fan,Mir Rayat Imtiaz Hossain,Leonid Sigal,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 该论文提出了一种基于人类眼动数据的图表问答(LVLM)改进方法，显著提高了模型的准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLM）在图表问答任务中有所进步，但模型常常关注于图表中无关区域，影响了其推理能力与可解释性，因此需要新的方法来提升模型表现。

Method: 作者构建了名为ChartGaze的新型眼动追踪数据集，记录人在图表推理过程中的注视区域。研究通过对比人类与模型的关注分布，量化两者差异，并提出了一种结合人类注视点来优化LVLM注意力机制的方法。

Result: 通过引入基于眼动数据的注意力优化方法，多个LVLM模型在准确率和注意力对齐性指标上最多获得2.56个百分点的提升。

Conclusion: 引入人类注视数据对准视觉—文本注意力，不仅提升了模型的答题精度，还增强了其决策过程的可解释性，为图表类视觉语言模型的进一步发展提供了新的思路。

Abstract: Charts are a crucial visual medium for communicating and representing
information. While Large Vision-Language Models (LVLMs) have made progress on
chart question answering (CQA), the task remains challenging, particularly when
models attend to irrelevant regions of the chart. In this work, we present
ChartGaze, a new eye-tracking dataset that captures human gaze patterns during
chart reasoning tasks. Through a systematic comparison of human and model
attention, we find that LVLMs often diverge from human gaze, leading to reduced
interpretability and accuracy. To address this, we propose a gaze-guided
attention refinement that aligns image-text attention with human fixations. Our
approach improves both answer accuracy and attention alignment, yielding gains
of up to 2.56 percentage points across multiple models. These results
demonstrate the promise of incorporating human gaze to enhance both the
reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [148] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)
*Zile Qiao,Guoxin Chen,Xuanzhong Chen,Donglei Yu,Wenbiao Yin,Xinyu Wang,Zhen Zhang,Baixuan Li,Huifeng Yin,Kuan Li,Rui Min,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出了WebResearcher系统，通过创新的框架与数据合成引擎，提升AI自主研究与知识整合能力，在六项基准测试上表现优异，超越了现有主流系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主深度研究能力的提升，现有系统存在单一上下文导致的噪声污染和上下文容量受限问题，亟需一种能够系统性地组织与总结外部知识的新方法。

Method: 作者提出将深度研究过程建模为马尔可夫决策过程（MDP），并设计了两大模块：1）WebResearcher模块，通过迭代式归纳与多任务空间保持，有效管理与整合知识；2）WebFrontier模块，通过工具增强的数据合成，实现高质量复杂研究任务数据的自动生成，以支持系统训练。

Result: 该系统生成的数据对传统单一上下文AI工具使用能力有显著提升作用，且借助多智能体并行思考，在六项有挑战性的基准测试上均取得了领先甚至超越闭源系统的成绩。

Conclusion: WebResearcher框架能显著提升AI在自主深度研究中的表现，克服了噪声和上下文限制的问题，在高质量数据支持下具有优越的通用性与扩展性，对AI自主知识构建具有推动作用。

Abstract: Recent advances in deep-research systems have demonstrated the potential for
AI agents to autonomously discover and synthesize knowledge from external
sources. In this paper, we introduce WebResearcher, a novel framework for
building such agents through two key components: (1) WebResearcher, an
iterative deep-research paradigm that reformulates deep research as a Markov
Decision Process, where agents periodically consolidate findings into evolving
reports while maintaining focused workspaces, overcoming the context
suffocation and noise contamination that plague existing mono-contextual
approaches; and (2) WebFrontier, a scalable data synthesis engine that
generates high-quality training data through tool-augmented complexity
escalation, enabling systematic creation of research tasks that bridge the gap
between passive knowledge recall and active knowledge construction. Notably, we
find that the training data from our paradigm significantly enhances tool-use
capabilities even for traditional mono-contextual methods. Furthermore, our
paradigm naturally scales through parallel thinking, enabling concurrent
multi-agent exploration for more comprehensive conclusions. Extensive
experiments across 6 challenging benchmarks demonstrate that WebResearcher
achieves state-of-the-art performance, even surpassing frontier proprietary
systems.

</details>


### [149] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
*Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为Agentic Continual Pre-training（Agentic CPT）的新方法，用于训练具备代理能力的大语言模型，并开发了AgentFounder模型。实验结果在多个基准测试上取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于通用基础模型的后训练方法在代理任务中表现不佳，尤其是在开源实现中效果更差。作者认为原因在于缺乏强有力的具备代理能力的基础模型，因此在后期训练阶段模型需要同时学习多种代理行为和对齐专家示范，造成了优化上的根本性矛盾。

Method: 作者首次提出在深度研究代理系统训练流程中引入Agentic Continual Pre-training（CPT），目的是构建具备代理能力的基础模型。基于该方法，开发了名为AgentFounder的深度研究代理模型。

Result: AgentFounder-30B模型在10个基准测试上进行了评估，取得了业界领先的性能。同时模型保持了出色的工具使用能力，在BrowseComp-en取得了39.9%、BrowseComp-zh取得了43.3%、HLE（Pass@1）取得了31.5%的成绩。

Conclusion: Agentic CPT有效提升了模型的代理任务能力，AgentFounder模型在多项基准测试中表现优异，验证了该方法的有效性。

Abstract: Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating Agentic Continual Pre-training (Agentic CPT) into the
deep research agents training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [150] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
*Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架，自动构建多样化模拟环境以提升大型语言模型的函数调用智能，并通过两阶段微调方法大幅提升了代理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在实际应用中需要具备强大的代理智能，尤其是在多样的真实世界API调用场景中，对精确、鲁棒的函数调用能力有更高要求。作者认为，要提升函数调用能力，必须让模型在多样环境中进行训练。然而环境扩展和如何有效训练代理能力，仍是关键挑战。

Method: 作者设计了一个可扩展的框架，该框架能够自动化地构建高度异质、完全模拟的训练环境，从而系统性地扩展函数调用场景的广度。此外，作者引用了两阶段微调策略：先培养基础的代理能力，再针对具体领域进行专门化训练。

Result: 在多个代理智能基准测试（如 tau-bench、tau2-Bench 和 ACEBench）上，作者提出的AgentScaler模型表现优异，显著提升了模型的函数调用能力。

Conclusion: 通过环境扩展和分阶段训练的方法，可以有效增强大模型的函数调用能力，为其实际落地提供了有力支持。AgentScaler能够大幅提升代理智能，是推进大模型实用化的重要一步。

Abstract: Advanced agentic intelligence is a prerequisite for deploying Large Language
Models in practical, real-world applications. Diverse real-world APIs demand
precise, robust function-calling intelligence, which needs agents to develop
these capabilities through interaction in varied environments. The breadth of
function-calling competence is closely tied to the diversity of environments in
which agents are trained. In this work, we scale up environments as a step
towards advancing general agentic intelligence. This gives rise to two central
challenges: (i) how to scale environments in a principled manner, and (ii) how
to effectively train agentic capabilities from experiences derived through
interactions with these environments. To address these, we design a scalable
framework that automatically constructs heterogeneous environments that are
fully simulated, systematically broadening the space of function-calling
scenarios. We further adapt a two-phase agent fine-tuning strategy: first
endowing agents with fundamental agentic capabilities, then specializing them
for domain-specific contexts. Extensive experiments on agentic benchmarks,
tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,
AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [151] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
*Zijian Li,Xin Guan,Bo Zhang,Shen Huang,Houquan Zhou,Shaopeng Lai,Ming Yan,Yong Jiang,Pengjun Xie,Fei Huang,Jun Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出WebWeaver框架，利用双智能体动态协作，有效提升AI在开放型深入研究任务中的报告写作质量与可信度。


<details>
  <summary>Details</summary>
Motivation: 面对AI在开放型深入研究（OEDR）任务中难以有效整合海量网络信息、同时现有方案在规划与证据获取脱节、文本生成易丢失关键信息且易产生幻觉等问题，亟需新方法提升报告的可靠性和结构化程度。

Method: 提出WebWeaver，采用双智能体合作：规划者智能体动态循环地交替进行证据获取和大纲优化，生成与内存证据库紧密关联的大纲；写作者智能体基于大纲，分段分层检索必要证据，再分别撰写各部分内容，避免长文本上下文损失和幻觉。

Result: WebWeaver在DeepResearch Bench、DeepConsult、DeepResearchGym等多个OEDR主流基准上实现了新SOTA表现。实验证明了该方法在报告的质量、可信性、结构化等方面优于现有方法。

Conclusion: 人类式、迭代式的自适应规划与针对性证据整合是高质量AI报告生成的关键。WebWeaver方法可大幅提升复杂研究任务的分析和写作效果，有望成为OEDR最佳实践。

Abstract: This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from evidence acquisition and one-shot
generation paradigms that easily suffer from long-context failure issues like
"loss in the middle" and hallucinations. To address these challenges, we
introduce WebWeaver, a novel dual-agent framework that emulates the human
research process. The planner operates in a dynamic cycle, iteratively
interleaving evidence acquisition with outline optimization to produce a
comprehensive, source-grounded outline linking to a memory bank of evidence.
The writer then executes a hierarchical retrieval and writing process,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the memory bank for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and
DeepResearchGym. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.

</details>


### [152] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
*Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了ReSum范式，通过周期性上下文总结，突破了大型语言模型Web智能体在处理复杂任务时因上下文窗口受限所带来的瓶颈，提高了推理效果。进一步提出的ReSum-GRPO训练方式进一步强化了智能体在概要条件下的推理能力，实验结果在多项基准任务上优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的Web智能体（如ReAct）在处理复杂涉及多个实体和复杂关系的问题时，由于交互历史急剧膨胀，很快就会消耗完有限的上下文窗口，导致模型无法获得完整推理过程和最优解，需要新的方法以突破上下文窗口限制。

Method: 提出了ReSum范式，在智能体推理过程中周期性地将扩展的交互历史摘要为紧凑的推理状态，从而节省上下文空间并维持对历史发现的记忆。针对该范式，设计了ReSum-GRPO训练方法，引入分段轨迹训练和优势传播机制，帮助模型适应基于摘要的推理方式。

Result: 大量实验显示，采用ReSum范式的智能体在三个Web任务基准数据集上平均提升4.5%，在应用ReSum-GRPO训练后可进一步提升至8.2%。仅用1000条训练样本，提出的WebResummer-30B模型在BrowseComp-zh、BrowseComp-en上分别达到33.3%和18.3% Pass@1的成绩，超过现有开源Web智能体。

Conclusion: 周期性上下文总结（ReSum范式）有效突破了上下文窗口瓶颈，显著提升大型语言模型Web智能体在复杂知识类任务中的表现。配合ReSum-GRPO训练策略，可以用较小数据量训练出强竞争力的智能体，为通用Web智能体的研究和应用带来新思路。

Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce ReSum, a novel paradigm that enables indefinite
exploration through periodic context summarization. ReSum converts growing
interaction histories into compact reasoning states, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and
advantage broadcasting to familiarize agents with summary-conditioned
reasoning. Extensive experiments on web agents of varying scales across three
benchmarks demonstrate that ReSum delivers an average absolute improvement of
4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO
training. Notably, with only 1K training samples, our WebResummer-30B (a
ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on
BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web
agents.

</details>


### [153] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 当前主流的LLM激活表述方法，实际可能更多反映了表述模型本身的知识，而非目标模型的内部工作机制。现有数据集难以区分这一点，因此需要更有针对性的基准和实验设计。


<details>
  <summary>Details</summary>
Motivation: LLM的可解释性研究希望通过把模型内部表示翻译为自然语言，来理解模型内部机制。然而，尚不清楚这些方法到底解释了模型内部，还是仅仅复述了输入信息。

Method: 评估了现有表述方法在前人使用的数据集上的表现，发现无需访问模型内部激活信息也能取得高分。随后设计对照实验，分析表述内容究竟来源于目标模型激活还是表述模型自身的知识。

Result: 实验发现：许多自然语言表述更多反映了表述LLM本身的参数知识，而不是目标模型实际的激活信息。

Conclusion: 现有基准不能有效评估激活表述方法是否真的揭示了目标模型内部机制。后续研究需开发更严谨的基准和对照实验，确保相关解释方法的有效性。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [154] [An integrated process for design and control of lunar robotics using AI and simulation](https://arxiv.org/abs/2509.12367)
*Daniel Lindmark,Jonas Andersson,Kenneth Bodin,Tora Bodin,Hugo Börjesson,Fredrik Nordfeldth,Martin Servin*

Main category: cs.RO

TL;DR: 该论文提出了一种开发月球施工设备的集成流程，将物理设计与控制系统并行探索。作者基于OpenPLX框架，将CAD模型与自主系统及高保真多体动力学仿真无缝连接，最后通过两个案例（包含结合视觉语言模型和强化学习导航月球车）展示了其实用性。


<details>
  <summary>Details</summary>
Motivation: 开发月球施工设备面临设计与控制高度耦合的挑战，现有的分离式流程难以满足复杂环境需求。作者希望通过并行物理设计与控制探索，提高开发效率与设备性能。

Method: 作者提出了一个基于OpenPLX的技术框架，通过可读写的声明式语言，将CAD模型、自动系统、高保真3D仿真、力接触和非理想传感器数据整合，支持一体化仿真和验证。

Result: 该框架被用于两个案例研究，特别是演示了一个可自主导航并结合视觉语言模型与强化学习控制策略的月球车，验证了系统的功能和灵活性。

Conclusion: 本文提出的集成流程与OpenPLX框架能够有效服务于月球建设设备的并行设计与控制开发，并通过实例验证了其实用性和前景。

Abstract: We envision an integrated process for developing lunar construction
equipment, where physical design and control are explored in parallel. In this
paper, we describe a technical framework that supports this process. It relies
on OpenPLX, a readable/writable declarative language that links CAD-models and
autonomous systems to high-fidelity, real-time 3D simulations of contacting
multibody dynamics, machine regolith interaction forces, and non-ideal sensors.
To demonstrate its capabilities, we present two case studies, including an
autonomous lunar rover that combines a vision-language model for navigation
with a reinforcement learning-based control policy for locomotion.

</details>


### [155] [Geometric Red-Teaming for Robotic Manipulation](https://arxiv.org/abs/2509.12379)
*Divyam Goel,Yufei Wang,Tiancheng Wu,Guixiu Qiao,Pavel Piliptchak,David Held,Zackory Erickson*

Main category: cs.RO

TL;DR: 该论文提出Geometric Red-Teaming (GRT) 框架，用于通过物体几何扰动系统性测试机器人操作策略的鲁棒性，并展示对策略失效模式的发现与改进。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略主要在有限、人工设定的测试集上评测，难以揭示实际环境变化下存在的脆弱性。作者希望建立一种更有针对性的评估机制，揭示和改进策略在真实复杂场景下的表现。

Method: GRT结合了基于Jacobian的物体网格变形模型和模拟器中的无梯度优化方法，自动生成CrashShapes（满足结构约束但极大降低策略性能的网格形变），并系统性地探索形变空间。结合任务级评估和约束条件，实现通用性的鲁棒性评估流程。此外，用CrashShapes微调策略来增强其稳健性（blue-teaming）。

Result: GRT能在插入、关节操作、抓取等任务中找到导致策略性能崩溃的形变，这些脆弱模式是静态基线测试难以发现的。对CrashShapes进行针对性微调后（blue-teaming），在这些形变上的任务成功率最高能提升60个百分点，同时原始物体上的性能未受影响。真实机器人实验验证CrashShapes可将成功率降至22.5%，blue-teaming后能恢复至90%。

Conclusion: GRT为机器人操作策略的结构化鲁棒性评估和改进提供了通用框架。由CrashShapes发现的策略脆弱性可通过有针对性的训练有效弥补。因此，几何red-teaming和对应的blue-teaming过程对提高实际系统的健壮性极为有效。

Abstract: Standard evaluation protocols in robotic manipulation typically assess policy
performance over curated, in-distribution test sets, offering limited insight
into how systems fail under plausible variation. We introduce Geometric
Red-Teaming (GRT), a red-teaming framework that probes robustness through
object-centric geometric perturbations, automatically generating CrashShapes --
structurally valid, user-constrained mesh deformations that trigger
catastrophic failures in pre-trained manipulation policies. The method
integrates a Jacobian field-based deformation model with a gradient-free,
simulator-in-the-loop optimization strategy. Across insertion, articulation,
and grasping tasks, GRT consistently discovers deformations that collapse
policy performance, revealing brittle failure modes missed by static
benchmarks. By combining task-level policy rollouts with constraint-aware shape
exploration, we aim to build a general purpose framework for structured,
object-centric robustness evaluation in robotic manipulation. We additionally
show that fine-tuning on individual CrashShapes, a process we refer to as
blue-teaming, improves task success by up to 60 percentage points on those
shapes, while preserving performance on the original object, demonstrating the
utility of red-teamed geometries for targeted policy refinement. Finally, we
validate both red-teaming and blue-teaming results with a real robotic arm,
observing that simulated CrashShapes reduce task success from 90% to as low as
22.5%, and that blue-teaming recovers performance to up to 90% on the
corresponding real-world geometry -- closely matching simulation outcomes.
Videos and code can be found on our project website:
https://georedteam.github.io/ .

</details>


### [156] [Distributed Event-Triggered Distance-Based Formation Control for Multi-Agent Systems](https://arxiv.org/abs/2509.12390)
*Evangelos Psomiadis,Panagiotis Tsiotras*

Main category: cs.RO

TL;DR: 本文提出了一种适用于多智能体系统的分布式事件触发编队控制器，实现高效节能地完成编队任务。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在编队时由于资源有限（如电量、带宽），频繁的控制更新会造成资源浪费，需要有效减少控制开销。

Method: 提出基于智能体间距离测量的分布式事件触发控制方法，仅当测量误差超过设定阈值时才触发控制更新，保证系统稳定性，并通过仿真与实际实验进行验证。

Result: 实验和仿真显示，该事件触发方法相比于传统的周期触发方法，能显著减少控制操作的数量，并保持一定的编队性能。

Conclusion: 该方法能有效降低多智能体系统的控制资源消耗，同时保障编队任务的准确性和系统的可扩展性。

Abstract: This paper addresses the problem of collaborative formation control for
multi-agent systems with limited resources. We consider a team of robots tasked
with achieving a desired formation from arbitrary initial configurations. To
reduce unnecessary control updates and conserve resources, we propose a
distributed event-triggered formation controller that relies on inter-agent
distance measurements. Control updates are triggered only when the measurement
error exceeds a predefined threshold, ensuring system stability. The proposed
controller is validated through extensive simulations and real-world
experiments involving different formations, communication topologies,
scalability tests, and variations in design parameters, while also being
compared against periodic triggering strategies. Results demonstrate that the
event-triggered approach significantly reduces control efforts while preserving
formation performance.

</details>


### [157] [MinJointTracker: Real-time inertial kinematic chain tracking with joint position estimation and minimal state size](https://arxiv.org/abs/2509.12398)
*Michael Lorenz,Bertram Taetz,Gabriele Bleser-Taetz,Didier Stricker*

Main category: cs.RO

TL;DR: 本论文提出了一种无需校准的惯性动作捕捉新方法，能够实时追踪运动链中的关节位置和IMU（惯性测量单元）姿态，减少了繁琐的前期准备工作。


<details>
  <summary>Details</summary>
Motivation: 当前惯性动作捕捉系统普遍存在设置烦琐、需离线校准多个参数（如IMU与身体段的相对姿态、关节位置等）的问题，导致实际使用不便。解决这些问题有助于推动惯性动作捕捉在实验室以外的广泛应用。

Method: 本文提出了一种基于递归贝叶斯估计的在线跟踪算法，无需专门校准，可以实时估计IMU的全局姿态（含绝对与相对方向）和关节在IMU坐标系下的位置。这一方法保持了较小的状态空间，提高了计算效率。

Result: 在三连杆机械臂（模拟数据）和人体行走（再模拟数据）实验中，该方法能够提供无漂移的相对和绝对姿态估计，且仅需一个IMU获得全局方向参考。同时，不同运动场景下关节位置估计准确且收敛速度快。

Conclusion: 提出的校准自由惯性动作捕捉算法减少了使用难度，提升了实用性，在不同运动情形下均展示出高精度、快速收敛和强健性，推动了惯性动作捕捉技术的实际应用。

Abstract: Inertial motion capture is a promising approach for capturing motion outside
the laboratory. However, as one major drawback, most of the current methods
require different quantities to be calibrated or computed offline as part of
the setup process, such as segment lengths, relative orientations between
inertial measurement units (IMUs) and segment coordinate frames (IMU-to-segment
calibrations) or the joint positions in the IMU frames. This renders the setup
process inconvenient. This work contributes to real-time capable
calibration-free inertial tracking of a kinematic chain, i.e. simultaneous
recursive Bayesian estimation of global IMU angular kinematics and joint
positions in the IMU frames, with a minimal state size. Experimental results on
simulated IMU data from a three-link kinematic chain (manipulator study) as
well as re-simulated IMU data from healthy humans walking (lower body study)
show that the calibration-free and lightweight algorithm provides not only
drift-free relative but also drift-free absolute orientation estimates with a
global heading reference for only one IMU as well as robust and fast
convergence of joint position estimates in the different movement scenarios.

</details>


### [158] [Computing forward statics from tendon-length in flexible-joint hyper-redundant manipulators](https://arxiv.org/abs/2509.12444)
*Weiting Feng,Kyle L. Walker,Yunjie Yang,Francesco Giorgio-Serchi*

Main category: cs.RO

TL;DR: 本文针对超冗余腱驱机器人在静态条件下的控制问题，提出了一种同时可用腱长度或腱张力作为输入的新型前向静力解法，并通过实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 常规以腱长度为输入的控制方式在考虑重力和弹性关节的大型超冗余机械臂情况下不再适用，因为需要同时解决静力力学问题，而依赖于腱张力或系统状态的测量往往并不精确或难以获得。

Method: 作者提出了一种基于螺旋理论的建模方法，对含弹性关节的多段超冗余腱驱机器人进行前向静力建模与求解。该方法允许系统在腱长度或腱张力作为输入时均可迭代求解姿态，并进行了相关实验验证。

Result: 实验分别以腱张力和腱长度作为输入，均可有效解出机械臂在静态下的姿态，且用腱长度作为开环控制输入可避开张力测量和状态估计的实际难题。

Conclusion: 新提出的螺旋理论静力解法拓展了超冗余腱驱机械臂在实际应用中的控制选择，可在静态任务下仅凭腱长度输入实现可靠控制，为相关系统设计与应用带来便利。

Abstract: Hyper-redundant tendon-driven manipulators of- fer greater flexibility and
compliance over traditional manipu- lators. A common way of controlling such
manipulators relies on adjusting tendon lengths, which is an accessible control
parameter. This approach works well when the kinematic configuration is
representative of the real operational con- ditions. However, when dealing with
manipulators of larger size subject to gravity, it becomes necessary to solve a
static force problem, using tendon force as the input and employing a mapping
from the configuration space to retrieve tendon length. Alternatively,
measurements of the manipulator posture can be used to iteratively adjust
tendon lengths to achieve a desired posture. Hence, either tension measurement
or state estimation of the manipulator are required, both of which are not
always accurately available. Here, we propose a solution by reconciling cables
tension and length as the input for the solution of the system forward statics.
We develop a screw-based formulation for a tendon-driven, multi-segment,
hyper-redundant manipulator with elastic joints and introduce a forward statics
iterative solution method that equivalently makes use of either tendon length
or tension as the input. This strategy is experimentally validated using a
traditional tension input first, subsequently showing the efficacy of the
method when exclusively tendon lengths are used. The results confirm the
possibility to perform open-loop control in static conditions using a kinematic
input only, thus bypassing some of the practical problems with tension
measurement and state estimation of hyper-redundant systems.

</details>


### [159] [Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.12458)
*Àlmos Veres-Vitàlyos,Genis Castillo Gomez-Raya,Filip Lemic,Daniel Johannes Bugelnig,Bernhard Rinner,Sergi Abadal,Xavier Costa-Pérez*

Main category: cs.RO

TL;DR: 本文提出了一种针对超轻型无人机的全自动高保真3D重建系统，通过实时与非实时双重重建流程，实现小型无人机对静态物体的高质量三维扫描。


<details>
  <summary>Details</summary>
Motivation: 小型无人机在室内或狭小空间探索有巨大潜力，但受限于载重和自主性，难以胜任高质量3D重建等复杂任务。

Method: 设计了一种新系统架构，结合结构光重建和NeRF神经辐射场。利用实时SfM算法生成点云，根据建模质量实时调整无人机飞行轨迹以优化数据采集，并最终用融合UWB定位的NeRF法实现高精度3D重建。系统在Crazyflie 2.1无人机（单机和多机）上实现并验证。

Result: 动态轨迹自适应远优于静态飞行路径，能够提升3D重建的质量。实验证明该系统可在受限环境下使用微型无人机完成细致3D重建。

Conclusion: 该方法实现了小型无人机自动高质量3D重建，拓展了微型无人机在复杂环境下的应用，是对以往需要大型平台能力的有力替代。

Abstract: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for
navigating indoor and hard-to-reach areas, yet their significant constraints in
payload and autonomy have largely prevented their use for complex tasks like
high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we
introduce a novel system architecture that enables fully autonomous,
high-fidelity 3D scanning of static objects using UAVs weighing under 100
grams. Our core innovation lies in a dual-reconstruction pipeline that creates
a real-time feedback loop between data capture and flight control. A
near-real-time (near-RT) process uses Structure from Motion (SfM) to generate
an instantaneous pointcloud of the object. The system analyzes the model
quality on the fly and dynamically adapts the UAV's trajectory to intelligently
capture new images of poorly covered areas. This ensures comprehensive data
acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline
employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)
approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)
location data to achieve superior accuracy. We implemented and validated this
architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both
single- and multi-UAV configurations, conclusively show that dynamic trajectory
adaptation consistently improves reconstruction quality over static flight
paths. This work demonstrates a scalable and autonomous solution that unlocks
the potential of miniaturized UAVs for fine-grained 3D reconstruction in
constrained environments, a capability previously limited to much larger
platforms.

</details>


### [160] [Bio-inspired tail oscillation enables robot fast crawling on deformable granular terrains](https://arxiv.org/abs/2509.12468)
*Shipeng Liu,Meghana Sagare,Shubham Patil,Feifei Qian*

Main category: cs.RO

TL;DR: 本文通过仿生泥鳅机器人，研究了尾部结构与控制对提升机器人在松散地形（如沙地、泥地）运动性能的影响。主动摆动尾部显著提高了移动速度并减少了阻力。论文提出了一种基于地形和尾部形态选择最优尾部动作的设计原则。


<details>
  <summary>Details</summary>
Motivation: 地面机器人在沙地、泥地等可变形地形上运动时，受限于复杂的地形与机器人相互作用，运动效率不高。泥鳅等两栖动物能通过调整尾部形态和动作有效适应这种环境，启发研究机器人如何通过尾部设计与控制提升在松散基底上的运动性能。

Method: 设计并制造了一款仿生泥鳅机器人，比较了静止尾部与主动摆动尾部两种运动配置下机器人的运动表现，并通过剪切力测试分析尾部动作与基底相互作用机理。还系统考察了不同尾部形态对运动效果和最佳摆动策略的影响。

Result: 主动摆动尾部使机器人速度提高了67%，身体阻力降低了46%。摆动尾部能够流化基底、降低剪切阻力。尾部表面积较大的设计在摆动过程中更有效降低了阻力，通过限制插入深度进一步优化了运动表现。

Conclusion: 尾部的主动动作与适应地形的形态可以大幅提升机器人在可变形地形上的移动能力。提出的基于地基强度与尾部形态选择尾部动作的设计原则为农业机器人、搜救及环境探索等领域的移动机器人设计提供了新思路。

Abstract: Deformable substrates such as sand and mud present significant challenges for
terrestrial robots due to complex robot-terrain interactions. Inspired by
mudskippers, amphibious animals that naturally adjust their tail morphology and
movement jointly to navigate such environments, we investigate how tail design
and control can jointly enhance flipper-driven locomotion on granular media.
Using a bio-inspired robot modeled after the mudskipper, we experimentally
compared locomotion performance between idle and actively oscillating tail
configurations. Tail oscillation increased robot speed by 67% and reduced body
drag by 46%. Shear force measurements revealed that this improvement was
enabled by tail oscillation fluidizing the substrate, thereby reducing
resistance. Additionally, tail morphology strongly influenced the oscillation
strategy: designs with larger horizontal surface areas leveraged the
oscillation-reduced shear resistance more effectively by limiting insertion
depth. Based on these findings, we present a design principle to inform tail
action selection based on substrate strength and tail morphology. Our results
offer new insights into tail design and control for improving robot locomotion
on deformable substrates, with implications for agricultural robotics, search
and rescue, and environmental exploration.

</details>


### [161] [Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents](https://arxiv.org/abs/2509.12507)
*Anna Deichler,Siyang Wang,Simon Alexanderson,Jonas Beskow*

Main category: cs.RO

TL;DR: 本文提出了一种结合模仿学习和强化学习的方法，使机器人能够生成自然且高效的指向性手势。实验表明该方法优于现有监督学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前机器人与智能体研究致力于实现与人的自然交互，尤其是在物理环境中。然而，大多数关注都放在了语言和语音等言语方式上，忽视了非言语交流的重要性，如手势。

Method: 作者提出了一个新框架，结合小规模动作捕捉数据，通过模仿学习和强化学习训练机器人运动控制策略，能够产生物理上合理、自然且具有较高指示性的手势。方法与监督学习和检索方法进行了对比评估，包括客观指标测试和虚拟现实中的人与人交互实验。

Result: 该方法在自然度和指示准确率上均超越了最先进的监督学习模型，无论是在客观指标还是虚拟现实人机互动实验中效果都更好。

Conclusion: 结合模仿-强化学习的方法在生成有交流性的手势上表现出显著优势，有助于推动智能体在现实应用中实现更自然的非言语交流。

Abstract: One of the main goals of robotics and intelligent agent research is to enable
natural communication with humans in physically situated settings. While recent
work has focused on verbal modes such as language and speech, non-verbal
communication is crucial for flexible interaction. We present a framework for
generating pointing gestures in embodied agents by combining imitation and
reinforcement learning. Using a small motion capture dataset, our method learns
a motor control policy that produces physically valid, naturalistic gestures
with high referential accuracy. We evaluate the approach against supervised
learning and retrieval baselines in both objective metrics and a virtual
reality referential game with human users. Results show that our system
achieves higher naturalness and accuracy than state-of-the-art supervised
models, highlighting the promise of imitation-RL for communicative gesture
generation and its potential application to robots.

</details>


### [162] [Zero to Autonomy in Real-Time: Online Adaptation of Dynamics in Unstructured Environments](https://arxiv.org/abs/2509.12516)
*William Ward,Sarah Etter,Jesse Quattrociocchi,Christian Ellis,Adam J. Thorpe,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出了一种结合函数编码器和递归最小二乘法的在线自适应方法，实现移动机器人在极短时间内适应突发地形变化并安全控制，大幅提升模型精度和实际导航效果。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在现实复杂环境中常面临突发地形变化（如冰面），如果不能实时自适应，其运动规划和控制会变得不稳定，进而造成碰撞等安全问题。因此，迫切需要一种能在极短时间内实现从零知识到高精度、安全控制的自适应方法。

Method: 将函数编码器与递归最小二乘法结合，将编码器系数视为可通过在线里程计数据流自适应更新的隐状态，无需传统的梯度更新。新方法能用几秒钟内的数据实时估算模型参数，实现常数时间的高效适应。

Result: 在Van der Pol系统（用于理论分析）、Unity高保真越野仿真器、以及真实Clearpath Jackal机器人（包括冰场极端场景）进行了实验。结果显示，该方法在所有测试场景下均有效提升模型精度和规划能力，显著减少碰撞次数，优于静态和元学习基线方法。

Conclusion: 所提方法能够快速适应环境动态变化，提升自主机器人安全性和任务执行力，适用性广泛，对实时导航与控制具有现实工程意义。

Abstract: Autonomous robots must go from zero prior knowledge to safe control within
seconds to operate in unstructured environments. Abrupt terrain changes, such
as a sudden transition to ice, create dynamics shifts that can destabilize
planners unless the model adapts in real-time. We present a method for online
adaptation that combines function encoders with recursive least squares,
treating the function encoder coefficients as latent states updated from
streaming odometry. This yields constant-time coefficient estimation without
gradient-based inner-loop updates, enabling adaptation from only a few seconds
of data. We evaluate our approach on a Van der Pol system to highlight
algorithmic behavior, in a Unity simulator for high-fidelity off-road
navigation, and on a Clearpath Jackal robot, including on a challenging terrain
at a local ice rink. Across these settings, our method improves model accuracy
and downstream planning, reducing collisions compared to static and
meta-learning baselines.

</details>


### [163] [Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning](https://arxiv.org/abs/2509.12531)
*Scott Jones,Liyou Zhou,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 本论文探讨了预训练视觉模型（PVM）在基于模型的强化学习（MBRL）中提升机器人控制策略视觉泛化能力的表现，并发现PVM在强视觉域转移下表现优异，且部分微调可实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 以往端到端联合训练策略和视觉编码器的一般化能力较弱，尤其在视觉场景变换下。已有研究表明，PVM在无模型强化学习中有助于提升鲁棒性，但其在采样效率更优的MBRL中的作用尚不明确。本文旨在澄清PVM在MBRL中的有效性，尤其是在处理视觉域转移方面。

Method: 作者在不同视觉域转移场景下，通过与从零训练的模型对比，考察了用PVM初始化视觉编码器的策略网络表现。同时，系统性地研究了PVM不同层级的部分微调对最终任务性能和泛化性的影响。

Result: 实验显示，在严酷的视觉域转移条件下，PVM初始化的模型远优于从头训练模型。尤其是部分微调PVM时，在极端分布转移下，整体任务表现达到最大化。

Conclusion: PVM在促进视觉策略学习的鲁棒性方面极具潜力，建议在基于模型的机器人学习中广泛采用PVM以提升泛化能力和鲁棒性。

Abstract: In visuomotor policy learning, the control policy for the robotic agent is
derived directly from visual inputs. The typical approach, where a policy and
vision encoder are trained jointly from scratch, generalizes poorly to novel
visual scene changes. Using pre-trained vision models (PVMs) to inform a policy
network improves robustness in model-free reinforcement learning (MFRL). Recent
developments in Model-based reinforcement learning (MBRL) suggest that MBRL is
more sample-efficient than MFRL. However, counterintuitively, existing work has
found PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness
in MBRL, specifically on generalization under visual domain shifts. We show
that, in scenarios with severe shifts, PVMs perform much better than a baseline
model trained from scratch. We further investigate the effects of varying
levels of fine-tuning of PVMs. Our results show that partial fine-tuning can
maintain the highest average task performance under the most extreme
distribution shifts. Our results demonstrate that PVMs are highly successful in
promoting robustness in visual policy learning, providing compelling evidence
for their wider adoption in model-based robotic learning applications.

</details>


### [164] [Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling](https://arxiv.org/abs/2509.12562)
*Zhefei Gong,Shangke Lyu,Pengxiang Ding,Wei Xiao,Donglin Wang*

Main category: cs.RO

TL;DR: 论文提出KORR框架，通过结合Koopman算子理论和残差策略学习，提高模仿学习在高精度和长时序任务的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习虽然可以高效学习技能，但在处理长时序和高精度控制任务时容易产生累计误差，且现有残差策略方法多只对局部状态进行修正，缺乏对全局状态演化的理解，限制了其鲁棒性和泛化能力。

Method: 借助Koopman算子理论，在学习到的潜在空间中引入线性时不变结构，实现对系统动态的全局建模，并用此来引导残差策略修正动作。提出KORR框架，让残差策略以Koopman预测的潜在状态为条件，进行全局知情且稳定的动作优化。

Result: 在细粒度、长时序的机器人家具装配任务及多个扰动条件下，KORR相比强基线方法，在性能、鲁棒性和泛化性上均有提升。

Conclusion: Koopman建模为现代学习方法和经典控制理论之间搭建了桥梁，通过结合残差策略和全局动态信息，能够显著提升长时序、高精度任务中的表现。

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory.

</details>


### [165] [The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning](https://arxiv.org/abs/2509.12594)
*Titong Jiang,Xuefeng Jiang,Yuan Ma,Xin Wen,Bailin Li,Kun Zhan,Peng Jia,Yahui Liu,Sheng Sun,Xianpeng Lang*

Main category: cs.RO

TL;DR: 本文提出了LightVLA，一种简单高效、可微分的视觉语言行动(VLA)模型视觉token剪枝框架。该方法在降低计算开销的同时还能提升任务执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在执行现实机器人任务时表现出色，但因依赖于大量视觉token进行注意力计算，导致在资源受限平台上部署困难，亟需方法优化token利用率以提升效率。

Method: LightVLA通过自适应和基于性能的方式剪枝视觉token，动态生成查询用以评估token重要性，使用Gumbel softmax实现可微分token选择。通过微调，模型学习保留关键token并剪除无用信息，且无须额外可训练参数或人为超参数设置。

Result: 在LIBERO基准的多项任务中，LightVLA优于不同VLA模型和现有token剪枝方法，成功率提升2.9%，FLOPs减少59.1%，延迟降低38.2%。增强版LightVLA*（加入可训练参数）也表现良好。

Conclusion: LightVLA证明了自适应剪枝视觉token可在兼顾效率与性能下服务VLA，更加适合资源有限的机器人系统，并首次将此类方法应用到VLA任务，是向高效、强大及实用实时机器人系统迈出的关键一步。

Abstract: We present LightVLA, a simple yet effective differentiable token pruning
framework for vision-language-action (VLA) models. While VLA models have shown
impressive capability in executing real-world robotic tasks, their deployment
on resource-constrained platforms is often bottlenecked by the heavy
attention-based computation over large sets of visual tokens. LightVLA
addresses this challenge through adaptive, performance-driven pruning of visual
tokens: It generates dynamic queries to evaluate visual token importance, and
adopts Gumbel softmax to enable differentiable token selection. Through
fine-tuning, LightVLA learns to preserve the most informative visual tokens
while pruning tokens which do not contribute to task execution, thereby
improving efficiency and performance simultaneously. Notably, LightVLA requires
no heuristic magic numbers and introduces no additional trainable parameters,
making it compatible with modern inference frameworks. Experimental results
demonstrate that LightVLA outperforms different VLA models and existing token
pruning methods across diverse tasks on the LIBERO benchmark, achieving higher
success rates with substantially reduced computational overhead. Specifically,
LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%
improvement in task success rate. Meanwhile, we also investigate the learnable
query-based token pruning method LightVLA* with additional trainable
parameters, which also achieves satisfactory performance. Our work reveals that
as VLA pursues optimal performance, LightVLA spontaneously learns to prune
tokens from a performance-driven perspective. To the best of our knowledge,
LightVLA is the first work to apply adaptive visual token pruning to VLA tasks
with the collateral goals of efficiency and performance, marking a significant
step toward more efficient, powerful and practical real-time robotic systems.

</details>


### [166] [ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation](https://arxiv.org/abs/2509.12618)
*Zekai Zhang,Weiye Zhu,Hewei Pan,Xiangchen Wang,Rongtao Xu,Xing Sun,Feng Zheng*

Main category: cs.RO

TL;DR: 本文提出了ActiveVLN方法，通过增强主动探索能力，改进了视觉与语言导航（VLN）任务中的强化学习训练方式，克服了现有方法在数据依赖和探索能力等方面的局限，实现了更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流的VLN方法依赖模仿学习（尤其是DAgger），需要大量专家数据和高昂的训练代价；强化学习虽然潜力巨大，但实际应用时探索能力有限，过度依赖专家轨迹用于奖励设计，导致探索多样性不足，影响导航路线发现。

Method: ActiveVLN采用两阶段框架：第一阶段利用少量专家轨迹进行模仿学习以初始化智能体；第二阶段智能体通过多轮强化学习（多次rollout），自主探索并收集多样轨迹，结合GRPO目标函数优化；引入动态早停机制，有效剪枝失败或异常轨迹，并进行多项工程优化提升效率。

Result: 实验结果表明，ActiveVLN在IL方法的基础上取得了最大性能提升，相较于DAgger和以往RL后训练方法均有显著优势；在模型规模更小的情况下也能达到与最新技术相当的效果。

Conclusion: ActiveVLN通过主动探索机制与高效的策略优化流程，提升了VLN任务中强化学习的性能与效率，减少了对专家演示的依赖，为后续研究和实际应用提供了更具可扩展性的方案。

Abstract: The Vision-and-Language Navigation (VLN) task requires an agent to follow
natural language instructions and navigate through complex environments.
Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and
often use DAgger for post-training to mitigate covariate shift. While
effective, these approaches incur substantial data collection and training
costs. Reinforcement learning (RL) offers a promising alternative. However,
prior VLN RL methods lack dynamic interaction with the environment and depend
on expert trajectories for reward shaping, rather than engaging in open-ended
active exploration. This restricts the agent's ability to discover diverse and
plausible navigation routes. To address these limitations, we propose
ActiveVLN, a VLN framework that explicitly enables active exploration through
multi-turn RL. In the first stage, a small fraction of expert trajectories is
used for IL to bootstrap the agent. In the second stage, the agent iteratively
predicts and executes actions, automatically collects diverse trajectories, and
optimizes multiple rollouts via the GRPO objective. To further improve RL
efficiency, we introduce a dynamic early-stopping strategy to prune long-tail
or likely failed trajectories, along with additional engineering optimizations.
Experiments show that ActiveVLN achieves the largest performance gains over IL
baselines compared to both DAgger-based and prior RL-based post-training
methods, while reaching competitive performance with state-of-the-art
approaches despite using a smaller model. Code and data will be released soon.

</details>


### [167] [PerchMobi^3: A Multi-Modal Robot with Power-Reuse Quad-Fan Mechanism for Air-Ground-Wall Locomotion](https://arxiv.org/abs/2509.12620)
*Yikai Chen,Zhi Zheng,Jin Wang,Bingye He,Xiangyu Xu,Jialu Zhang,Huan Yu,Guodong Lu*

Main category: cs.RO

TL;DR: PerchMobi^3是一种集成飞行、地面行驶和壁面攀爬的新型多模态机器人，无需额外吸附装置，设计轻巧高效，实验表现出良好的多场景适应性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态机器人在融合飞行、行驶和攀爬时，通常依赖专门的吸附执行器，导致结构复杂、效率降低和可靠性受损，因此需要更加简单高效的集成方式。

Method: 提出了基于四风扇的PerchMobi^3平台，利用风扇实现飞行推进力和负压吸附的动力复用，并配合主动驱动轮集成，实现无需独立吸附泵的轻量化设计，同时建立了建模与控制框架，实现跨地面、壁面和空中无缝切换。

Result: 通过大量实验验证了PerchMobi^3在地面行驶、负载壁面攀爬、空中飞行及多模式切换中的有效性和适应性。

Conclusion: PerchMobi^3展示了一种新颖且高效的多模态机器人设计理念，为未来自主化和应用化部署提供了基础和方向。

Abstract: Achieving seamless integration of aerial flight, ground driving, and wall
climbing within a single robotic platform remains a major challenge, as
existing designs often rely on additional adhesion actuators that increase
complexity, reduce efficiency, and compromise reliability. To address these
limitations, we present PerchMobi^3, a quad-fan, negative-pressure,
air-ground-wall robot that implements a propulsion-adhesion power-reuse
mechanism. By repurposing four ducted fans to simultaneously provide aerial
thrust and negative-pressure adhesion, and integrating them with four actively
driven wheels, PerchMobi^3 eliminates dedicated pumps while maintaining a
lightweight and compact design. To the best of our knowledge, this is the first
quad-fan prototype to demonstrate functional power reuse for multi-modal
locomotion. A modeling and control framework enables coordinated operation
across ground, wall, and aerial domains with fan-assisted transitions. The
feasibility of the design is validated through a comprehensive set of
experiments covering ground driving, payload-assisted wall climbing, aerial
flight, and cross-mode transitions, demonstrating robust adaptability across
locomotion scenarios. These results highlight the potential of PerchMobi^3 as a
novel design paradigm for multi-modal robotic mobility, paving the way for
future extensions toward autonomous and application-oriented deployment.

</details>


### [168] [Safety filtering of robotic manipulation under environment uncertainty: a computational approach](https://arxiv.org/abs/2509.12674)
*Anna Johansson,Daniel Lindmark,Viktor Wiberg,Martin Servin*

Main category: cs.RO

TL;DR: 本文提出了一种基于物理仿真的机器人安全过滤方法，可在环境参数存在不确定性下评估和保障操作安全，并通过仿真实验验证了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人安全机制普遍假定完全可观测环境，难以适应实际动态和非结构化环境，尤其是在世界参数存在不确定性的情况下。因此，亟需能在不完全可观测和参数不确定时依然有效的安全评估与过滤方法。

Method: 方法结合高保真物理仿真，对控制策略在不同世界参数不确定性的情况下进行评估。具体做法是利用密集仿真滚动（dense rollout）加名义参数，在关键状态转移点再进行稀疏的并行重评估（sparse re-evaluation），并通过安全因子（涵盖抓取和执行器限制）进行量化，辅以专门的试探性动作减少不确定性。

Result: 在一个双臂操作任务的仿真中，通过引入质量和摩擦不确定性，实验证明该方法能够有效识别并过滤掉不安全的轨迹。

Conclusion: 物理基础的稀疏安全评估能高效且可扩展地提升机器人在不确定环境下的操作安全性，对实际复杂任务具有较大应用潜力。

Abstract: Robotic manipulation in dynamic and unstructured environments requires safety
mechanisms that exploit what is known and what is uncertain about the world.
Existing safety filters often assume full observability, limiting their
applicability in real-world tasks. We propose a physics-based safety filtering
scheme that leverages high-fidelity simulation to assess control policies under
uncertainty in world parameters. The method combines dense rollout with nominal
parameters and parallelizable sparse re-evaluation at critical
state-transitions, quantified through generalized factors of safety for stable
grasping and actuator limits, and targeted uncertainty reduction through
probing actions. We demonstrate the approach in a simulated bimanual
manipulation task with uncertain object mass and friction, showing that unsafe
trajectories can be identified and filtered efficiently. Our results highlight
physics-based sparse safety evaluation as a scalable strategy for safe robotic
manipulation under uncertainty.

</details>


### [169] [UDON: Uncertainty-weighted Distributed Optimization for Multi-Robot Neural Implicit Mapping under Extreme Communication Constraints](https://arxiv.org/abs/2509.12702)
*Hongrui Zhao,Xunlan Zhou,Boris Ivanovic,Negar Mehr*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多机器人神经隐式地图库UDON，能在极端低通信成功率下实现高质量地图重建。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人神经隐式地图系统在数据包丢失和带宽受限等恶劣通信环境下容易性能下降，迫切需要提升鲁棒性的方法。

Method: 提出一种不确定性加权的分布式优化框架：通过对置信度高的地图部分加权，以及分布式优化来隔离与惩罚机器人间的地图分歧，应对通信受损。

Result: 在标准数据集和真实机器人环境中，UDON显著优于现有方法，即使通信成功率仅为1%也能保持高保真度和一致的场景重建。

Conclusion: UDON极大提升了多机器人神经隐式地图库在严峻通信条件下的稳定性与重建精度，具备实际应用潜力。

Abstract: Multi-robot mapping with neural implicit representations enables the compact
reconstruction of complex environments. However, it demands robustness against
communication challenges like packet loss and limited bandwidth. While prior
works have introduced various mechanisms to mitigate communication disruptions,
performance degradation still occurs under extremely low communication success
rates. This paper presents UDON, a real-time multi-agent neural implicit
mapping framework that introduces a novel uncertainty-weighted distributed
optimization to achieve high-quality mapping under severe communication
deterioration. The uncertainty weighting prioritizes more reliable portions of
the map, while the distributed optimization isolates and penalizes mapping
disagreement between individual pairs of communicating agents. We conduct
extensive experiments on standard benchmark datasets and real-world robot
hardware. We demonstrate that UDON significantly outperforms existing
baselines, maintaining high-fidelity reconstructions and consistent scene
representations even under extreme communication degradation (as low as 1%
success rate).

</details>


### [170] [MoiréTac: A Dual-Mode Visuotactile Sensor for Multidimensional Perception Using Moiré Pattern Amplification](https://arxiv.org/abs/2509.12714)
*Kit-Wa Sou,Junhao Gong,Shoujie Li,Chuqiao Lyu,Ziwu Song,Shilong Mu,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出了一种名为 MoiréTac 的新型视觉-触觉传感器，通过微结构光栅共同产生干涉（莫尔）图样，实现高分辨率、可同时进行6轴力/力矩测量、触点定位和视觉感知的传感器能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-触觉传感器多采用稀疏标记点，空间分辨率有限，且力与图像之间的关系难以解析，因此亟需一种能够精确、稠密测量力/力矩且具备良好视觉功能的新型传感器。

Method: 设计了一种透明结构的双模式传感器，在传感器内部重叠微型光栅，通过错位产生莫尔干涉图样，大幅提升微小变形的感知能力。将物理特征（亮度、相位梯度、方向和周期）及深度空间特征结合，并通过端到端学习映射到6轴力/力矩回归任务。

Result: 实验显示，该方法在各轴向力/力矩回归任务中R^2大于0.98，能通过结构参数调整灵敏度（最大三倍增益变化），且即使存在干涉图样依旧可以实现目标分类视觉功能。此外，传感器在机械臂上完成了拧盖任务演示了其实用性。

Conclusion: MoiréTac传感器不仅能高精度测量多轴力/力矩，还保留了良好的视觉功能，具备调节灵敏度的能力，有潜力提升机器人灵巧操作与感知水平。

Abstract: Visuotactile sensors typically employ sparse marker arrays that limit spatial
resolution and lack clear analytical force-to-image relationships. To solve
this problem, we present \textbf{Moir\'eTac}, a dual-mode sensor that generates
dense interference patterns via overlapping micro-gratings within a transparent
architecture. When two gratings overlap with misalignment, they create moir\'e
patterns that amplify microscopic deformations. The design preserves optical
clarity for vision tasks while producing continuous moir\'e fields for tactile
sensing, enabling simultaneous 6-axis force/torque measurement, contact
localization, and visual perception. We combine physics-based features
(brightness, phase gradient, orientation, and period) from moir\'e patterns
with deep spatial features. These are mapped to 6-axis force/torque
measurements, enabling interpretable regression through end-to-end learning.
Experimental results demonstrate three capabilities: force/torque measurement
with R^2 > 0.98 across tested axes; sensitivity tuning through geometric
parameters (threefold gain adjustment); and vision functionality for object
classification despite moir\'e overlay. Finally, we integrate the sensor into a
robotic arm for cap removal with coordinated force and torque control,
validating its potential for dexterous manipulation.

</details>


### [171] [NAMOUnc: Navigation Among Movable Obstacles with Decision Making on Uncertainty Interval](https://arxiv.org/abs/2509.12723)
*Kai Zhang,Eric Lucet,Julien Alexandre Dit Sandretto,Shoubin Chen,David Filait*

Main category: cs.RO

TL;DR: 提出NAMOUnc框架，专门处理真实环境中导航任务的不确定性，包括观测噪声、模型近似、动作失败和部分可观测性，实现更安全高效的机器人导航。


<details>
  <summary>Details</summary>
Motivation: 现有NAMO系统假定环境理想，导致在存在真实不确定性时表现不佳，易于做出次优或危险决策。需开发能主动处理环境不确定性的解决方案，提升任务成功率和效率。

Method: 提出NAMOUnc框架，通过量化各类不确定性，比较移除和绕过障碍物的预期耗时区间，将不确定性显式引入决策过程，优化安全性与时间成本。系统在模拟和真实机器人平台上进行了评估。

Result: 实验结果显示，NAMOUnc框架在成功率、安全性和时间效率方面显著优于传统NAMO方法。

Conclusion: NAMOUnc有效整合不确定性因素，提升了机器人在真实复杂环境中的导航表现。

Abstract: Navigation among movable obstacles (NAMO) is a critical task in robotics,
often challenged by real-world uncertainties such as observation noise, model
approximations, action failures, and partial observability. Existing solutions
frequently assume ideal conditions, leading to suboptimal or risky decisions.
This paper introduces NAMOUnc, a novel framework designed to address these
uncertainties by integrating them into the decision-making process. We first
estimate them and compare the corresponding time cost intervals for removing
and bypassing obstacles, optimizing both the success rate and time efficiency,
ensuring safer and more efficient navigation. We validate our method through
extensive simulations and real-world experiments, demonstrating significant
improvements over existing NAMO frameworks. More details can be found in our
website: https://kai-zhang-er.github.io/namo-uncertainty/

</details>


### [172] [Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors](https://arxiv.org/abs/2509.12739)
*Trung Kien La,Eric Guiffo Kaigom*

Main category: cs.RO

TL;DR: 本文利用深层神经网络（包括多层LSTM与前馈层）预测机器人关节电机的热行为，实现无需先验模型的预测。实验结果表明该方法在七关节机器人上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 传统的热行为建模依赖复杂的参数建模与辨识，难以在实际中获得所有所需数据。本文旨在提出一种不依赖复杂物理建模的可扩展、无模型机器学习方法，简化实际应用。

Method: 本文收集机器人物理操作时的关节扭矩数据，利用多层LSTM和前馈神经网络，通过监督学习框架直接学习关节电机温度动态规律，实现热行为的短期预测。

Result: 在一台拥有七个自由度的冗余机器人上，基于深度学习的方法能够有效捕捉并预测关节电机的温度变化，取得了良好的预测效果。

Conclusion: 无需物理建模的深度神经网络方法在预测机器人关节电机热行为方面表现良好，为复杂机器人系统的健康监控和保护提供了有效工具。

Abstract: In this work, deep neural networks made up of multiple hidden Long Short-Term
Memory (LSTM) and Feedforward layers are trained to predict the thermal
behavior of the joint motors of robot manipulators. A model-free and scalable
approach is adopted. It accommodates complexity and uncertainty challenges
stemming from the derivation, identification, and validation of a large number
of parameters of an approximation model that is hardly available. To this end,
sensed joint torques are collected and processed to foresee the thermal
behavior of joint motors. Promising prediction results of the machine learning
based capture of the temperature dynamics of joint motors of a redundant robot
with seven joints are presented.

</details>


### [173] [Deep Generative and Discriminative Digital Twin endowed with Variational Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of Physical Robots in Industry 6.0 and Society 6.0](https://arxiv.org/abs/2509.12740)
*Eric Guiffo Kaigom*

Main category: cs.RO

TL;DR: 本论文提出利用结合生成式AI（变分自编码器）的智能数字孪生技术，来提前预测和管理机器人因电机过热引发的热失效，实现工厂与社会机器人在长期运行中的自我保护与性能自维护。


<details>
  <summary>Details</summary>
Motivation: 随着工业4.0和5.0的发展，机器人广泛应用于制造业和人机协作场景中，不仅要保证运行效率，还需保障人类安全和自身稳定性。传统的机器人因过热而关机，不仅影响生产效率，也限制了智能机器人的应用范围。因此，急需一种新方法，让机器人能自主预测并适应热异常，延长其工作寿命，实现更高层次的自主与协作。

Method: 作者提出基于智能数字孪生和生成式AI（使用变分自编码器）的热管理系统。首先，通过数字孪生模型实时采集并仿真机器人运动与热态。然后，利用变分自编码器的重构误差定义“热难度”指标，来识别并生成非风险（uncritical）的运动状态。机器人借助该指标预测、评估和分享其任务在热力学上的可行性。

Result: 实验显示，结合生成式AI的数字孪生能有效提前识别和预测机器人面临的热风险，生成风险较低的运动策略，从而减少因过热导致的停机，提高了机器人在工业和服务场景中的连续可用性。

Conclusion: 本文方法不仅提升了机器人热管理的智能化水平，还通过能力预报告与任务适应，提高了工业与社会机器人系统的韧性、自维护能力和安全性，为工业6.0与社会6.0奠定了技术基础。

Abstract: Robots are unrelentingly used to achieve operational efficiency in Industry
4.0 along with symbiotic and sustainable assistance for the work-force in
Industry 5.0. As resilience, robustness, and well-being are required in
anti-fragile manufacturing and human-centric societal tasks, an autonomous
anticipation and adaption to thermal saturation and burns due to motors
overheating become instrumental for human safety and robot availability. Robots
are thereby expected to self-sustain their performance and deliver user
experience, in addition to communicating their capability to other agents in
advance to ensure fully automated thermally feasible tasks, and prolong their
lifetime without human intervention. However, the traditional robot shutdown,
when facing an imminent thermal saturation, inhibits productivity in factories
and comfort in the society, while cooling strategies are hard to implement
after the robot acquisition. In this work, smart digital twins endowed with
generative AI, i.e., variational autoencoders, are leveraged to manage
thermally anomalous and generate uncritical robot states. The notion of thermal
difficulty is derived from the reconstruction error of variational
autoencoders. A robot can use this score to predict, anticipate, and share the
thermal feasibility of desired motion profiles to meet requirements from
emerging applications in Industry 6.0 and Society 6.0.

</details>


### [174] [Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions](https://arxiv.org/abs/2509.12741)
*Alexis Yihong Hao,Yufei Wang,Navin Sriram Ravie,Bharath Hegde,David Held,Zackory Erickson*

Main category: cs.RO

TL;DR: 本文提出了一种能适应手臂运动且鲁棒性强的机器人辅助穿衣系统，通过融合视觉和力传感器数据，实现对遮挡和部分观察的处理，并在现实中通过少量数据微调，在实验中显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器人辅助穿衣系统多假设人体四肢静止，限制了其现实应用。解决人体运动、衣物变形、遮挡等带来的挑战，有助于真正为行动障碍者改善生活质量。

Method: 作者先在模拟环境下训练机器人穿衣策略，处理部分观察信息。然后，通过少量现实数据及多模态（视觉+力感）反馈微调策略，使其更加适应真实环境下手臂运动及环境变化。

Result: 在仿真和12位真实参与者共264次实验中，该方法能够成功为不同手臂运动状态下的用户穿上长袖衣物，任务完成率与用户体验均大幅超过现有方法。

Conclusion: 面向实际应用，提出的方法克服了穿衣过程中对动作适应和遮挡处理的难题，显著提升了机器人辅助穿衣的安全性和可靠性，具有推广价值。

Abstract: Robot-assisted dressing has the potential to significantly improve the lives
of individuals with mobility impairments. To ensure an effective and
comfortable dressing experience, the robot must be able to handle challenging
deformable garments, apply appropriate forces, and adapt to limb movements
throughout the dressing process. Prior work often makes simplifying assumptions
-- such as static human limbs during dressing -- which limits real-world
applicability. In this work, we develop a robot-assisted dressing system
capable of handling partial observations with visual occlusions, as well as
robustly adapting to arm motions during the dressing process. Given a policy
trained in simulation with partial observations, we propose a method to
fine-tune it in the real world using a small amount of data and multi-modal
feedback from vision and force sensing, to further improve the policy's
adaptability to arm motions and enhance safety. We evaluate our method in
simulation with simplified articulated human meshes and in a real world human
study with 12 participants across 264 dressing trials. Our policy successfully
dresses two long-sleeve everyday garments onto the participants while being
adaptive to various kinds of arm motions, and greatly outperforms prior
baselines in terms of task completion and user feedback. Video are available at
https://dressing-motion.github.io/.

</details>


### [175] [NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts](https://arxiv.org/abs/2509.12747)
*Botao He,Amir Hossein Shahidzadeh,Yu Chen,Jiayi Wu,Tianrui Guan,Guofei Chen,Howie Choset,Dinesh Manocha,Glen Chou,Cornelia Fermuller,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 本文提出了一种机器人可通行性估计新方法NAVMOE，通过动态调用多种地形类型的专家模型，提高了跨环境的导航鲁棒性和效率。在大幅降低计算成本的同时，几乎不损失路径质量。


<details>
  <summary>Details</summary>
Motivation: 传统可通行性估计在不同地形下难以兼顾鲁棒性、准确性和效率，尤其在多样化或未知环境中更显短板。作者受限于需要同时融合几何与语义信息，并且要兼顾效率和泛化能力。

Method: 提出层级化与模块化的Mixture of Experts (MoE)方法——NAVMOE。为每类地形训练或选用专门模型（可为基于传统方法或深度学习模型），通过门控网络根据环境输入动态加权并选择合适的专家组合。采用无训练惰性门控策略减少推理时激活专家数，同时设计两阶段训练实现非可微模块下的MoE训练。

Result: 大量实验表明，NAVMOE在不同领域/环境下，相较于单一专家或全体专家集成，能够更好平衡效率和性能。具体地，通过惰性门控降低了81.2%的平均计算成本，路径质量损失不足2%。

Conclusion: NAVMOE方法实现了高效、泛化性强的可通行性估计和本地导航，在不同地形及跨域任务中的表现均优于传统单一模型，提升了机器人导航的实用性和效率。

Abstract: This paper explores traversability estimation for robot navigation. A key
bottleneck in traversability estimation lies in efficiently achieving reliable
and robust predictions while accurately encoding both geometric and semantic
information across diverse environments. We introduce Navigation via Mixture of
Experts (NAVMOE), a hierarchical and modular approach for traversability
estimation and local navigation. NAVMOE combines multiple specialized models
for specific terrain types, each of which can be either a classical model-based
or a learning-based approach that predicts traversability for specific terrain
types. NAVMOE dynamically weights the contributions of different models based
on the input environment through a gating network. Overall, our approach offers
three advantages: First, NAVMOE enables traversability estimation to adaptively
leverage specialized approaches for different terrains, which enhances
generalization across diverse and unseen environments. Second, our approach
significantly improves efficiency with negligible cost of solution quality by
introducing a training-free lazy gating mechanism, which is designed to
minimize the number of activated experts during inference. Third, our approach
uses a two-stage training strategy that enables the training for the gating
networks within the hybrid MoE method that contains nondifferentiable modules.
Extensive experiments show that NAVMOE delivers a better efficiency and
performance balance than any individual expert or full ensemble across
different domains, improving cross- domain generalization and reducing average
computational cost by 81.2% via lazy gating, with less than a 2% loss in path
quality.

</details>


### [176] [Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model](https://arxiv.org/abs/2509.12754)
*Saki Hashimoto,Shoichi Hasegawa,Tomochika Ishikawa,Akira Taniguchi,Yoshinobu Hagiwara,Lotfi El Hafi,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: 该论文提出了一种名为Active Ownership Learning (ActOwL)的新方法，使机器人能够主动向用户提问，学习和理解物品的所有权，从而更好地执行与所有权相关的任务。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在日常和办公环境中执行任务时，常需理解物品所有权，但仅凭视觉特征难以判断所有权，这限制了机器人对指令的正确理解和执行能力。亟需一种高效、智能的所有权知识获取方式。

Method: 提出ActOwL框架，采用概率生成模型选择能够最大化信息增益的问题，主动向用户询问所有权。同时，结合大型语言模型（LLM）的常识推理，对物品进行共享/拥有预分类，只对拥有物品发起提问，提升学习效率。

Result: 在模拟家庭环境和真实实验室中，ActOwL在提出较少问题的情况下，物品所有权聚类准确率明显高于基线方法。

Conclusion: 主动推理和大型语言模型常识推理结合，有效提升机器人获取和判断物品所有权的能力，为实际和社交场景下机器人的任务执行能力带来新进展。

Abstract: Robots operating in domestic and office environments must understand object
ownership to correctly execute instructions such as ``Bring me my cup.''
However, ownership cannot be reliably inferred from visual features alone. To
address this gap, we propose Active Ownership Learning (ActOwL), a framework
that enables robots to actively generate and ask ownership-related questions to
users. ActOwL employs a probabilistic generative model to select questions that
maximize information gain, thereby acquiring ownership knowledge efficiently to
improve learning efficiency. Additionally, by leveraging commonsense knowledge
from Large Language Models (LLM), objects are pre-classified as either shared
or owned, and only owned objects are targeted for questioning. Through
experiments in a simulated home environment and a real-world laboratory
setting, ActOwL achieved significantly higher ownership clustering accuracy
with fewer questions than baseline methods. These findings demonstrate the
effectiveness of combining active inference with LLM-guided commonsense
reasoning, advancing the capability of robots to acquire ownership knowledge
for practical and socially appropriate task execution.

</details>


### [177] [Integrating Trajectory Optimization and Reinforcement Learning for Quadrupedal Jumping with Terrain-Adaptive Landing](https://arxiv.org/abs/2509.12776)
*Renjie Wang,Shangke Lyu,Xin Lang,Wei Xiao,Donglin Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合轨迹优化（TO）和强化学习（RL）的四足机器人安全着陆框架，实现了在复杂地形上的自适应跳跃着陆。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人跳跃研究多假设着陆面为平地，忽略了实际复杂地形问题，导致机器人着陆安全与适应性不足。

Method: 该方法将轨迹优化和强化学习结合。首先，通过轨迹优化生成跳跃着陆的参考运动轨迹，然后让强化学习智能体在复杂地形环境中学习跟踪这一参考运动。为了在复杂地形上学习到柔顺的着陆技能，引入了奖励放宽策略，鼓励在着陆恢复期探索更优动作。

Result: 大量实验证明，所提方法能够在各种复杂地形情景下，实现四足机器人对参考轨迹的准确跟踪和安全着陆。

Conclusion: 结合TO与RL的安全着陆框架提升了四足机器人在复杂地形下的自适应着陆能力，实验验证了该方法的有效性和实际应用价值。

Abstract: Jumping constitutes an essential component of quadruped robots' locomotion
capabilities, which includes dynamic take-off and adaptive landing. Existing
quadrupedal jumping studies mainly focused on the stance and flight phase by
assuming a flat landing ground, which is impractical in many real world cases.
This work proposes a safe landing framework that achieves adaptive landing on
rough terrains by combining Trajectory Optimization (TO) and Reinforcement
Learning (RL) together. The RL agent learns to track the reference motion
generated by TO in the environments with rough terrains. To enable the learning
of compliant landing skills on challenging terrains, a reward relaxation
strategy is synthesized to encourage exploration during landing recovery
period. Extensive experiments validate the accurate tracking and safe landing
skills benefiting from our proposed method in various scenarios.

</details>


### [178] [Bridging Perception and Planning: Towards End-to-End Planning for Signal Temporal Logic Tasks](https://arxiv.org/abs/2509.12813)
*Bowen Ye,Junyue Huang,Yang Liu,Xiaozhen Qiao,Xiang Yin*

Main category: cs.RO

TL;DR: 本文提出了S-MSP框架，可直接基于多视角相机数据和STL任务约束生成可行机器人轨迹，并在仿真中验证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的STL任务与运动规划方法依赖于预定义地图或移动性建模，导致在复杂、非结构化环境下效果不佳。为解决这一局限，需要可直接处理真实环境感知信息的规划方法。

Method: 提出了S-MSP（一种结构感知专家混合模型），将多视角相机观测和STL规范输入，通过可微分网络直接输出机器人轨迹。该方法结合了轨迹重构损失和STL稳健性损失进行训练，通过投影子任务实现时间感知的专家分工。在推理阶段叠加基于规则的安全过滤器，提升实际可执行性。

Result: 采用高保真工厂物流仿真环境评估，S-MSP在STL约束满足率和轨迹可行性方面均显著优于单一专家的基线方法。

Conclusion: S-MSP在无需预建地图的情况下，实现了更优的逻辑约束满足和高可行性轨迹规划，并通过安全过滤保证推理可执行性，证明方法在真实世界任务中的实用性。

Abstract: We investigate the task and motion planning problem for Signal Temporal Logic
(STL) specifications in robotics. Existing STL methods rely on pre-defined maps
or mobility representations, which are ineffective in unstructured real-world
environments. We propose the \emph{Structured-MoE STL Planner}
(\textbf{S-MSP}), a differentiable framework that maps synchronized multi-view
camera observations and an STL specification directly to a feasible trajectory.
S-MSP integrates STL constraints within a unified pipeline, trained with a
composite loss that combines trajectory reconstruction and STL robustness. A
\emph{structure-aware} Mixture-of-Experts (MoE) model enables horizon-aware
specialization by projecting sub-tasks into temporally anchored embeddings. We
evaluate S-MSP using a high-fidelity simulation of factory-logistics scenarios
with temporally constrained tasks. Experiments show that S-MSP outperforms
single-expert baselines in STL satisfaction and trajectory feasibility. A
rule-based \emph{safety filter} at inference improves physical executability
without compromising logical correctness, showcasing the practicality of the
approach.

</details>


### [179] [Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models](https://arxiv.org/abs/2509.12838)
*Kento Murata,Shoichi Hasegawa,Tomochika Ishikawa,Yoshinobu Hagiwara,Akira Taniguchi,Lotfi El Hafi,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: 本研究提出了一种结合大语言模型（LLM）和空间概念的多机器人任务分配框架，能够将自然语言指令分解为子任务，并有效分配给具备不同现场知识的机器人，实验证明该方法在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，执行如“找一个苹果和一个香蕉”或“准备去郊游”等需要理解上下文和多目标的复杂指令时，合理分配任务（尤其在机器人掌握不同空间知识的情况下）是一大难题。需要一种能理解模糊/复杂自然语言，且能结合每个机器人的特定知识，实现任务分解与高效分配的自动化方法。

Method: 作者提出了一个融合大语言模型和空间概念的任务规划框架，采用新颖的少样本提示方法，使LLM能够根据模糊自然语言指令推断所需目标与子任务；系统进一步结合机器人的空间知识，有效分解并分配复杂任务至不同机器人。

Result: 所提方法在实验中实现了47/50的成功分配率，显著优于随机分配（28/50）及常识分配（26/50）。还通过实际移动操作机器人验证了方法能处理如“准备去郊游”等具有临时类别的复杂指令，实现了任务分解、分配、规划与执行。

Conclusion: 结合LLM与空间知识的框架能有效理解复杂自然语言指令，并提升多机器人协同任务分解与分配的性能，尤其在处理现场知识异构与不明确指令时表现突出。

Abstract: It is crucial to efficiently execute instructions such as "Find an apple and
a banana" or "Get ready for a field trip," which require searching for multiple
objects or understanding context-dependent commands. This study addresses the
challenging problem of determining which robot should be assigned to which part
of a task when each robot possesses different situational on-site
knowledge-specifically, spatial concepts learned from the area designated to it
by the user. We propose a task planning framework that leverages large language
models (LLMs) and spatial concepts to decompose natural language instructions
into subtasks and allocate them to multiple robots. We designed a novel
few-shot prompting strategy that enables LLMs to infer required objects from
ambiguous commands and decompose them into appropriate subtasks. In our
experiments, the proposed method achieved 47/50 successful assignments,
outperforming random (28/50) and commonsense-based assignment (26/50).
Furthermore, we conducted qualitative evaluations using two actual mobile
manipulators. The results demonstrated that our framework could handle
instructions, including those involving ad hoc categories such as "Get ready
for a field trip," by successfully performing task decomposition, assignment,
sequential planning, and execution.

</details>


### [180] [Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration](https://arxiv.org/abs/2509.12846)
*Junlin Song,Antoine Richard,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了一种高效的视觉-惯性空间-时间校准方法，以提升机器人导航、增强现实等领域的状态估计性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-惯性融合在智能、自动化领域应用广泛，但现有的校准方法虽然精准，却由于采用B样条等连续时间表示导致计算量大，效率低，限制了在大规模设备上的应用。

Method: 本文提出基于离散时间状态表示的空间-时间校准新方法。这种方法极大提高了校准运算效率，并针对离散时间表示在时间校准中的短板给出了解决策略。

Result: 方法能以极高效率完成视觉-惯性系统的空间-时间校准，在计算速度和所需资源上表现优异，适用于大规模设备批量校准场景。

Conclusion: 该方法极大提升了空间-时间校准的效率，尤其适用于设备量巨大的应用场景，具有产业化潜力。代码将开源，方便学界和产业界广泛使用。

Abstract: Visual-inertial fusion is crucial for a large amount of intelligent and
autonomous applications, such as robot navigation and augmented reality. To
bootstrap and achieve optimal state estimation, the spatial-temporal
displacements between IMU and cameras must be calibrated in advance. Most
existing calibration methods adopt continuous-time state representation, more
specifically the B-spline. Despite these methods achieve precise
spatial-temporal calibration, they suffer from high computational cost caused
by continuous-time state representation. To this end, we propose a novel and
extremely efficient calibration method that unleashes the power of
discrete-time state representation. Moreover, the weakness of discrete-time
state representation in temporal calibration is tackled in this paper. With the
increasing production of drones, cellphones and other visual-inertial
platforms, if one million devices need calibration around the world, saving one
minute for the calibration of each device means saving 2083 work days in total.
To benefit both the research and industry communities, our code will be
open-source.

</details>


### [181] [A Novel Skill Modeling Approach: Integrating Vergnaud's Scheme with Cognitive Architectures](https://arxiv.org/abs/2509.12851)
*Antoine Lénat,Olivier Cheminat,Damien Chablat,Camilo Charron*

Main category: cs.RO

TL;DR: 本文探讨了随着工业5.0的发展，人-机交互对于工业的重要性进一步上升，强调了准确描述并理解操作员技能、技能转移和认知架构建模的重要性，并以焊接为案例场景讨论其挑战。


<details>
  <summary>Details</summary>
Motivation: 工业自动化和人-机协作趋势加剧，操作员技能对最终结果起决定作用，因此亟需精确描述和理解这些技能，特别是在机器人辅助与否的情况下进行对比。

Method: 提出将谓词逻辑（Vergnaud的schemes概念）与认知架构模型结合的方法，以系统化描述和建模操作员的技能，包括适应动态变化和各类认知限制。以复杂且要求高度适应性的焊接任务为案例，分析操作员在真实环境下的技能运用与挑战。

Result: 结果表明，仅有理论建模（谓词逻辑）不足以描述现实认知约束，结合认知架构能更全面反映技能的动态适应性和局限性；焊接的案例突显了认知系统对复杂参数变化的适应需求。

Conclusion: 整合认知架构与动作方案理论，有助于更加科学和系统化地建模人类操作员的技能，为人-机协作以及技能转移提供了理论和方法基础，尤其适用于如焊接等高要求场景。

Abstract: Human-machine interaction is increasingly important in industry, and this
trend will only intensify with the rise of Industry 5.0. Human operators have
skills that need to be adapted when using machines to achieve the best results.
It is crucial to highlight the operator's skills and understand how they use
and adapt them [18]. A rigorous description of these skills is necessary to
compare performance with and without robot assistance. Predicate logic, used by
Vergnaud within Piaget's scheme concept, offers a promising approach. However,
this theory doesn't account for cognitive system constraints, such as the
timing of actions, the limitation of cognitive resources, the parallelization
of tasks, or the activation of automatic gestures contrary to optimal
knowledge. Integrating these constraints is essential for representing agent
skills understanding skill transfer between biological and mechanical
structures. Cognitive architectures models [2] address these needs by
describing cognitive structure and can be combined with the scheme for mutual
benefit. Welding provides a relevant case study, as it highlights the
challenges faced by operators, even highly skilled ones. Welding's complexity
stems from the need for constant skill adaptation to variable parameters like
part position and process. This adaptation is crucial, as weld quality, a key
factor, is only assessed afterward via destructive testing. Thus, the welder is
confronted with a complex perception-decision-action cycle, where the
evaluation of the impact of his actions is delayed and where errors are
definitive. This dynamic underscores the importance of understanding and
modeling the skills of operators.

</details>


### [182] [Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion](https://arxiv.org/abs/2509.12858)
*Yidan Lu,Rurui Yang,Qiran Kou,Mengting Chen,Tao Fan,Peter Cui,Yinzhao Dong,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种结合感知前瞻性和本体感知控制鲁棒性的强化学习方法，实现了在不增加额外部署成本的情况下，提高了机器人步态的主动适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在真实机器人部署中经常面临一个难题——要么采用鲁棒但仅依赖本体感知的被动控制，要么使用主动但容易失效的感知驱动系统。如何兼顾两者，实现既主动又鲁棒的控制，是机器人运动领域的重要挑战。

Method: 作者设计了一种对比学习框架，使得强化学习策略的潜在状态能编码来自仿真的特权环境信息。这种"蒸馏感知"辅助策略内部建立起适应地形的主动节奏调整机制，既能预见地形变化，又避免了部署时的感知系统成本和脆弱性。

Result: 所提方法在无需任何域适配的情况下，实现了仿真到真实全尺寸人形机器人（zero-shot sim-to-real）的迁移。在多种复杂地形（如30厘米高台阶和26.5度斜坡）上展现出极高的步态稳定性和鲁棒性。

Conclusion: 该方法有效地解决了感知驱动控制与本体感知控制之间的权衡问题，实现了高效、稳健且具有前瞻性的机器人运动控制，对于实际机器人在复杂环境下的应用具有重要意义。

Abstract: Reinforcement learning has produced remarkable advances in humanoid
locomotion, yet a fundamental dilemma persists for real-world deployment:
policies must choose between the robustness of reactive proprioceptive control
or the proactivity of complex, fragile perception-driven systems. This paper
resolves this dilemma by introducing a paradigm that imbues a purely
proprioceptive policy with proactive capabilities, achieving the foresight of
perception without its deployment-time costs. Our core contribution is a
contrastive learning framework that compels the actor's latent state to encode
privileged environmental information from simulation. Crucially, this
``distilled awareness" empowers an adaptive gait clock, allowing the policy to
proactively adjust its rhythm based on an inferred understanding of the
terrain. This synergy resolves the classic trade-off between rigid, clocked
gaits and unstable clock-free policies. We validate our approach with zero-shot
sim-to-real transfer to a full-sized humanoid, demonstrating highly robust
locomotion over challenging terrains, including 30 cm high steps and 26.5{\deg}
slopes, proving the effectiveness of our method. Website:
https://lu-yidan.github.io/cra-loco.

</details>


### [183] [GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration](https://arxiv.org/abs/2509.12863)
*Haozhan Ni,Jingsong Liang,Chenyu He,Yuhong Cao,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）和图变换器（Graph Transformer）的自主机器人探索新方法（GRATE），在模拟和实际场景中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的自主机器人探索方法在处理图结构数据的推理能力有限，并且优化目标通常侧重于路径距离，忽略了时间效率，且难以保证机器人运动的可行性。

Method: 提出GRATE方法，利用图变换器捕捉信息图的局部结构和全球依赖关系；采用卡尔曼滤波平滑路径点，保证轨迹运动可行性。

Result: 在多个仿真基准上，GRATE在探索距离和时间效率上，分别比现有方法最多提升21.5%和21.3%。此外，在真实场景中也验证了方法的有效性。

Conclusion: GRATE方法增强了机器人在复杂环境下的推理与探索效率，有效克服了现有方法的局限性，并具有良好的实际应用前景。

Abstract: Autonomous robot exploration (ARE) is the process of a robot autonomously
navigating and mapping an unknown environment. Recent Reinforcement Learning
(RL)-based approaches typically formulate ARE as a sequential decision-making
problem defined on a collision-free informative graph. However, these methods
often demonstrate limited reasoning ability over graph-structured data.
Moreover, due to the insufficient consideration of robot motion, the resulting
RL policies are generally optimized to minimize travel distance, while
neglecting time efficiency. To overcome these limitations, we propose GRATE, a
Deep Reinforcement Learning (DRL)-based approach that leverages a Graph
Transformer to effectively capture both local structure patterns and global
contextual dependencies of the informative graph, thereby enhancing the model's
reasoning capability across the entire environment. In addition, we deploy a
Kalman filter to smooth the waypoint outputs, ensuring that the resulting path
is kinodynamically feasible for the robot to follow. Experimental results
demonstrate that our method exhibits better exploration efficiency (up to 21.5%
in distance and 21.3% in time to complete exploration) than state-of-the-art
conventional and learning-based baselines in various simulation benchmarks. We
also validate our planner in real-world scenarios.

</details>


### [184] [Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation](https://arxiv.org/abs/2509.12880)
*Anna Deichler,Siyang Wang,Simon Alexanderson,Jonas Beskow*

Main category: cs.RO

TL;DR: 本文提出并公开了一个涵盖多样手势风格、用手习惯和空间目标的人类指向动作捕捉数据集，并利用强化学习和动作模仿训练机器人生成类人的指向动作。


<details>
  <summary>Details</summary>
Motivation: 目前机器人领域中，指向手势作为关键的人机交互模式，已有工作多偏重于手势识别，而较少研究指向动作的生成。该工作动机是为了提升机器人类人指向动作的自然性和任务表现，实现更自然高效的交互。

Method: 作者采集了多样化的人类指向手势动作捕捉数据集，涵盖风格、用手习惯及空间目标多样性。随后采用强化学习结合动作模仿，训练能够模拟人类风格同时又具备高指向精度的机器人动作生成策略。

Result: 实验结果表明，所提方法在仿真中可以生成具有上下文感知的类人指向手势，兼顾了动作的自然性和任务精度。

Conclusion: 本文证明了通过融合动作模仿与强化学习，可以实现仿真环境下既自然又有效的机器人指向行为，对提升人机交互体验具有积极意义。

Abstract: Pointing is a key mode of interaction with robots, yet most prior work has
focused on recognition rather than generation. We present a motion capture
dataset of human pointing gestures covering diverse styles, handedness, and
spatial targets. Using reinforcement learning with motion imitation, we train
policies that reproduce human-like pointing while maximizing precision. Results
show our approach enables context-aware pointing behaviors in simulation,
balancing task performance with natural dynamics.

</details>


### [185] [Responsibility and Engagement - Evaluating Interactions in Social Robot Navigation](https://arxiv.org/abs/2509.12890)
*Malte Probst,Raphael Wenzel,Monica Dasi*

Main category: cs.RO

TL;DR: 本文提出并扩展了用于评估社交机器人导航中人-机互动冲突分担与参与度的指标体系。


<details>
  <summary>Details</summary>
Motivation: 在社交机器人导航中，人-机交互不可避免会出现多方冲突，如何科学、定量地评估各方对冲突解决的贡献及行为质量尤为关键。此前已有责任度（Responsibility）指标，但尚需更完善的评估体系。

Method: 本文在既有责任度指标的基础上，加入了时间归一化，以刻画冲突的积累过程。同时新提出了参与度（Engagement）指标，用于衡量代理行为对冲突激化的影响，并在模拟环境中（包括双人、小组和人群交互）检验了这些指标的有效性。

Result: 实验表明，所提出的责任度与参与度指标能够有效反映代理在冲突解决中的合作性、行为质量及前瞻性。

Conclusion: 这些指标有助于更细致地分析和评估社交机器人在复杂交互情境下的导航表现，但设计和应用中仍需关注可解释性及局限性。

Abstract: In Social Robot Navigation (SRN), the availability of meaningful metrics is
crucial for evaluating trajectories from human-robot interactions. In the SRN
context, such interactions often relate to resolving conflicts between two or
more agents. Correspondingly, the shares to which agents contribute to the
resolution of such conflicts are important. This paper builds on recent work,
which proposed a Responsibility metric capturing such shares. We extend this
framework in two directions: First, we model the conflict buildup phase by
introducing a time normalization. Second, we propose the related Engagement
metric, which captures how the agents' actions intensify a conflict. In a
comprehensive series of simulated scenarios with dyadic, group and crowd
interactions, we show that the metrics carry meaningful information about the
cooperative resolution of conflicts in interactions. They can be used to assess
behavior quality and foresightedness. We extensively discuss applicability,
design choices and limitations of the proposed metrics.

</details>


### [186] [Spotting the Unfriendly Robot - Towards better Metrics for Interactions](https://arxiv.org/abs/2509.12912)
*Raphael Wenzel,Malte Probst*

Main category: cs.RO

TL;DR: 论文提出了两种新的社交机器人导航（SRN）评估指标，补充了现有指标在衡量机器人与人互动合作性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前通用的SRN评估指标无法量化机器人在与人互动时的合作程度，尤其是在面对面接近场景中，指标难以反映是哪一方主动合作或回避，存在评估盲点。

Method: 作者提出了“冲突强度指标”和“责任指标”，用以评价人机互动中的冲突减少贡献度及责任归属，从而细致分析机器人算法在实际人群场景下的合作表现。

Result: 这两项新指标能够有效补足现有评估体系，不仅能够度量机器人行为的质量，还能判断冲突由谁主导解决，提高评测的细粒度和社会性。

Conclusion: 研究为社交机器人导航评估方法的标准化和全面化提供了新工具，有望提升机器人在以人为中心环境中的安全性、效率和社会接受度。

Abstract: Establishing standardized metrics for Social Robot Navigation (SRN)
algorithms for assessing the quality and social compliance of robot behavior
around humans is essential for SRN research. Currently, commonly used
evaluation metrics lack the ability to quantify how cooperative an agent
behaves in interaction with humans. Concretely, in a simple frontal approach
scenario, no metric specifically captures if both agents cooperate or if one
agent stays on collision course and the other agent is forced to evade. To
address this limitation, we propose two new metrics, a conflict intensity
metric and the responsibility metric. Together, these metrics are capable of
evaluating the quality of human-robot interactions by showing how much a given
algorithm has contributed to reducing a conflict and which agent actually took
responsibility of the resolution. This work aims to contribute to the
development of a comprehensive and standardized evaluation methodology for SRN,
ultimately enhancing the safety, efficiency, and social acceptance of robots in
human-centric environments.

</details>


### [187] [Spatiotemporal Calibration for Laser Vision Sensor in Hand-eye System Based on Straight-line Constraint](https://arxiv.org/abs/2509.12928)
*Peiwen Yang,Mingquan Jiang,Xinyue Shen,Heping Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种针对激光视觉传感器（LVS）在焊接应用中时空标定的新方法，有效解决了相机通讯延迟和手眼外参数漂移等问题。所提方法无需人工示教，通过扫描直线焊缝并施加直线约束进行参数优化，实验验证方法准确且可行。相关代码和数据已开源。


<details>
  <summary>Details</summary>
Motivation: 工业机器人焊接过程中依赖LVS获取工件几何信息，但存在相机通讯延迟（引发时序不同步）和长时间工作导致的手眼外参变化，影响测量精度。因此，亟需一种有效的时空标定方法提升感知可靠性。

Method: 建立考虑相机时延的LVS测量模型，提出基于直线约束的无示教时空标定方法。机器人按S型轨迹多次扫描直线焊缝，所有焊缝点通过Plücker坐标约束在直线上，建立非线性优化模型，并采用Levenberg-Marquardt算法联合优化时间偏移、手眼外参和直线参数。

Result: 通过对曲线焊缝扫描的实验，定量评估了所提方法的可行性和精度。对比验证证明该方法能有效修正相机时延和外参漂移带来的误差。

Conclusion: 提出的基于直线约束的教学自由时空标定方法能够同时有效校准时延与外参，提高了LVS在工业机器人焊接中的应用准确性和鲁棒性。相关成果已开源，便于推广和复现。

Abstract: Laser vision sensors (LVS) are critical perception modules for industrial
robots, facilitating real-time acquisition of workpiece geometric data in
welding applications. However, the camera communication delay will lead to a
temporal desynchronization between captured images and the robot motions.
Additionally, hand-eye extrinsic parameters may vary during prolonged
measurement. To address these issues, we introduce a measurement model of LVS
considering the effect of the camera's time-offset and propose a teaching-free
spatiotemporal calibration method utilizing line constraints. This method
involves a robot equipped with an LVS repeatedly scanning straight-line fillet
welds using S-shaped trajectories. Regardless of the robot's orientation
changes, all measured welding positions are constrained to a straight-line,
represented by Plucker coordinates. Moreover, a nonlinear optimization model
based on straight-line constraints is established. Subsequently, the
Levenberg-Marquardt algorithm (LMA) is employed to optimize parameters,
including time-offset, hand-eye extrinsic parameters, and straight-line
parameters. The feasibility and accuracy of the proposed approach are
quantitatively validated through experiments on curved weld scanning. We
open-sourced the code, dataset, and simulation report at
https://anonymous.4open.science/r/LVS_ST_CALIB-015F/README.md.

</details>


### [188] [Tendon-Based Proprioception in an Anthropomorphic Underactuated Robotic Hand with Series Elastic Actuators](https://arxiv.org/abs/2509.12969)
*Jae-Hyun Lee,Jonghoo Park,Kyu-Jin Cho*

Main category: cs.RO

TL;DR: 本文提出一种基于肌腱的本体感觉人工手，通过集成串联弹性驱动器实现无需视觉或触觉反馈的高精度抓取和对象识别。


<details>
  <summary>Details</summary>
Motivation: 现有的拟人型欠驱动手虽然结构简单但感知能力有限，特别是在实用抓取中如何高效集成传感与正确解释传感信号仍是难题；作者想要解决复杂抓取环境下手-物体交互的实时感知与操作问题。

Method: 开发了一种新型、高精度的紧凑型串联弹性驱动器（SEA），将其集成进无传感器手指内，并通过基于势能的建模，结合本体感觉信号来估算接触时间、关节角度、对象相对刚度和外部扰动等关键变量，实现了对手指状态与抓取动作的实时评估。

Result: 通过手指级实验和手部级演示验证，该系统能对不同形状与刚度的对象仅利用本体感觉实现安全、有效的盲抓取与抓取姿态重构。

Conclusion: 基于肌腱的本体感觉为实际操控提供了紧凑、可靠的感知方式，无需依赖视觉或触觉传感，在实际抓取任务中展现出良好的泛化能力和实用价值。

Abstract: Anthropomorphic underactuated hands are widely employed for their versatility
and structural simplicity. In such systems, compact sensing integration and
proper interpretation aligned with underactuation are crucial for realizing
practical grasp functionalities. This study proposes an anthropomorphic
underactuated hand that achieves comprehensive situational awareness of
hand-object interaction, utilizing tendon-based proprioception provided by
series elastic actuators (SEAs). We developed a compact SEA with high accuracy
and reliability that can be seamlessly integrated into sensorless fingers. By
coupling proprioceptive sensing with potential energy-based modeling, the
system estimates key grasp-related variables, including contact timing, joint
angles, relative object stiffness, and finger configuration changes indicating
external disturbances. These estimated variables enable grasp posture
reconstruction, safe handling of deformable objects, and blind grasping with
proprioceptive-only recognition of objects with varying geometry and stiffness.
Finger-level experiments and hand-level demonstrations confirmed the
effectiveness of the proposed approach. The results demonstrate that
tendon-based proprioception serves as a compact and robust sensing modality for
practical manipulation without reliance on vision or tactile feedback.

</details>


### [189] [Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins](https://arxiv.org/abs/2509.12982)
*Erblin Isaku,Hassan Sartaj,Shaukat Ali,Beatriz Sanguino,Tongtong Wang,Guoyuan Li,Houxiang Zhang,Thomas Peyrucain*

Main category: cs.RO

TL;DR: 本文提出了一种基于数字孪生（digital twin）的自适应机器人（SAR）异常行为与分布外（OOD）检测方法ODiSAR，能高效识别并解释异常行为，在工业与航海机器人场景均表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前自适应机器人在复杂与不确定环境下运行时，面临异常与OOD行为的主动检测难题。数字孪生作为虚实融合工具，具备捕捉异常的潜力，如何设计高效、可解释的OOD检测框架，提升机器人自适应能力，是该研究的主要动机。

Method: ODiSAR采用基于Transformer的数字孪生，预测机器人的未来状态，通过重构误差与MC dropout量化不确定性。二者结合用于OOD检测。同时，该数字孪生融合解释层，能够定位异常与具体机器人状态关联。方法在办公室机器人与航海机器人两场景上做了实验验证。

Result: ODiSAR在两个实际自适应机器人场景（办公室导航与船舶导航）均实现了高效OOD检测性能（最高98% AUROC, 96% TNR@TPR95, 95% F1分数），并提供异常行为的解释分析。

Conclusion: 基于数字孪生的ODiSAR达到高准确率的主动分布外检测，可用于支持机器人在复杂环境下的自适应，同时具有良好的解释性，对未来SAR系统具备实际应用价值。

Abstract: Self-adaptive robots (SARs) in complex, uncertain environments must
proactively detect and address abnormal behaviors, including
out-of-distribution (OOD) cases. To this end, digital twins offer a valuable
solution for OOD detection. Thus, we present a digital twin-based approach for
OOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to
forecast SAR states and employs reconstruction error and Monte Carlo dropout
for uncertainty quantification. By combining reconstruction error with
predictive variance, the digital twin effectively detects OOD behaviors, even
in previously unseen conditions. The digital twin also includes an
explainability layer that links potential OOD to specific SAR states, offering
insights for self-adaptation. We evaluated ODiSAR by creating digital twins of
two industrial robots: one navigating an office environment, and another
performing maritime ship navigation. In both cases, ODiSAR forecasts SAR
behaviors (i.e., robot trajectories and vessel motion) and proactively detects
OOD events. Our results showed that ODiSAR achieved high detection performance
-- up to 98\% AUROC, 96\% TNR@TPR95, and 95\% F1-score -- while providing
interpretable insights to support self-adaptation.

</details>


### [190] [DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D Perception](https://arxiv.org/abs/2509.13024)
*Haohan Min,Zhoujian Li,Yu Yang,Jinyu Chen,Shenghai Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种新型端到端视觉自动停靠方法DVDP，只需安装双目RGB-D相机，即可直接输出机器人的停靠轨迹，实现高效、精准的自动对接。通过大量虚实结合数据集训练、综合评测，方法在多项指标上优于主流方案，并在实际机器人上部署验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自动停靠方法对初始位置要求高，影响应用灵活性和实用价值，且需要进一步提升精度和降低系统布署成本。本文旨在解决这些瓶颈，提升移动机器人自动停靠的实用性。

Method: 提出DVDP端到端视觉停靠策略，利用简单的双目RGB-D相机直接输出机器人轨迹。采用Unity 3D平台结合真实场景，构建大规模虚实结合的数据集，同时研发新型评估指标系统，对方法进行量化测试，并与主流视觉感知方法改造后进行对比，最后在SCOUT Mini机器人上实地部署测试。

Result: 实验显示，DVDP在多项自动停靠性能指标上显著优于当前主流感知方法；模型能生成平滑、可行的轨迹，并在现实机器人上顺利实现对目标位姿的精准停靠，满足物理约束。

Conclusion: DVDP方法降低了初始位置约束，提升视觉自动停靠性能，具备高精度、低成本等优势，能有效应用于实际移动机器人自动停靠任务，具有良好推广前景。

Abstract: Automatic docking has long been a significant challenge in the field of
mobile robotics. Compared to other automatic docking methods, visual docking
methods offer higher precision and lower deployment costs, making them an
efficient and promising choice for this task. However, visual docking methods
impose strict requirements on the robot's initial position at the start of the
docking process. To overcome the limitations of current vision-based methods,
we propose an innovative end-to-end visual docking method named DVDP(direct
visual docking policy). This approach requires only a binocular RGB-D camera
installed on the mobile robot to directly output the robot's docking path,
achieving end-to-end automatic docking. Furthermore, we have collected a
large-scale dataset of mobile robot visual automatic docking dataset through a
combination of virtual and real environments using the Unity 3D platform and
actual mobile robot setups. We developed a series of evaluation metrics to
quantify the performance of the end-to-end visual docking method. Extensive
experiments, including benchmarks against leading perception backbones adapted
into our framework, demonstrate that our method achieves superior performance.
Finally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,
with our model generating smooth, feasible docking trajectories that meet
physical constraints and reach the target pose.

</details>


### [191] [Practical Handling of Dynamic Environments in Decentralised Multi-Robot Patrol](https://arxiv.org/abs/2509.13069)
*James C. Ward,Arthur Richards,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 本文提出了一种新的完全去中心化的多机器人巡逻方法，专门应对环境动态剧烈变化的场景，并在实验中取得了优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在安全、环境监测和灾后恢复等领域，对多机器人团队进行持续监控具有重要意义。由于环境的高度动态性，传统中心化或静态分配方法不够鲁棒和适应。因此，发展能够实时、自适应、去中心化的监控方法成为亟需解决的问题，以增强系统的鲁棒性和可扩展性。

Method: 作者针对多机器人巡逻任务，提出了一种新的完全在线分布式算法，每个机器人能够独立观测并实时响应环境中路线通行性的变化，从而持续最小化对兴趣点的访问间隔时间。该方法不依赖中心化控制，也无需提前了解环境动态。

Result: 实验结果显示，该方法在高度动态的场景下明显优于现实基线方法。此外，作者还分析了一些场景，在这些场景下，对环境动态进行显式建模可能不是必须的甚至不切实际。

Conclusion: 该方法能够有效改善多机器人巡逻队伍在高度动态环境下的表现，验证了去中心化、在线应对环境变化的实用性和优势。

Abstract: Persistent monitoring using robot teams is of interest in fields such as
security, environmental monitoring, and disaster recovery. Performing such
monitoring in a fully on-line decentralised fashion has significant potential
advantages for robustness, adaptability, and scalability of monitoring
solutions, including, in principle, the capacity to effectively adapt in
real-time to a changing environment. We examine this through the lens of
multi-robot patrol, in which teams of patrol robots must persistently minimise
time between visits to points of interest, within environments where
traversability of routes is highly dynamic. These dynamics must be observed by
patrol agents and accounted for in a fully decentralised on-line manner. In
this work, we present a new method of monitoring and adjusting for environment
dynamics in a decentralised multi-robot patrol team. We demonstrate that our
method significantly outperforms realistic baselines in highly dynamic
scenarios, and also investigate dynamic scenarios in which explicitly
accounting for environment dynamics may be unnecessary or impractical.

</details>


### [192] [Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of Freedom by Fusing the Abduction of Digits Four and Five](https://arxiv.org/abs/2509.13074)
*Simon Fritsch,Liam Achenbach,Riccardo Bianco,Nicola Irmiger,Gawain Marti,Samuel Visca,Chenyu Yang,Davide Liconti,Barnabas Gavin Cangan,Robert Jomar Malate,Ronan J. Hinchet,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本论文提出了一种16自由度（DoF）的SABD机械手，通过创新的结构设计提升了机械手的抓取范围和灵活性，并减少了所需执行器数量。实验和学习结果验证了该手的高性能和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统拟人机械手在仿真手部动作的同时，执行器数量多、抓取范围有限、难以完成超越人手的新型操控动作。为提升机械手抓取和操作能力，需要突破纯拟人设计的局限。

Method: 将四、五指的内收/外展（Add/Abd）关节合并为一个大幅度运动的联合关节，从而扩大手指运动空间，并用更少的自由度实现更灵活的操作。采用实验和强化学习测试其抓取能力和灵活性。

Result: 合并后的关节使手指可操作空间增加了400%，手可抓取最长侧宽达200mm的物体。在遥操作测试中，机械手在合适测试物体上的抓取成功率达86%。实验和强化学习均表明不仅适合大物体抓取，还提升了抓持稳定性。

Conclusion: 新型SABD机械手在无需增加结构复杂度的前提下，实现了更强的稳定抓握能力和灵活性，适用于多种实际应用场景，打破了拟人设计的瓶颈。

Abstract: This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand
that departs from purely anthropomorphic designs to achieve an expanded grasp
envelope, enable manipulation poses beyond human capability, and reduce the
required number of actuators. This is achieved by combining the
adduction/abduction (Add/Abd) joint of digits four and five into a single joint
with a large range of motion. The combined joint increases the workspace of the
digits by 400\% and reduces the required DoFs while retaining dexterity.
Experimental results demonstrate that the combined Add/Abd joint enables the
hand to grasp objects with a side distance of up to 200 mm. Reinforcement
learning-based investigations show that the design enables grasping policies
that are effective not only for handling larger objects but also for achieving
enhanced grasp stability. In teleoperated trials, the hand successfully
performed 86\% of attempted grasps on suitable YCB objects, including
challenging non-anthropomorphic configurations. These findings validate the
design's ability to enhance grasp stability, flexibility, and dexterous
manipulation without added complexity, making it well-suited for a wide range
of applications.

</details>


### [193] [A Design Co-Pilot for Task-Tailored Manipulators](https://arxiv.org/abs/2509.13077)
*Jonathan Külz,Sehoon Ha,Matthias Althoff*

Main category: cs.RO

TL;DR: 本文提出了一种自动化方案，可为特定环境快速设计并优化机器人形态，极大提升定制机器人的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 通用型机械臂难以针对特定任务优化，定制化开发周期长、成本高，而现有的模块化和计算设计方法尚无法实现高效的机器人定制，因此需要一种能自动、高效根据环境快速生成专用机器人设计的方法。

Method: 作者提出了一种生成式自动化设计方法，首先对不同类型的机器人学习逆运动学，通过全可微分框架进行梯度优化，实现对机器人结构和逆运动学解的联合微调，从而能快速生成环境特定的机器人形态。该方法极大加快了设计速度，并融合了人机协作。

Result: 通过数值实验，作者验证了方法能自动找到适应复杂环境和特定工作空间的机器人方案，同时适应不同硬件约束。并在现实中搭建了由仿真中设计的模块化机器人，成功完成障碍穿越任务。

Conclusion: 提出的新方法可作为设计助手，大幅缩短机器人定制设计时间，实现高效的人机协作并具有现实应用价值，有望广泛应用于需要快速机器人定制的工业场景。

Abstract: Although robotic manipulators are used in an ever-growing range of
applications, robot manufacturers typically follow a ``one-fits-all''
philosophy, employing identical manipulators in various settings. This often
leads to suboptimal performance, as general-purpose designs fail to exploit
particularities of tasks. The development of custom, task-tailored robots is
hindered by long, cost-intensive development cycles and the high cost of
customized hardware. Recently, various computational design methods have been
devised to overcome the bottleneck of human engineering. In addition, a surge
of modular robots allows quick and economical adaptation to changing industrial
settings. This work proposes an approach to automatically designing and
optimizing robot morphologies tailored to a specific environment. To this end,
we learn the inverse kinematics for a wide range of different manipulators. A
fully differentiable framework realizes gradient-based fine-tuning of designed
robots and inverse kinematics solutions. Our generative approach accelerates
the generation of specialized designs from hours with optimization-based
methods to seconds, serving as a design co-pilot that enables instant
adaptation and effective human-AI collaboration. Numerical experiments show
that our approach finds robots that can navigate cluttered environments,
manipulators that perform well across a specified workspace, and can be adapted
to different hardware constraints. Finally, we demonstrate the real-world
applicability of our method by setting up a modular robot designed in
simulation that successfully moves through an obstacle course.

</details>


### [194] [Empowering Multi-Robot Cooperation via Sequential World Models](https://arxiv.org/abs/2509.13095)
*Zijie Zhao,Honglei Guo,Shengqian Chen,Kaixuan Xu,Bo Jiang,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.RO

TL;DR: 提出了一种新的多智能体模型增强强化学习方法（SeqWM），通过顺序结构化的世界建模提升多机器人协作表现和采样效率。


<details>
  <summary>Details</summary>
Motivation: 模型增强强化学习在单机器人中效率高、规划能力强，但拓展到多机器人协作时，因联合动力学复杂导致困难。现有方法无法很好地解决多智能体意图分享以及高效通信的问题。

Method: 提出Sequential World Model（SeqWM），为每个智能体独立建立顺序结构化的世界模型，利用顺序通信机制实现智能体间的未来轨迹和意图分享，通信量随智能体数线性增长。通过这种方式分解复杂联合动力学，减少通信开销，提升协作效率。

Result: 在两个具有挑战性的模拟多机器人环境（Bi-DexHands和Multi-Quad）中，SeqWM在总体性能和采样效率上优于主流模型自由和模型增强多智能体强化学习基线，表现出预测性协作与角色分工等高级合作行为。实际四足机器人实验证明了方法在现实多机器人系统中的有效性。

Conclusion: SeqWM能够改善多机器人协作表现，提高采样效率，并具备现实部署潜力，是多智能体模型增强强化学习领域的重要进展。

Abstract: Model-based reinforcement learning (MBRL) has shown significant potential in
robotics due to its high sample efficiency and planning capability. However,
extending MBRL to multi-robot cooperation remains challenging due to the
complexity of joint dynamics. To address this, we propose the Sequential World
Model (SeqWM), a novel framework that integrates the sequential paradigm into
model-based multi-agent reinforcement learning. SeqWM employs independent,
sequentially structured agent-wise world models to decompose complex joint
dynamics. Latent rollouts and decision-making are performed through sequential
communication, where each agent generates its future trajectory and plans its
actions based on the predictions of its predecessors. This design enables
explicit intention sharing, enhancing cooperative performance, and reduces
communication overhead to linear complexity. Results in challenging simulated
environments (Bi-DexHands and Multi-Quad) show that SeqWM outperforms existing
state-of-the-art model-free and model-based baselines in both overall
performance and sample efficiency, while exhibiting advanced cooperative
behaviors such as predictive adaptation and role division. Furthermore, SeqWM
has been success fully deployed on physical quadruped robots, demonstrating its
effectiveness in real-world multi-robot systems. Demos and code are available
at: https://github.com/zhaozijie2022/seqwm-marl

</details>


### [195] [Model Predictive Control with Reference Learning for Soft Robotic Intracranial Pressure Waveform Modulation](https://arxiv.org/abs/2509.13109)
*Fabian Flürenbrock,Yanick Büchel,Johannes Köhler,Marianne Schmid Daners,Melanie N. Zeilinger*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的控制框架，用于软体机器人驱动系统，实现精准的颅内压（ICP）波形调控。它采用MPC和贝叶斯优化，有效提升了控制精度和效率。


<details>
  <summary>Details</summary>
Motivation: 调节颅内压波形对于研究脑脊液动力学和神经疾病的发病机制至关重要。以往方法难以实现安全、精确的波形控制，并存在系统非线性和未知扰动等挑战，因此需引入更先进的控制策略。

Method: 提出两层控制框架：第一层为带有扰动观测器的模型预测控制器（MPC），实现在安全约束下对电机位置轨迹的无偏跟踪；第二层采用贝叶斯优化（BO）算法，在线学习能够实现目标ICP波形的电机参考轨迹。实验在脑仿生体试验台上进行。

Result: MPC相较于传统PID控制器，将电机位置参考跟踪的平均和最大误差分别降低了83%和73%；BO算法通过不到20次迭代学得了能实现期望波形均值和幅值的轨迹。

Conclusion: 该学习型控制框架实现了对颅内压波形的高效、精确和安全调控，优于常规控制方法，为脑部动力学和疾病机理研究提供了更有效的控制工具。

Abstract: This paper introduces a learning-based control framework for a soft robotic
actuator system designed to modulate intracranial pressure (ICP) waveforms,
which is essential for studying cerebrospinal fluid dynamics and pathological
processes underlying neurological disorders. A two-layer framework is proposed
to safely achieve a desired ICP waveform modulation. First, a model predictive
controller (MPC) with a disturbance observer is used for offset-free tracking
of the system's motor position reference trajectory under safety constraints.
Second, to address the unknown nonlinear dependence of ICP on the motor
position, we employ a Bayesian optimization (BO) algorithm used for online
learning of a motor position reference trajectory that yields the desired ICP
modulation. The framework is experimentally validated using a test bench with a
brain phantom that replicates realistic ICP dynamics in vitro. Compared to a
previously employed proportional-integral-derivative controller, the MPC
reduces mean and maximum motor position reference tracking errors by 83 % and
73 %, respectively. In less than 20 iterations, the BO algorithm learns a motor
position reference trajectory that yields an ICP waveform with the desired mean
and amplitude.

</details>


### [196] [Hydrosoft: Non-Holonomic Hydroelastic Models for Compliant Tactile Manipulation](https://arxiv.org/abs/2509.13126)
*Miquel Oller,An Dang,Nima Fazeli*

Main category: cs.RO

TL;DR: 本文提出了一种高效的非完整水弹性模型，用于准确描述触觉传感器在与物体接触时产生的路径相关力分布及动态表面积变化。该模型首次将顺应性传感器产生的分布式力纳入系统状态空间，并采用可微分公式以实现基于梯度的轨迹优化，有效结合高分辨率触觉反馈。实验结果验证了方法的有效性，并强调了建模路径相关动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管触觉传感器为机器人提供了感知能力，但其内在的顺应性在力互动中作用巨大却未被充分建模。传统模型难以捕捉由顺应性部件引入的复杂、非线性和路径相关的动态行为。本文旨在解决如何高效准确地对这类复杂动态进行建模的问题。

Method: 作者提出了一种非完整水弹性建模方法，显式扩展了对象的状态空间，将顺应性传感器产生的分布式力纳入考量，并采用可微公式实现路径相关动力学的建模。通过此方法，能够利用高分辨率触觉信号，进行基于梯度的轨迹优化。

Result: 所提方法在多种模拟与实际机器人实验中表现出色，能够准确再现顺应性触觉传感器在接触过程中表现出的路径依赖力分布和表面积变化。同时，实验展示了模型在实际路径优化和机器人操作任务中的有效性和优势。

Conclusion: 本文工作证明了深入建模顺应性触觉传感器的路径依赖动态对于提升机器人操作性能具有显著价值。提出的高效水弹性建模与基于梯度的优化方法能够有效结合真实触觉反馈，推动顺应性触觉传感器在先进机器人系统中的全面应用。

Abstract: Tactile sensors have long been valued for their perceptual capabilities,
offering rich insights into the otherwise hidden interface between the robot
and grasped objects. Yet their inherent compliance -- a key driver of
force-rich interactions -- remains underexplored. The central challenge is to
capture the complex, nonlinear dynamics introduced by these passive-compliant
elements. Here, we present a computationally efficient non-holonomic
hydroelastic model that accurately models path-dependent contact force
distributions and dynamic surface area variations. Our insight is to extend the
object's state space, explicitly incorporating the distributed forces generated
by the compliant sensor. Our differentiable formulation not only accounts for
path-dependent behavior but also enables gradient-based trajectory
optimization, seamlessly integrating with high-resolution tactile feedback. We
demonstrate the effectiveness of our approach across a range of simulated and
real-world experiments and highlight the importance of modeling the path
dependence of sensor dynamics.

</details>


### [197] [An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios](https://arxiv.org/abs/2509.13132)
*Zhihao Zhang,Chengyang Peng,Minghao Zhu,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 本文提出了一种结合多通道鸟瞰图占用网格和Transformer序列建模的新框架，并通过引入不确定性加权决策Transformer（UWDT）有针对性地提升无人驾驶车辆在复杂环形路口环境中的决策安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶在处理密集且动态的交通环境（如环形路口）时，面临如何兼顾空间结构、长期时序依赖以及不确定性鲁棒性的挑战，常规方法难以有效处理稀有但高风险的场景。

Method: 方法上，作者构建了多通道鸟瞰图占用格作为输入，采用Transformer进行状态序列建模；提出了UWDT机制，通过冻结教师Transformer估算每步预测的不确定性（信息熵），并据此加权学生模型的损失函数，从而聚焦高影响状态的学习。

Result: 在不同交通密度的环形路口仿真实验中，UWDT在总体奖励、碰撞率和行为稳定性等指标均优于现有方法。

Conclusion: 基于不确定性感知的时空Transformer模型能在复杂交通场景下显著提升自动驾驶决策的安全性与效率。

Abstract: Autonomous driving in dense, dynamic environments requires decision-making
systems that can exploit both spatial structure and long-horizon temporal
dependencies while remaining robust to uncertainty. This work presents a novel
framework that integrates multi-channel bird's-eye-view occupancy grids with
transformer-based sequence modeling for tactical driving in complex roundabout
scenarios. To address the imbalance between frequent low-risk states and rare
safety-critical decisions, we propose the Uncertainty-Weighted Decision
Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate
per-token predictive entropy, which is then used as a weight in the student
model's loss function. This mechanism amplifies learning from uncertain,
high-impact states while maintaining stability across common low-risk
transitions. Experiments in a roundabout simulator, across varying traffic
densities, show that UWDT consistently outperforms other baselines in terms of
reward, collision rate, and behavioral stability. The results demonstrate that
uncertainty-aware, spatial-temporal transformers can deliver safer and more
efficient decision-making for autonomous driving in complex traffic
environments.

</details>


### [198] [TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving](https://arxiv.org/abs/2509.13164)
*Jiawei Wang,Haowei Sun,Xintao Yan,Shuo Feng,Jun Gao,Henry X. Liu*

Main category: cs.RO

TL;DR: 本文提出了TeraSim-World，一种能够自动合成现实且地理多样的自动驾驶关键安全场景数据的新系统，支持全球任何地理区域，为端到端（E2E）自动驾驶模型的训练与评测提供大规模高质量数据。


<details>
  <summary>Details</summary>
Motivation: 为了实现安全且可扩展的自动驾驶部署，现有的数据集存在短板：模拟仿真数据与真实世界有较大差距（sim-to-real gap），而实地采集又昂贵且风险高。因此，亟需能大规模生成真实感强、地理多样性高、覆盖安全关键事件的数据。

Method: TeraSim-World以任意目标地点为起点，从地理空间数据源获取真实地图与交通需求；结合基于自然驾驶行为数据进行智能体行为仿真；通过引入多样的异常与极端情况，合成大量边缘案例；再利用定位街景与先进的视频生成模型（Cosmos-Drive）实现传感器级的实景拟合视效。通过联动代理行为仿真与感知数据渲染，形成端到端数据闭环。

Result: TeraSim-World能在全球范围内自动、大规模地合成真实感和地理基础性均强的安全关键数据，显著提升了E2E自动驾驶系统的数据多样化、覆盖率与高危场景再现能力。

Conclusion: TeraSim-World为E2E自动驾驶系统的训练与评估提供了一个可拓展、高质量且安全的数据生成平台，有望加速自动驾驶的安全部署和技术进步。

Abstract: Safe and scalable deployment of end-to-end (E2E) autonomous driving requires
extensive and diverse data, particularly safety-critical events. Existing data
are mostly generated from simulators with a significant sim-to-real gap or
collected from on-road testing that is costly and unsafe. This paper presents
TeraSim-World, an automated pipeline that synthesizes realistic and
geographically diverse safety-critical data for E2E autonomous driving at
anywhere in the world. Starting from an arbitrary location, TeraSim-World
retrieves real-world maps and traffic demand from geospatial data sources.
Then, it simulates agent behaviors from naturalistic driving datasets, and
orchestrates diverse adversities to create corner cases. Informed by street
views of the same location, it achieves photorealistic, geographically grounded
sensor rendering via the frontier video generation model Cosmos-Drive. By
bridging agent and sensor simulations, TeraSim-World provides a scalable and
critical~data synthesis framework for training and evaluation of E2E autonomous
driving systems.

</details>


### [199] [ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation](https://arxiv.org/abs/2509.13177)
*Salvatore Esposito,Matías Mattamala,Daniel Rebain,Francis Xiatian Zhang,Kevin Dhaliwal,Mohsen Khadem,Subramanian Ramamoorthy*

Main category: cs.RO

TL;DR: 本文提出了ROOM（Realistic Optical Observation in Medicine）仿真框架，用于生成逼真的支气管镜训练数据，以支持连续体机器人在复杂肺部操作中的研发和测试。ROOM利用病人CT扫描生成多模态传感器数据，并通过实验证明了其在医学机器人应用如多视角位姿估计和单目深度估计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前连续体机器人在支气管镜检查中发展迅速，但因伦理和患者安全顾虑，现实数据采集受限，缺乏高质量训练和测试环境，影响自主算法迭代。迫切需要一种能够在不依赖真实病例的前提下，提供多样化、真实感强的训练数据与反馈的方法。

Method: ROOM框架从病人CT扫描入手，渲染生成包括带真实噪声和高光的RGB图像、度量深度图、表面法线、光流和点云等多模态医疗级别感知数据。通过这些数据在多视角位姿估计和单目深度估计两大机器人医学任务进行验证。

Result: ROOM生成的数据展现出符合临床需求的复杂多样性，能显著提升单目深度估计等主流模型在医学场景下的迁移能力。同时，为导航等下游任务提供了更扎实的数据基础。

Conclusion: ROOM为医学机器人领域提供了一种高质量模拟数据生成工具，有望推动大规模、多样病例与场景下的数据获取，促进相关算法在临床实际中的应用和发展。

Abstract: Continuum robots are advancing bronchoscopy procedures by accessing complex
lung airways and enabling targeted interventions. However, their development is
limited by the lack of realistic training and test environments: Real data is
difficult to collect due to ethical constraints and patient safety concerns,
and developing autonomy algorithms requires realistic imaging and physical
feedback. We present ROOM (Realistic Optical Observation in Medicine), a
comprehensive simulation framework designed for generating photorealistic
bronchoscopy training data. By leveraging patient CT scans, our pipeline
renders multi-modal sensor data including RGB images with realistic noise and
light specularities, metric depth maps, surface normals, optical flow and point
clouds at medically relevant scales. We validate the data generated by ROOM in
two canonical tasks for medical robotics -- multi-view pose estimation and
monocular depth estimation, demonstrating diverse challenges that
state-of-the-art methods must overcome to transfer to these medical settings.
Furthermore, we show that the data produced by ROOM can be used to fine-tune
existing depth estimation models to overcome these challenges, also enabling
other downstream applications such as navigation. We expect that ROOM will
enable large-scale data generation across diverse patient anatomies and
procedural scenarios that are challenging to capture in clinical settings. Code
and data: https://github.com/iamsalvatore/room.

</details>


### [200] [StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening](https://arxiv.org/abs/2509.13200)
*Moonyoung Lee,Dong Ki Kim,Jai Krishna Bandi,Max Smith,Aileen Liao,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 提出了一种新的人形机器人开门方法StageACT，通过引入任务阶段条件提升机器人在不确定环境下的表现，在真实办公环境中显著提高了开门成功率。


<details>
  <summary>Details</summary>
Motivation: 开门作为人形机器人进入现实环境的重要技能，面对门闩状态不可见等部分可观测问题时，传统模仿学习易出现动作混乱，成功率低，急需更鲁棒的学习方法。

Method: 提出了StageACT，一种阶段条件模仿学习方法，在低层策略中引入任务阶段信息，使机器人能根据不同阶段合理规划动作，增强对部分可观测信息的鲁棒性，并支持通过提示实现恢复行为。

Result: 在实际办公环境，人形机器人对未见过的门使用StageACT取得55%成功率，远超以往的基线方法，同时完成开门任务时间更短。

Conclusion: 任务阶段条件是一种轻量但有效的机制，有助于提升人形机器人解决长时序操作问题的能力，尤其在开门等多步操作任务中表现出色。

Abstract: Humanoid robots promise to operate in everyday human environments without
requiring modifications to the surroundings. Among the many skills needed,
opening doors is essential, as doors are the most common gateways in built
spaces and often limit where a robot can go. Door opening, however, poses
unique challenges as it is a long-horizon task under partial observability,
such as reasoning about the door's unobservable latch state that dictates
whether the robot should rotate the handle or push the door. This ambiguity
makes standard behavior cloning prone to mode collapse, yielding blended or
out-of-sequence actions. We introduce StageACT, a stage-conditioned imitation
learning framework that augments low-level policies with task-stage inputs.
This effective addition increases robustness to partial observability, leading
to higher success rates and shorter completion times. On a humanoid operating
in a real-world office environment, StageACT achieves a 55% success rate on
previously unseen doors, more than doubling the best baseline. Moreover, our
method supports intentional behavior guidance through stage prompting, enabling
recovery behaviors. These results highlight stage conditioning as a lightweight
yet powerful mechanism for long-horizon humanoid loco-manipulation.

</details>


### [201] [HARMONIC: A Content-Centric Cognitive Robotic Architecture](https://arxiv.org/abs/2509.13279)
*Sanjay Oruganti,Sergei Nirenburg,Marjorie McShane,Jesse English,Michael K. Roberts,Christian Arndt,Carlos Gonzalez,Mingyo Seo,Luis Sentis*

Main category: cs.RO

TL;DR: 本文提出了HARMONIC，这是一种面向人机团队的认知机器人架构，旨在提升机器人在语义认知、决策和语言交流能力，并保障安全与结果质量。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在与人协作时，普遍存在数据稀缺、可解释性差和安全性不高等问题，难以获得用户信任。本研究希望通过新的架构设计，提升人机协作的效果和信任度。

Method: 设计了HARMONIC认知机器人架构，使机器人具备语义感知解释、人类类似的决策能力和有意图的语言交流能力。此外，系统注重安全、结果质量和透明性，并通过高保真模拟环境以及实际机器人平台分别实现并验证了两个基于HARMONIC的系统。

Result: 开发了两套采用HARMONIC架构的机器人系统，通过仿真和物理平台的实践，证明了其可行性和有效性。

Conclusion: HARMONIC架构能够提升机器人在与人协作时的智能和沟通能力，同时加强安全性和可解释性，从而促进人机团队的透明和信任。

Abstract: This paper introduces HARMONIC, a cognitive-robotic architecture designed for
robots in human-robotic teams. HARMONIC supports semantic perception
interpretation, human-like decision-making, and intentional language
communication. It addresses the issues of safety and quality of results; aims
to solve problems of data scarcity, explainability, and safety; and promotes
transparency and trust. Two proof-of-concept HARMONIC-based robotic systems are
demonstrated, each implemented in both a high-fidelity simulation environment
and on physical robotic platforms.

</details>


### [202] [Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic Reward Curriculum](https://arxiv.org/abs/2509.13239)
*Tianxu An,Flavio De Vincenti,Yuntao Ma,Marco Hutter,Stelian Coros*

Main category: cs.RO

TL;DR: 本文提出了一种分层强化学习（RL）方法，用于让单臂足式机器人和双机器人系统端到端地完成长时间的抓取与放置任务，并通过动态奖励机制和分层策略大幅提升了训练效率和协作能力。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法在处理类似抓取与放置这样长时序、多阶段的任务时，效率较低，尤其在多机器人协作场景下，挑战更大。因此，亟需设计更高效、可扩展的方法来提升机器人自主操作和协作的效果。

Method: 本文设计了一个分层强化学习管线，包括动态奖励课程，根据任务的阶段性目标引导机器人逐步学习复杂任务。此外，提出了能在不同任务阶段自主调整关注点并协作完成任务的双机器人策略。

Result: 在仿真实验中，所提方法比现有长时序RL方法训练效率提高55%，执行时间缩短18.6%；双机器人策略表现出更好的阶段性注意力分配与协同能力。在真实的ANYmal D机器人上，同样验证了方法的有效性。

Conclusion: 本论文首次提出能同时支持单、双足式机械臂机器人高效协作完成复杂抓取与放置任务的RL管线，在提升训练效率、执行表现以及多机器人协同智能方面取得新的突破。

Abstract: We present a hierarchical RL pipeline for training one-armed legged robots to
perform pick-and-place (P&P) tasks end-to-end -- from approaching the payload
to releasing it at a target area -- in both single-robot and cooperative
dual-robot settings. We introduce a novel dynamic reward curriculum that
enables a single policy to efficiently learn long-horizon P&P operations by
progressively guiding the agents through payload-centered sub-objectives.
Compared to state-of-the-art approaches for long-horizon RL tasks, our method
improves training efficiency by 55% and reduces execution time by 18.6% in
simulation experiments. In the dual-robot case, we show that our policy enables
each robot to attend to different components of its observation space at
distinct task stages, promoting effective coordination via autonomous attention
shifts. We validate our method through real-world experiments using ANYmal D
platforms in both single- and dual-robot scenarios. To our knowledge, this is
the first RL pipeline that tackles the full scope of collaborative P&P with two
legged manipulators.

</details>


### [203] [Design and Control of a Perching Drone Inspired by the Prey-Capturing Mechanism of Venus Flytrap](https://arxiv.org/abs/2509.13249)
*Ye Li,Daming Liu,Yanhe Zhu,Junming Zhang,Yongsheng Luo,Ziqi Wang,Chenyu Liu,Jie Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新型无人机，采用仿生性强的灵活抓附结构，能在极短时间内实现高效稳固地停留（perching），从而节约能量并延长飞行任务时长。通过引入级联高增益观测器控制方法，有效提升了无人机在风和外界扰动下的鲁棒性。实验结果验证了该系统的优越性能。


<details>
  <summary>Details</summary>
Motivation: 无人机续航与能效一直是限制其任务时间和应用范围的瓶颈。现有研究多致力于通过抓附/停留等机制降低能耗，但在快速性、稳定性和适应复杂环境方面仍有不足。因此，亟需设计兼具高效抓附与动态适应性的无人机方案。

Method: 借鉴捕蝇草快速捕食的仿生原理，设计了一种主动型柔性抓附机制，无人机能在不足100毫秒内完成抓附动作。结构开发后，通过级联扩展高增益观测器(EHGO)设计实时估计及补偿外部扰动，提高系统鲁棒性，并开展实验验证系统性能。

Result: 结果显示新型无人机结构具备高度适应性，能在多种目标上高速停留。同时，级联EHGO方法使无人机在风力和抓附扰动条件下表现出更强的稳定性和抗干扰能力。

Conclusion: 基于捕蝇草的仿生柔性结构和级联高增益观测控制器的结合，显著提升了无人机抓附的速度、稳定性和环境适应性，为提升无人机能效和续航能力提供了切实有效的新思路。

Abstract: The endurance and energy efficiency of drones remain critical challenges in
their design and operation. To extend mission duration, numerous studies
explored perching mechanisms that enable drones to conserve energy by
temporarily suspending flight. This paper presents a new perching drone that
utilizes an active flexible perching mechanism inspired by the rapid predation
mechanism of the Venus flytrap, achieving perching in less than 100 ms. The
proposed system is designed for high-speed adaptability to the perching
targets. The overall drone design is outlined, followed by the development and
validation of the biomimetic perching structure. To enhance the system
stability, a cascade extended high-gain observer (EHGO) based control method is
developed, which can estimate and compensate for the external disturbance in
real time. The experimental results demonstrate the adaptability of the
perching structure and the superiority of the cascaded EHGO in resisting wind
and perching disturbances.

</details>
